__FILENAME__ = assertions
import re, cql

def assert_unavailable(fun, *args):
    import cql
    try:
        if len(args) == 0:
            fun(None)
        else:
            fun(*args)
    except cql.OperationalError as e:
        msg = str(e)
        assert re.search('one or more nodes were unavailable', msg), "Expecting unavailable exception, got: " + msg
    except Exception as e:
        assert False, "Expecting unavailable exception, got: " + str(e)
    else:
        assert False, "Expecting unavailable exception but no exception was raised"

def assert_almost_equal(*args, **kwargs):
    try:
        error = kwargs['error']
    except KeyError:
        error = 0.16

    vmax = max(args)
    vmin = min(args)
    assert vmin > vmax * (1.0 - error), "values not within %.2f%% of the max: %s" % (error * 100, args)

def assert_invalid(cursor, query, matching = None):
    try:
        cursor.execute(query)
        assert False, "Expecting query to be invalid: got %s" % cursor.fetchall()
    except cql.ProgrammingError as e:
        msg = str(e)
        if matching is not None:
            assert re.search(matching, msg), "Error message does not contain " + matching + " (error = " + msg + ")"

def assert_one(cursor, query, expected, cl='ONE'):
    cursor.execute(query, consistency_level=cl)
    res = cursor.fetchall()
    assert res == [expected], res

def assert_none(cursor, query, cl='ONE'):
    cursor.execute(query, consistency_level=cl)
    res = cursor.fetchall()
    assert res == [], res

def assert_all(cursor, query, expected, cl='ONE'):
    cursor.execute(query, consistency_level=cl)
    res = cursor.fetchall()
    assert res == expected, res

########NEW FILE########
__FILENAME__ = auth_test
import time

from cql import ProgrammingError
from cql.cassandra.ttypes import AuthenticationException
from dtest import Tester, debug
from tools import *

class TestAuth(Tester):

    def __init__(self, *args, **kwargs):
        self.ignore_log_patterns = [
            # This one occurs if we do a non-rolling upgrade, the node
            # it's trying to send the migration to hasn't started yet,
            # and when it does, it gets replayed and everything is fine.
            r'Can\'t send migration request: node.*is down',
        ]
        Tester.__init__(self, *args, **kwargs)

    @require('https://issues.apache.org/jira/browse/CASSANDRA-7011')
    def system_auth_ks_is_alterable_test(self):
        self.prepare(nodes=3)
        debug("nodes started")
        schema_query = """SELECT strategy_options
                          FROM system.schema_keyspaces
                          WHERE keyspace_name = 'system_auth'"""

        cursor = self.get_cursor(0, user='cassandra', password='cassandra')
        cursor.execute(schema_query)
        row = cursor.fetchone()
        self.assertEqual('{"replication_factor":"1"}', row[0])


        cursor.execute("""
            ALTER KEYSPACE system_auth
                WITH replication = {'class':'SimpleStrategy', 'replication_factor':3};
        """)

        # make sure schema change is persistent
        debug("Stopping cluster..")
        self.cluster.stop()
        debug("Restarting cluster..")
        self.cluster.start()

        time.sleep(15)

        for i in range(3):
            debug('Checking node: {i}'.format(i=i))
            cursor = self.get_cursor(i, user='cassandra', password='cassandra')
            cursor.execute(schema_query)
            row = cursor.fetchone()
            self.assertEqual('{"replication_factor":"3"}', row[0])

    def login_test(self):
        # also tests default user creation (cassandra/cassandra)
        self.prepare()
        self.get_cursor(user='cassandra', password='cassandra')
        with self.assertRaises(AuthenticationException):
            self.get_cursor(user='cassandra', password='badpassword')
        with self.assertRaises(AuthenticationException):
            self.get_cursor(user='doesntexist', password='doesntmatter')

    def only_superuser_can_create_users_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER jackob WITH PASSWORD '12345' NOSUPERUSER")

        jackob = self.get_cursor(user='jackob', password='12345')
        with self.assertRaises(ProgrammingError) as cm:
            jackob.execute("CREATE USER james WITH PASSWORD '54321' NOSUPERUSER")
        self.assertEqual('Bad Request: Only superusers are allowed to perform CREATE USER queries',
                         cm.exception.message)

    def password_authenticator_create_user_requires_password_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute("CREATE USER jackob NOSUPERUSER")
        self.assertEqual('Bad Request: PasswordAuthenticator requires PASSWORD option',
                         cm.exception.message)

    def cant_create_existing_user_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        cursor.execute("CREATE USER 'james@example.com' WITH PASSWORD '12345' NOSUPERUSER")
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute("CREATE USER 'james@example.com' WITH PASSWORD '12345' NOSUPERUSER")
        self.assertEqual('Bad Request: User james@example.com already exists',
                         cm.exception.message)

    def list_users_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        cursor.execute("CREATE USER alex WITH PASSWORD '12345' NOSUPERUSER")
        cursor.execute("CREATE USER bob WITH PASSWORD '12345' SUPERUSER")
        cursor.execute("CREATE USER cathy WITH PASSWORD '12345' NOSUPERUSER")
        cursor.execute("CREATE USER dave WITH PASSWORD '12345' SUPERUSER")

        cursor.execute("LIST USERS")
        rows = cursor.fetchall()
        self.assertEqual(5, len(rows))
        # {username: isSuperuser} dict.
        users = dict([(r[0], r[1]) for r in rows])

        self.assertTrue(users['cassandra'])
        self.assertFalse(users['alex'])
        self.assertTrue(users['bob'])
        self.assertFalse(users['cathy'])
        self.assertTrue(users['dave'])

    def user_cant_drop_themselves_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        self.assertUnauthorized("Users aren't allowed to DROP themselves",
                                cursor, "DROP USER cassandra")

    def only_superusers_can_drop_users_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345' NOSUPERUSER")
        cassandra.execute("CREATE USER dave WITH PASSWORD '12345' NOSUPERUSER")
        cassandra.execute("LIST USERS")
        self.assertEqual(3, cassandra.rowcount)

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized('Only superusers are allowed to perform DROP USER queries',
                                cathy, 'DROP USER dave')

        cassandra.execute("LIST USERS")
        self.assertEqual(3, cassandra.rowcount)

        cassandra.execute('DROP USER dave')
        cassandra.execute("LIST USERS")
        self.assertEqual(2, cassandra.rowcount)

    def dropping_nonexistent_user_throws_exception_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        self.assertUnauthorized("User nonexistent doesn't exist",
                                cursor, 'DROP USER nonexistent')

    def regular_users_can_alter_their_passwords_only_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE USER bob WITH PASSWORD '12345'")

        cathy = self.get_cursor(user='cathy', password='12345')
        cathy.execute("ALTER USER cathy WITH PASSWORD '54321'")
        cathy = self.get_cursor(user='cathy', password='54321')
        self.assertUnauthorized("You aren't allowed to alter this user",
                                cathy, "ALTER USER bob WITH PASSWORD 'cantchangeit'")

    def users_cant_alter_their_superuser_status_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        self.assertUnauthorized("You aren't allowed to alter your own superuser status",
                                cursor, "ALTER USER cassandra NOSUPERUSER")

    def only_superuser_alters_superuser_status_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("Only superusers are allowed to alter superuser status",
                                cathy, "ALTER USER cassandra NOSUPERUSER")

        cassandra.execute("ALTER USER cathy SUPERUSER")

    def altering_nonexistent_user_throws_exception_test(self):
        self.prepare()

        cursor = self.get_cursor(user='cassandra', password='cassandra')
        self.assertUnauthorized("User nonexistent doesn't exist",
                                cursor, "ALTER USER nonexistent WITH PASSWORD 'doesn''tmatter'")

    def create_ks_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no CREATE permission on <all keyspaces> or any of its parents",
                                cathy,
                                "CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")

        cassandra.execute("GRANT CREATE ON ALL KEYSPACES TO cathy")
        cathy.execute("""CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}""")

    def create_cf_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no CREATE permission on <keyspace ks> or any of its parents",
                                cathy, "CREATE TABLE ks.cf (id int primary key)")

        cassandra.execute("GRANT CREATE ON KEYSPACE ks TO cathy")
        cathy.execute("CREATE TABLE ks.cf (id int primary key)")

    def alter_ks_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no ALTER permission on <keyspace ks> or any of its parents",
                                cathy,
                                "ALTER KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':2}")

        cassandra.execute("GRANT ALTER ON KEYSPACE ks TO cathy")
        cathy.execute("ALTER KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':2}")

    def alter_cf_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key)")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no ALTER permission on <table ks.cf> or any of its parents",
                                cathy, "ALTER TABLE ks.cf ADD val int")

        cassandra.execute("GRANT ALTER ON ks.cf TO cathy")
        cathy.execute("ALTER TABLE ks.cf ADD val int")

        cassandra.execute("REVOKE ALTER ON ks.cf FROM cathy")
        self.assertUnauthorized("User cathy has no ALTER permission on <table ks.cf> or any of its parents",
                                cathy, "CREATE INDEX ON ks.cf(val)")

        cassandra.execute("GRANT ALTER ON ks.cf TO cathy")
        cathy.execute("CREATE INDEX ON ks.cf(val)")

        cassandra.execute("REVOKE ALTER ON ks.cf FROM cathy")

        cathy.execute("USE ks")
        self.assertUnauthorized("User cathy has no ALTER permission on <table ks.cf> or any of its parents",
                                cathy, "DROP INDEX cf_val_idx")

        cassandra.execute("GRANT ALTER ON ks.cf TO cathy")
        cathy.execute("DROP INDEX cf_val_idx")

    def drop_ks_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no DROP permission on <keyspace ks> or any of its parents",
                                cathy, "DROP KEYSPACE ks")

        cassandra.execute("GRANT DROP ON KEYSPACE ks TO cathy")
        cathy.execute("DROP KEYSPACE ks")

    def drop_cf_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key)")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no DROP permission on <table ks.cf> or any of its parents",
                                cathy, "DROP TABLE ks.cf")

        cassandra.execute("GRANT DROP ON ks.cf TO cathy")
        cathy.execute("DROP TABLE ks.cf")

    def modify_and_select_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key, val int)")

        cathy = self.get_cursor(user='cathy', password='12345')
        self.assertUnauthorized("User cathy has no SELECT permission on <table ks.cf> or any of its parents",
                                cathy, "SELECT * FROM ks.cf")

        cassandra.execute("GRANT SELECT ON ks.cf TO cathy")
        cathy.execute("SELECT * FROM ks.cf")
        self.assertEquals(0, cathy.rowcount)

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "INSERT INTO ks.cf (id, val) VALUES (0, 0)")

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "UPDATE ks.cf SET val = 1 WHERE id = 1")

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "DELETE FROM ks.cf WHERE id = 1")

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "TRUNCATE ks.cf")

        cassandra.execute("GRANT MODIFY ON ks.cf TO cathy")
        cathy.execute("INSERT INTO ks.cf (id, val) VALUES (0, 0)")
        cathy.execute("UPDATE ks.cf SET val = 1 WHERE id = 1")
        cathy.execute("SELECT * FROM ks.cf")
        self.assertEquals(2, cathy.rowcount)

        cathy.execute("DELETE FROM ks.cf WHERE id = 1")
        cathy.execute("SELECT * FROM ks.cf")
        self.assertEquals(1, cathy.rowcount)

        cathy.execute("TRUNCATE ks.cf")
        self.assertEquals(0, cathy.rowcount)

    def grant_revoke_auth_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE USER bob WITH PASSWORD '12345'")

        cathy = self.get_cursor(user='cathy', password='12345')
        # missing both SELECT and AUTHORIZE
        self.assertUnauthorized("User cathy has no AUTHORIZE permission on <all keyspaces> or any of its parents",
                                cathy, "GRANT SELECT ON ALL KEYSPACES TO bob")

        cassandra.execute("GRANT AUTHORIZE ON ALL KEYSPACES TO cathy")

        # still missing SELECT
        self.assertUnauthorized("User cathy has no SELECT permission on <all keyspaces> or any of its parents",
                                cathy, "GRANT SELECT ON ALL KEYSPACES TO bob")

        cassandra.execute("GRANT SELECT ON ALL KEYSPACES TO cathy")

        # should succeed now with both SELECT and AUTHORIZE
        cathy.execute("GRANT SELECT ON ALL KEYSPACES TO bob")

    def grant_revoke_validation_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")

        self.assertUnauthorized("<keyspace nonexistent> doesn't exist",
                                cassandra, "GRANT ALL ON KEYSPACE nonexistent TO cathy")

        self.assertUnauthorized("User nonexistent doesn't exist",
                                cassandra, "GRANT ALL ON KEYSPACE ks TO nonexistent")

        self.assertUnauthorized("<keyspace nonexistent> doesn't exist",
                                cassandra, "REVOKE ALL ON KEYSPACE nonexistent FROM cathy")

        self.assertUnauthorized("User nonexistent doesn't exist",
                                cassandra, "REVOKE ALL ON KEYSPACE ks FROM nonexistent")

    def grant_revoke_cleanup_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key, val int)")
        cassandra.execute("GRANT ALL ON ks.cf TO cathy")

        cathy = self.get_cursor(user='cathy', password='12345')
        cathy.execute("INSERT INTO ks.cf (id, val) VALUES (0, 0)")
        cathy.execute("SELECT * FROM ks.cf")
        self.assertEquals(1, cathy.rowcount)

        # drop and recreate the user, make sure permissions are gone
        cassandra.execute("DROP USER cathy")
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "INSERT INTO ks.cf (id, val) VALUES (0, 0)")

        self.assertUnauthorized("User cathy has no SELECT permission on <table ks.cf> or any of its parents",
                                cathy, "SELECT * FROM ks.cf")

        # grant all the permissions back
        cassandra.execute("GRANT ALL ON ks.cf TO cathy")
        cathy.execute("INSERT INTO ks.cf (id, val) VALUES (0, 0)")
        cathy.execute("SELECT * FROM ks.cf")
        self.assertEqual(1, cathy.rowcount)

        # drop and recreate the keyspace, make sure permissions are gone
        cassandra.execute("DROP KEYSPACE ks")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key, val int)")

        self.assertUnauthorized("User cathy has no MODIFY permission on <table ks.cf> or any of its parents",
                                cathy, "INSERT INTO ks.cf (id, val) VALUES (0, 0)")

        self.assertUnauthorized("User cathy has no SELECT permission on <table ks.cf> or any of its parents",
                                cathy, "SELECT * FROM ks.cf")

    def permissions_caching_test(self):
        self.prepare(permissions_expiry=2000)

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key, val int)")

        cathy = self.get_cursor(user='cathy', password='12345')
        # another user to make sure the cache is at user level
        cathy2 = self.get_cursor(user='cathy', password='12345')
        cathys = [cathy, cathy2]

        self.assertUnauthorized("User cathy has no SELECT permission on <table ks.cf> or any of its parents",
                                cathy, "SELECT * FROM ks.cf")

        # grant SELECT to cathy
        cassandra.execute("GRANT SELECT ON ks.cf TO cathy")
        # should still fail after 1 second.
        time.sleep(1.0)
        for c in cathys:
            self.assertUnauthorized("User cathy has no SELECT permission on <table ks.cf> or any of its parents",
                                    c, "SELECT * FROM ks.cf")

        # wait until the cache definitely expires and retry - should succeed now
        time.sleep(1.5)
        for c in cathys:
            c.execute("SELECT * FROM ks.cf")
            self.assertEqual(0, c.rowcount)

    def list_permissions_test(self):
        self.prepare()

        cassandra = self.get_cursor(user='cassandra', password='cassandra')
        cassandra.execute("CREATE USER cathy WITH PASSWORD '12345'")
        cassandra.execute("CREATE USER bob WITH PASSWORD '12345'")
        cassandra.execute("CREATE KEYSPACE ks WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}")
        cassandra.execute("CREATE TABLE ks.cf (id int primary key, val int)")
        cassandra.execute("CREATE TABLE ks.cf2 (id int primary key, val int)")

        cassandra.execute("GRANT CREATE ON ALL KEYSPACES TO cathy")
        cassandra.execute("GRANT ALTER ON KEYSPACE ks TO bob")
        cassandra.execute("GRANT MODIFY ON ks.cf TO cathy")
        cassandra.execute("GRANT DROP ON ks.cf TO bob")
        cassandra.execute("GRANT MODIFY ON ks.cf2 TO bob")
        cassandra.execute("GRANT SELECT ON ks.cf2 TO cathy")

        self.assertPermissionsListed([('cathy', '<all keyspaces>', 'CREATE'),
                                      ('cathy', '<table ks.cf>', 'MODIFY'),
                                      ('cathy', '<table ks.cf2>', 'SELECT'),
                                      ('bob', '<keyspace ks>', 'ALTER'),
                                      ('bob', '<table ks.cf>', 'DROP'),
                                      ('bob', '<table ks.cf2>', 'MODIFY')],
                                     cassandra, "LIST ALL PERMISSIONS")

        self.assertPermissionsListed([('cathy', '<all keyspaces>', 'CREATE'),
                                      ('cathy', '<table ks.cf>', 'MODIFY'),
                                      ('cathy', '<table ks.cf2>', 'SELECT')],
                                     cassandra, "LIST ALL PERMISSIONS OF cathy")

        self.assertPermissionsListed([('cathy', '<table ks.cf>', 'MODIFY'),
                                      ('bob', '<table ks.cf>', 'DROP')],
                                     cassandra, "LIST ALL PERMISSIONS ON ks.cf NORECURSIVE")

        self.assertPermissionsListed([('cathy', '<table ks.cf2>', 'SELECT')],
                                      cassandra, "LIST SELECT ON ks.cf2")

        self.assertPermissionsListed([('cathy', '<all keyspaces>', 'CREATE'),
                                      ('cathy', '<table ks.cf>', 'MODIFY')],
                                     cassandra, "LIST ALL ON ks.cf OF cathy")

        bob = self.get_cursor(user='bob', password='12345')
        self.assertPermissionsListed([('bob', '<keyspace ks>', 'ALTER'),
                                      ('bob', '<table ks.cf>', 'DROP'),
                                      ('bob', '<table ks.cf2>', 'MODIFY')],
                                     bob, "LIST ALL PERMISSIONS OF bob")

        self.assertUnauthorized("You are not authorized to view everyone's permissions",
                                bob, "LIST ALL PERMISSIONS")

        self.assertUnauthorized("You are not authorized to view cathy's permissions",
                                bob, "LIST ALL PERMISSIONS OF cathy")

    def prepare(self, nodes=1, permissions_expiry=0):
        config = {'authenticator' : 'org.apache.cassandra.auth.PasswordAuthenticator',
                  'authorizer' : 'org.apache.cassandra.auth.CassandraAuthorizer',
                  'permissions_validity_in_ms' : permissions_expiry}
        self.cluster.set_configuration_options(values=config)
        self.cluster.populate(nodes).start()
        # default user setup is delayed by 10 seconds to reduce log spam
        if nodes == 1:
            self.cluster.nodelist()[0].watch_log_for('Created default superuser')
        else:
            # can' just watch for log - the line will appear in just one of the nodes' logs
            # only one test uses more than 1 node, though, so some sleep is fine.
            time.sleep(15)

    def get_cursor(self, node_idx=0, user=None, password=None):
        node = self.cluster.nodelist()[node_idx]
        conn = self.cql_connection(node, version="3.0.1", user=user, password=password)
        return conn.cursor()

    def assertPermissionsListed(self, expected, cursor, query):
        cursor.execute(query)
        perms = [(str(r[0]), str(r[1]), str(r[2])) for r in cursor.fetchall()]
        self.assertEqual(sorted(expected), sorted(perms))

    def assertUnauthorized(self, message, cursor, query):
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute(query)
        self.assertEqual("Bad Request: " + message, cm.exception.message)

########NEW FILE########
__FILENAME__ = batch_test
import time

from assertions import assert_invalid, assert_unavailable
from cql.cassandra.ttypes import Compression, ConsistencyLevel, TimedOutException
from dtest import Tester

cql_version="3.0.0"

class TestBatch(Tester):

    def counter_batch_accepts_counter_mutations_test(self):
        """ Test that counter batch accepts counter mutations """
        cursor = self.prepare()
        cursor.execute("""
            BEGIN COUNTER BATCH
            UPDATE clicks SET total = total + 1 WHERE userid = 1 and url = 'http://foo.com'
            UPDATE clicks SET total = total + 1 WHERE userid = 1 and url = 'http://bar.com'
            UPDATE clicks SET total = total + 1 WHERE userid = 2 and url = 'http://baz.com'
            APPLY BATCH
        """)
        cursor.execute("SELECT total FROM clicks")
        res = cursor.fetchall()
        assert res == [[1], [1], [1]], res

    def counter_batch_rejects_regular_mutations_test(self):
        """ Test that counter batch rejects non-counter mutations """
        cursor = self.prepare()
        assert_invalid(cursor, """
            BEGIN COUNTER BATCH
            UPDATE clicks SET total = total + 1 WHERE userid = 1 and url = 'http://foo.com'
            UPDATE clicks SET total = total + 1 WHERE userid = 1 and url = 'http://bar.com'
            UPDATE clicks SET total = total + 1 WHERE userid = 2 and url = 'http://baz.com'
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            APPLY BATCH
        """, matching="Only counter mutations are allowed in COUNTER batches")

    def logged_batch_accepts_regular_mutations_test(self):
        """ Test that logged batch accepts regular mutations """
        cursor = self.prepare()
        cursor.execute("""
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """)
        cursor.execute("SELECT * FROM users")
        res = sorted(cursor.fetchall())
        assert res == [[0, u'Jack', u'Sparrow'], [1, u'Will', u'Turner']], res

    def logged_batch_rejects_counter_mutations_test(self):
        """ Test that logged batch rejects counter mutations """
        cursor = self.prepare()
        assert_invalid(cursor, """
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            UPDATE clicks SET total = total + 1 WHERE userid = 1 and url = 'http://foo.com'
            APPLY BATCH
        """, matching="Counter mutations are only allowed in COUNTER batches")

    def unlogged_batch_accepts_regular_mutations_test(self):
        """ Test that unlogged batch accepts regular mutations """
        cursor = self.prepare()
        cursor.execute("""
            BEGIN UNLOGGED BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (2, 'Elizabeth', 'Swann')
            APPLY BATCH
        """)
        cursor.execute("SELECT * FROM users")
        res = sorted(cursor.fetchall())
        assert res == [[0, u'Jack', u'Sparrow'], [2, u'Elizabeth', u'Swann']], res

    def unlogged_batch_rejects_counter_mutations_test(self):
        """ Test that unlogged batch rejects counter mutations """
        cursor = self.prepare()
        assert_invalid(cursor, """
            BEGIN UNLOGGED BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (2, 'Elizabeth', 'Swann')
            UPDATE clicks SET total = total + 1 WHERE userid = 1 AND url = 'http://foo.com'
            APPLY BATCH
        """, matching="Counter mutations are only allowed in COUNTER batches")

    def logged_batch_throws_uae_test(self):
        """ Test that logged batch throws UAE if there aren't enough live nodes """
        cursor = self.prepare(nodes=3)
        [ node.stop(wait_other_notice=True) for node in self.cluster.nodelist()[1:] ]
        cursor.consistency_level = 'ONE'
        assert_unavailable(cursor.execute, """
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """)

    def logged_batch_doesnt_throw_uae_test(self):
        """ Test that logged batch DOES NOT throw UAE if there are at least 2 live nodes """
        cursor = self.prepare(nodes=3)
        self.cluster.nodelist()[-1].stop(wait_other_notice=True)
        cursor.execute("""
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """, consistency_level="ANY")
        assert True

    def aknowledged_by_batchlog_not_set_when_batchlog_write_fails_test(self):
        """ Test that acknowledged_by_batchlog is False if batchlog can't be written """
        cursor = self.prepare(nodes=3)
        # kill 2 of the 3 nodes (all the batchlog write candidates).
        [ node.stop(gently=False) for node in self.cluster.nodelist()[1:] ]
        self.assert_timedout(cursor, """
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """, ConsistencyLevel.ONE, acknowledged_by_batchlog=False)

    def aknowledged_by_batchlog_set_when_batchlog_write_succeeds_test(self):
        """ Test that acknowledged_by_batchlog is True if batchlog can be written """
        cursor = self.prepare(nodes=3)
        # kill one of the nodes so that batchlog will be written, but the write will fail.
        self.cluster.nodelist()[-1].stop(gently=False)
        self.assert_timedout(cursor, """
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """, ConsistencyLevel.THREE, acknowledged_by_batchlog=True)

    def batch_uses_proper_timestamp_test(self):
        """ Test that each statement will be executed with provided BATCH timestamp """
        cursor = self.prepare()
        cursor.execute("""
            BEGIN BATCH USING TIMESTAMP 1111111111111111
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """)
        cursor.execute("SELECT id, writetime(firstname), writetime(lastname) FROM users")
        res = sorted(cursor.fetchall())
        assert res == [[0, 1111111111111111, 1111111111111111], [1, 1111111111111111, 1111111111111111]], res

    def only_one_timestamp_is_valid_test(self):
        """ Test that TIMESTAMP must not be used in the statements within the batch. """
        cursor = self.prepare()
        assert_invalid(cursor, """
            BEGIN BATCH USING TIMESTAMP 1111111111111111
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow') USING TIMESTAMP 2
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
        """, matching="Timestamp must be set either on BATCH or individual statements")

    def each_statement_in_batch_uses_proper_timestamp_test(self):
        """ Test that each statement will be executed with its own timestamp """
        cursor = self.prepare()
        cursor.execute("""
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow') USING TIMESTAMP 1111111111111111
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner') USING TIMESTAMP 1111111111111112
            APPLY BATCH
        """)
        cursor.execute("SELECT id, writetime(firstname), writetime(lastname) FROM users")
        res = sorted(cursor.fetchall())
        assert res == [[0, 1111111111111111, 1111111111111111], [1, 1111111111111112, 1111111111111112]], res

    def assert_timedout(self, cursor, query, cl, acknowledged_by=None,
                        acknowledged_by_batchlog=None):
        client = cursor._connection.client
        try:
            client.execute_cql3_query(query, Compression.NONE, cl)
        except TimedOutException as e:
            if not acknowledged_by_batchlog is None:
                msg = "Expecting acknowledged_by_batchlog to be %s, got: %s" % (
                        acknowledged_by_batchlog, e.acknowledged_by_batchlog,)
                assert e.acknowledged_by_batchlog == acknowledged_by_batchlog, msg
        except Exception as e:
            assert False, "Expecting TimedOutException, got:" + str(e)
        else:
            assert False, "Expecting TimedOutException but no exception was raised"

    def prepare(self, nodes=1):
        self.cluster.populate(nodes).start()
        time.sleep(.5)
        node1 = self.cluster.nodelist()[0]
        cursor = self.cql_connection(node1, version=cql_version).cursor()
        self.create_ks(cursor, 'ks', nodes)
        cursor.execute("""
            CREATE TABLE clicks (
                userid int,
                url text,
                total counter,
                PRIMARY KEY (userid, url)
             );
         """)
        cursor.execute("""
            CREATE TABLE users (
                id int,
                firstname text,
                lastname text,
                PRIMARY KEY (id)
             );
         """)
        time.sleep(.5)
        return cursor

########NEW FILE########
__FILENAME__ = bootstrap_test
import random, time
from dtest import Tester, debug
from tools import *
from assertions import *
from ccmlib.cluster import Cluster


class TestBootstrap(Tester):

    def __init__(self, *args, **kwargs):
        # Ignore these log patterns:
        self.ignore_log_patterns = [
            # This one occurs when trying to send the migration to a
            # node that hasn't started yet, and when it does, it gets
            # replayed and everything is fine.
            r'Can\'t send migration request: node.*is down',
        ]
        Tester.__init__(self, *args, **kwargs)

    def simple_bootstrap_test(self):
        cluster = self.cluster
        tokens = cluster.balanced_tokens(2)

        keys = 10000

        # Create a single node cluster
        cluster.populate(1, tokens=[tokens[0]]).start()
        node1 = cluster.nodes["node1"]

        time.sleep(.5)
        cursor = self.cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', columns={ 'c1' : 'text', 'c2' : 'text' })

        for n in xrange(0, keys):
            insert_c1c2(cursor, n, "ONE")

        node1.flush()
        initial_size = node1.data_size()

        # Reads inserted data all during the boostrap process. We shouldn't
        # get any error
        reader = self.go(lambda _: query_c1c2(cursor, random.randint(0, keys-1), "ONE"))

        # Boostraping a new node
        node2 = new_node(cluster, token=tokens[1])
        node2.start()
        time.sleep(.5)

        reader.check()
        node1.cleanup()
        time.sleep(.5)
        reader.check()

        size1 = node1.data_size()
        size2 = node2.data_size()
        assert_almost_equal(size1, size2, error=0.3)
        assert_almost_equal(initial_size, 2 * size1)

    def read_from_bootstrapped_node_test(self):
        """Test bootstrapped node sees existing data, eg. CASSANDRA-6648"""
        cluster = self.cluster
        cluster.populate(3)
        version = cluster.version()
        cluster.start()
        
        node1 = cluster.nodes['node1']
        if version < "2.1":
            node1.stress(['-n', '10000'])
        else:
            node1.stress(['write', 'n=10000', '-rate', 'threads=8'])
        
        node4 = new_node(cluster)
        node4.start()

        cursor = self.patient_cql_connection(node4).cursor()
        cursor.execute('select * from "Keyspace1"."Standard1" limit 10')
        assert len(list(cursor)) == 10

########NEW FILE########
__FILENAME__ = Cassandra
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException
from ttypes import *
from thrift.Thrift import TProcessor
from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol, TProtocol
try:
  from thrift.protocol import fastbinary
except:
  fastbinary = None


class Iface:
  def login(self, auth_request):
    """
    Parameters:
     - auth_request
    """
    pass

  def set_keyspace(self, keyspace):
    """
    Parameters:
     - keyspace
    """
    pass

  def get(self, key, column_path, consistency_level):
    """
    Get the Column or SuperColumn at the given column_path. If no value is present, NotFoundException is thrown. (This is
    the only method that can throw an exception under non-failure conditions.)

    Parameters:
     - key
     - column_path
     - consistency_level
    """
    pass

  def get_slice(self, key, column_parent, predicate, consistency_level):
    """
    Get the group of columns contained by column_parent (either a ColumnFamily name or a ColumnFamily/SuperColumn name
    pair) specified by the given SlicePredicate. If no matching values are found, an empty list is returned.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def get_count(self, key, column_parent, predicate, consistency_level):
    """
    returns the number of columns matching <code>predicate</code> for a particular <code>key</code>,
    <code>ColumnFamily</code> and optionally <code>SuperColumn</code>.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def multiget_slice(self, keys, column_parent, predicate, consistency_level):
    """
    Performs a get_slice for column_parent and predicate for the given keys in parallel.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def multiget_count(self, keys, column_parent, predicate, consistency_level):
    """
    Perform a get_count in parallel on the given list<binary> keys. The return value maps keys to the count found.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def get_range_slices(self, column_parent, predicate, range, consistency_level):
    """
    returns a subset of columns for a contiguous range of keys.

    Parameters:
     - column_parent
     - predicate
     - range
     - consistency_level
    """
    pass

  def get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    """
    Returns the subset of columns specified in SlicePredicate for the rows matching the IndexClause

    Parameters:
     - column_parent
     - index_clause
     - column_predicate
     - consistency_level
    """
    pass

  def insert(self, key, column_parent, column, consistency_level):
    """
    Insert a Column at the given column_parent.column_family and optional column_parent.super_column.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    pass

  def remove(self, key, column_path, timestamp, consistency_level):
    """
    Remove data from the row specified by key at the granularity specified by column_path, and the given timestamp. Note
    that all the values in column_path besides column_path.column_family are truly optional: you can remove the entire
    row by just specifying the ColumnFamily, or you can remove a SuperColumn or a single Column by specifying those levels too.

    Parameters:
     - key
     - column_path
     - timestamp
     - consistency_level
    """
    pass

  def batch_mutate(self, mutation_map, consistency_level):
    """
      Mutate many columns or super columns for many row keys. See also: Mutation.

      mutation_map maps key to column family to a list of Mutation objects to take place at that scope.
    *

    Parameters:
     - mutation_map
     - consistency_level
    """
    pass

  def truncate(self, cfname):
    """
    Truncate will mark and entire column family as deleted.
    From the user's perspective a successful call to truncate will result complete data deletion from cfname.
    Internally, however, disk space will not be immediatily released, as with all deletes in cassandra, this one
    only marks the data as deleted.
    The operation succeeds only if all hosts in the cluster at available and will throw an UnavailableException if
    some hosts are down.

    Parameters:
     - cfname
    """
    pass

  def describe_schema_versions(self, ):
    """
    for each schema version present in the cluster, returns a list of nodes at that version.
    hosts that do not respond will be under the key DatabaseDescriptor.INITIAL_VERSION.
    the cluster is all on the same version if the size of the map is 1.
    """
    pass

  def describe_keyspaces(self, ):
    """
    list the defined keyspaces in this cluster
    """
    pass

  def describe_cluster_name(self, ):
    """
    get the cluster name
    """
    pass

  def describe_version(self, ):
    """
    get the thrift api version
    """
    pass

  def describe_ring(self, keyspace):
    """
    get the token ring: a map of ranges to host addresses,
    represented as a set of TokenRange instead of a map from range
    to list of endpoints, because you can't use Thrift structs as
    map keys:
    https://issues.apache.org/jira/browse/THRIFT-162

    for the same reason, we can't return a set here, even though
    order is neither important nor predictable.

    Parameters:
     - keyspace
    """
    pass

  def describe_partitioner(self, ):
    """
    returns the partitioner used by this cluster
    """
    pass

  def describe_snitch(self, ):
    """
    returns the snitch used by this cluster
    """
    pass

  def describe_keyspace(self, keyspace):
    """
    describe specified keyspace

    Parameters:
     - keyspace
    """
    pass

  def describe_splits(self, cfName, start_token, end_token, keys_per_split):
    """
    experimental API for hadoop/parallel query support.
    may change violently and without warning.

    returns list of token strings such that first subrange is (list[0], list[1]],
    next is (list[1], list[2]], etc.

    Parameters:
     - cfName
     - start_token
     - end_token
     - keys_per_split
    """
    pass

  def system_add_column_family(self, cf_def):
    """
    adds a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    pass

  def system_drop_column_family(self, column_family):
    """
    drops a column family. returns the new schema id.

    Parameters:
     - column_family
    """
    pass

  def system_add_keyspace(self, ks_def):
    """
    adds a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - ks_def
    """
    pass

  def system_drop_keyspace(self, keyspace):
    """
    drops a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - keyspace
    """
    pass

  def system_update_keyspace(self, ks_def):
    """
    updates properties of a keyspace. returns the new schema id.

    Parameters:
     - ks_def
    """
    pass

  def system_update_column_family(self, cf_def):
    """
    updates properties of a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    pass


class Client(Iface):
  def __init__(self, iprot, oprot=None):
    self._iprot = self._oprot = iprot
    if oprot is not None:
      self._oprot = oprot
    self._seqid = 0

  def login(self, auth_request):
    """
    Parameters:
     - auth_request
    """
    self.send_login(auth_request)
    self.recv_login()

  def send_login(self, auth_request):
    self._oprot.writeMessageBegin('login', TMessageType.CALL, self._seqid)
    args = login_args()
    args.auth_request = auth_request
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_login(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = login_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.authnx is not None:
      raise result.authnx
    if result.authzx is not None:
      raise result.authzx
    return

  def set_keyspace(self, keyspace):
    """
    Parameters:
     - keyspace
    """
    self.send_set_keyspace(keyspace)
    self.recv_set_keyspace()

  def send_set_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('set_keyspace', TMessageType.CALL, self._seqid)
    args = set_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_set_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = set_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    return

  def get(self, key, column_path, consistency_level):
    """
    Get the Column or SuperColumn at the given column_path. If no value is present, NotFoundException is thrown. (This is
    the only method that can throw an exception under non-failure conditions.)

    Parameters:
     - key
     - column_path
     - consistency_level
    """
    self.send_get(key, column_path, consistency_level)
    return self.recv_get()

  def send_get(self, key, column_path, consistency_level):
    self._oprot.writeMessageBegin('get', TMessageType.CALL, self._seqid)
    args = get_args()
    args.key = key
    args.column_path = column_path
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.nfe is not None:
      raise result.nfe
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get failed: unknown result");

  def get_slice(self, key, column_parent, predicate, consistency_level):
    """
    Get the group of columns contained by column_parent (either a ColumnFamily name or a ColumnFamily/SuperColumn name
    pair) specified by the given SlicePredicate. If no matching values are found, an empty list is returned.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_get_slice(key, column_parent, predicate, consistency_level)
    return self.recv_get_slice()

  def send_get_slice(self, key, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('get_slice', TMessageType.CALL, self._seqid)
    args = get_slice_args()
    args.key = key
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_slice(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_slice_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_slice failed: unknown result");

  def get_count(self, key, column_parent, predicate, consistency_level):
    """
    returns the number of columns matching <code>predicate</code> for a particular <code>key</code>,
    <code>ColumnFamily</code> and optionally <code>SuperColumn</code>.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_get_count(key, column_parent, predicate, consistency_level)
    return self.recv_get_count()

  def send_get_count(self, key, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('get_count', TMessageType.CALL, self._seqid)
    args = get_count_args()
    args.key = key
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_count(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_count_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_count failed: unknown result");

  def multiget_slice(self, keys, column_parent, predicate, consistency_level):
    """
    Performs a get_slice for column_parent and predicate for the given keys in parallel.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_multiget_slice(keys, column_parent, predicate, consistency_level)
    return self.recv_multiget_slice()

  def send_multiget_slice(self, keys, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('multiget_slice', TMessageType.CALL, self._seqid)
    args = multiget_slice_args()
    args.keys = keys
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_multiget_slice(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = multiget_slice_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "multiget_slice failed: unknown result");

  def multiget_count(self, keys, column_parent, predicate, consistency_level):
    """
    Perform a get_count in parallel on the given list<binary> keys. The return value maps keys to the count found.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_multiget_count(keys, column_parent, predicate, consistency_level)
    return self.recv_multiget_count()

  def send_multiget_count(self, keys, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('multiget_count', TMessageType.CALL, self._seqid)
    args = multiget_count_args()
    args.keys = keys
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_multiget_count(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = multiget_count_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "multiget_count failed: unknown result");

  def get_range_slices(self, column_parent, predicate, range, consistency_level):
    """
    returns a subset of columns for a contiguous range of keys.

    Parameters:
     - column_parent
     - predicate
     - range
     - consistency_level
    """
    self.send_get_range_slices(column_parent, predicate, range, consistency_level)
    return self.recv_get_range_slices()

  def send_get_range_slices(self, column_parent, predicate, range, consistency_level):
    self._oprot.writeMessageBegin('get_range_slices', TMessageType.CALL, self._seqid)
    args = get_range_slices_args()
    args.column_parent = column_parent
    args.predicate = predicate
    args.range = range
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_range_slices(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_range_slices_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_range_slices failed: unknown result");

  def get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    """
    Returns the subset of columns specified in SlicePredicate for the rows matching the IndexClause

    Parameters:
     - column_parent
     - index_clause
     - column_predicate
     - consistency_level
    """
    self.send_get_indexed_slices(column_parent, index_clause, column_predicate, consistency_level)
    return self.recv_get_indexed_slices()

  def send_get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    self._oprot.writeMessageBegin('get_indexed_slices', TMessageType.CALL, self._seqid)
    args = get_indexed_slices_args()
    args.column_parent = column_parent
    args.index_clause = index_clause
    args.column_predicate = column_predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_indexed_slices(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_indexed_slices_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_indexed_slices failed: unknown result");

  def insert(self, key, column_parent, column, consistency_level):
    """
    Insert a Column at the given column_parent.column_family and optional column_parent.super_column.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    self.send_insert(key, column_parent, column, consistency_level)
    self.recv_insert()

  def send_insert(self, key, column_parent, column, consistency_level):
    self._oprot.writeMessageBegin('insert', TMessageType.CALL, self._seqid)
    args = insert_args()
    args.key = key
    args.column_parent = column_parent
    args.column = column
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_insert(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = insert_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def remove(self, key, column_path, timestamp, consistency_level):
    """
    Remove data from the row specified by key at the granularity specified by column_path, and the given timestamp. Note
    that all the values in column_path besides column_path.column_family are truly optional: you can remove the entire
    row by just specifying the ColumnFamily, or you can remove a SuperColumn or a single Column by specifying those levels too.

    Parameters:
     - key
     - column_path
     - timestamp
     - consistency_level
    """
    self.send_remove(key, column_path, timestamp, consistency_level)
    self.recv_remove()

  def send_remove(self, key, column_path, timestamp, consistency_level):
    self._oprot.writeMessageBegin('remove', TMessageType.CALL, self._seqid)
    args = remove_args()
    args.key = key
    args.column_path = column_path
    args.timestamp = timestamp
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_remove(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = remove_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def batch_mutate(self, mutation_map, consistency_level):
    """
      Mutate many columns or super columns for many row keys. See also: Mutation.

      mutation_map maps key to column family to a list of Mutation objects to take place at that scope.
    *

    Parameters:
     - mutation_map
     - consistency_level
    """
    self.send_batch_mutate(mutation_map, consistency_level)
    self.recv_batch_mutate()

  def send_batch_mutate(self, mutation_map, consistency_level):
    self._oprot.writeMessageBegin('batch_mutate', TMessageType.CALL, self._seqid)
    args = batch_mutate_args()
    args.mutation_map = mutation_map
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_batch_mutate(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = batch_mutate_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def truncate(self, cfname):
    """
    Truncate will mark and entire column family as deleted.
    From the user's perspective a successful call to truncate will result complete data deletion from cfname.
    Internally, however, disk space will not be immediatily released, as with all deletes in cassandra, this one
    only marks the data as deleted.
    The operation succeeds only if all hosts in the cluster at available and will throw an UnavailableException if
    some hosts are down.

    Parameters:
     - cfname
    """
    self.send_truncate(cfname)
    self.recv_truncate()

  def send_truncate(self, cfname):
    self._oprot.writeMessageBegin('truncate', TMessageType.CALL, self._seqid)
    args = truncate_args()
    args.cfname = cfname
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_truncate(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = truncate_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    return

  def describe_schema_versions(self, ):
    """
    for each schema version present in the cluster, returns a list of nodes at that version.
    hosts that do not respond will be under the key DatabaseDescriptor.INITIAL_VERSION.
    the cluster is all on the same version if the size of the map is 1.
    """
    self.send_describe_schema_versions()
    return self.recv_describe_schema_versions()

  def send_describe_schema_versions(self, ):
    self._oprot.writeMessageBegin('describe_schema_versions', TMessageType.CALL, self._seqid)
    args = describe_schema_versions_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_schema_versions(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_schema_versions_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_schema_versions failed: unknown result");

  def describe_keyspaces(self, ):
    """
    list the defined keyspaces in this cluster
    """
    self.send_describe_keyspaces()
    return self.recv_describe_keyspaces()

  def send_describe_keyspaces(self, ):
    self._oprot.writeMessageBegin('describe_keyspaces', TMessageType.CALL, self._seqid)
    args = describe_keyspaces_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_keyspaces(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_keyspaces_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_keyspaces failed: unknown result");

  def describe_cluster_name(self, ):
    """
    get the cluster name
    """
    self.send_describe_cluster_name()
    return self.recv_describe_cluster_name()

  def send_describe_cluster_name(self, ):
    self._oprot.writeMessageBegin('describe_cluster_name', TMessageType.CALL, self._seqid)
    args = describe_cluster_name_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_cluster_name(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_cluster_name_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_cluster_name failed: unknown result");

  def describe_version(self, ):
    """
    get the thrift api version
    """
    self.send_describe_version()
    return self.recv_describe_version()

  def send_describe_version(self, ):
    self._oprot.writeMessageBegin('describe_version', TMessageType.CALL, self._seqid)
    args = describe_version_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_version(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_version_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_version failed: unknown result");

  def describe_ring(self, keyspace):
    """
    get the token ring: a map of ranges to host addresses,
    represented as a set of TokenRange instead of a map from range
    to list of endpoints, because you can't use Thrift structs as
    map keys:
    https://issues.apache.org/jira/browse/THRIFT-162

    for the same reason, we can't return a set here, even though
    order is neither important nor predictable.

    Parameters:
     - keyspace
    """
    self.send_describe_ring(keyspace)
    return self.recv_describe_ring()

  def send_describe_ring(self, keyspace):
    self._oprot.writeMessageBegin('describe_ring', TMessageType.CALL, self._seqid)
    args = describe_ring_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_ring(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_ring_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_ring failed: unknown result");

  def describe_partitioner(self, ):
    """
    returns the partitioner used by this cluster
    """
    self.send_describe_partitioner()
    return self.recv_describe_partitioner()

  def send_describe_partitioner(self, ):
    self._oprot.writeMessageBegin('describe_partitioner', TMessageType.CALL, self._seqid)
    args = describe_partitioner_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_partitioner(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_partitioner_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_partitioner failed: unknown result");

  def describe_snitch(self, ):
    """
    returns the snitch used by this cluster
    """
    self.send_describe_snitch()
    return self.recv_describe_snitch()

  def send_describe_snitch(self, ):
    self._oprot.writeMessageBegin('describe_snitch', TMessageType.CALL, self._seqid)
    args = describe_snitch_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_snitch(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_snitch_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_snitch failed: unknown result");

  def describe_keyspace(self, keyspace):
    """
    describe specified keyspace

    Parameters:
     - keyspace
    """
    self.send_describe_keyspace(keyspace)
    return self.recv_describe_keyspace()

  def send_describe_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('describe_keyspace', TMessageType.CALL, self._seqid)
    args = describe_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.nfe is not None:
      raise result.nfe
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_keyspace failed: unknown result");

  def describe_splits(self, cfName, start_token, end_token, keys_per_split):
    """
    experimental API for hadoop/parallel query support.
    may change violently and without warning.

    returns list of token strings such that first subrange is (list[0], list[1]],
    next is (list[1], list[2]], etc.

    Parameters:
     - cfName
     - start_token
     - end_token
     - keys_per_split
    """
    self.send_describe_splits(cfName, start_token, end_token, keys_per_split)
    return self.recv_describe_splits()

  def send_describe_splits(self, cfName, start_token, end_token, keys_per_split):
    self._oprot.writeMessageBegin('describe_splits', TMessageType.CALL, self._seqid)
    args = describe_splits_args()
    args.cfName = cfName
    args.start_token = start_token
    args.end_token = end_token
    args.keys_per_split = keys_per_split
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_splits(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_splits_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_splits failed: unknown result");

  def system_add_column_family(self, cf_def):
    """
    adds a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    self.send_system_add_column_family(cf_def)
    return self.recv_system_add_column_family()

  def send_system_add_column_family(self, cf_def):
    self._oprot.writeMessageBegin('system_add_column_family', TMessageType.CALL, self._seqid)
    args = system_add_column_family_args()
    args.cf_def = cf_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_add_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_add_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_add_column_family failed: unknown result");

  def system_drop_column_family(self, column_family):
    """
    drops a column family. returns the new schema id.

    Parameters:
     - column_family
    """
    self.send_system_drop_column_family(column_family)
    return self.recv_system_drop_column_family()

  def send_system_drop_column_family(self, column_family):
    self._oprot.writeMessageBegin('system_drop_column_family', TMessageType.CALL, self._seqid)
    args = system_drop_column_family_args()
    args.column_family = column_family
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_drop_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_drop_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_drop_column_family failed: unknown result");

  def system_add_keyspace(self, ks_def):
    """
    adds a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - ks_def
    """
    self.send_system_add_keyspace(ks_def)
    return self.recv_system_add_keyspace()

  def send_system_add_keyspace(self, ks_def):
    self._oprot.writeMessageBegin('system_add_keyspace', TMessageType.CALL, self._seqid)
    args = system_add_keyspace_args()
    args.ks_def = ks_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_add_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_add_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_add_keyspace failed: unknown result");

  def system_drop_keyspace(self, keyspace):
    """
    drops a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - keyspace
    """
    self.send_system_drop_keyspace(keyspace)
    return self.recv_system_drop_keyspace()

  def send_system_drop_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('system_drop_keyspace', TMessageType.CALL, self._seqid)
    args = system_drop_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_drop_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_drop_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_drop_keyspace failed: unknown result");

  def system_update_keyspace(self, ks_def):
    """
    updates properties of a keyspace. returns the new schema id.

    Parameters:
     - ks_def
    """
    self.send_system_update_keyspace(ks_def)
    return self.recv_system_update_keyspace()

  def send_system_update_keyspace(self, ks_def):
    self._oprot.writeMessageBegin('system_update_keyspace', TMessageType.CALL, self._seqid)
    args = system_update_keyspace_args()
    args.ks_def = ks_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_update_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_update_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_update_keyspace failed: unknown result");

  def system_update_column_family(self, cf_def):
    """
    updates properties of a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    self.send_system_update_column_family(cf_def)
    return self.recv_system_update_column_family()

  def send_system_update_column_family(self, cf_def):
    self._oprot.writeMessageBegin('system_update_column_family', TMessageType.CALL, self._seqid)
    args = system_update_column_family_args()
    args.cf_def = cf_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_update_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_update_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_update_column_family failed: unknown result");


class Processor(Iface, TProcessor):
  def __init__(self, handler):
    self._handler = handler
    self._processMap = {}
    self._processMap["login"] = Processor.process_login
    self._processMap["set_keyspace"] = Processor.process_set_keyspace
    self._processMap["get"] = Processor.process_get
    self._processMap["get_slice"] = Processor.process_get_slice
    self._processMap["get_count"] = Processor.process_get_count
    self._processMap["multiget_slice"] = Processor.process_multiget_slice
    self._processMap["multiget_count"] = Processor.process_multiget_count
    self._processMap["get_range_slices"] = Processor.process_get_range_slices
    self._processMap["get_indexed_slices"] = Processor.process_get_indexed_slices
    self._processMap["insert"] = Processor.process_insert
    self._processMap["remove"] = Processor.process_remove
    self._processMap["batch_mutate"] = Processor.process_batch_mutate
    self._processMap["truncate"] = Processor.process_truncate
    self._processMap["describe_schema_versions"] = Processor.process_describe_schema_versions
    self._processMap["describe_keyspaces"] = Processor.process_describe_keyspaces
    self._processMap["describe_cluster_name"] = Processor.process_describe_cluster_name
    self._processMap["describe_version"] = Processor.process_describe_version
    self._processMap["describe_ring"] = Processor.process_describe_ring
    self._processMap["describe_partitioner"] = Processor.process_describe_partitioner
    self._processMap["describe_snitch"] = Processor.process_describe_snitch
    self._processMap["describe_keyspace"] = Processor.process_describe_keyspace
    self._processMap["describe_splits"] = Processor.process_describe_splits
    self._processMap["system_add_column_family"] = Processor.process_system_add_column_family
    self._processMap["system_drop_column_family"] = Processor.process_system_drop_column_family
    self._processMap["system_add_keyspace"] = Processor.process_system_add_keyspace
    self._processMap["system_drop_keyspace"] = Processor.process_system_drop_keyspace
    self._processMap["system_update_keyspace"] = Processor.process_system_update_keyspace
    self._processMap["system_update_column_family"] = Processor.process_system_update_column_family

  def process(self, iprot, oprot):
    (name, type, seqid) = iprot.readMessageBegin()
    if name not in self._processMap:
      iprot.skip(TType.STRUCT)
      iprot.readMessageEnd()
      x = TApplicationException(TApplicationException.UNKNOWN_METHOD, 'Unknown function %s' % (name))
      oprot.writeMessageBegin(name, TMessageType.EXCEPTION, seqid)
      x.write(oprot)
      oprot.writeMessageEnd()
      oprot.trans.flush()
      return
    else:
      self._processMap[name](self, seqid, iprot, oprot)
    return True

  def process_login(self, seqid, iprot, oprot):
    args = login_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = login_result()
    try:
      self._handler.login(args.auth_request)
    except AuthenticationException, authnx:
      result.authnx = authnx
    except AuthorizationException, authzx:
      result.authzx = authzx
    oprot.writeMessageBegin("login", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_set_keyspace(self, seqid, iprot, oprot):
    args = set_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = set_keyspace_result()
    try:
      self._handler.set_keyspace(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("set_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get(self, seqid, iprot, oprot):
    args = get_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_result()
    try:
      result.success = self._handler.get(args.key, args.column_path, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except NotFoundException, nfe:
      result.nfe = nfe
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_slice(self, seqid, iprot, oprot):
    args = get_slice_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_slice_result()
    try:
      result.success = self._handler.get_slice(args.key, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_slice", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_count(self, seqid, iprot, oprot):
    args = get_count_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_count_result()
    try:
      result.success = self._handler.get_count(args.key, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_count", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_multiget_slice(self, seqid, iprot, oprot):
    args = multiget_slice_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = multiget_slice_result()
    try:
      result.success = self._handler.multiget_slice(args.keys, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("multiget_slice", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_multiget_count(self, seqid, iprot, oprot):
    args = multiget_count_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = multiget_count_result()
    try:
      result.success = self._handler.multiget_count(args.keys, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("multiget_count", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_range_slices(self, seqid, iprot, oprot):
    args = get_range_slices_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_range_slices_result()
    try:
      result.success = self._handler.get_range_slices(args.column_parent, args.predicate, args.range, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_range_slices", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_indexed_slices(self, seqid, iprot, oprot):
    args = get_indexed_slices_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_indexed_slices_result()
    try:
      result.success = self._handler.get_indexed_slices(args.column_parent, args.index_clause, args.column_predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_indexed_slices", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_insert(self, seqid, iprot, oprot):
    args = insert_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = insert_result()
    try:
      self._handler.insert(args.key, args.column_parent, args.column, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("insert", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_remove(self, seqid, iprot, oprot):
    args = remove_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = remove_result()
    try:
      self._handler.remove(args.key, args.column_path, args.timestamp, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("remove", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_batch_mutate(self, seqid, iprot, oprot):
    args = batch_mutate_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = batch_mutate_result()
    try:
      self._handler.batch_mutate(args.mutation_map, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("batch_mutate", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_truncate(self, seqid, iprot, oprot):
    args = truncate_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = truncate_result()
    try:
      self._handler.truncate(args.cfname)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    oprot.writeMessageBegin("truncate", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_schema_versions(self, seqid, iprot, oprot):
    args = describe_schema_versions_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_schema_versions_result()
    try:
      result.success = self._handler.describe_schema_versions()
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_schema_versions", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_keyspaces(self, seqid, iprot, oprot):
    args = describe_keyspaces_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_keyspaces_result()
    try:
      result.success = self._handler.describe_keyspaces()
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_keyspaces", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_cluster_name(self, seqid, iprot, oprot):
    args = describe_cluster_name_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_cluster_name_result()
    result.success = self._handler.describe_cluster_name()
    oprot.writeMessageBegin("describe_cluster_name", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_version(self, seqid, iprot, oprot):
    args = describe_version_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_version_result()
    result.success = self._handler.describe_version()
    oprot.writeMessageBegin("describe_version", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_ring(self, seqid, iprot, oprot):
    args = describe_ring_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_ring_result()
    try:
      result.success = self._handler.describe_ring(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_ring", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_partitioner(self, seqid, iprot, oprot):
    args = describe_partitioner_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_partitioner_result()
    result.success = self._handler.describe_partitioner()
    oprot.writeMessageBegin("describe_partitioner", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_snitch(self, seqid, iprot, oprot):
    args = describe_snitch_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_snitch_result()
    result.success = self._handler.describe_snitch()
    oprot.writeMessageBegin("describe_snitch", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_keyspace(self, seqid, iprot, oprot):
    args = describe_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_keyspace_result()
    try:
      result.success = self._handler.describe_keyspace(args.keyspace)
    except NotFoundException, nfe:
      result.nfe = nfe
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_splits(self, seqid, iprot, oprot):
    args = describe_splits_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_splits_result()
    result.success = self._handler.describe_splits(args.cfName, args.start_token, args.end_token, args.keys_per_split)
    oprot.writeMessageBegin("describe_splits", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_add_column_family(self, seqid, iprot, oprot):
    args = system_add_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_add_column_family_result()
    try:
      result.success = self._handler.system_add_column_family(args.cf_def)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_add_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_drop_column_family(self, seqid, iprot, oprot):
    args = system_drop_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_drop_column_family_result()
    try:
      result.success = self._handler.system_drop_column_family(args.column_family)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_drop_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_add_keyspace(self, seqid, iprot, oprot):
    args = system_add_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_add_keyspace_result()
    try:
      result.success = self._handler.system_add_keyspace(args.ks_def)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_add_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_drop_keyspace(self, seqid, iprot, oprot):
    args = system_drop_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_drop_keyspace_result()
    try:
      result.success = self._handler.system_drop_keyspace(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_drop_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_update_keyspace(self, seqid, iprot, oprot):
    args = system_update_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_update_keyspace_result()
    try:
      result.success = self._handler.system_update_keyspace(args.ks_def)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_update_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_update_column_family(self, seqid, iprot, oprot):
    args = system_update_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_update_column_family_result()
    try:
      result.success = self._handler.system_update_column_family(args.cf_def)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("system_update_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()


# HELPER FUNCTIONS AND STRUCTURES

class login_args:
  """
  Attributes:
   - auth_request
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'auth_request', (AuthenticationRequest, AuthenticationRequest.thrift_spec), None, ), # 1
  )

  def __init__(self, auth_request=None,):
    self.auth_request = auth_request

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.auth_request = AuthenticationRequest()
          self.auth_request.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('login_args')
    if self.auth_request is not None:
      oprot.writeFieldBegin('auth_request', TType.STRUCT, 1)
      self.auth_request.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.auth_request is None:
      raise TProtocol.TProtocolException(message='Required field auth_request is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class login_result:
  """
  Attributes:
   - authnx
   - authzx
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'authnx', (AuthenticationException, AuthenticationException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'authzx', (AuthorizationException, AuthorizationException.thrift_spec), None, ), # 2
  )

  def __init__(self, authnx=None, authzx=None,):
    self.authnx = authnx
    self.authzx = authzx

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.authnx = AuthenticationException()
          self.authnx.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.authzx = AuthorizationException()
          self.authzx.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('login_result')
    if self.authnx is not None:
      oprot.writeFieldBegin('authnx', TType.STRUCT, 1)
      self.authnx.write(oprot)
      oprot.writeFieldEnd()
    if self.authzx is not None:
      oprot.writeFieldBegin('authzx', TType.STRUCT, 2)
      self.authzx.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_keyspace_result:
  """
  Attributes:
   - ire
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, ire=None,):
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_keyspace_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_args:
  """
  Attributes:
   - key
   - column_path
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_path', (ColumnPath, ColumnPath.thrift_spec), None, ), # 2
    (3, TType.I32, 'consistency_level', None,     1, ), # 3
  )

  def __init__(self, key=None, column_path=None, consistency_level=thrift_spec[3][4],):
    self.key = key
    self.column_path = column_path
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_path = ColumnPath()
          self.column_path.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_path is not None:
      oprot.writeFieldBegin('column_path', TType.STRUCT, 2)
      self.column_path.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 3)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_path is None:
      raise TProtocol.TProtocolException(message='Required field column_path is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_result:
  """
  Attributes:
   - success
   - ire
   - nfe
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'nfe', (NotFoundException, NotFoundException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 3
    (4, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 4
  )

  def __init__(self, success=None, ire=None, nfe=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.nfe = nfe
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = ColumnOrSuperColumn()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.nfe = NotFoundException()
          self.nfe.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.nfe is not None:
      oprot.writeFieldBegin('nfe', TType.STRUCT, 2)
      self.nfe.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 3)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 4)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_slice_args:
  """
  Attributes:
   - key
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_slice_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_slice_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype70, _size67) = iprot.readListBegin()
          for _i71 in xrange(_size67):
            _elem72 = ColumnOrSuperColumn()
            _elem72.read(iprot)
            self.success.append(_elem72)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_slice_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter73 in self.success:
        iter73.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_count_args:
  """
  Attributes:
   - key
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_count_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_count_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.I32, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.I32:
          self.success = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_count_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.I32, 0)
      oprot.writeI32(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_slice_args:
  """
  Attributes:
   - keys
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'keys', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, keys=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.keys = keys
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.keys = []
          (_etype77, _size74) = iprot.readListBegin()
          for _i78 in xrange(_size74):
            _elem79 = iprot.readString();
            self.keys.append(_elem79)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_slice_args')
    if self.keys is not None:
      oprot.writeFieldBegin('keys', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.keys))
      for iter80 in self.keys:
        oprot.writeString(iter80)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keys is None:
      raise TProtocol.TProtocolException(message='Required field keys is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_slice_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.LIST,(TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec))), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype82, _vtype83, _size81 ) = iprot.readMapBegin() 
          for _i85 in xrange(_size81):
            _key86 = iprot.readString();
            _val87 = []
            (_etype91, _size88) = iprot.readListBegin()
            for _i92 in xrange(_size88):
              _elem93 = ColumnOrSuperColumn()
              _elem93.read(iprot)
              _val87.append(_elem93)
            iprot.readListEnd()
            self.success[_key86] = _val87
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_slice_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.LIST, len(self.success))
      for kiter94,viter95 in self.success.items():
        oprot.writeString(kiter94)
        oprot.writeListBegin(TType.STRUCT, len(viter95))
        for iter96 in viter95:
          iter96.write(oprot)
        oprot.writeListEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_count_args:
  """
  Attributes:
   - keys
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'keys', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, keys=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.keys = keys
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.keys = []
          (_etype100, _size97) = iprot.readListBegin()
          for _i101 in xrange(_size97):
            _elem102 = iprot.readString();
            self.keys.append(_elem102)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_count_args')
    if self.keys is not None:
      oprot.writeFieldBegin('keys', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.keys))
      for iter103 in self.keys:
        oprot.writeString(iter103)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keys is None:
      raise TProtocol.TProtocolException(message='Required field keys is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_count_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.I32,None), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype105, _vtype106, _size104 ) = iprot.readMapBegin() 
          for _i108 in xrange(_size104):
            _key109 = iprot.readString();
            _val110 = iprot.readI32();
            self.success[_key109] = _val110
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_count_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.I32, len(self.success))
      for kiter111,viter112 in self.success.items():
        oprot.writeString(kiter111)
        oprot.writeI32(viter112)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_range_slices_args:
  """
  Attributes:
   - column_parent
   - predicate
   - range
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'range', (KeyRange, KeyRange.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, column_parent=None, predicate=None, range=None, consistency_level=thrift_spec[4][4],):
    self.column_parent = column_parent
    self.predicate = predicate
    self.range = range
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.range = KeyRange()
          self.range.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_range_slices_args')
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 1)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 2)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.range is not None:
      oprot.writeFieldBegin('range', TType.STRUCT, 3)
      self.range.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.range is None:
      raise TProtocol.TProtocolException(message='Required field range is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_range_slices_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KeySlice, KeySlice.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype116, _size113) = iprot.readListBegin()
          for _i117 in xrange(_size113):
            _elem118 = KeySlice()
            _elem118.read(iprot)
            self.success.append(_elem118)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_range_slices_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter119 in self.success:
        iter119.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_indexed_slices_args:
  """
  Attributes:
   - column_parent
   - index_clause
   - column_predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'index_clause', (IndexClause, IndexClause.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'column_predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, column_parent=None, index_clause=None, column_predicate=None, consistency_level=thrift_spec[4][4],):
    self.column_parent = column_parent
    self.index_clause = index_clause
    self.column_predicate = column_predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.index_clause = IndexClause()
          self.index_clause.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.column_predicate = SlicePredicate()
          self.column_predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_indexed_slices_args')
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 1)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.index_clause is not None:
      oprot.writeFieldBegin('index_clause', TType.STRUCT, 2)
      self.index_clause.write(oprot)
      oprot.writeFieldEnd()
    if self.column_predicate is not None:
      oprot.writeFieldBegin('column_predicate', TType.STRUCT, 3)
      self.column_predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.index_clause is None:
      raise TProtocol.TProtocolException(message='Required field index_clause is unset!')
    if self.column_predicate is None:
      raise TProtocol.TProtocolException(message='Required field column_predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_indexed_slices_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KeySlice, KeySlice.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype123, _size120) = iprot.readListBegin()
          for _i124 in xrange(_size120):
            _elem125 = KeySlice()
            _elem125.read(iprot)
            self.success.append(_elem125)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_indexed_slices_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter126 in self.success:
        iter126.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class insert_args:
  """
  Attributes:
   - key
   - column_parent
   - column
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'column', (Column, Column.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, column=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.column = column
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.column = Column()
          self.column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('insert_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRUCT, 3)
      self.column.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.column is None:
      raise TProtocol.TProtocolException(message='Required field column is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class insert_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('insert_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_args:
  """
  Attributes:
   - key
   - column_path
   - timestamp
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_path', (ColumnPath, ColumnPath.thrift_spec), None, ), # 2
    (3, TType.I64, 'timestamp', None, None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_path=None, timestamp=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_path = column_path
    self.timestamp = timestamp
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_path = ColumnPath()
          self.column_path.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_path is not None:
      oprot.writeFieldBegin('column_path', TType.STRUCT, 2)
      self.column_path.write(oprot)
      oprot.writeFieldEnd()
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 3)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_path is None:
      raise TProtocol.TProtocolException(message='Required field column_path is unset!')
    if self.timestamp is None:
      raise TProtocol.TProtocolException(message='Required field timestamp is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class batch_mutate_args:
  """
  Attributes:
   - mutation_map
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.MAP, 'mutation_map', (TType.STRING,None,TType.MAP,(TType.STRING,None,TType.LIST,(TType.STRUCT,(Mutation, Mutation.thrift_spec)))), None, ), # 1
    (2, TType.I32, 'consistency_level', None,     1, ), # 2
  )

  def __init__(self, mutation_map=None, consistency_level=thrift_spec[2][4],):
    self.mutation_map = mutation_map
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.MAP:
          self.mutation_map = {}
          (_ktype128, _vtype129, _size127 ) = iprot.readMapBegin() 
          for _i131 in xrange(_size127):
            _key132 = iprot.readString();
            _val133 = {}
            (_ktype135, _vtype136, _size134 ) = iprot.readMapBegin() 
            for _i138 in xrange(_size134):
              _key139 = iprot.readString();
              _val140 = []
              (_etype144, _size141) = iprot.readListBegin()
              for _i145 in xrange(_size141):
                _elem146 = Mutation()
                _elem146.read(iprot)
                _val140.append(_elem146)
              iprot.readListEnd()
              _val133[_key139] = _val140
            iprot.readMapEnd()
            self.mutation_map[_key132] = _val133
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('batch_mutate_args')
    if self.mutation_map is not None:
      oprot.writeFieldBegin('mutation_map', TType.MAP, 1)
      oprot.writeMapBegin(TType.STRING, TType.MAP, len(self.mutation_map))
      for kiter147,viter148 in self.mutation_map.items():
        oprot.writeString(kiter147)
        oprot.writeMapBegin(TType.STRING, TType.LIST, len(viter148))
        for kiter149,viter150 in viter148.items():
          oprot.writeString(kiter149)
          oprot.writeListBegin(TType.STRUCT, len(viter150))
          for iter151 in viter150:
            iter151.write(oprot)
          oprot.writeListEnd()
        oprot.writeMapEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 2)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.mutation_map is None:
      raise TProtocol.TProtocolException(message='Required field mutation_map is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class batch_mutate_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('batch_mutate_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class truncate_args:
  """
  Attributes:
   - cfname
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'cfname', None, None, ), # 1
  )

  def __init__(self, cfname=None,):
    self.cfname = cfname

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.cfname = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('truncate_args')
    if self.cfname is not None:
      oprot.writeFieldBegin('cfname', TType.STRING, 1)
      oprot.writeString(self.cfname)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cfname is None:
      raise TProtocol.TProtocolException(message='Required field cfname is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class truncate_result:
  """
  Attributes:
   - ire
   - ue
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
  )

  def __init__(self, ire=None, ue=None,):
    self.ire = ire
    self.ue = ue

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('truncate_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_schema_versions_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_schema_versions_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_schema_versions_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.LIST,(TType.STRING,None)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype153, _vtype154, _size152 ) = iprot.readMapBegin() 
          for _i156 in xrange(_size152):
            _key157 = iprot.readString();
            _val158 = []
            (_etype162, _size159) = iprot.readListBegin()
            for _i163 in xrange(_size159):
              _elem164 = iprot.readString();
              _val158.append(_elem164)
            iprot.readListEnd()
            self.success[_key157] = _val158
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_schema_versions_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.LIST, len(self.success))
      for kiter165,viter166 in self.success.items():
        oprot.writeString(kiter165)
        oprot.writeListBegin(TType.STRING, len(viter166))
        for iter167 in viter166:
          oprot.writeString(iter167)
        oprot.writeListEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspaces_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspaces_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspaces_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KsDef, KsDef.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype171, _size168) = iprot.readListBegin()
          for _i172 in xrange(_size168):
            _elem173 = KsDef()
            _elem173.read(iprot)
            self.success.append(_elem173)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspaces_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter174 in self.success:
        iter174.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_cluster_name_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_cluster_name_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_cluster_name_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_cluster_name_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_version_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_version_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_version_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_version_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_ring_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_ring_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_ring_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(TokenRange, TokenRange.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype178, _size175) = iprot.readListBegin()
          for _i179 in xrange(_size175):
            _elem180 = TokenRange()
            _elem180.read(iprot)
            self.success.append(_elem180)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_ring_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter181 in self.success:
        iter181.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_partitioner_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_partitioner_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_partitioner_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_partitioner_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_snitch_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_snitch_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_snitch_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_snitch_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspace_result:
  """
  Attributes:
   - success
   - nfe
   - ire
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (KsDef, KsDef.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'nfe', (NotFoundException, NotFoundException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, nfe=None, ire=None,):
    self.success = success
    self.nfe = nfe
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = KsDef()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.nfe = NotFoundException()
          self.nfe.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.nfe is not None:
      oprot.writeFieldBegin('nfe', TType.STRUCT, 1)
      self.nfe.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 2)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_splits_args:
  """
  Attributes:
   - cfName
   - start_token
   - end_token
   - keys_per_split
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'cfName', None, None, ), # 1
    (2, TType.STRING, 'start_token', None, None, ), # 2
    (3, TType.STRING, 'end_token', None, None, ), # 3
    (4, TType.I32, 'keys_per_split', None, None, ), # 4
  )

  def __init__(self, cfName=None, start_token=None, end_token=None, keys_per_split=None,):
    self.cfName = cfName
    self.start_token = start_token
    self.end_token = end_token
    self.keys_per_split = keys_per_split

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.cfName = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.keys_per_split = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_splits_args')
    if self.cfName is not None:
      oprot.writeFieldBegin('cfName', TType.STRING, 1)
      oprot.writeString(self.cfName)
      oprot.writeFieldEnd()
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 2)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 3)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.keys_per_split is not None:
      oprot.writeFieldBegin('keys_per_split', TType.I32, 4)
      oprot.writeI32(self.keys_per_split)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cfName is None:
      raise TProtocol.TProtocolException(message='Required field cfName is unset!')
    if self.start_token is None:
      raise TProtocol.TProtocolException(message='Required field start_token is unset!')
    if self.end_token is None:
      raise TProtocol.TProtocolException(message='Required field end_token is unset!')
    if self.keys_per_split is None:
      raise TProtocol.TProtocolException(message='Required field keys_per_split is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_splits_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRING,None), None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype185, _size182) = iprot.readListBegin()
          for _i186 in xrange(_size182):
            _elem187 = iprot.readString();
            self.success.append(_elem187)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_splits_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRING, len(self.success))
      for iter188 in self.success:
        oprot.writeString(iter188)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_column_family_args:
  """
  Attributes:
   - cf_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'cf_def', (CfDef, CfDef.thrift_spec), None, ), # 1
  )

  def __init__(self, cf_def=None,):
    self.cf_def = cf_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.cf_def = CfDef()
          self.cf_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_column_family_args')
    if self.cf_def is not None:
      oprot.writeFieldBegin('cf_def', TType.STRUCT, 1)
      self.cf_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cf_def is None:
      raise TProtocol.TProtocolException(message='Required field cf_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_column_family_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_column_family_args:
  """
  Attributes:
   - column_family
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'column_family', None, None, ), # 1
  )

  def __init__(self, column_family=None,):
    self.column_family = column_family

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_column_family_args')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 1)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_column_family_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_keyspace_args:
  """
  Attributes:
   - ks_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ks_def', (KsDef, KsDef.thrift_spec), None, ), # 1
  )

  def __init__(self, ks_def=None,):
    self.ks_def = ks_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ks_def = KsDef()
          self.ks_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_keyspace_args')
    if self.ks_def is not None:
      oprot.writeFieldBegin('ks_def', TType.STRUCT, 1)
      self.ks_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.ks_def is None:
      raise TProtocol.TProtocolException(message='Required field ks_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_keyspace_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_keyspace_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_keyspace_args:
  """
  Attributes:
   - ks_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ks_def', (KsDef, KsDef.thrift_spec), None, ), # 1
  )

  def __init__(self, ks_def=None,):
    self.ks_def = ks_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ks_def = KsDef()
          self.ks_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_keyspace_args')
    if self.ks_def is not None:
      oprot.writeFieldBegin('ks_def', TType.STRUCT, 1)
      self.ks_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.ks_def is None:
      raise TProtocol.TProtocolException(message='Required field ks_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_keyspace_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_column_family_args:
  """
  Attributes:
   - cf_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'cf_def', (CfDef, CfDef.thrift_spec), None, ), # 1
  )

  def __init__(self, cf_def=None,):
    self.cf_def = cf_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.cf_def = CfDef()
          self.cf_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_column_family_args')
    if self.cf_def is not None:
      oprot.writeFieldBegin('cf_def', TType.STRUCT, 1)
      self.cf_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cf_def is None:
      raise TProtocol.TProtocolException(message='Required field cf_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_column_family_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

########NEW FILE########
__FILENAME__ = constants
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException
from ttypes import *

VERSION = "19.4.0"

########NEW FILE########
__FILENAME__ = ttypes
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException

from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol, TProtocol
try:
  from thrift.protocol import fastbinary
except:
  fastbinary = None


class ConsistencyLevel:
  """
  The ConsistencyLevel is an enum that controls both read and write
  behavior based on the ReplicationFactor of the keyspace.  The
  different consistency levels have different meanings, depending on
  if you're doing a write or read operation.

  If W + R > ReplicationFactor, where W is the number of nodes to
  block for on write, and R the number to block for on reads, you
  will have strongly consistent behavior; that is, readers will
  always see the most recent write. Of these, the most interesting is
  to do QUORUM reads and writes, which gives you consistency while
  still allowing availability in the face of node failures up to half
  of <ReplicationFactor>. Of course if latency is more important than
  consistency then you can use lower values for either or both.

  Some ConsistencyLevels (ONE, TWO, THREE) refer to a specific number
  of replicas rather than a logical concept that adjusts
  automatically with the replication factor.  Of these, only ONE is
  commonly used; TWO and (even more rarely) THREE are only useful
  when you care more about guaranteeing a certain level of
  durability, than consistency.

  Write consistency levels make the following guarantees before reporting success to the client:
    ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node.
    ONE          Ensure that the write has been written to at least 1 node's commit log and memory table
    TWO          Ensure that the write has been written to at least 2 node's commit log and memory table
    THREE        Ensure that the write has been written to at least 3 node's commit log and memory table
    QUORUM       Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes
    LOCAL_QUORUM Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes, within the local datacenter (requires NetworkTopologyStrategy)
    EACH_QUORUM  Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes in each datacenter (requires NetworkTopologyStrategy)
    ALL          Ensure that the write is written to <code>&lt;ReplicationFactor&gt;</code> nodes before responding to the client.

  Read consistency levels make the following guarantees before returning successful results to the client:
    ANY          Not supported. You probably want ONE instead.
    ONE          Returns the record obtained from a single replica.
    TWO          Returns the record with the most recent timestamp once two replicas have replied.
    THREE        Returns the record with the most recent timestamp once three replicas have replied.
    QUORUM       Returns the record with the most recent timestamp once a majority of replicas have replied.
    LOCAL_QUORUM Returns the record with the most recent timestamp once a majority of replicas within the local datacenter have replied.
    EACH_QUORUM  Returns the record with the most recent timestamp once a majority of replicas within each datacenter have replied.
    ALL          Returns the record with the most recent timestamp once all replicas have replied (implies no replica may be down)..
  """
  ONE = 1
  QUORUM = 2
  LOCAL_QUORUM = 3
  EACH_QUORUM = 4
  ALL = 5
  ANY = 6
  TWO = 7
  THREE = 8

  _VALUES_TO_NAMES = {
    1: "ONE",
    2: "QUORUM",
    3: "LOCAL_QUORUM",
    4: "EACH_QUORUM",
    5: "ALL",
    6: "ANY",
    7: "TWO",
    8: "THREE",
  }

  _NAMES_TO_VALUES = {
    "ONE": 1,
    "QUORUM": 2,
    "LOCAL_QUORUM": 3,
    "EACH_QUORUM": 4,
    "ALL": 5,
    "ANY": 6,
    "TWO": 7,
    "THREE": 8,
  }

class IndexOperator:
  EQ = 0
  GTE = 1
  GT = 2
  LTE = 3
  LT = 4

  _VALUES_TO_NAMES = {
    0: "EQ",
    1: "GTE",
    2: "GT",
    3: "LTE",
    4: "LT",
  }

  _NAMES_TO_VALUES = {
    "EQ": 0,
    "GTE": 1,
    "GT": 2,
    "LTE": 3,
    "LT": 4,
  }

class IndexType:
  KEYS = 0

  _VALUES_TO_NAMES = {
    0: "KEYS",
  }

  _NAMES_TO_VALUES = {
    "KEYS": 0,
  }


class Column:
  """
  Basic unit of data within a ColumnFamily.
  @param name, the name by which this column is set and retrieved.  Maximum 64KB long.
  @param value. The data associated with the name.  Maximum 2GB long, but in practice you should limit it to small numbers of MB (since Thrift must read the full value into memory to operate on it).
  @param timestamp. The timestamp is used for conflict detection/resolution when two columns with same name need to be compared.
  @param ttl. An optional, positive delay (in seconds) after which the column will be automatically deleted.

  Attributes:
   - name
   - value
   - timestamp
   - ttl
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'value', None, None, ), # 2
    (3, TType.I64, 'timestamp', None, None, ), # 3
    (4, TType.I32, 'ttl', None, None, ), # 4
  )

  def __init__(self, name=None, value=None, timestamp=None, ttl=None,):
    self.name = name
    self.value = value
    self.timestamp = timestamp
    self.ttl = ttl

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.value = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.ttl = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Column')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.value is not None:
      oprot.writeFieldBegin('value', TType.STRING, 2)
      oprot.writeString(self.value)
      oprot.writeFieldEnd()
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 3)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.ttl is not None:
      oprot.writeFieldBegin('ttl', TType.I32, 4)
      oprot.writeI32(self.ttl)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.value is None:
      raise TProtocol.TProtocolException(message='Required field value is unset!')
    if self.timestamp is None:
      raise TProtocol.TProtocolException(message='Required field timestamp is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SuperColumn:
  """
  A named list of columns.
  @param name. see Column.name.
  @param columns. A collection of standard Columns.  The columns within a super column are defined in an adhoc manner.
                  Columns within a super column do not have to have matching structures (similarly named child columns).

  Attributes:
   - name
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(Column, Column.thrift_spec)), None, ), # 2
  )

  def __init__(self, name=None, columns=None,):
    self.name = name
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype3, _size0) = iprot.readListBegin()
          for _i4 in xrange(_size0):
            _elem5 = Column()
            _elem5.read(iprot)
            self.columns.append(_elem5)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SuperColumn')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter6 in self.columns:
        iter6.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnOrSuperColumn:
  """
  Methods for fetching rows/records from Cassandra will return either a single instance of ColumnOrSuperColumn or a list
  of ColumnOrSuperColumns (get_slice()). If you're looking up a SuperColumn (or list of SuperColumns) then the resulting
  instances of ColumnOrSuperColumn will have the requested SuperColumn in the attribute super_column. For queries resulting
  in Columns, those values will be in the attribute column. This change was made between 0.3 and 0.4 to standardize on
  single query methods that may return either a SuperColumn or Column.

  @param column. The Column returned by get() or get_slice().
  @param super_column. The SuperColumn returned by get() or get_slice().

  Attributes:
   - column
   - super_column
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column', (Column, Column.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'super_column', (SuperColumn, SuperColumn.thrift_spec), None, ), # 2
  )

  def __init__(self, column=None, super_column=None,):
    self.column = column
    self.super_column = super_column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column = Column()
          self.column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.super_column = SuperColumn()
          self.super_column.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnOrSuperColumn')
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRUCT, 1)
      self.column.write(oprot)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRUCT, 2)
      self.super_column.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class NotFoundException(TException):
  """
  A specific column was requested that does not exist.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('NotFoundException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class InvalidRequestException(TException):
  """
  Invalid request could mean keyspace or column family does not exist, required parameters are missing, or a parameter is malformed.
  why contains an associated error message.

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('InvalidRequestException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class UnavailableException(TException):
  """
  Not all the replicas required could be created and/or read.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('UnavailableException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TimedOutException(TException):
  """
  RPC timeout was exceeded.  either a node failed mid-operation, or load was too high, or the requested op was too large.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('TimedOutException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthenticationException(TException):
  """
  invalid authentication request (invalid keyspace, user does not exist, or credentials invalid)

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthenticationException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthorizationException(TException):
  """
  invalid authorization request (user does not have access to keyspace)

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthorizationException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnParent:
  """
  ColumnParent is used when selecting groups of columns from the same ColumnFamily. In directory structure terms, imagine
  ColumnParent as ColumnPath + '/../'.

  See also <a href="cassandra.html#Struct_ColumnPath">ColumnPath</a>

  Attributes:
   - column_family
   - super_column
  """

  thrift_spec = (
    None, # 0
    None, # 1
    None, # 2
    (3, TType.STRING, 'column_family', None, None, ), # 3
    (4, TType.STRING, 'super_column', None, None, ), # 4
  )

  def __init__(self, column_family=None, super_column=None,):
    self.column_family = column_family
    self.super_column = super_column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 3:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnParent')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 3)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 4)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnPath:
  """
  The ColumnPath is the path to a single column in Cassandra. It might make sense to think of ColumnPath and
  ColumnParent in terms of a directory structure.

  ColumnPath is used to looking up a single column.

  @param column_family. The name of the CF of the column being looked up.
  @param super_column. The super column name.
  @param column. The column name.

  Attributes:
   - column_family
   - super_column
   - column
  """

  thrift_spec = (
    None, # 0
    None, # 1
    None, # 2
    (3, TType.STRING, 'column_family', None, None, ), # 3
    (4, TType.STRING, 'super_column', None, None, ), # 4
    (5, TType.STRING, 'column', None, None, ), # 5
  )

  def __init__(self, column_family=None, super_column=None, column=None,):
    self.column_family = column_family
    self.super_column = super_column
    self.column = column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 3:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.STRING:
          self.column = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnPath')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 3)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 4)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRING, 5)
      oprot.writeString(self.column)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SliceRange:
  """
  A slice range is a structure that stores basic range, ordering and limit information for a query that will return
  multiple columns. It could be thought of as Cassandra's version of LIMIT and ORDER BY

  @param start. The column name to start the slice with. This attribute is not required, though there is no default value,
                and can be safely set to '', i.e., an empty byte array, to start with the first column name. Otherwise, it
                must a valid value under the rules of the Comparator defined for the given ColumnFamily.
  @param finish. The column name to stop the slice at. This attribute is not required, though there is no default value,
                 and can be safely set to an empty byte array to not stop until 'count' results are seen. Otherwise, it
                 must also be a valid value to the ColumnFamily Comparator.
  @param reversed. Whether the results should be ordered in reversed order. Similar to ORDER BY blah DESC in SQL.
  @param count. How many columns to return. Similar to LIMIT in SQL. May be arbitrarily large, but Thrift will
                materialize the whole result into memory before returning it to the client, so be aware that you may
                be better served by iterating through slices by passing the last value of one call in as the 'start'
                of the next instead of increasing 'count' arbitrarily large.

  Attributes:
   - start
   - finish
   - reversed
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start', None, None, ), # 1
    (2, TType.STRING, 'finish', None, None, ), # 2
    (3, TType.BOOL, 'reversed', None, False, ), # 3
    (4, TType.I32, 'count', None, 100, ), # 4
  )

  def __init__(self, start=None, finish=None, reversed=thrift_spec[3][4], count=thrift_spec[4][4],):
    self.start = start
    self.finish = finish
    self.reversed = reversed
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.finish = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.BOOL:
          self.reversed = iprot.readBool();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SliceRange')
    if self.start is not None:
      oprot.writeFieldBegin('start', TType.STRING, 1)
      oprot.writeString(self.start)
      oprot.writeFieldEnd()
    if self.finish is not None:
      oprot.writeFieldBegin('finish', TType.STRING, 2)
      oprot.writeString(self.finish)
      oprot.writeFieldEnd()
    if self.reversed is not None:
      oprot.writeFieldBegin('reversed', TType.BOOL, 3)
      oprot.writeBool(self.reversed)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 4)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.start is None:
      raise TProtocol.TProtocolException(message='Required field start is unset!')
    if self.finish is None:
      raise TProtocol.TProtocolException(message='Required field finish is unset!')
    if self.reversed is None:
      raise TProtocol.TProtocolException(message='Required field reversed is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SlicePredicate:
  """
  A SlicePredicate is similar to a mathematic predicate (see http://en.wikipedia.org/wiki/Predicate_(mathematical_logic)),
  which is described as "a property that the elements of a set have in common."

  SlicePredicate's in Cassandra are described with either a list of column_names or a SliceRange.  If column_names is
  specified, slice_range is ignored.

  @param column_name. A list of column names to retrieve. This can be used similar to Memcached's "multi-get" feature
                      to fetch N known column names. For instance, if you know you wish to fetch columns 'Joe', 'Jack',
                      and 'Jim' you can pass those column names as a list to fetch all three at once.
  @param slice_range. A SliceRange describing how to range, order, and/or limit the slice.

  Attributes:
   - column_names
   - slice_range
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'column_names', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'slice_range', (SliceRange, SliceRange.thrift_spec), None, ), # 2
  )

  def __init__(self, column_names=None, slice_range=None,):
    self.column_names = column_names
    self.slice_range = slice_range

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.column_names = []
          (_etype10, _size7) = iprot.readListBegin()
          for _i11 in xrange(_size7):
            _elem12 = iprot.readString();
            self.column_names.append(_elem12)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.slice_range = SliceRange()
          self.slice_range.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SlicePredicate')
    if self.column_names is not None:
      oprot.writeFieldBegin('column_names', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.column_names))
      for iter13 in self.column_names:
        oprot.writeString(iter13)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.slice_range is not None:
      oprot.writeFieldBegin('slice_range', TType.STRUCT, 2)
      self.slice_range.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class IndexExpression:
  """
  Attributes:
   - column_name
   - op
   - value
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'column_name', None, None, ), # 1
    (2, TType.I32, 'op', None, None, ), # 2
    (3, TType.STRING, 'value', None, None, ), # 3
  )

  def __init__(self, column_name=None, op=None, value=None,):
    self.column_name = column_name
    self.op = op
    self.value = value

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.column_name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.op = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.value = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('IndexExpression')
    if self.column_name is not None:
      oprot.writeFieldBegin('column_name', TType.STRING, 1)
      oprot.writeString(self.column_name)
      oprot.writeFieldEnd()
    if self.op is not None:
      oprot.writeFieldBegin('op', TType.I32, 2)
      oprot.writeI32(self.op)
      oprot.writeFieldEnd()
    if self.value is not None:
      oprot.writeFieldBegin('value', TType.STRING, 3)
      oprot.writeString(self.value)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_name is None:
      raise TProtocol.TProtocolException(message='Required field column_name is unset!')
    if self.op is None:
      raise TProtocol.TProtocolException(message='Required field op is unset!')
    if self.value is None:
      raise TProtocol.TProtocolException(message='Required field value is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class IndexClause:
  """
  Attributes:
   - expressions
   - start_key
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'expressions', (TType.STRUCT,(IndexExpression, IndexExpression.thrift_spec)), None, ), # 1
    (2, TType.STRING, 'start_key', None, None, ), # 2
    (3, TType.I32, 'count', None, 100, ), # 3
  )

  def __init__(self, expressions=None, start_key=None, count=thrift_spec[3][4],):
    self.expressions = expressions
    self.start_key = start_key
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.expressions = []
          (_etype17, _size14) = iprot.readListBegin()
          for _i18 in xrange(_size14):
            _elem19 = IndexExpression()
            _elem19.read(iprot)
            self.expressions.append(_elem19)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.start_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('IndexClause')
    if self.expressions is not None:
      oprot.writeFieldBegin('expressions', TType.LIST, 1)
      oprot.writeListBegin(TType.STRUCT, len(self.expressions))
      for iter20 in self.expressions:
        iter20.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.start_key is not None:
      oprot.writeFieldBegin('start_key', TType.STRING, 2)
      oprot.writeString(self.start_key)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 3)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.expressions is None:
      raise TProtocol.TProtocolException(message='Required field expressions is unset!')
    if self.start_key is None:
      raise TProtocol.TProtocolException(message='Required field start_key is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeyRange:
  """
  The semantics of start keys and tokens are slightly different.
  Keys are start-inclusive; tokens are start-exclusive.  Token
  ranges may also wrap -- that is, the end token may be less
  than the start one.  Thus, a range from keyX to keyX is a
  one-element range, but a range from tokenY to tokenY is the
  full ring.

  Attributes:
   - start_key
   - end_key
   - start_token
   - end_token
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start_key', None, None, ), # 1
    (2, TType.STRING, 'end_key', None, None, ), # 2
    (3, TType.STRING, 'start_token', None, None, ), # 3
    (4, TType.STRING, 'end_token', None, None, ), # 4
    (5, TType.I32, 'count', None, 100, ), # 5
  )

  def __init__(self, start_key=None, end_key=None, start_token=None, end_token=None, count=thrift_spec[5][4],):
    self.start_key = start_key
    self.end_key = end_key
    self.start_token = start_token
    self.end_token = end_token
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.end_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeyRange')
    if self.start_key is not None:
      oprot.writeFieldBegin('start_key', TType.STRING, 1)
      oprot.writeString(self.start_key)
      oprot.writeFieldEnd()
    if self.end_key is not None:
      oprot.writeFieldBegin('end_key', TType.STRING, 2)
      oprot.writeString(self.end_key)
      oprot.writeFieldEnd()
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 3)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 4)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 5)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeySlice:
  """
  A KeySlice is key followed by the data it maps to. A collection of KeySlice is returned by the get_range_slice operation.

  @param key. a row key
  @param columns. List of data represented by the key. Typically, the list is pared down to only the columns specified by
                  a SlicePredicate.

  Attributes:
   - key
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec)), None, ), # 2
  )

  def __init__(self, key=None, columns=None,):
    self.key = key
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype24, _size21) = iprot.readListBegin()
          for _i25 in xrange(_size21):
            _elem26 = ColumnOrSuperColumn()
            _elem26.read(iprot)
            self.columns.append(_elem26)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeySlice')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter27 in self.columns:
        iter27.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeyCount:
  """
  Attributes:
   - key
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.I32, 'count', None, None, ), # 2
  )

  def __init__(self, key=None, count=None,):
    self.key = key
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeyCount')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 2)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class Deletion:
  """
  Attributes:
   - timestamp
   - super_column
   - predicate
  """

  thrift_spec = (
    None, # 0
    (1, TType.I64, 'timestamp', None, None, ), # 1
    (2, TType.STRING, 'super_column', None, None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
  )

  def __init__(self, timestamp=None, super_column=None, predicate=None,):
    self.timestamp = timestamp
    self.super_column = super_column
    self.predicate = predicate

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Deletion')
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 1)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 2)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.timestamp is None:
      raise TProtocol.TProtocolException(message='Required field timestamp is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class Mutation:
  """
  A Mutation is either an insert, represented by filling column_or_supercolumn, or a deletion, represented by filling the deletion attribute.
  @param column_or_supercolumn. An insert to a column or supercolumn
  @param deletion. A deletion of a column or supercolumn

  Attributes:
   - column_or_supercolumn
   - deletion
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_or_supercolumn', (ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'deletion', (Deletion, Deletion.thrift_spec), None, ), # 2
  )

  def __init__(self, column_or_supercolumn=None, deletion=None,):
    self.column_or_supercolumn = column_or_supercolumn
    self.deletion = deletion

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_or_supercolumn = ColumnOrSuperColumn()
          self.column_or_supercolumn.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.deletion = Deletion()
          self.deletion.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Mutation')
    if self.column_or_supercolumn is not None:
      oprot.writeFieldBegin('column_or_supercolumn', TType.STRUCT, 1)
      self.column_or_supercolumn.write(oprot)
      oprot.writeFieldEnd()
    if self.deletion is not None:
      oprot.writeFieldBegin('deletion', TType.STRUCT, 2)
      self.deletion.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TokenRange:
  """
  Attributes:
   - start_token
   - end_token
   - endpoints
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start_token', None, None, ), # 1
    (2, TType.STRING, 'end_token', None, None, ), # 2
    (3, TType.LIST, 'endpoints', (TType.STRING,None), None, ), # 3
  )

  def __init__(self, start_token=None, end_token=None, endpoints=None,):
    self.start_token = start_token
    self.end_token = end_token
    self.endpoints = endpoints

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.LIST:
          self.endpoints = []
          (_etype31, _size28) = iprot.readListBegin()
          for _i32 in xrange(_size28):
            _elem33 = iprot.readString();
            self.endpoints.append(_elem33)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('TokenRange')
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 1)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 2)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.endpoints is not None:
      oprot.writeFieldBegin('endpoints', TType.LIST, 3)
      oprot.writeListBegin(TType.STRING, len(self.endpoints))
      for iter34 in self.endpoints:
        oprot.writeString(iter34)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.start_token is None:
      raise TProtocol.TProtocolException(message='Required field start_token is unset!')
    if self.end_token is None:
      raise TProtocol.TProtocolException(message='Required field end_token is unset!')
    if self.endpoints is None:
      raise TProtocol.TProtocolException(message='Required field endpoints is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthenticationRequest:
  """
  Authentication requests can contain any data, dependent on the IAuthenticator used

  Attributes:
   - credentials
  """

  thrift_spec = (
    None, # 0
    (1, TType.MAP, 'credentials', (TType.STRING,None,TType.STRING,None), None, ), # 1
  )

  def __init__(self, credentials=None,):
    self.credentials = credentials

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.MAP:
          self.credentials = {}
          (_ktype36, _vtype37, _size35 ) = iprot.readMapBegin() 
          for _i39 in xrange(_size35):
            _key40 = iprot.readString();
            _val41 = iprot.readString();
            self.credentials[_key40] = _val41
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthenticationRequest')
    if self.credentials is not None:
      oprot.writeFieldBegin('credentials', TType.MAP, 1)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.credentials))
      for kiter42,viter43 in self.credentials.items():
        oprot.writeString(kiter42)
        oprot.writeString(viter43)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.credentials is None:
      raise TProtocol.TProtocolException(message='Required field credentials is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnDef:
  """
  Attributes:
   - name
   - validation_class
   - index_type
   - index_name
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'validation_class', None, None, ), # 2
    (3, TType.I32, 'index_type', None, None, ), # 3
    (4, TType.STRING, 'index_name', None, None, ), # 4
  )

  def __init__(self, name=None, validation_class=None, index_type=None, index_name=None,):
    self.name = name
    self.validation_class = validation_class
    self.index_type = index_type
    self.index_name = index_name

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.validation_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.index_type = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.index_name = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnDef')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.validation_class is not None:
      oprot.writeFieldBegin('validation_class', TType.STRING, 2)
      oprot.writeString(self.validation_class)
      oprot.writeFieldEnd()
    if self.index_type is not None:
      oprot.writeFieldBegin('index_type', TType.I32, 3)
      oprot.writeI32(self.index_type)
      oprot.writeFieldEnd()
    if self.index_name is not None:
      oprot.writeFieldBegin('index_name', TType.STRING, 4)
      oprot.writeString(self.index_name)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.validation_class is None:
      raise TProtocol.TProtocolException(message='Required field validation_class is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CfDef:
  """
  Attributes:
   - keyspace
   - name
   - column_type
   - comparator_type
   - subcomparator_type
   - comment
   - row_cache_size
   - key_cache_size
   - read_repair_chance
   - column_metadata
   - gc_grace_seconds
   - default_validation_class
   - id
   - min_compaction_threshold
   - max_compaction_threshold
   - row_cache_save_period_in_seconds
   - key_cache_save_period_in_seconds
   - memtable_flush_after_mins
   - memtable_throughput_in_mb
   - memtable_operations_in_millions
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
    (2, TType.STRING, 'name', None, None, ), # 2
    (3, TType.STRING, 'column_type', None, "Standard", ), # 3
    None, # 4
    (5, TType.STRING, 'comparator_type', None, "BytesType", ), # 5
    (6, TType.STRING, 'subcomparator_type', None, None, ), # 6
    None, # 7
    (8, TType.STRING, 'comment', None, None, ), # 8
    (9, TType.DOUBLE, 'row_cache_size', None, 0, ), # 9
    None, # 10
    (11, TType.DOUBLE, 'key_cache_size', None, 200000, ), # 11
    (12, TType.DOUBLE, 'read_repair_chance', None, 1, ), # 12
    (13, TType.LIST, 'column_metadata', (TType.STRUCT,(ColumnDef, ColumnDef.thrift_spec)), None, ), # 13
    (14, TType.I32, 'gc_grace_seconds', None, None, ), # 14
    (15, TType.STRING, 'default_validation_class', None, None, ), # 15
    (16, TType.I32, 'id', None, None, ), # 16
    (17, TType.I32, 'min_compaction_threshold', None, None, ), # 17
    (18, TType.I32, 'max_compaction_threshold', None, None, ), # 18
    (19, TType.I32, 'row_cache_save_period_in_seconds', None, None, ), # 19
    (20, TType.I32, 'key_cache_save_period_in_seconds', None, None, ), # 20
    (21, TType.I32, 'memtable_flush_after_mins', None, None, ), # 21
    (22, TType.I32, 'memtable_throughput_in_mb', None, None, ), # 22
    (23, TType.DOUBLE, 'memtable_operations_in_millions', None, None, ), # 23
  )

  def __init__(self, keyspace=None, name=None, column_type=thrift_spec[3][4], comparator_type=thrift_spec[5][4], subcomparator_type=None, comment=None, row_cache_size=thrift_spec[9][4], key_cache_size=thrift_spec[11][4], read_repair_chance=thrift_spec[12][4], column_metadata=None, gc_grace_seconds=None, default_validation_class=None, id=None, min_compaction_threshold=None, max_compaction_threshold=None, row_cache_save_period_in_seconds=None, key_cache_save_period_in_seconds=None, memtable_flush_after_mins=None, memtable_throughput_in_mb=None, memtable_operations_in_millions=None,):
    self.keyspace = keyspace
    self.name = name
    self.column_type = column_type
    self.comparator_type = comparator_type
    self.subcomparator_type = subcomparator_type
    self.comment = comment
    self.row_cache_size = row_cache_size
    self.key_cache_size = key_cache_size
    self.read_repair_chance = read_repair_chance
    self.column_metadata = column_metadata
    self.gc_grace_seconds = gc_grace_seconds
    self.default_validation_class = default_validation_class
    self.id = id
    self.min_compaction_threshold = min_compaction_threshold
    self.max_compaction_threshold = max_compaction_threshold
    self.row_cache_save_period_in_seconds = row_cache_save_period_in_seconds
    self.key_cache_save_period_in_seconds = key_cache_save_period_in_seconds
    self.memtable_flush_after_mins = memtable_flush_after_mins
    self.memtable_throughput_in_mb = memtable_throughput_in_mb
    self.memtable_operations_in_millions = memtable_operations_in_millions

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.column_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.STRING:
          self.comparator_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 6:
        if ftype == TType.STRING:
          self.subcomparator_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 8:
        if ftype == TType.STRING:
          self.comment = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 9:
        if ftype == TType.DOUBLE:
          self.row_cache_size = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 11:
        if ftype == TType.DOUBLE:
          self.key_cache_size = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 12:
        if ftype == TType.DOUBLE:
          self.read_repair_chance = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 13:
        if ftype == TType.LIST:
          self.column_metadata = []
          (_etype47, _size44) = iprot.readListBegin()
          for _i48 in xrange(_size44):
            _elem49 = ColumnDef()
            _elem49.read(iprot)
            self.column_metadata.append(_elem49)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 14:
        if ftype == TType.I32:
          self.gc_grace_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 15:
        if ftype == TType.STRING:
          self.default_validation_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 16:
        if ftype == TType.I32:
          self.id = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 17:
        if ftype == TType.I32:
          self.min_compaction_threshold = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 18:
        if ftype == TType.I32:
          self.max_compaction_threshold = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 19:
        if ftype == TType.I32:
          self.row_cache_save_period_in_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 20:
        if ftype == TType.I32:
          self.key_cache_save_period_in_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 21:
        if ftype == TType.I32:
          self.memtable_flush_after_mins = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 22:
        if ftype == TType.I32:
          self.memtable_throughput_in_mb = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 23:
        if ftype == TType.DOUBLE:
          self.memtable_operations_in_millions = iprot.readDouble();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CfDef')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 2)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.column_type is not None:
      oprot.writeFieldBegin('column_type', TType.STRING, 3)
      oprot.writeString(self.column_type)
      oprot.writeFieldEnd()
    if self.comparator_type is not None:
      oprot.writeFieldBegin('comparator_type', TType.STRING, 5)
      oprot.writeString(self.comparator_type)
      oprot.writeFieldEnd()
    if self.subcomparator_type is not None:
      oprot.writeFieldBegin('subcomparator_type', TType.STRING, 6)
      oprot.writeString(self.subcomparator_type)
      oprot.writeFieldEnd()
    if self.comment is not None:
      oprot.writeFieldBegin('comment', TType.STRING, 8)
      oprot.writeString(self.comment)
      oprot.writeFieldEnd()
    if self.row_cache_size is not None:
      oprot.writeFieldBegin('row_cache_size', TType.DOUBLE, 9)
      oprot.writeDouble(self.row_cache_size)
      oprot.writeFieldEnd()
    if self.key_cache_size is not None:
      oprot.writeFieldBegin('key_cache_size', TType.DOUBLE, 11)
      oprot.writeDouble(self.key_cache_size)
      oprot.writeFieldEnd()
    if self.read_repair_chance is not None:
      oprot.writeFieldBegin('read_repair_chance', TType.DOUBLE, 12)
      oprot.writeDouble(self.read_repair_chance)
      oprot.writeFieldEnd()
    if self.column_metadata is not None:
      oprot.writeFieldBegin('column_metadata', TType.LIST, 13)
      oprot.writeListBegin(TType.STRUCT, len(self.column_metadata))
      for iter50 in self.column_metadata:
        iter50.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.gc_grace_seconds is not None:
      oprot.writeFieldBegin('gc_grace_seconds', TType.I32, 14)
      oprot.writeI32(self.gc_grace_seconds)
      oprot.writeFieldEnd()
    if self.default_validation_class is not None:
      oprot.writeFieldBegin('default_validation_class', TType.STRING, 15)
      oprot.writeString(self.default_validation_class)
      oprot.writeFieldEnd()
    if self.id is not None:
      oprot.writeFieldBegin('id', TType.I32, 16)
      oprot.writeI32(self.id)
      oprot.writeFieldEnd()
    if self.min_compaction_threshold is not None:
      oprot.writeFieldBegin('min_compaction_threshold', TType.I32, 17)
      oprot.writeI32(self.min_compaction_threshold)
      oprot.writeFieldEnd()
    if self.max_compaction_threshold is not None:
      oprot.writeFieldBegin('max_compaction_threshold', TType.I32, 18)
      oprot.writeI32(self.max_compaction_threshold)
      oprot.writeFieldEnd()
    if self.row_cache_save_period_in_seconds is not None:
      oprot.writeFieldBegin('row_cache_save_period_in_seconds', TType.I32, 19)
      oprot.writeI32(self.row_cache_save_period_in_seconds)
      oprot.writeFieldEnd()
    if self.key_cache_save_period_in_seconds is not None:
      oprot.writeFieldBegin('key_cache_save_period_in_seconds', TType.I32, 20)
      oprot.writeI32(self.key_cache_save_period_in_seconds)
      oprot.writeFieldEnd()
    if self.memtable_flush_after_mins is not None:
      oprot.writeFieldBegin('memtable_flush_after_mins', TType.I32, 21)
      oprot.writeI32(self.memtable_flush_after_mins)
      oprot.writeFieldEnd()
    if self.memtable_throughput_in_mb is not None:
      oprot.writeFieldBegin('memtable_throughput_in_mb', TType.I32, 22)
      oprot.writeI32(self.memtable_throughput_in_mb)
      oprot.writeFieldEnd()
    if self.memtable_operations_in_millions is not None:
      oprot.writeFieldBegin('memtable_operations_in_millions', TType.DOUBLE, 23)
      oprot.writeDouble(self.memtable_operations_in_millions)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KsDef:
  """
  Attributes:
   - name
   - strategy_class
   - strategy_options
   - replication_factor
   - cf_defs
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'strategy_class', None, None, ), # 2
    (3, TType.MAP, 'strategy_options', (TType.STRING,None,TType.STRING,None), None, ), # 3
    (4, TType.I32, 'replication_factor', None, None, ), # 4
    (5, TType.LIST, 'cf_defs', (TType.STRUCT,(CfDef, CfDef.thrift_spec)), None, ), # 5
  )

  def __init__(self, name=None, strategy_class=None, strategy_options=None, replication_factor=None, cf_defs=None,):
    self.name = name
    self.strategy_class = strategy_class
    self.strategy_options = strategy_options
    self.replication_factor = replication_factor
    self.cf_defs = cf_defs

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.strategy_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.MAP:
          self.strategy_options = {}
          (_ktype52, _vtype53, _size51 ) = iprot.readMapBegin() 
          for _i55 in xrange(_size51):
            _key56 = iprot.readString();
            _val57 = iprot.readString();
            self.strategy_options[_key56] = _val57
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.replication_factor = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.LIST:
          self.cf_defs = []
          (_etype61, _size58) = iprot.readListBegin()
          for _i62 in xrange(_size58):
            _elem63 = CfDef()
            _elem63.read(iprot)
            self.cf_defs.append(_elem63)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KsDef')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.strategy_class is not None:
      oprot.writeFieldBegin('strategy_class', TType.STRING, 2)
      oprot.writeString(self.strategy_class)
      oprot.writeFieldEnd()
    if self.strategy_options is not None:
      oprot.writeFieldBegin('strategy_options', TType.MAP, 3)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.strategy_options))
      for kiter64,viter65 in self.strategy_options.items():
        oprot.writeString(kiter64)
        oprot.writeString(viter65)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.replication_factor is not None:
      oprot.writeFieldBegin('replication_factor', TType.I32, 4)
      oprot.writeI32(self.replication_factor)
      oprot.writeFieldEnd()
    if self.cf_defs is not None:
      oprot.writeFieldBegin('cf_defs', TType.LIST, 5)
      oprot.writeListBegin(TType.STRUCT, len(self.cf_defs))
      for iter66 in self.cf_defs:
        iter66.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.strategy_class is None:
      raise TProtocol.TProtocolException(message='Required field strategy_class is unset!')
    if self.replication_factor is None:
      raise TProtocol.TProtocolException(message='Required field replication_factor is unset!')
    if self.cf_defs is None:
      raise TProtocol.TProtocolException(message='Required field cf_defs is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

########NEW FILE########
__FILENAME__ = Cassandra
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException
from ttypes import *
from thrift.Thrift import TProcessor
from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol, TProtocol
try:
  from thrift.protocol import fastbinary
except:
  fastbinary = None


class Iface:
  def login(self, auth_request):
    """
    Parameters:
     - auth_request
    """
    pass

  def set_keyspace(self, keyspace):
    """
    Parameters:
     - keyspace
    """
    pass

  def get(self, key, column_path, consistency_level):
    """
    Get the Column or SuperColumn at the given column_path. If no value is present, NotFoundException is thrown. (This is
    the only method that can throw an exception under non-failure conditions.)

    Parameters:
     - key
     - column_path
     - consistency_level
    """
    pass

  def get_slice(self, key, column_parent, predicate, consistency_level):
    """
    Get the group of columns contained by column_parent (either a ColumnFamily name or a ColumnFamily/SuperColumn name
    pair) specified by the given SlicePredicate. If no matching values are found, an empty list is returned.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def get_count(self, key, column_parent, predicate, consistency_level):
    """
    returns the number of columns matching <code>predicate</code> for a particular <code>key</code>,
    <code>ColumnFamily</code> and optionally <code>SuperColumn</code>.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def multiget_slice(self, keys, column_parent, predicate, consistency_level):
    """
    Performs a get_slice for column_parent and predicate for the given keys in parallel.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def multiget_count(self, keys, column_parent, predicate, consistency_level):
    """
    Perform a get_count in parallel on the given list<binary> keys. The return value maps keys to the count found.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    pass

  def get_range_slices(self, column_parent, predicate, range, consistency_level):
    """
    returns a subset of columns for a contiguous range of keys.

    Parameters:
     - column_parent
     - predicate
     - range
     - consistency_level
    """
    pass

  def get_paged_slice(self, column_family, range, start_column, consistency_level):
    """
    returns a range of columns, wrapping to the next rows if necessary to collect max_results.

    Parameters:
     - column_family
     - range
     - start_column
     - consistency_level
    """
    pass

  def get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    """
    Returns the subset of columns specified in SlicePredicate for the rows matching the IndexClause
    @Deprecated; use get_range_slices instead with range.row_filter specified

    Parameters:
     - column_parent
     - index_clause
     - column_predicate
     - consistency_level
    """
    pass

  def insert(self, key, column_parent, column, consistency_level):
    """
    Insert a Column at the given column_parent.column_family and optional column_parent.super_column.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    pass

  def add(self, key, column_parent, column, consistency_level):
    """
    Increment or decrement a counter.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    pass

  def remove(self, key, column_path, timestamp, consistency_level):
    """
    Remove data from the row specified by key at the granularity specified by column_path, and the given timestamp. Note
    that all the values in column_path besides column_path.column_family are truly optional: you can remove the entire
    row by just specifying the ColumnFamily, or you can remove a SuperColumn or a single Column by specifying those levels too.

    Parameters:
     - key
     - column_path
     - timestamp
     - consistency_level
    """
    pass

  def remove_counter(self, key, path, consistency_level):
    """
    Remove a counter at the specified location.
    Note that counters have limited support for deletes: if you remove a counter, you must wait to issue any following update
    until the delete has reached all the nodes and all of them have been fully compacted.

    Parameters:
     - key
     - path
     - consistency_level
    """
    pass

  def batch_mutate(self, mutation_map, consistency_level):
    """
      Mutate many columns or super columns for many row keys. See also: Mutation.

      mutation_map maps key to column family to a list of Mutation objects to take place at that scope.
    *

    Parameters:
     - mutation_map
     - consistency_level
    """
    pass

  def truncate(self, cfname):
    """
    Truncate will mark and entire column family as deleted.
    From the user's perspective a successful call to truncate will result complete data deletion from cfname.
    Internally, however, disk space will not be immediatily released, as with all deletes in cassandra, this one
    only marks the data as deleted.
    The operation succeeds only if all hosts in the cluster at available and will throw an UnavailableException if
    some hosts are down.

    Parameters:
     - cfname
    """
    pass

  def describe_schema_versions(self, ):
    """
    for each schema version present in the cluster, returns a list of nodes at that version.
    hosts that do not respond will be under the key DatabaseDescriptor.INITIAL_VERSION.
    the cluster is all on the same version if the size of the map is 1.
    """
    pass

  def describe_keyspaces(self, ):
    """
    list the defined keyspaces in this cluster
    """
    pass

  def describe_cluster_name(self, ):
    """
    get the cluster name
    """
    pass

  def describe_version(self, ):
    """
    get the thrift api version
    """
    pass

  def describe_ring(self, keyspace):
    """
    get the token ring: a map of ranges to host addresses,
    represented as a set of TokenRange instead of a map from range
    to list of endpoints, because you can't use Thrift structs as
    map keys:
    https://issues.apache.org/jira/browse/THRIFT-162

    for the same reason, we can't return a set here, even though
    order is neither important nor predictable.

    Parameters:
     - keyspace
    """
    pass

  def describe_partitioner(self, ):
    """
    returns the partitioner used by this cluster
    """
    pass

  def describe_snitch(self, ):
    """
    returns the snitch used by this cluster
    """
    pass

  def describe_keyspace(self, keyspace):
    """
    describe specified keyspace

    Parameters:
     - keyspace
    """
    pass

  def describe_splits(self, cfName, start_token, end_token, keys_per_split):
    """
    experimental API for hadoop/parallel query support.
    may change violently and without warning.

    returns list of token strings such that first subrange is (list[0], list[1]],
    next is (list[1], list[2]], etc.

    Parameters:
     - cfName
     - start_token
     - end_token
     - keys_per_split
    """
    pass

  def system_add_column_family(self, cf_def):
    """
    adds a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    pass

  def system_drop_column_family(self, column_family):
    """
    drops a column family. returns the new schema id.

    Parameters:
     - column_family
    """
    pass

  def system_add_keyspace(self, ks_def):
    """
    adds a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - ks_def
    """
    pass

  def system_drop_keyspace(self, keyspace):
    """
    drops a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - keyspace
    """
    pass

  def system_update_keyspace(self, ks_def):
    """
    updates properties of a keyspace. returns the new schema id.

    Parameters:
     - ks_def
    """
    pass

  def system_update_column_family(self, cf_def):
    """
    updates properties of a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    pass

  def execute_cql_query(self, query, compression):
    """
    Executes a CQL (Cassandra Query Language) statement and returns a
    CqlResult containing the results.

    Parameters:
     - query
     - compression
    """
    pass

  def prepare_cql_query(self, query, compression):
    """
    Prepare a CQL (Cassandra Query Language) statement by compiling and returning
    - the type of CQL statement
    - an id token of the compiled CQL stored on the server side.
    - a count of the discovered bound markers in the statement

    Parameters:
     - query
     - compression
    """
    pass

  def execute_prepared_cql_query(self, itemId, values):
    """
    Executes a prepared CQL (Cassandra Query Language) statement by passing an id token and  a list of variables
    to bind and returns a CqlResult containing the results.

    Parameters:
     - itemId
     - values
    """
    pass

  def set_cql_version(self, version):
    """
    Parameters:
     - version
    """
    pass


class Client(Iface):
  def __init__(self, iprot, oprot=None):
    self._iprot = self._oprot = iprot
    if oprot is not None:
      self._oprot = oprot
    self._seqid = 0

  def login(self, auth_request):
    """
    Parameters:
     - auth_request
    """
    self.send_login(auth_request)
    self.recv_login()

  def send_login(self, auth_request):
    self._oprot.writeMessageBegin('login', TMessageType.CALL, self._seqid)
    args = login_args()
    args.auth_request = auth_request
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_login(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = login_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.authnx is not None:
      raise result.authnx
    if result.authzx is not None:
      raise result.authzx
    return

  def set_keyspace(self, keyspace):
    """
    Parameters:
     - keyspace
    """
    self.send_set_keyspace(keyspace)
    self.recv_set_keyspace()

  def send_set_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('set_keyspace', TMessageType.CALL, self._seqid)
    args = set_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_set_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = set_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    return

  def get(self, key, column_path, consistency_level):
    """
    Get the Column or SuperColumn at the given column_path. If no value is present, NotFoundException is thrown. (This is
    the only method that can throw an exception under non-failure conditions.)

    Parameters:
     - key
     - column_path
     - consistency_level
    """
    self.send_get(key, column_path, consistency_level)
    return self.recv_get()

  def send_get(self, key, column_path, consistency_level):
    self._oprot.writeMessageBegin('get', TMessageType.CALL, self._seqid)
    args = get_args()
    args.key = key
    args.column_path = column_path
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.nfe is not None:
      raise result.nfe
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get failed: unknown result");

  def get_slice(self, key, column_parent, predicate, consistency_level):
    """
    Get the group of columns contained by column_parent (either a ColumnFamily name or a ColumnFamily/SuperColumn name
    pair) specified by the given SlicePredicate. If no matching values are found, an empty list is returned.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_get_slice(key, column_parent, predicate, consistency_level)
    return self.recv_get_slice()

  def send_get_slice(self, key, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('get_slice', TMessageType.CALL, self._seqid)
    args = get_slice_args()
    args.key = key
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_slice(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_slice_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_slice failed: unknown result");

  def get_count(self, key, column_parent, predicate, consistency_level):
    """
    returns the number of columns matching <code>predicate</code> for a particular <code>key</code>,
    <code>ColumnFamily</code> and optionally <code>SuperColumn</code>.

    Parameters:
     - key
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_get_count(key, column_parent, predicate, consistency_level)
    return self.recv_get_count()

  def send_get_count(self, key, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('get_count', TMessageType.CALL, self._seqid)
    args = get_count_args()
    args.key = key
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_count(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_count_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_count failed: unknown result");

  def multiget_slice(self, keys, column_parent, predicate, consistency_level):
    """
    Performs a get_slice for column_parent and predicate for the given keys in parallel.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_multiget_slice(keys, column_parent, predicate, consistency_level)
    return self.recv_multiget_slice()

  def send_multiget_slice(self, keys, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('multiget_slice', TMessageType.CALL, self._seqid)
    args = multiget_slice_args()
    args.keys = keys
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_multiget_slice(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = multiget_slice_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "multiget_slice failed: unknown result");

  def multiget_count(self, keys, column_parent, predicate, consistency_level):
    """
    Perform a get_count in parallel on the given list<binary> keys. The return value maps keys to the count found.

    Parameters:
     - keys
     - column_parent
     - predicate
     - consistency_level
    """
    self.send_multiget_count(keys, column_parent, predicate, consistency_level)
    return self.recv_multiget_count()

  def send_multiget_count(self, keys, column_parent, predicate, consistency_level):
    self._oprot.writeMessageBegin('multiget_count', TMessageType.CALL, self._seqid)
    args = multiget_count_args()
    args.keys = keys
    args.column_parent = column_parent
    args.predicate = predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_multiget_count(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = multiget_count_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "multiget_count failed: unknown result");

  def get_range_slices(self, column_parent, predicate, range, consistency_level):
    """
    returns a subset of columns for a contiguous range of keys.

    Parameters:
     - column_parent
     - predicate
     - range
     - consistency_level
    """
    self.send_get_range_slices(column_parent, predicate, range, consistency_level)
    return self.recv_get_range_slices()

  def send_get_range_slices(self, column_parent, predicate, range, consistency_level):
    self._oprot.writeMessageBegin('get_range_slices', TMessageType.CALL, self._seqid)
    args = get_range_slices_args()
    args.column_parent = column_parent
    args.predicate = predicate
    args.range = range
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_range_slices(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_range_slices_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_range_slices failed: unknown result");

  def get_paged_slice(self, column_family, range, start_column, consistency_level):
    """
    returns a range of columns, wrapping to the next rows if necessary to collect max_results.

    Parameters:
     - column_family
     - range
     - start_column
     - consistency_level
    """
    self.send_get_paged_slice(column_family, range, start_column, consistency_level)
    return self.recv_get_paged_slice()

  def send_get_paged_slice(self, column_family, range, start_column, consistency_level):
    self._oprot.writeMessageBegin('get_paged_slice', TMessageType.CALL, self._seqid)
    args = get_paged_slice_args()
    args.column_family = column_family
    args.range = range
    args.start_column = start_column
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_paged_slice(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_paged_slice_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_paged_slice failed: unknown result");

  def get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    """
    Returns the subset of columns specified in SlicePredicate for the rows matching the IndexClause
    @Deprecated; use get_range_slices instead with range.row_filter specified

    Parameters:
     - column_parent
     - index_clause
     - column_predicate
     - consistency_level
    """
    self.send_get_indexed_slices(column_parent, index_clause, column_predicate, consistency_level)
    return self.recv_get_indexed_slices()

  def send_get_indexed_slices(self, column_parent, index_clause, column_predicate, consistency_level):
    self._oprot.writeMessageBegin('get_indexed_slices', TMessageType.CALL, self._seqid)
    args = get_indexed_slices_args()
    args.column_parent = column_parent
    args.index_clause = index_clause
    args.column_predicate = column_predicate
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_get_indexed_slices(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = get_indexed_slices_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    raise TApplicationException(TApplicationException.MISSING_RESULT, "get_indexed_slices failed: unknown result");

  def insert(self, key, column_parent, column, consistency_level):
    """
    Insert a Column at the given column_parent.column_family and optional column_parent.super_column.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    self.send_insert(key, column_parent, column, consistency_level)
    self.recv_insert()

  def send_insert(self, key, column_parent, column, consistency_level):
    self._oprot.writeMessageBegin('insert', TMessageType.CALL, self._seqid)
    args = insert_args()
    args.key = key
    args.column_parent = column_parent
    args.column = column
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_insert(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = insert_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def add(self, key, column_parent, column, consistency_level):
    """
    Increment or decrement a counter.

    Parameters:
     - key
     - column_parent
     - column
     - consistency_level
    """
    self.send_add(key, column_parent, column, consistency_level)
    self.recv_add()

  def send_add(self, key, column_parent, column, consistency_level):
    self._oprot.writeMessageBegin('add', TMessageType.CALL, self._seqid)
    args = add_args()
    args.key = key
    args.column_parent = column_parent
    args.column = column
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_add(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = add_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def remove(self, key, column_path, timestamp, consistency_level):
    """
    Remove data from the row specified by key at the granularity specified by column_path, and the given timestamp. Note
    that all the values in column_path besides column_path.column_family are truly optional: you can remove the entire
    row by just specifying the ColumnFamily, or you can remove a SuperColumn or a single Column by specifying those levels too.

    Parameters:
     - key
     - column_path
     - timestamp
     - consistency_level
    """
    self.send_remove(key, column_path, timestamp, consistency_level)
    self.recv_remove()

  def send_remove(self, key, column_path, timestamp, consistency_level):
    self._oprot.writeMessageBegin('remove', TMessageType.CALL, self._seqid)
    args = remove_args()
    args.key = key
    args.column_path = column_path
    args.timestamp = timestamp
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_remove(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = remove_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def remove_counter(self, key, path, consistency_level):
    """
    Remove a counter at the specified location.
    Note that counters have limited support for deletes: if you remove a counter, you must wait to issue any following update
    until the delete has reached all the nodes and all of them have been fully compacted.

    Parameters:
     - key
     - path
     - consistency_level
    """
    self.send_remove_counter(key, path, consistency_level)
    self.recv_remove_counter()

  def send_remove_counter(self, key, path, consistency_level):
    self._oprot.writeMessageBegin('remove_counter', TMessageType.CALL, self._seqid)
    args = remove_counter_args()
    args.key = key
    args.path = path
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_remove_counter(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = remove_counter_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def batch_mutate(self, mutation_map, consistency_level):
    """
      Mutate many columns or super columns for many row keys. See also: Mutation.

      mutation_map maps key to column family to a list of Mutation objects to take place at that scope.
    *

    Parameters:
     - mutation_map
     - consistency_level
    """
    self.send_batch_mutate(mutation_map, consistency_level)
    self.recv_batch_mutate()

  def send_batch_mutate(self, mutation_map, consistency_level):
    self._oprot.writeMessageBegin('batch_mutate', TMessageType.CALL, self._seqid)
    args = batch_mutate_args()
    args.mutation_map = mutation_map
    args.consistency_level = consistency_level
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_batch_mutate(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = batch_mutate_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def truncate(self, cfname):
    """
    Truncate will mark and entire column family as deleted.
    From the user's perspective a successful call to truncate will result complete data deletion from cfname.
    Internally, however, disk space will not be immediatily released, as with all deletes in cassandra, this one
    only marks the data as deleted.
    The operation succeeds only if all hosts in the cluster at available and will throw an UnavailableException if
    some hosts are down.

    Parameters:
     - cfname
    """
    self.send_truncate(cfname)
    self.recv_truncate()

  def send_truncate(self, cfname):
    self._oprot.writeMessageBegin('truncate', TMessageType.CALL, self._seqid)
    args = truncate_args()
    args.cfname = cfname
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_truncate(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = truncate_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    return

  def describe_schema_versions(self, ):
    """
    for each schema version present in the cluster, returns a list of nodes at that version.
    hosts that do not respond will be under the key DatabaseDescriptor.INITIAL_VERSION.
    the cluster is all on the same version if the size of the map is 1.
    """
    self.send_describe_schema_versions()
    return self.recv_describe_schema_versions()

  def send_describe_schema_versions(self, ):
    self._oprot.writeMessageBegin('describe_schema_versions', TMessageType.CALL, self._seqid)
    args = describe_schema_versions_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_schema_versions(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_schema_versions_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_schema_versions failed: unknown result");

  def describe_keyspaces(self, ):
    """
    list the defined keyspaces in this cluster
    """
    self.send_describe_keyspaces()
    return self.recv_describe_keyspaces()

  def send_describe_keyspaces(self, ):
    self._oprot.writeMessageBegin('describe_keyspaces', TMessageType.CALL, self._seqid)
    args = describe_keyspaces_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_keyspaces(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_keyspaces_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_keyspaces failed: unknown result");

  def describe_cluster_name(self, ):
    """
    get the cluster name
    """
    self.send_describe_cluster_name()
    return self.recv_describe_cluster_name()

  def send_describe_cluster_name(self, ):
    self._oprot.writeMessageBegin('describe_cluster_name', TMessageType.CALL, self._seqid)
    args = describe_cluster_name_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_cluster_name(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_cluster_name_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_cluster_name failed: unknown result");

  def describe_version(self, ):
    """
    get the thrift api version
    """
    self.send_describe_version()
    return self.recv_describe_version()

  def send_describe_version(self, ):
    self._oprot.writeMessageBegin('describe_version', TMessageType.CALL, self._seqid)
    args = describe_version_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_version(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_version_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_version failed: unknown result");

  def describe_ring(self, keyspace):
    """
    get the token ring: a map of ranges to host addresses,
    represented as a set of TokenRange instead of a map from range
    to list of endpoints, because you can't use Thrift structs as
    map keys:
    https://issues.apache.org/jira/browse/THRIFT-162

    for the same reason, we can't return a set here, even though
    order is neither important nor predictable.

    Parameters:
     - keyspace
    """
    self.send_describe_ring(keyspace)
    return self.recv_describe_ring()

  def send_describe_ring(self, keyspace):
    self._oprot.writeMessageBegin('describe_ring', TMessageType.CALL, self._seqid)
    args = describe_ring_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_ring(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_ring_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_ring failed: unknown result");

  def describe_partitioner(self, ):
    """
    returns the partitioner used by this cluster
    """
    self.send_describe_partitioner()
    return self.recv_describe_partitioner()

  def send_describe_partitioner(self, ):
    self._oprot.writeMessageBegin('describe_partitioner', TMessageType.CALL, self._seqid)
    args = describe_partitioner_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_partitioner(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_partitioner_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_partitioner failed: unknown result");

  def describe_snitch(self, ):
    """
    returns the snitch used by this cluster
    """
    self.send_describe_snitch()
    return self.recv_describe_snitch()

  def send_describe_snitch(self, ):
    self._oprot.writeMessageBegin('describe_snitch', TMessageType.CALL, self._seqid)
    args = describe_snitch_args()
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_snitch(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_snitch_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_snitch failed: unknown result");

  def describe_keyspace(self, keyspace):
    """
    describe specified keyspace

    Parameters:
     - keyspace
    """
    self.send_describe_keyspace(keyspace)
    return self.recv_describe_keyspace()

  def send_describe_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('describe_keyspace', TMessageType.CALL, self._seqid)
    args = describe_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.nfe is not None:
      raise result.nfe
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_keyspace failed: unknown result");

  def describe_splits(self, cfName, start_token, end_token, keys_per_split):
    """
    experimental API for hadoop/parallel query support.
    may change violently and without warning.

    returns list of token strings such that first subrange is (list[0], list[1]],
    next is (list[1], list[2]], etc.

    Parameters:
     - cfName
     - start_token
     - end_token
     - keys_per_split
    """
    self.send_describe_splits(cfName, start_token, end_token, keys_per_split)
    return self.recv_describe_splits()

  def send_describe_splits(self, cfName, start_token, end_token, keys_per_split):
    self._oprot.writeMessageBegin('describe_splits', TMessageType.CALL, self._seqid)
    args = describe_splits_args()
    args.cfName = cfName
    args.start_token = start_token
    args.end_token = end_token
    args.keys_per_split = keys_per_split
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_describe_splits(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = describe_splits_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "describe_splits failed: unknown result");

  def system_add_column_family(self, cf_def):
    """
    adds a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    self.send_system_add_column_family(cf_def)
    return self.recv_system_add_column_family()

  def send_system_add_column_family(self, cf_def):
    self._oprot.writeMessageBegin('system_add_column_family', TMessageType.CALL, self._seqid)
    args = system_add_column_family_args()
    args.cf_def = cf_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_add_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_add_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_add_column_family failed: unknown result");

  def system_drop_column_family(self, column_family):
    """
    drops a column family. returns the new schema id.

    Parameters:
     - column_family
    """
    self.send_system_drop_column_family(column_family)
    return self.recv_system_drop_column_family()

  def send_system_drop_column_family(self, column_family):
    self._oprot.writeMessageBegin('system_drop_column_family', TMessageType.CALL, self._seqid)
    args = system_drop_column_family_args()
    args.column_family = column_family
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_drop_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_drop_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_drop_column_family failed: unknown result");

  def system_add_keyspace(self, ks_def):
    """
    adds a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - ks_def
    """
    self.send_system_add_keyspace(ks_def)
    return self.recv_system_add_keyspace()

  def send_system_add_keyspace(self, ks_def):
    self._oprot.writeMessageBegin('system_add_keyspace', TMessageType.CALL, self._seqid)
    args = system_add_keyspace_args()
    args.ks_def = ks_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_add_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_add_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_add_keyspace failed: unknown result");

  def system_drop_keyspace(self, keyspace):
    """
    drops a keyspace and any column families that are part of it. returns the new schema id.

    Parameters:
     - keyspace
    """
    self.send_system_drop_keyspace(keyspace)
    return self.recv_system_drop_keyspace()

  def send_system_drop_keyspace(self, keyspace):
    self._oprot.writeMessageBegin('system_drop_keyspace', TMessageType.CALL, self._seqid)
    args = system_drop_keyspace_args()
    args.keyspace = keyspace
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_drop_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_drop_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_drop_keyspace failed: unknown result");

  def system_update_keyspace(self, ks_def):
    """
    updates properties of a keyspace. returns the new schema id.

    Parameters:
     - ks_def
    """
    self.send_system_update_keyspace(ks_def)
    return self.recv_system_update_keyspace()

  def send_system_update_keyspace(self, ks_def):
    self._oprot.writeMessageBegin('system_update_keyspace', TMessageType.CALL, self._seqid)
    args = system_update_keyspace_args()
    args.ks_def = ks_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_update_keyspace(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_update_keyspace_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_update_keyspace failed: unknown result");

  def system_update_column_family(self, cf_def):
    """
    updates properties of a column family. returns the new schema id.

    Parameters:
     - cf_def
    """
    self.send_system_update_column_family(cf_def)
    return self.recv_system_update_column_family()

  def send_system_update_column_family(self, cf_def):
    self._oprot.writeMessageBegin('system_update_column_family', TMessageType.CALL, self._seqid)
    args = system_update_column_family_args()
    args.cf_def = cf_def
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_system_update_column_family(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = system_update_column_family_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "system_update_column_family failed: unknown result");

  def execute_cql_query(self, query, compression):
    """
    Executes a CQL (Cassandra Query Language) statement and returns a
    CqlResult containing the results.

    Parameters:
     - query
     - compression
    """
    self.send_execute_cql_query(query, compression)
    return self.recv_execute_cql_query()

  def send_execute_cql_query(self, query, compression):
    self._oprot.writeMessageBegin('execute_cql_query', TMessageType.CALL, self._seqid)
    args = execute_cql_query_args()
    args.query = query
    args.compression = compression
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_execute_cql_query(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = execute_cql_query_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "execute_cql_query failed: unknown result");

  def prepare_cql_query(self, query, compression):
    """
    Prepare a CQL (Cassandra Query Language) statement by compiling and returning
    - the type of CQL statement
    - an id token of the compiled CQL stored on the server side.
    - a count of the discovered bound markers in the statement

    Parameters:
     - query
     - compression
    """
    self.send_prepare_cql_query(query, compression)
    return self.recv_prepare_cql_query()

  def send_prepare_cql_query(self, query, compression):
    self._oprot.writeMessageBegin('prepare_cql_query', TMessageType.CALL, self._seqid)
    args = prepare_cql_query_args()
    args.query = query
    args.compression = compression
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_prepare_cql_query(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = prepare_cql_query_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    raise TApplicationException(TApplicationException.MISSING_RESULT, "prepare_cql_query failed: unknown result");

  def execute_prepared_cql_query(self, itemId, values):
    """
    Executes a prepared CQL (Cassandra Query Language) statement by passing an id token and  a list of variables
    to bind and returns a CqlResult containing the results.

    Parameters:
     - itemId
     - values
    """
    self.send_execute_prepared_cql_query(itemId, values)
    return self.recv_execute_prepared_cql_query()

  def send_execute_prepared_cql_query(self, itemId, values):
    self._oprot.writeMessageBegin('execute_prepared_cql_query', TMessageType.CALL, self._seqid)
    args = execute_prepared_cql_query_args()
    args.itemId = itemId
    args.values = values
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_execute_prepared_cql_query(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = execute_prepared_cql_query_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.success is not None:
      return result.success
    if result.ire is not None:
      raise result.ire
    if result.ue is not None:
      raise result.ue
    if result.te is not None:
      raise result.te
    if result.sde is not None:
      raise result.sde
    raise TApplicationException(TApplicationException.MISSING_RESULT, "execute_prepared_cql_query failed: unknown result");

  def set_cql_version(self, version):
    """
    Parameters:
     - version
    """
    self.send_set_cql_version(version)
    self.recv_set_cql_version()

  def send_set_cql_version(self, version):
    self._oprot.writeMessageBegin('set_cql_version', TMessageType.CALL, self._seqid)
    args = set_cql_version_args()
    args.version = version
    args.write(self._oprot)
    self._oprot.writeMessageEnd()
    self._oprot.trans.flush()

  def recv_set_cql_version(self, ):
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
    if mtype == TMessageType.EXCEPTION:
      x = TApplicationException()
      x.read(self._iprot)
      self._iprot.readMessageEnd()
      raise x
    result = set_cql_version_result()
    result.read(self._iprot)
    self._iprot.readMessageEnd()
    if result.ire is not None:
      raise result.ire
    return


class Processor(Iface, TProcessor):
  def __init__(self, handler):
    self._handler = handler
    self._processMap = {}
    self._processMap["login"] = Processor.process_login
    self._processMap["set_keyspace"] = Processor.process_set_keyspace
    self._processMap["get"] = Processor.process_get
    self._processMap["get_slice"] = Processor.process_get_slice
    self._processMap["get_count"] = Processor.process_get_count
    self._processMap["multiget_slice"] = Processor.process_multiget_slice
    self._processMap["multiget_count"] = Processor.process_multiget_count
    self._processMap["get_range_slices"] = Processor.process_get_range_slices
    self._processMap["get_paged_slice"] = Processor.process_get_paged_slice
    self._processMap["get_indexed_slices"] = Processor.process_get_indexed_slices
    self._processMap["insert"] = Processor.process_insert
    self._processMap["add"] = Processor.process_add
    self._processMap["remove"] = Processor.process_remove
    self._processMap["remove_counter"] = Processor.process_remove_counter
    self._processMap["batch_mutate"] = Processor.process_batch_mutate
    self._processMap["truncate"] = Processor.process_truncate
    self._processMap["describe_schema_versions"] = Processor.process_describe_schema_versions
    self._processMap["describe_keyspaces"] = Processor.process_describe_keyspaces
    self._processMap["describe_cluster_name"] = Processor.process_describe_cluster_name
    self._processMap["describe_version"] = Processor.process_describe_version
    self._processMap["describe_ring"] = Processor.process_describe_ring
    self._processMap["describe_partitioner"] = Processor.process_describe_partitioner
    self._processMap["describe_snitch"] = Processor.process_describe_snitch
    self._processMap["describe_keyspace"] = Processor.process_describe_keyspace
    self._processMap["describe_splits"] = Processor.process_describe_splits
    self._processMap["system_add_column_family"] = Processor.process_system_add_column_family
    self._processMap["system_drop_column_family"] = Processor.process_system_drop_column_family
    self._processMap["system_add_keyspace"] = Processor.process_system_add_keyspace
    self._processMap["system_drop_keyspace"] = Processor.process_system_drop_keyspace
    self._processMap["system_update_keyspace"] = Processor.process_system_update_keyspace
    self._processMap["system_update_column_family"] = Processor.process_system_update_column_family
    self._processMap["execute_cql_query"] = Processor.process_execute_cql_query
    self._processMap["prepare_cql_query"] = Processor.process_prepare_cql_query
    self._processMap["execute_prepared_cql_query"] = Processor.process_execute_prepared_cql_query
    self._processMap["set_cql_version"] = Processor.process_set_cql_version

  def process(self, iprot, oprot):
    (name, type, seqid) = iprot.readMessageBegin()
    if name not in self._processMap:
      iprot.skip(TType.STRUCT)
      iprot.readMessageEnd()
      x = TApplicationException(TApplicationException.UNKNOWN_METHOD, 'Unknown function %s' % (name))
      oprot.writeMessageBegin(name, TMessageType.EXCEPTION, seqid)
      x.write(oprot)
      oprot.writeMessageEnd()
      oprot.trans.flush()
      return
    else:
      self._processMap[name](self, seqid, iprot, oprot)
    return True

  def process_login(self, seqid, iprot, oprot):
    args = login_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = login_result()
    try:
      self._handler.login(args.auth_request)
    except AuthenticationException, authnx:
      result.authnx = authnx
    except AuthorizationException, authzx:
      result.authzx = authzx
    oprot.writeMessageBegin("login", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_set_keyspace(self, seqid, iprot, oprot):
    args = set_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = set_keyspace_result()
    try:
      self._handler.set_keyspace(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("set_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get(self, seqid, iprot, oprot):
    args = get_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_result()
    try:
      result.success = self._handler.get(args.key, args.column_path, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except NotFoundException, nfe:
      result.nfe = nfe
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_slice(self, seqid, iprot, oprot):
    args = get_slice_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_slice_result()
    try:
      result.success = self._handler.get_slice(args.key, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_slice", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_count(self, seqid, iprot, oprot):
    args = get_count_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_count_result()
    try:
      result.success = self._handler.get_count(args.key, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_count", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_multiget_slice(self, seqid, iprot, oprot):
    args = multiget_slice_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = multiget_slice_result()
    try:
      result.success = self._handler.multiget_slice(args.keys, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("multiget_slice", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_multiget_count(self, seqid, iprot, oprot):
    args = multiget_count_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = multiget_count_result()
    try:
      result.success = self._handler.multiget_count(args.keys, args.column_parent, args.predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("multiget_count", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_range_slices(self, seqid, iprot, oprot):
    args = get_range_slices_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_range_slices_result()
    try:
      result.success = self._handler.get_range_slices(args.column_parent, args.predicate, args.range, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_range_slices", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_paged_slice(self, seqid, iprot, oprot):
    args = get_paged_slice_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_paged_slice_result()
    try:
      result.success = self._handler.get_paged_slice(args.column_family, args.range, args.start_column, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_paged_slice", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_get_indexed_slices(self, seqid, iprot, oprot):
    args = get_indexed_slices_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = get_indexed_slices_result()
    try:
      result.success = self._handler.get_indexed_slices(args.column_parent, args.index_clause, args.column_predicate, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("get_indexed_slices", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_insert(self, seqid, iprot, oprot):
    args = insert_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = insert_result()
    try:
      self._handler.insert(args.key, args.column_parent, args.column, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("insert", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_add(self, seqid, iprot, oprot):
    args = add_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = add_result()
    try:
      self._handler.add(args.key, args.column_parent, args.column, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("add", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_remove(self, seqid, iprot, oprot):
    args = remove_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = remove_result()
    try:
      self._handler.remove(args.key, args.column_path, args.timestamp, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("remove", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_remove_counter(self, seqid, iprot, oprot):
    args = remove_counter_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = remove_counter_result()
    try:
      self._handler.remove_counter(args.key, args.path, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("remove_counter", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_batch_mutate(self, seqid, iprot, oprot):
    args = batch_mutate_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = batch_mutate_result()
    try:
      self._handler.batch_mutate(args.mutation_map, args.consistency_level)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("batch_mutate", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_truncate(self, seqid, iprot, oprot):
    args = truncate_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = truncate_result()
    try:
      self._handler.truncate(args.cfname)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    oprot.writeMessageBegin("truncate", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_schema_versions(self, seqid, iprot, oprot):
    args = describe_schema_versions_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_schema_versions_result()
    try:
      result.success = self._handler.describe_schema_versions()
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_schema_versions", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_keyspaces(self, seqid, iprot, oprot):
    args = describe_keyspaces_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_keyspaces_result()
    try:
      result.success = self._handler.describe_keyspaces()
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_keyspaces", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_cluster_name(self, seqid, iprot, oprot):
    args = describe_cluster_name_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_cluster_name_result()
    result.success = self._handler.describe_cluster_name()
    oprot.writeMessageBegin("describe_cluster_name", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_version(self, seqid, iprot, oprot):
    args = describe_version_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_version_result()
    result.success = self._handler.describe_version()
    oprot.writeMessageBegin("describe_version", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_ring(self, seqid, iprot, oprot):
    args = describe_ring_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_ring_result()
    try:
      result.success = self._handler.describe_ring(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_ring", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_partitioner(self, seqid, iprot, oprot):
    args = describe_partitioner_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_partitioner_result()
    result.success = self._handler.describe_partitioner()
    oprot.writeMessageBegin("describe_partitioner", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_snitch(self, seqid, iprot, oprot):
    args = describe_snitch_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_snitch_result()
    result.success = self._handler.describe_snitch()
    oprot.writeMessageBegin("describe_snitch", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_keyspace(self, seqid, iprot, oprot):
    args = describe_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_keyspace_result()
    try:
      result.success = self._handler.describe_keyspace(args.keyspace)
    except NotFoundException, nfe:
      result.nfe = nfe
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_describe_splits(self, seqid, iprot, oprot):
    args = describe_splits_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = describe_splits_result()
    try:
      result.success = self._handler.describe_splits(args.cfName, args.start_token, args.end_token, args.keys_per_split)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("describe_splits", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_add_column_family(self, seqid, iprot, oprot):
    args = system_add_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_add_column_family_result()
    try:
      result.success = self._handler.system_add_column_family(args.cf_def)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_add_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_drop_column_family(self, seqid, iprot, oprot):
    args = system_drop_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_drop_column_family_result()
    try:
      result.success = self._handler.system_drop_column_family(args.column_family)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_drop_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_add_keyspace(self, seqid, iprot, oprot):
    args = system_add_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_add_keyspace_result()
    try:
      result.success = self._handler.system_add_keyspace(args.ks_def)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_add_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_drop_keyspace(self, seqid, iprot, oprot):
    args = system_drop_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_drop_keyspace_result()
    try:
      result.success = self._handler.system_drop_keyspace(args.keyspace)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_drop_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_update_keyspace(self, seqid, iprot, oprot):
    args = system_update_keyspace_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_update_keyspace_result()
    try:
      result.success = self._handler.system_update_keyspace(args.ks_def)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_update_keyspace", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_system_update_column_family(self, seqid, iprot, oprot):
    args = system_update_column_family_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = system_update_column_family_result()
    try:
      result.success = self._handler.system_update_column_family(args.cf_def)
    except InvalidRequestException, ire:
      result.ire = ire
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("system_update_column_family", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_execute_cql_query(self, seqid, iprot, oprot):
    args = execute_cql_query_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = execute_cql_query_result()
    try:
      result.success = self._handler.execute_cql_query(args.query, args.compression)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("execute_cql_query", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_prepare_cql_query(self, seqid, iprot, oprot):
    args = prepare_cql_query_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = prepare_cql_query_result()
    try:
      result.success = self._handler.prepare_cql_query(args.query, args.compression)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("prepare_cql_query", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_execute_prepared_cql_query(self, seqid, iprot, oprot):
    args = execute_prepared_cql_query_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = execute_prepared_cql_query_result()
    try:
      result.success = self._handler.execute_prepared_cql_query(args.itemId, args.values)
    except InvalidRequestException, ire:
      result.ire = ire
    except UnavailableException, ue:
      result.ue = ue
    except TimedOutException, te:
      result.te = te
    except SchemaDisagreementException, sde:
      result.sde = sde
    oprot.writeMessageBegin("execute_prepared_cql_query", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()

  def process_set_cql_version(self, seqid, iprot, oprot):
    args = set_cql_version_args()
    args.read(iprot)
    iprot.readMessageEnd()
    result = set_cql_version_result()
    try:
      self._handler.set_cql_version(args.version)
    except InvalidRequestException, ire:
      result.ire = ire
    oprot.writeMessageBegin("set_cql_version", TMessageType.REPLY, seqid)
    result.write(oprot)
    oprot.writeMessageEnd()
    oprot.trans.flush()


# HELPER FUNCTIONS AND STRUCTURES

class login_args:
  """
  Attributes:
   - auth_request
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'auth_request', (AuthenticationRequest, AuthenticationRequest.thrift_spec), None, ), # 1
  )

  def __init__(self, auth_request=None,):
    self.auth_request = auth_request

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.auth_request = AuthenticationRequest()
          self.auth_request.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('login_args')
    if self.auth_request is not None:
      oprot.writeFieldBegin('auth_request', TType.STRUCT, 1)
      self.auth_request.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.auth_request is None:
      raise TProtocol.TProtocolException(message='Required field auth_request is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class login_result:
  """
  Attributes:
   - authnx
   - authzx
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'authnx', (AuthenticationException, AuthenticationException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'authzx', (AuthorizationException, AuthorizationException.thrift_spec), None, ), # 2
  )

  def __init__(self, authnx=None, authzx=None,):
    self.authnx = authnx
    self.authzx = authzx

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.authnx = AuthenticationException()
          self.authnx.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.authzx = AuthorizationException()
          self.authzx.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('login_result')
    if self.authnx is not None:
      oprot.writeFieldBegin('authnx', TType.STRUCT, 1)
      self.authnx.write(oprot)
      oprot.writeFieldEnd()
    if self.authzx is not None:
      oprot.writeFieldBegin('authzx', TType.STRUCT, 2)
      self.authzx.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_keyspace_result:
  """
  Attributes:
   - ire
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, ire=None,):
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_keyspace_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_args:
  """
  Attributes:
   - key
   - column_path
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_path', (ColumnPath, ColumnPath.thrift_spec), None, ), # 2
    (3, TType.I32, 'consistency_level', None,     1, ), # 3
  )

  def __init__(self, key=None, column_path=None, consistency_level=thrift_spec[3][4],):
    self.key = key
    self.column_path = column_path
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_path = ColumnPath()
          self.column_path.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_path is not None:
      oprot.writeFieldBegin('column_path', TType.STRUCT, 2)
      self.column_path.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 3)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_path is None:
      raise TProtocol.TProtocolException(message='Required field column_path is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_result:
  """
  Attributes:
   - success
   - ire
   - nfe
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'nfe', (NotFoundException, NotFoundException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 3
    (4, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 4
  )

  def __init__(self, success=None, ire=None, nfe=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.nfe = nfe
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = ColumnOrSuperColumn()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.nfe = NotFoundException()
          self.nfe.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.nfe is not None:
      oprot.writeFieldBegin('nfe', TType.STRUCT, 2)
      self.nfe.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 3)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 4)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_slice_args:
  """
  Attributes:
   - key
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_slice_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_slice_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype171, _size168) = iprot.readListBegin()
          for _i172 in xrange(_size168):
            _elem173 = ColumnOrSuperColumn()
            _elem173.read(iprot)
            self.success.append(_elem173)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_slice_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter174 in self.success:
        iter174.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_count_args:
  """
  Attributes:
   - key
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_count_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_count_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.I32, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.I32:
          self.success = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_count_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.I32, 0)
      oprot.writeI32(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_slice_args:
  """
  Attributes:
   - keys
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'keys', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, keys=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.keys = keys
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.keys = []
          (_etype178, _size175) = iprot.readListBegin()
          for _i179 in xrange(_size175):
            _elem180 = iprot.readString();
            self.keys.append(_elem180)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_slice_args')
    if self.keys is not None:
      oprot.writeFieldBegin('keys', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.keys))
      for iter181 in self.keys:
        oprot.writeString(iter181)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keys is None:
      raise TProtocol.TProtocolException(message='Required field keys is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_slice_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.LIST,(TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec))), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype183, _vtype184, _size182 ) = iprot.readMapBegin() 
          for _i186 in xrange(_size182):
            _key187 = iprot.readString();
            _val188 = []
            (_etype192, _size189) = iprot.readListBegin()
            for _i193 in xrange(_size189):
              _elem194 = ColumnOrSuperColumn()
              _elem194.read(iprot)
              _val188.append(_elem194)
            iprot.readListEnd()
            self.success[_key187] = _val188
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_slice_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.LIST, len(self.success))
      for kiter195,viter196 in self.success.items():
        oprot.writeString(kiter195)
        oprot.writeListBegin(TType.STRUCT, len(viter196))
        for iter197 in viter196:
          iter197.write(oprot)
        oprot.writeListEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_count_args:
  """
  Attributes:
   - keys
   - column_parent
   - predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'keys', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, keys=None, column_parent=None, predicate=None, consistency_level=thrift_spec[4][4],):
    self.keys = keys
    self.column_parent = column_parent
    self.predicate = predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.keys = []
          (_etype201, _size198) = iprot.readListBegin()
          for _i202 in xrange(_size198):
            _elem203 = iprot.readString();
            self.keys.append(_elem203)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_count_args')
    if self.keys is not None:
      oprot.writeFieldBegin('keys', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.keys))
      for iter204 in self.keys:
        oprot.writeString(iter204)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keys is None:
      raise TProtocol.TProtocolException(message='Required field keys is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class multiget_count_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.I32,None), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype206, _vtype207, _size205 ) = iprot.readMapBegin() 
          for _i209 in xrange(_size205):
            _key210 = iprot.readString();
            _val211 = iprot.readI32();
            self.success[_key210] = _val211
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('multiget_count_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.I32, len(self.success))
      for kiter212,viter213 in self.success.items():
        oprot.writeString(kiter212)
        oprot.writeI32(viter213)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_range_slices_args:
  """
  Attributes:
   - column_parent
   - predicate
   - range
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'range', (KeyRange, KeyRange.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, column_parent=None, predicate=None, range=None, consistency_level=thrift_spec[4][4],):
    self.column_parent = column_parent
    self.predicate = predicate
    self.range = range
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.range = KeyRange()
          self.range.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_range_slices_args')
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 1)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 2)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.range is not None:
      oprot.writeFieldBegin('range', TType.STRUCT, 3)
      self.range.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.predicate is None:
      raise TProtocol.TProtocolException(message='Required field predicate is unset!')
    if self.range is None:
      raise TProtocol.TProtocolException(message='Required field range is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_range_slices_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KeySlice, KeySlice.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype217, _size214) = iprot.readListBegin()
          for _i218 in xrange(_size214):
            _elem219 = KeySlice()
            _elem219.read(iprot)
            self.success.append(_elem219)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_range_slices_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter220 in self.success:
        iter220.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_paged_slice_args:
  """
  Attributes:
   - column_family
   - range
   - start_column
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'column_family', None, None, ), # 1
    (2, TType.STRUCT, 'range', (KeyRange, KeyRange.thrift_spec), None, ), # 2
    (3, TType.STRING, 'start_column', None, None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, column_family=None, range=None, start_column=None, consistency_level=thrift_spec[4][4],):
    self.column_family = column_family
    self.range = range
    self.start_column = start_column
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.range = KeyRange()
          self.range.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.start_column = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_paged_slice_args')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 1)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    if self.range is not None:
      oprot.writeFieldBegin('range', TType.STRUCT, 2)
      self.range.write(oprot)
      oprot.writeFieldEnd()
    if self.start_column is not None:
      oprot.writeFieldBegin('start_column', TType.STRING, 3)
      oprot.writeString(self.start_column)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    if self.range is None:
      raise TProtocol.TProtocolException(message='Required field range is unset!')
    if self.start_column is None:
      raise TProtocol.TProtocolException(message='Required field start_column is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_paged_slice_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KeySlice, KeySlice.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype224, _size221) = iprot.readListBegin()
          for _i225 in xrange(_size221):
            _elem226 = KeySlice()
            _elem226.read(iprot)
            self.success.append(_elem226)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_paged_slice_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter227 in self.success:
        iter227.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_indexed_slices_args:
  """
  Attributes:
   - column_parent
   - index_clause
   - column_predicate
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'index_clause', (IndexClause, IndexClause.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'column_predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, column_parent=None, index_clause=None, column_predicate=None, consistency_level=thrift_spec[4][4],):
    self.column_parent = column_parent
    self.index_clause = index_clause
    self.column_predicate = column_predicate
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.index_clause = IndexClause()
          self.index_clause.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.column_predicate = SlicePredicate()
          self.column_predicate.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_indexed_slices_args')
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 1)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.index_clause is not None:
      oprot.writeFieldBegin('index_clause', TType.STRUCT, 2)
      self.index_clause.write(oprot)
      oprot.writeFieldEnd()
    if self.column_predicate is not None:
      oprot.writeFieldBegin('column_predicate', TType.STRUCT, 3)
      self.column_predicate.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.index_clause is None:
      raise TProtocol.TProtocolException(message='Required field index_clause is unset!')
    if self.column_predicate is None:
      raise TProtocol.TProtocolException(message='Required field column_predicate is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class get_indexed_slices_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KeySlice, KeySlice.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, success=None, ire=None, ue=None, te=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype231, _size228) = iprot.readListBegin()
          for _i232 in xrange(_size228):
            _elem233 = KeySlice()
            _elem233.read(iprot)
            self.success.append(_elem233)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('get_indexed_slices_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter234 in self.success:
        iter234.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class insert_args:
  """
  Attributes:
   - key
   - column_parent
   - column
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'column', (Column, Column.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, column=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.column = column
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.column = Column()
          self.column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('insert_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRUCT, 3)
      self.column.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.column is None:
      raise TProtocol.TProtocolException(message='Required field column is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class insert_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('insert_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class add_args:
  """
  Attributes:
   - key
   - column_parent
   - column
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_parent', (ColumnParent, ColumnParent.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'column', (CounterColumn, CounterColumn.thrift_spec), None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_parent=None, column=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_parent = column_parent
    self.column = column
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_parent = ColumnParent()
          self.column_parent.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.column = CounterColumn()
          self.column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('add_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_parent is not None:
      oprot.writeFieldBegin('column_parent', TType.STRUCT, 2)
      self.column_parent.write(oprot)
      oprot.writeFieldEnd()
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRUCT, 3)
      self.column.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_parent is None:
      raise TProtocol.TProtocolException(message='Required field column_parent is unset!')
    if self.column is None:
      raise TProtocol.TProtocolException(message='Required field column is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class add_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('add_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_args:
  """
  Attributes:
   - key
   - column_path
   - timestamp
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'column_path', (ColumnPath, ColumnPath.thrift_spec), None, ), # 2
    (3, TType.I64, 'timestamp', None, None, ), # 3
    (4, TType.I32, 'consistency_level', None,     1, ), # 4
  )

  def __init__(self, key=None, column_path=None, timestamp=None, consistency_level=thrift_spec[4][4],):
    self.key = key
    self.column_path = column_path
    self.timestamp = timestamp
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.column_path = ColumnPath()
          self.column_path.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.column_path is not None:
      oprot.writeFieldBegin('column_path', TType.STRUCT, 2)
      self.column_path.write(oprot)
      oprot.writeFieldEnd()
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 3)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 4)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.column_path is None:
      raise TProtocol.TProtocolException(message='Required field column_path is unset!')
    if self.timestamp is None:
      raise TProtocol.TProtocolException(message='Required field timestamp is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_counter_args:
  """
  Attributes:
   - key
   - path
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.STRUCT, 'path', (ColumnPath, ColumnPath.thrift_spec), None, ), # 2
    (3, TType.I32, 'consistency_level', None,     1, ), # 3
  )

  def __init__(self, key=None, path=None, consistency_level=thrift_spec[3][4],):
    self.key = key
    self.path = path
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.path = ColumnPath()
          self.path.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_counter_args')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.path is not None:
      oprot.writeFieldBegin('path', TType.STRUCT, 2)
      self.path.write(oprot)
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 3)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.path is None:
      raise TProtocol.TProtocolException(message='Required field path is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class remove_counter_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('remove_counter_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class batch_mutate_args:
  """
  Attributes:
   - mutation_map
   - consistency_level
  """

  thrift_spec = (
    None, # 0
    (1, TType.MAP, 'mutation_map', (TType.STRING,None,TType.MAP,(TType.STRING,None,TType.LIST,(TType.STRUCT,(Mutation, Mutation.thrift_spec)))), None, ), # 1
    (2, TType.I32, 'consistency_level', None,     1, ), # 2
  )

  def __init__(self, mutation_map=None, consistency_level=thrift_spec[2][4],):
    self.mutation_map = mutation_map
    self.consistency_level = consistency_level

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.MAP:
          self.mutation_map = {}
          (_ktype236, _vtype237, _size235 ) = iprot.readMapBegin() 
          for _i239 in xrange(_size235):
            _key240 = iprot.readString();
            _val241 = {}
            (_ktype243, _vtype244, _size242 ) = iprot.readMapBegin() 
            for _i246 in xrange(_size242):
              _key247 = iprot.readString();
              _val248 = []
              (_etype252, _size249) = iprot.readListBegin()
              for _i253 in xrange(_size249):
                _elem254 = Mutation()
                _elem254.read(iprot)
                _val248.append(_elem254)
              iprot.readListEnd()
              _val241[_key247] = _val248
            iprot.readMapEnd()
            self.mutation_map[_key240] = _val241
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.consistency_level = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('batch_mutate_args')
    if self.mutation_map is not None:
      oprot.writeFieldBegin('mutation_map', TType.MAP, 1)
      oprot.writeMapBegin(TType.STRING, TType.MAP, len(self.mutation_map))
      for kiter255,viter256 in self.mutation_map.items():
        oprot.writeString(kiter255)
        oprot.writeMapBegin(TType.STRING, TType.LIST, len(viter256))
        for kiter257,viter258 in viter256.items():
          oprot.writeString(kiter257)
          oprot.writeListBegin(TType.STRUCT, len(viter258))
          for iter259 in viter258:
            iter259.write(oprot)
          oprot.writeListEnd()
        oprot.writeMapEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.consistency_level is not None:
      oprot.writeFieldBegin('consistency_level', TType.I32, 2)
      oprot.writeI32(self.consistency_level)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.mutation_map is None:
      raise TProtocol.TProtocolException(message='Required field mutation_map is unset!')
    if self.consistency_level is None:
      raise TProtocol.TProtocolException(message='Required field consistency_level is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class batch_mutate_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('batch_mutate_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class truncate_args:
  """
  Attributes:
   - cfname
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'cfname', None, None, ), # 1
  )

  def __init__(self, cfname=None,):
    self.cfname = cfname

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.cfname = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('truncate_args')
    if self.cfname is not None:
      oprot.writeFieldBegin('cfname', TType.STRING, 1)
      oprot.writeString(self.cfname)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cfname is None:
      raise TProtocol.TProtocolException(message='Required field cfname is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class truncate_result:
  """
  Attributes:
   - ire
   - ue
   - te
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
  )

  def __init__(self, ire=None, ue=None, te=None,):
    self.ire = ire
    self.ue = ue
    self.te = te

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('truncate_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_schema_versions_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_schema_versions_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_schema_versions_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.MAP, 'success', (TType.STRING,None,TType.LIST,(TType.STRING,None)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.MAP:
          self.success = {}
          (_ktype261, _vtype262, _size260 ) = iprot.readMapBegin() 
          for _i264 in xrange(_size260):
            _key265 = iprot.readString();
            _val266 = []
            (_etype270, _size267) = iprot.readListBegin()
            for _i271 in xrange(_size267):
              _elem272 = iprot.readString();
              _val266.append(_elem272)
            iprot.readListEnd()
            self.success[_key265] = _val266
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_schema_versions_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.MAP, 0)
      oprot.writeMapBegin(TType.STRING, TType.LIST, len(self.success))
      for kiter273,viter274 in self.success.items():
        oprot.writeString(kiter273)
        oprot.writeListBegin(TType.STRING, len(viter274))
        for iter275 in viter274:
          oprot.writeString(iter275)
        oprot.writeListEnd()
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspaces_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspaces_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspaces_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(KsDef, KsDef.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype279, _size276) = iprot.readListBegin()
          for _i280 in xrange(_size276):
            _elem281 = KsDef()
            _elem281.read(iprot)
            self.success.append(_elem281)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspaces_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter282 in self.success:
        iter282.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_cluster_name_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_cluster_name_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_cluster_name_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_cluster_name_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_version_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_version_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_version_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_version_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_ring_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_ring_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_ring_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRUCT,(TokenRange, TokenRange.thrift_spec)), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype286, _size283) = iprot.readListBegin()
          for _i287 in xrange(_size283):
            _elem288 = TokenRange()
            _elem288.read(iprot)
            self.success.append(_elem288)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_ring_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRUCT, len(self.success))
      for iter289 in self.success:
        iter289.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_partitioner_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_partitioner_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_partitioner_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_partitioner_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_snitch_args:

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_snitch_args')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_snitch_result:
  """
  Attributes:
   - success
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
  )

  def __init__(self, success=None,):
    self.success = success

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_snitch_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_keyspace_result:
  """
  Attributes:
   - success
   - nfe
   - ire
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (KsDef, KsDef.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'nfe', (NotFoundException, NotFoundException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, nfe=None, ire=None,):
    self.success = success
    self.nfe = nfe
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = KsDef()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.nfe = NotFoundException()
          self.nfe.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.nfe is not None:
      oprot.writeFieldBegin('nfe', TType.STRUCT, 1)
      self.nfe.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 2)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_splits_args:
  """
  Attributes:
   - cfName
   - start_token
   - end_token
   - keys_per_split
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'cfName', None, None, ), # 1
    (2, TType.STRING, 'start_token', None, None, ), # 2
    (3, TType.STRING, 'end_token', None, None, ), # 3
    (4, TType.I32, 'keys_per_split', None, None, ), # 4
  )

  def __init__(self, cfName=None, start_token=None, end_token=None, keys_per_split=None,):
    self.cfName = cfName
    self.start_token = start_token
    self.end_token = end_token
    self.keys_per_split = keys_per_split

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.cfName = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.keys_per_split = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_splits_args')
    if self.cfName is not None:
      oprot.writeFieldBegin('cfName', TType.STRING, 1)
      oprot.writeString(self.cfName)
      oprot.writeFieldEnd()
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 2)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 3)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.keys_per_split is not None:
      oprot.writeFieldBegin('keys_per_split', TType.I32, 4)
      oprot.writeI32(self.keys_per_split)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cfName is None:
      raise TProtocol.TProtocolException(message='Required field cfName is unset!')
    if self.start_token is None:
      raise TProtocol.TProtocolException(message='Required field start_token is unset!')
    if self.end_token is None:
      raise TProtocol.TProtocolException(message='Required field end_token is unset!')
    if self.keys_per_split is None:
      raise TProtocol.TProtocolException(message='Required field keys_per_split is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class describe_splits_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.LIST, 'success', (TType.STRING,None), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.LIST:
          self.success = []
          (_etype293, _size290) = iprot.readListBegin()
          for _i294 in xrange(_size290):
            _elem295 = iprot.readString();
            self.success.append(_elem295)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('describe_splits_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.LIST, 0)
      oprot.writeListBegin(TType.STRING, len(self.success))
      for iter296 in self.success:
        oprot.writeString(iter296)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_column_family_args:
  """
  Attributes:
   - cf_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'cf_def', (CfDef, CfDef.thrift_spec), None, ), # 1
  )

  def __init__(self, cf_def=None,):
    self.cf_def = cf_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.cf_def = CfDef()
          self.cf_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_column_family_args')
    if self.cf_def is not None:
      oprot.writeFieldBegin('cf_def', TType.STRUCT, 1)
      self.cf_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cf_def is None:
      raise TProtocol.TProtocolException(message='Required field cf_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_column_family_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_column_family_args:
  """
  Attributes:
   - column_family
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'column_family', None, None, ), # 1
  )

  def __init__(self, column_family=None,):
    self.column_family = column_family

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_column_family_args')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 1)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_column_family_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_keyspace_args:
  """
  Attributes:
   - ks_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ks_def', (KsDef, KsDef.thrift_spec), None, ), # 1
  )

  def __init__(self, ks_def=None,):
    self.ks_def = ks_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ks_def = KsDef()
          self.ks_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_keyspace_args')
    if self.ks_def is not None:
      oprot.writeFieldBegin('ks_def', TType.STRUCT, 1)
      self.ks_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.ks_def is None:
      raise TProtocol.TProtocolException(message='Required field ks_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_add_keyspace_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_add_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_keyspace_args:
  """
  Attributes:
   - keyspace
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
  )

  def __init__(self, keyspace=None,):
    self.keyspace = keyspace

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_keyspace_args')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_drop_keyspace_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_drop_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_keyspace_args:
  """
  Attributes:
   - ks_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ks_def', (KsDef, KsDef.thrift_spec), None, ), # 1
  )

  def __init__(self, ks_def=None,):
    self.ks_def = ks_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ks_def = KsDef()
          self.ks_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_keyspace_args')
    if self.ks_def is not None:
      oprot.writeFieldBegin('ks_def', TType.STRUCT, 1)
      self.ks_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.ks_def is None:
      raise TProtocol.TProtocolException(message='Required field ks_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_keyspace_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_keyspace_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_column_family_args:
  """
  Attributes:
   - cf_def
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'cf_def', (CfDef, CfDef.thrift_spec), None, ), # 1
  )

  def __init__(self, cf_def=None,):
    self.cf_def = cf_def

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.cf_def = CfDef()
          self.cf_def.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_column_family_args')
    if self.cf_def is not None:
      oprot.writeFieldBegin('cf_def', TType.STRUCT, 1)
      self.cf_def.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.cf_def is None:
      raise TProtocol.TProtocolException(message='Required field cf_def is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class system_update_column_family_result:
  """
  Attributes:
   - success
   - ire
   - sde
  """

  thrift_spec = (
    (0, TType.STRING, 'success', None, None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 2
  )

  def __init__(self, success=None, ire=None, sde=None,):
    self.success = success
    self.ire = ire
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRING:
          self.success = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('system_update_column_family_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRING, 0)
      oprot.writeString(self.success)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 2)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class execute_cql_query_args:
  """
  Attributes:
   - query
   - compression
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'query', None, None, ), # 1
    (2, TType.I32, 'compression', None, None, ), # 2
  )

  def __init__(self, query=None, compression=None,):
    self.query = query
    self.compression = compression

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.query = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.compression = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('execute_cql_query_args')
    if self.query is not None:
      oprot.writeFieldBegin('query', TType.STRING, 1)
      oprot.writeString(self.query)
      oprot.writeFieldEnd()
    if self.compression is not None:
      oprot.writeFieldBegin('compression', TType.I32, 2)
      oprot.writeI32(self.compression)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.query is None:
      raise TProtocol.TProtocolException(message='Required field query is unset!')
    if self.compression is None:
      raise TProtocol.TProtocolException(message='Required field compression is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class execute_cql_query_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
   - sde
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (CqlResult, CqlResult.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
    (4, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 4
  )

  def __init__(self, success=None, ire=None, ue=None, te=None, sde=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = CqlResult()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('execute_cql_query_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 4)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class prepare_cql_query_args:
  """
  Attributes:
   - query
   - compression
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'query', None, None, ), # 1
    (2, TType.I32, 'compression', None, None, ), # 2
  )

  def __init__(self, query=None, compression=None,):
    self.query = query
    self.compression = compression

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.query = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.compression = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('prepare_cql_query_args')
    if self.query is not None:
      oprot.writeFieldBegin('query', TType.STRING, 1)
      oprot.writeString(self.query)
      oprot.writeFieldEnd()
    if self.compression is not None:
      oprot.writeFieldBegin('compression', TType.I32, 2)
      oprot.writeI32(self.compression)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.query is None:
      raise TProtocol.TProtocolException(message='Required field query is unset!')
    if self.compression is None:
      raise TProtocol.TProtocolException(message='Required field compression is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class prepare_cql_query_result:
  """
  Attributes:
   - success
   - ire
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (CqlPreparedResult, CqlPreparedResult.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, success=None, ire=None,):
    self.success = success
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = CqlPreparedResult()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('prepare_cql_query_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class execute_prepared_cql_query_args:
  """
  Attributes:
   - itemId
   - values
  """

  thrift_spec = (
    None, # 0
    (1, TType.I32, 'itemId', None, None, ), # 1
    (2, TType.LIST, 'values', (TType.STRING,None), None, ), # 2
  )

  def __init__(self, itemId=None, values=None,):
    self.itemId = itemId
    self.values = values

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.I32:
          self.itemId = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.values = []
          (_etype300, _size297) = iprot.readListBegin()
          for _i301 in xrange(_size297):
            _elem302 = iprot.readString();
            self.values.append(_elem302)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('execute_prepared_cql_query_args')
    if self.itemId is not None:
      oprot.writeFieldBegin('itemId', TType.I32, 1)
      oprot.writeI32(self.itemId)
      oprot.writeFieldEnd()
    if self.values is not None:
      oprot.writeFieldBegin('values', TType.LIST, 2)
      oprot.writeListBegin(TType.STRING, len(self.values))
      for iter303 in self.values:
        oprot.writeString(iter303)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.itemId is None:
      raise TProtocol.TProtocolException(message='Required field itemId is unset!')
    if self.values is None:
      raise TProtocol.TProtocolException(message='Required field values is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class execute_prepared_cql_query_result:
  """
  Attributes:
   - success
   - ire
   - ue
   - te
   - sde
  """

  thrift_spec = (
    (0, TType.STRUCT, 'success', (CqlResult, CqlResult.thrift_spec), None, ), # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'ue', (UnavailableException, UnavailableException.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'te', (TimedOutException, TimedOutException.thrift_spec), None, ), # 3
    (4, TType.STRUCT, 'sde', (SchemaDisagreementException, SchemaDisagreementException.thrift_spec), None, ), # 4
  )

  def __init__(self, success=None, ire=None, ue=None, te=None, sde=None,):
    self.success = success
    self.ire = ire
    self.ue = ue
    self.te = te
    self.sde = sde

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 0:
        if ftype == TType.STRUCT:
          self.success = CqlResult()
          self.success.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.ue = UnavailableException()
          self.ue.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.te = TimedOutException()
          self.te.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.sde = SchemaDisagreementException()
          self.sde.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('execute_prepared_cql_query_result')
    if self.success is not None:
      oprot.writeFieldBegin('success', TType.STRUCT, 0)
      self.success.write(oprot)
      oprot.writeFieldEnd()
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    if self.ue is not None:
      oprot.writeFieldBegin('ue', TType.STRUCT, 2)
      self.ue.write(oprot)
      oprot.writeFieldEnd()
    if self.te is not None:
      oprot.writeFieldBegin('te', TType.STRUCT, 3)
      self.te.write(oprot)
      oprot.writeFieldEnd()
    if self.sde is not None:
      oprot.writeFieldBegin('sde', TType.STRUCT, 4)
      self.sde.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_cql_version_args:
  """
  Attributes:
   - version
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'version', None, None, ), # 1
  )

  def __init__(self, version=None,):
    self.version = version

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.version = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_cql_version_args')
    if self.version is not None:
      oprot.writeFieldBegin('version', TType.STRING, 1)
      oprot.writeString(self.version)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.version is None:
      raise TProtocol.TProtocolException(message='Required field version is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class set_cql_version_result:
  """
  Attributes:
   - ire
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ire', (InvalidRequestException, InvalidRequestException.thrift_spec), None, ), # 1
  )

  def __init__(self, ire=None,):
    self.ire = ire

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.ire = InvalidRequestException()
          self.ire.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('set_cql_version_result')
    if self.ire is not None:
      oprot.writeFieldBegin('ire', TType.STRUCT, 1)
      self.ire.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

########NEW FILE########
__FILENAME__ = constants
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException
from ttypes import *

VERSION = "19.30.0"

########NEW FILE########
__FILENAME__ = ttypes
#
# Autogenerated by Thrift Compiler (0.8.0)
#
# DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
#
#  options string: py
#

from thrift.Thrift import TType, TMessageType, TException

from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol, TProtocol
try:
  from thrift.protocol import fastbinary
except:
  fastbinary = None


class ConsistencyLevel:
  """
  The ConsistencyLevel is an enum that controls both read and write
  behavior based on the ReplicationFactor of the keyspace.  The
  different consistency levels have different meanings, depending on
  if you're doing a write or read operation.

  If W + R > ReplicationFactor, where W is the number of nodes to
  block for on write, and R the number to block for on reads, you
  will have strongly consistent behavior; that is, readers will
  always see the most recent write. Of these, the most interesting is
  to do QUORUM reads and writes, which gives you consistency while
  still allowing availability in the face of node failures up to half
  of <ReplicationFactor>. Of course if latency is more important than
  consistency then you can use lower values for either or both.

  Some ConsistencyLevels (ONE, TWO, THREE) refer to a specific number
  of replicas rather than a logical concept that adjusts
  automatically with the replication factor.  Of these, only ONE is
  commonly used; TWO and (even more rarely) THREE are only useful
  when you care more about guaranteeing a certain level of
  durability, than consistency.

  Write consistency levels make the following guarantees before reporting success to the client:
    ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node.
    ONE          Ensure that the write has been written to at least 1 node's commit log and memory table
    TWO          Ensure that the write has been written to at least 2 node's commit log and memory table
    THREE        Ensure that the write has been written to at least 3 node's commit log and memory table
    QUORUM       Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes
    LOCAL_QUORUM Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes, within the local datacenter (requires NetworkTopologyStrategy)
    EACH_QUORUM  Ensure that the write has been written to <ReplicationFactor> / 2 + 1 nodes in each datacenter (requires NetworkTopologyStrategy)
    ALL          Ensure that the write is written to <code>&lt;ReplicationFactor&gt;</code> nodes before responding to the client.

  Read consistency levels make the following guarantees before returning successful results to the client:
    ANY          Not supported. You probably want ONE instead.
    ONE          Returns the record obtained from a single replica.
    TWO          Returns the record with the most recent timestamp once two replicas have replied.
    THREE        Returns the record with the most recent timestamp once three replicas have replied.
    QUORUM       Returns the record with the most recent timestamp once a majority of replicas have replied.
    LOCAL_QUORUM Returns the record with the most recent timestamp once a majority of replicas within the local datacenter have replied.
    EACH_QUORUM  Returns the record with the most recent timestamp once a majority of replicas within each datacenter have replied.
    ALL          Returns the record with the most recent timestamp once all replicas have replied (implies no replica may be down)..
  """
  ONE = 1
  QUORUM = 2
  LOCAL_QUORUM = 3
  EACH_QUORUM = 4
  ALL = 5
  ANY = 6
  TWO = 7
  THREE = 8

  _VALUES_TO_NAMES = {
    1: "ONE",
    2: "QUORUM",
    3: "LOCAL_QUORUM",
    4: "EACH_QUORUM",
    5: "ALL",
    6: "ANY",
    7: "TWO",
    8: "THREE",
  }

  _NAMES_TO_VALUES = {
    "ONE": 1,
    "QUORUM": 2,
    "LOCAL_QUORUM": 3,
    "EACH_QUORUM": 4,
    "ALL": 5,
    "ANY": 6,
    "TWO": 7,
    "THREE": 8,
  }

class IndexOperator:
  EQ = 0
  GTE = 1
  GT = 2
  LTE = 3
  LT = 4

  _VALUES_TO_NAMES = {
    0: "EQ",
    1: "GTE",
    2: "GT",
    3: "LTE",
    4: "LT",
  }

  _NAMES_TO_VALUES = {
    "EQ": 0,
    "GTE": 1,
    "GT": 2,
    "LTE": 3,
    "LT": 4,
  }

class IndexType:
  KEYS = 0
  CUSTOM = 1

  _VALUES_TO_NAMES = {
    0: "KEYS",
    1: "CUSTOM",
  }

  _NAMES_TO_VALUES = {
    "KEYS": 0,
    "CUSTOM": 1,
  }

class Compression:
  """
  CQL query compression
  """
  GZIP = 1
  NONE = 2

  _VALUES_TO_NAMES = {
    1: "GZIP",
    2: "NONE",
  }

  _NAMES_TO_VALUES = {
    "GZIP": 1,
    "NONE": 2,
  }

class CqlResultType:
  ROWS = 1
  VOID = 2
  INT = 3

  _VALUES_TO_NAMES = {
    1: "ROWS",
    2: "VOID",
    3: "INT",
  }

  _NAMES_TO_VALUES = {
    "ROWS": 1,
    "VOID": 2,
    "INT": 3,
  }


class Column:
  """
  Basic unit of data within a ColumnFamily.
  @param name, the name by which this column is set and retrieved.  Maximum 64KB long.
  @param value. The data associated with the name.  Maximum 2GB long, but in practice you should limit it to small numbers of MB (since Thrift must read the full value into memory to operate on it).
  @param timestamp. The timestamp is used for conflict detection/resolution when two columns with same name need to be compared.
  @param ttl. An optional, positive delay (in seconds) after which the column will be automatically deleted.

  Attributes:
   - name
   - value
   - timestamp
   - ttl
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'value', None, None, ), # 2
    (3, TType.I64, 'timestamp', None, None, ), # 3
    (4, TType.I32, 'ttl', None, None, ), # 4
  )

  def __init__(self, name=None, value=None, timestamp=None, ttl=None,):
    self.name = name
    self.value = value
    self.timestamp = timestamp
    self.ttl = ttl

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.value = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.ttl = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Column')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.value is not None:
      oprot.writeFieldBegin('value', TType.STRING, 2)
      oprot.writeString(self.value)
      oprot.writeFieldEnd()
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 3)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.ttl is not None:
      oprot.writeFieldBegin('ttl', TType.I32, 4)
      oprot.writeI32(self.ttl)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SuperColumn:
  """
  A named list of columns.
  @param name. see Column.name.
  @param columns. A collection of standard Columns.  The columns within a super column are defined in an adhoc manner.
                  Columns within a super column do not have to have matching structures (similarly named child columns).

  Attributes:
   - name
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(Column, Column.thrift_spec)), None, ), # 2
  )

  def __init__(self, name=None, columns=None,):
    self.name = name
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype3, _size0) = iprot.readListBegin()
          for _i4 in xrange(_size0):
            _elem5 = Column()
            _elem5.read(iprot)
            self.columns.append(_elem5)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SuperColumn')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter6 in self.columns:
        iter6.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CounterColumn:
  """
  Attributes:
   - name
   - value
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.I64, 'value', None, None, ), # 2
  )

  def __init__(self, name=None, value=None,):
    self.name = name
    self.value = value

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I64:
          self.value = iprot.readI64();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CounterColumn')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.value is not None:
      oprot.writeFieldBegin('value', TType.I64, 2)
      oprot.writeI64(self.value)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.value is None:
      raise TProtocol.TProtocolException(message='Required field value is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CounterSuperColumn:
  """
  Attributes:
   - name
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(CounterColumn, CounterColumn.thrift_spec)), None, ), # 2
  )

  def __init__(self, name=None, columns=None,):
    self.name = name
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype10, _size7) = iprot.readListBegin()
          for _i11 in xrange(_size7):
            _elem12 = CounterColumn()
            _elem12.read(iprot)
            self.columns.append(_elem12)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CounterSuperColumn')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter13 in self.columns:
        iter13.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnOrSuperColumn:
  """
  Methods for fetching rows/records from Cassandra will return either a single instance of ColumnOrSuperColumn or a list
  of ColumnOrSuperColumns (get_slice()). If you're looking up a SuperColumn (or list of SuperColumns) then the resulting
  instances of ColumnOrSuperColumn will have the requested SuperColumn in the attribute super_column. For queries resulting
  in Columns, those values will be in the attribute column. This change was made between 0.3 and 0.4 to standardize on
  single query methods that may return either a SuperColumn or Column.

  If the query was on a counter column family, you will either get a counter_column (instead of a column) or a
  counter_super_column (instead of a super_column)

  @param column. The Column returned by get() or get_slice().
  @param super_column. The SuperColumn returned by get() or get_slice().
  @param counter_column. The Counterolumn returned by get() or get_slice().
  @param counter_super_column. The CounterSuperColumn returned by get() or get_slice().

  Attributes:
   - column
   - super_column
   - counter_column
   - counter_super_column
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column', (Column, Column.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'super_column', (SuperColumn, SuperColumn.thrift_spec), None, ), # 2
    (3, TType.STRUCT, 'counter_column', (CounterColumn, CounterColumn.thrift_spec), None, ), # 3
    (4, TType.STRUCT, 'counter_super_column', (CounterSuperColumn, CounterSuperColumn.thrift_spec), None, ), # 4
  )

  def __init__(self, column=None, super_column=None, counter_column=None, counter_super_column=None,):
    self.column = column
    self.super_column = super_column
    self.counter_column = counter_column
    self.counter_super_column = counter_super_column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column = Column()
          self.column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.super_column = SuperColumn()
          self.super_column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.counter_column = CounterColumn()
          self.counter_column.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.counter_super_column = CounterSuperColumn()
          self.counter_super_column.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnOrSuperColumn')
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRUCT, 1)
      self.column.write(oprot)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRUCT, 2)
      self.super_column.write(oprot)
      oprot.writeFieldEnd()
    if self.counter_column is not None:
      oprot.writeFieldBegin('counter_column', TType.STRUCT, 3)
      self.counter_column.write(oprot)
      oprot.writeFieldEnd()
    if self.counter_super_column is not None:
      oprot.writeFieldBegin('counter_super_column', TType.STRUCT, 4)
      self.counter_super_column.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class NotFoundException(TException):
  """
  A specific column was requested that does not exist.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('NotFoundException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class InvalidRequestException(TException):
  """
  Invalid request could mean keyspace or column family does not exist, required parameters are missing, or a parameter is malformed.
  why contains an associated error message.

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('InvalidRequestException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class UnavailableException(TException):
  """
  Not all the replicas required could be created and/or read.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('UnavailableException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TimedOutException(TException):
  """
  RPC timeout was exceeded.  either a node failed mid-operation, or load was too high, or the requested op was too large.
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('TimedOutException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthenticationException(TException):
  """
  invalid authentication request (invalid keyspace, user does not exist, or credentials invalid)

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthenticationException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthorizationException(TException):
  """
  invalid authorization request (user does not have access to keyspace)

  Attributes:
   - why
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'why', None, None, ), # 1
  )

  def __init__(self, why=None,):
    self.why = why

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.why = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthorizationException')
    if self.why is not None:
      oprot.writeFieldBegin('why', TType.STRING, 1)
      oprot.writeString(self.why)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.why is None:
      raise TProtocol.TProtocolException(message='Required field why is unset!')
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SchemaDisagreementException(TException):
  """
  schemas are not in agreement across all nodes
  """

  thrift_spec = (
  )

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SchemaDisagreementException')
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __str__(self):
    return repr(self)

  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnParent:
  """
  ColumnParent is used when selecting groups of columns from the same ColumnFamily. In directory structure terms, imagine
  ColumnParent as ColumnPath + '/../'.

  See also <a href="cassandra.html#Struct_ColumnPath">ColumnPath</a>

  Attributes:
   - column_family
   - super_column
  """

  thrift_spec = (
    None, # 0
    None, # 1
    None, # 2
    (3, TType.STRING, 'column_family', None, None, ), # 3
    (4, TType.STRING, 'super_column', None, None, ), # 4
  )

  def __init__(self, column_family=None, super_column=None,):
    self.column_family = column_family
    self.super_column = super_column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 3:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnParent')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 3)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 4)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnPath:
  """
  The ColumnPath is the path to a single column in Cassandra. It might make sense to think of ColumnPath and
  ColumnParent in terms of a directory structure.

  ColumnPath is used to looking up a single column.

  @param column_family. The name of the CF of the column being looked up.
  @param super_column. The super column name.
  @param column. The column name.

  Attributes:
   - column_family
   - super_column
   - column
  """

  thrift_spec = (
    None, # 0
    None, # 1
    None, # 2
    (3, TType.STRING, 'column_family', None, None, ), # 3
    (4, TType.STRING, 'super_column', None, None, ), # 4
    (5, TType.STRING, 'column', None, None, ), # 5
  )

  def __init__(self, column_family=None, super_column=None, column=None,):
    self.column_family = column_family
    self.super_column = super_column
    self.column = column

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 3:
        if ftype == TType.STRING:
          self.column_family = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.STRING:
          self.column = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnPath')
    if self.column_family is not None:
      oprot.writeFieldBegin('column_family', TType.STRING, 3)
      oprot.writeString(self.column_family)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 4)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    if self.column is not None:
      oprot.writeFieldBegin('column', TType.STRING, 5)
      oprot.writeString(self.column)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_family is None:
      raise TProtocol.TProtocolException(message='Required field column_family is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SliceRange:
  """
  A slice range is a structure that stores basic range, ordering and limit information for a query that will return
  multiple columns. It could be thought of as Cassandra's version of LIMIT and ORDER BY

  @param start. The column name to start the slice with. This attribute is not required, though there is no default value,
                and can be safely set to '', i.e., an empty byte array, to start with the first column name. Otherwise, it
                must a valid value under the rules of the Comparator defined for the given ColumnFamily.
  @param finish. The column name to stop the slice at. This attribute is not required, though there is no default value,
                 and can be safely set to an empty byte array to not stop until 'count' results are seen. Otherwise, it
                 must also be a valid value to the ColumnFamily Comparator.
  @param reversed. Whether the results should be ordered in reversed order. Similar to ORDER BY blah DESC in SQL.
  @param count. How many columns to return. Similar to LIMIT in SQL. May be arbitrarily large, but Thrift will
                materialize the whole result into memory before returning it to the client, so be aware that you may
                be better served by iterating through slices by passing the last value of one call in as the 'start'
                of the next instead of increasing 'count' arbitrarily large.

  Attributes:
   - start
   - finish
   - reversed
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start', None, None, ), # 1
    (2, TType.STRING, 'finish', None, None, ), # 2
    (3, TType.BOOL, 'reversed', None, False, ), # 3
    (4, TType.I32, 'count', None, 100, ), # 4
  )

  def __init__(self, start=None, finish=None, reversed=thrift_spec[3][4], count=thrift_spec[4][4],):
    self.start = start
    self.finish = finish
    self.reversed = reversed
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.finish = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.BOOL:
          self.reversed = iprot.readBool();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SliceRange')
    if self.start is not None:
      oprot.writeFieldBegin('start', TType.STRING, 1)
      oprot.writeString(self.start)
      oprot.writeFieldEnd()
    if self.finish is not None:
      oprot.writeFieldBegin('finish', TType.STRING, 2)
      oprot.writeString(self.finish)
      oprot.writeFieldEnd()
    if self.reversed is not None:
      oprot.writeFieldBegin('reversed', TType.BOOL, 3)
      oprot.writeBool(self.reversed)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 4)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.start is None:
      raise TProtocol.TProtocolException(message='Required field start is unset!')
    if self.finish is None:
      raise TProtocol.TProtocolException(message='Required field finish is unset!')
    if self.reversed is None:
      raise TProtocol.TProtocolException(message='Required field reversed is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class SlicePredicate:
  """
  A SlicePredicate is similar to a mathematic predicate (see http://en.wikipedia.org/wiki/Predicate_(mathematical_logic)),
  which is described as "a property that the elements of a set have in common."

  SlicePredicate's in Cassandra are described with either a list of column_names or a SliceRange.  If column_names is
  specified, slice_range is ignored.

  @param column_name. A list of column names to retrieve. This can be used similar to Memcached's "multi-get" feature
                      to fetch N known column names. For instance, if you know you wish to fetch columns 'Joe', 'Jack',
                      and 'Jim' you can pass those column names as a list to fetch all three at once.
  @param slice_range. A SliceRange describing how to range, order, and/or limit the slice.

  Attributes:
   - column_names
   - slice_range
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'column_names', (TType.STRING,None), None, ), # 1
    (2, TType.STRUCT, 'slice_range', (SliceRange, SliceRange.thrift_spec), None, ), # 2
  )

  def __init__(self, column_names=None, slice_range=None,):
    self.column_names = column_names
    self.slice_range = slice_range

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.column_names = []
          (_etype17, _size14) = iprot.readListBegin()
          for _i18 in xrange(_size14):
            _elem19 = iprot.readString();
            self.column_names.append(_elem19)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.slice_range = SliceRange()
          self.slice_range.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('SlicePredicate')
    if self.column_names is not None:
      oprot.writeFieldBegin('column_names', TType.LIST, 1)
      oprot.writeListBegin(TType.STRING, len(self.column_names))
      for iter20 in self.column_names:
        oprot.writeString(iter20)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.slice_range is not None:
      oprot.writeFieldBegin('slice_range', TType.STRUCT, 2)
      self.slice_range.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class IndexExpression:
  """
  Attributes:
   - column_name
   - op
   - value
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'column_name', None, None, ), # 1
    (2, TType.I32, 'op', None, None, ), # 2
    (3, TType.STRING, 'value', None, None, ), # 3
  )

  def __init__(self, column_name=None, op=None, value=None,):
    self.column_name = column_name
    self.op = op
    self.value = value

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.column_name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.op = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.value = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('IndexExpression')
    if self.column_name is not None:
      oprot.writeFieldBegin('column_name', TType.STRING, 1)
      oprot.writeString(self.column_name)
      oprot.writeFieldEnd()
    if self.op is not None:
      oprot.writeFieldBegin('op', TType.I32, 2)
      oprot.writeI32(self.op)
      oprot.writeFieldEnd()
    if self.value is not None:
      oprot.writeFieldBegin('value', TType.STRING, 3)
      oprot.writeString(self.value)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.column_name is None:
      raise TProtocol.TProtocolException(message='Required field column_name is unset!')
    if self.op is None:
      raise TProtocol.TProtocolException(message='Required field op is unset!')
    if self.value is None:
      raise TProtocol.TProtocolException(message='Required field value is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class IndexClause:
  """
  @Deprecated: use a KeyRange with row_filter in get_range_slices instead

  Attributes:
   - expressions
   - start_key
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.LIST, 'expressions', (TType.STRUCT,(IndexExpression, IndexExpression.thrift_spec)), None, ), # 1
    (2, TType.STRING, 'start_key', None, None, ), # 2
    (3, TType.I32, 'count', None, 100, ), # 3
  )

  def __init__(self, expressions=None, start_key=None, count=thrift_spec[3][4],):
    self.expressions = expressions
    self.start_key = start_key
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.LIST:
          self.expressions = []
          (_etype24, _size21) = iprot.readListBegin()
          for _i25 in xrange(_size21):
            _elem26 = IndexExpression()
            _elem26.read(iprot)
            self.expressions.append(_elem26)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.start_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('IndexClause')
    if self.expressions is not None:
      oprot.writeFieldBegin('expressions', TType.LIST, 1)
      oprot.writeListBegin(TType.STRUCT, len(self.expressions))
      for iter27 in self.expressions:
        iter27.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.start_key is not None:
      oprot.writeFieldBegin('start_key', TType.STRING, 2)
      oprot.writeString(self.start_key)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 3)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.expressions is None:
      raise TProtocol.TProtocolException(message='Required field expressions is unset!')
    if self.start_key is None:
      raise TProtocol.TProtocolException(message='Required field start_key is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeyRange:
  """
  The semantics of start keys and tokens are slightly different.
  Keys are start-inclusive; tokens are start-exclusive.  Token
  ranges may also wrap -- that is, the end token may be less
  than the start one.  Thus, a range from keyX to keyX is a
  one-element range, but a range from tokenY to tokenY is the
  full ring.

  Attributes:
   - start_key
   - end_key
   - start_token
   - end_token
   - row_filter
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start_key', None, None, ), # 1
    (2, TType.STRING, 'end_key', None, None, ), # 2
    (3, TType.STRING, 'start_token', None, None, ), # 3
    (4, TType.STRING, 'end_token', None, None, ), # 4
    (5, TType.I32, 'count', None, 100, ), # 5
    (6, TType.LIST, 'row_filter', (TType.STRUCT,(IndexExpression, IndexExpression.thrift_spec)), None, ), # 6
  )

  def __init__(self, start_key=None, end_key=None, start_token=None, end_token=None, row_filter=None, count=thrift_spec[5][4],):
    self.start_key = start_key
    self.end_key = end_key
    self.start_token = start_token
    self.end_token = end_token
    self.row_filter = row_filter
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.end_key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 6:
        if ftype == TType.LIST:
          self.row_filter = []
          (_etype31, _size28) = iprot.readListBegin()
          for _i32 in xrange(_size28):
            _elem33 = IndexExpression()
            _elem33.read(iprot)
            self.row_filter.append(_elem33)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeyRange')
    if self.start_key is not None:
      oprot.writeFieldBegin('start_key', TType.STRING, 1)
      oprot.writeString(self.start_key)
      oprot.writeFieldEnd()
    if self.end_key is not None:
      oprot.writeFieldBegin('end_key', TType.STRING, 2)
      oprot.writeString(self.end_key)
      oprot.writeFieldEnd()
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 3)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 4)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 5)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    if self.row_filter is not None:
      oprot.writeFieldBegin('row_filter', TType.LIST, 6)
      oprot.writeListBegin(TType.STRUCT, len(self.row_filter))
      for iter34 in self.row_filter:
        iter34.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeySlice:
  """
  A KeySlice is key followed by the data it maps to. A collection of KeySlice is returned by the get_range_slice operation.

  @param key. a row key
  @param columns. List of data represented by the key. Typically, the list is pared down to only the columns specified by
                  a SlicePredicate.

  Attributes:
   - key
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec)), None, ), # 2
  )

  def __init__(self, key=None, columns=None,):
    self.key = key
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype38, _size35) = iprot.readListBegin()
          for _i39 in xrange(_size35):
            _elem40 = ColumnOrSuperColumn()
            _elem40.read(iprot)
            self.columns.append(_elem40)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeySlice')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter41 in self.columns:
        iter41.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KeyCount:
  """
  Attributes:
   - key
   - count
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.I32, 'count', None, None, ), # 2
  )

  def __init__(self, key=None, count=None,):
    self.key = key
    self.count = count

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KeyCount')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 2)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class Deletion:
  """
  Note that the timestamp is only optional in case of counter deletion.

  Attributes:
   - timestamp
   - super_column
   - predicate
  """

  thrift_spec = (
    None, # 0
    (1, TType.I64, 'timestamp', None, None, ), # 1
    (2, TType.STRING, 'super_column', None, None, ), # 2
    (3, TType.STRUCT, 'predicate', (SlicePredicate, SlicePredicate.thrift_spec), None, ), # 3
  )

  def __init__(self, timestamp=None, super_column=None, predicate=None,):
    self.timestamp = timestamp
    self.super_column = super_column
    self.predicate = predicate

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.I64:
          self.timestamp = iprot.readI64();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.super_column = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRUCT:
          self.predicate = SlicePredicate()
          self.predicate.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Deletion')
    if self.timestamp is not None:
      oprot.writeFieldBegin('timestamp', TType.I64, 1)
      oprot.writeI64(self.timestamp)
      oprot.writeFieldEnd()
    if self.super_column is not None:
      oprot.writeFieldBegin('super_column', TType.STRING, 2)
      oprot.writeString(self.super_column)
      oprot.writeFieldEnd()
    if self.predicate is not None:
      oprot.writeFieldBegin('predicate', TType.STRUCT, 3)
      self.predicate.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class Mutation:
  """
  A Mutation is either an insert (represented by filling column_or_supercolumn) or a deletion (represented by filling the deletion attribute).
  @param column_or_supercolumn. An insert to a column or supercolumn (possibly counter column or supercolumn)
  @param deletion. A deletion of a column or supercolumn

  Attributes:
   - column_or_supercolumn
   - deletion
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'column_or_supercolumn', (ColumnOrSuperColumn, ColumnOrSuperColumn.thrift_spec), None, ), # 1
    (2, TType.STRUCT, 'deletion', (Deletion, Deletion.thrift_spec), None, ), # 2
  )

  def __init__(self, column_or_supercolumn=None, deletion=None,):
    self.column_or_supercolumn = column_or_supercolumn
    self.deletion = deletion

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRUCT:
          self.column_or_supercolumn = ColumnOrSuperColumn()
          self.column_or_supercolumn.read(iprot)
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRUCT:
          self.deletion = Deletion()
          self.deletion.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('Mutation')
    if self.column_or_supercolumn is not None:
      oprot.writeFieldBegin('column_or_supercolumn', TType.STRUCT, 1)
      self.column_or_supercolumn.write(oprot)
      oprot.writeFieldEnd()
    if self.deletion is not None:
      oprot.writeFieldBegin('deletion', TType.STRUCT, 2)
      self.deletion.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class EndpointDetails:
  """
  Attributes:
   - host
   - datacenter
   - rack
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'host', None, None, ), # 1
    (2, TType.STRING, 'datacenter', None, None, ), # 2
    (3, TType.STRING, 'rack', None, None, ), # 3
  )

  def __init__(self, host=None, datacenter=None, rack=None,):
    self.host = host
    self.datacenter = datacenter
    self.rack = rack

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.host = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.datacenter = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.rack = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('EndpointDetails')
    if self.host is not None:
      oprot.writeFieldBegin('host', TType.STRING, 1)
      oprot.writeString(self.host)
      oprot.writeFieldEnd()
    if self.datacenter is not None:
      oprot.writeFieldBegin('datacenter', TType.STRING, 2)
      oprot.writeString(self.datacenter)
      oprot.writeFieldEnd()
    if self.rack is not None:
      oprot.writeFieldBegin('rack', TType.STRING, 3)
      oprot.writeString(self.rack)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TokenRange:
  """
  A TokenRange describes part of the Cassandra ring, it is a mapping from a range to
  endpoints responsible for that range.
  @param start_token The first token in the range
  @param end_token The last token in the range
  @param endpoints The endpoints responsible for the range (listed by their configured listen_address)
  @param rpc_endpoints The endpoints responsible for the range (listed by their configured rpc_address)

  Attributes:
   - start_token
   - end_token
   - endpoints
   - rpc_endpoints
   - endpoint_details
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'start_token', None, None, ), # 1
    (2, TType.STRING, 'end_token', None, None, ), # 2
    (3, TType.LIST, 'endpoints', (TType.STRING,None), None, ), # 3
    (4, TType.LIST, 'rpc_endpoints', (TType.STRING,None), None, ), # 4
    (5, TType.LIST, 'endpoint_details', (TType.STRUCT,(EndpointDetails, EndpointDetails.thrift_spec)), None, ), # 5
  )

  def __init__(self, start_token=None, end_token=None, endpoints=None, rpc_endpoints=None, endpoint_details=None,):
    self.start_token = start_token
    self.end_token = end_token
    self.endpoints = endpoints
    self.rpc_endpoints = rpc_endpoints
    self.endpoint_details = endpoint_details

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.start_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.end_token = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.LIST:
          self.endpoints = []
          (_etype45, _size42) = iprot.readListBegin()
          for _i46 in xrange(_size42):
            _elem47 = iprot.readString();
            self.endpoints.append(_elem47)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.LIST:
          self.rpc_endpoints = []
          (_etype51, _size48) = iprot.readListBegin()
          for _i52 in xrange(_size48):
            _elem53 = iprot.readString();
            self.rpc_endpoints.append(_elem53)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.LIST:
          self.endpoint_details = []
          (_etype57, _size54) = iprot.readListBegin()
          for _i58 in xrange(_size54):
            _elem59 = EndpointDetails()
            _elem59.read(iprot)
            self.endpoint_details.append(_elem59)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('TokenRange')
    if self.start_token is not None:
      oprot.writeFieldBegin('start_token', TType.STRING, 1)
      oprot.writeString(self.start_token)
      oprot.writeFieldEnd()
    if self.end_token is not None:
      oprot.writeFieldBegin('end_token', TType.STRING, 2)
      oprot.writeString(self.end_token)
      oprot.writeFieldEnd()
    if self.endpoints is not None:
      oprot.writeFieldBegin('endpoints', TType.LIST, 3)
      oprot.writeListBegin(TType.STRING, len(self.endpoints))
      for iter60 in self.endpoints:
        oprot.writeString(iter60)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.rpc_endpoints is not None:
      oprot.writeFieldBegin('rpc_endpoints', TType.LIST, 4)
      oprot.writeListBegin(TType.STRING, len(self.rpc_endpoints))
      for iter61 in self.rpc_endpoints:
        oprot.writeString(iter61)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.endpoint_details is not None:
      oprot.writeFieldBegin('endpoint_details', TType.LIST, 5)
      oprot.writeListBegin(TType.STRUCT, len(self.endpoint_details))
      for iter62 in self.endpoint_details:
        iter62.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.start_token is None:
      raise TProtocol.TProtocolException(message='Required field start_token is unset!')
    if self.end_token is None:
      raise TProtocol.TProtocolException(message='Required field end_token is unset!')
    if self.endpoints is None:
      raise TProtocol.TProtocolException(message='Required field endpoints is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class AuthenticationRequest:
  """
  Authentication requests can contain any data, dependent on the IAuthenticator used

  Attributes:
   - credentials
  """

  thrift_spec = (
    None, # 0
    (1, TType.MAP, 'credentials', (TType.STRING,None,TType.STRING,None), None, ), # 1
  )

  def __init__(self, credentials=None,):
    self.credentials = credentials

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.MAP:
          self.credentials = {}
          (_ktype64, _vtype65, _size63 ) = iprot.readMapBegin() 
          for _i67 in xrange(_size63):
            _key68 = iprot.readString();
            _val69 = iprot.readString();
            self.credentials[_key68] = _val69
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('AuthenticationRequest')
    if self.credentials is not None:
      oprot.writeFieldBegin('credentials', TType.MAP, 1)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.credentials))
      for kiter70,viter71 in self.credentials.items():
        oprot.writeString(kiter70)
        oprot.writeString(viter71)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.credentials is None:
      raise TProtocol.TProtocolException(message='Required field credentials is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class ColumnDef:
  """
  Attributes:
   - name
   - validation_class
   - index_type
   - index_name
   - index_options
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'validation_class', None, None, ), # 2
    (3, TType.I32, 'index_type', None, None, ), # 3
    (4, TType.STRING, 'index_name', None, None, ), # 4
    (5, TType.MAP, 'index_options', (TType.STRING,None,TType.STRING,None), None, ), # 5
  )

  def __init__(self, name=None, validation_class=None, index_type=None, index_name=None, index_options=None,):
    self.name = name
    self.validation_class = validation_class
    self.index_type = index_type
    self.index_name = index_name
    self.index_options = index_options

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.validation_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.index_type = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.index_name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.MAP:
          self.index_options = {}
          (_ktype73, _vtype74, _size72 ) = iprot.readMapBegin() 
          for _i76 in xrange(_size72):
            _key77 = iprot.readString();
            _val78 = iprot.readString();
            self.index_options[_key77] = _val78
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('ColumnDef')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.validation_class is not None:
      oprot.writeFieldBegin('validation_class', TType.STRING, 2)
      oprot.writeString(self.validation_class)
      oprot.writeFieldEnd()
    if self.index_type is not None:
      oprot.writeFieldBegin('index_type', TType.I32, 3)
      oprot.writeI32(self.index_type)
      oprot.writeFieldEnd()
    if self.index_name is not None:
      oprot.writeFieldBegin('index_name', TType.STRING, 4)
      oprot.writeString(self.index_name)
      oprot.writeFieldEnd()
    if self.index_options is not None:
      oprot.writeFieldBegin('index_options', TType.MAP, 5)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.index_options))
      for kiter79,viter80 in self.index_options.items():
        oprot.writeString(kiter79)
        oprot.writeString(viter80)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.validation_class is None:
      raise TProtocol.TProtocolException(message='Required field validation_class is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CfDef:
  """
  Attributes:
   - keyspace
   - name
   - column_type
   - comparator_type
   - subcomparator_type
   - comment
   - read_repair_chance
   - column_metadata
   - gc_grace_seconds
   - default_validation_class
   - id
   - min_compaction_threshold
   - max_compaction_threshold
   - replicate_on_write
   - key_validation_class
   - key_alias
   - compaction_strategy
   - compaction_strategy_options
   - compression_options
   - bloom_filter_fp_chance
   - caching
   - column_aliases
   - value_alias
   - dclocal_read_repair_chance
   - row_cache_size: @deprecated
   - key_cache_size: @deprecated
   - row_cache_save_period_in_seconds: @deprecated
   - key_cache_save_period_in_seconds: @deprecated
   - memtable_flush_after_mins: @deprecated
   - memtable_throughput_in_mb: @deprecated
   - memtable_operations_in_millions: @deprecated
   - merge_shards_chance: @deprecated
   - row_cache_provider: @deprecated
   - row_cache_keys_to_save: @deprecated
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'keyspace', None, None, ), # 1
    (2, TType.STRING, 'name', None, None, ), # 2
    (3, TType.STRING, 'column_type', None, "Standard", ), # 3
    None, # 4
    (5, TType.STRING, 'comparator_type', None, "BytesType", ), # 5
    (6, TType.STRING, 'subcomparator_type', None, None, ), # 6
    None, # 7
    (8, TType.STRING, 'comment', None, None, ), # 8
    (9, TType.DOUBLE, 'row_cache_size', None, None, ), # 9
    None, # 10
    (11, TType.DOUBLE, 'key_cache_size', None, None, ), # 11
    (12, TType.DOUBLE, 'read_repair_chance', None, None, ), # 12
    (13, TType.LIST, 'column_metadata', (TType.STRUCT,(ColumnDef, ColumnDef.thrift_spec)), None, ), # 13
    (14, TType.I32, 'gc_grace_seconds', None, None, ), # 14
    (15, TType.STRING, 'default_validation_class', None, None, ), # 15
    (16, TType.I32, 'id', None, None, ), # 16
    (17, TType.I32, 'min_compaction_threshold', None, None, ), # 17
    (18, TType.I32, 'max_compaction_threshold', None, None, ), # 18
    (19, TType.I32, 'row_cache_save_period_in_seconds', None, None, ), # 19
    (20, TType.I32, 'key_cache_save_period_in_seconds', None, None, ), # 20
    (21, TType.I32, 'memtable_flush_after_mins', None, None, ), # 21
    (22, TType.I32, 'memtable_throughput_in_mb', None, None, ), # 22
    (23, TType.DOUBLE, 'memtable_operations_in_millions', None, None, ), # 23
    (24, TType.BOOL, 'replicate_on_write', None, None, ), # 24
    (25, TType.DOUBLE, 'merge_shards_chance', None, None, ), # 25
    (26, TType.STRING, 'key_validation_class', None, None, ), # 26
    (27, TType.STRING, 'row_cache_provider', None, None, ), # 27
    (28, TType.STRING, 'key_alias', None, None, ), # 28
    (29, TType.STRING, 'compaction_strategy', None, None, ), # 29
    (30, TType.MAP, 'compaction_strategy_options', (TType.STRING,None,TType.STRING,None), None, ), # 30
    (31, TType.I32, 'row_cache_keys_to_save', None, None, ), # 31
    (32, TType.MAP, 'compression_options', (TType.STRING,None,TType.STRING,None), None, ), # 32
    (33, TType.DOUBLE, 'bloom_filter_fp_chance', None, None, ), # 33
    (34, TType.STRING, 'caching', None, "keys_only", ), # 34
    (35, TType.LIST, 'column_aliases', (TType.STRING,None), None, ), # 35
    (36, TType.STRING, 'value_alias', None, None, ), # 36
    (37, TType.DOUBLE, 'dclocal_read_repair_chance', None, 0, ), # 37
  )

  def __init__(self, keyspace=None, name=None, column_type=thrift_spec[3][4], comparator_type=thrift_spec[5][4], subcomparator_type=None, comment=None, read_repair_chance=None, column_metadata=None, gc_grace_seconds=None, default_validation_class=None, id=None, min_compaction_threshold=None, max_compaction_threshold=None, replicate_on_write=None, key_validation_class=None, key_alias=None, compaction_strategy=None, compaction_strategy_options=None, compression_options=None, bloom_filter_fp_chance=None, caching=thrift_spec[34][4], column_aliases=None, value_alias=None, dclocal_read_repair_chance=thrift_spec[37][4], row_cache_size=None, key_cache_size=None, row_cache_save_period_in_seconds=None, key_cache_save_period_in_seconds=None, memtable_flush_after_mins=None, memtable_throughput_in_mb=None, memtable_operations_in_millions=None, merge_shards_chance=None, row_cache_provider=None, row_cache_keys_to_save=None,):
    self.keyspace = keyspace
    self.name = name
    self.column_type = column_type
    self.comparator_type = comparator_type
    self.subcomparator_type = subcomparator_type
    self.comment = comment
    self.read_repair_chance = read_repair_chance
    self.column_metadata = column_metadata
    self.gc_grace_seconds = gc_grace_seconds
    self.default_validation_class = default_validation_class
    self.id = id
    self.min_compaction_threshold = min_compaction_threshold
    self.max_compaction_threshold = max_compaction_threshold
    self.replicate_on_write = replicate_on_write
    self.key_validation_class = key_validation_class
    self.key_alias = key_alias
    self.compaction_strategy = compaction_strategy
    self.compaction_strategy_options = compaction_strategy_options
    self.compression_options = compression_options
    self.bloom_filter_fp_chance = bloom_filter_fp_chance
    self.caching = caching
    self.column_aliases = column_aliases
    self.value_alias = value_alias
    self.dclocal_read_repair_chance = dclocal_read_repair_chance
    self.row_cache_size = row_cache_size
    self.key_cache_size = key_cache_size
    self.row_cache_save_period_in_seconds = row_cache_save_period_in_seconds
    self.key_cache_save_period_in_seconds = key_cache_save_period_in_seconds
    self.memtable_flush_after_mins = memtable_flush_after_mins
    self.memtable_throughput_in_mb = memtable_throughput_in_mb
    self.memtable_operations_in_millions = memtable_operations_in_millions
    self.merge_shards_chance = merge_shards_chance
    self.row_cache_provider = row_cache_provider
    self.row_cache_keys_to_save = row_cache_keys_to_save

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.keyspace = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.column_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.STRING:
          self.comparator_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 6:
        if ftype == TType.STRING:
          self.subcomparator_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 8:
        if ftype == TType.STRING:
          self.comment = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 12:
        if ftype == TType.DOUBLE:
          self.read_repair_chance = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 13:
        if ftype == TType.LIST:
          self.column_metadata = []
          (_etype84, _size81) = iprot.readListBegin()
          for _i85 in xrange(_size81):
            _elem86 = ColumnDef()
            _elem86.read(iprot)
            self.column_metadata.append(_elem86)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 14:
        if ftype == TType.I32:
          self.gc_grace_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 15:
        if ftype == TType.STRING:
          self.default_validation_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 16:
        if ftype == TType.I32:
          self.id = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 17:
        if ftype == TType.I32:
          self.min_compaction_threshold = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 18:
        if ftype == TType.I32:
          self.max_compaction_threshold = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 24:
        if ftype == TType.BOOL:
          self.replicate_on_write = iprot.readBool();
        else:
          iprot.skip(ftype)
      elif fid == 26:
        if ftype == TType.STRING:
          self.key_validation_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 28:
        if ftype == TType.STRING:
          self.key_alias = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 29:
        if ftype == TType.STRING:
          self.compaction_strategy = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 30:
        if ftype == TType.MAP:
          self.compaction_strategy_options = {}
          (_ktype88, _vtype89, _size87 ) = iprot.readMapBegin() 
          for _i91 in xrange(_size87):
            _key92 = iprot.readString();
            _val93 = iprot.readString();
            self.compaction_strategy_options[_key92] = _val93
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 32:
        if ftype == TType.MAP:
          self.compression_options = {}
          (_ktype95, _vtype96, _size94 ) = iprot.readMapBegin() 
          for _i98 in xrange(_size94):
            _key99 = iprot.readString();
            _val100 = iprot.readString();
            self.compression_options[_key99] = _val100
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 33:
        if ftype == TType.DOUBLE:
          self.bloom_filter_fp_chance = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 34:
        if ftype == TType.STRING:
          self.caching = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 35:
        if ftype == TType.LIST:
          self.column_aliases = []
          (_etype104, _size101) = iprot.readListBegin()
          for _i105 in xrange(_size101):
            _elem106 = iprot.readString();
            self.column_aliases.append(_elem106)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 36:
        if ftype == TType.STRING:
          self.value_alias = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 37:
        if ftype == TType.DOUBLE:
          self.dclocal_read_repair_chance = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 9:
        if ftype == TType.DOUBLE:
          self.row_cache_size = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 11:
        if ftype == TType.DOUBLE:
          self.key_cache_size = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 19:
        if ftype == TType.I32:
          self.row_cache_save_period_in_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 20:
        if ftype == TType.I32:
          self.key_cache_save_period_in_seconds = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 21:
        if ftype == TType.I32:
          self.memtable_flush_after_mins = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 22:
        if ftype == TType.I32:
          self.memtable_throughput_in_mb = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 23:
        if ftype == TType.DOUBLE:
          self.memtable_operations_in_millions = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 25:
        if ftype == TType.DOUBLE:
          self.merge_shards_chance = iprot.readDouble();
        else:
          iprot.skip(ftype)
      elif fid == 27:
        if ftype == TType.STRING:
          self.row_cache_provider = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 31:
        if ftype == TType.I32:
          self.row_cache_keys_to_save = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CfDef')
    if self.keyspace is not None:
      oprot.writeFieldBegin('keyspace', TType.STRING, 1)
      oprot.writeString(self.keyspace)
      oprot.writeFieldEnd()
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 2)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.column_type is not None:
      oprot.writeFieldBegin('column_type', TType.STRING, 3)
      oprot.writeString(self.column_type)
      oprot.writeFieldEnd()
    if self.comparator_type is not None:
      oprot.writeFieldBegin('comparator_type', TType.STRING, 5)
      oprot.writeString(self.comparator_type)
      oprot.writeFieldEnd()
    if self.subcomparator_type is not None:
      oprot.writeFieldBegin('subcomparator_type', TType.STRING, 6)
      oprot.writeString(self.subcomparator_type)
      oprot.writeFieldEnd()
    if self.comment is not None:
      oprot.writeFieldBegin('comment', TType.STRING, 8)
      oprot.writeString(self.comment)
      oprot.writeFieldEnd()
    if self.row_cache_size is not None:
      oprot.writeFieldBegin('row_cache_size', TType.DOUBLE, 9)
      oprot.writeDouble(self.row_cache_size)
      oprot.writeFieldEnd()
    if self.key_cache_size is not None:
      oprot.writeFieldBegin('key_cache_size', TType.DOUBLE, 11)
      oprot.writeDouble(self.key_cache_size)
      oprot.writeFieldEnd()
    if self.read_repair_chance is not None:
      oprot.writeFieldBegin('read_repair_chance', TType.DOUBLE, 12)
      oprot.writeDouble(self.read_repair_chance)
      oprot.writeFieldEnd()
    if self.column_metadata is not None:
      oprot.writeFieldBegin('column_metadata', TType.LIST, 13)
      oprot.writeListBegin(TType.STRUCT, len(self.column_metadata))
      for iter107 in self.column_metadata:
        iter107.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.gc_grace_seconds is not None:
      oprot.writeFieldBegin('gc_grace_seconds', TType.I32, 14)
      oprot.writeI32(self.gc_grace_seconds)
      oprot.writeFieldEnd()
    if self.default_validation_class is not None:
      oprot.writeFieldBegin('default_validation_class', TType.STRING, 15)
      oprot.writeString(self.default_validation_class)
      oprot.writeFieldEnd()
    if self.id is not None:
      oprot.writeFieldBegin('id', TType.I32, 16)
      oprot.writeI32(self.id)
      oprot.writeFieldEnd()
    if self.min_compaction_threshold is not None:
      oprot.writeFieldBegin('min_compaction_threshold', TType.I32, 17)
      oprot.writeI32(self.min_compaction_threshold)
      oprot.writeFieldEnd()
    if self.max_compaction_threshold is not None:
      oprot.writeFieldBegin('max_compaction_threshold', TType.I32, 18)
      oprot.writeI32(self.max_compaction_threshold)
      oprot.writeFieldEnd()
    if self.row_cache_save_period_in_seconds is not None:
      oprot.writeFieldBegin('row_cache_save_period_in_seconds', TType.I32, 19)
      oprot.writeI32(self.row_cache_save_period_in_seconds)
      oprot.writeFieldEnd()
    if self.key_cache_save_period_in_seconds is not None:
      oprot.writeFieldBegin('key_cache_save_period_in_seconds', TType.I32, 20)
      oprot.writeI32(self.key_cache_save_period_in_seconds)
      oprot.writeFieldEnd()
    if self.memtable_flush_after_mins is not None:
      oprot.writeFieldBegin('memtable_flush_after_mins', TType.I32, 21)
      oprot.writeI32(self.memtable_flush_after_mins)
      oprot.writeFieldEnd()
    if self.memtable_throughput_in_mb is not None:
      oprot.writeFieldBegin('memtable_throughput_in_mb', TType.I32, 22)
      oprot.writeI32(self.memtable_throughput_in_mb)
      oprot.writeFieldEnd()
    if self.memtable_operations_in_millions is not None:
      oprot.writeFieldBegin('memtable_operations_in_millions', TType.DOUBLE, 23)
      oprot.writeDouble(self.memtable_operations_in_millions)
      oprot.writeFieldEnd()
    if self.replicate_on_write is not None:
      oprot.writeFieldBegin('replicate_on_write', TType.BOOL, 24)
      oprot.writeBool(self.replicate_on_write)
      oprot.writeFieldEnd()
    if self.merge_shards_chance is not None:
      oprot.writeFieldBegin('merge_shards_chance', TType.DOUBLE, 25)
      oprot.writeDouble(self.merge_shards_chance)
      oprot.writeFieldEnd()
    if self.key_validation_class is not None:
      oprot.writeFieldBegin('key_validation_class', TType.STRING, 26)
      oprot.writeString(self.key_validation_class)
      oprot.writeFieldEnd()
    if self.row_cache_provider is not None:
      oprot.writeFieldBegin('row_cache_provider', TType.STRING, 27)
      oprot.writeString(self.row_cache_provider)
      oprot.writeFieldEnd()
    if self.key_alias is not None:
      oprot.writeFieldBegin('key_alias', TType.STRING, 28)
      oprot.writeString(self.key_alias)
      oprot.writeFieldEnd()
    if self.compaction_strategy is not None:
      oprot.writeFieldBegin('compaction_strategy', TType.STRING, 29)
      oprot.writeString(self.compaction_strategy)
      oprot.writeFieldEnd()
    if self.compaction_strategy_options is not None:
      oprot.writeFieldBegin('compaction_strategy_options', TType.MAP, 30)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.compaction_strategy_options))
      for kiter108,viter109 in self.compaction_strategy_options.items():
        oprot.writeString(kiter108)
        oprot.writeString(viter109)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.row_cache_keys_to_save is not None:
      oprot.writeFieldBegin('row_cache_keys_to_save', TType.I32, 31)
      oprot.writeI32(self.row_cache_keys_to_save)
      oprot.writeFieldEnd()
    if self.compression_options is not None:
      oprot.writeFieldBegin('compression_options', TType.MAP, 32)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.compression_options))
      for kiter110,viter111 in self.compression_options.items():
        oprot.writeString(kiter110)
        oprot.writeString(viter111)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.bloom_filter_fp_chance is not None:
      oprot.writeFieldBegin('bloom_filter_fp_chance', TType.DOUBLE, 33)
      oprot.writeDouble(self.bloom_filter_fp_chance)
      oprot.writeFieldEnd()
    if self.caching is not None:
      oprot.writeFieldBegin('caching', TType.STRING, 34)
      oprot.writeString(self.caching)
      oprot.writeFieldEnd()
    if self.column_aliases is not None:
      oprot.writeFieldBegin('column_aliases', TType.LIST, 35)
      oprot.writeListBegin(TType.STRING, len(self.column_aliases))
      for iter112 in self.column_aliases:
        oprot.writeString(iter112)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.value_alias is not None:
      oprot.writeFieldBegin('value_alias', TType.STRING, 36)
      oprot.writeString(self.value_alias)
      oprot.writeFieldEnd()
    if self.dclocal_read_repair_chance is not None:
      oprot.writeFieldBegin('dclocal_read_repair_chance', TType.DOUBLE, 37)
      oprot.writeDouble(self.dclocal_read_repair_chance)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.keyspace is None:
      raise TProtocol.TProtocolException(message='Required field keyspace is unset!')
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class KsDef:
  """
  Attributes:
   - name
   - strategy_class
   - strategy_options
   - replication_factor: @deprecated, ignored
   - cf_defs
   - durable_writes
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'name', None, None, ), # 1
    (2, TType.STRING, 'strategy_class', None, None, ), # 2
    (3, TType.MAP, 'strategy_options', (TType.STRING,None,TType.STRING,None), None, ), # 3
    (4, TType.I32, 'replication_factor', None, None, ), # 4
    (5, TType.LIST, 'cf_defs', (TType.STRUCT,(CfDef, CfDef.thrift_spec)), None, ), # 5
    (6, TType.BOOL, 'durable_writes', None, True, ), # 6
  )

  def __init__(self, name=None, strategy_class=None, strategy_options=None, replication_factor=None, cf_defs=None, durable_writes=thrift_spec[6][4],):
    self.name = name
    self.strategy_class = strategy_class
    self.strategy_options = strategy_options
    self.replication_factor = replication_factor
    self.cf_defs = cf_defs
    self.durable_writes = durable_writes

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.name = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.STRING:
          self.strategy_class = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.MAP:
          self.strategy_options = {}
          (_ktype114, _vtype115, _size113 ) = iprot.readMapBegin() 
          for _i117 in xrange(_size113):
            _key118 = iprot.readString();
            _val119 = iprot.readString();
            self.strategy_options[_key118] = _val119
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.I32:
          self.replication_factor = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 5:
        if ftype == TType.LIST:
          self.cf_defs = []
          (_etype123, _size120) = iprot.readListBegin()
          for _i124 in xrange(_size120):
            _elem125 = CfDef()
            _elem125.read(iprot)
            self.cf_defs.append(_elem125)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 6:
        if ftype == TType.BOOL:
          self.durable_writes = iprot.readBool();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('KsDef')
    if self.name is not None:
      oprot.writeFieldBegin('name', TType.STRING, 1)
      oprot.writeString(self.name)
      oprot.writeFieldEnd()
    if self.strategy_class is not None:
      oprot.writeFieldBegin('strategy_class', TType.STRING, 2)
      oprot.writeString(self.strategy_class)
      oprot.writeFieldEnd()
    if self.strategy_options is not None:
      oprot.writeFieldBegin('strategy_options', TType.MAP, 3)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.strategy_options))
      for kiter126,viter127 in self.strategy_options.items():
        oprot.writeString(kiter126)
        oprot.writeString(viter127)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.replication_factor is not None:
      oprot.writeFieldBegin('replication_factor', TType.I32, 4)
      oprot.writeI32(self.replication_factor)
      oprot.writeFieldEnd()
    if self.cf_defs is not None:
      oprot.writeFieldBegin('cf_defs', TType.LIST, 5)
      oprot.writeListBegin(TType.STRUCT, len(self.cf_defs))
      for iter128 in self.cf_defs:
        iter128.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.durable_writes is not None:
      oprot.writeFieldBegin('durable_writes', TType.BOOL, 6)
      oprot.writeBool(self.durable_writes)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name is None:
      raise TProtocol.TProtocolException(message='Required field name is unset!')
    if self.strategy_class is None:
      raise TProtocol.TProtocolException(message='Required field strategy_class is unset!')
    if self.cf_defs is None:
      raise TProtocol.TProtocolException(message='Required field cf_defs is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CqlRow:
  """
  Row returned from a CQL query

  Attributes:
   - key
   - columns
  """

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'key', None, None, ), # 1
    (2, TType.LIST, 'columns', (TType.STRUCT,(Column, Column.thrift_spec)), None, ), # 2
  )

  def __init__(self, key=None, columns=None,):
    self.key = key
    self.columns = columns

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.STRING:
          self.key = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.columns = []
          (_etype132, _size129) = iprot.readListBegin()
          for _i133 in xrange(_size129):
            _elem134 = Column()
            _elem134.read(iprot)
            self.columns.append(_elem134)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CqlRow')
    if self.key is not None:
      oprot.writeFieldBegin('key', TType.STRING, 1)
      oprot.writeString(self.key)
      oprot.writeFieldEnd()
    if self.columns is not None:
      oprot.writeFieldBegin('columns', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.columns))
      for iter135 in self.columns:
        iter135.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.key is None:
      raise TProtocol.TProtocolException(message='Required field key is unset!')
    if self.columns is None:
      raise TProtocol.TProtocolException(message='Required field columns is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CqlMetadata:
  """
  Attributes:
   - name_types
   - value_types
   - default_name_type
   - default_value_type
  """

  thrift_spec = (
    None, # 0
    (1, TType.MAP, 'name_types', (TType.STRING,None,TType.STRING,None), None, ), # 1
    (2, TType.MAP, 'value_types', (TType.STRING,None,TType.STRING,None), None, ), # 2
    (3, TType.STRING, 'default_name_type', None, None, ), # 3
    (4, TType.STRING, 'default_value_type', None, None, ), # 4
  )

  def __init__(self, name_types=None, value_types=None, default_name_type=None, default_value_type=None,):
    self.name_types = name_types
    self.value_types = value_types
    self.default_name_type = default_name_type
    self.default_value_type = default_value_type

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.MAP:
          self.name_types = {}
          (_ktype137, _vtype138, _size136 ) = iprot.readMapBegin() 
          for _i140 in xrange(_size136):
            _key141 = iprot.readString();
            _val142 = iprot.readString();
            self.name_types[_key141] = _val142
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.MAP:
          self.value_types = {}
          (_ktype144, _vtype145, _size143 ) = iprot.readMapBegin() 
          for _i147 in xrange(_size143):
            _key148 = iprot.readString();
            _val149 = iprot.readString();
            self.value_types[_key148] = _val149
          iprot.readMapEnd()
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.STRING:
          self.default_name_type = iprot.readString();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRING:
          self.default_value_type = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CqlMetadata')
    if self.name_types is not None:
      oprot.writeFieldBegin('name_types', TType.MAP, 1)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.name_types))
      for kiter150,viter151 in self.name_types.items():
        oprot.writeString(kiter150)
        oprot.writeString(viter151)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.value_types is not None:
      oprot.writeFieldBegin('value_types', TType.MAP, 2)
      oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.value_types))
      for kiter152,viter153 in self.value_types.items():
        oprot.writeString(kiter152)
        oprot.writeString(viter153)
      oprot.writeMapEnd()
      oprot.writeFieldEnd()
    if self.default_name_type is not None:
      oprot.writeFieldBegin('default_name_type', TType.STRING, 3)
      oprot.writeString(self.default_name_type)
      oprot.writeFieldEnd()
    if self.default_value_type is not None:
      oprot.writeFieldBegin('default_value_type', TType.STRING, 4)
      oprot.writeString(self.default_value_type)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.name_types is None:
      raise TProtocol.TProtocolException(message='Required field name_types is unset!')
    if self.value_types is None:
      raise TProtocol.TProtocolException(message='Required field value_types is unset!')
    if self.default_name_type is None:
      raise TProtocol.TProtocolException(message='Required field default_name_type is unset!')
    if self.default_value_type is None:
      raise TProtocol.TProtocolException(message='Required field default_value_type is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CqlResult:
  """
  Attributes:
   - type
   - rows
   - num
   - schema
  """

  thrift_spec = (
    None, # 0
    (1, TType.I32, 'type', None, None, ), # 1
    (2, TType.LIST, 'rows', (TType.STRUCT,(CqlRow, CqlRow.thrift_spec)), None, ), # 2
    (3, TType.I32, 'num', None, None, ), # 3
    (4, TType.STRUCT, 'schema', (CqlMetadata, CqlMetadata.thrift_spec), None, ), # 4
  )

  def __init__(self, type=None, rows=None, num=None, schema=None,):
    self.type = type
    self.rows = rows
    self.num = num
    self.schema = schema

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.I32:
          self.type = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.LIST:
          self.rows = []
          (_etype157, _size154) = iprot.readListBegin()
          for _i158 in xrange(_size154):
            _elem159 = CqlRow()
            _elem159.read(iprot)
            self.rows.append(_elem159)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.I32:
          self.num = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 4:
        if ftype == TType.STRUCT:
          self.schema = CqlMetadata()
          self.schema.read(iprot)
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CqlResult')
    if self.type is not None:
      oprot.writeFieldBegin('type', TType.I32, 1)
      oprot.writeI32(self.type)
      oprot.writeFieldEnd()
    if self.rows is not None:
      oprot.writeFieldBegin('rows', TType.LIST, 2)
      oprot.writeListBegin(TType.STRUCT, len(self.rows))
      for iter160 in self.rows:
        iter160.write(oprot)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    if self.num is not None:
      oprot.writeFieldBegin('num', TType.I32, 3)
      oprot.writeI32(self.num)
      oprot.writeFieldEnd()
    if self.schema is not None:
      oprot.writeFieldBegin('schema', TType.STRUCT, 4)
      self.schema.write(oprot)
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.type is None:
      raise TProtocol.TProtocolException(message='Required field type is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class CqlPreparedResult:
  """
  Attributes:
   - itemId
   - count
   - variable_types
  """

  thrift_spec = (
    None, # 0
    (1, TType.I32, 'itemId', None, None, ), # 1
    (2, TType.I32, 'count', None, None, ), # 2
    (3, TType.LIST, 'variable_types', (TType.STRING,None), None, ), # 3
  )

  def __init__(self, itemId=None, count=None, variable_types=None,):
    self.itemId = itemId
    self.count = count
    self.variable_types = variable_types

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
      if ftype == TType.STOP:
        break
      if fid == 1:
        if ftype == TType.I32:
          self.itemId = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 2:
        if ftype == TType.I32:
          self.count = iprot.readI32();
        else:
          iprot.skip(ftype)
      elif fid == 3:
        if ftype == TType.LIST:
          self.variable_types = []
          (_etype164, _size161) = iprot.readListBegin()
          for _i165 in xrange(_size161):
            _elem166 = iprot.readString();
            self.variable_types.append(_elem166)
          iprot.readListEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.writeStructBegin('CqlPreparedResult')
    if self.itemId is not None:
      oprot.writeFieldBegin('itemId', TType.I32, 1)
      oprot.writeI32(self.itemId)
      oprot.writeFieldEnd()
    if self.count is not None:
      oprot.writeFieldBegin('count', TType.I32, 2)
      oprot.writeI32(self.count)
      oprot.writeFieldEnd()
    if self.variable_types is not None:
      oprot.writeFieldBegin('variable_types', TType.LIST, 3)
      oprot.writeListBegin(TType.STRING, len(self.variable_types))
      for iter167 in self.variable_types:
        oprot.writeString(iter167)
      oprot.writeListEnd()
      oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()

  def validate(self):
    if self.itemId is None:
      raise TProtocol.TProtocolException(message='Required field itemId is unset!')
    if self.count is None:
      raise TProtocol.TProtocolException(message='Required field count is unset!')
    return


  def __repr__(self):
    L = ['%s=%r' % (key, value)
      for key, value in self.__dict__.iteritems()]
    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

########NEW FILE########
__FILENAME__ = concurrent_schema_changes_test
import time
import os
import pprint
import glob
from threading import Thread

from dtest import Tester, debug
from ccmlib.node import Node

import cql

def wait(delay=2):
    """
    An abstraction so that the sleep delays can easily be modified.
    """
    time.sleep(delay)

class TestConcurrentSchemaChanges(Tester):

    def __init__(self, *argv, **kwargs):
        super(TestConcurrentSchemaChanges, self).__init__(*argv, **kwargs)
        self.allow_log_errors = True

    def prepare_for_changes(self, cursor, namespace='ns1'):
        """
        prepares for schema changes by creating a keyspace and column family.
        """
        debug("prepare_for_changes() " + str(namespace))
        # create a keyspace that will be used
        self.create_ks(cursor, "ks_%s" % namespace, 2)
        cursor.execute('USE ks_%s' % namespace)

        # create a column family with an index and a row of data
        query = """
            CREATE TABLE cf_%s (
                col1 text PRIMARY KEY,
                col2 text,
                col3 text
            );
        """ % namespace
        cursor.execute(query)
        wait(1)
        cursor.execute("INSERT INTO cf_%s (col1, col2, col3) VALUES ('a', 'b', 'c');" 
                % namespace)

        # create an index
        cursor.execute("CREATE INDEX index_%s ON cf_%s(col2)"%(namespace, namespace))

        # create a column family that can be deleted later.
        query = """
            CREATE TABLE cf2_%s (
                col1 uuid PRIMARY KEY,
                col2 text,
                col3 text
            );
        """ % namespace
        cursor.execute(query)

        # make a keyspace that can be deleted
        self.create_ks(cursor, "ks2_%s" % namespace, 2)

    def make_schema_changes(self, cursor, namespace='ns1'):
        """
        makes a heap of changes.

        create keyspace
        drop keyspace
        create column family
        drop column family
        update column family
        drop index
        create index (modify column family and add a key)
        rebuild index (via jmx)
        set default_validation_class
        """
        debug("make_schema_changes() " + str(namespace))
        cursor.execute('USE ks_%s' % namespace)
        # drop keyspace
        cursor.execute('DROP KEYSPACE ks2_%s' % namespace)
        wait(2)

        # create keyspace
        self.create_ks(cursor, "ks3_%s" % namespace, 2)
        cursor.execute('USE ks_%s' % namespace)

        wait(2)
        # drop column family
        cursor.execute("DROP COLUMNFAMILY cf2_%s" % namespace)

        # create column family
        query = """
            CREATE TABLE cf3_%s (
                col1 uuid PRIMARY KEY,
                col2 text,
                col3 text,
                col4 text
            );
        """ % (namespace)
        cursor.execute(query)

        # alter column family
        query = """
            ALTER COLUMNFAMILY cf_%s
            ADD col4 text;
        """ % namespace
        cursor.execute(query)

        # add index
        cursor.execute("CREATE INDEX index2_%s ON cf_%s(col3)"%(namespace, namespace))

        # remove an index
        cursor.execute("DROP INDEX index_%s" % namespace)


    def validate_schema_consistent(self, node):
        """ Makes sure that there is only one schema """
        debug("validate_schema_consistent() " + node.name)

        host, port = node.network_interfaces['thrift']
        conn = cql.connect(host, port, keyspace=None)
        schemas = conn.client.describe_schema_versions()
        num_schemas = len([ss for ss in schemas.keys() if ss != 'UNREACHABLE'])
        assert num_schemas == 1, "There were multiple schema versions: " + pprint.pformat(schemas)


    def basic_test(self):
        """
        make sevaral schema changes on the same node.
        """
        debug("basic_test()")

        cluster = self.cluster
        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        wait(2)
        cursor = self.cql_connection(node1).cursor()

        self.prepare_for_changes(cursor, namespace='ns1')

        self.make_schema_changes(cursor, namespace='ns1')

    
    def changes_to_different_nodes_test(self):
        debug("changes_to_different_nodes_test()")
        cluster = self.cluster
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        wait(2)
        cursor = self.cql_connection(node1).cursor()
        self.prepare_for_changes(cursor, namespace='ns1')
        self.make_schema_changes(cursor, namespace='ns1')
        wait(3)
        self.validate_schema_consistent(node1)

        # wait for changes to get to the first node
        wait(20)

        cursor = self.cql_connection(node2).cursor()
        self.prepare_for_changes(cursor, namespace='ns2')
        self.make_schema_changes(cursor, namespace='ns2')
        wait(3)
        self.validate_schema_consistent(node1)
        # check both, just because we can
        self.validate_schema_consistent(node2)

    
    def changes_while_node_down_test(self):
        """
        makes schema changes while a node is down.
        Make schema changes to node 1 while node 2 is down. 
        Then bring up 2 and make sure it gets the changes. 
        """
        debug("changes_while_node_down_test()")
        cluster = self.cluster
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        wait(2)
        cursor = self.cql_connection(node2).cursor()

        self.prepare_for_changes(cursor, namespace='ns2')
        node1.stop()
        wait(2)
        self.make_schema_changes(cursor, namespace='ns2')
        wait(2)
        node2.stop()
        wait(2)
        node1.start()
        node2.start()
        wait(20)
        self.validate_schema_consistent(node1)


    def changes_while_node_toggle_test(self):
        """
        makes schema changes while a node is down.

        Bring down 1 and change 2. 
        Bring down 2, bring up 1, and finally bring up 2. 
        1 should get the changes. 
        """
        debug("changes_while_node_toggle_test()")
        cluster = self.cluster
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        wait(2)
        cursor = self.cql_connection(node2).cursor()

        self.prepare_for_changes(cursor, namespace='ns2')
        node1.stop()
        wait(2)
        self.make_schema_changes(cursor, namespace='ns2')
        wait(2)
        node2.stop()
        wait(2)
        node1.start()
        node2.start()
        wait(20)
        self.validate_schema_consistent(node1)


    def decommission_node_test(self):
        debug("decommission_node_test()")
        cluster = self.cluster

        cluster.populate(1)
        # create and add a new node, I must not be a seed, otherwise
        # we get schema disagreement issues for awhile after decommissioning it.
        node2 = Node('node2', 
                    cluster,
                    True,
                    ('127.0.0.2', 9160),
                    ('127.0.0.2', 7000),
                    '7200',
                     '0',
                    None)
        cluster.add(node2, False)

        [node1, node2] = cluster.nodelist()
        node1.start()
        node2.start()
        wait(2)

        cursor = self.cql_connection(node1).cursor()
        self.prepare_for_changes(cursor)

        node2.decommission()
        wait(30)

        self.validate_schema_consistent(node1)
        self.make_schema_changes(cursor, namespace='ns1')

        # create and add a new node
        node3 = Node('node3', 
                    cluster,
                    True,
                    ('127.0.0.3', 9160),
                    ('127.0.0.3', 7000),
                    '7300',
                     '0',
                    None)

        cluster.add(node3, True)
        node3.start()

        wait(30)
        self.validate_schema_consistent(node1)


    def snapshot_test(self):
        debug("snapshot_test()")
        cluster = self.cluster
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        wait(2)
        cursor = self.cql_connection(node1).cursor()
        self.prepare_for_changes(cursor, namespace='ns2')

        wait(2)
        cluster.flush()

        wait(2)
        node1.nodetool('snapshot -t testsnapshot')
        node2.nodetool('snapshot -t testsnapshot')

        wait(2)
        self.make_schema_changes(cursor, namespace='ns2')

        wait(2)

        cluster.stop()

        ### restore the snapshots ##
        # clear the commitlogs and data
        dirs = (    '%s/commitlogs' % node1.get_path(),
                    '%s/commitlogs' % node2.get_path(),
                    '%s/data/ks_ns2/cf_*/*' % node1.get_path(),
                    '%s/data/ks_ns2/cf_*/*' % node2.get_path(),
                )
        for dirr in dirs:
            for f in glob.glob(os.path.join(dirr)):
                if os.path.isfile(f):
                    os.unlink(f)

        # copy the snapshot. TODO: This could be replaced with the creation of hard links.
        os.system('cp -p %s/data/ks_ns2/cf_*/snapshots/testsnapshot/* %s/data/ks_ns2/cf_*/' % (node1.get_path(), node1.get_path()))
        os.system('cp -p %s/data/ks_ns2/cf_*/snapshots/testsnapshot/* %s/data/ks_ns2/cf_*/' % (node2.get_path(), node2.get_path()))

        # restart the cluster
        cluster.start()

        wait(2)
        self.validate_schema_consistent(node1)



    def load_test(self):
        """
        apply schema changes while the cluster is under load.
        """
        debug("load_test()")

        cluster = self.cluster
        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        version = cluster.version()
        wait(2)
        cursor = self.cql_connection(node1).cursor()

        def stress(args=[]):
            debug("Stressing")
            node1.stress(args)
            debug("Done Stressing")

        def compact():
            debug("Compacting...")
            node1.nodetool('compact')
            debug("Done Compacting.")

        # put some data into the cluster
        if version < "2.1":
            stress(['--num-keys=30000'])
        else:
            stress(['write', 'n=30000', '-rate', 'threads=8'])

        # now start stressing and compacting at the same time
        tcompact = Thread(target=compact)
        tcompact.start()
        wait(1)

        # now the cluster is under a lot of load. Make some schema changes.
        if version >= "1.2":
            cursor.execute('USE "Keyspace1"')
            wait(1)
            cursor.execute('DROP COLUMNFAMILY "Standard1"')
            wait(3)
            cursor.execute('CREATE COLUMNFAMILY "Standard1" (KEY text PRIMARY KEY)')
        else:
            cursor.execute('USE Keyspace1')
            wait(1)
            cursor.execute('DROP COLUMNFAMILY Standard1')
            wait(3)
            cursor.execute('CREATE COLUMNFAMILY Standard1 (KEY text PRIMARY KEY)')

        tcompact.join()



########NEW FILE########
__FILENAME__ = configuration_test
from dtest import Tester
from assertions import *

import re
from ccmlib.cluster import Cluster
import ast

class TestConfiguration(Tester):

    def compression_chunk_length_test(self):
        """ Verify the setting of compression chunk_length [#3558]"""
        cluster = self.cluster

        cluster.populate(1).start()
        node = cluster.nodelist()[0]
        cursor = self.patient_cql_connection(node).cursor()
        self.create_ks(cursor, 'ks', 1)

        create_table_query = "CREATE TABLE test_table (row varchar, name varchar, value int, PRIMARY KEY (row, name));"
        alter_chunk_len_query = "ALTER TABLE test_table WITH compression = {{'sstable_compression' : 'SnappyCompressor', 'chunk_length_kb' : {chunk_length}}};"

        cursor.execute( create_table_query, 1 )

        cursor.execute( alter_chunk_len_query.format(chunk_length=32) )
        self._check_chunk_length( cursor, 32 )

        cursor.execute( alter_chunk_len_query.format(chunk_length=64) )
        self._check_chunk_length( cursor, 64 )

        
    def _check_chunk_length(self, cursor, value):
        describe_table_query = "SELECT * FROM system.schema_columnfamilies WHERE keyspace_name='ks' AND columnfamily_name='test_table';"
        cursor.execute( describe_table_query )
        results = cursor.fetchall()[0]
        #Now extract the param list
        params = ''
        for result in results:
            if 'sstable_compression' in str(result):
                params = result

        assert params is not '', "Looking for a row with the string 'sstable_compression' in system.schema_columnfamilies, but could not find it."

        params = ast.literal_eval( params )
        chunk_length = int( params['chunk_length_kb'] )

        assert chunk_length == value, "Expected chunk_length: %s.  We got: %s" % (value, chunk_length)

########NEW FILE########
__FILENAME__ = consistency_test
import time

from dtest import Tester, debug, DISABLE_VNODES
from assertions import assert_unavailable
from tools import (create_c1c2_table, insert_c1c2, query_c1c2, retry_till_success,
                   insert_columns)

class TestConsistency(Tester):

    def quorum_quorum_test(self):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor1 = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor1, 'ks', 3)
        create_c1c2_table(self, cursor1)

        cursor2 = self.patient_cql_connection(node2, 'ks').cursor()

        # insert and get at CL.QUORUM
        for n in xrange(0, 100):
            insert_c1c2(cursor1, n, "QUORUM")
            query_c1c2(cursor2, n, "QUORUM")


        # shutdown a node an test again
        node3.stop(wait_other_notice=True)
        for n in xrange(100, 200):
            insert_c1c2(cursor1, n, "QUORUM")
            query_c1c2(cursor2, n, "QUORUM")

        # shutdown another node and test we get unavailabe exception
        node2.stop(wait_other_notice=True)
        assert_unavailable(insert_c1c2, cursor1, 200, "QUORUM")

    def all_all_test(self):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor1 = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor1, 'ks', 3)
        create_c1c2_table(self, cursor1)

        cursor2 = self.patient_cql_connection(node2, 'ks').cursor()

        # insert and get at CL.ALL
        for n in xrange(0, 100):
            insert_c1c2(cursor1, n, "ALL")
            query_c1c2(cursor2, n, "ALL")

        # shutdown one node and test we get unavailabe exception
        node3.stop(wait_other_notice=True)
        assert_unavailable(insert_c1c2, cursor1, 100, "ALL")

    def one_one_test(self):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor1 = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor1, 'ks', 3)
        create_c1c2_table(self, cursor1)

        cursor2 = self.patient_cql_connection(node2, 'ks').cursor()

        # insert and get at CL.ONE
        for n in xrange(0, 100):
            insert_c1c2(cursor1, n, "ONE")
            retry_till_success(query_c1c2, cursor2, n, "ONE", timeout=5)

        # shutdown a node an test again
        node3.stop(wait_other_notice=True)
        for n in xrange(100, 200):
            insert_c1c2(cursor1, n, "ONE")
            retry_till_success(query_c1c2, cursor2, n, "ONE", timeout=5)

        # shutdown a second node an test again
        node2.stop(wait_other_notice=True)
        for n in xrange(200, 300):
            insert_c1c2(cursor1, n, "ONE")
            retry_till_success(query_c1c2, cursor1, n, "ONE", timeout=5)

    def one_all_test(self):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor1 = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor1, 'ks', 3)
        create_c1c2_table(self, cursor1)

        cursor2 = self.patient_cql_connection(node2, 'ks').cursor()

        # insert and get at CL.ONE
        for n in xrange(0, 100):
            insert_c1c2(cursor1, n, "ONE")
            query_c1c2(cursor2, n, "ALL")

        # shutdown a node an test again
        node3.stop(wait_other_notice=True)
        insert_c1c2(cursor1, 100, "ONE")
        assert_unavailable(query_c1c2, cursor2, 100, "ALL")

    def all_one_test(self):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor1 = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor1, 'ks', 3)
        create_c1c2_table(self, cursor1)

        cursor2 = self.patient_cql_connection(node2, 'ks').cursor()

        # insert and get at CL.ONE
        for n in xrange(0, 100):
            insert_c1c2(cursor1, n, "ALL")
            query_c1c2(cursor2, n, "ONE")

        # shutdown a node an test again
        node3.stop(wait_other_notice=True)
        assert_unavailable(insert_c1c2, cursor1, 100, "ALL")

    def short_read_test(self):
        cluster = self.cluster

        # Disable hinted handoff and set batch commit log so this doesn't
        # interfer with the test
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False}, batch_commitlog=True)

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()
        time.sleep(.5)

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', read_repair=0.0)
        # insert 9 columns in one row
        insert_columns(self, cursor, 0, 9)
        cursor.close()

        # Deleting 3 first columns with a different node dead each time
        self.stop_delete_and_restart(1, 0)
        self.stop_delete_and_restart(2, 1)
        self.stop_delete_and_restart(3, 2)

        # Query 3 firsts columns
        cursor = self.patient_cql_connection(node1, 'ks').cursor()
        if self.cluster.version() >= "1.2":
            cursor.execute('SELECT c, v FROM cf WHERE key=\'k0\' LIMIT 3', consistency_level="QUORUM")
            res = cursor.fetchall()
            assert len(res) == 3, 'Expecting 3 values, got %d (%s)' % (len(res), str(res))
            # value 0, 1 and 2 have been deleted
            for i in xrange(1, 4):
                assert res[i-1][1] == 'value%d' % (i+2), 'Expecting value%d, got %s (%s)' % (i+2, res[i-1][1], str(res))
        else:
            cursor.execute('SELECT FIRST 3 * FROM cf USING CONSISTENCY QUORUM WHERE key=k0')
            assert cursor.rowcount == 1
            res = cursor.fetchone()
            # the key is returned
            assert len(res) == 3, 'Expecting 3 values, got %d (%s)' % (len(res), str(res))
            # value 0, 1 and 2 have been deleted
            for i in xrange(1, 4):
                assert res[i] == 'value%d' % (i+2), 'Expecting value%d, got %s (%s)' % (i+2, res[i], str(res))

    def short_read_delete_test(self):
        """ Test short reads ultimately leaving no columns alive [#4000] """
        cluster = self.cluster

        # Disable hinted handoff and set batch commit log so this doesn't
        # interfer with the test
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False}, batch_commitlog=True)

        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        time.sleep(.5)

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', read_repair=0.0)
        # insert 2 columns in one row
        insert_columns(self, cursor, 0, 2)
        cursor.close()

        # Delete the row while first node is dead
        node1.flush()
        node1.stop(wait_other_notice=True)
        cursor = self.patient_cql_connection(node2, 'ks').cursor()
        if self.cluster.version() >= "1.2":
            cursor.execute('DELETE FROM cf WHERE key=\'k0\'', consistency_level="ONE")
        else:
            cursor.execute('DELETE FROM cf USING CONSISTENCY ONE WHERE key=k0')
        cursor.close()
        node1.start(wait_other_notice=True)
        time.sleep(.5)

        # Query first column
        cursor = self.patient_cql_connection(node1, 'ks').cursor()
        if self.cluster.version() >= "1.2":
            cursor.execute('SELECT c, v FROM cf WHERE key=\'k0\' LIMIT 1', consistency_level="QUORUM")
            res = cursor.fetchone()
            assert cursor.rowcount == 0, res
        else:
            cursor.execute('SELECT FIRST 1 * FROM cf USING CONSISTENCY QUORUM WHERE key=k0')
            assert cursor.rowcount == 1
            res = cursor.fetchone()
            assert len(res) == 0, 'Expecting no value, got %d (%s)' % (len(res), str(res))

    def hintedhandoff_test(self):
        cluster = self.cluster

        if DISABLE_VNODES:
            cluster.populate(2).start()
        else:
            tokens = cluster.balanced_tokens(2)
            cluster.populate(2, tokens=tokens).start()
        [node1, node2] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 2)
        create_c1c2_table(self, cursor)

        node2.stop(wait_other_notice=True)

        for n in xrange(0, 100):
            insert_c1c2(cursor, n, "ONE")

        log_mark = node1.mark_log()
        node2.start()
        node1.watch_log_for(["Finished hinted"], from_mark=log_mark, timeout=90)

        node1.stop(wait_other_notice=True)

        # Check node2 for all the keys that should have been delivered via HH
        cursor = self.patient_cql_connection(node2, keyspace='ks').cursor()
        for n in xrange(0, 100):
            query_c1c2(cursor, n, "ONE")

    def readrepair_test(self):
        cluster = self.cluster
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False})

        if DISABLE_VNODES:
            cluster.populate(2).start()
        else:
            tokens = cluster.balanced_tokens(2)
            cluster.populate(2, tokens=tokens).start()
        [node1, node2] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 2)
        create_c1c2_table(self, cursor, read_repair=1.0)

        node2.stop(wait_other_notice=True)

        for n in xrange(0, 10000):
            insert_c1c2(cursor, n, "ONE")

        node2.start(wait_other_notice=True)
        time.sleep(5)
       # query everything to cause RR
        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "QUORUM")

        node1.stop(wait_other_notice=True)

        # Check node2 for all the keys that should have been repaired
        cursor = self.patient_cql_connection(node2, keyspace='ks').cursor()
        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "ONE")

    def short_read_reversed_test(self):
        cluster = self.cluster

        # Disable hinted handoff and set batch commit log so this doesn't
        # interfer with the test
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False}, batch_commitlog=True)

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()
        time.sleep(.5)

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', read_repair=0.0)
        # insert 9 columns in one row
        insert_columns(self, cursor, 0, 9)
        cursor.close()

        # Deleting 3 last columns with a different node dead each time
        self.stop_delete_and_restart(1, 6)
        self.stop_delete_and_restart(2, 7)
        self.stop_delete_and_restart(3, 8)

        # Query 3 firsts columns
        cursor = self.patient_cql_connection(node1, 'ks').cursor()
        if self.cluster.version() >= "1.2":
            cursor.execute('SELECT c, v FROM cf WHERE key=\'k0\' ORDER BY c DESC LIMIT 3', consistency_level="QUORUM")
            res = cursor.fetchall()
            assert len(res) == 3, 'Expecting 3 values, got %d (%s)' % (len(res), str(res))
            # value 6, 7 and 8 have been deleted
            for i in xrange(0, 3):
                assert res[i][1] == 'value%d' % (5-i), 'Expecting value%d, got %s (%s)' % (5-i, res[i][1], str(res))
        else:
            cursor.execute('SELECT FIRST 3 REVERSED * FROM cf USING CONSISTENCY QUORUM WHERE key=k0')
            assert cursor.rowcount == 1
            res = cursor.fetchone()
            # the key is returned
            assert len(res) == 3, 'Expecting 3 values, got %d (%s)' % (len(res), str(res))
            # value 6, 7 and 8 have been deleted
            for i in xrange(0, 3):
                assert res[i] == 'value%d' % (5-i), 'Expecting value%d, got %s (%s)' % (5-i, res[i], str(res))

    def quorum_available_during_failure_test(self):
        CL = 'QUORUM'
        RF = 3

        debug("Creating a ring")
        cluster = self.cluster
        if DISABLE_VNODES:
            cluster.populate(3).start()
        else:
            tokens = cluster.balanced_tokens(3)
            cluster.populate(3, tokens=tokens).start()
        [node1, node2, node3] = cluster.nodelist()
        cluster.start()

        debug("Set to talk to node 2")
        cursor = self.patient_cql_connection(node2).cursor()
        self.create_ks(cursor, 'ks', RF)
        create_c1c2_table(self, cursor)

        debug("Generating some data")
        for n in xrange(100):
            insert_c1c2(cursor, n, CL)

        debug("Taking down node1")
        node1.stop(wait_other_notice=True)

        debug("Reading back data.")
        for n in xrange(100):
            query_c1c2(cursor, n, CL)

    def stop_delete_and_restart(self, node_number, column):
        to_stop = self.cluster.nodes["node%d" % node_number]
        next_node = self.cluster.nodes["node%d" % (((node_number + 1) % 3) + 1)]
        to_stop.flush()
        to_stop.stop(wait_other_notice=True)
        cursor = self.patient_cql_connection(next_node, 'ks').cursor()
        if self.cluster.version() >= "1.2":
            query = 'BEGIN BATCH '
            query = query + 'DELETE FROM cf WHERE key=\'k0\' AND c=\'c%06d\'; ' % column
            query = query + 'DELETE FROM cf WHERE key=\'k0\' AND c=\'c2\'; '
            query = query + 'APPLY BATCH;'
            cursor.execute(query, consistency_level="QUORUM")
        else:
            cursor.execute('DELETE c%06d, c2 FROM cf USING CONSISTENCY QUORUM WHERE key=k0' % column)
        cursor.close()
        to_stop.start(wait_other_notice=True)


########NEW FILE########
__FILENAME__ = consistent_bootstrap_test
import time

from dtest import Tester, debug
from assertions import assert_unavailable
from tools import (create_c1c2_table, insert_c1c2, query_c1c2, retry_till_success,
                   insert_columns, new_node, no_vnodes, since)

class TestBootstrapConsistency(Tester):

    @no_vnodes()
    @since('2.1')
    def consistent_reads_after_move_test(self):
        debug("Creating a ring")
        cluster = self.cluster
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False, 'write_request_timeout_in_ms' : 60000, 'read_request_timeout_in_ms' : 60000, 'dynamic_snitch_badness_threshold' : 0.0}, batch_commitlog=True)
    
        cluster.populate(3, tokens=[0, 2**48, 2**62]).start()
        [node1, node2, node3] = cluster.nodelist()
        cluster.start()

        debug("Set to talk to node 2")
        n2cursor = self.patient_cql_connection(node2).cursor()
        self.create_ks(n2cursor, 'ks', 2)
        create_c1c2_table(self, n2cursor)

        debug("Generating some data for all nodes")
        for n in xrange(10,20):
            insert_c1c2(n2cursor, n, 'ALL')

        node1.flush()
        debug("Taking down node1")
        node1.stop(wait_other_notice=True)

        debug("Writing data to node2")
        for n in xrange(30,1000):
            insert_c1c2(n2cursor, n, 'ONE')
        node2.flush()

        debug("Restart node1")
        node1.start(wait_other_notice=True)

        debug("Move token on node3")
        node3.move(2)

        debug("Checking that no data was lost")
        for n in xrange(10,20):
            query_c1c2(n2cursor, n, 'ALL')

        for n in xrange(30,1000):
            query_c1c2(n2cursor, n, 'ALL')

    @since('2.1')
    def consistent_reads_after_bootstrap_test(self):
        debug("Creating a ring")
        cluster = self.cluster
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False, 'write_request_timeout_in_ms' : 60000, 'read_request_timeout_in_ms' : 60000, 'dynamic_snitch_badness_threshold' : 0.0}, batch_commitlog=True)
    
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        cluster.start()

        debug("Set to talk to node 2")
        n2cursor = self.patient_cql_connection(node2).cursor()
        self.create_ks(n2cursor, 'ks', 2)
        create_c1c2_table(self, n2cursor)

        debug("Generating some data for all nodes")
        for n in xrange(10,20):
            insert_c1c2(n2cursor, n, 'ALL')

        node1.flush()
        debug("Taking down node1")
        node1.stop(wait_other_notice=True)

        debug("Writing data to only node2")
        for n in xrange(30,1000):
            insert_c1c2(n2cursor, n, 'ONE')
        node2.flush()

        debug("Restart node1")
        node1.start(wait_other_notice=True)

        debug("Boostraping node3")
        node3 = new_node(cluster)
        node3.start()

        n3cursor = self.patient_cql_connection(node3).cursor()
        n3cursor.execute("USE ks");
        debug("Checking that no data was lost")
        for n in xrange(10,20):
            query_c1c2(n3cursor, n, 'ALL')

        for n in xrange(30,1000):
            query_c1c2(n3cursor, n, 'ALL')

    @since('2.1')
    def consistent_reads_after_relocate_test(self):
        debug("Creating a ring")
        cluster = self.cluster
        cluster.set_configuration_options(values={
                'initial_token': None, 
                'num_tokens': 10,
                'hinted_handoff_enabled' : False, 
                'write_request_timeout_in_ms' : 60000, 
                'read_request_timeout_in_ms' : 60000, 
                'dynamic_snitch_badness_threshold' : 0.0}, batch_commitlog=True)
    
        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()
        cluster.start()
       
        debug("Set to talk to node 2")
        n2cursor = self.patient_cql_connection(node2).cursor()
        self.create_ks(n2cursor, 'ks', 2)
        create_c1c2_table(self, n2cursor)

        debug("Generating some data for all nodes")
        for n in xrange(10,20):
            insert_c1c2(n2cursor, n, 'ALL')

        node1.flush()
        debug("Taking down node1")
        node3.stop(wait_other_notice=True)
        
        debug("Writing data to node2")
        for n in xrange(30,1000):
            insert_c1c2(n2cursor, n, 'ONE')
        node2.flush()

        debug("Restart node1")
        node3.start(wait_other_notice=True)
        
        debug("Getting token from node 1")
        n1cursor = self.patient_cql_connection(node1).cursor()
        n1cursor.execute('SELECT tokens FROM system.local')
        tokens = n1cursor.fetchone()

        debug("Relocate tokens from node1 to node3")
        tl = " ".join(str(t) for t in list(tokens[0])[:8])
        cmd = "taketoken %s" % tl
        debug(cmd)
        node3.nodetool(cmd)

        n1cursor.execute('SELECT tokens FROM system.local')
        tokens = n1cursor.fetchone()
        debug("%s" % tokens)
        assert len(tokens) == 2

        debug("Checking that no data was lost")
        for n in xrange(10,20):
            query_c1c2(n2cursor, n, 'ALL')

        for n in xrange(30,1000):
            query_c1c2(n2cursor, n, 'ALL')


########NEW FILE########
__FILENAME__ = counter_tests
from dtest import Tester

import random, time, uuid

class TestCounters(Tester):

    def simple_increment_test(self):
        """ Simple incrementation test (Created for #3465, that wasn't a bug) """
        cluster = self.cluster

        cluster.populate(3).start()
        nodes = cluster.nodelist()

        cursor = self.patient_cql_connection(nodes[0]).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', validation="CounterColumnType", columns={'c': 'counter'})
        cursor.close()


        cursors = [ self.patient_cql_connection(node, 'ks').cursor() for node in nodes ]
        nb_increment=50
        nb_counter=10

        for i in xrange(0, nb_increment):
            for c in xrange(0, nb_counter):
                cursor = cursors[(i + c) % len(nodes)]
                if cluster.version() >= '1.2':
                    cursor.execute("UPDATE cf SET c = c + 1 WHERE key = 'counter%i'" % c, consistency_level='QUORUM')
                else:
                    cursor.execute("UPDATE cf USING CONSISTENCY QUORUM SET c = c + 1 WHERE key = 'counter%i'" % c)

            cursor = cursors[i % len(nodes)]
            keys = ",".join(["'counter%i'" % c for c in xrange(0, nb_counter)])
            if cluster.version() >= '1.2':
                cursor.execute("SELECT key, c FROM cf WHERE key IN (%s)" % keys, consistency_level='QUORUM')
            else:
                cursor.execute("SELECT key, c FROM cf USING CONSISTENCY QUORUM WHERE key IN (%s)" % keys)
            res = cursor.fetchall()
            assert len(res) == nb_counter
            for c in xrange(0, nb_counter):
                assert len(res[c]) == 2, "Expecting key and counter for counter%i, got %s" % (c, str(res[c]))
                assert res[c][1] == i + 1, "Expecting counter%i = %i, got %i" % (c, i + 1, res[c][0])

    def upgrade_test(self):
        """ Test for bug of #4436 """

        cluster = self.cluster

        cluster.populate(2).start()
        nodes = cluster.nodelist()

        cql_version=None

        cursor = self.patient_cql_connection(nodes[0], version=cql_version).cursor()
        self.create_ks(cursor, 'ks', 2)

        query = """
            CREATE TABLE counterTable (
                k int PRIMARY KEY,
                c counter
            )
        """
        if cluster.version() >= '1.2':
            query = query +  "WITH compression = { 'sstable_compression' : 'SnappyCompressor' }"
        else:
            query = query +  "WITH compression_parameters:sstable_compression='SnappyCompressor'"

        cursor.execute(query)
        time.sleep(2)

        keys = range(0, 4)
        updates = 50

        def make_updates():
            cursor = self.patient_cql_connection(nodes[0], keyspace='ks', version=cql_version).cursor()
            upd = "UPDATE counterTable SET c = c + 1 WHERE k = %d;"
            #upd = "UPDATE counterTable SET c = c + 1 WHERE k = :k%d;"
            if cluster.version() >= '1.2':
                batch = " ".join(["BEGIN COUNTER BATCH"] + [upd % x for x in keys] + ["APPLY BATCH;"])
            else:
                batch = " ".join(["BEGIN BATCH USING CONSISTENCY LEVEL QUORUM"] + [upd % x for x in keys] + ["APPLY BATCH;"])

            #query = cursor.prepare_query(batch)

            kmap = { "k%d" % i : i for i in keys }
            for i in range(0, updates):
                if cluster.version() >= '1.2':
                    cursor.execute(batch, consistency_level='QUORUM')
                else:
                    cursor.execute(batch)

                #cursor.execute_prepared(query, kmap)

        def check(i):
            cursor = self.patient_cql_connection(nodes[0], keyspace='ks', version=cql_version).cursor()
            if cluster.version() >= '1.2':
                cursor.execute("SELECT * FROM counterTable", consistency_level='QUORUM')
            else:
                cursor.execute("SELECT * FROM counterTable USING CONSISTENCY QUORUM")
            assert cursor.rowcount == len(keys), "Expected %d rows, got %d: %s" % (len(keys), cursor.rowcount, str(cursor.fetchall()))
            for row in cursor:
                assert row[1] == i * updates, "Unexpected value %s" % str(row)

        def rolling_restart():
            # Rolling restart
            for i in range(0, 2):
                time.sleep(.2)
                nodes[i].nodetool("drain")
                nodes[i].stop(wait_other_notice=False)
                nodes[i].start(wait_other_notice=True)
                time.sleep(.2)

        make_updates()
        check(1)
        rolling_restart()

        make_updates()
        check(2)
        rolling_restart()

        make_updates()
        check(3)
        rolling_restart()

        check(3)

    def counter_consistency_test(self):
        """
        Do a bunch of writes with ONE, read back with ALL and check results.
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'counter_tests', 3)
        
        stmt = """
              CREATE TABLE counter_table (
              id uuid PRIMARY KEY,
              counter_one COUNTER,
              counter_two COUNTER,
              )
           """
        cursor.execute(stmt)
        
        counters = []
        # establish 50 counters (2x25 rows)
        for i in xrange(25):
            _id = str(uuid.uuid4())
            counters.append(
                {_id: {'counter_one':1, 'counter_two':1}}
            )
        
            cursor.execute("""
                UPDATE counter_table
                SET counter_one = counter_one + 1, counter_two = counter_two + 1
                where id = {uuid}""".format(uuid=_id), consistency_level='ONE')
        
        # increment a bunch of counters with CL.ONE
        for i in xrange(10000):
            counter = counters[random.randint(0, len(counters)-1)]
            counter_id = counter.keys()[0]

            cursor.execute("""
                UPDATE counter_table
                SET counter_one = counter_one + 2
                where id = {uuid}""".format(uuid=counter_id), consistency_level='ONE')
            
            cursor.execute("""
                UPDATE counter_table
                SET counter_two = counter_two + 10
                where id = {uuid}""".format(uuid=counter_id), consistency_level='ONE')
            
            cursor.execute("""
                UPDATE counter_table
                SET counter_one = counter_one - 1
                where id = {uuid}""".format(uuid=counter_id), consistency_level='ONE')
            
            cursor.execute("""
                UPDATE counter_table
                SET counter_two = counter_two - 5
                where id = {uuid}""".format(uuid=counter_id), consistency_level='ONE')
            
            # update expectations to match (assumed) db state
            counter[counter_id]['counter_one'] += 1
            counter[counter_id]['counter_two'] += 5
        
        # let's verify the counts are correct, using CL.ALL
        for counter_dict in counters:
            counter_id = counter_dict.keys()[0]
            
            cursor.execute("""
                SELECT counter_one, counter_two
                FROM counter_table WHERE id = {uuid}
                """.format(uuid=counter_id), consistency_level='ALL')
            
            counter_one_actual, counter_two_actual = cursor.fetchone()
            
            self.assertEqual(counter_one_actual, counter_dict[counter_id]['counter_one'])
            self.assertEqual(counter_two_actual, counter_dict[counter_id]['counter_two'])
    
    def multi_counter_update_test(self):
        """
        Test for singlular update statements that will affect multiple counters.
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'counter_tests', 3)
        
        cursor.execute("""
            CREATE TABLE counter_table (
            id text,
            myuuid uuid,
            counter_one COUNTER,
            PRIMARY KEY (id, myuuid))
            """)
        
        expected_counts = {}
        
        # set up expectations
        for i in range(1,6):
            _id = uuid.uuid4()
            
            expected_counts[_id] = i

        for k, v in expected_counts.items():
            cursor.execute("""
                UPDATE counter_table set counter_one = counter_one + {v}
                WHERE id='foo' and myuuid = {k}
                """.format(k=k, v=v))

        for k, v in expected_counts.items():
            cursor.execute("""
                SELECT counter_one FROM counter_table
                WHERE id = 'foo' and myuuid = {k}
                """.format(k=k))
            
            count = cursor.fetchone()[0]
            
            self.assertEqual(v, count)

########NEW FILE########
__FILENAME__ = cqlsh_tests
# -*- coding: utf-8 -*-
from dtest import Tester, debug
from tools import since, require
from ccmlib import common
import subprocess
import binascii
from decimal import Decimal
import sys, os
from uuid import UUID
from distutils.version import LooseVersion

class TestCqlsh(Tester):

    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

    def test_simple_insert(self):

        self.cluster.populate(1)
        self.cluster.start()

        node1, = self.cluster.nodelist()

        node1.run_cqlsh(cmds = """
            CREATE KEYSPACE simple WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
            use simple;
            create TABLE simple (id int PRIMARY KEY , value text ) ;
            insert into simple (id, value) VALUES (1, 'one');
            insert into simple (id, value) VALUES (2, 'two');
            insert into simple (id, value) VALUES (3, 'three');
            insert into simple (id, value) VALUES (4, 'four');
            insert into simple (id, value) VALUES (5, 'five')""")

        cursor = self.patient_cql_connection(node1).cursor()
        cursor.execute("select id, value from simple.simple");

        self.assertEqual({1:'one', 2:'two', 3:'three', 4:'four', 5:'five'}, 
                         {k : v for k,v in cursor})

    def test_eat_glass(self):
        
        self.cluster.populate(1)
        self.cluster.start()

        node1, = self.cluster.nodelist()

        node1.run_cqlsh(cmds = u"""create KEYSPACE testks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
use testks;

CREATE TABLE varcharmaptable ( 
        varcharkey varchar , 
        varcharasciimap map<varchar, ascii>,
        varcharbigintmap map<varchar, bigint>,
        varcharblobmap map<varchar, blob>,
        varcharbooleanmap map<varchar, boolean>,
        varchardecimalmap map<varchar, decimal>,
        varchardoublemap map<varchar, double>,
        varcharfloatmap map<varchar, float>,
        varcharintmap map<varchar, int>,
        varcharinetmap map<varchar, inet>,
        varchartextmap map<varchar, text>,
        varchartimestampmap map<varchar, timestamp>,
        varcharuuidmap map<varchar, uuid>,
        varchartimeuuidmap map<varchar, timeuuid>,
        varcharvarcharmap map<varchar, varchar>,
        varcharvarintmap map<varchar, varint>,
        PRIMARY KEY (varcharkey));

INSERT INTO varcharmaptable (varcharkey, varcharasciimap ) VALUES      ('',  {' ': 'My','   ,    .': 'Name','I can eat glass and it does not hurt me': 'Is'} );

UPDATE varcharmaptable SET varcharasciimap = varcharasciimap + {'Vitrum edere possum, mihi non nocet.':'Cassandra'} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharasciimap['Vitrum edere possum, mihi non nocet.'] = 'Hello' WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharbigintmap ) VALUES      ('',  {' ': -45,'   ,    .': 12300,'I can eat glass and it does not hurt me': 0} );

UPDATE varcharmaptable SET varcharbigintmap = varcharbigintmap + {'Vitrum edere possum, mihi non nocet.':23} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharbigintmap['Vitrum edere possum, mihi non nocet.'] = 5100003 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharblobmap ) VALUES      ('',  {' ': 0xDEADBEEF,'   ,    .': 0xBEEFBEEF,'I can eat glass and it does not hurt me': 0xFEEB} );

UPDATE varcharmaptable SET varcharblobmap = varcharblobmap + {'Vitrum edere possum, mihi non nocet.':0x10} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharblobmap['Vitrum edere possum, mihi non nocet.'] = 0xFEED103A WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharbooleanmap ) VALUES      ('',  {' ': FALSE,'   ,    .': FALSE,'I can eat glass and it does not hurt me': FALSE} );

UPDATE varcharmaptable SET varcharbooleanmap = varcharbooleanmap + {'Vitrum edere possum, mihi non nocet.':TRUE} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharbooleanmap['Vitrum edere possum, mihi non nocet.'] = TRUE WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varchardecimalmap ) VALUES      ('',  {' ': -20.4,'   ,    .': 11234234.3,'I can eat glass and it does not hurt me': 10.0} );

UPDATE varcharmaptable SET varchardecimalmap = varchardecimalmap + {'Vitrum edere possum, mihi non nocet.':0.0} WHERE varcharkey= '';

UPDATE varcharmaptable SET varchardecimalmap['Vitrum edere possum, mihi non nocet.'] = 50.0 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varchardoublemap ) VALUES      ('',  {' ': -432.311,'   ,    .': 3.1415,'I can eat glass and it does not hurt me': 20000.0} );

UPDATE varcharmaptable SET varchardoublemap = varchardoublemap + {'Vitrum edere possum, mihi non nocet.':11} WHERE varcharkey= '';

UPDATE varcharmaptable SET varchardoublemap['Vitrum edere possum, mihi non nocet.'] = 4234243 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharfloatmap ) VALUES      ('',  {' ': -234.3,'   ,    .': -234234,'I can eat glass and it does not hurt me': 1000.5} );

UPDATE varcharmaptable SET varcharfloatmap = varcharfloatmap + {'Vitrum edere possum, mihi non nocet.':-3.14} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharfloatmap['Vitrum edere possum, mihi non nocet.'] = 10.0 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharintmap ) VALUES      ('',  {' ': 2,'   ,    .': -3,'I can eat glass and it does not hurt me': -500} );

UPDATE varcharmaptable SET varcharintmap = varcharintmap + {'Vitrum edere possum, mihi non nocet.':20000} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharintmap['Vitrum edere possum, mihi non nocet.'] = 1 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharinetmap ) VALUES      ('',  {' ': '127.0.0.1','   ,    .': '8.8.8.8','I can eat glass and it does not hurt me': '8.8.4.4'} );

UPDATE varcharmaptable SET varcharinetmap = varcharinetmap + {'Vitrum edere possum, mihi non nocet.':'241.30.12.24'} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharinetmap['Vitrum edere possum, mihi non nocet.'] = '192.168.0.1' WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varchartextmap ) VALUES      ('',  {' ': 'On a trip','   ,    .': 'Across','I can eat glass and it does not hurt me': 'The '} );

UPDATE varcharmaptable SET varchartextmap = varchartextmap + {'Vitrum edere possum, mihi non nocet.':'Sea'} WHERE varcharkey= '';

UPDATE varcharmaptable SET varchartextmap['Vitrum edere possum, mihi non nocet.'] = 'Once I went' WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varchartimestampmap ) VALUES      ('',  {' ': '1985-08-03T04:21:01+0000','   ,    .': '2000-01-01T00:20:01+0000','I can eat glass and it does not hurt me': '1942-03-11T5:21:01+0000'} );

UPDATE varcharmaptable SET varchartimestampmap = varchartimestampmap + {'Vitrum edere possum, mihi non nocet.':'2043-11-04T11:21:01+0000'} WHERE varcharkey= '';

UPDATE varcharmaptable SET varchartimestampmap['Vitrum edere possum, mihi non nocet.'] = '2013-06-19T03:21:01+0000' WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharuuidmap ) VALUES      ('',  {' ': 1df0b6ac-f3d3-456c-8b78-2bc70e585107,'   ,    .': e2ed2164-31dc-42cb-8ee9-47376e071210,'I can eat glass and it does not hurt me': a487fe45-8af5-4454-ac66-2614286d7e89} );

UPDATE varcharmaptable SET varcharuuidmap = varcharuuidmap + {'Vitrum edere possum, mihi non nocet.':d25bdfc7-eb81-472c-bf5b-b4e6afdf66c2} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharuuidmap['Vitrum edere possum, mihi non nocet.'] = 7787064c-ce54-4324-abdd-05775b89ead7 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varchartimeuuidmap ) VALUES      ('',  {' ': 670c7f90-d8ec-11e2-a28f-0800200c9a66,'   ,    .': 750c2d70-d8ec-11e2-a28f-0800200c9a66,'I can eat glass and it does not hurt me': 80d74810-d8ec-11e2-a28f-0800200c9a66} );

UPDATE varcharmaptable SET varchartimeuuidmap = varchartimeuuidmap + {'Vitrum edere possum, mihi non nocet.':93e276f0-d8ec-11e2-a28f-0800200c9a66} WHERE varcharkey= '';

UPDATE varcharmaptable SET varchartimeuuidmap['Vitrum edere possum, mihi non nocet.'] = 4a36c100-d8ec-11e2-a28f-0800200c9a66 WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharvarcharmap ) VALUES      ('',  {' ': ' ','   ,    .': '   ,    .','I can eat glass and it does not hurt me': 'I can eat glass and it does not hurt me'} );

UPDATE varcharmaptable SET varcharvarcharmap = varcharvarcharmap + {'Vitrum edere possum, mihi non nocet.':'Vitrum edere possum, mihi non nocet.'} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharvarcharmap['Vitrum edere possum, mihi non nocet.'] = '' WHERE varcharkey= '';

INSERT INTO varcharmaptable (varcharkey, varcharvarintmap ) VALUES      ('',  {' ': -40,'   ,    .': 110230,'I can eat glass and it does not hurt me': 1400} );

UPDATE varcharmaptable SET varcharvarintmap = varcharvarintmap + {'Vitrum edere possum, mihi non nocet.':20000} WHERE varcharkey= '';

UPDATE varcharmaptable SET varcharvarintmap['Vitrum edere possum, mihi non nocet.'] = 1010010101020400204143243 WHERE varcharkey= ''
        """.encode("utf-8"))

        cursor = self.patient_cql_connection(node1).cursor()
        def verify_varcharmap(map_name, expected, encode_value=False):
            cursor.execute((u"SELECT %s FROM testks.varcharmaptable WHERE varcharkey= '';" % map_name).encode("utf-8"))
            if encode_value:
                got = {k.encode("utf-8"):v.encode("utf-8") for k,v in cursor.fetchone()[0].iteritems()}
            else:
                got = {k.encode("utf-8"):v for k,v in cursor.fetchone()[0].iteritems()}
            self.assertEqual(got, expected)

        verify_varcharmap('varcharasciimap', {
            'Vitrum edere possum, mihi non nocet.' : 'Hello',
            ' ' : 'My',
            '   ,    .' : 'Name',
            'I can eat glass and it does not hurt me' : 'Is'
        })

        verify_varcharmap('varcharbigintmap', {
            'Vitrum edere possum, mihi non nocet.' : 5100003,
            ' ' : -45,
            '   ,    .' : 12300,
            'I can eat glass and it does not hurt me' : 0
        })

        verify_varcharmap('varcharblobmap', {
            'Vitrum edere possum, mihi non nocet.' : binascii.a2b_hex("FEED103A"),
            ' ' : binascii.a2b_hex("DEADBEEF"),
            '   ,    .' : binascii.a2b_hex("BEEFBEEF"),
            'I can eat glass and it does not hurt me' : binascii.a2b_hex("FEEB")
        })


        verify_varcharmap('varcharbooleanmap', {
            'Vitrum edere possum, mihi non nocet.' : True,
            ' ' : False,
            '   ,    .' : False,
            'I can eat glass and it does not hurt me' : False
        })

        verify_varcharmap('varchardecimalmap', {
            'Vitrum edere possum, mihi non nocet.' : Decimal('50'),
            ' ' : Decimal('-20.4'),
            '   ,    .' : Decimal('11234234.3'),
            'I can eat glass and it does not hurt me' : Decimal('10.0')
        })

        verify_varcharmap('varchardoublemap', {
            'Vitrum edere possum, mihi non nocet.' : 4234243,
            ' ' : -432.311,
            '   ,    .' : 3.1415,
            'I can eat glass and it does not hurt me' : 20000.0
        })

        verify_varcharmap('varcharfloatmap', {
            'Vitrum edere possum, mihi non nocet.' : 10.0,
            ' ' : -234.3000030517578,
            '   ,    .' : -234234,
            'I can eat glass and it does not hurt me' : 1000.5
        })

        verify_varcharmap('varcharintmap', {
            'Vitrum edere possum, mihi non nocet.' : 1,
            ' ' : 2,
            '   ,    .' : -3,
            'I can eat glass and it does not hurt me' : -500
        })

        verify_varcharmap('varcharinetmap', {
            'Vitrum edere possum, mihi non nocet.' : '192.168.0.1',
            ' ' : '127.0.0.1',
            '   ,    .' : '8.8.8.8',
            'I can eat glass and it does not hurt me' : '8.8.4.4'
        })

        verify_varcharmap('varchartextmap', {
            'Vitrum edere possum, mihi non nocet.' : 'Once I went',
            ' ' : 'On a trip',
            '   ,    .' : 'Across',
            'I can eat glass and it does not hurt me' : 'The '
        })

        verify_varcharmap('varchartimestampmap', {
            'Vitrum edere possum, mihi non nocet.' : '\x00\x00\x01?Zs\x1dH',
            ' ' : '\x00\x00\x00r\x86\xfa\xe3\xc8',
            '   ,    .' : '\x00\x00\x00\xdcj\xe1\xffh',
            'I can eat glass and it does not hurt me' : '\xff\xff\xff3\xa9\x0f^H'
        })

        verify_varcharmap('varcharuuidmap', {
            'Vitrum edere possum, mihi non nocet.' : UUID('7787064c-ce54-4324-abdd-05775b89ead7'),
            ' ' : UUID('1df0b6ac-f3d3-456c-8b78-2bc70e585107'),
            '   ,    .' : UUID('e2ed2164-31dc-42cb-8ee9-47376e071210'),
            'I can eat glass and it does not hurt me' : UUID('a487fe45-8af5-4454-ac66-2614286d7e89')
        })

        verify_varcharmap('varchartimeuuidmap', {
            'Vitrum edere possum, mihi non nocet.' : UUID('4a36c100-d8ec-11e2-a28f-0800200c9a66'),
            ' ' : UUID('670c7f90-d8ec-11e2-a28f-0800200c9a66'),
            '   ,    .' : UUID('750c2d70-d8ec-11e2-a28f-0800200c9a66'),
            'I can eat glass and it does not hurt me' : UUID('80d74810-d8ec-11e2-a28f-0800200c9a66')
        })

        verify_varcharmap('varcharvarcharmap', {
            'Vitrum edere possum, mihi non nocet.' : '',
            ' ' : ' ',
            '   ,    .' : '   ,    .',
            'I can eat glass and it does not hurt me' : 'I can eat glass and it does not hurt me'
        }, encode_value=True)

        verify_varcharmap('varcharvarintmap', {
            'Vitrum edere possum, mihi non nocet.' : 1010010101020400204143243,
            ' ' : -40,
            '   ,    .' : 110230,
            'I can eat glass and it does not hurt me' : 1400
        })

        output = self.run_cqlsh(node1, 'use testks; SELECT * FROM varcharmaptable')

        self.assertEquals(output.count('   ,    .'), 16)
        self.assertEquals(output.count(' '), 16)
        self.assertEquals(output.count(''), 2)

    def test_with_empty_values(self):
        """
        CASSANDRA-7196. Make sure the server returns empty values and CQLSH prints them properly
        """
        self.cluster.populate(1)
        self.cluster.start()

        node1, = self.cluster.nodelist()

        node1.run_cqlsh(cmds = u"""create keyspace  CASSANDRA_7196 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1} ;

use CASSANDRA_7196;

CREATE TABLE has_all_types (
    num int PRIMARY KEY,
    intcol int,
    asciicol ascii,
    bigintcol bigint,
    blobcol blob,
    booleancol boolean,
    decimalcol decimal,
    doublecol double,
    floatcol float,
    textcol text,
    timestampcol timestamp,
    uuidcol uuid,
    varcharcol varchar,
    varintcol varint
) WITH compression = {'sstable_compression':'LZ4Compressor'};

INSERT INTO has_all_types (num, intcol, asciicol, bigintcol, blobcol, booleancol,
                           decimalcol, doublecol, floatcol, textcol,
                           timestampcol, uuidcol, varcharcol, varintcol)
VALUES (0, -12, 'abcdefg', 1234567890123456789, 0x000102030405fffefd, true,
        19952.11882, 1.0, -2.1, 'Voil!', '2012-05-14 12:53:20+0000',
        bd1924e1-6af8-44ae-b5e1-f24131dbd460, '"', 10000000000000000000000000);

INSERT INTO has_all_types (num, intcol, asciicol, bigintcol, blobcol, booleancol,
                           decimalcol, doublecol, floatcol, textcol,
                           timestampcol, uuidcol, varcharcol, varintcol)
VALUES (1, 2147483647, '__!''$#@!~"', 9223372036854775807, 0xffffffffffffffffff, true,
        0.00000000000001, 9999999.999, 99999.99, '''', '1900-01-01+0000',
        ffffffff-ffff-ffff-ffff-ffffffffffff, 'newline->
<-', 9);

INSERT INTO has_all_types (num, intcol, asciicol, bigintcol, blobcol, booleancol,
                           decimalcol, doublecol, floatcol, textcol,
                           timestampcol, uuidcol, varcharcol, varintcol)
VALUES (2, 0, '', 0, 0x, false,
        0.0, 0.0, 0.0, '', 0,
        00000000-0000-0000-0000-000000000000, '', 0);

INSERT INTO has_all_types (num, intcol, asciicol, bigintcol, blobcol, booleancol,
                           decimalcol, doublecol, floatcol, textcol,
                           timestampcol, uuidcol, varcharcol, varintcol)
VALUES (3, -2147483648, '''''''', -9223372036854775808, 0x80, false,
        10.0000000000000, -1004.10, 100000000.9, '', '2038-01-19T03:14-1200',
        ffffffff-ffff-1fff-8fff-ffffffffffff, '''', -10000000000000000000000000);

INSERT INTO has_all_types (num, intcol, asciicol, bigintcol, blobcol, booleancol,
                           decimalcol, doublecol, floatcol, textcol,
                           timestampcol, uuidcol, varcharcol, varintcol)
VALUES (4, blobAsInt(0x), '', blobAsBigint(0x), 0x, blobAsBoolean(0x), blobAsDecimal(0x),
        blobAsDouble(0x), blobAsFloat(0x), '', blobAsTimestamp(0x), blobAsUuid(0x), '',
        blobAsVarint(0x))""".encode("utf-8"))

        output = self.run_cqlsh(node1, "select intcol, bigintcol, varintcol from CASSANDRA_7196.has_all_types where num in (0, 1, 2, 3, 4)")

        expected = """
 intcol      | bigintcol            | varintcol
-------------+----------------------+-----------------------------
         -12 |  1234567890123456789 |  10000000000000000000000000
  2147483647 |  9223372036854775807 |                           9
           0 |                    0 |                           0
 -2147483648 | -9223372036854775808 | -10000000000000000000000000
             |                      |                            \n\n(5 rows)"""

        self.assertTrue(expected in output, "Output \n {%s} \n doesn't contain expected\n {%s}" % (output, expected))

    def run_cqlsh(self, node, cmds, cqlsh_options=[]):
        cdir = node.get_cassandra_dir()
        cli = os.path.join(cdir, 'bin', 'cqlsh')
        env = common.make_cassandra_env(cdir, node.get_path())
        env['LANG'] = 'en_US.UTF-8'
        if LooseVersion(self.cluster.version()) >= LooseVersion('2.1'):
            host = node.network_interfaces['binary'][0]
            port = node.network_interfaces['binary'][1]
        else:
            host = node.network_interfaces['thrift'][0]
            port = node.network_interfaces['thrift'][1]
        args = cqlsh_options + [ host, str(port) ]
        sys.stdout.flush()
        p = subprocess.Popen([ cli ] + args, env=env, stdin=subprocess.PIPE, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
        for cmd in cmds.split(';'):
            p.stdin.write(cmd + ';\n')
        p.stdin.write("quit;\n")
        return p.communicate()[0]

########NEW FILE########
__FILENAME__ = cql_prepared_test
from dtest import Tester
from assertions import *
from tools import *

import os, sys, time, tools
from uuid import UUID
from ccmlib.cluster import Cluster

cql_version="3.0.0"

class TestCQL(Tester):

    def prepare(self):
        cluster = self.cluster

        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.cql_connection(node1, version=cql_version).cursor()
        self.create_ks(cursor, 'ks', 1)
        return cursor

    @since("1.2")
    def batch_preparation_test(self):
        """ Test preparation of batch statement (#4202) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE cf (
                k varchar PRIMARY KEY,
                c int,
            )
        """)

        query = "BEGIN BATCH INSERT INTO cf (k, c) VALUES (:key, :val); APPLY BATCH";
        pq = cursor.prepare_query(query);

        cursor.execute_prepared(pq, params={'key' : 'foo', 'val' : 4})

########NEW FILE########
__FILENAME__ = cql_tests
# coding: utf-8

import random, math
import re
import time
from uuid import uuid4, UUID

from dtest import Tester
from assertions import assert_invalid, assert_one, assert_none, assert_all
from cql import ProgrammingError
from tools import since, require

cql_version="3.0.0"

class TestCQL(Tester):

    def prepare(self, ordered=False, create_keyspace=True, use_cache=False, nodes=1, rf=1):
        cluster = self.cluster

        if (ordered):
            cluster.set_partitioner("org.apache.cassandra.dht.ByteOrderedPartitioner")

        if (use_cache):
            cluster.set_configuration_options(values={ 'row_cache_size_in_mb' : 100 })

        cluster.populate(nodes).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.patient_cql_connection(node1, version=cql_version).cursor()
        if create_keyspace:
            self.create_ks(cursor, 'ks', rf)
        return cursor

    def static_cf_test(self):
        """ Test static CF syntax """
        cursor = self.prepare()

        # Create
        cursor.execute("""
            CREATE TABLE users (
                userid uuid PRIMARY KEY,
                firstname text,
                lastname text,
                age int
            );
        """)

        # Inserts
        cursor.execute("INSERT INTO users (userid, firstname, lastname, age) VALUES (550e8400-e29b-41d4-a716-446655440000, 'Frodo', 'Baggins', 32)")
        cursor.execute("UPDATE users SET firstname = 'Samwise', lastname = 'Gamgee', age = 33 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479")

        # Queries
        cursor.execute("SELECT firstname, lastname FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ 'Frodo', 'Baggins' ]], res

        cursor.execute("SELECT * FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ UUID('550e8400-e29b-41d4-a716-446655440000'), 32, 'Frodo', 'Baggins' ]], res

        cursor.execute("SELECT * FROM users")
        res = cursor.fetchall()
        assert res == [
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 33, 'Samwise', 'Gamgee' ],
            [ UUID('550e8400-e29b-41d4-a716-446655440000'), 32, 'Frodo', 'Baggins' ],
        ], res

        # Test batch inserts
        cursor.execute("""
            BEGIN BATCH
                INSERT INTO users (userid, age) VALUES (550e8400-e29b-41d4-a716-446655440000, 36)
                UPDATE users SET age = 37 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479
                DELETE firstname, lastname FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000
                DELETE firstname, lastname FROM users WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479
            APPLY BATCH
        """)

        cursor.execute("SELECT * FROM users")
        res = cursor.fetchall()
        assert res == [
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 37, None, None ],
            [ UUID('550e8400-e29b-41d4-a716-446655440000'), 36, None, None ],
        ], res

    def noncomposite_static_cf_test(self):
        """ Test non-composite static CF syntax """
        cursor = self.prepare()

        # Create
        cursor.execute("""
            CREATE TABLE users (
                userid uuid PRIMARY KEY,
                firstname text,
                lastname text,
                age int
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        cursor.execute("INSERT INTO users (userid, firstname, lastname, age) VALUES (550e8400-e29b-41d4-a716-446655440000, 'Frodo', 'Baggins', 32)")
        cursor.execute("UPDATE users SET firstname = 'Samwise', lastname = 'Gamgee', age = 33 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479")

        # Queries
        cursor.execute("SELECT firstname, lastname FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ 'Frodo', 'Baggins' ]], res

        cursor.execute("SELECT * FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ UUID('550e8400-e29b-41d4-a716-446655440000'), 32, 'Frodo', 'Baggins' ]], res

        cursor.execute("SELECT * FROM users")
        res = cursor.fetchall()
        assert res == [
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 33, 'Samwise', 'Gamgee' ],
            [ UUID('550e8400-e29b-41d4-a716-446655440000'), 32, 'Frodo', 'Baggins' ],
        ], res

        # Test batch inserts
        cursor.execute("""
            BEGIN BATCH
                INSERT INTO users (userid, age) VALUES (550e8400-e29b-41d4-a716-446655440000, 36)
                UPDATE users SET age = 37 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479
                DELETE firstname, lastname FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000
                DELETE firstname, lastname FROM users WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479
            APPLY BATCH
        """)

        cursor.execute("SELECT * FROM users")
        res = cursor.fetchall()
        assert res == [
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 37, None, None ],
            [ UUID('550e8400-e29b-41d4-a716-446655440000'), 36, None, None ],
        ], res

    def dynamic_cf_test(self):
        """ Test non-composite dynamic CF syntax """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE clicks (
                userid uuid,
                url text,
                time bigint,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        cursor.execute("INSERT INTO clicks (userid, url, time) VALUES (550e8400-e29b-41d4-a716-446655440000, 'http://foo.bar', 42)")
        cursor.execute("INSERT INTO clicks (userid, url, time) VALUES (550e8400-e29b-41d4-a716-446655440000, 'http://foo-2.bar', 24)")
        cursor.execute("INSERT INTO clicks (userid, url, time) VALUES (550e8400-e29b-41d4-a716-446655440000, 'http://bar.bar', 128)")
        cursor.execute("UPDATE clicks SET time = 24 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479 and url = 'http://bar.foo'")
        cursor.execute("UPDATE clicks SET time = 12 WHERE userid IN (f47ac10b-58cc-4372-a567-0e02b2c3d479, 550e8400-e29b-41d4-a716-446655440000) and url = 'http://foo-3'")

        # Queries
        cursor.execute("SELECT url, time FROM clicks WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ 'http://bar.bar', 128 ], [ 'http://foo-2.bar', 24 ], [ 'http://foo-3', 12 ], [ 'http://foo.bar', 42 ]], res

        cursor.execute("SELECT * FROM clicks WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479")
        res = cursor.fetchall()
        assert res == [
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 'http://bar.foo', 24 ],
            [ UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479'), 'http://foo-3', 12 ]
        ], res

        cursor.execute("SELECT time FROM clicks")
        res = cursor.fetchall()
        # Result from 'f47ac10b-58cc-4372-a567-0e02b2c3d479' are first
        assert res == [[24], [12], [128], [24], [12], [42]], res

        # Check we don't allow empty values for url since this is the full underlying cell name (#6152)
        assert_invalid(cursor, "INSERT INTO clicks (userid, url, time) VALUES (810e8500-e29b-41d4-a716-446655440000, '', 42)")

    def dense_cf_test(self):
        """ Test composite 'dense' CF syntax """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE connections (
                userid uuid,
                ip text,
                port int,
                time bigint,
                PRIMARY KEY (userid, ip, port)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        cursor.execute("INSERT INTO connections (userid, ip, port, time) VALUES (550e8400-e29b-41d4-a716-446655440000, '192.168.0.1', 80, 42)")
        cursor.execute("INSERT INTO connections (userid, ip, port, time) VALUES (550e8400-e29b-41d4-a716-446655440000, '192.168.0.2', 80, 24)")
        cursor.execute("INSERT INTO connections (userid, ip, port, time) VALUES (550e8400-e29b-41d4-a716-446655440000, '192.168.0.2', 90, 42)")
        cursor.execute("UPDATE connections SET time = 24 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479 AND ip = '192.168.0.2' AND port = 80")

        # Queries
        cursor.execute("SELECT ip, port, time FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert res == [[ '192.168.0.1', 80, 42 ], [ '192.168.0.2', 80, 24 ], [ '192.168.0.2', 90, 42 ]], res

        cursor.execute("SELECT ip, port, time FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000 and ip >= '192.168.0.2'")
        res = cursor.fetchall()
        assert res == [[ '192.168.0.2', 80, 24 ], [ '192.168.0.2', 90, 42 ]], res

        cursor.execute("SELECT ip, port, time FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000 and ip = '192.168.0.2'")
        res = cursor.fetchall()
        assert res == [[ '192.168.0.2', 80, 24 ], [ '192.168.0.2', 90, 42 ]], res

        cursor.execute("SELECT ip, port, time FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000 and ip > '192.168.0.2'")
        res = cursor.fetchall()
        assert res == [], res

        # Deletion
        cursor.execute("DELETE time FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND ip = '192.168.0.2' AND port = 80")
        cursor.execute("SELECT * FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("DELETE FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        cursor.execute("SELECT * FROM connections WHERE userid = 550e8400-e29b-41d4-a716-446655440000")
        res = cursor.fetchall()
        assert len(res) == 0, res

    def sparse_cf_test(self):
        """ Test composite 'sparse' CF syntax """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE timeline (
                userid uuid,
                posted_month int,
                posted_day int,
                body text,
                posted_by text,
                PRIMARY KEY (userid, posted_month, posted_day)
            );
        """)

        # Inserts
        cursor.execute("INSERT INTO timeline (userid, posted_month, posted_day, body, posted_by) VALUES (550e8400-e29b-41d4-a716-446655440000, 1, 12, 'Something else', 'Frodo Baggins')")
        cursor.execute("INSERT INTO timeline (userid, posted_month, posted_day, body, posted_by) VALUES (550e8400-e29b-41d4-a716-446655440000, 1, 24, 'Something something', 'Frodo Baggins')")
        cursor.execute("UPDATE timeline SET body = 'Yo Froddo', posted_by = 'Samwise Gamgee' WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479 AND posted_month = 1 AND posted_day = 3")
        cursor.execute("UPDATE timeline SET body = 'Yet one more message' WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND posted_month = 1 and posted_day = 30")

        # Queries
        cursor.execute("SELECT body, posted_by FROM timeline WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND posted_month = 1 AND posted_day = 24")
        res = cursor.fetchall()
        assert res == [[ 'Something something', 'Frodo Baggins' ]], res

        cursor.execute("SELECT posted_day, body, posted_by FROM timeline WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND posted_month = 1 AND posted_day > 12")
        res = cursor.fetchall()
        assert res == [
            [ 24, 'Something something', 'Frodo Baggins' ],
            [ 30, 'Yet one more message', None ]
        ], res

        cursor.execute("SELECT posted_day, body, posted_by FROM timeline WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND posted_month = 1")
        res = cursor.fetchall()
        assert res == [
            [ 12, 'Something else', 'Frodo Baggins' ],
            [ 24, 'Something something', 'Frodo Baggins' ],
            [ 30, 'Yet one more message', None ]
        ], res

    def create_invalid_test(self):
        """ Check invalid CREATE TABLE requests """

        cursor = self.prepare()

        assert_invalid(cursor, "CREATE TABLE test ()")
        if self.cluster.version() < "1.2":
            assert_invalid(cursor, "CREATE TABLE test (key text PRIMARY KEY)")
        assert_invalid(cursor, "CREATE TABLE test (c1 text, c2 text, c3 text)")
        assert_invalid(cursor, "CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)")

        assert_invalid(cursor, "CREATE TABLE test (key text PRIMARY KEY, key int)")
        assert_invalid(cursor, "CREATE TABLE test (key text PRIMARY KEY, c int, c text)")

        assert_invalid(cursor, "CREATE TABLE test (key text, key2 text, c int, d text, PRIMARY KEY (key, key2)) WITH COMPACT STORAGE")

    def limit_ranges_test(self):
        """ Validate LIMIT option for 'range queries' in SELECT statements """
        cursor = self.prepare(ordered=True)

        cursor.execute("""
            CREATE TABLE clicks (
                userid int,
                url text,
                time bigint,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for id in xrange(0, 100):
            for tld in [ 'com', 'org', 'net' ]:
                cursor.execute("INSERT INTO clicks (userid, url, time) VALUES (%i, 'http://foo.%s', 42)" % (id, tld))

        # Queries
        cursor.execute("SELECT * FROM clicks WHERE token(userid) >= token(2) LIMIT 1")
        res = cursor.fetchall()
        assert res == [[ 2, 'http://foo.com', 42 ]], res

        cursor.execute("SELECT * FROM clicks WHERE token(userid) > token(2) LIMIT 1")
        res = cursor.fetchall()
        assert res == [[ 3, 'http://foo.com', 42 ]], res

    def limit_multiget_test(self):
        """ Validate LIMIT option for 'multiget' in SELECT statements """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE clicks (
                userid int,
                url text,
                time bigint,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for id in xrange(0, 100):
            for tld in [ 'com', 'org', 'net' ]:
                cursor.execute("INSERT INTO clicks (userid, url, time) VALUES (%i, 'http://foo.%s', 42)" % (id, tld))

        # Check that we do limit the output to 1 *and* that we respect query
        # order of keys (even though 48 is after 2)
        cursor.execute("SELECT * FROM clicks WHERE userid IN (48, 2) LIMIT 1")
        res = cursor.fetchall()
        assert res == [[ 48, 'http://foo.com', 42 ]], res

    def limit_sparse_test(self):
        """ Validate LIMIT option for sparse table in SELECT statements """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE clicks (
                userid int,
                url text,
                day int,
                month text,
                year int,
                PRIMARY KEY (userid, url)
            );
        """)

        # Inserts
        for id in xrange(0, 100):
            for tld in [ 'com', 'org', 'net' ]:
                cursor.execute("INSERT INTO clicks (userid, url, day, month, year) VALUES (%i, 'http://foo.%s', 1, 'jan', 2012)" % (id, tld))

        # Queries
        # Check we do get as many rows as requested
        cursor.execute("SELECT * FROM clicks LIMIT 4")
        res = cursor.fetchall()
        assert len(res) == 4, res

    def counters_test(self):
        """ Validate counter support """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE clicks (
                userid int,
                url text,
                total counter,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """)

        cursor.execute("UPDATE clicks SET total = total + 1 WHERE userid = 1 AND url = 'http://foo.com'")
        cursor.execute("SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'")
        res = cursor.fetchall()
        assert res == [[ 1 ]], res

        cursor.execute("UPDATE clicks SET total = total - 4 WHERE userid = 1 AND url = 'http://foo.com'")
        cursor.execute("SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'")
        res = cursor.fetchall()
        assert res == [[ -3 ]], res

        cursor.execute("UPDATE clicks SET total = total+1 WHERE userid = 1 AND url = 'http://foo.com'")
        cursor.execute("SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'")
        res = cursor.fetchall()
        assert res == [[ -2 ]], res

        cursor.execute("UPDATE clicks SET total = total -2 WHERE userid = 1 AND url = 'http://foo.com'")
        cursor.execute("SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'")
        res = cursor.fetchall()
        assert res == [[ -4 ]], res

    def indexed_with_eq_test(self):
        """ Check that you can query for an indexed column even with a key EQ clause """
        cursor = self.prepare()

        # Create
        cursor.execute("""
            CREATE TABLE users (
                userid uuid PRIMARY KEY,
                firstname text,
                lastname text,
                age int
            );
        """)

        cursor.execute("CREATE INDEX byAge ON users(age)")

        # Inserts
        cursor.execute("INSERT INTO users (userid, firstname, lastname, age) VALUES (550e8400-e29b-41d4-a716-446655440000, 'Frodo', 'Baggins', 32)")
        cursor.execute("UPDATE users SET firstname = 'Samwise', lastname = 'Gamgee', age = 33 WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479")

        # Queries
        cursor.execute("SELECT firstname FROM users WHERE userid = 550e8400-e29b-41d4-a716-446655440000 AND age = 33")
        res = cursor.fetchall()
        assert res == [], res

        cursor.execute("SELECT firstname FROM users WHERE userid = f47ac10b-58cc-4372-a567-0e02b2c3d479 AND age = 33")
        res = cursor.fetchall()
        assert res == [[ 'Samwise' ]], res

    def select_key_in_test(self):
        """ Query for KEY IN (...) """
        cursor = self.prepare()

        # Create
        cursor.execute("""
            CREATE TABLE users (
                userid uuid PRIMARY KEY,
                firstname text,
                lastname text,
                age int
            );
        """)

        # Inserts
        cursor.execute("""
                INSERT INTO users (userid, firstname, lastname, age)
                VALUES (550e8400-e29b-41d4-a716-446655440000, 'Frodo', 'Baggins', 32)
        """)
        cursor.execute("""
                INSERT INTO users (userid, firstname, lastname, age)
                VALUES (f47ac10b-58cc-4372-a567-0e02b2c3d479, 'Samwise', 'Gamgee', 33)
        """)

        # Select
        cursor.execute("""
                SELECT firstname, lastname FROM users
                WHERE userid IN (550e8400-e29b-41d4-a716-446655440000, f47ac10b-58cc-4372-a567-0e02b2c3d479)
        """)

        res = cursor.fetchall()
        assert len(res) == 2, res

    def exclusive_slice_test(self):
        """ Test SELECT respects inclusive and exclusive bounds """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for x in range(0, 10):
            cursor.execute("INSERT INTO test (k, c, v) VALUES (0, %i, %i)" % (x, x))

        # Queries
        cursor.execute("SELECT v FROM test WHERE k = 0")
        res = cursor.fetchall()
        assert len(res) == 10, res

        cursor.execute("SELECT v FROM test WHERE k = 0 AND c >= 2 AND c <= 6")
        res = cursor.fetchall()
        assert len(res) == 5 and res[0][0] == 2 and res[len(res) - 1][0] == 6, res

        cursor.execute("SELECT v FROM test WHERE k = 0 AND c > 2 AND c <= 6")
        res = cursor.fetchall()
        assert len(res) == 4 and res[0][0] == 3 and res[len(res) - 1][0] == 6, res

        cursor.execute("SELECT v FROM test WHERE k = 0 AND c >= 2 AND c < 6")
        res = cursor.fetchall()
        assert len(res) == 4 and res[0][0] == 2 and res[len(res) - 1][0] == 5, res

        cursor.execute("SELECT v FROM test WHERE k = 0 AND c > 2 AND c < 6")
        res = cursor.fetchall()
        assert len(res) == 3 and res[0][0] == 3 and res[len(res) - 1][0] == 5, res

        # With LIMIT
        cursor.execute("SELECT v FROM test WHERE k = 0 AND c > 2 AND c <= 6 LIMIT 2")
        res = cursor.fetchall()
        assert len(res) == 2 and res[0][0] == 3 and res[len(res) - 1][0] == 4, res

        cursor.execute("SELECT v FROM test WHERE k = 0 AND c >= 2 AND c < 6 ORDER BY c DESC LIMIT 2")
        res = cursor.fetchall()
        assert len(res) == 2 and res[0][0] == 5 and res[len(res) - 1][0] == 4, res

    def in_clause_wide_rows_test(self):
        """ Check IN support for 'wide rows' in SELECT statement """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test1 (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for x in range(0, 10):
            cursor.execute("INSERT INTO test1 (k, c, v) VALUES (0, %i, %i)" % (x, x))

        cursor.execute("SELECT v FROM test1 WHERE k = 0 AND c IN (5, 2, 8)")
        res = cursor.fetchall()
        if self.cluster.version() <= "1.2":
            assert res == [[5], [2], [8]], res
        else:
            assert res == [[2], [5], [8]], res

        # composites
        cursor.execute("""
            CREATE TABLE test2 (
                k int,
                c1 int,
                c2 int,
                v int,
                PRIMARY KEY (k, c1, c2)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for x in range(0, 10):
            cursor.execute("INSERT INTO test2 (k, c1, c2, v) VALUES (0, 0, %i, %i)" % (x, x))

        # Check first we don't allow IN everywhere
        assert_invalid(cursor, "SELECT v FROM test2 WHERE k = 0 AND c1 IN (5, 2, 8) AND c2 = 3")

        cursor.execute("SELECT v FROM test2 WHERE k = 0 AND c1 = 0 AND c2 IN (5, 2, 8)")
        res = cursor.fetchall()
        assert res == [[2], [5], [8]], res

    def order_by_test(self):
        """ Check ORDER BY support in SELECT statement """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test1 (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE;
        """)

        # Inserts
        for x in range(0, 10):
            cursor.execute("INSERT INTO test1 (k, c, v) VALUES (0, %i, %i)" % (x, x))

        cursor.execute("SELECT v FROM test1 WHERE k = 0 ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[x] for x in range(9, -1, -1)], res

        # composites
        cursor.execute("""
            CREATE TABLE test2 (
                k int,
                c1 int,
                c2 int,
                v int,
                PRIMARY KEY (k, c1, c2)
            );
        """)

        # Inserts
        for x in range(0, 4):
            for y in range(0, 2):
                cursor.execute("INSERT INTO test2 (k, c1, c2, v) VALUES (0, %i, %i, %i)" % (x, y, x * 2 + y))

        # Check first we don't always ORDER BY
        assert_invalid(cursor, "SELECT v FROM test2 WHERE k = 0 ORDER BY c DESC")
        assert_invalid(cursor, "SELECT v FROM test2 WHERE k = 0 ORDER BY c2 DESC")
        assert_invalid(cursor, "SELECT v FROM test2 WHERE k = 0 ORDER BY k DESC")

        cursor.execute("SELECT v FROM test2 WHERE k = 0 ORDER BY c1 DESC")
        res = cursor.fetchall()
        assert res == [[x] for x in range(7, -1, -1)], res

        cursor.execute("SELECT v FROM test2 WHERE k = 0 ORDER BY c1")
        res = cursor.fetchall()
        assert res == [[x] for x in range(0, 8)], res

    def more_order_by_test(self):
        """ More ORDER BY checks (#4160) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE COLUMNFAMILY Test (
                row text,
                number int,
                string text,
                PRIMARY KEY (row, number)
            ) WITH COMPACT STORAGE
        """)

        cursor.execute("INSERT INTO Test (row, number, string) VALUES ('row', 1, 'one');")
        cursor.execute("INSERT INTO Test (row, number, string) VALUES ('row', 2, 'two');")
        cursor.execute("INSERT INTO Test (row, number, string) VALUES ('row', 3, 'three');")
        cursor.execute("INSERT INTO Test (row, number, string) VALUES ('row', 4, 'four');")

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number < 3 ORDER BY number ASC;")
        res = cursor.fetchall()
        assert res == [[1], [2]], res

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number >= 3 ORDER BY number ASC;")
        res = cursor.fetchall()
        assert res == [[3], [4]], res

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number < 3 ORDER BY number DESC;")
        res = cursor.fetchall()
        assert res == [[2], [1]], res

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number >= 3 ORDER BY number DESC;")
        res = cursor.fetchall()
        assert res == [[4], [3]], res

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number > 3 ORDER BY number DESC;")
        res = cursor.fetchall()
        assert res == [[4]], res

        cursor.execute("SELECT number FROM Test WHERE row='row' AND number <= 3 ORDER BY number DESC;")
        res = cursor.fetchall()
        assert res == [[3], [2], [1]], res

    def order_by_validation_test(self):
        """ Check we don't allow order by on row key (#4246) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k1 int,
                k2 int,
                v int,
                PRIMARY KEY (k1, k2)
            )
        """)

        q = "INSERT INTO test (k1, k2, v) VALUES (%d, %d, %d)"
        cursor.execute(q % (0, 0, 0))
        cursor.execute(q % (1, 1, 1))
        cursor.execute(q % (2, 2, 2))

        assert_invalid(cursor, "SELECT * FROM test ORDER BY k2")

    def order_by_with_in_test(self):
        """ Check that order-by works with IN (#4327) """
        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test(
                my_id varchar,
                col1 int,
                value varchar,
                PRIMARY KEY (my_id, col1)
            )
        """)
        cursor.execute("INSERT INTO test(my_id, col1, value) VALUES ( 'key1', 1, 'a')")
        cursor.execute("INSERT INTO test(my_id, col1, value) VALUES ( 'key2', 3, 'c')")
        cursor.execute("INSERT INTO test(my_id, col1, value) VALUES ( 'key3', 2, 'b')")
        cursor.execute("INSERT INTO test(my_id, col1, value) VALUES ( 'key4', 4, 'd')")
        # Currently this breaks due to CASSANDRA-4612
        cursor.execute("SELECT col1 FROM test WHERE my_id in('key1', 'key2', 'key3') ORDER BY col1")

        res = cursor.fetchall()
        assert res == [[1], [2], [3]], res


    def reversed_comparator_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH CLUSTERING ORDER BY (c DESC);
        """)

        # Inserts
        for x in range(0, 10):
            cursor.execute("INSERT INTO test (k, c, v) VALUES (0, %i, %i)" % (x, x))

        cursor.execute("SELECT c, v FROM test WHERE k = 0 ORDER BY c ASC")
        res = cursor.fetchall()
        assert res == [[x, x] for x in range(0, 10)], res

        cursor.execute("SELECT c, v FROM test WHERE k = 0 ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[x, x] for x in range(9, -1, -1)], res

        cursor.execute("""
            CREATE TABLE test2 (
                k int,
                c1 int,
                c2 int,
                v text,
                PRIMARY KEY (k, c1, c2)
            ) WITH CLUSTERING ORDER BY (c1 ASC, c2 DESC);
        """)

        # Inserts
        for x in range(0, 10):
            for y in range(0, 10):
                cursor.execute("INSERT INTO test2 (k, c1, c2, v) VALUES (0, %i, %i, '%i%i')" % (x, y, x, y))

        assert_invalid(cursor, "SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c1 ASC, c2 ASC")
        assert_invalid(cursor, "SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c1 DESC, c2 DESC")

        cursor.execute("SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c1 ASC")
        res = cursor.fetchall()
        assert res == [[x, y, '%i%i' % (x, y)] for x in range(0, 10) for y in range(9, -1, -1)], res

        cursor.execute("SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c1 ASC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[x, y, '%i%i' % (x, y)] for x in range(0, 10) for y in range(9, -1, -1)], res

        cursor.execute("SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c1 DESC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[x, y, '%i%i' % (x, y)] for x in range(9, -1, -1) for y in range(0, 10)], res

        assert_invalid(cursor, "SELECT c1, c2, v FROM test2 WHERE k = 0 ORDER BY c2 DESC, c1 ASC")

    def invalid_old_property_test(self):
        """ Check obsolete properties from CQL2 are rejected """
        cursor = self.prepare()

        assert_invalid(cursor, "CREATE TABLE test (foo text PRIMARY KEY, c int) WITH default_validation=timestamp")

        cursor.execute("CREATE TABLE test (foo text PRIMARY KEY, c int)")
        assert_invalid(cursor, "ALTER TABLE test WITH default_validation=int;")

    def null_support_test(self):
        """ Test support for nulls """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v1 int,
                v2 set<text>,
                PRIMARY KEY (k, c)
            );
        """)

        # Inserts
        cursor.execute("INSERT INTO test (k, c, v1, v2) VALUES (0, 0, null, {'1', '2'})")
        cursor.execute("INSERT INTO test (k, c, v1) VALUES (0, 1, 1)")

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [ [0, 0, None, set(['1', '2'])], [0, 1, 1, None]], res

        cursor.execute("INSERT INTO test (k, c, v1) VALUES (0, 1, null)")
        cursor.execute("INSERT INTO test (k, c, v2) VALUES (0, 0, null)")

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [ [0, 0, None, None], [0, 1, None, None]], res

        assert_invalid(cursor, "INSERT INTO test (k, c, v2) VALUES (0, 2, {1, null})")
        assert_invalid(cursor, "SELECT * FROM test WHER k = null")
        assert_invalid(cursor, "INSERT INTO test (k, c, v2) VALUES (0, 0, { 'foo', 'bar', null })")


    def nameless_index_test(self):
        """ Test CREATE INDEX without name and validate the index can be dropped """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE users (
                id text PRIMARY KEY,
                birth_year int,
            )
        """)

        cursor.execute("CREATE INDEX on users(birth_year)")

        cursor.execute("INSERT INTO users (id, birth_year) VALUES ('Tom', 42)")
        cursor.execute("INSERT INTO users (id, birth_year) VALUES ('Paul', 24)")
        cursor.execute("INSERT INTO users (id, birth_year) VALUES ('Bob', 42)")

        cursor.execute("SELECT id FROM users WHERE birth_year = 42")
        res = cursor.fetchall()
        assert res == [['Tom'], ['Bob']]

        cursor.execute("DROP INDEX users_birth_year_idx")

        assert_invalid(cursor, "SELECT id FROM users WHERE birth_year = 42")

    def deletion_test(self):
        """ Test simple deletion and in particular check for #4193 bug """

        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE testcf (
                username varchar,
                id int,
                name varchar,
                stuff varchar,
                PRIMARY KEY(username, id)
            );
        """)

        q = "INSERT INTO testcf (username, id, name, stuff) VALUES ('%s', %d, '%s', '%s');"
        row1 = ('abc', 2, 'rst', 'some value')
        row2 = ('abc', 4, 'xyz', 'some other value')
        cursor.execute(q % row1)
        cursor.execute(q % row2)

        cursor.execute("SELECT * FROM testcf")
        res = cursor.fetchall()
        assert res == [ list(row1), list(row2) ], res

        cursor.execute("DELETE FROM testcf WHERE username='abc' AND id=2")

        cursor.execute("SELECT * FROM testcf")
        res = cursor.fetchall()
        assert res == [ list(row2) ], res

        # Compact case
        cursor.execute("""
            CREATE TABLE testcf2 (
                username varchar,
                id int,
                name varchar,
                stuff varchar,
                PRIMARY KEY(username, id, name)
            ) WITH COMPACT STORAGE;
        """)

        q = "INSERT INTO testcf2 (username, id, name, stuff) VALUES ('%s', %d, '%s', '%s');"
        row1 = ('abc', 2, 'rst', 'some value')
        row2 = ('abc', 4, 'xyz', 'some other value')
        cursor.execute(q % row1)
        cursor.execute(q % row2)

        cursor.execute("SELECT * FROM testcf2")
        res = cursor.fetchall()
        assert res == [ list(row1), list(row2) ], res

        # Won't be allowed until #3708 is in
        if self.cluster.version() < "1.2":
            assert_invalid(cursor, "DELETE FROM testcf2 WHERE username='abc' AND id=2")

    def count_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE events (
                kind text,
                time int,
                value1 int,
                value2 int,
                PRIMARY KEY(kind, time)
            )
        """)

        full = "INSERT INTO events (kind, time, value1, value2) VALUES ('ev1', %d, %d, %d)"
        no_v2 = "INSERT INTO events (kind, time, value1) VALUES ('ev1', %d, %d)"

        cursor.execute(full  % (0, 0, 0))
        cursor.execute(full  % (1, 1, 1))
        cursor.execute(no_v2 % (2, 2))
        cursor.execute(full  % (3, 3, 3))
        cursor.execute(no_v2 % (4, 4))
        cursor.execute("INSERT INTO events (kind, time, value1, value2) VALUES ('ev2', 0, 0, 0)")

        cursor.execute("SELECT COUNT(*) FROM events WHERE kind = 'ev1'")
        res = cursor.fetchall()
        assert res == [[5]], res

        cursor.execute("SELECT COUNT(1) FROM events WHERE kind IN ('ev1', 'ev2') AND time=0")
        res = cursor.fetchall()
        assert res == [[2]], res

    def reserved_keyword_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test1 (
                key text PRIMARY KEY,
                count counter,
            )
        """)

        assert_invalid(cursor, "CREATE TABLE test2 ( select text PRIMARY KEY, x int)")

    def identifier_test(self):
        cursor = self.prepare()

        # Test case insensitivity
        cursor.execute("CREATE TABLE test1 (key_23 int PRIMARY KEY, CoLuMn int)")

        # Should work
        cursor.execute("INSERT INTO test1 (Key_23, Column) VALUES (0, 0)")
        cursor.execute("INSERT INTO test1 (KEY_23, COLUMN) VALUES (0, 0)")

        # Reserved keywords
        assert_invalid(cursor, "CREATE TABLE test1 (select int PRIMARY KEY, column int)")

    #def keyspace_test(self):
    #    cursor = self.prepare()

    #    assert_invalid(cursor, "CREATE KEYSPACE test1")
    #    cursor.execute("CREATE KEYSPACE test2 WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }")
    #    assert_invalid(cursor, "CREATE KEYSPACE My_much_much_too_long_identifier_that_should_not_work WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }")

    #    cursor.execute("DROP KEYSPACE test2")
    #    assert_invalid(cursor, "DROP KEYSPACE non_existing")
    #    cursor.execute("CREATE KEYSPACE test2 WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }")

    #def table_test(self):
    #    cursor = self.prepare()

    #    cursor.execute("""
    #        CREATE TABLE test1 (
    #            k int PRIMARY KEY,
    #            c int
    #        )
    #    """)

    #    cursor.execute("""
    #        CREATE TABLE test2 (
    #            k int,
    #            name int,
    #            value int,
    #            PRIMARY KEY(k, name)
    #        ) WITH COMPACT STORAGE
    #    """)

    #    cursor.execute("""
    #        CREATE TABLE test3 (
    #            k int,
    #            c int,
    #            PRIMARY KEY (k),
    #        )
    #    """)

    #    # existing table
    #    assert_invalid(cursor, "CREATE TABLE test3 (k int PRIMARY KEY, c int)")
    #    # repeated column
    #    assert_invalid(cursor, "CREATE TABLE test4 (k int PRIMARY KEY, c int, k text)")

    #    # compact storage limitations
    #    assert_invalid(cursor, "CREATE TABLE test4 (k int, name, int, c1 int, c2 int, PRIMARY KEY(k, name)) WITH COMPACT STORAGE")

    #    cursor.execute("DROP TABLE test1")
    #    cursor.execute("TRUNCATE test2")

    #    cursor.execute("""
    #        CREATE TABLE test1 (
    #            k int PRIMARY KEY,
    #            c1 int,
    #            c2 int,
    #        )
    #    """)

    def batch_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE users (
                userid text PRIMARY KEY,
                name text,
                password text
            )
        """)

        if self.cluster.version() >= '1.2':
            cursor.execute("""
                BEGIN BATCH
                    INSERT INTO users (userid, password, name) VALUES ('user2', 'ch@ngem3b', 'second user');
                    UPDATE users SET password = 'ps22dhds' WHERE userid = 'user3';
                    INSERT INTO users (userid, password) VALUES ('user4', 'ch@ngem3c');
                    DELETE name FROM users WHERE userid = 'user1';
                APPLY BATCH;
            """, consistency_level='QUORUM')
        else:
            cursor.execute("""
                BEGIN BATCH USING CONSISTENCY QUORUM
                    INSERT INTO users (userid, password, name) VALUES ('user2', 'ch@ngem3b', 'second user');
                    UPDATE users SET password = 'ps22dhds' WHERE userid = 'user3';
                    INSERT INTO users (userid, password) VALUES ('user4', 'ch@ngem3c');
                    DELETE name FROM users WHERE userid = 'user1';
                APPLY BATCH;
            """)

    def token_range_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                c int,
                v int
            )
        """)

        c = 100
        for i in range(0, c):
            cursor.execute("INSERT INTO test (k, c, v) VALUES (%d, %d, %d)" % (i, i, i))

        cursor.execute("SELECT k FROM test")
        inOrder = [ x[0] for x in cursor.fetchall() ]
        assert len(inOrder) == c, 'Expecting %d elements, got %d' % (c, len(inOrder))

        if self.cluster.version() < '1.2':
            cursor.execute("SELECT k FROM test WHERE token(k) >= 0")
        else:
            min_token = -2**63
            cursor.execute("SELECT k FROM test WHERE token(k) >= %d" % min_token)
        res = cursor.fetchall()
        assert len(res) == c, "%s [all: %s]" % (str(res), str(inOrder))

        #assert_invalid(cursor, "SELECT k FROM test WHERE token(k) >= 0")
        #cursor.execute("SELECT k FROM test WHERE token(k) >= 0")

        cursor.execute("SELECT k FROM test WHERE token(k) >= token(%d) AND token(k) < token(%d)" % (inOrder[32], inOrder[65]))
        res = cursor.fetchall()
        assert res == [ [inOrder[x]] for x in range(32, 65) ], "%s [all: %s]" % (str(res), str(inOrder))

    def table_options_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                c int
            ) WITH comment = 'My comment'
               AND read_repair_chance = 0.5
               AND dclocal_read_repair_chance = 0.5
               AND gc_grace_seconds = 4
               AND bloom_filter_fp_chance = 0.01
               AND compaction = { 'class' : 'LeveledCompactionStrategy',
                                  'sstable_size_in_mb' : 10 }
               AND compression = { 'sstable_compression' : '' }
               AND caching = 'all'
        """)

        cursor.execute("""
            ALTER TABLE test
            WITH comment = 'other comment'
             AND read_repair_chance = 0.3
             AND dclocal_read_repair_chance = 0.3
             AND gc_grace_seconds = 100
             AND bloom_filter_fp_chance = 0.1
             AND compaction = { 'class' : 'SizeTieredCompactionStrategy',
                                'min_sstable_size' : 42 }
             AND compression = { 'sstable_compression' : 'SnappyCompressor' }
             AND caching = 'rows_only'
        """)

    def timestamp_and_ttl_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                c text,
                d text
            )
        """)

        cursor.execute("INSERT INTO test (k, c) VALUES (1, 'test')")
        cursor.execute("INSERT INTO test (k, c) VALUES (2, 'test') USING TTL 400")

        cursor.execute("SELECT k, c, writetime(c), ttl(c) FROM test")
        res = cursor.fetchall()
        assert len(res) == 2, res
        for r in res:
            assert isinstance(r[2], (int, long))
            if r[0] == 1:
                assert r[3] == None, res
            else:
                assert isinstance(r[3], (int, long)), res

        assert_invalid(cursor, "SELECT k, c, writetime(k) FROM test")

        cursor.execute("SELECT k, d, writetime(d) FROM test WHERE k = 1")
        res = cursor.fetchall()
        assert res == [[1, None, None]]

    def no_range_ghost_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v int
            )
        """)

        for k in range(0, 5):
            cursor.execute("INSERT INTO test (k, v) VALUES (%d, 0)" % k)

        cursor.execute("SELECT k FROM test")
        res = sorted(cursor.fetchall())
        assert res == [[k] for k in range(0, 5)], res

        cursor.execute("DELETE FROM test WHERE k=2")

        cursor.execute("SELECT k FROM test")
        res = sorted(cursor.fetchall())
        assert res == [[k] for k in range(0, 5) if k is not 2], res

        # Example from #3505
        cursor.execute("CREATE KEYSPACE ks1 with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };")
        cursor.execute("USE ks1")
        cursor.execute("""
            CREATE COLUMNFAMILY users (
                KEY varchar PRIMARY KEY,
                password varchar,
                gender varchar,
                birth_year bigint)
        """)

        cursor.execute("INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a')")
        cursor.execute("UPDATE users SET gender = 'm', birth_year = 1980 WHERE KEY = 'user1'")
        cursor.execute("SELECT * FROM users WHERE KEY='user1'")
        res = cursor.fetchall()
        assert res == [[ 'user1', 1980, 'm', 'ch@ngem3a' ]], res

        cursor.execute("TRUNCATE users")

        cursor.execute("SELECT * FROM users")
        res = cursor.fetchall()
        assert res == [], res

        cursor.execute("SELECT * FROM users WHERE KEY='user1'")
        res = cursor.fetchall()
        assert res == [], res


    def undefined_column_handling_test(self):
        cursor = self.prepare(ordered=True)

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v1 int,
                v2 int,
            )
        """)

        cursor.execute("INSERT INTO test (k, v1, v2) VALUES (0, 0, 0)")
        cursor.execute("INSERT INTO test (k, v1) VALUES (1, 1)")
        cursor.execute("INSERT INTO test (k, v1, v2) VALUES (2, 2, 2)")

        cursor.execute("SELECT v2 FROM test")
        res = cursor.fetchall()
        assert res == [[0], [None], [2]], res

        cursor.execute("SELECT v2 FROM test WHERE k = 1")
        res = cursor.fetchall()
        assert res == [[None]], res

    def range_tombstones_test(self):
        """ Test deletion by 'composite prefix' (range tombstones) """
        cluster = self.cluster

        # Uses 3 nodes just to make sure RowMutation are correctly serialized
        cluster.populate(3).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.patient_cql_connection(node1, version=cql_version).cursor()
        self.create_ks(cursor, 'ks', 1)

        cursor.execute("""
            CREATE TABLE test1 (
                k int,
                c1 int,
                c2 int,
                v1 int,
                v2 int,
                PRIMARY KEY (k, c1, c2)
            );
        """)
        time.sleep(1)

        rows = 5
        col1 = 2
        col2 = 2
        cpr = col1 * col2
        for i in xrange(0, rows):
            for j in xrange(0, col1):
                for k in xrange(0, col2):
                    n = (i * cpr) + (j * col2) + k
                    cursor.execute("INSERT INTO test1 (k, c1, c2, v1, v2) VALUES (%d, %d, %d, %d, %d)" % (i, j, k, n, n))

        for i in xrange(0, rows):
            cursor.execute("SELECT v1, v2 FROM test1 where k = %d" % i)
            res = cursor.fetchall()
            assert res == [[x, x] for x in xrange(i * cpr, (i + 1) * cpr)], res

        for i in xrange(0, rows):
            cursor.execute("DELETE FROM test1 WHERE k = %d AND c1 = 0" % i)

        for i in xrange(0, rows):
            cursor.execute("SELECT v1, v2 FROM test1 WHERE k = %d" % i)
            res = cursor.fetchall()
            assert res == [[x, x] for x in xrange(i * cpr + col1, (i + 1) * cpr)], res

        cluster.flush()
        time.sleep(0.2)

        for i in xrange(0, rows):
            cursor.execute("SELECT v1, v2 FROM test1 WHERE k = %d" % i)
            res = cursor.fetchall()
            assert res == [[x, x] for x in xrange(i * cpr + col1, (i + 1) * cpr)], res

    def range_tombstones_compaction_test(self):
        """ Test deletion by 'composite prefix' (range tombstones) with compaction """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test1 (
                k int,
                c1 int,
                c2 int,
                v1 text,
                PRIMARY KEY (k, c1, c2)
            );
        """)


        for c1 in range(0, 4):
            for c2 in range(0, 2):
                cursor.execute("INSERT INTO test1 (k, c1, c2, v1) VALUES (0, %d, %d, '%s')" % (c1, c2, '%i%i' % (c1, c2)))

        self.cluster.flush()

        cursor.execute("DELETE FROM test1 WHERE k = 0 AND c1 = 1")

        self.cluster.flush()
        self.cluster.compact()

        cursor.execute("SELECT v1 FROM test1 WHERE k = 0")
        res = cursor.fetchall()
        assert res == [ ['%i%i' % (c1, c2)] for c1 in xrange(0, 4) for c2 in xrange(0, 2) if c1 != 1], res

    def delete_row_test(self):
        """ Test deletion of rows """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                 k int,
                 c1 int,
                 c2 int,
                 v1 int,
                 v2 int,
                 PRIMARY KEY (k, c1, c2)
            );
        """)

        q = "INSERT INTO test (k, c1, c2, v1, v2) VALUES (%d, %d, %d, %d, %d)"
        cursor.execute(q % (0, 0, 0, 0, 0))
        cursor.execute(q % (0, 0, 1, 1, 1))
        cursor.execute(q % (0, 0, 2, 2, 2))
        cursor.execute(q % (0, 1, 0, 3, 3))

        cursor.execute("DELETE FROM test WHERE k = 0 AND c1 = 0 AND c2 = 0")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert len(res) == 3, res

    def range_query_2ndary_test(self):
        """ Test range queries with 2ndary indexes (#4257) """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE indextest (id int primary key, row int, setid int);")
        cursor.execute("CREATE INDEX indextest_setid_idx ON indextest (setid)")

        q =  "INSERT INTO indextest (id, row, setid) VALUES (%d, %d, %d);"
        cursor.execute(q % (0, 0, 0))
        cursor.execute(q % (1, 1, 0))
        cursor.execute(q % (2, 2, 0))
        cursor.execute(q % (3, 3, 0))

        assert_invalid(cursor, "SELECT * FROM indextest WHERE setid = 0 AND row < 1;")
        cursor.execute("SELECT * FROM indextest WHERE setid = 0 AND row < 1 ALLOW FILTERING;")
        res = cursor.fetchall()
        assert res == [[0, 0, 0]], res

    def compression_option_validation_test(self):
        """ Check for unknown compression parameters options (#4266) """
        cursor = self.prepare()

        assert_invalid(cursor, """
          CREATE TABLE users (key varchar PRIMARY KEY, password varchar, gender varchar)
          WITH compression_parameters:sstable_compressor = 'DeflateCompressor';
        """)

        if self.cluster.version() >= '1.2':
            assert_invalid(cursor, """
              CREATE TABLE users (key varchar PRIMARY KEY, password varchar, gender varchar)
              WITH compression = { 'sstable_compressor' : 'DeflateCompressor' };
            """)

    def keyspace_creation_options_test(self):
        """ Check one can use arbitrary name for datacenter when creating keyspace (#4278) """
        cursor = self.prepare()

        # we just want to make sure the following is valid
        if self.cluster.version() >= '1.2':
            cursor.execute("""
                CREATE KEYSPACE Foo
                    WITH replication = { 'class' : 'NetworkTopologyStrategy',
                                         'us-east' : 1,
                                         'us-west' : 1 };
            """)
        else:
            cursor.execute("""
                CREATE KEYSPACE Foo
                    WITH strategy_class='NetworkTopologyStrategy'
                     AND strategy_options:"us-east"=1
                     AND strategy_options:"us-west"=1;
            """)

    def set_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE user (
                fn text,
                ln text,
                tags set<text>,
                PRIMARY KEY (fn, ln)
            )
        """)

        q = "UPDATE user SET %s WHERE fn='Tom' AND ln='Bombadil'"
        cursor.execute(q % "tags = tags + { 'foo' }")
        cursor.execute(q % "tags = tags + { 'bar' }")
        cursor.execute(q % "tags = tags + { 'foo' }")
        cursor.execute(q % "tags = tags + { 'foobar' }")
        cursor.execute(q % "tags = tags - { 'bar' }")

        cursor.execute("SELECT tags FROM user")
        res = cursor.fetchall()
        assert res == [[set(['foo', 'foobar'])]], res

        q = "UPDATE user SET %s WHERE fn='Bilbo' AND ln='Baggins'"
        cursor.execute(q % "tags = { 'a', 'c', 'b' }")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[set(['a', 'b', 'c'])]], res

        time.sleep(.01)

        cursor.execute(q % "tags = { 'm', 'n' }")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[set(['m', 'n'])]], res

        cursor.execute("DELETE tags['m'] FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[set(['n'])]], res

        cursor.execute("DELETE tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        if self.cluster.version() <= "1.2":
            assert res == [None], res
        else:
            assert res == [], res


    def map_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE user (
                fn text,
                ln text,
                m map<text, int>,
                PRIMARY KEY (fn, ln)
            )
        """)

        q = "UPDATE user SET %s WHERE fn='Tom' AND ln='Bombadil'"
        cursor.execute(q % "m['foo'] = 3")
        cursor.execute(q % "m['bar'] = 4")
        cursor.execute(q % "m['woot'] = 5")
        cursor.execute(q % "m['bar'] = 6")
        cursor.execute("DELETE m['foo'] FROM user WHERE fn='Tom' AND ln='Bombadil'")

        cursor.execute("SELECT m FROM user")
        res = cursor.fetchall()
        assert res == [[{ 'woot': 5, 'bar' : 6 }]], res

        q = "UPDATE user SET %s WHERE fn='Bilbo' AND ln='Baggins'"
        cursor.execute(q % "m = { 'a' : 4 , 'c' : 3, 'b' : 2 }")
        cursor.execute("SELECT m FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ {'a' : 4, 'b' : 2, 'c' : 3 } ]], res

        time.sleep(.01)

        # Check we correctly overwrite
        cursor.execute(q % "m = { 'm' : 4 , 'n' : 1, 'o' : 2 }")
        cursor.execute("SELECT m FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ {'m' : 4, 'n' : 1, 'o' : 2 } ]], res

        cursor.execute(q % "m = {}")
        cursor.execute("SELECT m FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        if self.cluster.version() <= "1.2":
            assert res == [None], res
        else:
            assert res == [], res

    def list_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE user (
                fn text,
                ln text,
                tags list<text>,
                PRIMARY KEY (fn, ln)
            )
        """)

        q = "UPDATE user SET %s WHERE fn='Tom' AND ln='Bombadil'"
        cursor.execute(q % "tags = tags + [ 'foo' ]")
        cursor.execute(q % "tags = tags + [ 'bar' ]")
        cursor.execute(q % "tags = tags + [ 'foo' ]")
        cursor.execute(q % "tags = tags + [ 'foobar' ]")

        cursor.execute("SELECT tags FROM user")
        res = cursor.fetchall()
        assert res == [[ ('foo', 'bar', 'foo', 'foobar') ]], res

        q = "UPDATE user SET %s WHERE fn='Bilbo' AND ln='Baggins'"
        cursor.execute(q % "tags = [ 'a', 'c', 'b', 'c' ]")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ ('a', 'c', 'b', 'c') ]], res

        cursor.execute(q % "tags = [ 'm', 'n' ] + tags")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ ('n', 'm', 'a', 'c', 'b', 'c') ]], res

        cursor.execute(q % "tags[2] = 'foo', tags[4] = 'bar'")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ ('n', 'm', 'foo', 'c', 'bar', 'c') ]], res

        cursor.execute("DELETE tags[2] FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ ('n', 'm', 'c', 'bar', 'c') ]], res

        cursor.execute(q % "tags = tags - [ 'bar' ]")
        cursor.execute("SELECT tags FROM user WHERE fn='Bilbo' AND ln='Baggins'")
        res = cursor.fetchall()
        assert res == [[ ('n', 'm', 'c', 'c') ]], res

    def multi_collection_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE foo(
                k uuid PRIMARY KEY,
                L list<int>,
                M map<text, int>,
                S set<int>
            );
        """)

        cursor.execute("UPDATE ks.foo SET L = [1, 3, 5] WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")
        cursor.execute("UPDATE ks.foo SET L = L + [7, 11, 13] WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")
        cursor.execute("UPDATE ks.foo SET S = {1, 3, 5} WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")
        cursor.execute("UPDATE ks.foo SET S = S + {7, 11, 13} WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")
        cursor.execute("UPDATE ks.foo SET M = {'foo': 1, 'bar' : 3} WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")
        cursor.execute("UPDATE ks.foo SET M = M + {'foobar' : 4} WHERE k = b017f48f-ae67-11e1-9096-005056c00008;")

        cursor.execute("SELECT L, M, S FROM foo WHERE k = b017f48f-ae67-11e1-9096-005056c00008")
        res = cursor.fetchall()
        assert res == [[
            (1, 3, 5, 7, 11, 13),
            {'foo' : 1, 'bar' : 3, 'foobar' : 4},
            set([1, 3, 5, 7, 11, 13]),
        ]], res

    def range_query_test(self):
        """ Range test query from #4372 """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (a int, b int, c int, d int, e int, f text, PRIMARY KEY (a, b, c, d, e) )")

        cursor.execute("INSERT INTO test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 2, '2');")
        cursor.execute("INSERT INTO test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 1, '1');")
        cursor.execute("INSERT INTO test (a, b, c, d, e, f) VALUES (1, 1, 1, 2, 1, '1');")
        cursor.execute("INSERT INTO test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 3, '3');")
        cursor.execute("INSERT INTO test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 5, '5');")

        cursor.execute("SELECT a, b, c, d, e, f FROM test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e >= 2;")
        res = cursor.fetchall()
        assert res == [[1, 1, 1, 1, 2, u'2'], [1, 1, 1, 1, 3, u'3'], [1, 1, 1, 1, 5, u'5']], res

    def update_type_test(self):
        """ Test altering the type of a column, including the one in the primary key (#4041) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k text,
                c text,
                s set<text>,
                v text,
                PRIMARY KEY (k, c)
            )
        """)

        req = "INSERT INTO test (k, c, v, s) VALUES ('%s', '%s', '%s', {'%s'})"
        # using utf8 character so that we can see the transition to BytesType
        cursor.execute(req % ('', '', '', ''))

        cursor.execute("SELECT * FROM test")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[u'', u'', set([u'']), u'']], res

        cursor.execute("ALTER TABLE test ALTER v TYPE blob")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        # the last should not be utf8 but a raw string
        assert res == [[u'', u'', set([u'']), '']], res

        cursor.execute("ALTER TABLE test ALTER k TYPE blob")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [['', u'', set([u'']), '']], res

        cursor.execute("ALTER TABLE test ALTER c TYPE blob")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [['', '', set([u'']), '']], res

        if self.cluster.version() < "2.1":
            assert_invalid(cursor, "ALTER TABLE test ALTER s TYPE set<blob>")
        else:
            cursor.execute("ALTER TABLE test ALTER s TYPE set<blob>")
            cursor.execute("SELECT * FROM test")
            res = cursor.fetchall()
            assert res == [['', '', set(['']), '']], res


    def composite_row_key_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k1 int,
                k2 int,
                c int,
                v int,
                PRIMARY KEY ((k1, k2), c)
            )
        """)

        req = "INSERT INTO test (k1, k2, c, v) VALUES (%d, %d, %d, %d)"
        for i in range(0, 4):
            cursor.execute(req % (0, i, i, i))

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[0, 2, 2, 2], [0, 3, 3, 3], [0, 0, 0, 0], [0, 1, 1, 1]], res

        cursor.execute("SELECT * FROM test WHERE k1 = 0 and k2 IN (1, 3)")
        res = cursor.fetchall()
        assert res == [[0, 1, 1, 1], [0, 3, 3, 3]], res

        assert_invalid(cursor, "SELECT * FROM test WHERE k2 = 3")
        assert_invalid(cursor, "SELECT * FROM test WHERE k1 IN (0, 1) and k2 = 3")

        cursor.execute("SELECT * FROM test WHERE token(k1, k2) = token(0, 1)")
        res = cursor.fetchall()
        assert res == [[0, 1, 1, 1]], res

        cursor.execute("SELECT * FROM test WHERE token(k1, k2) > " + str(-((2**63)-1)))
        res = cursor.fetchall()
        assert res == [[0, 2, 2, 2], [0, 3, 3, 3], [0, 0, 0, 0], [0, 1, 1, 1]], res

    def cql3_insert_thrift_test(self):
        """ Check that we can insert from thrift into a CQL3 table (#4377) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            )
        """)

        cli = self.cluster.nodelist()[0].cli()
        cli.do("use ks")
        cli.do("set test[2]['4:v'] = int(200)")
        assert not cli.has_errors(), cli.errors()

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [ [2, 4, 200] ], res

    def row_existence_test(self):
        """ Check the semantic of CQL row existence (part of #4361) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v1 int,
                v2 int,
                PRIMARY KEY (k, c)
            )
        """)

        cursor.execute("INSERT INTO test (k, c, v1, v2) VALUES (1, 1, 1, 1)")

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[1, 1, 1, 1]], res

        assert_invalid(cursor, "DELETE c FROM test WHERE k = 1 AND c = 1")

        cursor.execute("DELETE v2 FROM test WHERE k = 1 AND c = 1")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[1, 1, 1, None]], res

        cursor.execute("DELETE v1 FROM test WHERE k = 1 AND c = 1")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[1, 1, None, None]], res

        cursor.execute("DELETE FROM test WHERE k = 1 AND c = 1")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [], res

        cursor.execute("INSERT INTO test (k, c) VALUES (2, 2)")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[2, 2, None, None]], res

    def only_pk_test(self):
        """ Check table with only a PK (#4361) """
        cursor = self.prepare(ordered=True)

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                PRIMARY KEY (k, c)
            )
        """)

        q = "INSERT INTO test (k, c) VALUES (%d, %d)"
        for k in range(0, 2):
            for c in range(0, 2):
                cursor.execute(q % (k, c))

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[x, y] for x in range(0, 2) for y in range(0, 2)], res

        # Check for dense tables too
        cursor.execute("""
            CREATE TABLE test2 (
                k int,
                c int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE
        """)

        q = "INSERT INTO test2 (k, c) VALUES (%d, %d)"
        for k in range(0, 2):
            for c in range(0, 2):
                cursor.execute(q % (k, c))

        cursor.execute("SELECT * FROM test2")
        res = cursor.fetchall()
        assert res == [[x, y] for x in range(0, 2) for y in range(0, 2)], res

    def date_test(self):
        """ Check dates are correctly recognized and validated """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                t timestamp
            )
        """)

        cursor.execute("INSERT INTO test (k, t) VALUES (0, '2011-02-03')")
        assert_invalid(cursor, "INSERT INTO test (k, t) VALUES (0, '2011-42-42')")

    def range_slice_test(self):
        """ Test a regression from #1337 """

        cluster = self.cluster

        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.patient_cql_connection(node1, version=cql_version).cursor()
        self.create_ks(cursor, 'ks', 1)

        cursor.execute("""
            CREATE TABLE test (
                k text PRIMARY KEY,
                v int
            );
        """)
        time.sleep(1)

        cursor.execute("INSERT INTO test (k, v) VALUES ('foo', 0)")
        cursor.execute("INSERT INTO test (k, v) VALUES ('bar', 1)")

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert len(res) == 2, res

    def composite_index_with_pk_test(self):

        cursor = self.prepare(ordered=True)
        cursor.execute("""
            CREATE TABLE blogs (
                blog_id int,
                time1 int,
                time2 int,
                author text,
                content text,
                PRIMARY KEY (blog_id, time1, time2)
            )
        """)

        cursor.execute("CREATE INDEX ON blogs(author)")

        req = "INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (%d, %d, %d, '%s', '%s')"
        cursor.execute(req % (1, 0, 0, 'foo', 'bar1'))
        cursor.execute(req % (1, 0, 1, 'foo', 'bar2'))
        cursor.execute(req % (2, 1, 0, 'foo', 'baz'))
        cursor.execute(req % (3, 0, 1, 'gux', 'qux'))

        cursor.execute("SELECT blog_id, content FROM blogs WHERE author='foo'")
        res = cursor.fetchall()
        assert res == [[1, 'bar1'], [1, 'bar2'], [2, 'baz']], res

        cursor.execute("SELECT blog_id, content FROM blogs WHERE time1 > 0 AND author='foo'")
        res = cursor.fetchall()
        assert res == [[2, 'baz']], res

        cursor.execute("SELECT blog_id, content FROM blogs WHERE time1 = 1 AND author='foo'")
        res = cursor.fetchall()
        assert res == [[2, 'baz']], res

        cursor.execute("SELECT blog_id, content FROM blogs WHERE time1 = 1 AND time2 = 0 AND author='foo'")
        res = cursor.fetchall()
        assert res == [[2, 'baz']], res

        cursor.execute("SELECT content FROM blogs WHERE time1 = 1 AND time2 = 1 AND author='foo'")
        res = cursor.fetchall()
        assert res == [], res

        cursor.execute("SELECT content FROM blogs WHERE time1 = 1 AND time2 > 0 AND author='foo'")
        res = cursor.fetchall()
        assert res == [], res

        assert_invalid(cursor, "SELECT content FROM blogs WHERE time2 >= 0 AND author='foo'")

    def limit_bugs_test(self):
        """ Test for LIMIT bugs from 4579 """

        cursor = self.prepare(ordered=True)
        cursor.execute("""
            CREATE TABLE testcf (
                a int,
                b int,
                c int,
                d int,
                e int,
                PRIMARY KEY (a, b)
            );
        """)

        cursor.execute("INSERT INTO testcf (a, b, c, d, e) VALUES (1, 1, 1, 1, 1);")
        cursor.execute("INSERT INTO testcf (a, b, c, d, e) VALUES (2, 2, 2, 2, 2);")
        cursor.execute("INSERT INTO testcf (a, b, c, d, e) VALUES (3, 3, 3, 3, 3);")
        cursor.execute("INSERT INTO testcf (a, b, c, d, e) VALUES (4, 4, 4, 4, 4);")

        cursor.execute("SELECT * FROM testcf;")
        res = cursor.fetchall()
        assert res == [[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]], res

        cursor.execute("SELECT * FROM testcf LIMIT 1;") # columns d and e in result row are null
        res = cursor.fetchall()
        assert res == [[1, 1, 1, 1, 1]], res

        cursor.execute("SELECT * FROM testcf LIMIT 2;") # columns d and e in last result row are null
        res = cursor.fetchall()
        assert res == [[1, 1, 1, 1, 1], [2, 2, 2, 2, 2]], res

        cursor.execute("""
            CREATE TABLE testcf2 (
                a int primary key,
                b int,
                c int,
            );
        """)

        cursor.execute("INSERT INTO testcf2 (a, b, c) VALUES (1, 1, 1);")
        cursor.execute("INSERT INTO testcf2 (a, b, c) VALUES (2, 2, 2);")
        cursor.execute("INSERT INTO testcf2 (a, b, c) VALUES (3, 3, 3);")
        cursor.execute("INSERT INTO testcf2 (a, b, c) VALUES (4, 4, 4);")

        cursor.execute("SELECT * FROM testcf2;")
        res = cursor.fetchall()
        assert res == [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]], res

        cursor.execute("SELECT * FROM testcf2 LIMIT 1;") # gives 1 row
        res = cursor.fetchall()
        assert res == [[1, 1, 1]], res

        cursor.execute("SELECT * FROM testcf2 LIMIT 2;") # gives 1 row
        res = cursor.fetchall()
        assert res == [[1, 1, 1], [2, 2, 2]], res

        cursor.execute("SELECT * FROM testcf2 LIMIT 3;") # gives 2 rows
        res = cursor.fetchall()
        assert res == [[1, 1, 1], [2, 2, 2], [3, 3, 3]], res

        cursor.execute("SELECT * FROM testcf2 LIMIT 4;") # gives 2 rows
        res = cursor.fetchall()
        assert res == [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]], res

        cursor.execute("SELECT * FROM testcf2 LIMIT 5;") # gives 3 rows
        res = cursor.fetchall()
        assert res == [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]], res

    def bug_4532_test(self):

        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE compositetest(
                status ascii,
                ctime bigint,
                key ascii,
                nil ascii,
                PRIMARY KEY (status, ctime, key)
            )
        """)

        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key1','')")
        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key2','')")
        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key3','')")
        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key4','')")
        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key5','')")
        cursor.execute("INSERT INTO compositetest(status,ctime,key,nil) VALUES ('C',12345680,'key6','')")

        assert_invalid(cursor, "SELECT * FROM compositetest WHERE ctime>=12345679 AND key='key3' AND ctime<=12345680 LIMIT 3;")
        assert_invalid(cursor, "SELECT * FROM compositetest WHERE ctime=12345679  AND key='key3' AND ctime<=12345680 LIMIT 3")

    def order_by_multikey_test(self):
        """ Test for #4612 bug and more generaly order by when multiple C* rows are queried """

        cursor = self.prepare(ordered=True)
        cursor.execute("""
            CREATE TABLE test(
                my_id varchar,
                col1 int,
                col2 int,
                value varchar,
                PRIMARY KEY (my_id, col1, col2)
            );
        """)

        cursor.execute("INSERT INTO test(my_id, col1, col2, value) VALUES ( 'key1', 1, 1, 'a');")
        cursor.execute("INSERT INTO test(my_id, col1, col2, value) VALUES ( 'key2', 3, 3, 'a');")
        cursor.execute("INSERT INTO test(my_id, col1, col2, value) VALUES ( 'key3', 2, 2, 'b');")
        cursor.execute("INSERT INTO test(my_id, col1, col2, value) VALUES ( 'key4', 2, 1, 'b');")

        cursor.execute("SELECT col1 FROM test WHERE my_id in('key1', 'key2', 'key3') ORDER BY col1;")
        res = cursor.fetchall()
        assert res == [[1], [2], [3]], res

        cursor.execute("SELECT col1, value, my_id, col2 FROM test WHERE my_id in('key3', 'key4') ORDER BY col1, col2;")
        res = cursor.fetchall()
        assert res == [[2, 'b', 'key4', 1], [2, 'b', 'key3', 2]], res

        assert_invalid(cursor, "SELECT col1 FROM test ORDER BY col1;")
        assert_invalid(cursor, "SELECT col1 FROM test WHERE my_id > 'key1' ORDER BY col1;")

    def create_alter_options_test(self):
        cursor = self.prepare(create_keyspace=False)

        assert_invalid(cursor, "CREATE KEYSPACE ks1")
        assert_invalid(cursor, "CREATE KEYSPACE ks1 WITH replication= { 'replication_factor' : 1 }")

        cursor.execute("CREATE KEYSPACE ks1 WITH replication={ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }")
        cursor.execute("CREATE KEYSPACE ks2 WITH replication={ 'class' : 'SimpleStrategy', 'replication_factor' : 1 } AND durable_writes=false")

        cursor.execute("SELECT keyspace_name, durable_writes FROM system.schema_keyspaces")
        res = cursor.fetchall()
        assert res == [ ['ks1', True], ['system', True], ['system_traces', True], ['ks2', False] ], res

        cursor.execute("ALTER KEYSPACE ks1 WITH replication = { 'class' : 'NetworkTopologyStrategy', 'dc1' : 1 } AND durable_writes=False")
        cursor.execute("ALTER KEYSPACE ks2 WITH durable_writes=true")
        cursor.execute("SELECT keyspace_name, durable_writes, strategy_class FROM system.schema_keyspaces")
        res = cursor.fetchall()
        assert res == [ [u'ks1', False, u'org.apache.cassandra.locator.NetworkTopologyStrategy'],
                      [u'system', True, u'org.apache.cassandra.locator.LocalStrategy'],
                      [u'system_traces', True, u'org.apache.cassandra.locator.SimpleStrategy'],
                      [u'ks2', True, u'org.apache.cassandra.locator.SimpleStrategy'] ]

        cursor.execute("USE ks1")

        assert_invalid(cursor, "CREATE TABLE cf1 (a int PRIMARY KEY, b int) WITH compaction = { 'min_threshold' : 4 }")
        cursor.execute("CREATE TABLE cf1 (a int PRIMARY KEY, b int) WITH compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_threshold' : 7 }")
        cursor.execute("SELECT columnfamily_name, min_compaction_threshold FROM system.schema_columnfamilies WHERE keyspace_name='ks1'")
        res = cursor.fetchall()
        assert res == [ ['cf1', 7] ], res

    def remove_range_slice_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v int
            )
        """)

        for i in range(0, 3):
            cursor.execute("INSERT INTO test (k, v) VALUES (%d, %d)" % (i, i))

        cursor.execute("DELETE FROM test WHERE k = 1")
        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert res == [[0, 0], [2, 2]], res

    def indexes_composite_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                blog_id int,
                timestamp int,
                author text,
                content text,
                PRIMARY KEY (blog_id, timestamp)
            )
        """)

        req = "INSERT INTO test (blog_id, timestamp, author, content) VALUES (%d, %d, '%s', '%s')"
        cursor.execute(req % (0, 0, "bob", "1st post"))
        cursor.execute(req % (0, 1, "tom", "2nd post"))
        cursor.execute(req % (0, 2, "bob", "3rd post"))
        cursor.execute(req % (0, 3, "tom", "4nd post"))
        cursor.execute(req % (1, 0, "bob", "5th post"))

        cursor.execute("CREATE INDEX ON test(author)")
        time.sleep(1)

        cursor.execute("SELECT blog_id, timestamp FROM test WHERE author = 'bob'")
        res = cursor.fetchall()
        assert res == [[1, 0], [0, 0], [0, 2]], res

        cursor.execute(req % (1, 1, "tom", "6th post"))
        cursor.execute(req % (1, 2, "tom", "7th post"))
        cursor.execute(req % (1, 3, "bob", "8th post"))

        cursor.execute("SELECT blog_id, timestamp FROM test WHERE author = 'bob'")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 3], [0, 0], [0, 2]], res

        cursor.execute("DELETE FROM test WHERE blog_id = 0 AND timestamp = 2")

        cursor.execute("SELECT blog_id, timestamp FROM test WHERE author = 'bob'")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 3], [0, 0]], res


    def refuse_in_with_indexes_test(self):
        """ Test for the validation bug of #4709 """

        cursor = self.prepare()
        cursor.execute("create table t1 (pk varchar primary key, col1 varchar, col2 varchar);")
        cursor.execute("create index t1_c1 on t1(col1);")
        cursor.execute("create index t1_c2 on t1(col2);")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk1','foo1','bar1');")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk1a','foo1','bar1');")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk1b','foo1','bar1');")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk1c','foo1','bar1');")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk2','foo2','bar2');")
        cursor.execute("insert into t1  (pk, col1, col2) values ('pk3','foo3','bar3');")
        assert_invalid(cursor, "select * from t1 where col2 in ('bar1', 'bar2');")

    def validate_counter_regular_test(self):
        """ Test for the validation bug of #4706 """

        cursor = self.prepare()
        assert_invalid(cursor, "CREATE TABLE test (id bigint PRIMARY KEY, count counter, things set<text>)")

    def reversed_compact_test(self):
        """ Test for #4716 bug and more generally for good behavior of ordering"""

        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test1 (
                k text,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE
              AND CLUSTERING ORDER BY (c DESC);
        """)

        for i in range(0, 10):
            cursor.execute("INSERT INTO test1(k, c, v) VALUES ('foo', %i, %i)" % (i, i))

        cursor.execute("SELECT c FROM test1 WHERE c > 2 AND c < 6 AND k = 'foo'")
        res = cursor.fetchall()
        assert res == [[5], [4], [3]], res

        cursor.execute("SELECT c FROM test1 WHERE c >= 2 AND c <= 6 AND k = 'foo'")
        res = cursor.fetchall()
        assert res == [[6], [5], [4], [3], [2]], res

        cursor.execute("SELECT c FROM test1 WHERE c > 2 AND c < 6 AND k = 'foo' ORDER BY c ASC")
        res = cursor.fetchall()
        assert res == [[3], [4], [5]], res

        cursor.execute("SELECT c FROM test1 WHERE c >= 2 AND c <= 6 AND k = 'foo' ORDER BY c ASC")
        res = cursor.fetchall()
        assert res == [[2], [3], [4], [5], [6]], res

        cursor.execute("SELECT c FROM test1 WHERE c > 2 AND c < 6 AND k = 'foo' ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[5], [4], [3]], res

        cursor.execute("SELECT c FROM test1 WHERE c >= 2 AND c <= 6 AND k = 'foo' ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[6], [5], [4], [3], [2]], res

        cursor.execute("""
            CREATE TABLE test2 (
                k text,
                c int,
                v int,
                PRIMARY KEY (k, c)
            ) WITH COMPACT STORAGE;
        """)

        for i in range(0, 10):
            cursor.execute("INSERT INTO test2(k, c, v) VALUES ('foo', %i, %i)" % (i, i))

        cursor.execute("SELECT c FROM test2 WHERE c > 2 AND c < 6 AND k = 'foo'")
        res = cursor.fetchall()
        assert res == [[3], [4], [5]], res

        cursor.execute("SELECT c FROM test2 WHERE c >= 2 AND c <= 6 AND k = 'foo'")
        res = cursor.fetchall()
        assert res == [[2], [3], [4], [5], [6]], res

        cursor.execute("SELECT c FROM test2 WHERE c > 2 AND c < 6 AND k = 'foo' ORDER BY c ASC")
        res = cursor.fetchall()
        assert res == [[3], [4], [5]], res

        cursor.execute("SELECT c FROM test2 WHERE c >= 2 AND c <= 6 AND k = 'foo' ORDER BY c ASC")
        res = cursor.fetchall()
        assert res == [[2], [3], [4], [5], [6]], res

        cursor.execute("SELECT c FROM test2 WHERE c > 2 AND c < 6 AND k = 'foo' ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[5], [4], [3]], res

        cursor.execute("SELECT c FROM test2 WHERE c >= 2 AND c <= 6 AND k = 'foo' ORDER BY c DESC")
        res = cursor.fetchall()
        assert res == [[6], [5], [4], [3], [2]], res

    def unescaped_string_test(self):

        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test (
                k text PRIMARY KEY,
                c text,
            )
        """)

        assert_invalid(cursor, "INSERT INTO test (k, c) VALUES ('foo', 'CQL is cassandra's best friend')")

    def reversed_compact_multikey_test(self):
        """ Test for the bug from #4760 and #4759 """

        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test (
                key text,
                c1 int,
                c2 int,
                value text,
                PRIMARY KEY(key, c1, c2)
                ) WITH COMPACT STORAGE
                  AND CLUSTERING ORDER BY(c1 DESC, c2 DESC);
        """)

        for i in range(0, 3):
            for j in range(0, 3):
                cursor.execute("INSERT INTO test(key, c1, c2, value) VALUES ('foo', %i, %i, 'bar');" % (i, j))

        # Equalities

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 = 1")
        res = cursor.fetchall()
        assert res == [[1, 2], [1, 1], [1, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 = 1 ORDER BY c1 ASC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 1], [1, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 = 1 ORDER BY c1 DESC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[1, 2], [1, 1], [1, 0]], res

        # GT

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 > 1")
        res = cursor.fetchall()
        assert res == [[2, 2], [2, 1], [2, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 > 1 ORDER BY c1 ASC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[2, 0], [2, 1], [2, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 > 1 ORDER BY c1 DESC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[2, 2], [2, 1], [2, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 >= 1")
        res = cursor.fetchall()
        assert res == [[2, 2], [2, 1], [2, 0], [1, 2], [1, 1], [1, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 >= 1 ORDER BY c1 ASC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 >= 1 ORDER BY c1 ASC")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 >= 1 ORDER BY c1 DESC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[2, 2], [2, 1], [2, 0], [1, 2], [1, 1], [1, 0]], res

        # LT

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 < 1")
        res = cursor.fetchall()
        assert res == [[0, 2], [0, 1], [0, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 < 1 ORDER BY c1 ASC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[0, 0], [0, 1], [0, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 < 1 ORDER BY c1 DESC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[0, 2], [0, 1], [0, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 <= 1")
        res = cursor.fetchall()
        assert res == [[1, 2], [1, 1], [1, 0], [0, 2], [0, 1], [0, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 <= 1 ORDER BY c1 ASC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 <= 1 ORDER BY c1 ASC")
        res = cursor.fetchall()
        assert res == [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE key='foo' AND c1 <= 1 ORDER BY c1 DESC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[1, 2], [1, 1], [1, 0], [0, 2], [0, 1], [0, 0]], res

    def collection_and_regular_test(self):

        cursor = self.prepare()

        cursor.execute("""
          CREATE TABLE test (
            k int PRIMARY KEY,
            l list<int>,
            c int
          )
        """)

        cursor.execute("INSERT INTO test(k, l, c) VALUES(3, [0, 1, 2], 4)")
        cursor.execute("UPDATE test SET l[0] = 1, c = 42 WHERE k = 3")
        cursor.execute("SELECT l, c FROM test WHERE k = 3")
        res = cursor.fetchall()
        assert res == [[(1, 1, 2), 42]], res

    def batch_and_list_test(self):
        cursor = self.prepare()

        cursor.execute("""
          CREATE TABLE test (
            k int PRIMARY KEY,
            l list<int>
          )
        """)

        cursor.execute("""
          BEGIN BATCH
            UPDATE test SET l = l + [ 1 ] WHERE k = 0;
            UPDATE test SET l = l + [ 2 ] WHERE k = 0;
            UPDATE test SET l = l + [ 3 ] WHERE k = 0;
          APPLY BATCH
        """)

        cursor.execute("SELECT l FROM test WHERE k = 0")
        res = cursor.fetchall()
        assert res == [[(1, 2, 3)]], res

        cursor.execute("""
          BEGIN BATCH
            UPDATE test SET l = [ 1 ] + l WHERE k = 1;
            UPDATE test SET l = [ 2 ] + l WHERE k = 1;
            UPDATE test SET l = [ 3 ] + l WHERE k = 1;
          APPLY BATCH
        """)

        cursor.execute("SELECT l FROM test WHERE k = 1")
        res = cursor.fetchall()
        assert res == [[(3, 2, 1)]], res

    def boolean_test(self):
        cursor = self.prepare()

        cursor.execute("""
          CREATE TABLE test (
            k boolean PRIMARY KEY,
            b boolean
          )
        """)

        cursor.execute("INSERT INTO test (k, b) VALUES (true, false)")
        cursor.execute("SELECT * FROM test WHERE k = true")
        res = cursor.fetchall()
        assert res == [[True, False]], res

    def multiordering_test(self):
        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test (
                k text,
                c1 int,
                c2 int,
                PRIMARY KEY (k, c1, c2)
            ) WITH CLUSTERING ORDER BY (c1 ASC, c2 DESC);
        """)

        for i in range(0, 2):
            for j in range(0, 2):
                cursor.execute("INSERT INTO test(k, c1, c2) VALUES ('foo', %i, %i)" % (i, j))

        cursor.execute("SELECT c1, c2 FROM test WHERE k = 'foo'")
        res = cursor.fetchall()
        assert res == [[0, 1], [0, 0], [1, 1], [1, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE k = 'foo' ORDER BY c1 ASC, c2 DESC")
        res = cursor.fetchall()
        assert res == [[0, 1], [0, 0], [1, 1], [1, 0]], res

        cursor.execute("SELECT c1, c2 FROM test WHERE k = 'foo' ORDER BY c1 DESC, c2 ASC")
        res = cursor.fetchall()
        assert res == [[1, 0], [1, 1], [0, 0], [0, 1]], res

        assert_invalid(cursor, "SELECT c1, c2 FROM test WHERE k = 'foo' ORDER BY c2 DESC")
        assert_invalid(cursor, "SELECT c1, c2 FROM test WHERE k = 'foo' ORDER BY c2 ASC")
        assert_invalid(cursor, "SELECT c1, c2 FROM test WHERE k = 'foo' ORDER BY c1 ASC, c2 ASC")

    def multiordering_validation_test(self):
        cursor = self.prepare()

        assert_invalid(cursor, "CREATE TABLE test (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2)) WITH CLUSTERING ORDER BY (c2 DESC)")
        assert_invalid(cursor, "CREATE TABLE test (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2)) WITH CLUSTERING ORDER BY (c2 ASC, c1 DESC)")
        assert_invalid(cursor, "CREATE TABLE test (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2)) WITH CLUSTERING ORDER BY (c1 DESC, c2 DESC, c3 DESC)")

        cursor.execute("CREATE TABLE test1 (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2)) WITH CLUSTERING ORDER BY (c1 DESC, c2 DESC)")
        cursor.execute("CREATE TABLE test2 (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2)) WITH CLUSTERING ORDER BY (c1 ASC, c2 DESC)")

    def bug_4882_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c1 int,
                c2 int,
                v int,
                PRIMARY KEY (k, c1, c2)
            ) WITH CLUSTERING ORDER BY (c1 ASC, c2 DESC);
        """)

        cursor.execute("INSERT INTO test (k, c1, c2, v) VALUES (0, 0, 0, 0);")
        cursor.execute("INSERT INTO test (k, c1, c2, v) VALUES (0, 1, 1, 1);")
        cursor.execute("INSERT INTO test (k, c1, c2, v) VALUES (0, 0, 2, 2);")
        cursor.execute("INSERT INTO test (k, c1, c2, v) VALUES (0, 1, 3, 3);")

        cursor.execute("select * from test where k = 0 limit 1;")
        res = cursor.fetchall()
        assert res == [[0, 0, 2, 2]], res

    def multi_list_set_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                l1 list<int>,
                l2 list<int>
            )
        """)

        cursor.execute("INSERT INTO test (k, l1, l2) VALUES (0, [1, 2, 3], [4, 5, 6])")
        cursor.execute("UPDATE test SET l2[1] = 42, l1[1] = 24  WHERE k = 0")

        cursor.execute("SELECT l1, l2 FROM test WHERE k = 0")
        res = cursor.fetchall()
        assert res == [[(1, 24, 3), (4, 42, 6)]], res

    def buggy_prepare(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                l list<int>,
            )
        """)

        from cql import query
        p = query.prepare_query("INSERT INTO test (k, l) VALUES (0, [?, ?])")
        print p

    def composite_index_collections_test(self):
        cursor = self.prepare(ordered=True)
        cursor.execute("""
            CREATE TABLE blogs (
                blog_id int,
                time1 int,
                time2 int,
                author text,
                content set<text>,
                PRIMARY KEY (blog_id, time1, time2)
            )
        """)

        cursor.execute("CREATE INDEX ON blogs(author)")

        req = "INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (%d, %d, %d, '%s', %s)"
        cursor.execute(req % (1, 0, 0, 'foo', "{ 'bar1', 'bar2' }"))
        cursor.execute(req % (1, 0, 1, 'foo', "{ 'bar2', 'bar3' }"))
        cursor.execute(req % (2, 1, 0, 'foo', "{ 'baz' }"))
        cursor.execute(req % (3, 0, 1, 'gux', "{ 'qux' }"))

        cursor.execute("SELECT blog_id, content FROM blogs WHERE author='foo'")
        res = cursor.fetchall()
        assert res == [[1, set(['bar1', 'bar2'])], [1, set(['bar2', 'bar3'])], [2, set(['baz'])]], res

    def truncate_clean_cache_test(self):
        cursor = self.prepare(ordered=True, use_cache=True)

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v1 int,
                v2 int,
            ) WITH CACHING = ALL;
        """)

        for i in range(0, 3):
            cursor.execute("INSERT INTO test(k, v1, v2) VALUES (%d, %d, %d)" % (i, i, i*2))

        cursor.execute("SELECT v1, v2 FROM test WHERE k IN (0, 1, 2)")
        res = cursor.fetchall()
        assert res == [[0, 0], [1, 2], [2, 4]], res

        cursor.execute("TRUNCATE test")

        cursor.execute("SELECT v1, v2 FROM test WHERE k IN (0, 1, 2)")
        res = cursor.fetchall()
        assert res == [], res

    def allow_filtering_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                c int,
                v int,
                PRIMARY KEY (k, c)
            )
        """)

        for i in range(0, 3):
            for j in range(0, 3):
                cursor.execute("INSERT INTO test(k, c, v) VALUES(%d, %d, %d)" % (i, j, j))

        # Don't require filtering, always allowed
        queries = [ "SELECT * FROM test WHERE k = 1",
                    "SELECT * FROM test WHERE k = 1 AND c > 2",
                    "SELECT * FROM test WHERE k = 1 AND c = 2" ]
        for q in queries:
            cursor.execute(q)
            cursor.execute(q + " ALLOW FILTERING")

        # Require filtering, allowed only with ALLOW FILTERING
        queries = [ "SELECT * FROM test WHERE c = 2",
                    "SELECT * FROM test WHERE c > 2 AND c <= 4" ]
        for q in queries:
            assert_invalid(cursor, q)
            cursor.execute(q + " ALLOW FILTERING")

        cursor.execute("""
            CREATE TABLE indexed (
                k int PRIMARY KEY,
                a int,
                b int,
            )
        """)

        cursor.execute("CREATE INDEX ON indexed(a)")

        for i in range(0, 5):
            cursor.execute("INSERT INTO indexed(k, a, b) VALUES(%d, %d, %d)" % (i, i * 10, i * 100))

        # Don't require filtering, always allowed
        queries = [ "SELECT * FROM indexed WHERE k = 1",
                    "SELECT * FROM indexed WHERE a = 20" ]
        for q in queries:
            cursor.execute(q)
            cursor.execute(q + " ALLOW FILTERING")

        # Require filtering, allowed only with ALLOW FILTERING
        queries = [ "SELECT * FROM indexed WHERE a = 20 AND b = 200" ]
        for q in queries:
            assert_invalid(cursor, q)
            cursor.execute(q + " ALLOW FILTERING")

    def range_with_deletes_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v int,
            )
        """)

        nb_keys = 30
        nb_deletes = 5

        for i in range(0, nb_keys):
            cursor.execute("INSERT INTO test(k, v) VALUES (%d, %d)" % (i, i))

        for i in random.sample(xrange(nb_keys), nb_deletes):
            cursor.execute("DELETE FROM test WHERE k = %d" % i)

        cursor.execute("SELECT * FROM test LIMIT %d" % (nb_keys/2))
        res = cursor.fetchall()
        assert len(res) == nb_keys/2, "Expected %d but got %d" % (nb_keys/2, len(res))

    def alter_with_collections_test(self):
        """ Test you can add columns in a table with collections (#4982 bug) """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE collections (key int PRIMARY KEY, aset set<text>)")
        cursor.execute("ALTER TABLE collections ADD c text")
        cursor.execute("ALTER TABLE collections ADD alist list<text>")

    def collection_compact_test(self):
        cursor = self.prepare()

        assert_invalid(cursor, """
            CREATE TABLE test (
                user ascii PRIMARY KEY,
                mails list<text>
            ) WITH COMPACT STORAGE;
        """)

    def collection_function_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                l set<int>
            )
        """)

        assert_invalid(cursor, "SELECT ttl(l) FROM test WHERE k = 0")
        assert_invalid(cursor, "SELECT writetime(l) FROM test WHERE k = 0")

    def collection_counter_test(self):
        cursor = self.prepare()

        assert_invalid(cursor, """
            CREATE TABLE test (
                k int PRIMARY KEY,
                l list<counter>
            )
        """)

        assert_invalid(cursor, """
            CREATE TABLE test (
                k int PRIMARY KEY,
                s set<counter>
            )
        """)

        assert_invalid(cursor, """
            CREATE TABLE test (
                k int PRIMARY KEY,
                m map<text, counter>
            )
        """)

    def composite_partition_key_validation_test(self):
        """ Test for bug from #5122 """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE foo (a int, b text, c uuid, PRIMARY KEY ((a, b)));")

        cursor.execute("INSERT INTO foo (a, b , c ) VALUES (  1 , 'aze', 4d481800-4c5f-11e1-82e0-3f484de45426)")
        cursor.execute("INSERT INTO foo (a, b , c ) VALUES (  1 , 'ert', 693f5800-8acb-11e3-82e0-3f484de45426)")
        cursor.execute("INSERT INTO foo (a, b , c ) VALUES (  1 , 'opl', d4815800-2d8d-11e0-82e0-3f484de45426)")

        cursor.execute("SELECT * FROM foo")
        res = cursor.fetchall()
        assert len(res) == 3, res

        assert_invalid(cursor, "SELECT * FROM foo WHERE a=1")

    @require('https://issues.apache.org/jira/browse/CASSANDRA-4762')
    def multi_in_test(self):
        self.__multi_in(False)

    @require('https://issues.apache.org/jira/browse/CASSANDRA-4762')
    def multi_in_compact_test(self):
        self.__multi_in(True)

    def __multi_in(self, compact):
        cursor = self.prepare()

        data = [
            ( 'test', '06029', 'CT',  9, 'Ellington'     ),
            ( 'test', '06031', 'CT',  9, 'Falls Village' ),
            ( 'test', '06902', 'CT',  9, 'Stamford'      ),
            ( 'test', '06927', 'CT',  9, 'Stamford'      ),
            ( 'test', '10015', 'NY', 36, 'New York'      ),
            ( 'test', '07182', 'NJ', 34, 'Newark'        ),
            ( 'test', '73301', 'TX', 48, 'Austin'        ),
            ( 'test', '94102', 'CA', 06, 'San Francisco' ),

            ( 'test2', '06029', 'CT',  9, 'Ellington'     ),
            ( 'test2', '06031', 'CT',  9, 'Falls Village' ),
            ( 'test2', '06902', 'CT',  9, 'Stamford'      ),
            ( 'test2', '06927', 'CT',  9, 'Stamford'      ),
            ( 'test2', '10015', 'NY', 36, 'New York'      ),
            ( 'test2', '07182', 'NJ', 34, 'Newark'        ),
            ( 'test2', '73301', 'TX', 48, 'Austin'        ),
            ( 'test2', '94102', 'CA', 06, 'San Francisco' ),
        ]

        create = """
            CREATE TABLE zipcodes (
                group text,
                zipcode text,
                state text,
                fips_regions int,
                city text,
                PRIMARY KEY(group,zipcode,state,fips_regions)
            )"""

        if compact:
            create = create + " WITH COMPACT STORAGE"

        cursor.execute(create)

        for d in data:
            cursor.execute("INSERT INTO zipcodes (group, zipcode, state, fips_regions, city) VALUES ('%s', '%s', '%s', %i, '%s')" % d)

        cursor.execute("select zipcode from zipcodes")
        res = cursor.fetchall()
        assert len(res) == 16, res

        cursor.execute("select zipcode from zipcodes where group='test'")
        res = cursor.fetchall()
        assert len(res) == 8, res

        assert_invalid(cursor, "select zipcode from zipcodes where zipcode='06902'")

        cursor.execute("select zipcode from zipcodes where zipcode='06902' ALLOW FILTERING")
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("select zipcode from zipcodes where group='test' and zipcode='06902'")
        res = cursor.fetchall()
        assert len(res) == 1, res

        cursor.execute("select zipcode from zipcodes where group='test' and zipcode IN ('06902','73301','94102')")
        res = cursor.fetchall()
        assert len(res) == 3, res

        cursor.execute("select zipcode from zipcodes where group='test' AND zipcode IN ('06902','73301','94102') and state IN ('CT','CA')")
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("select zipcode from zipcodes where group='test' AND zipcode IN ('06902','73301','94102') and state IN ('CT','CA') and fips_regions = 9")
        res = cursor.fetchall()
        assert len(res) == 1, res

        cursor.execute("select zipcode from zipcodes where group='test' AND zipcode IN ('06902','73301','94102') and state IN ('CT','CA') ORDER BY zipcode DESC")
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("select zipcode from zipcodes where group='test' AND zipcode IN ('06902','73301','94102') and state IN ('CT','CA') and fips_regions > 0")
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("select zipcode from zipcodes where group='test' AND zipcode IN ('06902','73301','94102') and state IN ('CT','CA') and fips_regions < 0")
        res = cursor.fetchall()
        assert len(res) == 0, res

    @require('https://issues.apache.org/jira/browse/CASSANDRA-4762')
    def multi_in_compact_non_composite_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                key int,
                c int,
                v int,
                PRIMARY KEY (key, c)
            ) WITH COMPACT STORAGE
        """)

        cursor.execute("INSERT INTO test (key, c, v) VALUES (0, 0, 0)")
        cursor.execute("INSERT INTO test (key, c, v) VALUES (0, 1, 1)")
        cursor.execute("INSERT INTO test (key, c, v) VALUES (0, 2, 2)")

        cursor.execute("SELECT * FROM test WHERE key=0 AND c IN (0, 2)")
        res = cursor.fetchall()
        assert res == [[0, 0, 0], [0, 2, 2]], res

    @since('1.2.1')
    def timeuuid_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                t timeuuid,
                PRIMARY KEY (k, t)
            )
        """)

        assert_invalid(cursor, "INSERT INTO test (k, t) VALUES (0, 2012-11-07 18:18:22-0800)")

        for i in range(4):
            cursor.execute("INSERT INTO test (k, t) VALUES (0, now())")
            time.sleep(1)

        cursor.execute("SELECT * FROM test")
        res = cursor.fetchall()
        assert len(res) == 4, res
        dates = [ d[1] for d in res ]

        cursor.execute("SELECT * FROM test WHERE k = 0 AND t >= %s" % dates[0])
        res = cursor.fetchall()
        assert len(res) == 4, res

        cursor.execute("SELECT * FROM test WHERE k = 0 AND t < %s" % dates[0])
        res = cursor.fetchall()
        assert len(res) == 0, res

        cursor.execute("SELECT * FROM test WHERE k = 0 AND t > %s AND t <= %s" % (dates[0], dates[2]))
        res = cursor.fetchall()
        assert len(res) == 2, res

        cursor.execute("SELECT * FROM test WHERE k = 0 AND t = %s" % dates[0])
        res = cursor.fetchall()
        assert len(res) == 1, res

        assert_invalid(cursor, "SELECT dateOf(k) FROM test WHERE k = 0 AND t = %s" % dates[0])

        cursor.execute("SELECT dateOf(t), unixTimestampOf(t) FROM test WHERE k = 0 AND t = %s" % dates[0])
        cursor.execute("SELECT t FROM test WHERE k = 0 AND t > maxTimeuuid(1234567) AND t < minTimeuuid('2012-11-07 18:18:22-0800')")
        # not sure what to check exactly so just checking the query returns

    def float_with_exponent_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                d double,
                f float
            )
        """)

        cursor.execute("INSERT INTO test(k, d, f) VALUES (0, 3E+10, 3.4E3)")
        cursor.execute("INSERT INTO test(k, d, f) VALUES (1, 3.E10, -23.44E-3)")
        cursor.execute("INSERT INTO test(k, d, f) VALUES (2, 3, -2)")

    def compact_metadata_test(self):
        """ Test regression from #5189 """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE bar (
                id int primary key,
                i int
            ) WITH COMPACT STORAGE;
        """)

        cursor.execute("INSERT INTO bar (id, i) VALUES (1, 2);")
        cursor.execute("SELECT * FROM bar")
        res = cursor.fetchall()
        assert res == [[1, 2]], res

    @since('2.0')
    def clustering_indexing_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE posts (
                id1 int,
                id2 int,
                author text,
                time bigint,
                content text,
                PRIMARY KEY ((id1, id2), author, time)
            )
        """)

        cursor.execute("CREATE INDEX ON posts(time)")
        cursor.execute("CREATE INDEX ON posts(id2)")

        cursor.execute("INSERT INTO posts(id1, id2, author, time, content) VALUES(0, 0, 'bob', 0, 'A')")
        cursor.execute("INSERT INTO posts(id1, id2, author, time, content) VALUES(0, 0, 'bob', 1, 'B')")
        cursor.execute("INSERT INTO posts(id1, id2, author, time, content) VALUES(0, 1, 'bob', 2, 'C')")
        cursor.execute("INSERT INTO posts(id1, id2, author, time, content) VALUES(0, 0, 'tom', 0, 'D')")
        cursor.execute("INSERT INTO posts(id1, id2, author, time, content) VALUES(0, 1, 'tom', 1, 'E')")

        cursor.execute("SELECT content FROM posts WHERE time = 1")
        res = cursor.fetchall()
        assert res == [ ['B'], ['E'] ], res

        cursor.execute("SELECT content FROM posts WHERE id2 = 1")
        res = cursor.fetchall()
        assert res == [ ['C'], ['E'] ], res

        cursor.execute("SELECT content FROM posts WHERE id1 = 0 AND id2 = 0 AND author = 'bob' AND time = 0")
        res = cursor.fetchall()
        assert res == [ ['A'] ], res

    @since('2.0')
    def edge_2i_on_complex_pk_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE indexed (
                pk0 int,
                pk1 int,
                ck0 int,
                ck1 int,
                ck2 int,
                value int,
                PRIMARY KEY ((pk0, pk1), ck0, ck1, ck2)
            )
        """)

        cursor.execute("CREATE INDEX ON indexed(pk0)")
        cursor.execute("CREATE INDEX ON indexed(ck0)")
        cursor.execute("CREATE INDEX ON indexed(ck1)")
        cursor.execute("CREATE INDEX ON indexed(ck2)")

        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (0, 1, 2, 3, 4, 5)")
        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (1, 2, 3, 4, 5, 0)")
        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (2, 3, 4, 5, 0, 1)")
        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (3, 4, 5, 0, 1, 2)")
        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (4, 5, 0, 1, 2, 3)")
        cursor.execute("INSERT INTO indexed (pk0, pk1, ck0, ck1, ck2, value) VALUES (5, 0, 1, 2, 3, 4)")

        cursor.execute("SELECT value FROM indexed WHERE pk0 = 2")
        self.assertEqual([[1]], cursor.fetchall())

        cursor.execute("SELECT value FROM indexed WHERE ck0 = 0")
        self.assertEqual([[3]], cursor.fetchall())

        cursor.execute("SELECT value FROM indexed WHERE pk0 = 3 AND pk1 = 4 AND ck1 = 0")
        self.assertEqual([[2]], cursor.fetchall())

        cursor.execute("SELECT value FROM indexed WHERE pk0 = 5 AND pk1 = 0 AND ck0 = 1 AND ck2 = 3 ALLOW FILTERING")
        self.assertEqual([[4]], cursor.fetchall())

    def bug_5240_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test(
                interval text,
                seq int,
                id int,
                severity int,
                PRIMARY KEY ((interval, seq), id)
            ) WITH CLUSTERING ORDER BY (id DESC);
        """)

        cursor.execute("CREATE INDEX ON test(severity);")

        cursor.execute("insert into test(interval, seq, id , severity) values('t',1, 1, 1);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',1, 2, 1);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',1, 3, 2);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',1, 4, 3);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',2, 1, 3);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',2, 2, 3);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',2, 3, 1);")
        cursor.execute("insert into test(interval, seq, id , severity) values('t',2, 4, 2);")

        cursor.execute("select * from test where severity = 3 and interval = 't' and seq =1;")
        res = cursor.fetchall()
        assert res == [['t', 1, 4, 3]], res

    def ticket_5230_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE foo (
                key text,
                c text,
                v text,
                PRIMARY KEY (key, c)
            )
        """)

        cursor.execute("INSERT INTO foo(key, c, v) VALUES ('foo', '1', '1')")
        cursor.execute("INSERT INTO foo(key, c, v) VALUES ('foo', '2', '2')")
        cursor.execute("INSERT INTO foo(key, c, v) VALUES ('foo', '3', '3')")

        cursor.execute("SELECT c FROM foo WHERE key = 'foo' AND c IN ('1', '2');")
        res = cursor.fetchall()
        assert res == [['1'], ['2']], res

    def conversion_functions_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                i varint,
                b blob
            )
        """)

        cursor.execute("INSERT INTO test(k, i, b) VALUES (0, blobAsVarint(bigintAsBlob(3)), textAsBlob('foobar'))")
        cursor.execute("SELECT i, blobAsText(b) FROM test WHERE k = 0")
        res = cursor.fetchall()
        assert res == [[3, 'foobar']], res

    def alter_bug_test(self):
        """ Test for bug of 5232 """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE t1 (id int PRIMARY KEY, t text);")

        cursor.execute("UPDATE t1 SET t = '111' WHERE id = 1;")
        cursor.execute("ALTER TABLE t1 ADD l list<text>;")

        time.sleep(.5)

        cursor.execute("SELECT * FROM t1;")
        res = cursor.fetchall()
        assert res == [[1, None, '111']], res

        cursor.execute("ALTER TABLE t1 ADD m map<int, text>;")
        time.sleep(.5)
        cursor.execute("SELECT * FROM t1;")
        res = cursor.fetchall()
        assert res == [[1, None, None, '111']], res

    def bug_5376(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                key text,
                c bigint,
                v text,
                x set<text>,
                PRIMARY KEY (key, c)
            );
        """)

        assert_invalid(cursor, "select * from test where key = 'foo' and c in (1,3,4);")

    def function_and_reverse_type_test(self):
        """ Test for #5386 """

        cursor = self.prepare()
        cursor.execute("""
            CREATE TABLE test (
                k int,
                c timeuuid,
                v int,
                PRIMARY KEY (k, c)
            ) WITH CLUSTERING ORDER BY (c DESC)
        """)

        cursor.execute("INSERT INTO test (k, c, v) VALUES (0, now(), 0);")

    def bug_5404(self):
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (key text PRIMARY KEY)")
        # We just want to make sure this doesn't NPE server side
        assert_invalid(cursor, "select * from test where token(key) > token(int(3030343330393233)) limit 1;")

    def empty_blob_test(self):
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (k int PRIMARY KEY, b blob)")
        cursor.execute("INSERT INTO test (k, b) VALUES (0, 0x)");
        cursor.execute("SELECT * FROM test");
        res = cursor.fetchall()
        assert res == [[ 0, '' ]], res

    @since('2.0')
    def rename_test(self):
        cursor = self.prepare()

        # The goal is to test renaming from an old cli value
        cli = self.cluster.nodelist()[0].cli()
        cli.do("use ks")
        cli.do("create column family test with comparator='CompositeType(Int32Type, Int32Type, Int32Type)' "
                + "and key_validation_class=UTF8Type and default_validation_class=UTF8Type");
        cli.do("set test['foo']['4:3:2'] = 'bar'")
        assert not cli.has_errors(), cli.errors()

        time.sleep(1)

        cursor.execute("ALTER TABLE test RENAME column1 TO foo1 AND column2 TO foo2 AND column3 TO foo3")
        assert_one(cursor, "SELECT foo1, foo2, foo3 FROM test", [4, 3, 2])

    def clustering_order_and_functions_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                t timeuuid,
                PRIMARY KEY (k, t)
            ) WITH CLUSTERING ORDER BY (t DESC)
        """)

        for i in range(0, 5):
            cursor.execute("INSERT INTO test (k, t) VALUES (%d, now())" % i)

        cursor.execute("SELECT dateOf(t) FROM test");

    @since('2.0')
    def conditional_update_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v1 int,
                v2 text,
                v3 int
            )
        """)

        # Shouldn't apply
        assert_one(cursor, "UPDATE test SET v1 = 3, v2 = 'bar' WHERE k = 0 IF v1 = 4", [ False ])

        # Should apply
        assert_one(cursor, "INSERT INTO test (k, v1, v2) VALUES (0, 2, 'foo') IF NOT EXISTS", [ True ])

        # Shouldn't apply
        assert_one(cursor, "INSERT INTO test (k, v1, v2) VALUES (0, 5, 'bar') IF NOT EXISTS", [ False, 0, 2, 'foo', None ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 2, 'foo', None ])

        # Should not apply
        assert_one(cursor, "UPDATE test SET v1 = 3, v2 = 'bar' WHERE k = 0 IF v1 = 4", [ False, 2 ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 2, 'foo', None ])

        # Should apply (note: we want v2 before v1 in the statement order to exercise #5786)
        assert_one(cursor, "UPDATE test SET v2 = 'bar', v1 = 3 WHERE k = 0 IF v1 = 2", [ True ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 3, 'bar', None ])

        # Shouldn't apply, only one condition is ok
        assert_one(cursor, "UPDATE test SET v1 = 5, v2 = 'foobar' WHERE k = 0 IF v1 = 3 AND v2 = 'foo'", [ False, 3, 'bar' ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 3, 'bar', None ])

        # Should apply
        assert_one(cursor, "UPDATE test SET v1 = 5, v2 = 'foobar' WHERE k = 0 IF v1 = 3 AND v2 = 'bar'", [ True ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 5, 'foobar', None ])

        # Shouldn't apply
        assert_one(cursor, "DELETE v2 FROM test WHERE k = 0 IF v1 = 3", [ False, 5 ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 5, 'foobar', None ])

        # Shouldn't apply
        assert_one(cursor, "DELETE v2 FROM test WHERE k = 0 IF v1 = null", [ False, 5 ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 5, 'foobar', None ])

        # Should apply
        assert_one(cursor, "DELETE v2 FROM test WHERE k = 0 IF v1 = 5", [ True ])
        assert_one(cursor, "SELECT * FROM test", [ 0, 5, None, None ])

        # Shouln't apply
        assert_one(cursor, "DELETE v1 FROM test WHERE k = 0 IF v3 = 4", [ False, None ])

        # Should apply
        assert_one(cursor, "DELETE v1 FROM test WHERE k = 0 IF v3 = null", [ True ])
        assert_one(cursor, "SELECT * FROM test", [ 0, None, None, None ])

        # Should apply
        assert_one(cursor, "DELETE FROM test WHERE k = 0 IF v1 = null", [ True ])
        assert_none(cursor, "SELECT * FROM test")

    @since('2.0.7')
    def conditional_delete_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                v1 int,
            )
        """)

        assert_one(cursor, "DELETE FROM test WHERE k=1 IF EXISTS", [False])

        cursor.execute("INSERT INTO test (k, v1) VALUES (1, 2)")
        assert_one(cursor, "DELETE FROM test WHERE k=1 IF EXISTS", [True])
        assert_none(cursor, "SELECT * FROM test WHERE k=1")
        assert_one(cursor, "DELETE FROM test WHERE k=1 IF EXISTS", [False])

        cursor.execute("UPDATE test USING TTL 1 SET v1=2 WHERE k=1")
        time.sleep(1.5)
        assert_one(cursor, "DELETE FROM test WHERE k=1 IF EXISTS", [False])
        assert_none(cursor, "SELECT * FROM test WHERE k=1")

        cursor.execute("INSERT INTO test (k, v1) VALUES (2, 2) USING TTL 1")
        time.sleep(1.5)
        assert_one(cursor, "DELETE FROM test WHERE k=2 IF EXISTS", [False])
        assert_none(cursor, "SELECT * FROM test WHERE k=2")

        cursor.execute("INSERT INTO test (k, v1) VALUES (3, 2)")
        assert_one(cursor, "DELETE v1 FROM test WHERE k=3 IF EXISTS", [True])
        assert_one(cursor, "SELECT * FROM test WHERE k=3", [3, None])
        assert_one(cursor, "DELETE v1 FROM test WHERE k=3 IF EXISTS", [True])
        assert_one(cursor, "DELETE FROM test WHERE k=3 IF EXISTS", [True])

        # static columns
        cursor.execute("""
            CREATE TABLE test2 (
                k text,
                s text static,
                i int,
                v text,
                PRIMARY KEY (k, i)
            )""")

        cursor.execute("INSERT INTO test2 (k, s, i, v) VALUES ('k', 's', 0, 'v')")
        assert_one(cursor, "DELETE v FROM test2 WHERE k='k' AND i=0 IF EXISTS", [True])
        assert_one(cursor, "DELETE FROM test2 WHERE k='k' AND i=0 IF EXISTS", [True])
        assert_one(cursor, "DELETE v FROM test2 WHERE k='k' AND i=0 IF EXISTS", [False])
        assert_one(cursor, "DELETE FROM test2 WHERE k='k' AND i=0 IF EXISTS", [False])

    def range_key_ordered_test(self):
        cursor = self.prepare(ordered=True)

        cursor.execute("CREATE TABLE test ( k int PRIMARY KEY)")

        cursor.execute("INSERT INTO test(k) VALUES (-1)")
        cursor.execute("INSERT INTO test(k) VALUES ( 0)")
        cursor.execute("INSERT INTO test(k) VALUES ( 1)")

        assert_all(cursor, "SELECT * FROM test", [[0], [1], [-1]])
        assert_invalid(cursor, "SELECT * FROM test WHERE k >= -1 AND k < 1;")

    @since('2.0')
    def select_with_alias_test(self):
        cursor = self.prepare()
        cursor.execute('CREATE TABLE users (id int PRIMARY KEY, name text)')

        for id in range(0, 5):
            cursor.execute("INSERT INTO users (id, name) VALUES (%d, 'name%d') USING TTL 10 AND TIMESTAMP 0" % (id, id))

        # test aliasing count(*)
        cursor.execute('SELECT count(*) AS user_count FROM users')
        self.assertEqual('user_count', cursor.name_info[0][0])
        self.assertEqual([5], cursor.fetchone())

        # test aliasing regular value
        cursor.execute('SELECT name AS user_name FROM users WHERE id = 0')
        self.assertEqual('user_name', cursor.name_info[0][0])
        self.assertEqual(['name0'], cursor.fetchone())

        # test aliasing writetime
        cursor.execute('SELECT writeTime(name) AS name_writetime FROM users WHERE id = 0')
        self.assertEqual('name_writetime', cursor.name_info[0][0])
        self.assertEqual([0], cursor.fetchone())

        # test aliasing ttl
        cursor.execute('SELECT ttl(name) AS name_ttl FROM users WHERE id = 0')
        self.assertEqual('name_ttl', cursor.name_info[0][0])
        assert cursor.fetchone()[0] in (9, 10)

        # test aliasing a regular function
        cursor.execute('SELECT intAsBlob(id) AS id_blob FROM users WHERE id = 0')
        self.assertEqual('id_blob', cursor.name_info[0][0])
        self.assertEqual(['\x00\x00\x00\x00'], cursor.fetchone())

        # test that select throws a meaningful exception for aliases in where clause
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute('SELECT id AS user_id, name AS user_name FROM users WHERE user_id = 0')
            message = cm.exception.message
            self.assertTrue((
                    message == "Bad Request: Aliases aren't allowed in where clause ('user_id EQ 0')" or
                    message == "Bad Request: Aliases aren't allowed in where clause ('user_id = 0')"))

        # test that select throws a meaningful exception for aliases in order by clause
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute('SELECT id AS user_id, name AS user_name FROM users WHERE id IN (0) ORDER BY user_name')
        self.assertEqual("Bad Request: Aliases are not allowed in order by clause ('user_name')",
                         cm.exception.message)

    def nonpure_function_collection_test(self):
        """ Test for bug #5795 """

        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k int PRIMARY KEY, v list<timeuuid>)")

        # we just want to make sure this doesn't throw
        cursor.execute("INSERT INTO test(k, v) VALUES (0, [now()])")

    def empty_in_test(self):
        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k1 int, k2 int, v int, PRIMARY KEY (k1, k2))")

        def fill(table):
            for i in range(0, 2):
                for j in range(0, 2):
                    cursor.execute("INSERT INTO %s (k1, k2, v) VALUES (%d, %d, %d)" % (table, i, j, i+j))

        def assert_nothing_changed(table):
            cursor.execute("SELECT * FROM %s" % table) # make sure nothing got removed
            self.assertEqual([[0,0,0], [0,1,1], [1,0,1], [1,1,2]], sorted(cursor.fetchall()))

        # Inserts a few rows to make sure we don't actually query something
        fill("test")

        # Test empty IN () in SELECT
        assert_none(cursor, "SELECT v FROM test WHERE k1 IN ()")
        assert_none(cursor, "SELECT v FROM test WHERE k1 = 0 AND k2 IN ()")

        # Test empty IN () in DELETE
        cursor.execute("DELETE FROM test WHERE k1 IN ()");
        assert_nothing_changed("test")

        # Test empty IN () in UPDATE
        cursor.execute("UPDATE test SET v = 3 WHERE k1 IN () AND k2 = 2")
        assert_nothing_changed("test")

        # Same test, but for compact
        cursor.execute("CREATE TABLE test_compact (k1 int, k2 int, v int, PRIMARY KEY (k1, k2)) WITH COMPACT STORAGE")

        fill("test_compact")

        assert_none(cursor, "SELECT v FROM test_compact WHERE k1 IN ()")
        assert_none(cursor, "SELECT v FROM test_compact WHERE k1 = 0 AND k2 IN ()")

        # Test empty IN () in DELETE
        cursor.execute("DELETE FROM test_compact WHERE k1 IN ()");
        assert_nothing_changed("test_compact")

        # Test empty IN () in UPDATE
        cursor.execute("UPDATE test_compact SET v = 3 WHERE k1 IN () AND k2 = 2")
        assert_nothing_changed("test_compact")


    def collection_flush_test(self):
        """ Test for 5805 bug """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (k int PRIMARY KEY, s set<int>)")

        cursor.execute("INSERT INTO test(k, s) VALUES (1, {1})");
        self.cluster.flush()
        cursor.execute("INSERT INTO test(k, s) VALUES (1, {2})");
        self.cluster.flush()

        assert_one(cursor, "SELECT * FROM test", [ 1, set([2]) ])

    @since('2.0.1')
    def select_distinct_test(self):
        cursor = self.prepare()

        # Test a regular (CQL3) table.
        cursor.execute('CREATE TABLE regular (pk0 int, pk1 int, ck0 int, val int, PRIMARY KEY((pk0, pk1), ck0))')

        for i in xrange(0, 3):
            cursor.execute('INSERT INTO regular (pk0, pk1, ck0, val) VALUES (%d, %d, 0, 0)' % (i, i))
            cursor.execute('INSERT INTO regular (pk0, pk1, ck0, val) VALUES (%d, %d, 1, 1)' % (i, i))

        cursor.execute('SELECT DISTINCT pk0, pk1 FROM regular LIMIT 1')
        self.assertEqual([[0, 0]], cursor.fetchall())

        cursor.execute('SELECT DISTINCT pk0, pk1 FROM regular LIMIT 3')
        self.assertEqual([[0, 0], [1, 1], [2, 2]], sorted(cursor.fetchall()))

        # Test a 'compact storage' table.
        cursor.execute('CREATE TABLE compact (pk0 int, pk1 int, val int, PRIMARY KEY((pk0, pk1))) WITH COMPACT STORAGE')

        for i in xrange(0, 3):
            cursor.execute('INSERT INTO compact (pk0, pk1, val) VALUES (%d, %d, %d)' % (i, i, i))

        cursor.execute('SELECT DISTINCT pk0, pk1 FROM compact LIMIT 1')
        self.assertEqual([[0, 0]], cursor.fetchall())

        cursor.execute('SELECT DISTINCT pk0, pk1 FROM compact LIMIT 3')
        self.assertEqual([[0, 0], [1, 1], [2, 2]], sorted(cursor.fetchall()))

        # Test a 'wide row' thrift table.
        cursor.execute('CREATE TABLE wide (pk int, name text, val int, PRIMARY KEY(pk, name)) WITH COMPACT STORAGE')

        for i in xrange(0, 3):
            cursor.execute("INSERT INTO wide (pk, name, val) VALUES (%d, 'name0', 0)" % i)
            cursor.execute("INSERT INTO wide (pk, name, val) VALUES (%d, 'name1', 1)" % i)

        cursor.execute('SELECT DISTINCT pk FROM wide LIMIT 1')
        self.assertEqual([[1]], cursor.fetchall())

        cursor.execute('SELECT DISTINCT pk FROM wide LIMIT 3')
        self.assertEqual([[0], [1], [2]], sorted(cursor.fetchall()))

        # Test selection validation.
        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute('SELECT DISTINCT pk0 FROM regular')
        self.assertEqual('Bad Request: SELECT DISTINCT queries must request all the partition key columns (missing pk1)',
                         cm.exception.message)

        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute('SELECT DISTINCT pk0, pk1, ck0 FROM regular')
        self.assertEqual('Bad Request: SELECT DISTINCT queries must only request partition key columns (not ck0)',
                         cm.exception.message)

    def function_with_null_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                t timeuuid,
            )
        """)

        cursor.execute("INSERT INTO test(k) VALUES (0)")
        assert_one(cursor, "SELECT dateOf(t) FROM test WHERE k=0", [ None ])

    @since('2.0')
    def cas_simple_test(self):
        cursor = self.prepare(nodes=3, rf=3)

        cursor.execute("CREATE TABLE tkns (tkn int, consumed boolean, PRIMARY KEY (tkn));")

        for i in range(1, 10):
            cursor.execute("INSERT INTO tkns (tkn, consumed) VALUES (%i,FALSE);" % i, consistency_level='QUORUM')
            assert_one(cursor, "UPDATE tkns SET consumed = TRUE WHERE tkn = %i IF consumed = FALSE;" % i, [True], cl='QUORUM')
            assert_one(cursor, "UPDATE tkns SET consumed = TRUE WHERE tkn = %i IF consumed = FALSE;" % i, [False, True], cl='QUORUM')


    def bug_6050_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                a int,
                b int
            )
        """)

        cursor.execute("CREATE INDEX ON test(a)")
        assert_invalid(cursor, "SELECT * FROM test WHERE a = 3 AND b IN (1, 3)")

    @since('2.0')
    def bug_6069_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int PRIMARY KEY,
                s set<int>
            )
        """)

        assert_one(cursor, "INSERT INTO test(k, s) VALUES (0, {1, 2, 3}) IF NOT EXISTS", [True])
        assert_one(cursor, "SELECT * FROM test", [0, {1, 2, 3}])

    def bug_6115_test(self):
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (k int, v int, PRIMARY KEY (k, v))")

        cursor.execute("INSERT INTO test (k, v) VALUES (0, 1)")
        cursor.execute("BEGIN BATCH DELETE FROM test WHERE k=0 AND v=1; INSERT INTO test (k, v) VALUES (0, 2); APPLY BATCH")

        assert_one(cursor, "SELECT * FROM test", [0, 2])

    def secondary_index_counters(self):
        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k int PRIMARY KEY, c counter)")
        assert_invalid(cursor, "CREATE INDEX ON test(c)");

    def column_name_validation_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k text,
                c int,
                v timeuuid,
                PRIMARY KEY (k, c)
            )
        """)

        assert_invalid(cursor, "INSERT INTO test(k, c) VALUES ('', 0)")

        # Insert a value that don't fit 'int'
        assert_invalid(cursor, "INSERT INTO test(k, c) VALUES (0, 10000000000)")

        # Insert a non-version 1 uuid
        assert_invalid(cursor, "INSERT INTO test(k, c, v) VALUES (0, 0, 550e8400-e29b-41d4-a716-446655440000)")

    @since('2.1')
    def user_types_test(self):
        cursor = self.prepare()

        userID_1 = uuid4()
        stmt = """
              CREATE TYPE address (
              street text,
              city text,
              zip_code int,
              phones set<text>
              )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TYPE fullname (
               firstname text,
               lastname text
              )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TABLE users (
               id uuid PRIMARY KEY,
               name fullname,
               addresses map<text, address>
              )
           """
        cursor.execute(stmt)

        stmt = """
              INSERT INTO users (id, name)
              VALUES ({id}, {{ firstname: 'Paul', lastname: 'smith'}});
           """.format(id=userID_1)
        cursor.execute(stmt)

        stmt = """
              SELECT name.firstname FROM users WHERE id = {id}
        """.format(id=userID_1)
        cursor.execute(stmt)
        self.assertEqual(['Paul'], cursor.fetchone())

        stmt = """
              UPDATE users
              SET addresses = addresses + {{ 'home': {{ street: '...', city: 'SF', zip_code: 94102, phones: {{}} }} }}
              WHERE id={id};
           """.format(id=userID_1)
        cursor.execute(stmt)

        stmt = """
              SELECT addresses FROM users WHERE id = {id}
        """.format(id=userID_1)
        cursor.execute(stmt)
        res = cursor.fetchone()
        ## TODO: deserialize the value here and check it's right.

    @since('2.1')
    def more_user_types_test(self):
        """ user type test that does a little more nesting"""

        cursor = self.prepare()

        cursor.execute("""
            CREATE TYPE type1 (
                s set<text>,
                m map<text, text>,
                l list<text>
            )
        """)

        cursor.execute("""
            CREATE TYPE type2 (
                s set<type1>,
            )
        """)

        cursor.execute("""
            CREATE TABLE test (id int PRIMARY KEY, val type2)
        """)


        cursor.execute("INSERT INTO test(id, val) VALUES (0, { s : {{ s : {'foo', 'bar'}, m : { 'foo' : 'bar' }, l : ['foo', 'bar']} }})")

        # TODO: check result once we have an easy way to do it. For now we just check it doesn't crash
        cursor.execute("SELECT * FROM test")


    @since('1.2')
    def bug_6327_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                v int,
                PRIMARY KEY (k, v)
            )
        """)

        cursor.execute("INSERT INTO test (k, v) VALUES (0, 0)")
        self.cluster.flush()
        assert_one(cursor, "SELECT v FROM test WHERE k=0 AND v IN (1, 0)", [0])

    @since('1.2')
    def large_count_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                v int,
                PRIMARY KEY (k)
            )
        """)

        # We know we page at 10K, so test counting just before, at 10K, just after and
        # a bit after that.
        for k in range(1, 10000):
            cursor.execute("INSERT INTO test(k) VALUES (%d)" % k)

        assert_one(cursor, "SELECT COUNT(*) FROM test", [9999])

        cursor.execute("INSERT INTO test(k) VALUES (%d)" % 10000)

        assert_one(cursor, "SELECT COUNT(*) FROM test", [10000])

        cursor.execute("INSERT INTO test(k) VALUES (%d)" % 10001)

        assert_one(cursor, "SELECT COUNT(*) FROM test", [10001])

        for k in range(10002, 15001):
            cursor.execute("INSERT INTO test(k) VALUES (%d)" % k)

        assert_one(cursor, "SELECT COUNT(*) FROM test", [15000])

    @since('2.1')
    def collection_indexing_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                v int,
                l list<int>,
                s set<text>,
                m map<text, int>,
                PRIMARY KEY (k, v)
            )
        """)

        cursor.execute("CREATE INDEX ON test(l)")
        cursor.execute("CREATE INDEX ON test(s)")
        cursor.execute("CREATE INDEX ON test(m)")

        cursor.execute("INSERT INTO test (k, v, l, s, m) VALUES (0, 0, [1, 2],    {'a'},      {'a' : 1})")
        cursor.execute("INSERT INTO test (k, v, l, s, m) VALUES (0, 1, [3, 4],    {'b', 'c'}, {'a' : 1, 'b' : 2})")
        cursor.execute("INSERT INTO test (k, v, l, s, m) VALUES (0, 2, [1],       {'a', 'c'}, {'c' : 3})")
        cursor.execute("INSERT INTO test (k, v, l, s, m) VALUES (1, 0, [1, 2, 4], {},         {'b' : 1})")
        cursor.execute("INSERT INTO test (k, v, l, s, m) VALUES (1, 1, [4, 5],    {'d'},      {'a' : 1, 'b' : 3})")

        # lists
        assert_all(cursor, "SELECT k, v FROM test WHERE l CONTAINS 1", [[1, 0], [0, 0], [0, 2]])
        assert_all(cursor, "SELECT k, v FROM test WHERE l CONTAINS 2", [[1, 0], [0, 0]])
        assert_none(cursor, "SELECT k, v FROM test WHERE l CONTAINS 6")

        # sets
        assert_all(cursor, "SELECT k, v FROM test WHERE s CONTAINS 'a'", [[0, 0], [0, 2]])
        assert_all(cursor, "SELECT k, v FROM test WHERE s CONTAINS 'd'", [[1, 1]])
        assert_none(cursor, "SELECT k, v FROM test  WHERE s CONTAINS 'e'")

        # maps
        assert_all(cursor, "SELECT k, v FROM test WHERE m CONTAINS 1", [[1, 0], [1, 1], [0, 0], [0, 1]])
        assert_all(cursor, "SELECT k, v FROM test WHERE m CONTAINS 2", [[0, 1]])
        assert_none(cursor, "SELECT k, v FROM test  WHERE m CONTAINS 4")


    @since('2.1')
    def map_keys_indexing(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                v int,
                m map<text, int>,
                PRIMARY KEY (k, v)
            )
        """)

        cursor.execute("CREATE INDEX ON test(keys(m))")

        cursor.execute("INSERT INTO test (k, v, m) VALUES (0, 0, {'a' : 1})")
        cursor.execute("INSERT INTO test (k, v, m) VALUES (0, 1, {'a' : 1, 'b' : 2})")
        cursor.execute("INSERT INTO test (k, v, m) VALUES (0, 2, {'c' : 3})")
        cursor.execute("INSERT INTO test (k, v, m) VALUES (1, 0, {'b' : 1})")
        cursor.execute("INSERT INTO test (k, v, m) VALUES (1, 1, {'a' : 1, 'b' : 3})")

        # maps
        assert_all(cursor, "SELECT k, v FROM test WHERE m CONTAINS KEY 'a'", [[1, 1], [0, 0], [0, 1]])
        assert_all(cursor, "SELECT k, v FROM test WHERE m CONTAINS KEY 'c'", [[0, 2]])
        assert_none(cursor, "SELECT k, v FROM test  WHERE m CONTAINS KEY 'd'")

        # we're not allowed to create a value index if we already have a key one
        assert_invalid(cursor, "CREATE INDEX ON test(m)")

    #def nan_infinity_test(self):
    #    cursor = self.prepare()

    #    cursor.execute("CREATE TABLE test (f float PRIMARY KEY)")

    #    cursor.execute("INSERT INTO test(f) VALUES (NaN)")
    #    cursor.execute("INSERT INTO test(f) VALUES (-NaN)")
    #    cursor.execute("INSERT INTO test(f) VALUES (Infinity)")
    #    cursor.execute("INSERT INTO test(f) VALUES (-Infinity)")

    #    assert_all(cursor, "SELECT * FROM test", [[nan], [inf], [-inf]])

    @since('2.0')
    def static_columns_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                p int,
                s int static,
                v int,
                PRIMARY KEY (k, p)
            )
        """)

        cursor.execute("INSERT INTO test(k, s) VALUES (0, 42)")

        assert_one(cursor, "SELECT * FROM test", [0, None, 42, None])

        # Check that writetime works (#7081) -- we can't predict the exact value easily so
        # we just check that it's non zero
        cursor.execute("SELECT s, writetime(s) FROM test WHERE k=0")
        row = cursor.fetchone()
        assert row[0] == 42 and row[1] > 0, row

        cursor.execute("INSERT INTO test(k, p, s, v) VALUES (0, 0, 12, 0)")
        cursor.execute("INSERT INTO test(k, p, s, v) VALUES (0, 1, 24, 1)")

        # Check the static columns in indeed "static"
        assert_all(cursor, "SELECT * FROM test", [[0, 0, 24, 0], [0, 1, 24, 1]])

        # Check we do correctly get the static column value with a SELECT *, even
        # if we're only slicing part of the partition
        assert_one(cursor, "SELECT * FROM test WHERE k=0 AND p=0", [0, 0, 24, 0])
        assert_one(cursor, "SELECT * FROM test WHERE k=0 AND p=1", [0, 1, 24, 1])

        # Test for IN on the clustering key (#6769)
        assert_all(cursor, "SELECT * FROM test WHERE k=0 AND p IN (0, 1)", [[0, 0, 24, 0], [0, 1, 24, 1]])

        # Check things still work if we don't select the static column. We also want
        # this to not request the static columns internally at all, though that part
        # require debugging to assert
        assert_one(cursor, "SELECT p, v FROM test WHERE k=0 AND p=1", [1, 1])

        # Check selecting only a static column is also ok, and only yield one value
        # (as we only query the static columns)
        assert_one(cursor, "SELECT s FROM test WHERE k=0", [24])
        # but that querying other columns does correctly yield the full partition
        assert_all(cursor, "SELECT s, v FROM test WHERE k=0", [[24, 0], [24, 1]])
        assert_one(cursor, "SELECT s, v FROM test WHERE k=0 AND p=1", [24, 1])
        assert_one(cursor, "SELECT p, s FROM test WHERE k=0 AND p=1", [1, 24])
        assert_one(cursor, "SELECT k, p, s FROM test WHERE k=0 AND p=1", [0, 1, 24])

        # Check that deleting a row don't implicitely deletes statics
        cursor.execute("DELETE FROM test WHERE k=0 AND p=0")
        assert_all(cursor, "SELECT * FROM test", [[0, 1, 24, 1]])

        # But that explicitely deleting the static column does remove it
        cursor.execute("DELETE s FROM test WHERE k=0")
        assert_all(cursor, "SELECT * FROM test", [[0, 1, None, 1]])

        # Check we can add a static column ...
        cursor.execute("ALTER TABLE test ADD s2 int static")
        assert_all(cursor, "SELECT * FROM test", [[0, 1, None, None, 1]])
        cursor.execute("INSERT INTO TEST (k, p, s2, v) VALUES(0, 2, 42, 2)")
        assert_all(cursor, "SELECT * FROM test", [[0, 1, None, 42, 1], [0, 2, None, 42, 2]])
        # ... and that we can drop it
        cursor.execute("ALTER TABLE test DROP s2")
        assert_all(cursor, "SELECT * FROM test", [[0, 1, None, 1], [0, 2, None, 2]])


    @since('2.0')
    def static_columns_cas_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                id int,
                k text,
                version int static,
                v text,
                PRIMARY KEY (id, k)
            )
        """)

        # Test that INSERT IF NOT EXISTS concerns only the static column if no clustering nor regular columns
        # is provided, but concerns the CQL3 row targetted by the clustering columns otherwise
        cursor.execute("INSERT INTO test(id, k, v) VALUES (1, 'foo', 'foo')")
        assert_one(cursor, "INSERT INTO test(id, k, version) VALUES (1, 'foo', 1) IF NOT EXISTS", [False, 1, 'foo', None, 'foo']);
        assert_one(cursor, "INSERT INTO test(id, version) VALUES (1, 1) IF NOT EXISTS", [True]);
        assert_one(cursor, "SELECT * FROM test", [1, 'foo', 1, 'foo'])
        cursor.execute("DELETE FROM test WHERE id = 1");

        cursor.execute("INSERT INTO test(id, version) VALUES (0, 0)")

        assert_one(cursor, "UPDATE test SET v='foo', version=1 WHERE id=0 AND k='k1' IF version = 0", [True])
        assert_all(cursor, "SELECT * FROM test", [[0, 'k1', 1, 'foo']])

        assert_one(cursor, "UPDATE test SET v='bar', version=1 WHERE id=0 AND k='k2' IF version = 0", [False, 1])
        assert_all(cursor, "SELECT * FROM test", [[0, 'k1', 1, 'foo']])

        assert_one(cursor, "UPDATE test SET v='bar', version=2 WHERE id=0 AND k='k2' IF version = 1", [True])
        assert_all(cursor, "SELECT * FROM test", [[0, 'k1', 2, 'foo'], [0, 'k2', 2, 'bar']])

        # Testing batches
        assert_one(cursor,
        """
          BEGIN BATCH
            UPDATE test SET v='foobar' WHERE id=0 AND k='k1';
            UPDATE test SET v='barfoo' WHERE id=0 AND k='k2';
            UPDATE test SET version=3 WHERE id=0 IF version=1;
          APPLY BATCH
        """, [False, 0, None, 2])

        assert_one(cursor,
        """
          BEGIN BATCH
            UPDATE test SET v='foobar' WHERE id=0 AND k='k1';
            UPDATE test SET v='barfoo' WHERE id=0 AND k='k2';
            UPDATE test SET version=3 WHERE id=0 IF version=2;
          APPLY BATCH
        """, [True])
        assert_all(cursor, "SELECT * FROM test", [[0, 'k1', 3, 'foobar'], [0, 'k2', 3, 'barfoo']])

        assert_all(cursor,
        """
          BEGIN BATCH
            UPDATE test SET version=4 WHERE id=0 IF version=3;
            UPDATE test SET v='row1' WHERE id=0 AND k='k1' IF v='foo';
            UPDATE test SET v='row2' WHERE id=0 AND k='k2' IF v='bar';
          APPLY BATCH
        """, [[False, 0, 'k1', 3, 'foobar'], [False, 0, 'k2', 3, 'barfoo']])

        assert_one(cursor,
        """
          BEGIN BATCH
            UPDATE test SET version=4 WHERE id=0 IF version=3;
            UPDATE test SET v='row1' WHERE id=0 AND k='k1' IF v='foobar';
            UPDATE test SET v='row2' WHERE id=0 AND k='k2' IF v='barfoo';
          APPLY BATCH
        """, [True])
        assert_all(cursor, "SELECT * FROM test", [[0, 'k1', 4, 'row1'], [0, 'k2', 4, 'row2']])

        assert_invalid(cursor,
        """
          BEGIN BATCH
            UPDATE test SET version=5 WHERE id=0 IF version=4;
            UPDATE test SET v='row1' WHERE id=0 AND k='k1';
            UPDATE test SET v='row2' WHERE id=1 AND k='k2';
          APPLY BATCH
        """)

        assert_one(cursor,
        """
          BEGIN BATCH
            INSERT INTO TEST (id, k, v) VALUES(1, 'k1', 'val1') IF NOT EXISTS;
            INSERT INTO TEST (id, k, v) VALUES(1, 'k2', 'val2') IF NOT EXISTS;
          APPLY BATCH
        """, [True])
        assert_all(cursor, "SELECT * FROM test WHERE id=1", [[1, 'k1', None, 'val1'], [1, 'k2', None, 'val2']])

        assert_one(cursor,
        """
          BEGIN BATCH
            INSERT INTO TEST (id, k, v) VALUES(1, 'k2', 'val2') IF NOT EXISTS;
            INSERT INTO TEST (id, k, v) VALUES(1, 'k3', 'val3') IF NOT EXISTS;
          APPLY BATCH
        """, [False, 1, 'k2', None, 'val2'])

        assert_one(cursor,
        """
          BEGIN BATCH
            UPDATE test SET v='newVal' WHERE id=1 AND k='k2' IF v='val0';
            INSERT INTO TEST (id, k, v) VALUES(1, 'k3', 'val3') IF NOT EXISTS;
          APPLY BATCH
        """, [False, 1, 'k2', None, 'val2'])
        assert_all(cursor, "SELECT * FROM test WHERE id=1", [[1, 'k1', None, 'val1'], [1, 'k2', None, 'val2']])

        assert_one(cursor,
        """
          BEGIN BATCH
            UPDATE test SET v='newVal' WHERE id=1 AND k='k2' IF v='val2';
            INSERT INTO TEST (id, k, v, version) VALUES(1, 'k3', 'val3', 1) IF NOT EXISTS;
          APPLY BATCH
        """, [True])
        assert_all(cursor, "SELECT * FROM test WHERE id=1", [[1, 'k1', 1, 'val1'], [1, 'k2', 1, 'newVal'], [1, 'k3', 1, 'val3']])

        assert_invalid(cursor,
        """
          BEGIN BATCH
            UPDATE test SET v='newVal1' WHERE id=1 AND k='k2' IF v='val2';
            UPDATE test SET v='newVal2' WHERE id=1 AND k='k2' IF v='val3';
          APPLY BATCH
        """)


    @since('2.0')
    def static_columns_with_2i_test(self):
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                p int,
                s int static,
                v int,
                PRIMARY KEY (k, p)
            )
        """)

        cursor.execute("CREATE INDEX ON test(v)")

        cursor.execute("INSERT INTO test(k, p, s, v) VALUES (0, 0, 42, 1)")
        cursor.execute("INSERT INTO test(k, p, v) VALUES (0, 1, 1)")
        cursor.execute("INSERT INTO test(k, p, v) VALUES (0, 2, 2)")

        assert_all(cursor, "SELECT * FROM test WHERE v = 1", [[0, 0, 42, 1], [0, 1, 42, 1]])
        assert_all(cursor, "SELECT p, s FROM test WHERE v = 1", [[0, 42], [1, 42]])
        assert_all(cursor, "SELECT p FROM test WHERE v = 1", [[0], [1]])
        # We don't support that
        assert_invalid(cursor, "SELECT s FROM test WHERE v = 1")


    def select_count_paging_test(self):
        """ Test for the #6579 'select count' paging bug """

        cursor = self.prepare()
        cursor.execute("create table test(field1 text, field2 timeuuid, field3 boolean, primary key(field1, field2));")
        cursor.execute("create index test_index on test(field3);")

        cursor.execute("insert into test(field1, field2, field3) values ('hola', now(), false);");
        cursor.execute("insert into test(field1, field2, field3) values ('hola', now(), false);");

        assert_one(cursor, "select count(*) from test where field3 = false limit 1;", [1])


    @since('2.0')
    def cas_and_ttl_test(self):
        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k int PRIMARY KEY, v int, lock boolean)")

        cursor.execute("INSERT INTO test (k, v, lock) VALUES (0, 0, false)")
        cursor.execute("UPDATE test USING TTL 1 SET lock=true WHERE k=0")
        time.sleep(2)
        assert_one(cursor, "UPDATE test SET v = 1 WHERE k = 0 IF lock = null", [True])

    @since('2.0')
    def tuple_notation_test(self):
        """ Test the syntax introduced by #4851 """
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (k int, v1 int, v2 int, v3 int, PRIMARY KEY (k, v1, v2, v3))")
        for i in range(0, 2):
            for j in range(0, 2):
                for k in range(0, 2):
                    cursor.execute("INSERT INTO test(k, v1, v2, v3) VALUES (0, %d, %d, %d)" % (i, j, k))

        assert_all(cursor, "SELECT v1, v2, v3 FROM test WHERE k = 0", [[0, 0, 0],
                                                                       [0, 0, 1],
                                                                       [0, 1, 0],
                                                                       [0, 1, 1],
                                                                       [1, 0, 0],
                                                                       [1, 0, 1],
                                                                       [1, 1, 0],
                                                                       [1, 1, 1]])

        assert_all(cursor, "SELECT v1, v2, v3 FROM test WHERE k = 0 AND (v1, v2, v3) >= (1, 0, 1)", [[1, 0, 1], [1, 1, 0], [1, 1, 1]])
        assert_all(cursor, "SELECT v1, v2, v3 FROM test WHERE k = 0 AND (v1, v2) >= (1, 1)", [[1, 1, 0], [1, 1, 1]])
        assert_all(cursor, "SELECT v1, v2, v3 FROM test WHERE k = 0 AND (v1, v2) > (0, 1) AND (v1, v2, v3) <= (1, 1, 0)", [[1, 0, 0], [1, 0, 1], [1, 1, 0]])

        assert_invalid(cursor, "SELECT v1, v2, v3 FROM test WHERE k = 0 AND (v1, v3) > (1, 0)")

    def in_with_desc_order_test(self):
        cursor = self.prepare()

        cursor.execute("CREATE TABLE test (k int, c1 int, c2 int, PRIMARY KEY (k, c1, c2))")
        cursor.execute("INSERT INTO test(k, c1, c2) VALUES (0, 0, 0)");
        cursor.execute("INSERT INTO test(k, c1, c2) VALUES (0, 0, 1)");
        cursor.execute("INSERT INTO test(k, c1, c2) VALUES (0, 0, 2)");

        assert_all(cursor, "SELECT * FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0) ORDER BY c1 DESC", [[0, 0, 2], [0, 0, 0]])

    @since('2.1')
    def in_order_by_without_selecting_test(self):
        """ Test that columns don't need to be selected for ORDER BY when there is a IN (#4911) """

        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k int, c1 int, c2 int, v int, PRIMARY KEY (k, c1, c2))")

        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (0, 0, 0, 0)");
        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (0, 0, 1, 1)");
        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (0, 0, 2, 2)");
        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (1, 1, 0, 3)");
        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (1, 1, 1, 4)");
        cursor.execute("INSERT INTO test(k, c1, c2, v) VALUES (1, 1, 2, 5)");

        assert_all(cursor, "SELECT * FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0)", [[0, 0, 0, 0], [0, 0, 2, 2]])
        assert_all(cursor, "SELECT * FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0) ORDER BY c1 ASC, c2 ASC", [[0, 0, 0, 0], [0, 0, 2, 2]])

        # check that we don't need to select the column on which we order
        assert_all(cursor, "SELECT v FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0)", [[0], [2]])
        assert_all(cursor, "SELECT v FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0) ORDER BY c1 ASC", [[0], [2]])
        assert_all(cursor, "SELECT v FROM test WHERE k=0 AND c1 = 0 AND c2 IN (2, 0) ORDER BY c1 DESC", [[2], [0]])
        assert_all(cursor, "SELECT v FROM test WHERE k IN (1, 0)", [[3], [4], [5], [0], [1], [2]])
        assert_all(cursor, "SELECT v FROM test WHERE k IN (1, 0) ORDER BY c1 ASC", [[0], [1], [2], [3], [4], [5]])

    @since('2.0')
    def cas_and_compact_test(self):
        """ Test for CAS with compact storage table, and #6813 in particular """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE lock (
                partition text,
                key text,
                owner text,
                PRIMARY KEY (partition, key)
            ) WITH COMPACT STORAGE
        """)

        cursor.execute("INSERT INTO lock(partition, key, owner) VALUES ('a', 'b', null)")
        assert_one(cursor, "UPDATE lock SET owner='z' WHERE partition='a' AND key='b' IF owner=null", [True])

        assert_one(cursor, "UPDATE lock SET owner='b' WHERE partition='a' AND key='b' IF owner='a'", [False, 'z'])
        assert_one(cursor, "UPDATE lock SET owner='b' WHERE partition='a' AND key='b' IF owner='z'", [True])

        assert_one(cursor, "INSERT INTO lock(partition, key, owner) VALUES ('a', 'c', 'x') IF NOT EXISTS", [True])

    def cas_and_map_syntax_test(self):
        cursor = self.prepare()

        # Maps
        cursor.execute("""
            CREATE TABLE tmap (
                k int PRIMARY KEY,
                m map<text, text>,
            )
        """)

        cursor.execute("INSERT INTO tmap(k, m) VALUES (0, {'foo' : 'bar'})")
        assert_invalid(cursor, "DELETE FROM tmap WHERE k=0 IF m[null] = 'foo'")
        assert_one(cursor, "DELETE FROM tmap WHERE k=0 IF m['foo'] = 'foo'", [False, {'foo' : 'bar'}])
        assert_one(cursor, "DELETE FROM tmap WHERE k=0 IF m['foo'] = null", [False, {'foo' : 'bar'}])
        assert_one(cursor, "SELECT * FROM tmap", [0, {'foo' : 'bar'}])

        assert_one(cursor, "DELETE FROM tmap WHERE k=0 IF m['foo'] = 'bar'", [True])
        assert_none(cursor, "SELECT * FROM tmap")

        cursor.execute("INSERT INTO tmap(k, m) VALUES (1, {'foo' : 'bar', 'bar' : 'foo'})")
        assert_one(cursor, "DELETE FROM tmap WHERE k=1 IF m['foo'] = 'bar' AND m['bar'] = 'bar'", [False, {'foo' : 'bar', 'bar' : 'foo'}])
        assert_one(cursor, "DELETE FROM tmap WHERE k=1 IF m['foo'] = 'bar' AND m['bar'] = 'foo'", [True])
        assert_none(cursor, "SELECT * FROM tmap")

        # Lists
        cursor.execute("""
            CREATE TABLE tlist (
                k int PRIMARY KEY,
                l list<text>,
            )
        """)

        cursor.execute("INSERT INTO tlist(k, l) VALUES (0, ['foo', 'bar', 'foobar'])")
        assert_invalid(cursor, "DELETE FROM tlist WHERE k=0 IF l[null] = 'foobar'")
        assert_invalid(cursor, "DELETE FROM tlist WHERE k=0 IF l[-2] = 'foobar'")
        assert_invalid(cursor, "DELETE FROM tlist WHERE k=0 IF l[3] = 'foobar'")
        assert_one(cursor, "DELETE FROM tlist WHERE k=0 IF l[1] = null", [False, ('foo', 'bar', 'foobar')])
        assert_one(cursor, "DELETE FROM tlist WHERE k=0 IF l[1] = 'foobar'", [False, ('foo', 'bar', 'foobar')])
        assert_one(cursor, "SELECT * FROM tlist", [0, ('foo', 'bar', 'foobar')])

        assert_one(cursor, "DELETE FROM tlist WHERE k=0 IF l[1] = 'bar'", [True])
        assert_none(cursor, "SELECT * FROM tlist")

        # Sanity checks for sets
        cursor.execute("""
            CREATE TABLE tset (
                k int PRIMARY KEY,
                s set<text>,
            )
        """)
        cursor.execute("INSERT INTO tset(k, s) VALUES (0, {'foo', 'bar', 'foobar'})")
        assert_invalid(cursor, "DELETE FROM tset WHERE k=0 IF s['foo'] = 'foobar'")


    @since("2.0")
    def static_with_limit_test(self):
        """ Test LIMIT when static columns are present (#6956) """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                s int static,
                v int,
                PRIMARY KEY (k, v)
            )
        """)

        cursor.execute("INSERT INTO test(k, s) VALUES(0, 42)")
        for i in range(0, 4):
            cursor.execute("INSERT INTO test(k, v) VALUES(0, %d)" % i)

        assert_one(cursor, "SELECT * FROM test WHERE k = 0 LIMIT 1", [0, 0, 42])
        assert_all(cursor, "SELECT * FROM test WHERE k = 0 LIMIT 2", [[0, 0, 42], [0, 1, 42]])
        assert_all(cursor, "SELECT * FROM test WHERE k = 0 LIMIT 3", [[0, 0, 42], [0, 1, 42], [0, 2, 42]])

    @since("1.2")
    def limit_compact_table(self):
        """ Check for #7052 bug """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k int,
                v int,
                PRIMARY KEY (k, v)
            ) WITH COMPACT STORAGE
        """)

        for i in range(0, 4):
            for j in range(0, 4):
                cursor.execute("INSERT INTO test(k, v) VALUES (%d, %d)" % (i, j))

        assert_all(cursor, "SELECT v FROM test WHERE k=0 AND v > 0 AND v <= 4 LIMIT 2", [[1], [2]])
        assert_all(cursor, "SELECT v FROM test WHERE k=0 AND v > -1 AND v <= 4 LIMIT 2", [[0], [1]])

        assert_all(cursor, "SELECT * FROM test WHERE k IN (0, 1, 2) AND v > 0 AND v <= 4 LIMIT 2", [[0, 1], [0, 2]])
        assert_all(cursor, "SELECT * FROM test WHERE k IN (0, 1, 2) AND v > -1 AND v <= 4 LIMIT 2", [[0, 0], [0, 1]])
        assert_all(cursor, "SELECT * FROM test WHERE k IN (0, 1, 2) AND v > 0 AND v <= 4 LIMIT 6", [[0, 1], [0, 2], [0, 3], [1, 1], [1, 2], [1, 3]])

        # This doesn't work -- see #7059
        #assert_all(cursor, "SELECT * FROM test WHERE v > 1 AND v <= 3 LIMIT 6 ALLOW FILTERING", [[1, 2], [1, 3], [0, 2], [0, 3], [2, 2], [2, 3]])

    @require("6950")
    def key_index_with_reverse_clustering(self):
        """ Test for #6950 bug """
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                k1 int,
                k2 int,
                v int,
                PRIMARY KEY ((k1, k2), v)
            ) WITH CLUSTERING ORDER BY (v DESC)
        """)

        cursor.execute("CREATE INDEX ON test(k2)")

        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (0, 0, 1)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (0, 1, 2)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (0, 0, 3)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (1, 0, 4)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (1, 1, 5)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (2, 0, 7)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (2, 1, 8)")
        cursor.execute("INSERT INTO test(k1, k2, v) VALUES (3, 0, 1)")

        assert_all(cursor, "SELECT * FROM test WHERE k2 = 0 AND v >= 2 ALLOW FILTERING", [[2, 0, 7], [0, 0, 3], [1, 0, 4]]);

    @since('2.1')
    def invalid_custom_timestamp_test(self):
        cursor = self.prepare()

        # Conditional updates
        cursor.execute("CREATE TABLE test (k int, v int, PRIMARY KEY (k, v))")

        cursor.execute("BEGIN BATCH INSERT INTO test(k, v) VALUES(0, 0) IF NOT EXISTS; INSERT INTO test(k, v) VALUES(0, 1) IF NOT EXISTS; APPLY BATCH")
        assert_invalid(cursor, "BEGIN BATCH INSERT INTO test(k, v) VALUES(0, 2) IF NOT EXISTS USING TIMESTAMP 1; INSERT INTO test(k, v) VALUES(0, 3) IF NOT EXISTS; APPLY BATCH")
        assert_invalid(cursor, "BEGIN BATCH USING TIMESTAMP 1 INSERT INTO test(k, v) VALUES(0, 4) IF NOT EXISTS; INSERT INTO test(k, v) VALUES(0, 1) IF NOT EXISTS; APPLY BATCH")

        cursor.execute("INSERT INTO test(k, v) VALUES(1, 0) IF NOT EXISTS");
        assert_invalid(cursor, "INSERT INTO test(k, v) VALUES(1, 1) IF NOT EXISTS USING TIMESTAMP 5");

        # Counters
        cursor.execute("CREATE TABLE counters (k int PRIMARY KEY, c counter)")

        cursor.execute("UPDATE counters SET c = c + 1 WHERE k = 0")
        assert_invalid(cursor, "UPDATE counters USING TIMESTAMP 10 SET c = c + 1 WHERE k = 0")

        cursor.execute("BEGIN COUNTER BATCH UPDATE counters SET c = c + 1 WHERE k = 0; UPDATE counters SET c = c + 1 WHERE k = 0; APPLY BATCH")
        assert_invalid(cursor, "BEGIN COUNTER BATCH UPDATE counters USING TIMESTAMP 3 SET c = c + 1 WHERE k = 0; UPDATE counters SET c = c + 1 WHERE k = 0; APPLY BATCH")
        assert_invalid(cursor, "BEGIN COUNTER BATCH USING TIMESTAMP 3 UPDATE counters SET c = c + 1 WHERE k = 0; UPDATE counters SET c = c + 1 WHERE k = 0; APPLY BATCH")


    @since('2.1')
    def add_field_to_udt_test(self):
        cursor = self.prepare()

        cursor.execute("CREATE TYPE footype (fooint int, fooset set <text>)")
        cursor.execute("CREATE TABLE test (key int PRIMARY KEY, data footype)")

        cursor.execute("INSERT INTO test (key, data) VALUES (1, {fooint: 1, fooset: {'2'}})")
        cursor.execute("ALTER TYPE footype ADD foomap map <int,text>")
        cursor.execute("INSERT INTO test (key, data) VALUES (1, {fooint: 1, fooset: {'2'}, foomap: {3 : 'bar'}})")

    @since('1.2')
    def clustering_order_in_test(self):
        """Test for #7105 bug"""
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                a int,
                b int,
                c int,
                PRIMARY KEY ((a, b), c)
            ) with clustering order by (c desc)
        """);

        cursor.execute("INSERT INTO test (a, b, c) VALUES (1, 2, 3)")
        cursor.execute("INSERT INTO test (a, b, c) VALUES (4, 5, 6)")

        assert_one(cursor, "SELECT * FROM test WHERE a=1 AND b=2 AND c IN (3)", [1, 2, 3])
        assert_one(cursor, "SELECT * FROM test WHERE a=1 AND b=2 AND c IN (3, 4)", [1, 2, 3])

    @since('1.2')
    def bug7105_test(self):
        """Test for #7105 bug"""
        cursor = self.prepare()

        cursor.execute("""
            CREATE TABLE test (
                a int,
                b int,
                c int,
                d int,
                PRIMARY KEY (a, b)
            )
        """);

        cursor.execute("INSERT INTO test (a, b, c, d) VALUES (1, 2, 3, 3)")
        cursor.execute("INSERT INTO test (a, b, c, d) VALUES (1, 4, 6, 5)")

        assert_one(cursor, "SELECT * FROM test WHERE a=1 AND b=2 ORDER BY b DESC", [1, 2, 3, 3])

########NEW FILE########
__FILENAME__ = delete_insert_test
from dtest import Tester, debug
import uuid
import cql
import os
import threading
import random

class DeleteInsertTest(Tester):
    """
    Examines scenarios around deleting data and adding data back with the same key
    """

    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

        # Generate 1000 rows in memory so we can re-use the same ones over again:
        self.groups = ['group1', 'group2', 'group3', 'group4']
        self.rows = [(str(uuid.uuid1()),x,random.choice(self.groups)) for x in range(1000)]
    
    def create_ddl(self, cursor, rf={'dc1':2, 'dc2':2}):
        self.create_ks(cursor, 'delete_insert_search_test', rf)
        cursor.execute('CREATE TABLE test (id uuid PRIMARY KEY, val1 text, group text)')
        cursor.execute('CREATE INDEX group_idx ON test (group)')

    def delete_all_rows(self, cursor):
        for id, val, group in self.rows:
            cursor.execute("DELETE FROM test WHERE id=%s" % u)

    def delete_group_rows(self, cursor, group):
        """Delete rows from a given group and return them"""
        rows = [r for r in self.rows if r[2]==group]
        ids = [r[0] for r in rows]
        cursor.execute('DELETE FROM test WHERE id in (%s)' % ', '.join(ids))
        return rows

    def insert_all_rows(self, cursor):
        self.insert_some_rows(cursor, self.rows)

    def insert_some_rows(self, cursor, rows):
        for row in rows:
            cursor.execute("INSERT INTO test (id, val1, group) VALUES (%s, '%s', '%s')" % row)

    def delete_insert_search_test(self):
        cluster = self.cluster
        cluster.populate([2,2]).start()
        node1 = cluster.nodelist()[0]

        cursor = self.cql_connection(node1).cursor()
        cursor.consistency_level = 'LOCAL_QUORUM'
        
        self.create_ddl(cursor)
        # Create 1000 rows:
        self.insert_all_rows(cursor)
        # Delete all of group2:
        deleted = self.delete_group_rows(cursor, 'group2')
        # Put that group back:
        self.insert_some_rows(cursor, rows=deleted)
        
        # Verify that all of group2 is back, 20 times, in parallel
        # querying across all nodes:

        class ThreadedQuery(threading.Thread):
            def __init__(self, connection):
                threading.Thread.__init__(self)
                self.connection = connection
                
            def run(self):
                cursor = self.connection.cursor()
                cursor.consistency_level = 'LOCAL_QUORUM'
                cursor.execute("SELECT * FROM delete_insert_search_test.test WHERE group = 'group2'")
                assert cursor.rowcount == len(deleted)

        threads = []
        for x in range(20):
            conn = self.cql_connection(random.choice(cluster.nodelist()))
            threads.append(ThreadedQuery(conn))
        for t in threads:
            t.start()
        for t in threads:
            t.join()
                           

########NEW FILE########
__FILENAME__ = deletion_test
from dtest import Tester
from assertions import *

import os, sys, time, tools
from ccmlib.cluster import Cluster

class TestDeletion(Tester):

    def gc_test(self):
        """ Test that tombstone are fully purge after gc_grace """
        cluster = self.cluster

        cluster.populate(1).start()
        [node1] = cluster.nodelist()

        time.sleep(.5)
        cursor = self.cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', gc_grace=0, key_type='int', columns={'c1': 'int'})

        cursor.execute('insert into cf (key, c1) values (1,1)')
        cursor.execute('insert into cf (key, c1) values (2,1)')
        node1.flush()

        cursor.execute('select * from cf;')
        result = cursor.fetchall()
        assert len(result) == 2 and len(result[0]) == 2 and len(result[1]) == 2, result

        cursor.execute('delete from cf where key=1')
        cursor.execute('select * from cf;')
        result = cursor.fetchall()
        if cluster.version() < '1.2': # > 1.2 doesn't show tombstones
            assert len(result) == 2 and len(result[0]) == 1 and len(result[1]) == 1, result

        node1.flush()
        time.sleep(.5)
        node1.compact()
        time.sleep(.5)

        cursor.execute('select * from cf;')
        result = cursor.fetchall()
        assert len(result) == 1 and len(result[0]) == 2, result


########NEW FILE########
__FILENAME__ = dtest
from __future__ import with_statement
import os, tempfile, sys, shutil, subprocess, types, time, threading, ConfigParser, logging
import fnmatch
import re
import copy

from ccmlib.cluster import Cluster
from ccmlib.node import Node
from cql.thrifteries import ThriftCursor
from uuid import UUID
from nose.exc import SkipTest
from thrift.transport import TSocket
from unittest import TestCase

LOG_SAVED_DIR="logs"
try:
    os.mkdir(LOG_SAVED_DIR)
except OSError:
    pass

LAST_LOG = os.path.join(LOG_SAVED_DIR, "last")

LAST_TEST_DIR='last_test_dir'

DEFAULT_DIR='./'
config = ConfigParser.RawConfigParser()
if len(config.read(os.path.expanduser('~/.cassandra-dtest'))) > 0:
    if config.has_option('main', 'default_dir'):
        DEFAULT_DIR=os.path.expanduser(config.get('main', 'default_dir'))

NO_SKIP = os.environ.get('SKIP', '').lower() in ('no', 'false')
DEBUG = os.environ.get('DEBUG', '').lower() in ('yes', 'true')
TRACE = os.environ.get('TRACE', '').lower() in ('yes', 'true')
KEEP_LOGS = os.environ.get('KEEP_LOGS', '').lower() in ('yes', 'true')
KEEP_TEST_DIR = os.environ.get('KEEP_TEST_DIR', '').lower() in ('yes', 'true')
PRINT_DEBUG = os.environ.get('PRINT_DEBUG', '').lower() in ('yes', 'true')
DISABLE_VNODES = os.environ.get('DISABLE_VNODES', '').lower() in ('yes', 'true')
OFFHEAP_MEMTABLES = os.environ.get('OFFHEAP_MEMTABLES', '').lower() in ('yes', 'true')
NUM_TOKENS = os.environ.get('NUM_TOKENS', '256')

CURRENT_TEST = ""

logging.basicConfig(filename=os.path.join(LOG_SAVED_DIR,"dtest.log"),
                    filemode='w',
                    format='%(asctime)s,%(msecs)d %(name)s %(current_test)s %(levelname)s %(message)s',
                    datefmt='%H:%M:%S',
                    level=logging.DEBUG)

LOG = logging.getLogger('dtest')

# copy the initial environment variables so we can reset them later:
initial_environment = copy.deepcopy(os.environ)
def reset_environment_vars():
    os.environ.clear()
    os.environ.update(initial_environment)

def debug(msg):
    LOG.debug(msg, extra={"current_test":CURRENT_TEST})
    if PRINT_DEBUG:
        print msg

def retry_till_success(fun, *args, **kwargs):
    timeout = kwargs.pop('timeout', 60)
    bypassed_exception = kwargs.pop('bypassed_exception', Exception)

    deadline = time.time() + timeout
    while True:
        try:
            return fun(*args, **kwargs)
        except bypassed_exception:
            if time.time() > deadline:
                raise
            else:
                # brief pause before next attempt
                time.sleep(0.25)

class Tester(TestCase):

    def __init__(self, *argv, **kwargs):
        # if False, then scan the log of each node for errors after every test.
        self.allow_log_errors = False
        self.cluster_options = kwargs.pop('cluster_options', None)
        super(Tester, self).__init__(*argv, **kwargs)

    def _get_cluster(self, name='test'):
        self.test_path = tempfile.mkdtemp(prefix='dtest-')
        # ccm on cygwin needs absolute path to directory - it crosses from cygwin space into
        # regular Windows space on wmic calls which will otherwise break pathing
        if sys.platform == "cygwin":
            self.test_path = subprocess.Popen(["cygpath", "-m", self.test_path], stdout = subprocess.PIPE, stderr = subprocess.STDOUT).communicate()[0].rstrip()
        debug("cluster ccm directory: "+self.test_path)
        version = os.environ.get('CASSANDRA_VERSION')
        cdir = os.environ.get('CASSANDRA_DIR', DEFAULT_DIR)
        
        if version:
            cluster = Cluster(self.test_path, name, cassandra_version=version)
        else:
            cluster = Cluster(self.test_path, name, cassandra_dir=cdir)
        
        if cluster.version() >= "1.2":
            if DISABLE_VNODES:
                cluster.set_configuration_options(values={'num_tokens': None})
            else:
                cluster.set_configuration_options(values={'initial_token': None, 'num_tokens': NUM_TOKENS})

        if cluster.version() >= "2.1":
            if OFFHEAP_MEMTABLES:
                cluster.set_configuration_options(values={'memtable_allocation_type': 'offheap_objects'})

        return cluster

    def _cleanup_cluster(self):
        if KEEP_TEST_DIR:
            # Just kill it, leave the files where they are:
            self.cluster.stop(gently=False)
        else:
            # Cleanup everything:
            debug("removing ccm cluster " + self.cluster.name + " at: " + self.test_path)
            self.cluster.remove()
            os.rmdir(self.test_path)
        if os.path.exists(LAST_TEST_DIR):
            os.remove(LAST_TEST_DIR)

    def set_node_to_current_version(self, node):
        version = os.environ.get('CASSANDRA_VERSION')
        cdir = os.environ.get('CASSANDRA_DIR', DEFAULT_DIR)
        
        if version:
            node.set_cassandra_dir(cassandra_version=version)
        else:
            node.set_cassandra_dir(cassandra_dir=cdir)
        
    def setUp(self):
        global CURRENT_TEST
        CURRENT_TEST = self.id() + self._testMethodName
        # cleaning up if a previous execution didn't trigger tearDown (which
        # can happen if it is interrupted by KeyboardInterrupt)
        # TODO: move that part to a generic fixture
        if os.path.exists(LAST_TEST_DIR):
            with open(LAST_TEST_DIR) as f:
                self.test_path = f.readline().strip('\n')
                name = f.readline()
                try:
                    self.cluster = Cluster.load(self.test_path, name)
                    # Avoid waiting too long for node to be marked down
                    self._cleanup_cluster()
                except IOError:
                    # after a restart, /tmp will be emptied so we'll get an IOError when loading the old cluster here
                    pass

        self.cluster = self._get_cluster()
        self.__setup_cobertura()
        # the failure detector can be quite slow in such tests with quick start/stop
        self.cluster.set_configuration_options(values={'phi_convict_threshold': 5})

        timeout = 10000
        if self.cluster_options is not None:
            self.cluster.set_configuration_options(values=self.cluster_options)
        elif self.cluster.version() < "1.2":
            self.cluster.set_configuration_options(values={'rpc_timeout_in_ms': timeout})
        else:
            self.cluster.set_configuration_options(values={
                'read_request_timeout_in_ms' : timeout,
                'range_request_timeout_in_ms' : timeout,
                'write_request_timeout_in_ms' : timeout,
                'truncate_request_timeout_in_ms' : timeout,
                'request_timeout_in_ms' : timeout
            })

        with open(LAST_TEST_DIR, 'w') as f:
            f.write(self.test_path + '\n')
            f.write(self.cluster.name)
        if DEBUG:
            self.cluster.set_log_level("DEBUG")
        if TRACE:
            self.cluster.set_log_level("TRACE")
        self.connections = []
        self.runners = []

    @classmethod
    def tearDownClass(cls):
        reset_environment_vars()

    def tearDown(self):
        reset_environment_vars()

        for con in self.connections:
            con.close()

        for runner in self.runners:
            try:
                runner.stop()
            except:
                pass

        failed = sys.exc_info() != (None, None, None)
        try:
            for node in self.cluster.nodelist():
                if self.allow_log_errors == False:
                    errors = list(self.__filter_errors([ msg for msg, i in node.grep_log("ERROR")]))
                    if len(errors) is not 0:
                        failed = True
                        raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
        finally:
            try:
                if failed or KEEP_LOGS:
                    # means the test failed. Save the logs for inspection.
                    self.copy_logs()
            except Exception as e:
                    print "Error saving log:", str(e)
            finally:
                self._cleanup_cluster()

    def copy_logs(self, directory=None, name=None):
        """Copy the current cluster's log files somewhere, by default to LOG_SAVED_DIR with a name of 'last'"""
        if directory is None:
            directory = LOG_SAVED_DIR
        if name is None:
            name = LAST_LOG
        else:
            name = os.path.join(directory, name)
        if not os.path.exists(directory):
            os.mkdir(directory)
        logs = [ (node.name, node.logfilename()) for node in self.cluster.nodes.values() ]
        if len(logs) is not 0:
            basedir = str(int(time.time() * 1000)) + '_' + self.id()
            logdir = os.path.join(directory, basedir)
            os.mkdir(logdir)
            for n, log in logs:
                shutil.copyfile(log, os.path.join(logdir, n + ".log"))
            if os.path.exists(name):
                os.unlink(name)
            os.symlink(basedir, name)

    def cql_connection(self, node, keyspace=None, version=None, user=None, password=None):
        import cql
        host, port = node.network_interfaces['thrift']
        if not version and self.cluster.version() >= "1.2":
            version = "3.0.0"
        elif not version and self.cluster.version() >= "1.1":
            version = "2.0.0"

        if version:
            con = cql.connect(host, port, keyspace=keyspace, cql_version=version, user=user, password=password)
        else:
            con = cql.connect(host, port, keyspace=keyspace, user=user, password=password)
        self.connections.append(con)
        return con

    def patient_cql_connection(self, node, keyspace=None, version=None, user=None, password=None, timeout=10):
        """
        Returns a connection after it stops throwing TTransportExceptions due to not being ready.

        If the timeout is exceeded, the exception is raised.
        """
        return retry_till_success(
            self.cql_connection,
            node,
            keyspace=keyspace,
            version=version,
            user=user,
            password=password,
            timeout=timeout,
            bypassed_exception=TSocket.TTransportException
        )

    def create_ks(self, cursor, name, rf):
        if self.cluster.version() >= "1.2" and cursor.cql_major_version >= 3:
            query = 'CREATE KEYSPACE %s WITH replication={%s}'
            if isinstance(rf, types.IntType):
                # we assume simpleStrategy
                cursor.execute(query % (name, "'class':'SimpleStrategy', 'replication_factor':%d" % rf))
            else:
                assert len(rf) != 0, "At least one datacenter/rf pair is needed"
                # we assume networkTopolyStrategy
                options = (', ').join([ '\'%s\':%d' % (d, r) for d, r in rf.iteritems() ])
                cursor.execute(query % (name, "'class':'NetworkTopologyStrategy', %s" % options))
        else:
            query = 'CREATE KEYSPACE %s WITH strategy_class=%s AND %s'
            if isinstance(rf, types.IntType):
                # we assume simpleStrategy
                cursor.execute(query % (name, 'SimpleStrategy', 'strategy_options:replication_factor=%d' % rf))
            else:
                assert len(rf) != 0, "At least one datacenter/rf pair is needed"
                # we assume networkTopolyStrategy
                options = (' AND ').join([ 'strategy_options:%s=%d' % (d, r) for d, r in rf.iteritems() ])
                cursor.execute(query % (name, 'NetworkTopologyStrategy', options))
        cursor.execute('USE %s' % name)

    # We default to UTF8Type because it's simpler to use in tests
    def create_cf(self, cursor, name, key_type="varchar", speculative_retry=None, read_repair=None, compression=None, gc_grace=None, columns=None, validation="UTF8Type"):
        additional_columns = ""
        if columns is not None:
            for k, v in columns.items():
                additional_columns = "%s, %s %s" % (additional_columns, k, v)

        if self.cluster.version() >= "1.2":
            if additional_columns == "":
                query = 'CREATE COLUMNFAMILY %s (key %s, c varchar, v varchar, PRIMARY KEY(key, c)) WITH comment=\'test cf\'' % (name, key_type)
            else:
                query = 'CREATE COLUMNFAMILY %s (key %s PRIMARY KEY%s) WITH comment=\'test cf\'' % (name, key_type, additional_columns)
            if compression is not None:
                query = '%s AND compression = { \'sstable_compression\': \'%sCompressor\' }' % (query, compression)
        else:
            query = 'CREATE COLUMNFAMILY %s (key %s PRIMARY KEY%s) WITH comparator=UTF8Type AND default_validation=%s' % (name, key_type, additional_columns, validation)
            if compression is not None:
                query = '%s AND compression_parameters:sstable_compression=%sCompressor' % (query, compression)

        if read_repair is not None:
            query = '%s AND read_repair_chance=%f' % (query, read_repair)
        if gc_grace is not None:
            query = '%s AND gc_grace_seconds=%d' % (query, gc_grace)
        if self.cluster.version() >= "2.0":
            if speculative_retry is not None:
                query = '%s AND speculative_retry=\'%s\'' % (query, speculative_retry)

        cursor.execute(query)
        time.sleep(0.2)

    def go(self, func):
        runner = Runner(func)
        self.runners.append(runner)
        runner.start()
        return runner

    def skip(self, msg):
        if not NO_SKIP:
            raise SkipTest(msg)
        
    def __setup_cobertura(self, cluster_name='test'):
        """Setup Cobertura code coverage support"""
        # Find the cobertura jar file:
        cobertura_jar = None
        if 'M2_REPO' in os.environ:
            m2_dir = os.environ['M2_REPO']
        else:
            m2_dir = os.path.join(os.path.expanduser('~'),'.m2')
        for root, dirnames, filenames in os.walk(m2_dir):
            for filename in fnmatch.filter(filenames, 'cobertura-*.jar'):
                cobertura_jar = os.path.join(root, filename)
                break
            if cobertura_jar:
                break
        else:
            LOG.warning(
                'Could not setup code coverage analysis because no cobertura '
                'jar file was found in the m2 repository.')
            return

        # Create a cluster-wide cassandra include file in the ccm
        # staging directory:
        with open(os.path.join(
                self.test_path, cluster_name, 'cassandra.in.sh'),'w') as f:
            f.write('CLASSPATH=$CASSANDRA_HOME/build/cobertura/classes:'
                    '$CLASSPATH:{cobertura_jar}\n'.format(
                    cobertura_jar=cobertura_jar))
            f.write('JVM_OPTS="$JVM_OPTS -Dnet.sourceforge.cobertura.datafile='
                    '$CASSANDRA_HOME/build/cobertura/cassandra-dtest/cobertura.ser -XX:-UseSplitVerifier"\n')

    def __filter_errors(self, errors):
        """Filter errors, removing those that match self.ignore_log_patterns"""
        if not hasattr(self, 'ignore_log_patterns'):
            self.ignore_log_patterns = []
        for e in errors:
            for pattern in self.ignore_log_patterns:
                if re.search(pattern, e):
                    break
            else:
                yield e

    # Disable docstrings printing in nosetest output
    def shortDescription(self):
        return None

class Runner(threading.Thread):
    def __init__(self, func):
        threading.Thread.__init__(self)
        self.__func = func
        self.__error = None
        self.__stopped = False
        self.daemon = True

    def run(self):
        i = 0
        while True:
            if self.__stopped:
                return
            try:
                self.__func(i)
            except Exception as e:
                self.__error = e
                return
            i = i + 1

    def stop(self):
        self.__stopped = True
        self.join()
        if self.__error is not None:
            raise self.__error

    def check(self):
        if self.__error is not None:
            raise self.__error


class TracingCursor(ThriftCursor):
    """A CQL Cursor with query tracing ability"""
    def __init__(self, connection):
        ThriftCursor.__init__(self, connection)
        self.last_session_id = None
        self.connection = connection

    def execute(self, cql_query, params={}, decoder=None, 
                consistency_level=None, trace=True):
        if trace:
            self.last_session_id = UUID(bytes=self.connection.client.trace_next_query())
        ThriftCursor.execute(self, cql_query, params=params, decoder=decoder, 
                             consistency_level=consistency_level)

    def get_last_trace(self):
        if self.last_session_id:
            time.sleep(0.5) # Tracing is done async, so wait a little.
            self.execute("SELECT session_id, event_id, activity, source, "
                         "source_elapsed, thread FROM system_traces.events "
                         "WHERE session_id=%s" % self.last_session_id)
            return [event for event in self]
        else:
            raise AssertionError('No query to trace')
        

########NEW FILE########
__FILENAME__ = global_row_key_cache_test
import time

from dtest import Tester, debug
from loadmaker import LoadMaker

class TestGlobalRowKeyCache(Tester):

    def __init__(self, *argv, **kwargs):
        super(TestGlobalRowKeyCache, self).__init__(*argv, **kwargs)
        # When a node goes down under load it prints an error in it's log. 
        # If we don't allow log errors, then the test will fail.
#        self.allow_log_errors = True

    def functional_test(self):
        """
        Test global caches.

        Test that save and load work in the situation when you write to
        different CFs. Read 2 or 3 times to make sure the page cache doesn't
        skew the results.
        """

        # create some rows to insert
        NUM_INSERTS = 100
        NUM_UPDATES = 10
        NUM_DELETES = 1

        cluster = self.cluster
        cluster.populate(3)
        node1 = cluster.nodelist()[0]

        for kcsim in (0, 10):
            for rcsim in (0, 10):
                setup_name = "%d_%d" % (kcsim, rcsim)
                ks_name = 'ks_' + setup_name

                debug("setup " + setup_name)
                cluster.set_configuration_options(values={
                        'key_cache_size_in_mb': kcsim,
                        'row_cache_size_in_mb': rcsim,
                        'row_cache_save_period': 5,
                        'key_cache_save_period': 5,
                        })
                cluster.start()
                time.sleep(.5)
                cursor = self.cql_connection(node1).cursor()
                self.create_ks(cursor, ks_name, 3)
                time.sleep(1) # wait for propagation

                host, port = node1.network_interfaces['thrift']

                # create some load makers
                lm_standard = LoadMaker(host, port, 
                        keyspace_name=ks_name, column_family_type='standard')
                lm_counter = LoadMaker(host, port,
                        keyspace_name=ks_name, column_family_type='standard', is_counter=True)

                # insert some rows
                lm_standard.generate(NUM_INSERTS)
                lm_counter.generate(NUM_INSERTS)

                # flush everything to get it into sstables
                for node in cluster.nodelist():
                    node.flush()

                debug("Validating")
                for i in range(3):
                    # read and modify multiple times to get data into and invalidated out of the cache.
                    lm_standard.update(NUM_UPDATES).delete(NUM_DELETES).validate()
                    lm_counter.generate().validate()

                # let the data be written to the row/key caches.
                debug("Letting caches be written")
                time.sleep(10)
                debug("Stopping cluster")
                cluster.stop()
                time.sleep(1)
                debug("Starting cluster")
                cluster.start()
                time.sleep(5) # read the data back from row and key caches

                lm_standard.refresh_connection()
                lm_counter.refresh_connection()

                debug("Validating again...")
                for i in range(2):
                    # read and modify multiple times to get data into and invalidated out of the cache.
                    lm_standard.validate()
                    lm_counter.validate()


                cluster.stop()

########NEW FILE########
__FILENAME__ = loadmaker
#!/usr/bin/env python

from dtest import debug
import inspect
import os
import pickle
import platform
import re
import sys
import tempfile
import time
import uuid
import pprint
import threading
import cql

try:
    import cStringIO as StringIO
except ImportError:
    import StringIO


class LoadMaker(object):
    """
    Allows you to send data to cassandra multiple times using the generate()
    function. Then using the validate() function you can load it back and
    verify that what comes back is what was sent.

    Defaults are provided in _DEFAULTS, and each can be overwritten
    by passing parameters with the same name to the constructor.
    """

    _DEFAULTS = {
        'keyspace_name': 'Keyspace_lm',
        'replication_factor': '3',

        'is_counter': False,
        'is_indexed': False,
        'column_family_name': None, # needs to be overwritten for the moment
        'consistency_level': 'QUORUM',
        'column_family_type': 'standard',
        'validation_type': 'text',
        'comparator_type': 'text',
        'key_validation_type': 'text',
        'num_cols': 5,
        'num_counter_rows': 10, # only applies to counter columns
    }


    def __init__(self, host='localhost', port=9160, create_ks=True, create_cf=True, **kwargs):

        # allow for overwriting any of the defaults
        self._params = LoadMaker._DEFAULTS.copy()
        cf_name_generated = 'cf'
        for key, value in kwargs.items():
            if key not in self._params.keys():
                raise AttributeError("%s is an illegal arguement!" % key)
            self._params[key] = value
            cf_name_generated += '_' + key + '_' + re.sub('\W', '', str(value))

        # use our generated column family name if not given:
        if self._params['column_family_name'] is None:
            # keep the column family name within 32 chars:
            if len(cf_name_generated) > 32:
                import hashlib
                cf_name_generated = hashlib.md5(cf_name_generated).hexdigest()
            self._params['column_family_name'] = ('cf_' + cf_name_generated)[:32]

        # column_family_type should be lowercase so that future comparisons will work.
        self._params['column_family_type'] = self._params['column_family_type'].lower()

        self._num_generate_calls = 0

        # Keys are in a sort of ever-growing queue. They are inserted at the
        # end of high indexes, and updated and deleted from low indexes.
        # Currently you can only delete() keys that you have update()d.
        self._inserted_key_count = 0
        self._updated_key_count = 0
        self._deleted_key_count = 0

        # time each DB operationan and return the last one. Only times,
        # as much as possible, the DB portion of the operation.
        self.last_operation_time = 0

        self._is_keyspace_created = False
        self.refresh_connection(host, port)
        if create_ks:
            self.create_keyspace(self._cursor)
        self._is_keyspace_created = True
        cql_str = "USE %s" % self._params['keyspace_name']
        self.execute_query(cql_str)

        if create_cf:
            self.create_column_family()
        elif self._params['is_counter']:
            # Now find the value that the counters should have. They should all have the same value.
            row_key = self._generate_row_key(0)
            col_name = self._generate_col_name(0)
            cql_str = "SELECT '%s' FROM %s WHERE KEY='%s'"%(
                    col_name, self._params['column_family_name'], row_key)
            self.execute_query(cql_str)
            from_db = self._cursor.fetchone()
            self._num_generate_calls = from_db[0] or 0

    
    @property
    def _cursor(self):
        if self._cached_cursor == None:
            self.refresh_connection()
        return self._cached_cursor
                

    def refresh_connection(self, host=None, port=None, cql_version="2.0.0", num_retries=10):
        """
        establish a connection to the server. retry if needed.
        """
        if host:
            self._host = host
        if port:
            self._port = port
        self._cql_version = cql_version or None
            
        for try_num in xrange(1+num_retries):
            try:
                if self._cql_version:
                    con = cql.connect(self._host, self._port, keyspace=None, cql_version=self._cql_version)
                else:
                    con = cql.connect(self._host, self._port, keyspace=None)
                self._cached_cursor = con.cursor()
                if self._is_keyspace_created:
                    cql_str = "USE %s" % self._params['keyspace_name']
                    self.execute_query(cql_str)
            except:
                if try_num < num_retries:
                    time.sleep(1)
                else:
                    raise


    def __str__(self):
        d = {'is_counter': self._params['is_counter'],
             'column_family_type': self._params['column_family_type']}
        return "<LoadMaker %s>" % str(d)


    def str_info(self):
        # print out the _params and some other stuff
        params = dict(list(self._params.items()) + list({
            '_inserted_key_count': self._inserted_key_count,
            '_num_generate_calls': self._num_generate_calls,
        }.items()))
        return "LoadMaker<" + str(params) + ">"
    
    
    def batch_insert(self, rows):
        cql_stream = StringIO.StringIO()
        cql_stream.write("BEGIN BATCH USING CONSISTENCY %s\n" % self._params['consistency_level'])
        for key, col_dict in rows.items():
            col_names = col_dict.keys()
            columns = ', '.join("'"+str(a)+"'" for a in col_names)
            values = ', '.join("'"+str(col_dict[a])+"'" for a in col_names)
            cql_stream.write("INSERT INTO %s (KEY, %s) VALUES ('%s', %s);\n" % (
                    self._params['column_family_name'],
                    columns,
                    key,
                    values,
                    ))
        cql_stream.write("APPLY BATCH;")
        self.execute_query(cql_stream.getvalue())


    def generate(self, num_keys=10000):
        """
        Generates a bunch of data and inserts it into cassandra
        """
        debug("Generate() starting " + str(self))
        new_inserted_key_count = self._inserted_key_count + num_keys
        
        if self._params['is_counter']:
            self._generate_counter()
        else:
            rows = self._gen_rows(self._inserted_key_count, new_inserted_key_count)
            self.batch_insert(rows)
            debug("Generate inserted %d rows" % len(rows))

        self._inserted_key_count = new_inserted_key_count
        self._num_generate_calls += 1

        return self


    def update(self, num_keys=1000):
        """
        Update some keys that were previously inserted.
        """
        saved_updated_key_count = self._updated_key_count
        self._updated_key_count += num_keys
        assert self._updated_key_count <= self._inserted_key_count, "You have to generate() more then you update()!"

        if self._params['is_counter']:
            raise NotImplemented("Counter updates have not been implemented yet.")
        else:
            rows = self._gen_rows(saved_updated_key_count, self._updated_key_count)
            # do the update
            self.batch_insert(rows)
            debug("update() inserted %d rows" % len(rows))

            # remove the first column from each row
            for row_key in rows.keys():
                col_name = self._generate_col_name(0)
                cql_str = "DELETE '%s' FROM %s WHERE KEY='%s'"%(col_name, self._params['column_family_name'], row_key)
                self.execute_query(cql_str)

        return self


    def delete(self, num_keys=100):
        """
        deletes some rows.
        """
        saved_deleted_key_count = self._deleted_key_count
        self._deleted_key_count += num_keys
        assert self._deleted_key_count <= self._updated_key_count, "You have to update() more then you delete()!"

        row_keys = [self._generate_row_key(i) for i in xrange(saved_deleted_key_count, self._deleted_key_count)]
        for row_key in row_keys:
            cql_str = "DELETE FROM %s WHERE KEY='%s'"%(self._params['column_family_name'], row_key)
            self.execute_query(cql_str)
        return self
            

    def _iterate_over_counter_columns(self, func):
        """
        calls func on every column that should be in the counter column family.

        func should have a signature like this:
        func(row_key, col_name, subcol_name=None)
        """
        col_count = 0
        for row_index in xrange(self._params['num_counter_rows']):
            row_key = self._generate_row_key(row_index)
            for col_index in xrange(self._params['num_cols']):
                col_name = self._generate_col_name(col_index)
                func(row_key, col_name)
                col_count += 1
        debug("iterated over %d counter columns" % col_count)


    def _generate_counter(self):
        """
        increments all counters. There are num_keys counter rows,
        each with self._params['num_cols'] individual counters.
        This increments each by one.
        """
        def add_func(row_key, col_name, subcol_name=None):
            cql_str = """UPDATE %(cf_name)s SET %(col_name)s=%(col_name)s+1
            WHERE KEY=%(row_key)s;""" % {
                'cf_name': self._params['column_family_name'],
                'col_name': col_name,
                'row_key': row_key,
            }
            self.execute_query(cql_str, num_retries=0)

        self._iterate_over_counter_columns(add_func)


    def multiget(self, keys):
        assert len(keys) > 0, "At least one key must be specified!"
        keys_str = ', '.join("'"+str(key)+"'" for key in keys)
        cql_str = "SELECT * FROM %s USING CONSISTENCY %s WHERE KEY in (%s)" % (
            self._params['column_family_name'], self._params['consistency_level'], keys_str)
        self.execute_query(cql_str)
        rows = self._cursor.result
        out = {}
        for row in rows:
            row_key = row[0].value
            out[row_key] = {}
            cols = row[1:]
            for col in cols:
                col_name = col.name
                col_value = col.value
                out[row_key][col_name] = col_value

        return out
        

    def validate(self, start_index=0, end_index=sys.maxint, step=1, server_list=['localhost:9160']):
        """
        gets the rows from start_index (inclusive) to end_index (exclusive) and
        compares them against what they are supposed to be. If end_index
        is greater than what has been inserted, it will read to the last
        value that was inserted.
        """
        debug("validate() starting " + str(self))
        if end_index > self._inserted_key_count:
            end_index = self._inserted_key_count
        assert(start_index <= end_index)

        if self._params['is_counter']:
            self._validate_counter()

        else:
            # generate what we expect to read
            rows = self._gen_rows(start_index, end_index, step)
            read_rows = self.multiget(rows.keys())

            if len(list(read_rows)) < len(rows):
                raise Exception("number of rows (%s) doesn't match expected number (%s)" % (str(len(read_rows)), str(len(rows))))

            # check every row to make sure everything matches
            for row_key, row_value in rows.items():
                read_row_value = read_rows[row_key]
                if row_value != read_row_value:
                    raise AssertionError(
                    "The value written does not match the value read! should be: %s was: %s" %
                    (pprint.pformat(row_value), pprint.pformat(read_row_value)))

            # make sure that deleted rows really are gone.
            row_keys = [self._generate_row_key(i) for i in xrange(0, self._deleted_key_count, step)]

        debug("validate() succeeded")
        return self


    def _validate_counter(self):
        def validate_func(row_key, col_name):
            assert self._num_generate_calls > 0, "Data must be generated before validating!"
            cql_str = "SELECT '%s' FROM %s WHERE KEY='%s'"%(col_name, self._params['column_family_name'], row_key)
            self.execute_query(cql_str)
            from_db = self._cursor.fetchone()
            val = from_db[0]
            assert self._num_generate_calls == val, "A counter did not have the right value! %s != %s, QUERY: %s" %(val, self._num_generate_calls, cql_str)
        self._iterate_over_counter_columns(validate_func)


    def _gen_rows(self, start_index, end_index, step=1):
        """
        Generates a bunch of rows from start_index (inclusive) to end_index (exclusive).
        Properly generates updated or non-updated rows.
        """
        rows = dict()
        for row_num in xrange(max(start_index, self._deleted_key_count), end_index, step):
            if row_num < self._updated_key_count:
                is_update = True
                shift_by_one = 1
            else:
                is_update = False
                shift_by_one = 0
            cols = dict((
                    self._generate_col_name(i),
                    self._generate_col_value(i, is_update))
                    for i in xrange(shift_by_one, self._params['num_cols']+shift_by_one))

            rows[self._generate_row_key(row_num)] = cols

        return rows


    def _generate_row_key(self, num):
        return self._convert(prefix='row_', num=num)


    def _generate_col_name(self, num):
        return self._convert(prefix='col_', num=num)


    def _generate_col_value(self, num, is_update=False):
        # is_update means that the value should be modified to indicate it has been updated.
        prefix = 'val_'
        if is_update:
            num += 10000
            prefix = 'val_updated_'
        return self._convert(prefix=prefix, num=num)


    def _convert(self, prefix=None, num=None):
        return str(prefix + str(num))


    def create_keyspace(self, cursor):
        keyspace_name = self._params['keyspace_name']
        cql_str = ("CREATE KEYSPACE %s WITH strategy_class=SimpleStrategy AND "
                "strategy_options:replication_factor=%d" % (keyspace_name, int(self._params['replication_factor'])))
        try:
            self.execute_query(cql_str)
        except cql.ProgrammingError, e:
            # the ks already exists
            pass


    def create_column_family(self):
        """ The columnfamily should not already exist! """
        cf_name = self._params['column_family_name']
        if self._params['is_counter']:
            cql_str = """
            CREATE COLUMNFAMILY %s (KEY text PRIMARY KEY) WITH default_validation=counter""" % cf_name
            self.execute_query(cql_str)
        else:
            cql_str = """
            CREATE COLUMNFAMILY %s (KEY %s PRIMARY KEY) WITH comparator=%s
            AND default_validation=%s""" % (cf_name, 
                self._params['key_validation_type'],
                self._params['comparator_type'],
                self._params['validation_type'])
            self.execute_query(cql_str)

    
    def execute_query(self, cql_str, num_retries=10):
        """
        execute the query, and retry several times if needed.
        """
        debug(cql_str)
        for try_num in xrange(num_retries+1):
            try:
                self._cursor.execute(cql_str)
            except cql.ProgrammingError:
                raise
            except Exception, e:
                if try_num == num_retries:
                    raise
                # try to reconnect
                time.sleep(1)
                self.refresh_connection()
            else:
                break



class ContinuousLoader(threading.Thread):
    """
    Hits the db continuously with LoadMaker. Can handle standard and 
    counter columnfamilies

    Applies each type of load in a round-robin fashion
    """
    def __init__(self, load_makers=[], sleep_between=1):
        """
        load_makers is a list of load_makers to run

        sleep_between will slow down loading of the cluster
        by sleeping between every insert operation this many seconds.
        """
        self._load_makers = load_makers
        self._sleep_between = sleep_between
        self._inserting_lock = threading.Lock()
        self._is_loading = True
        self._should_exit = False
        super(ContinuousLoader, self).__init__()
        self.setDaemon(True)
        self.exception = None

        # make sure each loader gets called at least once.
        debug("calling ContinuousLoader()._generate_load_once() from __init__().")
        self._generate_load_once()

        # now fire up the loaders to continuously load the system.
        self.start()


    def run(self):
        """
        applies load whenever it isn't paused.
        """
        debug("Loadmaker started")
        while True:
            if self._should_exit:
                break
            self._generate_load_once()
        debug("continuous loader exiting.")


    def _generate_load_once(self):
        """
        runs one round of load with all the load_makers.
        """
        debug("ContinuousLoader()._generate_load_once() starting")
        for load_maker in self._load_makers:
            self._inserting_lock.acquire()

            # exit if needed.
            if self._should_exit:
                return

            try:
                load_maker.generate(num_keys=3)
            except Exception, e:
                # if anything goes wrong, store the exception
                e.args = e.args + (str(load_maker), )
                self.exception = (e, sys.exc_info()[2])
                raise
            finally:
                self._inserting_lock.release()
            if self._sleep_between:
                time.sleep(self._sleep_between)
        debug("ContinuousLoader()._generate_load_once() done.")


    def exit(self):
        self._should_exit = True
        if self._is_loading == False:
            self.unpause()


    def check_exc(self):
        """
        checks to see if anything has gone wrong inserting data, and bails
        out if it has.
        """
        if self.exception:
            raise self.exception[0], None, self.exception[1]


    def read_and_validate(self, step=100, pause_before_validate=3):
        """
        reads back all the data that has been inserted.
        Pauses loading while validating. Cannot already be paused.
        """
        debug("read_and_validate()")
        self.check_exc()
        self.pause()
        debug("Sleeping %.2f seconds.." % pause_before_validate)
        time.sleep(pause_before_validate)
        for load_maker in self._load_makers:
            try:
                load_maker.validate(step=step)
            except Exception, e:
                # put the loader into the exception to make life easier
                e.args = e.args + (str(load_maker), )
                raise
        self.unpause()


    def pause(self):
        """
        acquires the _inserting_lock to stop the loading from happening.
        """
        assert self._is_loading == True, "Called Pause while not loading!"
        self._inserting_lock.acquire()
        debug("paused continuousloader...")
        self._is_loading = False


    def unpause(self):
        """
        releases the _inserting_lock to resume loading.
        """
        assert self._is_loading == False, "Called Pause while loading!"
        debug("unpausing continuousloader...")
        self._inserting_lock.release()
        self._is_loading = True




########NEW FILE########
__FILENAME__ = loadmaker_test
import time

import loadmaker

from dtest import Tester

class TestLoadmaker(Tester):

    def loadmaker_test(self):
        cluster = self.cluster
        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        time.sleep(.2)
        host, port = node1.network_interfaces['thrift']

        lm = loadmaker.LoadMaker(host, port, column_family_name='cf_standard',
                consistency_level='ONE')
        lm.generate(500)
        lm.validate()
        lm.update(100)
        lm.validate()
        lm.delete(10)
        lm.validate()

        lm = loadmaker.LoadMaker(host, port, column_family_name='cf_counter',
                is_counter=True,
                consistency_level='ONE')
        lm.generate(200)
        lm.validate()


        lm1 = loadmaker.LoadMaker(host, port, column_family_name='cf_standard2',
                consistency_level='ONE')
        lm2 = loadmaker.LoadMaker(host, port, column_family_name='cf_counter2',
                is_counter=True,
                consistency_level='ONE')

        cont_loader = loadmaker.ContinuousLoader([lm1, lm2])
        time.sleep(10)
        cont_loader.read_and_validate()

        cont_loader.exit()

########NEW FILE########
__FILENAME__ = multidc_putget_test
from dtest import Tester
from tools import putget
from ccmlib.cluster import Cluster

class TestMultiDCPutGet(Tester):

    def putget_2dc_rf1_test(self):
        """ Simple put-get test for 2 DC with one node each (RF=1) [catches #3539] """
        cluster = self.cluster
        cluster.populate([1, 1]).start()

        cursor = self.patient_cql_connection(cluster.nodelist()[0]).cursor()
        self.create_ks(cursor, 'ks', { 'dc1' : 1, 'dc2' : 1})
        self.create_cf(cursor, 'cf')

        putget(cluster, cursor)

    def putget_2dc_rf2_test(self):
        """ Simple put-get test for 2 DC with 2 node each (RF=2) -- tests cross-DC efficient writes """
        cluster = self.cluster
        cluster.populate([2, 2]).start()

        cursor = self.patient_cql_connection(cluster.nodelist()[0]).cursor()
        self.create_ks(cursor, 'ks', { 'dc1' : 2, 'dc2' : 2})
        self.create_cf(cursor, 'cf')

        putget(cluster, cursor)

########NEW FILE########
__FILENAME__ = paxos_tests
# coding: utf-8

from dtest import Tester
from assertions import *
from cql import ProgrammingError, OperationalError
from tools import *

import time
from threading import Thread
from ccmlib.cluster import Cluster

class TestPaxos(Tester):

    def prepare(self, ordered=False, create_keyspace=True, use_cache=False, nodes=1, rf=1):
        cluster = self.cluster

        if (ordered):
            cluster.set_partitioner("org.apache.cassandra.dht.ByteOrderedPartitioner")

        if (use_cache):
            cluster.set_configuration_options(values={ 'row_cache_size_in_mb' : 100 })

        cluster.populate(nodes).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.cql_connection(node1, version="3.0.0").cursor()
        if create_keyspace:
            self.create_ks(cursor, 'ks', rf)
        return cursor

    @since('2.0.6')
    def contention_test_multi_iterations(self):
        self._contention_test(8, 100)

    @since('2.0.6')
    def contention_test_many_threds(self):
        self._contention_test(300, 1)

    def _contention_test(self, threads, iterations):
        """ Test threads repeatedly contending on the same row """

        verbose = False

        cursor = self.prepare()
        cursor.execute("CREATE TABLE test (k int, v int static, id int, PRIMARY KEY (k, id))")
        cursor.execute("INSERT INTO test(k, v) VALUES (0, 0)");

        class Worker(Thread):
            def __init__(self, wid, cursor, iterations, query):
                Thread.__init__(self)
                self.wid = wid
                self.iterations = iterations
                self.query = query
                self.cursor = cursor
                self.errors = 0
                self.retries = 0

            def run(self):
                global worker_done
                i = 0
                prev = 0
                while i < self.iterations:
                    done = False
                    while not done:
                        try:
                            self.cursor.execute_prepared(self.query, { 'new_value' : prev+1, 'prev_value' : prev, 'worker_id' : self.wid })
                            res = self.cursor.fetchall()
                            if verbose:
                                print "[%3d] CAS %3d -> %3d (res: %s)" % (self.wid, prev, prev+1, str(res))
                            if res[0][0] is True:
                                done = True
                                prev = prev + 1
                            else:
                                self.retries = self.retries + 1
                                # There is 2 conditions, so 2 reasons to fail: if we failed because the row with our
                                # worker ID already exists, it means we timeout earlier but our update did went in,
                                # so do consider this as a success
                                prev = res[0][3]
                                if res[0][2] is not None:
                                    if verbose:
                                        print "[%3d] Update was inserted on previous try (res = %s)" % (self.wid, str(res))
                                    done = True
                        except OperationalError as e:
                            if verbose:
                                print "[%3d] TIMEOUT (%s)" % (self.wid, str(e))
                            # This means a timeout: just retry, if it happens that our update was indeed persisted,
                            # we'll figure it out on the next run.
                            self.retries = self.retries + 1
                        except Exception as e:
                            if verbose:
                                print "[%3d] ERROR: %s" % (self.wid, str(e))
                            self.errors = self.errors + 1
                            done = True
                    i = i + 1
                    # Clean up for next iteration
                    while True:
                        try:
                            self.cursor.execute("DELETE FROM test WHERE k = 0 AND id = %d IF EXISTS" % self.wid)
                            break;
                        except OperationalError as e:
                            # Timeout, just retry
                            pass


        nodes = self.cluster.nodelist()
        workers = []
        for n in range(0, threads):
            c = self.cql_connection(nodes[n % len(nodes)], version="3.0.0").cursor()
            # This is overkill, we prepare the query multiple time per-host, but not a huge deal
            c.execute("USE ks")
            q = c.prepare_query("""
                BEGIN BATCH
                   UPDATE test SET v = :new_value WHERE k = 0 IF v = :prev_value;
                   INSERT INTO test (k, id) VALUES (0, :worker_id) IF NOT EXISTS;
                APPLY BATCH
            """)
            workers.append(Worker(n, c, iterations, q))

        start = time.time()

        for w in workers:
            w.start()

        for w in workers:
            w.join()

        if verbose:
            runtime = time.time() - start
            print "runtime:", runtime

        cursor.execute("SELECT v FROM test WHERE k = 0", consistency_level='ALL')
        value = cursor.fetchall()[0][0]

        errors = 0
        retries = 0
        for w in workers:
            errors = errors + w.errors
            retries = retries + w.retries

        assert (value == threads * iterations) and (errors == 0), "value=%d, errors=%d, retries=%d" % (value, errors, retries)


########NEW FILE########
__FILENAME__ = putget_test
from dtest import Tester
import tools
from tools import no_vnodes, create_c1c2_table, ThriftConnection

import time

class TestPutGet(Tester):

    def putget_test(self):
        """ Simple put/get on a single row, hitting multiple sstables """
        self._putget()

    def putget_snappy_test(self):
        """ Simple put/get on a single row, but hitting multiple sstables (with snappy compression) """
        self._putget(compression="Snappy")

    def putget_deflate_test(self):
        """ Simple put/get on a single row, but hitting multiple sstables (with deflate compression) """
        self._putget(compression="Deflate")

    # Simple queries, but with flushes in between inserts to make sure we hit
    # sstables (and more than one) on reads
    def _putget(self, compression=None):
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', compression=compression)

        tools.putget(cluster, cursor)

    def non_local_read_test(self):
        """ This test reads from a coordinator we know has no copy of the data """
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 2)
        create_c1c2_table(self, cursor)

        # insert and get at CL.QUORUM (since RF=2, node1 won't have all key locally)
        for n in xrange(0, 1000):
            tools.insert_c1c2(cursor, n, "QUORUM")
            tools.query_c1c2(cursor, n, "QUORUM")

    def rangeputget_test(self):
        """ Simple put/get on ranges of rows, hitting multiple sstables """

        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 2)
        self.create_cf(cursor, 'cf')

        tools.range_putget(cluster, cursor)

    def wide_row_test(self):
        """ Test wide row slices """
        cluster = self.cluster

        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf')

        key = 'wide'

        for x in xrange(1, 5001):
            tools.insert_columns(self, cursor, key, 100, offset=x-1)

        for size in (10, 100, 1000):
            for x in xrange(1, (50001 - size) / size):
                tools.query_columns(self, cursor, key, size, offset=x*size-1)

    @no_vnodes()
    def wide_slice_test(self):
        """ 
        Check slicing a wide row. 
        See https://issues.apache.org/jira/browse/CASSANDRA-4919 

        From Sylvain about duplicating:

        Ok, so now that I think about it, you can't reproduce that with CQL currently.
        You'll have to use the thrift get_paged_slice call as it's the only way to
        trigger this.

        Then, I think you'll be able to reproduce with the following steps:
        1) you'd want to use 2 nodes with RF=1 and with ByteOrderedPartitioner (it's
        possible to reproduce with a random partitioner but a tad more painful)
        2) picks token for the nodes so that you know what goes on which node. For
        example you may want that any row key starting with 'a' goes on node1, and
        anything starting with a 'b' goes on node 2.
        3) insers data that span the two nodes. Say inserts 20 rows 'a0' ... 'a9' and
        'b0' ...'b9' (so 10 rows on each node) with say 10 columns on row.
        4) then do a get_paged_slice for keys 'a5' to 'b4' and for the column filter, a
        slice filter that picks the fifth last columns.
        5) the get_paged_slice is supposed to return 95 columns (it should return the 5
        last columns of a5 and then all 10 columns for 'a6' to 'b4'), but without
        CASSANDRA-4919 it will return 90 columns only (it will only return the 5 last
        columns of 'b0').
        """
        cluster = self.cluster
        cluster.set_configuration_options(values={'partitioner': 'org.apache.cassandra.dht.ByteOrderedPartitioner'})
        cluster.populate(2)
        [node1, node2] = cluster.nodelist()
        node1.set_configuration_options(values={'initial_token': "a".encode('hex')  })
        node1.set_configuration_options(values={'initial_token': "b".encode('hex')  })
        cluster.start()
        time.sleep(.5)
        cursor = self.patient_cql_connection(node1, version="2.0.0").cursor()
        self.create_ks(cursor, 'ks', 1)

        query = """
            CREATE TABLE test (
                k text PRIMARY KEY
            );
        """
        cursor.execute(query)
        time.sleep(.5)

        for i in xrange(10):
            key_num = str(i).zfill(2)
            query1 = "INSERT INTO test (k, 'col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'col9') VALUES ('a%s', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)" % (key_num)
            query2 = "INSERT INTO test (k, 'col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'col9') VALUES ('b%s', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)" % (key_num)
            cursor.execute(query1)
            cursor.execute(query2)

        cursor.close()

        tc = ThriftConnection(node1, ks_name='ks', cf_name='test')
        tc.use_ks()

        # Slice on the keys
        rnge = tc.Cassandra.KeyRange(
            start_key="a%s" % ('5'.zfill(2)),
            end_key="b%s" % ('4'.zfill(2)),
            count=9999,
        )
        rows = tc.client.get_paged_slice(
            column_family='test',
            range=rnge,
            start_column='col5',
            consistency_level=tc.Cassandra.ConsistencyLevel.ONE,
        )
        keys = [fd.key for fd in rows]
        columns = []
        for row in rows:
            cols = [col.column.name for col in row.columns]
            columns.extend(cols)
            #print row.key
            #print cols
        
        assert len(columns) == 95, "Regression in cassandra-4919. Expected 95 columns, got %d." % len(columns)

########NEW FILE########
__FILENAME__ = range_ghost_test
from dtest import Tester
from tools import *
from assertions import *

import os, sys, time
from ccmlib.cluster import Cluster

class TestRangeGhosts(Tester):

    def ghosts_test(self):
        """ Check range ghost are correctly removed by the system """
        cluster = self.cluster
        cluster.populate(1).start()
        [node1] = cluster.nodelist()

        time.sleep(.5)
        cursor = self.cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', gc_grace=0, columns={'c': 'text'})

        rows = 1000

        for i in xrange(0, rows):
            cursor.execute("UPDATE cf SET c = 'value' WHERE key = 'k%i'" % i)

        cursor.execute("SELECT * FROM cf LIMIT 10000")
        res = cursor.fetchall()
        assert len(res) == rows, res

        node1.flush()

        for i in xrange(0, rows/2):
            cursor.execute("DELETE FROM cf WHERE key = 'k%i'" % i)

        cursor.execute("SELECT * FROM cf LIMIT 10000")
        res = cursor.fetchall()
        # no ghosts in 1.2+
        if cluster.version() >= '1.2':
            assert len(res) == rows/2, len(res)
        else:
            assert len(res) == rows, len(res)

        node1.flush()
        time.sleep(1) # make sure tombstones are collected
        node1.compact()

        cursor.execute("SELECT * FROM cf LIMIT 10000")
        res = cursor.fetchall()
        assert len(res) == rows/2, len(res)

########NEW FILE########
__FILENAME__ = repair_test
import time, re
from dtest import Tester, debug
from tools import *

class TestRepair(Tester):

    def check_rows_on_node(self, node_to_check, rows, found=[], missings=[], restart=True):
        stopped_nodes = []
        for node in self.cluster.nodes.values():
            if node.is_running() and node is not node_to_check:
                stopped_nodes.append(node)
                node.stop(wait_other_notice=True)

        cursor = self.patient_cql_connection(node_to_check, 'ks').cursor()
        cursor.execute("SELECT * FROM cf LIMIT %d" % (rows * 2))
        assert cursor.rowcount == rows, cursor.rowcount

        for k in found:
            query_c1c2(cursor, k, "ONE")

        for k in missings:
            if self.cluster.version() >= '1.2':
                cursor.execute("SELECT c1, c2 FROM cf WHERE key='k%d'" % k, consistency_level='ONE')
            else:
                cursor.execute('SELECT c1, c2 FROM cf USING CONSISTENCY ONE WHERE key=k%d' % k)
            res = cursor.fetchall()
            assert len(filter(lambda x: len(x) != 0, res)) == 0, res

        cursor.close()

        if restart:
            for node in stopped_nodes:
                node.start(wait_other_notice=True)

    def simple_repair_test(self, ):
        self._simple_repair()

    @no_vnodes()  # https://issues.apache.org/jira/browse/CASSANDRA-5220
    def simple_repair_order_preserving_test(self, ):
        self._simple_repair(order_preserving_partitioner=True)

    def _simple_repair(self, order_preserving_partitioner=False):
        cluster = self.cluster

        if order_preserving_partitioner:
            cluster.set_partitioner('org.apache.cassandra.dht.ByteOrderedPartitioner')

        # Disable hinted handoff and set batch commit log so this doesn't
        # interfer with the test (this must be after the populate)
        cluster.set_configuration_options(values={ 'hinted_handoff_enabled' : False}, batch_commitlog=True)
        debug("Starting cluster..")
        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', read_repair=0.0, columns={'c1': 'text', 'c2': 'text'})

        # Insert 1000 keys, kill node 3, insert 1 key, restart node 3, insert 1000 more keys
        debug("Inserting data...")
        for i in xrange(0, 1000):
            insert_c1c2(cursor, i, "ALL")
        node3.flush()
        node3.stop()
        insert_c1c2(cursor, 1000, "TWO")
        node3.start(wait_other_notice=True)
        for i in xrange(1001, 2001):
            insert_c1c2(cursor, i, "ALL")
        cursor.close()

        cluster.flush()

        # Verify that node3 has only 2000 keys
        debug("Checking data on node3...")
        self.check_rows_on_node(node3, 2000, missings=[1000])


        # Verify that node1 has 2001 keys
        debug("Checking data on node1...")
        self.check_rows_on_node(node1, 2001, found=[1000])

        # Verify that node2 has 2001 keys
        debug("Checking data on node2...")
        self.check_rows_on_node(node2, 2001, found=[1000])

        time.sleep(10) # see CASSANDRA-4373
        # Run repair
        start = time.time()
        debug("starting repair...")
        node1.repair()
        debug("Repair time: {end}".format(end=time.time() - start))

        # Validate that only one range was transfered
        l = node1.grep_log("/([0-9.]+) and /([0-9.]+) have ([0-9]+) range\(s\) out of sync")
        if cluster.version() > "1":
            assert len(l) == 2, "Lines matching: " + str([elt[0] for elt in l])
        else:
            # In pre-1.0, we should have only one line
            assert len(l) == 1, "Lines matching: " + str([elt[0] for elt in l])
        valid = [(node1.address(), node3.address()), (node3.address(), node1.address()),
                 (node2.address(), node3.address()), (node3.address(), node2.address())]
        for line, m in l:
            assert int(m.group(3)) == 1, "Expecting 1 range out of sync, got " + int(m.group(1))
            assert (m.group(1), m.group(2)) in valid, str((m.group(1), m.group(2)))
            valid.remove((m.group(1), m.group(2)))
            valid.remove((m.group(2), m.group(1)))

        # Check node3 now has the key
        self.check_rows_on_node(node3, 2001, found=[1000], restart=False)


########NEW FILE########
__FILENAME__ = replace_address_test
from dtest import Tester, debug, DISABLE_VNODES
import unittest
from tools import *
from ccmlib.cluster import Cluster
from ccmlib.node import NodeError
import time
from cql import OperationalError
from cql.cassandra.ttypes import UnavailableException

class NodeUnavailable(Exception):
    pass

class TestReplaceAddress(Tester):

    def __init__(self, *args, **kwargs):
        # Ignore these log patterns:
        self.ignore_log_patterns = [
            # This one occurs when trying to send the migration to a
            # node that hasn't started yet, and when it does, it gets
            # replayed and everything is fine.
            r'Can\'t send migration request: node.*is down',
            # This is caused by starting a node improperly (replacing active/nonexistent)
            r'Exception encountered during startup',
            # This is caused by trying to replace a nonexistent node
            r'Exception in thread Thread'
        ]
        Tester.__init__(self, *args, **kwargs)

    def replace_stopped_node_test(self):
        """Check that the replace address function correctly replaces a node that has failed in a cluster. 
        Create a cluster, cause a node to fail, and bring up a new node with the replace_address parameter.
        Check that tokens are migrated and that data is replicated properly.
        """
        debug("Starting cluster with 3 nodes.")
        cluster = self.cluster
        cluster.populate(3).start()
        [node1,node2, node3] = cluster.nodelist()

        debug("Inserting Data...")
        node1.stress(['write', 'n=10000', '-schema', 'replication(factor=3)'])
        cursor = self.patient_cql_connection(node1).cursor()
        cursor.execute('select * from "Keyspace1"."Standard1" LIMIT 1', consistency_level='THREE')
        initialData = cursor.fetchall()
        
        #stop node, query should not work with consistency 3
        debug("Stopping node 3.")
        node3.stop(gently=False)
        time.sleep(5)

        debug("Testing node stoppage (query should fail).")
        with self.assertRaises(NodeUnavailable):
            try:
                cursor.execute('select * from "Keyspace1"."Standard1" LIMIT 1', consistency_level='THREE')
            except (UnavailableException, OperationalError):
                raise NodeUnavailable("Node could not be queried.")

        #replace node 3 with node 4
        debug("Starting node 4 to replace node 3")
        node4 = Node('node4', cluster, True, ('127.0.0.4', 9160), ('127.0.0.4', 7000), '7400', '0', None, ('127.0.0.4',9042))
        cluster.add(node4, False)
        node4.start(replace_address='127.0.0.3')

        #query should work again
        debug("Verifying querying works again.")
        cursor.execute('select * from "Keyspace1"."Standard1" LIMIT 1', consistency_level='THREE')
        finalData = cursor.fetchall()
        self.assertListEqual(initialData, finalData)
        
        debug("Verifying tokens migrated sucessfully")
        movedTokensList = node4.grep_log("Token .* changing ownership from /127.0.0.3 to /127.0.0.4")
        debug(movedTokensList[0])
        if DISABLE_VNODES:
            self.assertEqual(len(movedTokensList), 1)
        else:
            self.assertEqual(len(movedTokensList), 256)

        #check that restarting node 3 doesn't work
        debug("Try to restart node 3 (should fail)")
        node3.start() 
        checkCollision = node1.grep_log("between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner")
        debug(checkCollision)
        self.assertEqual(len(checkCollision), 1)

    def replace_active_node_test(self):

        debug("Starting cluster with 3 nodes.")
        cluster = self.cluster
        cluster.populate(3).start()
        [node1,node2, node3] = cluster.nodelist()

        debug("Inserting Data...")
        node1.stress(['write', 'n=10000', '-schema', 'replication(factor=3)'])
        cursor = self.patient_cql_connection(node1).cursor()
        cursor.execute('select * from "Keyspace1"."Standard1" LIMIT 1', consistency_level='THREE')
        initialData = cursor.fetchall()

        #replace active node 3 with node 4
        debug("Starting node 4 to replace active node 3")
        node4 = Node('node4', cluster, True, ('127.0.0.4', 9160), ('127.0.0.4', 7000), '7400', '0', None, ('127.0.0.4',9042))
        cluster.add(node4, False)
        
        with self.assertRaises(NodeError):
            node4.start(replace_address='127.0.0.3')

        checkError = node4.grep_log("java.lang.UnsupportedOperationException: Cannnot replace a live node...")

        self.assertEqual(len(checkError), 1)

    def replace_nonexistent_node_test(self):
        debug("Starting cluster with 3 nodes.")
        cluster = self.cluster
        cluster.populate(3).start()
        [node1,node2, node3] = cluster.nodelist()

        debug("Inserting Data...")
        node1.stress(['write', 'n=10000', '-schema', 'replication(factor=3)'])
        cursor = self.patient_cql_connection(node1).cursor()
        cursor.execute('select * from "Keyspace1"."Standard1" LIMIT 1', consistency_level='THREE')
        initialData = cursor.fetchall()

        debug('Start node 4 and replace an address with no node')
        node4 = Node('node4', cluster, True, ('127.0.0.4', 9160), ('127.0.0.4', 7000), '7400', '0', None, ('127.0.0.4',9042))
        cluster.add(node4, False)
        
        #try to replace an unassigned ip address
        with self.assertRaises(NodeError):
            node4.start(replace_address='127.0.0.5')

########NEW FILE########
__FILENAME__ = replication_test
from dtest import Tester, debug, TracingCursor, PRINT_DEBUG
from tools import *
from ccmlib.cluster import Cluster
import re
import os
import time
from collections import defaultdict

TRACE_DETERMINE_REPLICAS = re.compile('Determining replicas for mutation')
TRACE_SEND_MESSAGE = re.compile('Sending message to /([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)')
TRACE_RESPOND_MESSAGE = re.compile('Message received from /([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)')
TRACE_COMMIT_LOG = re.compile('Appending to commitlog')
TRACE_FORWARD_WRITE = re.compile('Enqueuing forwarded write to /([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)')

# Some pre-computed murmur 3 hashes; there are no good python murmur3
# hashing libraries :(
murmur3_hashes = {
    5: -7509452495886106294,
    10: -6715243485458697746,
    16: -5477287129830487822,
    13: -5034495173465742853,
    11: -4156302194539278891,
    1: -4069959284402364209,
    19: -3974532302236993209,
    8: -3799847372828181882,
    2: -3248873570005575792,
    4: -2729420104000364805,
    18: -2695747960476065067,
    15: -1191135763843456182,
    20:  1388667306199997068,
    7:  1634052884888577606,
    6:  2705480034054113608,
    9:  3728482343045213994,
    14:  4279681877540623768,
    17:  5467144456125416399,
    12:  8582886034424406875,
    3:  9010454139840013625
}


class ReplicationTest(Tester):
    """This test suite looks at how data is replicated across a cluster
    and who the coordinator, replicas and forwarders involved are.
    """

    def get_replicas_from_trace(self, trace):
        """Look at trace and return a list of the replicas contacted"""
        coordinator = None
        nodes_sent_write = set([]) #Nodes sent a write request
        nodes_responded_write = set([]) #Nodes that acknowledges a write
        replicas_written = set([]) #Nodes that wrote to their commitlog
        forwarders = set([]) #Nodes that forwarded a write to another node
        nodes_contacted = defaultdict(set) #node -> list of nodes that were contacted

        for session, event, activity, source, source_elapsed, thread in trace:
            # Step 1, find coordinator node:
            if activity.startswith('Determining replicas for mutation'):
                if not coordinator:
                    coordinator = source
            if not coordinator:
                continue

            # Step 2, find all the nodes that each node talked to:
            send_match = TRACE_SEND_MESSAGE.search(activity)
            recv_match = TRACE_RESPOND_MESSAGE.search(activity)
            if send_match:
                node_contacted = send_match.groups()[0]
                if source == coordinator:
                    nodes_sent_write.add(node_contacted)
                nodes_contacted[source].add(node_contacted)
            elif recv_match:
                node_contacted = recv_match.groups()[0]
                if source == coordinator:
                    nodes_responded_write.add(recv_match.groups()[0])

            # Step 3, find nodes that forwarded to other nodes:
            # (Happens in multi-datacenter clusters)
            if source != coordinator:
                forward_match = TRACE_FORWARD_WRITE.search(activity)
                if forward_match:
                    forwarding_node = forward_match.groups()[0]
                    nodes_sent_write.add(forwarding_node)
                    forwarders.add(forwarding_node)

            # Step 4, find nodes who actually wrote data:
            if TRACE_COMMIT_LOG.search(activity):
                replicas_written.add(source)

        return {"coordinator": coordinator,
                "forwarders": forwarders,
                "replicas": replicas_written,
                "nodes_sent_write": nodes_sent_write,
                "nodes_responded_write": nodes_responded_write,
                "nodes_contacted": nodes_contacted
        }

    def get_replicas_for_token(self, token, replication_factor,
                               strategy='SimpleStrategy', nodes=None):
        """Figure out which node(s) should receive data for a given token and
        replication factor"""
        if not nodes:
            nodes = self.cluster.nodelist()
        token_ranges = sorted(zip([n.initial_token for n in nodes], nodes))
        replicas = []

        # Find first replica:
        for i, (r, node) in enumerate(token_ranges):
            if token <= r:
                replicas.append(node.address())
                first_ring_position = i
                break
        else:
            replicas.append(token_ranges[0][1].address())
            first_ring_position = 0

        # Find other replicas:
        if strategy == 'SimpleStrategy':
            for node in nodes[first_ring_position+1:]:
                replicas.append(node.address())
                if len(replicas) == replication_factor:
                    break
            if len(replicas) != replication_factor:
                # Replication token range looped:
                for node in nodes:
                    replicas.append(node.address())
                    if len(replicas) == replication_factor:
                        break
        elif strategy == 'NetworkTopologyStrategy':
            # NetworkTopologyStrategy can be broken down into multiple 
            # SimpleStrategies, just once per datacenter:
            for dc,rf in replication_factor.items():
                dc_nodes = [n for n in nodes if n.data_center == dc]
                replicas.extend(self.get_replicas_for_token(
                    token, rf, nodes=dc_nodes))
        else:
            raise NotImplemented('replication strategy not implemented: %s' 
                                 % strategy)

        return replicas
    
    def pprint_trace(self, trace):
        """Pretty print a trace"""
        if PRINT_DEBUG:
            print("-" * 40)
            for t in trace:
                print("%s\t%s\t%s\t%s" % (t[3], t[4], t[2], t[5]))
            print("-" * 40)

    @no_vnodes()
    def simple_test(self):
        """Test the SimpleStrategy on a 3 node cluster"""
        self.cluster.populate(3).start()
        time.sleep(5)
        node1 = self.cluster.nodelist()[0]
        self.conn = self.patient_cql_connection(node1)
        
        # Install a tracing cursor so we can get info about who the
        # coordinator is contacting: 
        self.conn.cursorclass = TracingCursor
        cursor = self.conn.cursor()

        replication_factor = 3
        self.create_ks(cursor, 'test', replication_factor)
        cursor.execute('CREATE TABLE test.test (id int PRIMARY KEY, value text)', trace=False)
        # Wait for table creation, otherwise trace times out -
        # CASSANDRA-5658
        time.sleep(5)

        for key, token in murmur3_hashes.items():
            cursor.execute("INSERT INTO test (id, value) VALUES (%s, 'asdf')" % key)
            time.sleep(5)
            trace = cursor.get_last_trace()
            stats = self.get_replicas_from_trace(trace)
            replicas_should_be = set(self.get_replicas_for_token(
                token, replication_factor))
            debug('\nreplicas should be: %s' % replicas_should_be)
            debug('replicas were: %s' % stats['replicas'])
            self.pprint_trace(trace)

            #Make sure the correct nodes are replicas:
            self.assertEqual(stats['replicas'], replicas_should_be)
            #Make sure that each replica node was contacted and
            #acknowledged the write:
            self.assertEqual(stats['nodes_sent_write'], stats['nodes_responded_write'])

    @no_vnodes()
    def network_topology_test(self):
        """Test the NetworkTopologyStrategy on a 2DC 3:3 node cluster"""
        self.cluster.populate([3,3]).start()
        time.sleep(5)
        node1 = self.cluster.nodelist()[0]
        ip_nodes = dict((node.address(), node) for node in self.cluster.nodelist())
        self.conn = self.patient_cql_connection(node1)
        
        # Install a tracing cursor so we can get info about who the
        # coordinator is contacting: 
        self.conn.cursorclass = TracingCursor
        cursor = self.conn.cursor()

        replication_factor = {'dc1':2, 'dc2':2}
        self.create_ks(cursor, 'test', replication_factor)
        cursor.execute('CREATE TABLE test.test (id int PRIMARY KEY, value text)', trace=False)
        # Wait for table creation, otherwise trace times out -
        # CASSANDRA-5658
        time.sleep(5)

        forwarders_used = set()

        for key, token in murmur3_hashes.items():
            cursor.execute("INSERT INTO test (id, value) VALUES (%s, 'asdf')" % key)
            time.sleep(5)
            trace = cursor.get_last_trace()
            stats = self.get_replicas_from_trace(trace)
            replicas_should_be = set(self.get_replicas_for_token(
                token, replication_factor, strategy='NetworkTopologyStrategy'))
            debug('\nreplicas should be: %s' % replicas_should_be)
            debug('replicas were: %s' % stats['replicas'])
            self.pprint_trace(trace)

            #Make sure the coordinator only talked to a single node in
            #the second datacenter - CASSANDRA-5632:
            num_in_other_dcs_contacted = 0
            for node_contacted in stats['nodes_contacted'][node1.address()]:
                if ip_nodes[node_contacted].data_center != node1.data_center:
                    num_in_other_dcs_contacted += 1
            self.assertEqual(num_in_other_dcs_contacted, 1)

            # Record the forwarder used for each INSERT:
            forwarders_used = forwarders_used.union(stats['forwarders'])

            #Make sure the correct nodes are replicas:
            self.assertEqual(stats['replicas'], replicas_should_be)
            #Make sure that each replica node was contacted and
            #acknowledged the write:
            self.assertEqual(stats['nodes_sent_write'], stats['nodes_responded_write'])
            
        #Given a diverse enough keyset, each node in the second
        #datacenter should get a chance to be a forwarder:
        self.assertEqual(len(forwarders_used), 3)

########NEW FILE########
__FILENAME__ = schema_test
from cql import ProgrammingError
from dtest import Tester
from tools import *
import time

class TestSchema(Tester):
    @since('2.0')
    def drop_column_compact_test(self):
        cursor = self.prepare()

        cursor.execute("USE ks")
        cursor.execute("CREATE TABLE cf (key int PRIMARY KEY, c1 int, c2 int) WITH COMPACT STORAGE")

        with self.assertRaises(ProgrammingError) as cm:
            cursor.execute("ALTER TABLE cf DROP c1")
        self.assertTrue(cm.exception.message.startswith("Bad Request: Cannot drop columns from a"))

    @since('2.0')
    def drop_column_compaction_test(self):
        cursor = self.prepare()
        cursor.execute("USE ks")
        cursor.execute("CREATE TABLE cf (key int PRIMARY KEY, c1 int, c2 int)")

        # insert some data.
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (0, 1, 2)")
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (1, 2, 3)")
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (2, 3, 4)")

        # drop and readd c1.
        cursor.execute("ALTER TABLE cf DROP c1")
        cursor.execute("ALTER TABLE cf ADD c1 int")

        # add another row.
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (3, 4, 5)")

        node = self.cluster.nodelist()[0]
        node.flush()
        node.compact()

        # erase info on dropped 'c1' column and restart.
        cursor.execute("""UPDATE system.schema_columnfamilies
                          SET dropped_columns = null
                          WHERE keyspace_name = 'ks' AND columnfamily_name = 'cf'""")
        node.stop(gently=False)
        node.start()
        time.sleep(.5)

        # test that c1 values have been compacted away.
        cursor = self.cql_connection(node, version='3.0.10').cursor()
        cursor.execute("SELECT c1 FROM ks.cf")
        self.assertEqual([[None], [None], [None], [4]], sorted(cursor.fetchall()))

    @since('2.0')
    def drop_column_queries_test(self):
        cursor = self.prepare()

        cursor.execute("USE ks")
        cursor.execute("CREATE TABLE cf (key int PRIMARY KEY, c1 int, c2 int)")
        cursor.execute("CREATE INDEX ON cf(c2)")

        # insert some data.
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (0, 1, 2)")
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (1, 2, 3)")
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (2, 3, 4)")

        # drop and readd c1.
        cursor.execute("ALTER TABLE cf DROP c1")
        cursor.execute("ALTER TABLE cf ADD c1 int")

        # add another row.
        cursor.execute("INSERT INTO cf (key, c1, c2) VALUES (3, 4, 5)")

        # test that old (pre-drop) c1 values aren't returned and new ones are.
        cursor.execute("SELECT c1 FROM cf")
        self.assertEqual([[None], [None], [None], [4]], sorted(cursor.fetchall()))

        cursor.execute("SELECT * FROM cf")
        self.assertEqual([[0,None,2], [1,None,3], [2,None,4], [3,4,5]], sorted(cursor.fetchall()))

        cursor.execute("SELECT c1 FROM cf WHERE key = 0")
        self.assertEqual([None], cursor.fetchone())

        cursor.execute("SELECT c1 FROM cf WHERE key = 3")
        self.assertEqual([4], cursor.fetchone())

        cursor.execute("SELECT * FROM cf WHERE c2 = 2")
        self.assertEqual([0,None,2], cursor.fetchone())

        cursor.execute("SELECT * FROM cf WHERE c2 = 5")
        self.assertEqual([3,4,5], cursor.fetchone())

    def prepare(self):
        cluster = self.cluster
        cluster.populate(1).start()
        time.sleep(.5)
        nodes = cluster.nodelist()
        cursor = self.cql_connection(nodes[0], version='3.0.10').cursor()
        self.create_ks(cursor, 'ks', 1)
        return cursor

########NEW FILE########
__FILENAME__ = secondary_indexes_test
import random
import time
import uuid

from cql import ProgrammingError
from dtest import Tester, TracingCursor, debug
from tools import since


class TestSecondaryIndexes(Tester):

    def bug3367_test(self):
        cluster = self.cluster
        cluster.populate(1).start()
        [node1] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)

        columns = {"password": "varchar", "gender": "varchar", "session_token": "varchar", "state": "varchar", "birth_year": "bigint"}
        self.create_cf(cursor, 'users', columns=columns)

        # insert data
        cursor.execute("INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', 1968);")
        cursor.execute("INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3b', 'm', 'CA', 1971);")

        # create index
        cursor.execute("CREATE INDEX gender_key ON users (gender);")
        cursor.execute("CREATE INDEX state_key ON users (state);")
        cursor.execute("CREATE INDEX birth_year_key ON users (birth_year);")

        # insert data
        cursor.execute("INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3c', 'f', 'FL', 1978);")
        cursor.execute("INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user4', 'ch@ngem3d', 'm', 'TX', 1974);")

        cursor.execute("SELECT * FROM users;")
        result = cursor.fetchall()
        assert len(result) == 4, "Expecting 4 users, got" + str(result)

        cursor.execute("SELECT * FROM users WHERE state='TX';")
        result = cursor.fetchall()
        assert len(result) == 2, "Expecting 2 users, got" + str(result)

        cursor.execute("SELECT * FROM users WHERE state='CA';")
        result = cursor.fetchall()
        assert len(result) == 1, "Expecting 1 users, got" + str(result)

    @since('2.1')
    def test_low_cardinality_indexes(self):
        """
        Checks that low-cardinality secondary index subqueries are executed
        concurrently
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()

        conn = self.patient_cql_connection(node1, version='3.0.0')
        cursor = conn.cursor()
        cursor.execute("CREATE KEYSPACE ks WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': '1'};")
        cursor.execute("CREATE TABLE ks.cf (a text PRIMARY KEY, b text);")
        cursor.execute("CREATE INDEX b_index ON ks.cf (b);")
        num_rows = 100
        for i in range(num_rows):
            indexed_value = i % (num_rows / 3)
            # use the same indexed value three times
            cursor.execute("INSERT INTO ks.cf (a, b) VALUES ('%d', '%d');" % (i, indexed_value))

        cluster.flush()

        conn.cursorclass = TracingCursor
        cursor = conn.cursor()

        def check_request_order():
            # we should see multiple requests get enqueued prior to index scan
            # execution happening
            trace = cursor.get_last_trace()
            relevant_events = [(desc, ip) for (_, _, desc, ip, _, _) in trace
                               if 'Enqueuing request' in desc or
                               ('Executing indexed scan' in desc and ip == node1.address())]

            self.assertTrue('Enqueuing' in relevant_events[0][0], str(relevant_events[0]))
            self.assertTrue('Enqueuing' in relevant_events[1][0], str(relevant_events[0]))
            self.assertTrue('Executing indexed scan' in relevant_events[-1][0], str(relevant_events[-1]))

        cursor.execute("SELECT * FROM ks.cf WHERE b='1';")
        result = cursor.fetchall()
        self.assertEqual(3, len(result))
        check_request_order()

        cursor.execute("SELECT * FROM ks.cf WHERE b='1' LIMIT 100;")
        result = cursor.fetchall()
        self.assertEqual(3, len(result))
        check_request_order()

        cursor.execute("SELECT * FROM ks.cf WHERE b='1' LIMIT 3;")
        result = cursor.fetchall()
        self.assertEqual(3, len(result))
        check_request_order()

        for limit in (1, 2):
            cursor.execute("SELECT * FROM ks.cf WHERE b='1' LIMIT %d;" % (limit,))
            result = cursor.fetchall()
            self.assertEqual(limit, len(result))

    @since('2.1')
    def test_6924_dropping_ks(self):
        """Tests CASSANDRA-6924
        
        Data inserted immediately after dropping and recreating a
        keyspace with an indexed column familiy is not included
        in the index.
        """
                # Reproducing requires at least 3 nodes:
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        conn = self.patient_cql_connection(node1)
        cursor = conn.cursor()

        #This only occurs when dropping and recreating with
        #the same name, so loop through this test a few times:
        for i in range(10):
            debug("round %s" % i)
            try:
                cursor.execute("DROP KEYSPACE ks")
            except ProgrammingError:
                pass

            self.create_ks(cursor, 'ks', 1)
            cursor.execute("CREATE TABLE ks.cf (key text PRIMARY KEY, col1 text);")
            cursor.execute("CREATE INDEX on ks.cf (col1);")

            for r in range(10):
                stmt = "INSERT INTO ks.cf (key, col1) VALUES ('%s','asdf');" % r
                cursor.execute(stmt)

            self.wait_for_schema_agreement(cursor)

            cursor.execute("select count(*) from ks.cf WHERE col1='asdf'")
            count = cursor.fetchone()[0]
            self.assertEqual(count, 10)

    @since('2.1')
    def test_6924_dropping_cf(self):
        """Tests CASSANDRA-6924

        Data inserted immediately after dropping and recreating an
        indexed column family is not included in the index.
        """
        # Reproducing requires at least 3 nodes:
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        conn = self.patient_cql_connection(node1)
        cursor = conn.cursor()
        self.create_ks(cursor, 'ks', 1)

        #This only occurs when dropping and recreating with
        #the same name, so loop through this test a few times:
        for i in range(10):
            debug("round %s" % i)
            try:
                cursor.execute("DROP COLUMNFAMILY ks.cf")
            except ProgrammingError:
                pass

            cursor.execute("CREATE TABLE ks.cf (key text PRIMARY KEY, col1 text);")
            cursor.execute("CREATE INDEX on ks.cf (col1);")

            for r in range(10):
                stmt = "INSERT INTO ks.cf (key, col1) VALUES ('%s','asdf');" % r
                cursor.execute(stmt)

            self.wait_for_schema_agreement(cursor)

            cursor.execute("select count(*) from ks.cf WHERE col1='asdf'")
            count = cursor.fetchone()[0]
            self.assertEqual(count, 10)

    def wait_for_schema_agreement(self, cursor):
        cursor.execute("SELECT schema_version FROM system.local")
        local_version = cursor.fetchone()

        all_match = True
        cursor.execute("SELECT schema_version FROM system.peers")
        for peer_version in cursor.fetchall():
            if peer_version != local_version:
                all_match = False
                break

        if all_match:
            return
        else:
            time.sleep(0.10)
            self.wait_for_schema_agreement(cursor)


class TestSecondaryIndexesOnCollections(Tester):
    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

    @since('2.1')
    def test_list_indexes(self):
        """
        Checks that secondary indexes on lists work for querying.
        """
        cluster = self.cluster
        cluster.populate(1).start()
        [node1] = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'list_index_search', 1)

        stmt = ("CREATE TABLE list_index_search.users ("
               "user_id uuid PRIMARY KEY,"
               "email text,"
               "uuids list<uuid>"
              ");")
        cursor.execute(stmt)

        # no index present yet, make sure there's an error trying to query column
        stmt = ("SELECT * from list_index_search.users where uuids contains {some_uuid}"
            ).format(some_uuid=uuid.uuid4())
        with self.assertRaisesRegexp(ProgrammingError, 'No indexed columns present in by-columns clause'):
            cursor.execute(stmt)

        # add index and query again (even though there are no rows in the table yet)
        stmt = "CREATE INDEX user_uuids on list_index_search.users (uuids);"
        cursor.execute(stmt)

        stmt = ("SELECT * from list_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        # add a row which doesn't specify data for the indexed column, and query again
        user1_uuid = uuid.uuid4()
        stmt = ("INSERT INTO list_index_search.users (user_id, email)"
              "values ({user_id}, 'test@example.com')"
            ).format(user_id=user1_uuid)
        cursor.execute(stmt)

        time.sleep(5)
        stmt = ("SELECT * from list_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        _id = uuid.uuid4()
        # alter the row to add a single item to the indexed list
        stmt = ("UPDATE list_index_search.users set uuids = [{id}] where user_id = {user_id}"
            ).format(id=_id, user_id=user1_uuid)
        cursor.execute(stmt)

        stmt = ("SELECT * from list_index_search.users where uuids contains {some_uuid}").format(some_uuid=_id)
        cursor.execute(stmt)
        self.assertEqual(1, cursor.rowcount)

        # add a bunch of user records and query them back
        shared_uuid = uuid.uuid4() # this uuid will be on all records

        log = []

        for i in range(50000):
            user_uuid = uuid.uuid4()
            unshared_uuid = uuid.uuid4()

            # give each record a unique email address using the int index
            stmt = ("INSERT INTO list_index_search.users (user_id, email, uuids)"
                  "values ({user_uuid}, '{prefix}@example.com', [{s_uuid}, {u_uuid}])"
               ).format(user_uuid=user_uuid, prefix=i, s_uuid=shared_uuid, u_uuid=unshared_uuid)
            cursor.execute(stmt)

            log.append(
                {'user_id': user_uuid,
                 'email':str(i)+'@example.com',
                 'unshared_uuid':unshared_uuid}
            )

        # confirm there is now 50k rows with the 'shared' uuid above in the secondary index
        stmt = ("SELECT * from list_index_search.users where uuids contains {shared_uuid}").format(shared_uuid=shared_uuid)
        cursor.execute(stmt)
        self.assertEqual(50000, cursor.rowcount)

        # shuffle the log in-place, and double-check a slice of records by querying the secondary index
        random.shuffle(log)

        for log_entry in log[:1000]:
            stmt = ("SELECT user_id, email, uuids FROM list_index_search.users where uuids contains {unshared_uuid}"
                ).format(unshared_uuid=log_entry['unshared_uuid'])
            cursor.execute(stmt)

            self.assertEqual(1, cursor.rowcount)

            db_user_id, db_email, db_uuids = cursor.fetchone()

            self.assertEqual(db_user_id, log_entry['user_id'])
            self.assertEqual(db_email, log_entry['email'])
            self.assertEqual(str(db_uuids[0]), str(shared_uuid))
            self.assertEqual(str(db_uuids[1]), str(log_entry['unshared_uuid']))

    def test_set_indexes(self):
        """
        Checks that secondary indexes on sets work for querying.
        """
        cluster = self.cluster
        cluster.populate(1).start()
        [node1] = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'set_index_search', 1)

        stmt = ("CREATE TABLE set_index_search.users ("
               "user_id uuid PRIMARY KEY,"
               "email text,"
               "uuids set<uuid>);")
        cursor.execute(stmt)

        # no index present yet, make sure there's an error trying to query column
        stmt = ("SELECT * from set_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        with self.assertRaisesRegexp(ProgrammingError, 'No indexed columns present in by-columns clause'):
            cursor.execute(stmt)

        # add index and query again (even though there are no rows in the table yet)
        stmt = "CREATE INDEX user_uuids on set_index_search.users (uuids);"
        cursor.execute(stmt)

        stmt = ("SELECT * from set_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        # add a row which doesn't specify data for the indexed column, and query again
        user1_uuid = uuid.uuid4()
        stmt = ("INSERT INTO set_index_search.users (user_id, email) values ({user_id}, 'test@example.com')"
            ).format(user_id=user1_uuid)
        cursor.execute(stmt)

        time.sleep(5)
        stmt = ("SELECT * from set_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        _id = uuid.uuid4()
        # alter the row to add a single item to the indexed set
        stmt = ("UPDATE set_index_search.users set uuids = {{{id}}} where user_id = {user_id}").format(id=_id, user_id=user1_uuid)
        cursor.execute(stmt)

        stmt = ("SELECT * from set_index_search.users where uuids contains {some_uuid}").format(some_uuid=_id)
        cursor.execute(stmt)
        self.assertEqual(1, cursor.rowcount)

        # add a bunch of user records and query them back
        shared_uuid = uuid.uuid4() # this uuid will be on all records

        log = []

        for i in range(50000):
            user_uuid = uuid.uuid4()
            unshared_uuid = uuid.uuid4()

            # give each record a unique email address using the int index
            stmt = ("INSERT INTO set_index_search.users (user_id, email, uuids)"
                  "values ({user_uuid}, '{prefix}@example.com', {{{s_uuid}, {u_uuid}}})"
                ).format(user_uuid=user_uuid, prefix=i, s_uuid=shared_uuid, u_uuid=unshared_uuid)
            cursor.execute(stmt)

            log.append(
                {'user_id': user_uuid,
                 'email':str(i)+'@example.com',
                 'unshared_uuid':unshared_uuid}
            )

        # confirm there is now 50k rows with the 'shared' uuid above in the secondary index
        stmt = ("SELECT * from set_index_search.users where uuids contains {shared_uuid}").format(shared_uuid=shared_uuid)
        cursor.execute(stmt)
        self.assertEqual(50000, cursor.rowcount)

        # shuffle the log in-place, and double-check a slice of records by querying the secondary index
        random.shuffle(log)

        for log_entry in log[:1000]:
            stmt = ("SELECT user_id, email, uuids FROM set_index_search.users where uuids contains {unshared_uuid}"
                ).format(unshared_uuid=log_entry['unshared_uuid'])
            cursor.execute(stmt)

            self.assertEqual(1, cursor.rowcount)

            db_user_id, db_email, db_uuids = cursor.fetchone()

            self.assertEqual(db_user_id, log_entry['user_id'])
            self.assertEqual(db_email, log_entry['email'])
            self.assertTrue(shared_uuid in db_uuids)
            self.assertTrue(log_entry['unshared_uuid'] in db_uuids)

    def test_map_indexes(self):
        """
        Checks that secondary indexes on maps work for querying on both keys and values
        """
        cluster = self.cluster
        cluster.populate(1).start()
        [node1] = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'map_index_search', 1)

        stmt = ("CREATE TABLE map_index_search.users ("
               "user_id uuid PRIMARY KEY,"
               "email text,"
               "uuids map<uuid, uuid>);")
        cursor.execute(stmt)

        # no index present yet, make sure there's an error trying to query column
        stmt = ("SELECT * from map_index_search.users where uuids contains {some_uuid}").format(some_uuid=uuid.uuid4())
        with self.assertRaisesRegexp(ProgrammingError, 'No indexed columns present in by-columns clause'):
            cursor.execute(stmt)

        stmt = ("SELECT * from map_index_search.users where uuids contains key {some_uuid}"
            ).format(some_uuid=uuid.uuid4())
        with self.assertRaisesRegexp(ProgrammingError, 'No indexed columns present in by-columns clause'):
            cursor.execute(stmt)

        # add index on keys and query again (even though there are no rows in the table yet)
        stmt = "CREATE INDEX user_uuids on map_index_search.users (KEYS(uuids));"
        cursor.execute(stmt)

        stmt = "SELECT * from map_index_search.users where uuids contains key {some_uuid}".format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        # add a row which doesn't specify data for the indexed column, and query again
        user1_uuid = uuid.uuid4()
        stmt = ("INSERT INTO map_index_search.users (user_id, email)"
              "values ({user_id}, 'test@example.com')"
            ).format(user_id=user1_uuid)
        cursor.execute(stmt)

        time.sleep(5)
        stmt = ("SELECT * from map_index_search.users where uuids contains key {some_uuid}").format(some_uuid=uuid.uuid4())
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        _id = uuid.uuid4()

        # alter the row to add a single item to the indexed map
        stmt = ("UPDATE map_index_search.users set uuids = {{{id}:{user_id}}} where user_id = {user_id}"
            ).format(id=_id, user_id=user1_uuid)
        cursor.execute(stmt)

        stmt = ("SELECT * from map_index_search.users where uuids contains key {some_uuid}").format(some_uuid=_id)
        cursor.execute(stmt)
        self.assertEqual(1, cursor.rowcount)

        # add a bunch of user records and query them back
        shared_uuid = uuid.uuid4() # this uuid will be on all records

        log = []

        for i in range(50000):
            user_uuid = uuid.uuid4()
            unshared_uuid1 = uuid.uuid4()
            unshared_uuid2 = uuid.uuid4()

            # give each record a unique email address using the int index, add unique ids for keys and values
            stmt = ("INSERT INTO map_index_search.users (user_id, email, uuids)"
                  "values ({user_uuid}, '{prefix}@example.com', {{{u_uuid1}:{u_uuid2}, {s_uuid}:{s_uuid}}})"
                ).format(user_uuid=user_uuid, prefix=i, s_uuid=shared_uuid, u_uuid1=unshared_uuid1, u_uuid2=unshared_uuid2)
            cursor.execute(stmt)

            log.append(
                {'user_id': user_uuid,
                 'email':str(i)+'@example.com',
                 'unshared_uuid1':unshared_uuid1,
                 'unshared_uuid2':unshared_uuid2}
            )

        # confirm there is now 50k rows with the 'shared' uuid above in the secondary index
        stmt = ("SELECT * from map_index_search.users where uuids contains key {shared_uuid}"
            ).format(shared_uuid=shared_uuid)
        cursor.execute(stmt)
        self.assertEqual(50000, cursor.rowcount)

        # shuffle the log in-place, and double-check a slice of records by querying the secondary index on keys
        random.shuffle(log)

        for log_entry in log[:1000]:
            stmt = ("SELECT user_id, email, uuids FROM map_index_search.users where uuids contains key {unshared_uuid1}"
                ).format(unshared_uuid1=log_entry['unshared_uuid1'])
            cursor.execute(stmt)

            self.assertEqual(1, cursor.rowcount)

            db_user_id, db_email, db_uuids = cursor.fetchone()

            self.assertEqual(db_user_id, log_entry['user_id'])
            self.assertEqual(db_email, log_entry['email'])

            self.assertTrue(shared_uuid in db_uuids)
            self.assertTrue(log_entry['unshared_uuid1'] in db_uuids)

        # attempt to add an index on map values as well (should fail)
        stmt = "CREATE INDEX user_uuids on map_index_search.users (uuids);"
        with self.assertRaisesRegexp(ProgrammingError, """Bad Request: Cannot create index on uuids values, an index on uuids keys already exists and indexing a map on both keys and values at the same time is not currently supported"""):
            cursor.execute(stmt)

        # since cannot have index on map keys and values remove current index on keys
        stmt = "DROP INDEX user_uuids;"
        cursor.execute(stmt)  
        
        # add index on values (will index rows added prior)
        stmt = "CREATE INDEX user_uids on map_index_search.users (uuids);"
        cursor.execute(stmt)

        # shuffle the log in-place, and double-check a slice of records by querying the secondary index
        random.shuffle(log)

        time.sleep(10)

        # since we already inserted unique ids for values as well, check that appropriate recors are found
        for log_entry in log[:1000]:
            stmt = ("SELECT user_id, email, uuids FROM map_index_search.users where uuids contains {unshared_uuid2}"
                ).format(unshared_uuid2=log_entry['unshared_uuid2'])

            cursor.execute(stmt)
            self.assertEqual(1, cursor.rowcount)

            db_user_id, db_email, db_uuids = cursor.fetchone()
            self.assertEqual(db_user_id, log_entry['user_id'])
            self.assertEqual(db_email, log_entry['email'])

            self.assertTrue(shared_uuid in db_uuids)
            self.assertTrue(log_entry['unshared_uuid2'] in db_uuids.values())
########NEW FILE########
__FILENAME__ = snapshot_test
from dtest import Tester, debug
from tools import replace_in_file
import tempfile
import shutil
import glob
import os
import time

class SnapshotTester(Tester):
    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

    def insert_rows(self, cursor, start, end):
        for r in range(start, end):
            cursor.execute("INSERT INTO ks.cf (key, val) VALUES ({r}, 'asdf');".format(r=r))

    def make_snapshot(self, node, ks, cf, name):
        debug("Making snapshot....")
        node.flush()
        snapshot_cmd = 'snapshot {ks} -cf {cf} -t {name}'.format(**locals())
        debug("Running snapshot cmd: {snapshot_cmd}".format(snapshot_cmd=snapshot_cmd))
        node.nodetool(snapshot_cmd)
        tmpdir = tempfile.mkdtemp()
        os.mkdir(os.path.join(tmpdir,ks))
        os.mkdir(os.path.join(tmpdir,ks,cf))
        node_dir = node.get_path()
        
        # Find the snapshot dir, it's different in various C* versions:
        snapshot_dir = "{node_dir}/data/{ks}/{cf}/snapshots/{name}".format(**locals())
        if not os.path.isdir(snapshot_dir):
            snapshot_dir = glob.glob("{node_dir}/data/{ks}/{cf}-*/snapshots/{name}".format(**locals()))[0]
        debug("snapshot_dir is : " + snapshot_dir)
        debug("snapshot copy is : " + tmpdir)

        os.system('cp -a {snapshot_dir}/* {tmpdir}/{ks}/{cf}/'.format(**locals()))
        return tmpdir

    def restore_snapshot(self, snapshot_dir, node, ks, cf):
        debug("Restoring snapshot....")
        node_dir = node.get_path()
        snapshot_dir = os.path.join(snapshot_dir, ks, cf)
        ip = node.address()
        os.system('{node_dir}/bin/sstableloader -d {ip} {snapshot_dir}'.format(**locals()))

class TestSnapshot(SnapshotTester):

    def __init__(self, *args, **kwargs):
        SnapshotTester.__init__(self, *args, **kwargs)

    def test_basic_snapshot_and_restore(self):
        cluster = self.cluster
        cluster.populate(1).start()
        (node1,) = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        cursor.execute('CREATE TABLE ks.cf ( key int PRIMARY KEY, val text);')

        self.insert_rows(cursor, 0, 100)
        snapshot_dir = self.make_snapshot(node1, 'ks', 'cf', 'basic')

        # Write more data after the snapshot, this will get thrown
        # away when we restore:
        self.insert_rows(cursor, 100, 200)
        cursor.execute('SELECT count(*) from ks.cf')
        self.assertEqual(cursor.fetchone()[0], 200)

        # Drop the keyspace, make sure we have no data:
        cursor.execute('DROP KEYSPACE ks')
        self.create_ks(cursor, 'ks', 1)
        cursor.execute('CREATE TABLE ks.cf ( key int PRIMARY KEY, val text);')
        cursor.execute('SELECT count(*) from ks.cf')
        self.assertEqual(cursor.fetchone()[0], 0)

        # Restore data from snapshot:
        self.restore_snapshot(snapshot_dir, node1, 'ks', 'cf')
        node1.nodetool('refresh ks cf')
        cursor.execute('SELECT count(*) from ks.cf')

        # clean up
        debug("removing snapshot_dir: " + snapshot_dir)
        shutil.rmtree(snapshot_dir)

        self.assertEqual(cursor.fetchone()[0], 100)

class TestArchiveCommitlog(SnapshotTester):
    def __init__(self, *args, **kwargs):
        kwargs['cluster_options'] = {'commitlog_segment_size_in_mb':1}
        SnapshotTester.__init__(self, *args, **kwargs)

    def test_archive_commitlog(self):
        self.run_archive_commitlog(restore_point_in_time = False)

    def test_archive_commitlog_with_active_commitlog(self):
        """Copy the active commitlogs to the archive directory before restoration"""
        self.run_archive_commitlog(restore_point_in_time = False, archive_active_commitlogs=True)

    def dont_test_archive_commitlog(self):
        """Run the archive commitlog test, but forget to add the restore commands:"""
        self.run_archive_commitlog(restore_point_in_time = False, restore_archived_commitlog=False)

    def test_archive_commitlog_point_in_time(self):
        """Test archive commit log with restore_point_in_time setting"""
        self.run_archive_commitlog(restore_point_in_time = True)

    def test_archive_commitlog_point_in_time_with_active_commitlog(self):
        """Test archive commit log with restore_point_in_time setting"""
        self.run_archive_commitlog(restore_point_in_time = True, archive_active_commitlogs=True)

    def run_archive_commitlog(self, restore_point_in_time=False, restore_archived_commitlog=True, archive_active_commitlogs=False):
        """Run archive commit log restoration test"""

        cluster = self.cluster
        cluster.populate(1)
        (node1,) = cluster.nodelist()

        # Create a temp directory for storing commitlog archives:
        tmp_commitlog = tempfile.mkdtemp()
        debug("tmp_commitlog: " + tmp_commitlog)

        # Edit commitlog_archiving.properties and set an archive
        # command:
        replace_in_file(os.path.join(node1.get_path(),'conf','commitlog_archiving.properties'),
                        [(r'^archive_command=.*$', 'archive_command=/bin/cp %path {tmp_commitlog}/%name'.format(
                            tmp_commitlog=tmp_commitlog))])

        cluster.start()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        cursor.execute('CREATE TABLE ks.cf ( key bigint PRIMARY KEY, val text);')
        debug("Writing first 30,000 rows...")
        self.insert_rows(cursor, 0, 30000)
        # Record when this first set of inserts finished:
        insert_cutoff_times = [time.gmtime()]

        # Delete all commitlog backups so far:
        os.system('rm {tmp_commitlog}/*'.format(tmp_commitlog=tmp_commitlog))

        snapshot_dir = self.make_snapshot(node1, 'ks', 'cf', 'basic')

        # Write more data:
        debug("Writing second 30,000 rows...")
        self.insert_rows(cursor, 30000, 60000)
        node1.flush()
        time.sleep(10)
        # Record when this second set of inserts finished:
        insert_cutoff_times.append(time.gmtime())
        
        debug("Writing final 5,000 rows...")
        self.insert_rows(cursor,60000, 65000)
        # Record when the third set of inserts finished:
        insert_cutoff_times.append(time.gmtime())

        cursor.execute('SELECT count(*) from ks.cf')
        # Make sure we have the same amount of rows as when we snapshotted:
        self.assertEqual(cursor.fetchone()[0], 65000)

        # Check that there are at least one commit log backed up that
        # is not one of the active commit logs:
        commitlog_dir = os.path.join(node1.get_path(), 'commitlogs')
        debug("node1 commitlog dir: " + commitlog_dir)
        self.assertTrue(len(set(os.listdir(tmp_commitlog)) - set(os.listdir(commitlog_dir))) > 0)

        cluster.flush()
        cluster.compact()
        node1.drain()
        if archive_active_commitlogs:
            # Copy the active commitlogs to the backup directory:
            for f in glob.glob(commitlog_dir+"/*"):
                shutil.copy2(f, tmp_commitlog)

        # Destroy the cluster
        cluster.stop()
        self.copy_logs(name=self.id().split(".")[0]+"_pre-restore")
        self._cleanup_cluster()
        cluster = self.cluster = self._get_cluster()
        cluster.populate(1)
        (node1,) = cluster.nodelist()
        cluster.start()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        cursor.execute('CREATE TABLE ks.cf ( key bigint PRIMARY KEY, val text);')

        # Restore from snapshot:
        self.restore_snapshot(snapshot_dir, node1, 'ks', 'cf')
        cursor.execute('SELECT count(*) from ks.cf')
        # Make sure we have the same amount of rows as when we snapshotted:
        self.assertEqual(cursor.fetchone()[0], 30000)
        
        # Edit commitlog_archiving.properties. Remove the archive
        # command  and set a restore command and restore_directories:
        if restore_archived_commitlog:
            replace_in_file(os.path.join(node1.get_path(),'conf','commitlog_archiving.properties'),
                            [(r'^archive_command=.*$', 'archive_command='),
                             (r'^restore_command=.*$', 'restore_command=cp -f %from %to'),
                             (r'^restore_directories=.*$', 'restore_directories={tmp_commitlog}'.format(
                                 tmp_commitlog=tmp_commitlog))])
            
            if restore_point_in_time:
                restore_time = time.strftime("%Y:%m:%d %H:%M:%S", insert_cutoff_times[1])
                replace_in_file(os.path.join(node1.get_path(),'conf','commitlog_archiving.properties'),
                                [(r'^restore_point_in_time=.*$', 'restore_point_in_time={restore_time}'.format(**locals()))])
        
        debug("Restarting node1..")
        node1.stop()
        node1.start()

        node1.nodetool('flush')
        node1.nodetool('compact')

        cursor = self.patient_cql_connection(node1).cursor()
        cursor.execute('SELECT count(*) from ks.cf')

        # clean up
        debug("removing snapshot_dir: " + snapshot_dir)
        shutil.rmtree(snapshot_dir)
        debug("removing tmp_commitlog: " + tmp_commitlog)
        shutil.rmtree(tmp_commitlog)

        # Now we should have 30000 rows from the snapshot + 30000 rows
        # from the commitlog backups:
        if not restore_archived_commitlog:
            self.assertEqual(cursor.fetchone()[0], 30000)
        elif restore_point_in_time:
            self.assertEqual(cursor.fetchone()[0], 60000)
        else:
            self.assertEqual(cursor.fetchone()[0], 65000)

########NEW FILE########
__FILENAME__ = sstablesplit_test
from dtest import Tester, debug
from assertions import *
from tools import *

from os.path import getsize
import time

class TestSSTableSplit(Tester):

    def split_test(self):
        """
        Check that after running compaction, sstablessplit can succesfully split
        The resultant sstable.  Check that split is reversable and that data is readable
        after carrying out these operations.
        """
        cluster = self.cluster
        cluster.populate(1).start()
        node = cluster.nodelist()[0]
        version = cluster.version()
        debug("Run stress to insert data")
        if version < "2.1":
            node.stress( ['-o', 'insert'] )
        else:
            node.stress( ['write', 'n=1100000', '-rate', 'threads=8'] )
        
        self._do_compaction(node)
        self._do_split(node, version)
        self._do_compaction(node)
        self._do_split(node, version)

        debug("Run stress to ensure data is readable")
        if version < "2.1":
            node.stress( ['-o', 'read'] )
        else:
            node.stress( ['read', 'n=1100000', '-rate', 'threads=8'] )

    def _do_compaction(self, node):
        debug("Compact sstables.")
        node.compact()
        sstables = node.get_sstables('Keyspace1', '')

    def _do_split(self, node, version):
        debug("run sstablesplit")
        time.sleep(5.0)
        node.stop()
        node.run_sstablesplit( keyspace='Keyspace1' )
        sstables = node.get_sstables('Keyspace1', '')
        assert len(sstables) == 6, "Incorrect number of sstables after running sstablesplit."
        if version < "2.1":
            assert max( [ getsize( sstable ) for sstable in sstables ] ) <= 52428960, "Max sstables size should be 52428960."
        else:
            assert max( [ getsize( sstable ) for sstable in sstables ] ) <= 52428980, "Max sstables size should be 52428980."
        node.start()

########NEW FILE########
__FILENAME__ = sstable_generation_loading_test
import time
import os
from distutils import dir_util
import subprocess

from dtest import Tester, debug
from tools import *
from ccmlib import common as ccmcommon

class TestSSTableGenerationAndLoading(Tester):

    def __init__(self, *argv, **kwargs):
        super(TestSSTableGenerationAndLoading, self).__init__(*argv, **kwargs)
        self.allow_log_errors = True

    def incompressible_data_in_compressed_table_test(self):
        """
        tests for the bug that caused #3370:
        https://issues.apache.org/jira/browse/CASSANDRA-3370

        inserts random data into a compressed table. The compressed SSTable was
        compared to the uncompressed and was found to indeed be larger then
        uncompressed.
        """
        cluster = self.cluster
        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        time.sleep(.5)

        cursor = self.cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', compression="Deflate")

        # make unique column names, and values that are incompressible
        rnd = open('/dev/urandom', 'rb')
        for col in xrange(10):
            col_name = str(col)
            col_val = rnd.read(5000)
            col_val = col_val.encode('hex')
            cql = "UPDATE cf SET v='%s' WHERE KEY='0' AND c='%s'" % (col_val, col_name)
            # print cql
            cursor.execute(cql)
        rnd.close()

        node1.flush()
        time.sleep(2)
        cursor.execute("SELECT * FROM cf WHERE KEY = '0' AND c < '8'")
        cursor.fetchone()

    def remove_index_file_test(self):
        """
        tests for situations similar to that found in #343:
        https://issues.apache.org/jira/browse/CASSANDRA-343
        """
        cluster = self.cluster
        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        version = cluster.version()
        if version < "2.1":
            node1.stress(['--num-keys=10000'])
        else:
            node1.stress(['write', 'n=10000', '-rate', 'threads=8'])
        node1.flush()
        node1.compact()
        node1.stop()
        time.sleep(1)
        path = ""
        if version < "2.1":
            path = os.path.join(node1.get_path(), 'data', 'Keyspace1', 'Standard1')
        else:
            basepath = os.path.join(node1.get_path(), 'data', 'Keyspace1')
            for x in os.listdir(basepath):
                if x.startswith("Standard1"):
                    path = os.path.join(basepath, x)

        os.system('rm %s/*Index.db' % path)
        os.system('rm %s/*Filter.db' % path)
        os.system('rm %s/*Statistics.db' % path)
        os.system('rm %s/*Digest.sha1' % path)

        node1.start()

        time.sleep(10)

        data_found = 0
        for fname in os.listdir(path):
            if fname.endswith('Data.db'):
                data_found += 1
        assert data_found > 0, "After removing index, filter, stats, and digest files, the data file was deleted!"

    def sstableloader_compression_none_to_none_test(self):
        self.load_sstable_with_configuration(None, None)

    def sstableloader_compression_none_to_snappy_test(self):
        self.load_sstable_with_configuration(None, 'Snappy')

    def sstableloader_compression_none_to_deflate_test(self):
        self.load_sstable_with_configuration(None, 'Deflate')

    def sstableloader_compression_snappy_to_none_test(self):
        self.load_sstable_with_configuration('Snappy', None)

    def sstableloader_compression_snappy_to_snappy_test(self):
        self.load_sstable_with_configuration('Snappy', 'Snappy')

    def sstableloader_compression_snappy_to_deflate_test(self):
        self.load_sstable_with_configuration('Snappy', 'Deflate')

    def sstableloader_compression_deflate_to_none_test(self):
        self.load_sstable_with_configuration('Deflate', None)

    def sstableloader_compression_deflate_to_snappy_test(self):
        self.load_sstable_with_configuration('Deflate', 'Snappy')

    def sstableloader_compression_deflate_to_deflate_test(self):
        self.load_sstable_with_configuration('Deflate', 'Deflate')

    def load_sstable_with_configuration(self, pre_compression=None, post_compression=None):
        """
        tests that the sstableloader works by using it to load data.
        Compression of the columnfamilies being loaded, and loaded into
        can be specified.

        pre_compression and post_compression can be these values:
        None, 'Snappy', or 'Deflate'.
        """
        NUM_KEYS = 1000

        for compression_option in (pre_compression, post_compression):
            assert compression_option in (None, 'Snappy', 'Deflate')

        debug("Testing sstableloader with pre_compression=%s and post_compression=%s" % (pre_compression, post_compression))

        cluster = self.cluster
        cluster.populate(2).start()
        [node1, node2] = cluster.nodelist()
        time.sleep(.5)

        def create_schema(cursor, compression):
            self.create_ks(cursor, "ks", rf=2)
            self.create_cf(cursor, "standard1", compression=compression)
            self.create_cf(cursor, "counter1", compression=compression, columns={'v': 'counter'})

        debug("creating keyspace and inserting")
        cursor = self.cql_connection(node1).cursor()
        create_schema(cursor, pre_compression)

        for i in range(NUM_KEYS):
            cursor.execute("UPDATE standard1 SET v='%d' WHERE KEY='%d' AND c='col'" % (i, i))
            cursor.execute("UPDATE counter1 SET v=v+1 WHERE KEY='%d'" % i)

        node1.nodetool('drain')
        node1.stop()
        node2.nodetool('drain')
        node2.stop()

        debug("Making a copy of the sstables")
        # make a copy of the sstables
        data_dir = os.path.join(node1.get_path(), 'data')
        copy_root = os.path.join(node1.get_path(), 'data_copy')
        for ddir in os.listdir(data_dir):
            keyspace_dir = os.path.join(data_dir, ddir)
            if os.path.isdir(keyspace_dir) and ddir != 'system':
                copy_dir = os.path.join(copy_root, ddir)
                dir_util.copy_tree(keyspace_dir, copy_dir)

        debug("Wiping out the data and restarting cluster")
        # wipe out the node data.
        cluster.clear()
        cluster.start()
        time.sleep(5) # let gossip figure out what is going on

        debug("re-creating the keyspace and column families.")
        cursor = self.cql_connection(node1).cursor()
        create_schema(cursor, post_compression)
        time.sleep(2)

        debug("Calling sstableloader")
        # call sstableloader to re-load each cf.
        cdir = node1.get_cassandra_dir()
        sstableloader = os.path.join(cdir, 'bin', 'sstableloader')
        env = ccmcommon.make_cassandra_env(cdir, node1.get_path())
        host = node1.address()
        sstablecopy_dir = copy_root + '/ks'
        for cf_dir in os.listdir(sstablecopy_dir):
            full_cf_dir = os.path.join(sstablecopy_dir, cf_dir)
            if os.path.isdir(full_cf_dir):
                cmd_args = [sstableloader, '--nodes', host, full_cf_dir]
                p = subprocess.Popen(cmd_args, env=env)
                exit_status = p.wait()
                self.assertEqual(0, exit_status,
                        "sstableloader exited with a non-zero status: %d" % exit_status)

        def read_and_validate_data(cursor):
            for i in range(NUM_KEYS):
                cursor.execute("SELECT * FROM standard1 WHERE KEY='%d'" % i)
                self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
                cursor.execute("SELECT * FROM counter1 WHERE KEY='%d'" % i)
                self.assertEquals([str(i), 1], cursor.fetchone())

        debug("Reading data back")
        # Now we should have sstables with the loaded data, and the existing
        # data. Lets read it all to make sure it is all there.
        read_and_validate_data(cursor)

        debug("scrubbing, compacting, and repairing")
        # do some operations and try reading the data again.
        node1.nodetool('scrub')
        node1.nodetool('compact')
        node1.nodetool('repair')

        debug("Reading data back one more time")
        read_and_validate_data(cursor)

########NEW FILE########
__FILENAME__ = super_column_cache_test
from dtest import Tester
from assertions import *
from tools import *

import os, sys, time, tools
from uuid import UUID
from ccmlib.cluster import Cluster

def assert_columns(cli, names):
    assert not cli.has_errors(), cli.errors()
    output = cli.last_output()

    for name in names:
        assert re.search('name=%s' % name, output) is not None, 'Cannot find column %s in %s' % (name, output)

class TestSCCache(Tester):

    def sc_with_row_cache_test(self):
        """ Test for bug reported in #4190 """
        cluster = self.cluster

        cluster.populate(1).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cli = node1.cli()
        cli.do("create keyspace ks")
        cli.do("use ks")
        if cluster.version() < "1.1":
            cli.do("""
               create column family Users
               with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type'
               and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';
            """)
        else:
            cli.do("""
               create column family Users
               with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type'
               and caching='ROWS_ONLY';
            """)

        cli.do("set Users['mina']['attrs']['name'] = 'Mina'")
        cli.do("get Users['mina']")
        assert_columns(cli, ['name'])

        cli.do("set Users['mina']['attrs']['country'] = 'Canada'")
        cli.do("get Users['mina']")
        assert_columns(cli, ['name', 'country'])

        cli.do("set Users['mina']['attrs']['region'] = 'Quebec'")
        cli.do("get Users['mina']")
        assert_columns(cli, ['name', 'country', 'region'])

########NEW FILE########
__FILENAME__ = super_counter_test
import time

from dtest import Tester, debug

import cql
from cql.cassandra.ttypes import CfDef, ColumnParent, CounterColumn, \
        ConsistencyLevel, ColumnPath

class TestSuperCounterClusterRestart(Tester):
    """
    This test is part of this issue:
    https://issues.apache.org/jira/browse/CASSANDRA-3821
    """

    def functional_test(self):
        NUM_SUBCOLS = 100
        NUM_ADDS = 100

        cluster = self.cluster
        cluster.populate(3).start()
        node1 = cluster.nodelist()[0]

        time.sleep(.5)
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        time.sleep(1) # wait for propagation

        # create the columnfamily using thrift
        host, port = node1.network_interfaces['thrift']
        thrift_conn = cql.connect(host, port, keyspace='ks')
        cf_def = CfDef(keyspace='ks', name='cf', column_type='Super', 
                default_validation_class='CounterColumnType')
        thrift_conn.client.system_add_column_family(cf_def)

        # let the sediment settle to to the bottom before drinking...
        time.sleep(2)

        for subcol in xrange(NUM_SUBCOLS):
            for add in xrange(NUM_ADDS):
                column_parent = ColumnParent(column_family='cf', 
                        super_column='subcol_%d' % subcol)
                counter_column = CounterColumn('col_0', 1)
                thrift_conn.client.add('row_0', column_parent, counter_column,
                        ConsistencyLevel.QUORUM)
        time.sleep(1)

        # flush everything and the problem will be mostly corrected.
#        for node in cluster.nodelist():
#            node.flush()

        debug("Stopping cluster")
        cluster.stop()
        time.sleep(5)
        debug("Starting cluster")
        cluster.start()
        time.sleep(5)

        thrift_conn = cql.connect(host, port, keyspace='ks')

        from_db = []

        for i in xrange(NUM_SUBCOLS):
            column_path = ColumnPath(column_family='cf', column='col_0', 
                    super_column='subcol_%d'%i)
            column_or_super_column = thrift_conn.client.get('row_0', column_path, 
                    ConsistencyLevel.QUORUM)
            val = column_or_super_column.counter_column.value
            debug(str(val)),
            from_db.append(val)
        debug("")

        expected = [NUM_ADDS for i in xrange(NUM_SUBCOLS)]

        if from_db != expected:
            raise Exception("Expected a bunch of the same values out of the db. Got this: " + str(from_db))

########NEW FILE########
__FILENAME__ = taketoken_test
import time

from dtest import Tester, debug
from assertions import assert_unavailable
from tools import (create_c1c2_table, insert_c1c2, query_c1c2, retry_till_success,
                   insert_columns, new_node, no_vnodes, since)

class TestTakeToken(Tester):

    @since('2.0')
    def taketoken_test(self):
        debug("Creating a ring")
        cluster = self.cluster
        cluster.set_configuration_options(values={
                'initial_token': None, 
                'num_tokens': 10,
                'hinted_handoff_enabled' : False, 
                'write_request_timeout_in_ms' : 60000, 
                'read_request_timeout_in_ms' : 60000, 
                'dynamic_snitch_badness_threshold' : 0.0}, batch_commitlog=True)
    
        cluster.populate(3).start()
        [node1, node2, node3] = cluster.nodelist()
        cluster.start()
       
        debug("Set to talk to node 2")
        n2cursor = self.patient_cql_connection(node2).cursor()
        self.create_ks(n2cursor, 'ks', 2)
        create_c1c2_table(self, n2cursor)

        debug("Generating some data for all nodes")
        for n in xrange(10,20):
            insert_c1c2(n2cursor, n, 'ALL')

        node1.flush()
       
        debug("Writing data to node2")
        for n in xrange(30,1000):
            insert_c1c2(n2cursor, n, 'ONE')
        node2.flush()
   
        debug("Getting token from node 1")
        n1cursor = self.patient_cql_connection(node1).cursor()
        n1cursor.execute('SELECT tokens FROM system.local')
        n1tokens = n1cursor.fetchone()

        n3cursor = self.patient_cql_connection(node3).cursor()
        n3cursor.execute('SELECT tokens FROM system.local')
        n3tokens = n3cursor.fetchone()

        debug("Relocate tokens from node1 to node3")
        i = 0;
        tl = "";
        for t in n1tokens[0]:
            if i == 8:
                break
            t = '\\%s' % t
            tl = "%s %s" % (tl, t);
            i += 1

        cmd = "taketoken %s" % tl
        debug(cmd)
        node3.nodetool(cmd)
        time.sleep(1)

        debug("Check that the tokens were really moved")
        n3cursor.execute('SELECT tokens FROM system.local')
        n3tokens = n3cursor.fetchone()

        n1cursor.execute('SELECT tokens FROM system.local')
        n1tokens = n1cursor.fetchone()

        debug("n1 %s n3 %s" % (n1tokens,n3tokens))
        assert len(n3tokens[0]) == 18
        assert len(n1tokens[0]) == 2

        debug("Checking that no data was lost")
        for n in xrange(10,20):
            query_c1c2(n2cursor, n, 'ALL')

        for n in xrange(30,1000):
            query_c1c2(n2cursor, n, 'ALL')


########NEW FILE########
__FILENAME__ = thrift_hsha_test
from dtest import Tester, debug, DEFAULT_DIR
import unittest
import time
import os
import subprocess
import shlex 
import pycassa
import glob
import sys

JNA_PATH = '/usr/share/java/jna.jar'
ATTACK_JAR = 'cassandra-attack.jar'

# Use jna.jar in {CASSANDRA_DIR,DEFAULT_DIR}/lib/, since >=2.1 needs correct version
try:
    if glob.glob('%s/lib/jna-*.jar' % os.environ['CASSANDRA_DIR']):
        debug('Using jna.jar in CASSANDRA_DIR/lib..')
        JNA_IN_LIB = glob.glob('%s/lib/jna-*.jar' % os.environ['CASSANDRA_DIR'])
        JNA_PATH = JNA_IN_LIB[0]
except KeyError:
    if glob.glob('%s/lib/jna-*.jar' % DEFAULT_DIR):
        print ('Using jna.jar in DEFAULT_DIR/lib/..')
        JNA_IN_LIB = glob.glob('%s/lib/jna-*.jar' % DEFAULT_DIR)
        JNA_PATH = JNA_IN_LIB[0]

class ThriftHSHATest(Tester):

    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)


    @unittest.skipIf(sys.platform == "win32", 'Could not be executed on Windows')
    def test_closing_connections(self):
        """Test CASSANDRA-6546 - do connections get closed when disabling / renabling thrift service?"""
        cluster = self.cluster
        cluster.set_configuration_options(values={
            'rpc_server_type' : 'hsha',
            'rpc_max_threads' : 20
        })
        
        cluster.populate(1)
        cluster.start()
        (node1,) = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'test', 1)
        cursor.execute("CREATE TABLE \"CF\" (key text PRIMARY KEY, val text) WITH COMPACT STORAGE;")
        def make_connection():
            pool = pycassa.ConnectionPool('test', timeout=None)
            cf = pycassa.ColumnFamily(pool, 'CF')
            return pool

        pools = []
        for i in xrange(10):
            debug("Creating connection pools..")
            for x in xrange(3):
                pools.append(make_connection())
            debug("Disabling/Enabling thrift iteration #{i}".format(i=i))
            node1.nodetool('disablethrift')
            node1.nodetool('enablethrift')
            debug("Closing connections from the client side..")
            for pool in pools:
                pool.dispose()
            stdout = subprocess.Popen(["lsof -a -p %s -iTCP -sTCP:CLOSE_WAIT" % node1.pid], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True).communicate()[0]
            lines = stdout.splitlines()
            self.assertEqual(len(lines), 0, "There are non-closed connections: %s" % stdout)

    @unittest.skipIf(not os.path.exists(ATTACK_JAR), "No attack jar found")
    @unittest.skipIf(not os.path.exists(JNA_PATH), "No JNA jar found")
    def test_6285(self):
        """Test CASSANDRA-6285 with Viktor Kuzmin's  attack jar.

        This jar file is not a part of this repository, you can
        compile it yourself from sources found on CASSANDRA-6285. This
        test will be skipped if the jar file is not found.
        """
        cluster = self.cluster
        cluster.set_configuration_options(values={ 'rpc_server_type' : 'hsha'})

        # Enable JNA:
        with open(os.path.join(self.test_path, 'test', 'cassandra.in.sh'),'w') as f:
            f.write('CLASSPATH={jna_path}:$CLASSPATH\n'.format(
                jna_path=JNA_PATH))

        cluster.populate(2)
        nodes = (node1, node2) = cluster.nodelist()
        [n.start(use_jna=True) for n in nodes]
        debug("Cluster started.")

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'tmp', 2)

        cursor.execute("""CREATE TABLE "CF" (
  key blob,
  column1 timeuuid,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE;
""")


        debug("running attack jar...")
        p = subprocess.Popen(shlex.split("java -jar {attack_jar}".format(attack_jar=ATTACK_JAR)))
        p.communicate()

        debug("Stopping cluster..")
        cluster.stop()
        debug("Starting cluster..")
        cluster.start(no_wait=True)
        debug("Waiting 10 seconds before we're done..")
        time.sleep(10)
        

########NEW FILE########
__FILENAME__ = tools
import time
from ccmlib.node import Node
from decorator  import decorator
from distutils.version import LooseVersion
import cql
import re
import os
import sys
import fileinput

from thrift.transport import TTransport, TSocket
from thrift.protocol import TBinaryProtocol

def retry_till_success(fun, *args, **kwargs):
    timeout = kwargs.pop('timeout', 60)
    bypassed_exception = kwargs.pop('bypassed_exception', Exception)

    deadline = time.time() + timeout
    while True:
        try:
            return fun(*args, **kwargs)
        except bypassed_exception:
            if time.time() > deadline:
                raise
            else:
                # brief pause before next attempt
                time.sleep(0.25)

def create_c1c2_table(tester, cursor, read_repair=None):
    tester.create_cf(cursor, 'cf', columns={ 'c1' : 'text', 'c2' : 'text' }, read_repair=read_repair)

def insert_c1c2(cursor, key, consistency="QUORUM"):
    if cursor.cql_major_version >= 3:
        cursor.execute('UPDATE cf SET c1=\'value1\', c2=\'value2\' WHERE key=\'k%d\'' % key, consistency_level=consistency)
    else:
        cursor.execute('UPDATE cf USING CONSISTENCY %s SET c1=\'value1\', c2=\'value2\' WHERE key=\'k%d\'' % (consistency, key))

def insert_columns(tester, cursor, key, columns_count, consistency="QUORUM", offset=0):
    if tester.cluster.version() >= "1.2":
        upds = [ "UPDATE cf SET v=\'value%d\' WHERE key=\'k%s\' AND c=\'c%06d\'" % (i, key, i) for i in xrange(offset*columns_count, columns_count*(offset+1))]
        query = 'BEGIN BATCH %s; APPLY BATCH' % '; '.join(upds)
        cursor.execute(query, consistency_level=consistency)
    else:
        kvs = [ "c%06d=value%d" % (i, i) for i in xrange(offset*columns_count, columns_count*(offset+1))]
        query = 'UPDATE cf USING CONSISTENCY %s SET %s WHERE key=k%s' % (consistency, ', '.join(kvs), key)
        cursor.execute(query)

def query_c1c2(cursor, key, consistency="QUORUM"):
    if cursor.cql_major_version >= 3:
        cursor.execute('SELECT c1, c2 FROM cf WHERE key=\'k%d\'' % key, consistency_level=consistency)
    else:
        cursor.execute('SELECT c1, c2 FROM cf USING CONSISTENCY %s WHERE key=\'k%d\'' % (consistency, key))
    assert cursor.rowcount == 1
    res = cursor.fetchone()
    assert len(res) == 2 and res[0] == 'value1' and res[1] == 'value2', res

def query_columns(tester, cursor, key, columns_count, consistency="QUORUM", offset=0):
    if tester.cluster.version() >= "1.2":
        cursor.execute('SELECT c, v FROM cf WHERE key=\'k%s\' AND c >= \'c%06d\' AND c <= \'c%06d\'' % (key, offset, columns_count+offset-1), consistency_level=consistency)
        res = cursor.fetchall()
        assert len(res) == columns_count, "%s != %s (%s-%s)" % (len(res), columns_count, offset, columns_count+offset-1)
        for i in xrange(0, columns_count):
            assert res[i][1] == 'value%d' % (i+offset)
    else:
        cursor.execute('SELECT c%06d..c%06d FROM cf USING CONSISTENCY %s WHERE key=k%s' % (offset, columns_count+offset-1, consistency, key))
        assert cursor.rowcount == 1
        res = cursor.fetchone()
        assert len(res) == columns_count, "%s != %s (%s-%s)" % (len(res), columns_count, offset, columns_count+offset-1)
        for i in xrange(0, columns_count):
            assert res[i] == 'value%d' % (i+offset)

def remove_c1c2(cursor, key, consistency="QUORUM"):
    if cursor.cql_major_version >= 3:
        cursor.execute('DELETE c1, c2 FROM cf WHERE key=k%d' % key, consistency_level=consistency)
    else:
        cursor.execute('DELETE c1, c2 FROM cf USING CONSISTENCY %s WHERE key=k%d' % (consistency, key))

# work for cluster started by populate
def new_node(cluster, bootstrap=True, token=None, remote_debug_port='2000', data_center=None):
    i = len(cluster.nodes) + 1
    node = Node('node%s' % i,
                cluster,
                bootstrap,
                ('127.0.0.%s' % i, 9160),
                ('127.0.0.%s' % i, 7000),
                str(7000 + i * 100),
                remote_debug_port,
                token,
                binary_interface=('127.0.0.%s' % i, 9042))
    cluster.add(node, not bootstrap, data_center=data_center)
    return node

def _put_with_overwrite(cluster, cursor, nb_keys, cl="QUORUM"):
    if cluster.version() >= "1.2":
        for k in xrange(0, nb_keys):
            kvs = [ "UPDATE cf SET v=\'value%d\' WHERE key=\'k%s\' AND c=\'c%02d\'" % (i, k, i) for i in xrange(0, 100) ]
            cursor.execute('BEGIN BATCH %s APPLY BATCH' % '; '.join(kvs), consistency_level=cl)
            time.sleep(.01)
        cluster.flush()
        for k in xrange(0, nb_keys):
            kvs = [ "UPDATE cf SET v=\'value%d\' WHERE key=\'k%s\' AND c=\'c%02d\'" % (i*4, k, i*2) for i in xrange(0, 50) ]
            cursor.execute('BEGIN BATCH %s APPLY BATCH' % '; '.join(kvs), consistency_level=cl)
            time.sleep(.01)
        cluster.flush()
        for k in xrange(0, nb_keys):
            kvs = [ "UPDATE cf SET v=\'value%d\' WHERE key=\'k%s\' AND c=\'c%02d\'" % (i*20, k, i*5) for i in xrange(0, 20) ]
            cursor.execute('BEGIN BATCH %s APPLY BATCH' % '; '.join(kvs), consistency_level=cl)
            time.sleep(.01)
        cluster.flush()
    else:
        for k in xrange(0, nb_keys):
            kvs = [ "c%02d=value%d" % (i, i) for i in xrange(0, 100) ]
            cursor.execute('UPDATE cf USING CONSISTENCY %s SET %s WHERE key=k%s' % (cl, ','.join(kvs), k))
            time.sleep(.01)
        cluster.flush()
        for k in xrange(0, nb_keys):
            kvs = [ "c%02d=value%d" % (i*2, i*4) for i in xrange(0, 50) ]
            cursor.execute('UPDATE cf USING CONSISTENCY %s SET %s WHERE key=k%d' % (cl, ','.join(kvs), k))
            time.sleep(.01)
        cluster.flush()
        for k in xrange(0, nb_keys):
            kvs = [ "c%02d=value%d" % (i*5, i*20) for i in xrange(0, 20) ]
            cursor.execute('UPDATE cf USING CONSISTENCY %s SET %s WHERE key=k%d' % (cl, ','.join(kvs), k))
            time.sleep(.01)
        cluster.flush()

def _validate_row(cluster, res):
    if cluster.version() >= "1.2":
        assert len(res) == 100, len(res)
        for i in xrange(0, 100):
            if i % 5 == 0:
                assert res[i][2] == 'value%d' % (i*4), 'for %d, expecting value%d, got %s' % (i, i*4, res[i][2])
            elif i % 2 == 0:
                assert res[i][2] == 'value%d' % (i*2), 'for %d, expecting value%d, got %s' % (i, i*2, res[i][2])
            else:
                assert res[i][2] == 'value%d' % i, 'for %d, expecting value%d, got %s' % (i, i, res[i][2])
    else:
        assert len(res) == 100, len(res)
        for i in xrange(0, 100):
            if i % 5 == 0:
                assert res[i] == 'value%d' % (i*4), 'for %d, expecting value%d, got %s' % (i, i*4, res[i])
            elif i % 2 == 0:
                assert res[i] == 'value%d' % (i*2), 'for %d, expecting value%d, got %s' % (i, i*2, res[i])
            else:
                assert res[i] == 'value%d' % i, 'for %d, expecting value%d, got %s' % (i, i, res[i])

# Simple puts and get (on one row), testing both reads by names and by slice,
# with overwrites and flushes between inserts to make sure we hit multiple
# sstables on reads
def putget(cluster, cursor, cl="QUORUM"):

    _put_with_overwrite(cluster, cursor, 1, cl)

    # reads by name
    ks = [ "\'c%02d\'" % i for i in xrange(0, 100) ]
    # We do not support proper IN queries yet
    #if cluster.version() >= "1.2":
    #    cursor.execute('SELECT * FROM cf USING CONSISTENCY %s WHERE key=\'k0\' AND c IN (%s)' % (cl, ','.join(ks)))
    #else:
    #    cursor.execute('SELECT %s FROM cf USING CONSISTENCY %s WHERE key=\'k0\'' % (','.join(ks), cl))
    #_validate_row(cluster, cursor)
    if cluster.version() < "1.2":
        cursor.execute('SELECT %s FROM cf USING CONSISTENCY %s WHERE key=\'k0\'' % (','.join(ks), cl))
        assert cursor.rowcount == 1
        res = cursor.fetchone() #[1:] # removing key
        _validate_row(cluster, res)

    # slice reads
    if cluster.version() >= "1.2":
        cursor.execute('SELECT * FROM cf WHERE key=\'k0\'', consistency_level=cl)
        _validate_row(cluster, cursor.fetchall())
    else:
        cursor.execute('SELECT * FROM cf USING CONSISTENCY %s WHERE key=\'k0\'' % cl)
        _validate_row(cluster, cursor.fetchone()[1:])

# Simple puts and range gets, with overwrites and flushes between inserts to
# make sure we hit multiple sstables on reads
def range_putget(cluster, cursor, cl="QUORUM"):
    keys = 100

    _put_with_overwrite(cluster, cursor, keys, cl)

    if cluster.version() >= "1.2":
        cursor.execute('SELECT * FROM cf LIMIT 10000000')
    else:
        cursor.execute('SELECT * FROM cf USING CONSISTENCY %s LIMIT 10000000' % cl)
    if cluster.version() >= "1.2":
        assert cursor.rowcount == keys * 100, cursor.rowcount
        for k in xrange(0, keys):
            res = cursor.fetchmany(100)
            _validate_row(cluster, res)
    else:
        assert cursor.rowcount == keys
        for res in cursor:
            res = res[1:] # removing key
            _validate_row(cluster, res)

class since(object):
    def __init__(self, cass_version, max_version=None):
        self.cass_version = LooseVersion(cass_version)
        self.max_version = max_version
        if self.max_version is not None:
            self.max_version = LooseVersion(self.max_version)

    def __call__(self, f):
        def wrapped(obj):
            cluster_version = LooseVersion(obj.cluster.version())
            if cluster_version < self.cass_version:
                obj.skip("%s < %s" % (cluster_version, self.cass_version))
            if self.max_version and \
                    cluster_version[:len(self.max_version)] > self.max_version:
                obj.skip("%s > %s" %(cluster_version, self.max_version)) 
            f(obj)
        wrapped.__name__ = f.__name__
        wrapped.__doc__ = f.__doc__
        return wrapped

from dtest import DISABLE_VNODES
# Use this decorator to skip a test when vnodes are enabled.
class no_vnodes(object):
    def __call__(self, f):
        def wrapped(obj):
            if not DISABLE_VNODES:
                obj.skip("Test disabled for vnodes")
            f(obj)
        wrapped.__name__ = f.__name__
        wrapped.__doc__ = f.__doc__
        return wrapped
    

class require(object):
    def __init__(self, msg):
        self.msg = msg

    def __call__(self, f):
        def wrapped(obj):
            obj.skip("require " + self.msg)
            f(obj)
        wrapped.__name__ = f.__name__
        wrapped.__doc__ = f.__doc__
        return wrapped

def not_implemented(f):
    def wrapped(obj):
        obj.skip("this test not implemented")
        f(obj)
    wrapped.__name__ = f.__name__
    wrapped.__doc__ = f.__doc__
    return wrapped


def replace_in_file(filepath, search_replacements):
    """In-place file search and replace.
    
    filepath - The path of the file to edit 
    search_replacements - a list of tuples (regex, replacement) that
    represent however many search and replace operations you wish to
    perform.

    Note: This does not work with multi-line regexes.
    """
    for line in fileinput.input(filepath, inplace=True):
        for regex, replacement in search_replacements:
            line = re.sub(regex, replacement, line)
        sys.stdout.write(line)

class ThriftConnection(object):
    """
    A thrift connection. For when CQL doesn't do what we need.
    """

    def __init__(self, node=None, host=None, port=None, ks_name='ks', cf_name='cf',
            cassandra_interface='11'):
        """
        initializes the connection.
         - node: a ccm node. If supplied, the host and port, and cassandra_interface
           will be pulled from the node.
         - host, port: overwritten if node is supplied
         - ks_name, cf_name: all operations are done on the supplied ks and cf
         - cassandra_interface: '07' and '11' are currently supported. This is the
           thrift interface to cassandra. '11' suffices for now except when creating
           keyspaces against cassandra0.7, in which case 07 must be used.
        """
        if node:
            host, port = node.network_interfaces['thrift']
            if re.findall('0\.7\.\d+', node.get_cassandra_dir()):
                cassandra_interface='07'
        self.node = node
        self.host = host
        self.port = port
        self.cassandra_interface = cassandra_interface

        # import the correct version of the cassandra thrift interface
        # and set self.Cassandra as the imported module
        module_name = 'cassandra.v%s' % cassandra_interface
        imp = __import__(module_name, globals(), locals(), ['Cassandra'])
        self.Cassandra = imp.Cassandra

        socket = TSocket.TSocket(host, port)
        self.transport = TTransport.TFramedTransport(socket)
        protocol = TBinaryProtocol.TBinaryProtocolAccelerated(self.transport)
        self.client = self.Cassandra.Client(protocol)

        socket.open()
        self.open_socket = True

        self.ks_name = ks_name
        self.cf_name = cf_name


    def create_ks(self, replication_factor=1):
        if self.cassandra_interface == '07':
            ks_def = self.Cassandra.KsDef(name=self.ks_name,
                    strategy_class='org.apache.cassandra.locator.SimpleStrategy',
                    replication_factor=int(replication_factor),
                    cf_defs=[])
        else:
            ks_def = self.Cassandra.KsDef(name=self.ks_name,
                    strategy_class='org.apache.cassandra.locator.SimpleStrategy',
                    strategy_options={'replication_factor': str(replication_factor)},
                    cf_defs=[])
        retry_till_success(self.client.system_add_keyspace, ks_def, timeout=30)
        time.sleep(0.5)
        retry_till_success(self.wait_for_agreement, timeout=10)
        time.sleep(0.5)
        self.use_ks()
        return self


    def use_ks(self):
        retry_till_success(self.client.set_keyspace, self.ks_name, timeout=30)
        return self


    def create_cf(self):
        cf_def = self.Cassandra.CfDef(name=self.cf_name, keyspace=self.ks_name)
        retry_till_success(self.client.system_add_column_family, cf_def, timeout=30)
        time.sleep(0.5)
        retry_till_success(self.wait_for_agreement, timeout=10)
        time.sleep(0.5)
        return self

    def wait_for_agreement(self):
        schemas = self.client.describe_schema_versions()
        if len([ss for ss in schemas.keys() if ss != 'UNREACHABLE']) > 1:
            raise Exception("schema agreement not reached")

    def _translate_cl(self, cl):
        return self.Cassandra.ConsistencyLevel._NAMES_TO_VALUES[cl]


    def insert_columns(self, num_rows=10, consistency_level='QUORUM'):
        """ Insert some basic values """
        cf_parent = self.Cassandra.ColumnParent(column_family=self.cf_name)

        for row_key in ('row_%d'%i for i in xrange(num_rows)):
            col = self.Cassandra.Column(name='col_0', value='val_0',
                    timestamp=int(time.time()*1000))
            retry_till_success(self.client.insert,
                    key=row_key, column_parent=cf_parent, column=col,
                    consistency_level=self._translate_cl(consistency_level),
                    timeout=30)
        return self


    def query_columns(self, num_rows=10, consistency_level='QUORUM'):
        """ Check that the values inserted in insert_columns() are present """
        for row_key in ('row_%d'%i for i in xrange(num_rows)):
            cpath = self.Cassandra.ColumnPath(column_family=self.cf_name,
                    column='col_0')
            cosc = retry_till_success(self.client.get, key=row_key, column_path=cpath,
                    consistency_level=self._translate_cl(consistency_level),
                    timeout=30)
            col = cosc.column
            value = col.value
            assert value == 'val_0', "column did not have the same value that was inserted!"
        return self



########NEW FILE########
__FILENAME__ = topology_test
from dtest import Tester
from tools import *
from assertions import *

import os, sys, time
from ccmlib.cluster import Cluster

class TestTopology(Tester):

    @no_vnodes()
    def movement_test(self):
        cluster = self.cluster

        # Create an unbalanced ring
        cluster.populate(3, tokens=[0, 2**48, 2**62]).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', columns={'c1': 'text', 'c2': 'text'})

        for n in xrange(0, 10000):
            insert_c1c2(cursor, n, "ONE")

        cluster.flush()

        # Move nodes to balance the cluster
        balancing_tokens = cluster.balanced_tokens(3)
        if cluster.version() >= '1.2':
            escformat = '\\%s'
            if cluster.version() >= '2.1':
                escformat = '%s'
            node1.move(escformat % balancing_tokens[0]) # can't assume 0 is balanced with m3p
            node2.move(escformat % balancing_tokens[1])
            node3.move(escformat % balancing_tokens[2])
        else:
            node2.move(balancing_tokens[1])
            node3.move(balancing_tokens[2])
        time.sleep(1)

        cluster.cleanup()

        # Check we can get all the keys
        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "ONE")

        # Now the load should be basically even
        sizes = [ node.data_size() for node in [node1, node2, node3] ]

        assert_almost_equal(sizes[0], sizes[1])
        assert_almost_equal(sizes[0], sizes[2])
        assert_almost_equal(sizes[1], sizes[2])

    @no_vnodes()
    def decomission_test(self):
        cluster = self.cluster

        tokens = cluster.balanced_tokens(4)
        cluster.populate(4, tokens=tokens).start()
        [node1, node2, node3, node4] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 2)
        self.create_cf(cursor, 'cf',columns={'c1': 'text', 'c2': 'text'})

        for n in xrange(0, 10000):
            insert_c1c2(cursor, n, "QUORUM")

        cluster.flush()
        sizes = [ node.data_size() for node in cluster.nodelist() if node.is_running()]
        init_size = sizes[0]
        assert_almost_equal(*sizes)

        time.sleep(.5)
        node4.decommission()
        node4.stop()
        cluster.cleanup()
        time.sleep(.5)

        # Check we can get all the keys
        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "QUORUM")

        sizes = [ node.data_size() for node in cluster.nodelist() if node.is_running() ]
        three_node_sizes = sizes
        assert_almost_equal(sizes[0], sizes[1])
        assert_almost_equal((2.0/3.0) * sizes[0], sizes[2])
        assert_almost_equal(sizes[2], init_size)

        if cluster.version() <= '1.2':
            node3.stop(wait_other_notice=True)
            node1.removeToken(tokens[2])
            time.sleep(.5)
            cluster.cleanup()
            time.sleep(.5)

            # Check we can get all the keys
            for n in xrange(0, 10000):
                query_c1c2(cursor, n, "QUORUM")

            sizes = [ node.data_size() for node in cluster.nodelist() if node.is_running() ]
            assert_almost_equal(*sizes)
            assert_almost_equal(sizes[0], 2 * init_size)

            node5 = new_node(cluster, token=(tokens[2]+1)).start()
            time.sleep(.5)
            cluster.cleanup()
            time.sleep(.5)
            cluster.compact()
            time.sleep(.5)

            # Check we can get all the keys
            for n in xrange(0, 10000):
                query_c1c2(cursor, n, "QUORUM")

            sizes = [ node.data_size() for node in cluster.nodelist() if node.is_running() ]
            # We should be back to the earlir 3 nodes situation
            for i in xrange(0, len(sizes)):
                assert_almost_equal(sizes[i], three_node_sizes[i])

    @no_vnodes()
    def replace_test(self):
        cluster = self.cluster

        tokens = cluster.balanced_tokens(3)
        cluster.populate(3, tokens=tokens).start()
        [node1, node2, node3] = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 3)
        self.create_cf(cursor, 'cf', columns={'c1': 'text', 'c2': 'text'})

        for n in xrange(0, 10000):
            insert_c1c2(cursor, n, "QUORUM")

        cluster.flush()

        node3.stop(wait_other_notice=True)
        time.sleep(.5)

        node4 = new_node(cluster, token=tokens[2])
        node4.start(replace_token=tokens[2])
        time.sleep(.5)
        cluster.cleanup()
        time.sleep(.5)

        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "QUORUM")

        sizes = [ node.data_size() for node in cluster.nodelist() if node.is_running()]
        assert_almost_equal(*sizes)

    @no_vnodes()
    def move_single_node_test(self):
        """ Test moving a node in a single-node cluster (#4200) """
        cluster = self.cluster

        # Create an unbalanced ring
        cluster.populate(1, tokens=[0]).start()
        node1 = cluster.nodelist()[0]
        time.sleep(0.2)

        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'ks', 1)
        self.create_cf(cursor, 'cf', columns={'c1': 'text', 'c2': 'text'})

        for n in xrange(0, 10000):
            insert_c1c2(cursor, n, "ONE")

        cluster.flush()

        node1.move(2**25)
        time.sleep(1)

        cluster.cleanup()

        # Check we can get all the keys
        for n in xrange(0, 10000):
            query_c1c2(cursor, n, "ONE")

########NEW FILE########
__FILENAME__ = upgrade_supercolumns_test
from dtest import Tester
from tools import *
from assertions import *
from ccmlib.cluster import Cluster
import random

# Tests upgrade between 1.2->2.0 for super columns (since that's where
# we removed then internally)
class TestSCUpgrade(Tester):

    def upgrade_with_index_creation_test(self):
        cluster = self.cluster

        # Forcing cluster version on purpose
        cluster.set_cassandra_dir(cassandra_version="1.2.16")
        cluster.populate(2).start()

        [node1, node2] = cluster.nodelist()

        cli = node1.cli()
        cli.do("create keyspace test with placement_strategy = 'SimpleStrategy' and strategy_options = {replication_factor : 2} and durable_writes = true")
        cli.do("use test")
        cli.do("create column family sc_test with column_type = 'Super' and comparator = 'UTF8Type' and subcomparator = 'UTF8Type' and default_validation_class = 'UTF8Type' and key_validation_class = 'UTF8Type'")

        for i in range(0, 2):
            for j in range(0, 2):
                cli.do("set sc_test['k0']['sc%d']['c%d'] = 'v'" % (i, j))

        assert not cli.has_errors(), cli.errors()
        cli.close()

        # Upgrade node 1
        node1.flush()
        time.sleep(.5)
        node1.stop(wait_other_notice=True)
        self.set_node_to_current_version(node1)
        node1.start(wait_other_notice=True)
        time.sleep(.5)

        cli = node1.cli()
        cli.do("use test")
        cli.do("consistencylevel as quorum")

        # Check we can still get data properly
        cli.do("get sc_test['k0']")
        assert_scs(cli, ['sc0', 'sc1'])
        assert_columns(cli, ['c0', 'c1'])

        cli.do("get sc_test['k0']['sc1']")
        assert_columns(cli, ['c0', 'c1'])

        cli.do("get sc_test['k0']['sc1']['c1']")
        assert_columns(cli, ['c1'])


def assert_scs(cli, names):
    assert not cli.has_errors(), cli.errors()
    output = cli.last_output()

    for name in names:
        assert re.search('super_column=%s' % name, output) is not None, 'Cannot find super column %s in %s' % (name, output)

def assert_columns(cli, names):
    assert not cli.has_errors(), cli.errors()
    output = cli.last_output()

    for name in names:
        assert re.search('name=%s' % name, output) is not None, 'Cannot find column %s in %s' % (name, output)

########NEW FILE########
__FILENAME__ = upgrade_through_versions_test
import bisect, os, random, re, subprocess, time, uuid, unittest
from collections import defaultdict
from distutils.version import LooseVersion
from cql import OperationalError
from dtest import Tester, debug, DISABLE_VNODES, DEFAULT_DIR
from tools import new_node, not_implemented

TRUNK_VER = '2.2'

# Used to build upgrade path(s) for tests. Some tests will go from start to finish,
# other tests will focus on single upgrades from UPGRADE_PATH[n] to UPGRADE_PATH[n+1]
# Note that these strings should match git branch names, and will be used to search for
# tags which are related to a particular branch as well.
UPGRADE_PATH = ['cassandra-1.1', 'cassandra-1.2', 'cassandra-2.0', 'cassandra-2.1', 'trunk']


class GitSemVer(object):
    """
    Wraps a git ref up with a semver (as LooseVersion)
    """
    git_ref = None
    semver = None
    
    def __init__(self, git_ref, semver_str):
        self.git_ref = 'git:' + git_ref
        self.semver = LooseVersion(semver_str)
        if semver_str == 'trunk':
            self.semver = LooseVersion(TRUNK_VER)
    
    def __cmp__(self, other):
        return cmp(self.semver, other.semver)

def latest_tag_matching(match_string='cassandra-1.1'):
    """
    Returns the latest tag matching match_string*
    """
    git_path = os.environ.get('CASSANDRA_DIR', DEFAULT_DIR)
    
    tags = subprocess.check_output(
        ["git", "tag", "-l", "{search}*".format(search=match_string)], cwd=git_path)\
        .rstrip()\
        .split('\n')
    
    wrappers = []
    for t in tags:
        match = re.match('^cassandra-(\d+\.\d+\.\d+(-+\w+)*)$', t)
        if match:
            gsv = GitSemVer(t, match.group(1))
            bisect.insort(wrappers, gsv)
            
    if wrappers:
        return wrappers.pop().git_ref
    return None

def get_version_from_tag(tag):
    if tag == 'trunk':
        return TRUNK_VER
    
    match = re.match('^(git:)*cassandra-(\d+\.\d+\.*\d*(-+\w+)*)$', tag)
    if match:
        return match.group(2)
    return None

def get_version_from_build():
    path = os.environ.get('CASSANDRA_DIR', DEFAULT_DIR)
    
    build = os.path.join(path, 'build.xml')
    with open(build) as f:
        for line in f:
            match = re.search('name="base\.version" value="([0-9.]+)[^"]*"', line)
            if match:
                return LooseVersion(match.group(1))


class TestUpgradeThroughVersions(Tester):
    """
    Upgrades a 3-node Murmur3Partitioner cluster through versions specified in test_versions.
    """
    test_versions = None # set on init to know which versions to use
    
    def __init__(self, *args, **kwargs):
        # Ignore these log patterns:
        self.ignore_log_patterns = [
            # This one occurs if we do a non-rolling upgrade, the node
            # it's trying to send the migration to hasn't started yet,
            # and when it does, it gets replayed and everything is fine.
            r'Can\'t send migration request: node.*is down',
        ]
        
        # Force cluster options that are common among versions:
        kwargs['cluster_options'] = {'partitioner':'org.apache.cassandra.dht.Murmur3Partitioner'}
        Tester.__init__(self, *args, **kwargs)

    @property
    def test_versions(self):
        # Murmur was not present until 1.2+
        return ['git:'+v for v in UPGRADE_PATH if get_version_from_tag(v) >= '1.2']

    def setUp(self):
        # Forcing cluster version on purpose
        os.environ['CASSANDRA_VERSION'] = self.test_versions[0]
        debug("Versions to test (%s): %s" % (type(self), str([v for v in self.test_versions])))
        super(TestUpgradeThroughVersions, self).setUp()

    def upgrade_test(self):
        self.upgrade_scenario()

    def upgrade_test_mixed(self):
        """Only upgrade part of the cluster, so we have mixed versions part way through."""
        self.upgrade_scenario(mixed_version=True)

    def upgrade_scenario(self, populate=True, create_schema=True, mixed_version=False, after_upgrade_call=()):
        # Record the rows we write as we go:
        self.row_values = set()
        cluster = self.cluster
        
        if populate:
            # Start with 3 node cluster
            debug('Creating cluster (%s)' % self.test_versions[0])
            cluster.populate(3)
            [node.start(use_jna=True) for node in cluster.nodelist()]
        else:
            debug("Skipping cluster creation (should already be built)")

        # add nodes to self for convenience
        for i, node in enumerate(cluster.nodelist(), 1):
            node_name = 'node'+str(i)
            setattr(self, node_name, node)
        
        if create_schema:
            self._create_schema()
        else:
            debug("Skipping schema creation (should already be built)")
        time.sleep(5) #sigh...
            
        self._log_current_ver(self.test_versions[0])
        
        # upgrade through versions
        for tag in self.test_versions[1:]:
            if mixed_version:
                for num, node in enumerate(self.cluster.nodelist()):
                    # do a write and check for each new node as upgraded
                    self._write_values()
                    self._increment_counters()
                    
                    self.upgrade_to_version(tag, mixed_version=True, nodes=(node,))
                    
                    self._check_values()
                    self._check_counters()
                    
                    debug('Successfully upgraded %d of %d nodes to %s' % 
                          (num+1, len(self.cluster.nodelist()), tag))
            else:
                self._write_values()
                self._increment_counters()
                
                self.upgrade_to_version(tag)
                
                self._check_values()
                self._check_counters()
                
            # run custom post-upgrade callables
            for call in after_upgrade_call:
                call()
                
            debug('All nodes successfully upgraded to %s' % tag)
            self._log_current_ver(tag)
            
        cluster.stop()

    def upgrade_to_version(self, tag, mixed_version=False, nodes=None):
        """Upgrade Nodes - if *mixed_version* is True, only upgrade those nodes
        that are specified by *nodes*, otherwise ignore *nodes* specified
        and upgrade all nodes.
        """
        debug('Upgrading to ' + tag)
        if not mixed_version:
            nodes = self.cluster.nodelist()
        
        for node in nodes:
            debug('Shutting down node: ' + node.name)
            node.drain()
            node.watch_log_for("DRAINED")
            node.stop(wait_other_notice=False)

        # Update Cassandra Directory
        for node in nodes:
            node.set_cassandra_dir(cassandra_version=tag)
            debug("Set new cassandra dir for %s: %s" % (node.name, node.get_cassandra_dir()))
        self.cluster.set_cassandra_dir(cassandra_version=tag)

        # Restart nodes on new version
        for node in nodes:
            debug('Starting %s on new version (%s)' % (node.name, tag))
            # Setup log4j / logback again (necessary moving from 2.0 -> 2.1):
            node.set_log_level("INFO")
            node.start(wait_other_notice=True)
            node.nodetool('upgradesstables upgrade cf countertable')
    
    def _log_current_ver(self, current_tag):
        """
        Logs where we currently are in the upgrade path, surrounding the current branch/tag, like ***sometag***
        """
        vers = self.test_versions
        curr_index = vers.index(current_tag)
        debug(
            "Current upgrade path: {}".format(
                vers[:curr_index] + ['***'+current_tag+'***'] + vers[curr_index+1:]))
    
    def _create_schema(self):
        cursor = self.patient_cql_connection(self.node2).cursor()
        
        if self.cluster.version() >= '1.2':
            #DDL for C* 1.2+
            cursor.execute("""CREATE KEYSPACE upgrade WITH replication = {'class':'SimpleStrategy', 
                'replication_factor':2};
                """)
        else:
            # DDL for C* 1.1
            cursor.execute("""CREATE KEYSPACE upgrade WITH strategy_class = 'SimpleStrategy' 
            AND strategy_options:replication_factor = 2;
            """)

        cursor.execute('use upgrade')
        cursor.execute('CREATE TABLE cf ( k int PRIMARY KEY , v text )')
        cursor.execute('CREATE INDEX vals ON cf (v)')
        
        if self.cluster.version() >= '1.2':
            cursor.execute("""
                CREATE TABLE countertable (k text PRIMARY KEY, c counter);""")
        else:
            cursor.execute("""
                CREATE TABLE countertable (k text PRIMARY KEY, c counter)
                WITH default_validation=CounterColumnType;""")

    def _write_values(self, num=100):
        cursor = self.patient_cql_connection(self.node2).cursor()
        cursor.execute("use upgrade")
        for i in xrange(num):
            x = len(self.row_values) + 1
            cursor.execute("UPDATE cf SET v='%d' WHERE k=%d" % (x,x))
            self.row_values.add(x)

    def _check_values(self, consistency_level='ALL'):
        for node in self.cluster.nodelist():
            cursor = self.patient_cql_connection(node).cursor()
            cursor.execute("use upgrade")
            for x in self.row_values:
                cursor.execute("SELECT k,v FROM cf WHERE k=%d" % x, consistency_level=consistency_level)
                k,v = cursor.fetchone()
                self.assertEqual(x, k)
                self.assertEqual(str(x), v)

    def _increment_counters(self, seconds=15):
        debug("incrementing counter for {time} seconds".format(time=seconds))
        cursor = self.patient_cql_connection(self.node2).cursor()
        cursor.execute("use upgrade;")
        
        update_counter_query = ("UPDATE countertable SET c = c + 1 WHERE k='{key}'")
        
        uuids = [uuid.uuid4() for i in range(100)]
        self.expected_counts = defaultdict(int)
        
        expiry=time.time()+seconds
        while time.time() < expiry:
            counter_key = random.choice(uuids)
            
            try:
                cursor.execute( update_counter_query.format(key=counter_key) )
            except OperationalError:
                pass
            else:
                self.expected_counts[counter_key] += 1
            
        # make sure 100 succeeded
        assert sum(self.expected_counts.values()) > 100

    def _check_counters(self, consistency_level='ALL'):
        debug("Checking counter values...")
        cursor = self.patient_cql_connection(self.node2).cursor()
        cursor.execute("use upgrade;")
        
        for counter_key, value in self.expected_counts.items():
            cursor.execute("SELECT c from countertable where k='{key}';".format(key=counter_key), consistency_level=consistency_level)
            res = cursor.fetchone()[0]
            assert res == value, "Counter not at expected value."


class TestRandomPartitionerUpgrade(TestUpgradeThroughVersions):
    """
    Upgrades a 3-node RandomPartitioner cluster through versions specified in test_versions.
    """
    def __init__(self, *args, **kwargs):
        # Ignore these log patterns:
        self.ignore_log_patterns = [
            # This one occurs if we do a non-rolling upgrade, the node
            # it's trying to send the migration to hasn't started yet,
            # and when it does, it gets replayed and everything is fine.
            r'Can\'t send migration request: node.*is down',
        ]
        
        # Force cluster options that are common among versions:
        kwargs['cluster_options'] = {'partitioner':'org.apache.cassandra.dht.RandomPartitioner'}
        Tester.__init__(self, *args, **kwargs)

    @property
    def test_versions(self):
        return ['git:'+v for v in UPGRADE_PATH]


class PointToPointUpgradeBase(TestUpgradeThroughVersions):
    """
    Base class for testing a single upgrade (ver1->ver2).
    
    We are dynamically creating subclasses of this for testing point upgrades, so this is a convenient
    place to add functionality/tests for those subclasses to run.
    
    __test__ is False for this class. Subclasses need to revert to True to run tests!
    """
    __test__ = False

    def setUp(self):
        # Forcing cluster version on purpose
        os.environ['CASSANDRA_VERSION'] = self.test_versions[0]
        
        super(TestUpgradeThroughVersions, self).setUp()
        
        # if this is a shuffle test, we want to specifically disable vnodes initially
        # so that we can enable them later and do shuffle
        if self.id().split('.')[-1] in ('shuffle_test', 'shuffle_multidc_test'):
            debug("setting custom cluster config for shuffle_test")
            if self.cluster.version() >= "1.2":
                self.cluster.set_configuration_options(values={'num_tokens': None})
            
        debug("Versions to test (%s): %s" % (type(self), str([v for v in self.test_versions])))
    
    def _bootstrap_new_node(self):
        # Check we can bootstrap a new node on the upgraded cluster:
        debug("Adding a node to the cluster")
        nnode = new_node(self.cluster, remote_debug_port=str(2000+len(self.cluster.nodes)))
        nnode.start(use_jna=True, wait_other_notice=True)
        self._write_values()
        self._increment_counters()
        self._check_values()
        self._check_counters()
    
    def _bootstrap_new_node_multidc(self):
        # Check we can bootstrap a new node on the upgraded cluster:
        debug("Adding a node to the cluster")
        nnode = new_node(self.cluster, remote_debug_port=str(2000+len(self.cluster.nodes)), data_center='dc2')
        
        nnode.start(use_jna=True, wait_other_notice=True)
        self._write_values()
        self._increment_counters()
        self._check_values()
        self._check_counters()
    
    def _migrate_to_vnodes(self):
        if not DISABLE_VNODES and self.cluster.version() >= '1.2':
            for node in self.cluster.nodelist():
                debug('Shutting down node: ' + node.name)
                node.drain()
                node.watch_log_for("DRAINED")
                node.stop(wait_other_notice=False)

            debug("moving cluster to vnodes")
            self.cluster.set_configuration_options(values={'initial_token': None, 'num_tokens': 10})
            
            # just a hacky way to get the topology set again, since it seems to get lost
            self.cluster.set_cassandra_dir(cassandra_dir=self.cluster.get_cassandra_dir())

            # Restart nodes on new version
            for node in self.cluster.nodelist():
                # Setup log4j / logback again (necessary moving from 2.0 -> 2.1):
                node.set_log_level("INFO")
                node.start(wait_other_notice=True)
        
            debug("Running shuffle")            
            mark = self.node1.mark_log()
            self.node1.shuffle("create")
            self.node1.shuffle("enable")
            self.node1.watch_log_for("Pausing until token count stabilizes", from_mark=mark, timeout=60)
        else:
            debug("Not migrating to vnodes because they are disabled or cluster is not above v1.2")
    
    @unittest.skipIf(DISABLE_VNODES, "vnodes disabled for this test run")
    def shuffle_test(self):
        # go from non-vnodes to vnodes, and run shuffle to distribute the data.
        self.upgrade_scenario(
            after_upgrade_call=(self._migrate_to_vnodes, self._check_values, self._check_counters))
    
    def bootstrap_test(self):
        # try and add a new node
        self.upgrade_scenario(after_upgrade_call=(self._bootstrap_new_node,))
    
    @unittest.skipIf(DISABLE_VNODES, "vnodes disabled for this test run")
    def shuffle_multidc_test(self):
        # go from non-vnodes to vnodes, and run shuffle to distribute the data.
        # multi dc, 2 nodes in each dc
        self.cluster.populate([2,2])
        [node.start(use_jna=True) for node in self.cluster.nodelist()]
        self._multidc_schema_create()
        self.upgrade_scenario(populate=False, create_schema=False,
            after_upgrade_call=(self._migrate_to_vnodes, self._check_values, self._check_counters))
    
    def bootstrap_multidc_test(self):
        # try and add a new node
        # multi dc, 2 nodes in each dc
        self.cluster.populate([2,2])
        [node.start(use_jna=True) for node in self.cluster.nodelist()]
        self._multidc_schema_create()
        self.upgrade_scenario(populate=False, create_schema=False, after_upgrade_call=(self._bootstrap_new_node_multidc,))

    def _multidc_schema_create(self):
        cursor = self.patient_cql_connection(self.cluster.nodelist()[0]).cursor()
        
        if self.cluster.version() >= '1.2':
            #DDL for C* 1.2+
            cursor.execute("""CREATE KEYSPACE upgrade WITH replication = {'class':'NetworkTopologyStrategy', 
                'dc1':1, 'dc2':1};
                """)
        else:
            # DDL for C* 1.1
            cursor.execute("""CREATE KEYSPACE upgrade WITH strategy_class = 'NetworkTopologyStrategy' 
            AND strategy_options:'dc1':1
            AND strategy_options:'dc2':1;
            """)

        cursor.execute('use upgrade')
        cursor.execute('CREATE TABLE cf ( k int PRIMARY KEY , v text )')
        cursor.execute('CREATE INDEX vals ON cf (v)')
        
        if self.cluster.version() >= '1.2':
            cursor.execute("""
                CREATE TABLE countertable (k text PRIMARY KEY, c counter);""")
        else:
            cursor.execute("""
                CREATE TABLE countertable (k text PRIMARY KEY, c counter)
                WITH default_validation=CounterColumnType;""")


# create test classes for upgrading from latest tag on branch to the head of that same branch
for from_ver in UPGRADE_PATH:
    # we only want to do single upgrade tests for 1.2+
    # and trunk is the final version, so there's no test where trunk is upgraded to something else
    if get_version_from_tag(from_ver) >= '1.2' and from_ver != 'trunk':
        cls_name = ('TestUpgrade_from_'+from_ver+'_latest_tag_to_'+from_ver+'_HEAD').replace('-', '_').replace('.', '_')
        debug('Creating test upgrade class: {}'.format(cls_name))
        vars()[cls_name] = type(
            cls_name,
            (PointToPointUpgradeBase,),
            {'test_versions': [latest_tag_matching(from_ver), 'git:'+from_ver,], '__test__':True})

# build a list of tuples like so:
# [(A, B), (B, C) ... ]
# each pair in the list represents an upgrade test (A, B)
# where we will upgrade from the latest *tag* matching A, to the HEAD of branch B
POINT_UPGRADES = []
points = [v for v in UPGRADE_PATH if get_version_from_tag(v) >= '1.2']
for i, _ in enumerate(points):
    verslice = tuple(points[i:i+2])
    if len(verslice) == 2: # exclude dangling version at end
        POINT_UPGRADES.append( tuple(points[i:i+2]) )

# create test classes for upgrading from latest tag on one branch, to head of the next branch (see comment above)
for (from_ver, to_branch) in POINT_UPGRADES:
    cls_name = ('TestUpgrade_from_'+from_ver+'_latest_tag_to_'+to_branch+'_HEAD').replace('-', '_').replace('.', '_')
    debug('Creating test upgrade class: {}'.format(cls_name))
    vars()[cls_name] = type(
        cls_name,
        (PointToPointUpgradeBase,),
        {'test_versions': [latest_tag_matching(from_ver), 'git:'+to_branch,], '__test__':True})

# create test classes for upgrading from HEAD of one branch to HEAD of next.
for (from_branch, to_branch) in POINT_UPGRADES:
    cls_name = ('TestUpgrade_from_'+from_branch+'_HEAD_to_'+to_branch+'_HEAD').replace('-', '_').replace('.', '_')
    debug('Creating test upgrade class: {}'.format(cls_name))
    vars()[cls_name] = type(
        cls_name,
        (PointToPointUpgradeBase,),
        {'test_versions': ['git:'+from_branch, 'git:'+to_branch,], '__test__':True})

# create test classes for upgrading from HEAD of one branch, to latest tag of next branch
for (from_branch, to_branch) in POINT_UPGRADES:
    cls_name = ('TestUpgrade_from_'+from_branch+'_HEAD_to_'+to_branch+'_latest_tag').replace('-', '_').replace('.', '_')
    debug('Creating test upgrade class: {}'.format(cls_name))
    
    # in some cases we might not find a tag (like when the to_branch is trunk)
    # so these will be skipped.
    if latest_tag_matching(to_branch) is None:
        continue
    
    vars()[cls_name] = type(
        cls_name,
        (PointToPointUpgradeBase,),
        {'test_versions': ['git:'+from_branch, latest_tag_matching(to_branch),], '__test__':True})

########NEW FILE########
__FILENAME__ = user_types
import os
import datetime
import random
import struct
import time
import uuid
from cql import ProgrammingError
from dtest import Tester, debug
from tools import since


def decode_text(string):
    """
    decode bytestring as utf-8
    """
    return string.decode('utf-8')

def len_unpacker(val):
    return struct.Struct('>i').unpack(val)[0]

def unpack(bytestr):
    # The composite format for each component is:
    #   <len>   <value>
    # 4 bytes | <len> bytes
    components = []
    while bytestr:
        length = len_unpacker(bytestr[:4])
        if length < 0:
            components.append(None)
            bytestr = bytestr[4:]
        else:
            components.append(decode_text(bytestr[4:4 + length]))
            bytestr = bytestr[4 + length:]
    return tuple(components)

def decode(item):
    """
    decode a query result consisting of user types
    
    returns nested arrays representing user type ordering
    """
    decoded = []
       
    if isinstance(item, tuple) or isinstance(item, list):
        for i in item:
            decoded.extend(decode(i))
    else:
        if item is not None and item.startswith('\x00'):
            unpacked = unpack(item)
            decoded.append(decode(unpacked))
        else:
            decoded.append(item)
    
    return decoded


class TestUserTypes(Tester):

    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

    # We've removed type renaming for now (CASSANDRA-6940)
    #@since('2.1')
    #def test_type_renaming(self):
    #  """
    #  Confirm that types can be renamed and the proper associations are updated.
    #  """
    #  cluster = self.cluster
    #  cluster.populate(3).start()
    #  node1, node2, node3 = cluster.nodelist()
    #  cursor = self.patient_cql_connection(node1).cursor()
    #  self.create_ks(cursor, 'user_type_renaming', 2)

    #  stmt = """
    #        CREATE TYPE simple_type (
    #        user_number int
    #        )
    #     """
    #  cursor.execute(stmt)

    #  stmt = """
    #        CREATE TABLE simple_table (
    #        id uuid PRIMARY KEY,
    #        number simple_type
    #        )
    #     """
    #  cursor.execute(stmt)

    #  stmt = """
    #      ALTER TYPE simple_type rename to renamed_type;
    #     """
    #  cursor.execute(stmt)

    #  stmt = """
    #      SELECT type_name from system.schema_usertypes;
    #     """
    #  cursor.execute(stmt)
    #  # we should only have one user type in this test
    #  self.assertEqual(1, cursor.rowcount)

    #  # finally let's look for the new type name
    #  self.assertEqual(cursor.fetchone()[0], u'renamed_type')

    #@since('2.1')
    #def test_nested_type_renaming(self):
    #    """
    #    Confirm type renaming works as expected on nested types.
    #    """
    #    cluster = self.cluster
    #    cluster.populate(3).start()
    #    node1, node2, node3 = cluster.nodelist()
    #    cursor = self.patient_cql_connection(node1).cursor()
    #    self.create_ks(cursor, 'nested_user_type_renaming', 2)

    #    stmt = """
    #          USE nested_user_type_renaming
    #       """
    #    cursor.execute(stmt)

    #    stmt = """
    #          CREATE TYPE simple_type (
    #          user_number int,
    #          user_text text
    #          )
    #       """
    #    cursor.execute(stmt)

    #    stmt = """
    #          CREATE TYPE another_type (
    #          somefield simple_type
    #          )
    #       """
    #    cursor.execute(stmt)

    #    stmt = """
    #          CREATE TYPE yet_another_type (
    #          some_other_field another_type
    #          )
    #       """
    #    cursor.execute(stmt)

    #    stmt = """
    #          CREATE TABLE uses_nested_type (
    #          id uuid PRIMARY KEY,
    #          field_name yet_another_type
    #          )
    #       """
    #    cursor.execute(stmt)

    #    # let's insert some basic data using the nested types
    #    _id = uuid.uuid4()
    #    stmt = """
    #          INSERT INTO uses_nested_type (id, field_name)
    #          VALUES (%s, {some_other_field: {somefield: {user_number: 1, user_text: 'original'}}});
    #       """ % _id
    #    cursor.execute(stmt)

    #    # rename one of the types used in the nesting
    #    stmt = """
    #          ALTER TYPE another_type rename to another_type2;
    #       """
    #    cursor.execute(stmt)

    #    # confirm nested data can be queried without error
    #    stmt = """
    #          SELECT field_name FROM uses_nested_type where id = {id}
    #       """.format(id=_id)
    #    cursor.execute(stmt)

    #    data = cursor.fetchone()[0]
    #    self.assertIn('original', data)

    #    # confirm we can alter/query the data after altering the type
    #    stmt = """
    #          UPDATE uses_nested_type
    #          SET field_name = {some_other_field: {somefield: {user_number: 2, user_text: 'altered'}}}
    #          WHERE id=%s;
    #       """ % _id
    #    cursor.execute(stmt)

    #    stmt = """
    #          SELECT field_name FROM uses_nested_type where id = {id}
    #       """.format(id=_id)
    #    cursor.execute(stmt)

    #    data = cursor.fetchone()[0]
    #    self.assertIn('altered', data)

    #    # and confirm we can add/query new data after the type rename
    #    _id = uuid.uuid4()
    #    stmt = """
    #          INSERT INTO uses_nested_type (id, field_name)
    #          VALUES (%s, {some_other_field: {somefield: {user_number: 1, user_text: 'inserted'}}});
    #       """ % _id
    #    cursor.execute(stmt)

    #    stmt = """
    #          SELECT field_name FROM uses_nested_type where id = {id}
    #       """.format(id=_id)
    #    cursor.execute(stmt)

    #    data = cursor.fetchone()[0]
    #    self.assertIn('inserted', data)

    @since('2.1')
    def test_type_dropping(self):
        """
        Tests that a type cannot be dropped when in use, and otherwise can be dropped.
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_type_dropping', 2)

        stmt = """
              USE user_type_dropping
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TYPE simple_type (
              user_number int
              )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TABLE simple_table (
              id uuid PRIMARY KEY,
              number simple_type
              )
           """
        cursor.execute(stmt)
        # Make sure the scheam propagate
        time.sleep(2)

        _id = uuid.uuid4()
        stmt = """
              INSERT INTO simple_table (id, number)
              VALUES ({id}, {{user_number: 1}});
           """.format(id=_id)
        cursor.execute(stmt)

        stmt = """
              DROP TYPE simple_type;
           """
        with self.assertRaisesRegexp(ProgrammingError, 'Cannot drop user type user_type_dropping.simple_type as it is still used by table user_type_dropping.simple_table'):
            cursor.execute(stmt)

        # now that we've confirmed that a user type cannot be dropped while in use
        # let's remove the offending table

        # TODO: uncomment below after CASSANDRA-6472 is resolved
        # and add another check to make sure the table/type drops succeed
        stmt = """
              DROP TABLE simple_table;
           """.format(id=_id)        
        cursor.execute(stmt)
        
        stmt = """
              DROP TYPE simple_type;
           """
        cursor.execute(stmt)
        
        # now let's have a look at the system schema and make sure no user types are defined
        stmt = """
              SELECT type_name from system.schema_usertypes;
           """
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

    @since('2.1')
    def test_nested_type_dropping(self):
        """
        Confirm a user type can't be dropped when being used by another user type. 
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'nested_user_type_dropping', 2)

        stmt = """
              USE nested_user_type_dropping
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TYPE simple_type (
              user_number int,
              user_text text
              )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TYPE another_type (
              somefield simple_type
              )
           """
        cursor.execute(stmt)

        stmt = """
              DROP TYPE simple_type;
           """
        with self.assertRaisesRegexp(ProgrammingError, 'Cannot drop user type nested_user_type_dropping.simple_type as it is still used by user type another_type'):
            cursor.execute(stmt)

        # drop the type that's impeding the drop, and then try again
        stmt = """
              DROP TYPE another_type;
           """
        cursor.execute(stmt)

        stmt = """
              DROP TYPE simple_type;
           """
        cursor.execute(stmt)

        # now let's have a look at the system schema and make sure no user types are defined
        stmt = """
              SELECT type_name from system.schema_usertypes;
           """
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

    @since('2.1')
    def test_type_enforcement(self):
        """
        Confirm error when incorrect data type used for user type
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        cursor = self.cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_type_enforcement', 2)

        stmt = """
              USE user_type_enforcement
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TYPE simple_type (
              user_number int
              )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TABLE simple_table (
              id uuid PRIMARY KEY,
              number simple_type
              )
           """
        cursor.execute(stmt)
        # Make sure the scheam propagate
        time.sleep(2)

        # here we will attempt an insert statement which should fail
        # because the user type is an int, but the insert statement is
        # providing text
        _id = uuid.uuid4()
        stmt = """
              INSERT INTO simple_table (id, number)
              VALUES ({id}, {{user_number: 'uh oh....this is not a number'}});
           """.format(id=_id)
        with self.assertRaisesRegexp(ProgrammingError, 'field user_number is not of type int'):
            cursor.execute(stmt)

        # let's check the rowcount and make sure the data
        # didn't get inserted when the exception asserted above was thrown
        stmt = """
              SELECT * FROM simple_table;
           """
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

    @since('2.1')
    def test_nested_user_types(self):
        """Tests user types within user types"""
        cluster = self.cluster
        cluster.populate(3).start()
        node1,node2,node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_types', 2)

        stmt = """
              USE user_types
           """
        cursor.execute(stmt)

        #### Create a user type to go inside another one:
        stmt = """
              CREATE TYPE item (
              sub_one text,
              sub_two text,
              )
           """
        cursor.execute(stmt)

        #### Create a user type to contain the item:
        stmt = """
              CREATE TYPE container (
              stuff text,
              more_stuff item
              )
           """
        cursor.execute(stmt)

        ### Create a table that holds and item, a container, and a
        ### list of containers:
        stmt = """
              CREATE TABLE bucket (
               id uuid PRIMARY KEY,
               primary_item item,
               other_items container,
               other_containers list<container>
              )
           """
        cursor.execute(stmt)
        # Make sure the scheam propagate
        time.sleep(2)

        ### Insert some data:
        _id = uuid.uuid4()
        stmt = """
              INSERT INTO bucket (id, primary_item)
              VALUES ({id}, {{sub_one: 'test', sub_two: 'test2'}});
           """.format(id=_id)
        cursor.execute(stmt)

        stmt = """
              UPDATE bucket
              SET other_items = {{stuff: 'stuff', more_stuff: {{sub_one: 'one', sub_two: 'two'}}}}
              WHERE id={id};
           """.format(id=_id)
        cursor.execute(stmt)

        stmt = """
              UPDATE bucket
              SET other_containers = other_containers + [{{stuff: 'stuff2', more_stuff: {{sub_one: 'one_other', sub_two: 'two_other'}}}}]
              WHERE id={id};
           """.format(id=_id)
        cursor.execute(stmt)

        stmt = """
              UPDATE bucket
              SET other_containers = other_containers + [{{stuff: 'stuff3', more_stuff: {{sub_one: 'one_2_other', sub_two: 'two_2_other'}}}}, {{stuff: 'stuff4', more_stuff: {{sub_one: 'one_3_other', sub_two: 'two_3_other'}}}}]
              WHERE id={id};
           """.format(id=_id)
        cursor.execute(stmt)

        stmt = """
              SELECT primary_item, other_items, other_containers from bucket where id={id};
           """.format(id=_id)
        cursor.execute(stmt)
        
        primary_item, other_items, other_containers = cursor.fetchone()
        
        self.assertEqual(decode(primary_item), [[u'test', u'test2']])
        self.assertEqual(decode(other_items), [[u'stuff', [u'one', u'two']]])
        self.assertEqual(decode(other_containers), [[u'stuff2', [u'one_other', u'two_other']], [u'stuff3', [u'one_2_other', u'two_2_other']], [u'stuff4', [u'one_3_other', u'two_3_other']]])
        
        ### Generate some repetitive data and check it for it's contents:
        for x in xrange(50):

            ### Create row:
            _id = uuid.uuid4()
            stmt = """
              UPDATE bucket
              SET other_containers = other_containers + [{{stuff: 'stuff3', more_stuff: {{sub_one: 'one_2_other', sub_two: 'two_2_other'}}}}, {{stuff: 'stuff4', more_stuff: {{sub_one: 'one_3_other', sub_two: 'two_3_other'}}}}]
              WHERE id={id};
           """.format(id=_id)
            cursor.execute(stmt)
            
            time.sleep(0.1)
            
            ### Check it:
            stmt = """
              SELECT other_containers from bucket WHERE id={id}
            """.format(id=_id)
            cursor.execute(stmt)

            items = cursor.fetchone()[0]
            self.assertEqual(decode(items), [[u'stuff3', [u'one_2_other', u'two_2_other']], [u'stuff4', [u'one_3_other', u'two_3_other']]])

    @since('2.1')
    def test_type_as_part_of_pkey(self):
        """Tests user types as part of a composite pkey"""
        # make sure we can define a table with a user type as part of the pkey
        # and do a basic insert/query of data in that table.
        cluster = self.cluster
        cluster.populate(3).start()
        node1,node2,node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_type_pkeys', 2)
        
        stmt = """
              CREATE TYPE t_person_name (
              first text,
              middle text,
              last text
            )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TABLE person_likes (
              id uuid,
              name t_person_name,
              like text,
              PRIMARY KEY ((id, name))
              )
           """
        cursor.execute(stmt)
        # Make sure the scheam propagate
        time.sleep(2)

        _id = uuid.uuid4()

        stmt = """
              INSERT INTO person_likes (id, name, like)
              VALUES ({id}, {{first:'Nero', middle:'Claudius Caesar Augustus', last:'Germanicus'}}, 'arson');
           """.format(id=_id)
        cursor.execute(stmt)
        
        # attempt to query without the user type portion of the pkey and confirm there is an error
        stmt = """
              SELECT id, name.first from person_likes where id={id};
           """.format(id=_id)
        with self.assertRaisesRegexp(ProgrammingError, 'Partition key part name must be restricted since preceding part is'):
            cursor.execute(stmt)
            
        stmt = """
              SELECT id, name.first, like from person_likes where id={id} and name = {{first:'Nero', middle: 'Claudius Caesar Augustus', last: 'Germanicus'}};
           """.format(id=_id)
        cursor.execute(stmt)
        
        row_uuid, first_name, like = cursor.fetchone()
        
        self.assertEqual(first_name, u'Nero')
        self.assertEqual(like, u'arson')

    @since('2.1')
    def test_type_secondary_indexing(self):
        """
        Confirm that user types are secondary-indexable
        Similar procedure to TestSecondaryIndexesOnCollections.test_list_indexes
        """
        cluster = self.cluster
        cluster.populate(3).start()
        node1,node2,node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_type_indexing', 2)
                
        stmt = """
              CREATE TYPE t_person_name (
              first text,
              middle text,
              last text
            )
           """
        cursor.execute(stmt)

        stmt = """
              CREATE TABLE person_likes (
              id uuid PRIMARY KEY,
              name t_person_name,
              like text
              )
           """
        cursor.execute(stmt)
        # Make sure the scheam propagate
        time.sleep(2)
        
        # no index present yet, make sure there's an error trying to query column
        stmt = """
              SELECT * from person_likes where name = {first:'Nero', middle: 'Claudius Caesar Augustus', last: 'Germanicus'};
            """
        with self.assertRaisesRegexp(ProgrammingError, 'No indexed columns present in by-columns clause'):
            cursor.execute(stmt)
        
        # add index and query again (even though there are no rows in the table yet)
        stmt = """
              CREATE INDEX person_likes_name on person_likes (name);
            """
        cursor.execute(stmt)

        stmt = """
              SELECT * from person_likes where name = {first:'Nero', middle: 'Claudius Caesar Augustus', last: 'Germanicus'};
            """
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)

        # add a row which doesn't specify data for the indexed column, and query again
        _id = uuid.uuid4()
        stmt = """
              INSERT INTO person_likes (id, like)
              VALUES ({id}, 'long walks on the beach');
           """.format(id=_id)
        cursor.execute(stmt)
        
        stmt = """
              SELECT * from person_likes where name = {first:'Bob', middle: 'Testy', last: 'McTesterson'};
            """
        cursor.execute(stmt)
        self.assertEqual(0, cursor.rowcount)
        
        # finally let's add a queryable row, and get it back using the index
        _id = uuid.uuid4()

        stmt = """
              INSERT INTO person_likes (id, name, like)
              VALUES ({id}, {{first:'Nero', middle:'Claudius Caesar Augustus', last:'Germanicus'}}, 'arson');
           """.format(id=_id)
        cursor.execute(stmt)
        
        stmt = """
              SELECT id, name.first, like from person_likes where name = {first:'Nero', middle: 'Claudius Caesar Augustus', last: 'Germanicus'};
           """
        cursor.execute(stmt)
        
        row_uuid, first_name, like = cursor.fetchone()
        
        self.assertEqual(str(row_uuid), str(_id))
        self.assertEqual(first_name, u'Nero')
        self.assertEqual(like, u'arson')
        
        #rename the type and make sure the index still works
        stmt = """
            ALTER TYPE t_person_name rename to t_person_name2;
            """
        cursor.execute(stmt)
        
        stmt = """
            SELECT id, name.first, like from person_likes where name = {first:'Nero', middle: 'Claudius Caesar Augustus', last: 'Germanicus'};
            """
        cursor.execute(stmt)
        
        row_uuid, first_name, like = cursor.fetchone()
        
        self.assertEqual(str(row_uuid), str(_id))
        self.assertEqual(first_name, u'Nero')
        self.assertEqual(like, u'arson')

        # add another row to be sure the index is still adding new data
        _id = uuid.uuid4()

        stmt = """
              INSERT INTO person_likes (id, name, like)
              VALUES ({id}, {{first:'Abraham', middle:'', last:'Lincoln'}}, 'preserving unions');
           """.format(id=_id)
        cursor.execute(stmt)
        
        stmt = """
            SELECT id, name.first, like from person_likes where name = {first:'Abraham', middle:'', last:'Lincoln'};
            """
        cursor.execute(stmt)
        
        row_uuid, first_name, like = cursor.fetchone()
        
        self.assertEqual(str(row_uuid), str(_id))
        self.assertEqual(first_name, u'Abraham')
        self.assertEqual(like, u'preserving unions')
        
    @since('2.1')
    def test_type_keyspace_permission_isolation(self):
        """
        Confirm permissions are respected for types in different keyspaces
        """
        self.ignore_log_patterns = [
            # I think this happens when permissions change and a node becomes temporarily unavailable
            # and it's probably ok to ignore on this test, as I can see the schema changes propogating
            # almost immediately after
            r'Can\'t send migration request: node.*is down',
        ]
        
        cluster = self.cluster
        config = {'authenticator' : 'org.apache.cassandra.auth.PasswordAuthenticator',
                  'authorizer' : 'org.apache.cassandra.auth.CassandraAuthorizer',
                  'permissions_validity_in_ms' : 0}
        cluster.set_configuration_options(values=config)
        cluster.populate(3).start()
        node1, node2, node3 = cluster.nodelist()
        # need a bit of time for user to be created and propagate
        time.sleep(5)
        
        # do setup that requires a super user
        superuser_cursor = self.patient_cql_connection(node1, user='cassandra', password='cassandra').cursor()
        superuser_cursor.execute("create user ks1_user with password 'cassandra' nosuperuser;")
        superuser_cursor.execute("create user ks2_user with password 'cassandra' nosuperuser;")
        self.create_ks(superuser_cursor, 'ks1', 2)
        self.create_ks(superuser_cursor, 'ks2', 2)
        superuser_cursor.execute("grant all permissions on keyspace ks1 to ks1_user;")
        superuser_cursor.execute("grant all permissions on keyspace ks2 to ks2_user;")
        
        user1_cursor = self.patient_cql_connection(node1, user='ks1_user', password='cassandra').cursor()
        user2_cursor = self.patient_cql_connection(node1, user='ks2_user', password='cassandra').cursor()

        # first make sure the users can't create types in each other's ks
        with self.assertRaisesRegexp(ProgrammingError, 'User ks1_user has no CREATE permission on <keyspace ks2> or any of its parents'):
            user1_cursor.execute("CREATE TYPE ks2.simple_type (user_number int, user_text text );")
        
        with self.assertRaisesRegexp(ProgrammingError, 'User ks2_user has no CREATE permission on <keyspace ks1> or any of its parents'):
            user2_cursor.execute("CREATE TYPE ks1.simple_type (user_number int, user_text text );")
        
        # now, actually create the types in the correct keyspaces
        user1_cursor.execute("CREATE TYPE ks1.simple_type (user_number int, user_text text );")
        user2_cursor.execute("CREATE TYPE ks2.simple_type (user_number int, user_text text );")
        
        # each user now has a type belonging to their granted keyspace
        # let's make sure they can't drop each other's types (for which they have no permissions)
        with self.assertRaisesRegexp(ProgrammingError, 'User ks1_user has no DROP permission on <keyspace ks2> or any of its parents'):
            user1_cursor.execute("DROP TYPE ks2.simple_type;")
        
        with self.assertRaisesRegexp(ProgrammingError, 'User ks2_user has no DROP permission on <keyspace ks1> or any of its parents'):
            user2_cursor.execute("DROP TYPE ks1.simple_type;")
        
        # let's make sure they can't rename each other's types (for which they have no permissions)
        with self.assertRaisesRegexp(ProgrammingError, 'User ks1_user has no ALTER permission on <keyspace ks2> or any of its parents'):
            user1_cursor.execute("ALTER TYPE ks2.simple_type RENAME TO ks2.renamed_type;")
        
        with self.assertRaisesRegexp(ProgrammingError, 'User ks2_user has no ALTER permission on <keyspace ks1> or any of its parents'):
            user2_cursor.execute("ALTER TYPE ks1.simple_type RENAME TO ks1.renamed_type;")

        #rename the types using the correct user w/permissions to do so
        user1_cursor.execute("ALTER TYPE ks1.simple_type RENAME TO ks1.renamed_type;")
        user2_cursor.execute("ALTER TYPE ks2.simple_type RENAME TO ks2.renamed_type;")
        
        #finally, drop the types using the correct user w/permissions to do so
        user1_cursor.execute("DROP TYPE ks1.renamed_type;")
        user2_cursor.execute("DROP TYPE ks2.renamed_type;")
        
        #verify user type metadata is gone from the system schema
        superuser_cursor.execute("SELECT * from system.schema_usertypes")
        self.assertEqual(0, superuser_cursor.rowcount)

    @since('2.1')
    def test_nulls_in_user_types(self):
        """Tests user types with null values"""
        cluster = self.cluster
        cluster.populate(3).start()
        node1,node2,node3 = cluster.nodelist()
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'user_types', 2)

        stmt = """
              USE user_types
           """
        cursor.execute(stmt)

        #### Create a user type to go inside another one:
        stmt = """
              CREATE TYPE item (
              sub_one text,
              sub_two text,
              )
           """
        cursor.execute(stmt)

        ### Create a table that holds an item
        stmt = """
              CREATE TABLE bucket (
               id int PRIMARY KEY,
               my_item item,
              )
           """
        cursor.execute(stmt)
        # Make sure the schema propagates
        time.sleep(2)

        # Adds an explicit null
        cursor.execute("INSERT INTO bucket (id, my_item) VALUES (0, {sub_one: 'test', sub_two: null})");
        # Adds with an implicit null
        cursor.execute("INSERT INTO bucket (id, my_item) VALUES (1, {sub_one: 'test'})");

        cursor.execute("SELECT my_item FROM bucket WHERE id=0")
        self.assertEqual(decode(cursor.fetchone()), [[u'test', None]])

        cursor.execute("SELECT my_item FROM bucket WHERE id=1")
        self.assertEqual(decode(cursor.fetchone()), [[u'test', None]])

########NEW FILE########
__FILENAME__ = wide_rows_test
from dtest import Tester, debug
import datetime
import random

status_messages = (
    "I''m going to the Cassandra Summit in June!",
    "C* is awesome!",
    "All your sstables are belong to us.",
    "Just turned on another 50 C* nodes at <insert tech startup here>, scales beautifully.",
    "Oh, look! Cats, on reddit!",
    "Netflix recommendations are really good, wonder why?",
    "Spotify playlists are always giving me good tunes, wonder why?"
)

clients = (
    "Android",
    "iThing",
    "Chromium",
    "Mozilla",
    "Emacs"
    )

class TestWideRows(Tester):

    def __init__(self, *args, **kwargs):
        Tester.__init__(self, *args, **kwargs)

    def test_wide_rows(self):
        self.write_wide_rows()

    def write_wide_rows(self, version=None):
        cluster = self.cluster
        if version:
            self.cluster.set_cassandra_dir(cassandra_version=version)
        cluster.populate(1).start()
        (node1,) = cluster.nodelist()

        cursor = self.patient_cql_connection(node1).cursor()
        start_time = datetime.datetime.now()
        self.create_ks(cursor, 'wide_rows', 1)
        # Simple timeline:  user -> {date: value, ...}
        debug('Create Table....')
        cursor.execute('CREATE TABLE user_events (userid text, event timestamp, value text, PRIMARY KEY (userid, event));', 1)
        date = datetime.datetime.now()
        # Create a large timeline for each of a group of users:
        for user in ('ryan', 'cathy', 'mallen', 'joaquin', 'erin', 'ham'): 
            debug("Writing values for: %s" % user)
            for day in xrange(5000):
                date_str = (date + datetime.timedelta(day)).strftime("%Y-%m-%d")
                client = random.choice(clients)
                msg = random.choice(status_messages)
                query = "UPDATE user_events SET value = '{msg:%s, client:%s}' WHERE userid='%s' and event='%s';" \
                               % (msg, client, user, date_str)
                #debug(query)
                cursor.execute(query, 1)

        #debug('Duration of test: %s' % (datetime.datetime.now() - start_time))

        # Pick out an update for a specific date:
        query = "SELECT value FROM user_events WHERE userid='ryan' and event='%s'" % \
                (date + datetime.timedelta(10)).strftime("%Y-%m-%d")
        cursor.execute(query, 1)
        for value in cursor:
            debug(value)
            assert len(value[0]) > 0

    def test_column_index_stress(self):
        """Write a large number of columns to a single row and set
        'column_index_size_in_kb' to a sufficiently low value to force
        the creation of a column index. The test will then randomly
        read columns from that row and ensure that all data is
        returned. See CASSANDRA-5225.
        """
        cluster = self.cluster
        cluster.populate(1).start()
        (node1,) = cluster.nodelist()
        cluster.set_configuration_options(values={ 'column_index_size_in_kb' : 1 }) #reduce this value to force column index creation
        cursor = self.patient_cql_connection(node1).cursor()
        self.create_ks(cursor, 'wide_rows', 1)
        
        create_table_query = 'CREATE TABLE test_table (row varchar, name varchar, value int, PRIMARY KEY (row, name));'
        cursor.execute(create_table_query, 1)

        #Now insert 100,000 columns to row 'row0'
        insert_column_query = "UPDATE test_table SET value = {value} WHERE row = '{row}' AND name = '{name}';"
        for i in range(100000):
            row = 'row0'
            name = 'val' + str(i)
            cursor.execute( insert_column_query.format( value=i, row=row, name=name) )

        #now randomly fetch columns: 1 to 3 at a time
        for i in range(10000):
            select_column_query = "SELECT value FROM test_table WHERE row='row0' AND name in ('{name1}', '{name2}', '{name3}');"
            values2fetch = [str(random.randint(0, 99999)) for i in range(3)]
            #values2fetch is a list of random values.  Because they are random, they will not be unique necessarily.
            #To simplify the template logic in the select_column_query I will not expect the query to
            #necessarily return 3 values.  Hence I am computing the number of unique values in values2fetch
            #and using that in the assert at the end.
            expected_rows = len( set( values2fetch ) )
            cursor.execute( select_column_query.format(name1="val" + values2fetch[0],
                                                       name2="val" + values2fetch[1],
                                                       name3="val" + values2fetch[2]), 1)
            assert cursor.rowcount == expected_rows

########NEW FILE########

CGI Examples
------------

Here are some example CGI programs.  For a larger example, see
../../Tools/faqwiz/.

cgi0.sh -- A shell script to test your server is configured for CGI
cgi1.py -- A Python script to test your server is configured for CGI
cgi2.py -- A Python script showing how to parse a form
cgi3.py -- A Python script for driving an arbitrary CGI application
wiki.py -- Sample CGI application: a minimal Wiki implementation

Examples of classes that implement special operators (see reference manual):

Complex.py	Complex numbers
Dates.py	Date manipulation package by Tim Peters
Dbm.py		Wrapper around built-in dbm, supporting	arbitrary values
Range.py	Example of a generator: re-implement built-in range()
Rev.py		Yield the reverse of a sequence
Vec.py		A simple vector class
bitvec.py	A bit-vector class by Jan-Hein B\"uhrman

(For straightforward examples of basic class features, such as use of
methods and inheritance, see the library code.)

Subject: Re: What language would you use?
From: Tom Christiansen <tchrist@mox.perl.com>
Date: 6 Nov 1994 15:14:51 GMT
Newsgroups: comp.lang.python,comp.lang.tcl,comp.lang.scheme,comp.lang.misc,comp.lang.perl
Message-Id: <39irtb$3t4@csnews.cs.Colorado.EDU>
References: <39b7ha$j9v@zeno.nscf.org> <39hhjp$lgn@csnews.cs.Colorado.EDU> <39hvsu$dus@mathserv.mps.ohio-state.edu>

[...]
If you're really into benchmarks, I'd love it if someone were to code up
the following problems in tcl, python, and scheme (and whatever else you'd
like).  Separate versions (one optimized for speed, one for beauty :-) are
ok.  Post your code so we can time it on our own systems.

0)  Factorial Test  (numerics and function calls)

        (we did this already)

1)  Regular Expressions Test

    Read a file of (extended per egrep) regular expressions (one per line), 
    and apply those to all files whose names are listed on the command line.
    Basically, an 'egrep -f' simulator.  Test it with 20 "vt100" patterns
    against a five /etc/termcap files.  Tests using more elaborate patters
    would also be interesting.  Your code should not break if given hundreds
    of regular expressions or binary files to scan.  

2)  Sorting Test

    Sort an input file that consists of lines like this

        var1=23 other=14 ditto=23 fred=2

    such that each output line is sorted WRT to the number.  Order
    of output lines does not change.  Resolve collisions using the
    variable name.   e.g.

        fred=2 other=14 ditto=23 var1=23 

    Lines may be up to several kilobytes in length and contain
    zillions of variables.

3)  System Test

    Given a list of directories, report any bogus symbolic links contained
    anywhere in those subtrees.  A bogus symbolic link is one that cannot
    be resolved because it points to a nonexistent or otherwise
    unresolvable file.  Do *not* use an external find executable.
    Directories may be very very deep.  Print a warning immediately if the
    system you're running on doesn't support symbolic links.


I'll post perl solutions if people post the others.


--tom
-- 
Tom Christiansen      Perl Consultant, Gamer, Hiker      tchrist@mox.perl.com

 "But Billy! A *small* allowance prepares you for a lifetime of small
 salaries and for your Social Security payments."    --Family Circus

This is a collection of demos and tests for the curses module. 

ncurses demos
=============

These demos are converted from the C versions in the ncurses
distribution, and were contributed by Thomas Gellekum <tg@FreeBSD.org>
I didn't strive for a `pythonic' style, but bluntly copied the
originals. I won't attempt to `beautify' the program anytime soon, but
I wouldn't mind someone else making an effort in that direction, of
course.

ncurses.py      -- currently only a panels demo
rain.py         -- raindrops keep falling on my desktop
tclock.py       -- ASCII clock, by Howard Jones
xmas.py         -- I'm dreaming of an ASCII christmas

Please submit bugfixes and new contributions to the Python bug tracker.


Other demos
===========

life.py         -- Simple game of Life
repeat.py       -- Repeatedly execute a shell command (like watch(1))

This directory show how to embed the Python interpreter in your own
application.  The file demo.c shows you all that is needed in your C
code.

To build it, you may have to edit the Makefile:

1) set blddir to the directory where you built Python, if it isn't in
the source directory (../..)

2) change the variables that together define the list of libraries
(MODLIBS, LIBS, SYSLIBS) to link with, to match their definitions in
$(blddir)/Modules/Makefile

An additional test program, loop.c, is used to experiment with memory
leakage caused by repeated initialization and finalization of the
interpreter.  It can be build by saying "make loop" and tested with
"make looptest".  Command line usage is "./loop <python-command>",
e.g. "./loop 'print 2+2'" should spit out an endless number of lines
containing the number 4.

This is the Python version of the MD5 test program from the MD5
Internet Draft (Rivest and Dusse, The MD5 Message-Digest Algorithm, 10
July 1991).  The file "foo" contains the string "abc" with no trailing
newline.

When called without arguments, it acts as a filter.  When called with
"-x", it executes a self-test, and the output should literally match
the output given in the RFC.

Code by Jan-Hein B\"uhrman after the original in C.

These files are from the large example of using the `parser' module.  Refer
to the Python Library Reference for more information.

It also contains examples for the AST parser.

Files:
------

    FILES        -- list of files associated with the parser module.

    README       -- this file.

    docstring.py -- sample source file containing only a module docstring.

    example.py   -- module that uses the `parser' module to extract
                    information from the parse tree of Python source
                    code.

    simple.py    -- sample source containing a "short form" definition.

    source.py    -- sample source code used to demonstrate ability to
                    handle nested constructs easily using the functions
                    and classes in example.py.

    test_parser.py  program to put the parser module through its paces.

    test_unparse.py tests for the unparse module

    unparse.py      AST (2.7) based example to recreate source code
                    from an AST.

Enjoy!

Filesystem, RCS and CVS client and server classes
=================================================

*** See the security warning at the end of this file! ***

This directory contains various modules and classes that support
remote file system operations.

CVS stuff
---------

rcvs			Script to put in your bin directory
rcvs.py			Remote CVS client command line interface

cvslib.py		CVS admin files classes (used by rrcs)
cvslock.py		CVS locking algorithms

RCS stuff
---------

rrcs			Script to put in your bin directory
rrcs.py			Remote RCS client command line interface

rcsclient.py		Return an RCSProxyClient instance
			(has reasonable default server/port/directory)

RCSProxy.py		RCS proxy and server classes (on top of rcslib.py)

rcslib.py		Local-only RCS base class (affects stdout &
			local work files)

FSProxy stuff
-------------

sumtree.py		Old demo for FSProxy
cmptree.py		First FSProxy client (used to sync from the Mac)
FSProxy.py		Filesystem interface classes

Generic client/server stuff
---------------------------

client.py		Client class
server.py		Server class

security.py		Security mix-in class (not very secure I think)

Other generic stuff
-------------------

cmdfw.py		CommandFrameWork class
			(used by rcvs, should be used by rrcs as well)


Client/Server operation
-----------------------

The Client and Server classes implement a simple-minded RPC protocol,
using Python's pickle module to transfer arguments, return values and
exceptions with the most generality.  The Server class is instantiated
with a port number on which it should listen for requests; the Client
class is instantiated with a host name and a port number where it
should connect to.  Once a client is connected, a TCP connection is
maintained between client and server.

The Server class currently handles only one connection at a time;
however it could be rewritten to allow various modes of operations,
using multiple threads or processes or the select() system call as
desired to serve multiple clients simultaneously (when using select(),
still handling one request at a time).  This would not require
rewriting of the Client class.  It may also be possible to adapt the
code to use UDP instead of TCP, but then both classes will have to be
rewritten (and unless extensive acknowlegements and request serial
numbers are used, the server should handle duplicate requests, so its
semantics should be idempotent -- shrudder).

Even though the FSProxy and RCSProxy modules define client classes,
the client class is fully generic -- what methods it supports is
determined entirely by the server.  The server class, however, must be
derived from.  This is generally done as follows:

	from server import Server
	from client import Client

	# Define a class that performs the operations locally
	class MyClassLocal:
		def __init__(self): ...
		def _close(self): ...

	# Derive a server class using multiple inheritance
	class MyClassServer(MyClassLocal, Server):
		def __init__(self, address):
			# Must initialize MyClassLocal as well as Server
			MyClassLocal.__init__(self)
			Server.__init__(self, address)
		def _close(self):
			Server._close()
			MyClassLocal._close()

	# A dummy client class
	class MyClassClient(Client): pass

Note that because MyClassLocal isn't used in the definition of
MyClassClient, it would actually be better to place it in a separate
module so the definition of MyClassLocal isn't executed when we only
instantiate a client.

The modules client and server should probably be renamed to Client and
Server in order to match the class names.


*** Security warning: this version requires that you have a file
$HOME/.python_keyfile at the server and client side containing two
comma- separated numbers.  The security system at the moment makes no
guarantees of actuallng being secure -- however it requires that the
key file exists and contains the same numbers at both ends for this to
work.  (You can specify an alternative keyfile in $PYTHON_KEYFILE).
Have a look at the Security class in security.py for details;
basically, if the key file contains (x, y), then the security server
class chooses a random number z (the challenge) in the range
10..100000 and the client must be able to produce pow(z, x, y)
(i.e. z**x mod y).

This is an example of a multi-threaded C application embedding a
Python interpreter.

The particular application is a multi-threaded telnet-like server that
provides you with a Python prompt (instead of a shell prompt).

The file pysvr.py is a prototype in Python.

THIS APPLICATION IS NOT SECURE -- ONLY USE IT FOR TESTING!

This directory contains various demonstrations of what you can do with
Python.  They were all written by me except where explicitly stated
otherwise -- in general, demos contributed by others ends up in the
../Contrib directory, unless I think they're of utmost general
importance (like Matt Conway's Tk demos).

A fair number of utilities that are useful when while developing
Python code can be found in the ../Tools directory -- some of these
can also be considered good examples of how to write Python code.

Finally, in order to save disk space and net bandwidth, not all
subdirectories listed here are distributed.  They are listed just
in case I change my mind about them.


cgi             CGI examples (see also ../Tools/faqwiz/.)

classes		Some examples of how to use classes.

comparisons	A set of responses to a really old language-comparison
		challenge.

curses		A set of curses demos.

embed		An example of embedding Python in another application
		(see also pysvr).

imputil		Demonstration subclasses of imputil.Importer.

md5test		Test program for the optional md5 module.

metaclasses	The code from the 1.5 metaclasses paper on the web.

parser		Example using the parser module.

pdist		Old, unfinished code messing with CVS, RCS and remote
		files.

pysvr		An example of embedding Python in a threaded
		application.

rpc		A set of classes for building clients and servers for
		Sun RPC.

scripts		Some useful Python scripts that I put in my bin
		directory.  No optional built-in modules needed.

sockets		Examples for the new built-in module 'socket'.

threads		Demos that use the 'thread' module.  (Currently these
		only run on SGIs, but this may change in the future.)

tix		Demos using the Tix widget set addition to Tkinter.

tkinter		Demos using the Tk interface (including Matt Conway's
		excellent set of demos).

xml		Some XML demos.

zlib		Some demos for the zlib module (see also the standard
		library module gzip.py).

This is a Python interface to Sun RPC, designed and implemented mostly
by reading the Internet RFCs about the subject.

*** NOTE: xdr.py has evolved into the standard module xdrlib.py ***

There are two library modules, xdr.py and rpc.py, and several example
clients: mountclient.py, nfsclient.py, and rnusersclient.py,
implementing the NFS Mount protocol, (part of) the NFS protocol, and
the "rnusers" protocol (used by rusers(1)), respectively.  The latter
demonstrates the use of broadcast via the Port mapper's CALLIT
procedure.

There is also a way to create servers in Python.

To test the nfs client, run it from the shell with something like this:

  python -c 'import nfsclient; nfsclient.test()' [hostname [filesystemname]]

When called without a filesystemname, it lists the filesystems at the
host; default host is the local machine.

Other clients are tested similarly.

For hostname, use e.g. wuarchive.wustl.edu or gatekeeper.dec.com (two
hosts that are known to export NFS filesystems with little restrictions).

There are now two different RPC compilers:

1) Wim Lewis rpcgen.py found on http://www.omnigroup.com/~wiml/soft/stale-index.html#python. 

2) Peter Åstrands rpcgen.py, which is part of "pynfs" (http://www.cendio.se/~peter/pynfs/). 

This directory contains a collection of executable Python scripts.

See also the Tools/scripts directory!

beer.py			Print the classic 'bottles of beer' list
eqfix.py		Fix .py files to use the correct equality test operator
fact.py			Factorize numbers
find-uname.py		Search for Unicode characters using regexps
from.py			Summarize mailbox
lpwatch.py		Watch BSD line printer queues
makedir.py		Like mkdir -p
markov.py		Markov chain simulation of words or characters
mboxconvert.py		Convert MH or MMDF mailboxes to unix mailbox format
morse.py		Produce morse code (audible or on AIFF file)
newslist.py		List all newsgroups on a NNTP server as HTML pages
pi.py			Print all digits of pi -- given enough time and memory
pp.py			Emulate some Perl command line options
primes.py		Print prime numbers
queens.py		Dijkstra's solution to Wirth's "N Queens problem"
script.py		Equivalent to BSD script(1) -- by Steen Lumholt
unbirthday.py		Print unbirthday count
update.py		Update a bunch of files according to a script.

This directory contains some demonstrations of the socket module:

broadcast.py	 	Broadcast the time to radio.py.
echosvr.py		About the simplest TCP server possible.
finger.py		Client for the 'finger' protocol.
ftp.py			A very simple ftp client.
gopher.py		A simple gopher client.
mcast.py		IPv4/v6 multicast example
radio.py		Receive time broadcasts from broadcast.py.
telnet.py		Client for the 'telnet' protocol.
throughput.py		Client and server to measure TCP throughput.
unixclient.py		Unix socket example, client side
unixserver.py		Unix socket example, server side
udpecho.py		Client and server for the UDP echo protocol.

This directory contains some demonstrations of the thread module.

These are mostly "proof of concept" type applications:

Generator.py	Generator class implemented with threads.
sync.py		Condition variables primitives by Tim Peters.
telnet.py	Version of ../sockets/telnet.py using threads.

Coroutine.py	Coroutines using threads, by Tim Peters (22 May 94)
fcmp.py		Example of above, by Tim
squasher.py	Another example of above, also by Tim

About Tix.py
-----------

Tix.py is based on an idea of Jean-Marc Lugrin (lugrin@ms.com) who wrote
pytix (another Python-Tix marriage). Tix widgets are an attractive and
useful extension to Tk. See http://tix.sourceforge.net
for more details about Tix and how to get it.

Features:
	1) It is almost complete.
	2) Tix widgets are represented by classes in Python. Sub-widgets
	   are members of the mega-widget class. For example, if a
	   particular TixWidget (e.g. ScrolledText) has an embedded widget
	   (Text in this case), it is possible to call the methods of the
	   child directly.
	3) The members of the class are created automatically. In the case
	   of widgets like ButtonBox, the members are added dynamically.



This directory contains some ad-hoc examples of Tkinter widget
creation. The files named 

		*-simple.py

are the ones to start with if you're looking for a bare-bones usage of
a widget. The other files are meant to show common usage patters that
are a tad more involved. 

If you have a suggestion for an example program, please send mail to 
	
	conway@virginia.edu

and I'll include it.


matt

TODO
-------
The X selection
Dialog Boxes
More canvas examples
Message widgets
Text Editors
Scrollbars
Listboxes




Several collections of example code for Tkinter.

See the toplevel README for an explanation of the difference between
Tkinter and _tkinter, how to enable the Python Tk interface, and where
to get Matt Conway's lifesaver document.

Subdirectories:

guido		my original example set (fairly random collection)
matt		Matt Conway's examples, to go with his lifesaver document
ttk         Examples using the ttk module

.. highlightlang:: c

.. _descriptor-objects:

Descriptor Objects
------------------

"Descriptors" are objects that describe some attribute of an object. They are
found in the dictionary of type objects.


.. c:var:: PyTypeObject PyProperty_Type

   The type object for the built-in descriptor types.

   .. versionadded:: 2.2


.. c:function:: PyObject* PyDescr_NewGetSet(PyTypeObject *type, struct PyGetSetDef *getset)

   .. versionadded:: 2.2


.. c:function:: PyObject* PyDescr_NewMember(PyTypeObject *type, struct PyMemberDef *meth)

   .. versionadded:: 2.2


.. c:function:: PyObject* PyDescr_NewMethod(PyTypeObject *type, struct PyMethodDef *meth)

   .. versionadded:: 2.2


.. c:function:: PyObject* PyDescr_NewWrapper(PyTypeObject *type, struct wrapperbase *wrapper, void *wrapped)

   .. versionadded:: 2.2


.. c:function:: PyObject* PyDescr_NewClassMethod(PyTypeObject *type, PyMethodDef *method)

   .. versionadded:: 2.3


.. c:function:: int PyDescr_IsData(PyObject *descr)

   Return true if the descriptor objects *descr* describes a data attribute, or
   false if it describes a method.  *descr* must be a descriptor object; there is
   no error checking.

   .. versionadded:: 2.2


.. c:function:: PyObject* PyWrapper_New(PyObject *, PyObject *)

   .. versionadded:: 2.2

======================
Descriptor HowTo Guide
======================

:Author: Raymond Hettinger
:Contact: <python at rcn dot com>

.. Contents::

Abstract
--------

Defines descriptors, summarizes the protocol, and shows how descriptors are
called.  Examines a custom descriptor and several built-in python descriptors
including functions, properties, static methods, and class methods.  Shows how
each works by giving a pure Python equivalent and a sample application.

Learning about descriptors not only provides access to a larger toolset, it
creates a deeper understanding of how Python works and an appreciation for the
elegance of its design.


Definition and Introduction
---------------------------

In general, a descriptor is an object attribute with "binding behavior", one
whose attribute access has been overridden by methods in the descriptor
protocol.  Those methods are :meth:`__get__`, :meth:`__set__`, and
:meth:`__delete__`.  If any of those methods are defined for an object, it is
said to be a descriptor.

The default behavior for attribute access is to get, set, or delete the
attribute from an object's dictionary.  For instance, ``a.x`` has a lookup chain
starting with ``a.__dict__['x']``, then ``type(a).__dict__['x']``, and
continuing through the base classes of ``type(a)`` excluding metaclasses. If the
looked-up value is an object defining one of the descriptor methods, then Python
may override the default behavior and invoke the descriptor method instead.
Where this occurs in the precedence chain depends on which descriptor methods
were defined.  Note that descriptors are only invoked for new style objects or
classes (a class is new style if it inherits from :class:`object` or
:class:`type`).

Descriptors are a powerful, general purpose protocol.  They are the mechanism
behind properties, methods, static methods, class methods, and :func:`super()`.
They are used throughout Python itself to implement the new style classes
introduced in version 2.2.  Descriptors simplify the underlying C-code and offer
a flexible set of new tools for everyday Python programs.


Descriptor Protocol
-------------------

``descr.__get__(self, obj, type=None) --> value``

``descr.__set__(self, obj, value) --> None``

``descr.__delete__(self, obj) --> None``

That is all there is to it.  Define any of these methods and an object is
considered a descriptor and can override default behavior upon being looked up
as an attribute.

If an object defines both :meth:`__get__` and :meth:`__set__`, it is considered
a data descriptor.  Descriptors that only define :meth:`__get__` are called
non-data descriptors (they are typically used for methods but other uses are
possible).

Data and non-data descriptors differ in how overrides are calculated with
respect to entries in an instance's dictionary.  If an instance's dictionary
has an entry with the same name as a data descriptor, the data descriptor
takes precedence.  If an instance's dictionary has an entry with the same
name as a non-data descriptor, the dictionary entry takes precedence.

To make a read-only data descriptor, define both :meth:`__get__` and
:meth:`__set__` with the :meth:`__set__` raising an :exc:`AttributeError` when
called.  Defining the :meth:`__set__` method with an exception raising
placeholder is enough to make it a data descriptor.


Invoking Descriptors
--------------------

A descriptor can be called directly by its method name.  For example,
``d.__get__(obj)``.

Alternatively, it is more common for a descriptor to be invoked automatically
upon attribute access.  For example, ``obj.d`` looks up ``d`` in the dictionary
of ``obj``.  If ``d`` defines the method :meth:`__get__`, then ``d.__get__(obj)``
is invoked according to the precedence rules listed below.

The details of invocation depend on whether ``obj`` is an object or a class.
Either way, descriptors only work for new style objects and classes.  A class is
new style if it is a subclass of :class:`object`.

For objects, the machinery is in :meth:`object.__getattribute__` which
transforms ``b.x`` into ``type(b).__dict__['x'].__get__(b, type(b))``.  The
implementation works through a precedence chain that gives data descriptors
priority over instance variables, instance variables priority over non-data
descriptors, and assigns lowest priority to :meth:`__getattr__` if provided.  The
full C implementation can be found in :c:func:`PyObject_GenericGetAttr()` in
`Objects/object.c <http://svn.python.org/view/python/trunk/Objects/object.c?view=markup>`_\.

For classes, the machinery is in :meth:`type.__getattribute__` which transforms
``B.x`` into ``B.__dict__['x'].__get__(None, B)``.  In pure Python, it looks
like::

    def __getattribute__(self, key):
        "Emulate type_getattro() in Objects/typeobject.c"
        v = object.__getattribute__(self, key)
        if hasattr(v, '__get__'):
           return v.__get__(None, self)
        return v

The important points to remember are:

* descriptors are invoked by the :meth:`__getattribute__` method
* overriding :meth:`__getattribute__` prevents automatic descriptor calls
* :meth:`__getattribute__` is only available with new style classes and objects
* :meth:`object.__getattribute__` and :meth:`type.__getattribute__` make
  different calls to :meth:`__get__`.
* data descriptors always override instance dictionaries.
* non-data descriptors may be overridden by instance dictionaries.

The object returned by ``super()`` also has a custom :meth:`__getattribute__`
method for invoking descriptors.  The call ``super(B, obj).m()`` searches
``obj.__class__.__mro__`` for the base class ``A`` immediately following ``B``
and then returns ``A.__dict__['m'].__get__(obj, B)``.  If not a descriptor,
``m`` is returned unchanged.  If not in the dictionary, ``m`` reverts to a
search using :meth:`object.__getattribute__`.

Note, in Python 2.2, ``super(B, obj).m()`` would only invoke :meth:`__get__` if
``m`` was a data descriptor.  In Python 2.3, non-data descriptors also get
invoked unless an old-style class is involved.  The implementation details are
in :c:func:`super_getattro()` in
`Objects/typeobject.c <http://svn.python.org/view/python/trunk/Objects/typeobject.c?view=markup>`_
and a pure Python equivalent can be found in `Guido's Tutorial`_.

.. _`Guido's Tutorial`: http://www.python.org/2.2.3/descrintro.html#cooperation

The details above show that the mechanism for descriptors is embedded in the
:meth:`__getattribute__()` methods for :class:`object`, :class:`type`, and
:func:`super`.  Classes inherit this machinery when they derive from
:class:`object` or if they have a meta-class providing similar functionality.
Likewise, classes can turn-off descriptor invocation by overriding
:meth:`__getattribute__()`.


Descriptor Example
------------------

The following code creates a class whose objects are data descriptors which
print a message for each get or set.  Overriding :meth:`__getattribute__` is
alternate approach that could do this for every attribute.  However, this
descriptor is useful for monitoring just a few chosen attributes::

    class RevealAccess(object):
        """A data descriptor that sets and returns values
           normally and prints a message logging their access.
        """

        def __init__(self, initval=None, name='var'):
            self.val = initval
            self.name = name

        def __get__(self, obj, objtype):
            print 'Retrieving', self.name
            return self.val

        def __set__(self, obj, val):
            print 'Updating' , self.name
            self.val = val

    >>> class MyClass(object):
        x = RevealAccess(10, 'var "x"')
        y = 5

    >>> m = MyClass()
    >>> m.x
    Retrieving var "x"
    10
    >>> m.x = 20
    Updating var "x"
    >>> m.x
    Retrieving var "x"
    20
    >>> m.y
    5

The protocol is simple and offers exciting possibilities.  Several use cases are
so common that they have been packaged into individual function calls.
Properties, bound and unbound methods, static methods, and class methods are all
based on the descriptor protocol.


Properties
----------

Calling :func:`property` is a succinct way of building a data descriptor that
triggers function calls upon access to an attribute.  Its signature is::

    property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute ``x``::

    class C(object):
        def getx(self): return self.__x
        def setx(self, value): self.__x = value
        def delx(self): del self.__x
        x = property(getx, setx, delx, "I'm the 'x' property.")

To see how :func:`property` is implemented in terms of the descriptor protocol,
here is a pure Python equivalent::

    class Property(object):
        "Emulate PyProperty_Type() in Objects/descrobject.c"

        def __init__(self, fget=None, fset=None, fdel=None, doc=None):
            self.fget = fget
            self.fset = fset
            self.fdel = fdel
            if doc is None and fget is not None:
                doc = fget.__doc__
            self.__doc__ = doc

        def __get__(self, obj, objtype=None):
            if obj is None:
                return self
            if self.fget is None:
                raise AttributeError("unreadable attribute")
            return self.fget(obj)

        def __set__(self, obj, value):
            if self.fset is None:
                raise AttributeError("can't set attribute")
            self.fset(obj, value)

        def __delete__(self, obj):
            if self.fdel is None:
                raise AttributeError("can't delete attribute")
            self.fdel(obj)

        def getter(self, fget):
            return type(self)(fget, self.fset, self.fdel, self.__doc__)

        def setter(self, fset):
            return type(self)(self.fget, fset, self.fdel, self.__doc__)

        def deleter(self, fdel):
            return type(self)(self.fget, self.fset, fdel, self.__doc__)

The :func:`property` builtin helps whenever a user interface has granted
attribute access and then subsequent changes require the intervention of a
method.

For instance, a spreadsheet class may grant access to a cell value through
``Cell('b10').value``. Subsequent improvements to the program require the cell
to be recalculated on every access; however, the programmer does not want to
affect existing client code accessing the attribute directly.  The solution is
to wrap access to the value attribute in a property data descriptor::

    class Cell(object):
        . . .
        def getvalue(self, obj):
            "Recalculate cell before returning value"
            self.recalc()
            return obj._value
        value = property(getvalue)


Functions and Methods
---------------------

Python's object oriented features are built upon a function based environment.
Using non-data descriptors, the two are merged seamlessly.

Class dictionaries store methods as functions.  In a class definition, methods
are written using :keyword:`def` and :keyword:`lambda`, the usual tools for
creating functions.  The only difference from regular functions is that the
first argument is reserved for the object instance.  By Python convention, the
instance reference is called *self* but may be called *this* or any other
variable name.

To support method calls, functions include the :meth:`__get__` method for
binding methods during attribute access.  This means that all functions are
non-data descriptors which return bound or unbound methods depending whether
they are invoked from an object or a class.  In pure python, it works like
this::

    class Function(object):
        . . .
        def __get__(self, obj, objtype=None):
            "Simulate func_descr_get() in Objects/funcobject.c"
            return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in practice::

    >>> class D(object):
         def f(self, x):
              return x

    >>> d = D()
    >>> D.__dict__['f'] # Stored internally as a function
    <function f at 0x00C45070>
    >>> D.f             # Get from a class becomes an unbound method
    <unbound method D.f>
    >>> d.f             # Get from an instance becomes a bound method
    <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different types.
While they could have been implemented that way, the actual C implementation of
:c:type:`PyMethod_Type` in
`Objects/classobject.c <http://svn.python.org/view/python/trunk/Objects/classobject.c?view=markup>`_
is a single object with two different representations depending on whether the
:attr:`im_self` field is set or is *NULL* (the C equivalent of *None*).

Likewise, the effects of calling a method object depend on the :attr:`im_self`
field. If set (meaning bound), the original function (stored in the
:attr:`im_func` field) is called as expected with the first argument set to the
instance.  If unbound, all of the arguments are passed unchanged to the original
function. The actual C implementation of :func:`instancemethod_call()` is only
slightly more complex in that it includes some type checking.


Static Methods and Class Methods
--------------------------------

Non-data descriptors provide a simple mechanism for variations on the usual
patterns of binding functions into methods.

To recap, functions have a :meth:`__get__` method so that they can be converted
to a method when accessed as attributes.  The non-data descriptor transforms a
``obj.f(*args)`` call into ``f(obj, *args)``.  Calling ``klass.f(*args)``
becomes ``f(*args)``.

This chart summarizes the binding and its two most useful variants:

      +-----------------+----------------------+------------------+
      | Transformation  | Called from an       | Called from a    |
      |                 | Object               | Class            |
      +=================+======================+==================+
      | function        | f(obj, \*args)       | f(\*args)        |
      +-----------------+----------------------+------------------+
      | staticmethod    | f(\*args)            | f(\*args)        |
      +-----------------+----------------------+------------------+
      | classmethod     | f(type(obj), \*args) | f(klass, \*args) |
      +-----------------+----------------------+------------------+

Static methods return the underlying function without changes.  Calling either
``c.f`` or ``C.f`` is the equivalent of a direct lookup into
``object.__getattribute__(c, "f")`` or ``object.__getattribute__(C, "f")``. As a
result, the function becomes identically accessible from either an object or a
class.

Good candidates for static methods are methods that do not reference the
``self`` variable.

For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the average,
mean, median, and other descriptive statistics that depend on the data. However,
there may be useful functions which are conceptually related but do not depend
on the data.  For instance, ``erf(x)`` is handy conversion routine that comes up
in statistical work but does not directly depend on a particular dataset.
It can be called either from an object or the class:  ``s.erf(1.5) --> .9332`` or
``Sample.erf(1.5) --> .9332``.

Since staticmethods return the underlying function with no changes, the example
calls are unexciting::

    >>> class E(object):
         def f(x):
              print x
         f = staticmethod(f)

    >>> print E.f(3)
    3
    >>> print E().f(3)
    3

Using the non-data descriptor protocol, a pure Python version of
:func:`staticmethod` would look like this::

    class StaticMethod(object):
     "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

     def __init__(self, f):
          self.f = f

     def __get__(self, obj, objtype=None):
          return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same
for whether the caller is an object or a class::

    >>> class E(object):
         def f(klass, x):
              return klass.__name__, x
         f = classmethod(f)

    >>> print E.f(3)
    ('E', 3)
    >>> print E().f(3)
    ('E', 3)


This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for classmethods
is to create alternate class constructors.  In Python 2.3, the classmethod
:func:`dict.fromkeys` creates a new dictionary from a list of keys.  The pure
Python equivalent is::

    class Dict(object):
        . . .
        def fromkeys(klass, iterable, value=None):
            "Emulate dict_fromkeys() in Objects/dictobject.c"
            d = klass()
            for key in iterable:
                d[key] = value
            return d
        fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this::

    >>> Dict.fromkeys('abracadabra')
    {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of
:func:`classmethod` would look like this::

    class ClassMethod(object):
         "Emulate PyClassMethod_Type() in Objects/funcobject.c"

         def __init__(self, f):
              self.f = f

         def __get__(self, obj, klass=None):
              if klass is None:
                   klass = type(obj)
              def newfunc(*args):
                   return self.f(klass, *args)
              return newfunc


Python Documentation README
~~~~~~~~~~~~~~~~~~~~~~~~~~~

This directory contains the reStructuredText (reST) sources to the Python
documentation.  You don't need to build them yourself, prebuilt versions are
available at http://docs.python.org/download/.

Documentation on the authoring Python documentation, including information about
both style and markup, is available in the "Documenting Python" chapter of the
documentation.  There's also a chapter intended to point out differences to
those familiar with the previous docs written in LaTeX.


Building the docs
=================

You need to have Python 2.4 or higher installed; the toolset used to build the
docs is written in Python.  It is called *Sphinx*, it is not included in this
tree, but maintained separately.  Also needed are the docutils, supplying the
base markup that Sphinx uses, Jinja, a templating engine, and optionally
Pygments, a code highlighter.


Using make
----------

Luckily, a Makefile has been prepared so that on Unix, provided you have
installed Python and Subversion, you can just run ::

   make html

to check out the necessary toolset in the `tools/` subdirectory and build the
HTML output files.  To view the generated HTML, point your favorite browser at
the top-level index `build/html/index.html` after running "make".

Available make targets are:

 * "html", which builds standalone HTML files for offline viewing.

 * "htmlhelp", which builds HTML files and a HTML Help project file usable to
   convert them into a single Compiled HTML (.chm) file -- these are popular
   under Microsoft Windows, but very handy on every platform.

   To create the CHM file, you need to run the Microsoft HTML Help Workshop over
   the generated project (.hhp) file.

 * "latex", which builds LaTeX source files as input to "pdflatex" to produce
   PDF documents.

 * "text", which builds a plain text file for each source file.

 * "linkcheck", which checks all external references to see whether they are
   broken, redirected or malformed, and outputs this information to stdout as
   well as a plain-text (.txt) file.

 * "changes", which builds an overview over all versionadded/versionchanged/
   deprecated items in the current version. This is meant as a help for the
   writer of the "What's New" document.

 * "coverage", which builds a coverage overview for standard library modules and
   C API.

 * "pydoc-topics", which builds a Python module containing a dictionary with
   plain text documentation for the labels defined in
   `tools/sphinxext/pyspecific.py` -- pydoc needs these to show topic and
   keyword help.

A "make update" updates the Subversion checkouts in `tools/`.


Without make
------------

You'll need to install the Sphinx package, either by checking it out via ::

   svn co http://svn.python.org/projects/external/Sphinx-0.6.7/sphinx tools/sphinx

or by installing it from PyPI.

Then, you need to install Docutils, either by checking it out via ::

   svn co http://svn.python.org/projects/external/docutils-0.6/docutils tools/docutils

or by installing it from http://docutils.sf.net/.

You also need Jinja2, either by checking it out via ::

   svn co http://svn.python.org/projects/external/Jinja-2.3.1/jinja2 tools/jinja2

or by installing it from PyPI.

You can optionally also install Pygments, either as a checkout via ::

   svn co http://svn.python.org/projects/external/Pygments-1.3.1/pygments tools/pygments

or from PyPI at http://pypi.python.org/pypi/Pygments.


Then, make an output directory, e.g. under `build/`, and run ::

   python tools/sphinx-build.py -b<builder> . build/<outputdirectory>

where `<builder>` is one of html, text, latex, or htmlhelp (for explanations see
the make targets above).


Contributing
============

Bugs in the content should be reported to the Python bug tracker at
http://bugs.python.org.

Bugs in the toolset should be reported in the Sphinx bug tracker at
http://www.bitbucket.org/birkenfeld/sphinx/issues/.

You can also send a mail to the Python Documentation Team at docs@python.org,
and we will process your request as soon as possible.

If you want to help the Documentation Team, you are always welcome.  Just send
a mail to docs@python.org.


Copyright notice
================

The Python source is copyrighted, but you can freely use and copy it
as long as you don't change or remove the copyright notice:

----------------------------------------------------------------------
Copyright (c) 2000-2013 Python Software Foundation.
All rights reserved.

Copyright (c) 2000 BeOpen.com.
All rights reserved.

Copyright (c) 1995-2000 Corporation for National Research Initiatives.
All rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.
All rights reserved.

See the file "license.rst" for information on usage and redistribution
of this file, and for a DISCLAIMER OF ALL WARRANTIES.
----------------------------------------------------------------------

Files in this directory from from Bob Ippolito's py2app.

License: Any components of the py2app suite may be distributed under
the MIT or PSF open source licenses.

This is version 1.0, SVN revision 789, from 2006/01/25.
The main repository is http://svn.red-bean.com/bob/macholib/trunk/macholib/
This directory contains the Distutils package.

There's a full documentation available at:

    http://docs.python.org/distutils/

The Distutils-SIG web page is also a good starting point:

    http://www.python.org/sigs/distutils-sig/

WARNING : Distutils must remain compatible with 2.3

$Id$

README FOR IDLE TESTS IN IDLELIB.IDLE_TEST


1. Test Files

The idle directory, idlelib, has over 60 xyz.py files. The idle_test
subdirectory should contain a test_xyy.py for each. (For test modules, make
'xyz' lower case, and possibly shorten it.) Each file should start with the
something like the following template, with the blanks after after '.' and 'as',
and before and after '_' filled in.
---
import unittest
from test.support import requires
import idlelib. as

class _Test(unittest.TestCase):

    def test_(self):

if __name__ == '__main__':
    unittest.main(verbosity=2, exit=2)
---
Idle tests are run with unittest; do not use regrtest's test_main.

Once test_xyy is written, the following should go at the end of xyy.py,
with xyz (lowercased) added after 'test_'.
---
if __name__ == "__main__":
    from test import support; support.use_resources = ['gui']
    import unittest
    unittest.main('idlelib.idle_test.test_', verbosity=2, exit=False)
---


2. Gui Tests

Gui tests need 'requires' and 'use_resources' from test.support
(test.test_support in 2.7). A test is a gui test if it creates a Tk root or
master object either directly or indirectly by instantiating a tkinter or
idle class. For the benefit of buildbot machines that do not have a graphics
screen, gui tests must be 'guarded' by "requires('gui')" in a setUp
function or method. This will typically be setUpClass.

All gui objects must be destroyed by the end of the test, perhaps in a tearDown
function. Creating the Tk root directly in a setUp allows a reference to be saved
so it can be properly destroyed in the corresponding tearDown. 
---
    @classmethod
    def setUpClass(cls):
        requires('gui')
        cls.root = tk.Tk()

    @classmethod
    def tearDownClass(cls):
        cls.root.destroy()
---

Support.requires('gui') returns true if it is either called in a main module
(which never happens on buildbots) or if use_resources contains 'gui'.
Use_resources is set by test.regrtest but not by unittest. So when running
tests in another module with unittest, we set it ourselves, as in the xyz.py
template above.

Since non-gui tests always run, but gui tests only sometimes, tests of non-gui
operations should best avoid needing a gui. Methods that make incidental use of
tkinter (tk) variables and messageboxes can do this by using the mock classes in
idle_test/mock_tk.py. There is also a mock text that will handle some uses of the
tk Text widget.


3. Running Tests

Assume that xyz.py and test_xyz.py end with the "if __name__" statements given
above. In Idle, pressing F5 in an editor window with either loaded will run all
tests in the test_xyz file with the version of Python running Idle.  The test
report and any tracebacks will appear in the Shell window. The options in these
"if __name__" statements are appropriate for developers running (as opposed to
importing) either of the files during development: verbosity=2 lists all test
methods in the file; exit=False avoids a spurious sys.exit traceback that would
otherwise occur when running in Idle. The following command lines also run
all test methods, including gui tests, in test_xyz.py. (The exceptions are that
idlelib and idlelib.idle start Idle and idlelib.PyShell should (issue 18330).)

python -m idlelib.xyz  # With the capitalization of the xyz module
python -m idlelib.idle_test.test_xyz

To run all idle_test/test_*.py tests, either interactively
('>>>', with unittest imported) or from a command line, use one of the
following. (Notes: unittest does not run gui tests; in 2.7, 'test ' (with the
space) is 'test.regrtest '; where present, -v and -ugui can be omitted.)

>>> unittest.main('idlelib.idle_test', verbosity=2, exit=False)
python -m unittest -v idlelib.idle_test
python -m test -v -ugui test_idle
python -m test.test_idle

The idle tests are 'discovered' by idlelib.idle_test.__init__.load_tests,
which is also imported into test.test_idle. Normally, neither file should be
changed when working on individual test modules. The third command runs runs
unittest indirectly through regrtest. The same happens when the entire test
suite is run with 'python -m test'. So that command must work for buildbots
to stay green. Idle tests must not disturb the environment in a way that
makes other tests fail (issue 18081).

To run an individual Testcase or test method, extend the dotted name given to
unittest on the command line. (But gui tests will not this way.)

python -m unittest -v idlelib.idle_test.text_xyz.Test_case.test_meth

IDLE is Python's Tkinter-based Integrated DeveLopment Environment.

IDLE emphasizes a lightweight, clean design with a simple user interface.
Although it is suitable for beginners, even advanced users will find that
IDLE has everything they really need to develop pure Python code.

IDLE features a multi-window text editor with multiple undo, Python colorizing,
and many other capabilities, e.g. smart indent, call tips, and autocompletion.

The editor has comprehensive search functions, including searching through
multiple files.  Class browsers and path browsers provide fast access to
code objects from a top level viewpoint without dealing with code folding.

There is a Python Shell window which features colorizing and command recall.

IDLE executes Python code in a separate process, which is restarted for each
Run (F5) initiated from an editor window.  The environment can also be 
restarted from the Shell window without restarting IDLE.

This enhancement has often been requested, and is now finally available.  The
magic "reload/import *" incantations are no longer required when editing and
testing a module two or three steps down the import chain.

(Personal firewall software may warn about the connection IDLE makes to its
subprocess using this computer's internal loopback interface.  This connection
is not visible on any external interface and no data is sent to or received
from the Internet.)

It is possible to interrupt tightly looping user code, even on Windows.

Applications which cannot support subprocesses and/or sockets can still run
IDLE in a single process.

IDLE has an integrated debugger with stepping, persistent breakpoints, and call
stack visibility.

There is a GUI configuration manager which makes it easy to select fonts,
colors, keybindings, and startup options.  This facility includes a feature
which allows the user to specify additional help sources, either locally or on
the web.

IDLE is coded in 100% pure Python, using the Tkinter GUI toolkit (Tk/Tcl)
and is cross-platform, working on Unix, Mac, and Windows.

IDLE accepts command line arguments.  Try idle -h to see the options.


If you find bugs or have suggestions, let us know about them by using the
Python Bug Tracker:

http://sourceforge.net/projects/python

Patches are always appreciated at the Python Patch Tracker, and change
requests should be posted to the RFE Tracker.

For further details and links, read the Help files and check the IDLE home
page at

http://www.python.org/idle/

There is a mail list for IDLE: idle-dev@python.org.  You can join at

http://mail.python.org/mailman/listinfo/idle-dev

Writing new tests
=================

Precaution
----------

    New tests should always use only one Tk window at once, like all the
    current tests do. This means that you have to destroy the current window
    before creating another one, and clean up after the test. The motivation
    behind this is that some tests may depend on having its window focused
    while it is running to work properly, and it may be hard to force focus
    on your window across platforms (right now only test_traversal at
    test_ttk.test_widgets.NotebookTest depends on this).


In this directory:
- py2_test_grammar.py -- test file that exercises most/all of Python 2.x's grammar.
- py3_test_grammar.py -- test file that exercises most/all of Python 3.x's grammar.
- infinite_recursion.py -- test file that causes lib2to3's faster recursive pattern matching
  scheme to fail, but passes when lib2to3 falls back to iterative pattern matching.
- fixes/ -- for use by test_refactor.py

This directory exists so that 3rd party packages can be installed
here.  Read the source for site.py for more details.

Building a Python Mac OS X distribution
=======================================

The ``build-install.py`` script creates Python distributions, including
certain third-party libraries as necessary.  It builds a complete 
framework-based Python out-of-tree, installs it in a funny place with 
$DESTROOT, massages that installation to remove .pyc files and such, creates 
an Installer package from the installation plus other files in ``resources`` 
and ``scripts`` and placed that on a ``.dmg`` disk image.

For Python 2.7.x and 3.2.x, PSF practice is to build two installer variants
for each release.

1.  32-bit-only, i386 and PPC universal, capable on running on all machines
    supported by Mac OS X 10.3.9 through (at least) 10.8::

        /usr/bin/python build-installer.py \
            --sdk-path=/Developer/SDKs/MacOSX10.4u.sdk \
            --universal-archs=32-bit \
            --dep-target=10.3 

    - builds the following third-party libraries

        * Bzip2
        * NCurses
        * GNU Readline (GPL)
        * SQLite 3.7.13
        * Zlib 1.2.3
        * Oracle Sleepycat DB 4.8 (Python 2.x only)

    - requires ActiveState ``Tcl/Tk 8.4`` (currently 8.4.19) to be installed for building

    - recommended build environment:
        
        * Mac OS X 10.5.8 PPC or Intel
        * Xcode 3.1.4
        * ``MacOSX10.4u`` SDK (later SDKs do not support PPC G3 processors)
        * ``MACOSX_DEPLOYMENT_TARGET=10.3``
        * Apple ``gcc-4.0``
        * system Python 2.5 for documentation build with Sphinx

    - alternate build environments:

        * Mac OS X 10.6.8 with Xcode 3.2.6
            - need to change ``/System/Library/Frameworks/{Tcl,Tk}.framework/Version/Current`` to ``8.4``
        * Note Xcode 4.* does not support building for PPC so cannot be used for this build


2.  64-bit / 32-bit, x86_64 and i386 universal, for OS X 10.6 (and later)::

        /usr/bin/python build-installer.py \
            --sdk-path=/Developer/SDKs/MacOSX10.6.sdk \
            --universal-archs=intel \
            --dep-target=10.6

    - builds the following third-party libraries

        * NCurses 5.9 (http://bugs.python.org/issue15037)
        * SQLite 3.7.13

    - uses system-supplied versions of third-party libraries

        * readline module links with Apple BSD editline (libedit)
        * builds Oracle Sleepycat DB 4.8 (Python 2.x only)

    - requires ActiveState Tcl/Tk 8.5.9 (or later) to be installed for building

    - recommended build environment:

        * Mac OS X 10.6.8 (or later)
        * Xcode 3.2.6
        * ``MacOSX10.6`` SDK
        * ``MACOSX_DEPLOYMENT_TARGET=10.6``
        * Apple ``gcc-4.2``
        * system Python 2.6 for documentation build with Sphinx

    - alternate build environments:

        * none.  Xcode 4.x currently supplies two C compilers.
          ``llvm-gcc-4.2.1`` has been found to miscompile Python 3.3.x and
          produce a non-functional Python executable.  As it appears to be
          considered a migration aid by Apple and is not likely to be fixed,
          its use should be avoided.  The other compiler, ``clang``, has been
          undergoing rapid development.  While it appears to have become
          production-ready in the most recent Xcode 4 releases (Xcode 4.5.x
          as of this writing), there are still some open issues when
          building Python and there has not yet been the level of exposure in
          production environments that the Xcode 3 gcc-4.2 compiler has had.


General Prerequisites
---------------------

* No Fink (in ``/sw``) or MacPorts (in ``/opt/local``) or other local
  libraries or utilities (in ``/usr/local``) as they could
  interfere with the build.

* The documentation for the release is built using Sphinx
  because it is included in the installer.

* It is safest to start each variant build with an empty source directory
  populated with a fresh copy of the untarred source.

* It is recommended that you remove any existing installed version of the
  Python being built::

      sudo rm -rf /Library/Frameworks/Python.framework/Versions/n.n


The Recipe
----------

Here are the steps you need to follow to build a Python installer:

* Run ``build-installer.py``. Optionally you can pass a number of arguments
  to specify locations of various files. Please see the top of
  ``build-installer.py`` for its usage.

  Running this script takes some time, it will not only build Python itself
  but also some 3th-party libraries that are needed for extensions.

* When done the script will tell you where the DMG image is (by default
  somewhere in ``/tmp/_py``).

Building other universal installers
...................................

It is also possible to build a 4-way universal installer that runs on 
OS X 10.5 Leopard or later::

    /usr/bin/python /build-installer.py \
        --dep-target=10.5
        --universal-archs=all
        --sdk-path=/Developer/SDKs/MacOSX10.5.sdk

This requires that the deployment target is 10.5, and hence
also that you are building on at least OS X 10.5.  4-way includes
``i386``, ``x86_64``, ``ppc``, and ``ppc64`` (G5).  ``ppc64`` executable
variants can only be run on G5 machines running 10.5.  Note that,
while OS X 10.6 is only supported on Intel-based machines, it is possible
to run ``ppc`` (32-bit) executables unmodified thanks to the Rosetta ppc
emulation in OS X 10.5 and 10.6.  The 4-way installer variant must be
built with Xcode 3.  It is not regularly built or tested.

Other ``--universal-archs`` options are ``64-bit`` (``x86_64``, ``ppc64``),
and ``3-way`` (``ppc``, ``i386``, ``x86_64``).  None of these options
are regularly exercised; use at your own risk.


Testing
-------

Ideally, the resulting binaries should be installed and the test suite run
on all supported OS X releases and architectures.  As a practical matter,
that is generally not possible.  At a minimum, variant 1 should be run on
a PPC G4 system with OS X 10.5 and at least one Intel system running OS X
10.8, 10.7, 10.6, or 10.5.  Variant 2 should be run on 10.8, 10.7, and 10.6
systems in both 32-bit and 64-bit modes.::

    /usr/local/bin/pythonn.n -m test -w -u all,-largefile
    /usr/local/bin/pythonn.n-32 -m test -w -u all
    
Certain tests will be skipped and some cause the interpreter to fail
which will likely generate ``Python quit unexpectedly`` alert messages
to be generated at several points during a test run.  These are normal
during testing and can be ignored.

It is also recommend to launch IDLE and verify that it is at least
functional.  Double-click on the IDLE app icon in ``/Applications/Pythonn.n``.
It should also be tested from the command line::

    /usr/local/bin/idlen.n


This package will install Python $FULL_VERSION for Mac OS X
$MACOSX_DEPLOYMENT_TARGET for the following architecture(s):
$ARCHITECTURES.

               **** IMPORTANT ****

Installing on OS X 10.8 (Mountain Lion) or later systems
========================================================

If you are attempting to install on an OS X 10.8+ system, you may
see a message that Python can't be installed because it is from an
unidentified developer.  This is because this Python installer
package is not yet compatible with the Gatekeeper security feature
introduced in OS X 10.8.  To allow Python to be installed, you
can override the Gatekeeper policy for this install.  In the Finder,
instead of double-clicking, control-click or right click the "Python"
installer package icon.  Then select "Open using ... Installer" from
the contextual menu that appears.

               **** IMPORTANT ****

Update your version of Tcl/Tk to use IDLE or other Tk applications
==================================================================

To use IDLE or other programs that use the Tkinter graphical user
interface toolkit, you may need to install a newer third-party version
of the Tcl/Tk frameworks.  Visit http://www.python.org/download/mac/tcltk/
for current information about supported and recommended versions of
Tcl/Tk for this version of Python and of Mac OS X.


Using this version of Python on OS X
====================================

Python consists of the Python programming language interpreter, plus
a set of programs to allow easy access to it for Mac users including
an integrated development environment, IDLE, plus a set of pre-built
extension modules that open up specific Macintosh technologies to
Python programs.

The installer puts applications, an "Update Shell Profile" command,
and a link to the optionally installed Python Documentation into the
"Python $VERSION" subfolder of the system Applications folder,
and puts the underlying machinery into the folder
$PYTHONFRAMEWORKINSTALLDIR. It can
optionally place links to the command-line tools in /usr/local/bin as
well. Double-click on the "Update Shell Profile" command to add the
"bin" directory inside the framework to your shell's search path.

You must install onto your current boot disk, even though the
installer may not enforce this, otherwise things will not work.

You can verify the integrity of the disk image file containing the
installer package and this ReadMe file by comparing its md5 checksum
and size with the values published on the release page linked at
http://www.python.org/download/

Installation requires approximately $INSTALL_SIZE MB of disk space,
ignore the message that it will take zero bytes.

More information on Python in general can be found at
http://www.python.org.

This folder contains examples of Python usage and useful scripts and tools.

You should be aware that these are not Macintosh-specific but are shared
among Python on all platforms, so there are some that only run on Windows
or Unix or another platform, and/or make little sense on a Macintosh.

The icons for use on MacOS X were created by Jacob Rus <jrus@fas.harvard.edu>
with some feedback from the folks on pythonmac-sig@python.org.


A quick note on what all the files here are, currently (16-7-95),
and whether they really are source or generated.

aegen.py		Generated by aescan, temporary file
AEModule.c		Generated by aescan, from AppleEvents.h
AEObjects.py	Generated by aescan, from AEObjects.h
aepack.py		Routines to convert python objects <-> AEDesc record
				(formerly part of aetools, now imported there)
AERegistry.py	Generated by aescan, from AERegistry.h
aescan.py		Program to scan headers and generate AE modules
aesupport.py	Helper code for aescan
aetools.py		Routines/classes to create and send appleevents
aetypes.py		Classes for python objects corresponding to AEDesc types
				(formerly part of aetools, now imported there)
AppleEvents.py	Generated by aescan, from AppleEvents.h
AppleScript_Suite.py	Generated by gensuitemodule
echo.py			Old test program (may still work) to echo events back to sender
gensuitemodule.py	Program to scan aete/aeut resources and generate python
				interface modules
Required_Suite.py	Generated by gensuitemodule
Standard_Suite.py	Generated by gensuitemodule
tae.py			Old test program (may still work) to send an appleevent
tell.py			Old test program (may still work) to send an appleevent
test_suite.py	Test program to test bits of the _Suite modules and aetools/etc

# CGStubLib was created by issuing this command in MPW:

MakeStub CGStubLib.exp -o CGStubLib

============
MacOSX Notes
============

This document provides a quick overview of some Mac OS X specific features in
the Python distribution.

Mac-specific arguments to configure
===================================

* ``--enable-framework[=DIR]``

  If this argument is specified the build will create a Python.framework rather
  than a traditional Unix install. See the section
  _`Building and using a framework-based Python on Mac OS X` for more 
  information on frameworks.

  If the optional directory argument is specified the framework it installed
  into that directory. This can be used to install a python framework into
  your home directory::

     $ configure --enable-framework=/Users/ronald/Library/Frameworks
     $ make && make install

  This will install the framework itself in ``/Users/ronald/Library/Frameworks``,
  the applications in a subdirectory of ``/Users/ronald/Applications`` and the
  command-line tools in ``/Users/ronald/bin``.

* ``--with-framework-name=NAME``

  Specify the name for the python framework, defaults to ``Python``. This option
  is only valid when ``--enable-framework`` is specified.

* ``--enable-universalsdk[=PATH]``

  Create a universal binary build of Python. This can be used with both
  regular and framework builds.

  The optional argument specifies which OSX SDK should be used to perform the
  build. This defaults to ``/Developer/SDKs/MacOSX.10.4u.sdk``, specify 
  ``/`` when building on a 10.5 system, especially when building 64-bit code.

  See the section _`Building and using a universal binary of Python on Mac OS X`
  for more information.

* ``--with-univeral-archs=VALUE``

  Specify the kind of universal binary that should be created. This option is 
  only valid when ``--enable-universalsdk`` is specified.

  

Building and using a universal binary of Python on Mac OS X
===========================================================

1. What is a universal binary
-----------------------------

A universal binary build of Python contains object code for both PPC and i386
and can therefore run at native speed on both classic powerpc based macs and
the newer intel based macs.

2. How do I build a universal binary
------------------------------------

You can enable universal binaries by specifying the "--enable-universalsdk"
flag to configure::

  $ ./configure --enable-universalsdk
  $ make
  $ make install

This flag can be used with a framework build of python, but also with a classic
unix build. Either way you will have to build python on Mac OS X 10.4 (or later)
with Xcode 2.1 (or later). You also have to install the 10.4u SDK when 
installing Xcode.

The option ``--enable-universalsdk`` has an optional argument to specify an
SDK, which defaults to the 10.4u SDK. When you build on OSX 10.5 or later 
you can use the system headers instead of an SDK::

  $ ./configure --enable-universalsdk=/

2.1 Flavours of universal binaries
..................................

It is possible to build a number of flavours of the universal binary build,
the default is a 32-bit only binary (i386 and ppc). The flavour can be
specified using the option ``--with-universal-archs=VALUE``. The following
values are available:

  * ``32-bit``:   ``ppc``, ``i386``

  * ``64-bit``:   ``ppc64``, ``x86_64``

  * ``all``:      ``ppc``, ``ppc64``, ``i386``, ``x86_64``

  * ``3-way``:	  ``ppc``, ``i386`` and ``x86_64``

  * ``intel``:	  ``i386``, ``x86_64``

To build a universal binary that includes a 64-bit architecture, you must build
on a system running OSX 10.5 or later. The ``all`` flavour can only be built on
OSX 10.5.

The makefile for a framework build will install ``python32`` and ``pythonw32`` 
binaries when the universal architecures includes at least one 32-bit architecture
(that is, for all flavours but ``64-bit``).

Running a specific archicture
.............................

You can run code using a specific architecture using the ``arch`` command::

   $ arch -i386 python

Or to explicitly run in 32-bit mode, regardless of the machine hardware::

   $ arch -i386 -ppc python

NOTE: When you're using a framework install of Python this requires at least
Python 2.7 or 3.2, in earlier versions the python (and pythonw) commands are
wrapper tools that execute the real interpreter without ensuring that the
real interpreter runs with the same architecture.

Building and using a framework-based Python on Mac OS X.
========================================================


1. Why would I want a framework Python instead of a normal static Python?
--------------------------------------------------------------------------

The main reason is because you want to create GUI programs in Python. With the
exception of X11/XDarwin-based GUI toolkits all GUI programs need to be run 
from a fullblown MacOSX application (a ".app" bundle).

While it is technically possible to create a .app without using frameworks you
will have to do the work yourself if you really want this.

A second reason for using frameworks is that they put Python-related items in
only two places: "/Library/Framework/Python.framework" and 
"/Applications/MacPython 2.6".  This simplifies matters for users installing 
Python from a binary distribution if they want to get rid of it again. Moreover,
due to the way frameworks work a user without admin privileges can install a 
binary distribution in his or her home directory without recompilation.

2. How does a framework Python differ from a normal static Python?
------------------------------------------------------------------

In everyday use there is no difference, except that things are stored in
a different place. If you look in /Library/Frameworks/Python.framework
you will see lots of relative symlinks, see the Apple documentation for
details. If you are used to a normal unix Python file layout go down to
Versions/Current and you will see the familiar bin and lib directories.

3. Do I need extra packages?
----------------------------

Yes, probably.  If you want Tkinter support you need to get the OSX AquaTk 
distribution, this is installed by default on Mac OS X 10.4 or later. If
you want wxPython you need to get that. If you want Cocoa you need to get
PyObjC. 

4. How do I build a framework Python?
-------------------------------------

This directory contains a Makefile that will create a couple of python-related
applications (fullblown OSX .app applications, that is) in
"/Applications/MacPython 2.6", and a hidden helper application Python.app 
inside the Python.framework, and unix tools "python" and "pythonw" into 
/usr/local/bin.  In addition it has a target "installmacsubtree" that installs 
the relevant portions of the Mac subtree into the Python.framework.

It is normally invoked indirectly through the main Makefile, as the last step
in the sequence::

 $ ./configure --enable-framework
 $ make
 $ make install

This sequence will put the framework in /Library/Framework/Python.framework,
the applications in "/Applications/MacPython 2.6" and the unix tools in 
/usr/local/bin.

It is possible to select a different name for the framework using the configure 
option ``--with-framework-name=NAME``. This makes it possible to have several
parallel installs of a Python framework.

Installing in another place, for instance $HOME/Library/Frameworks if you have
no admin privileges on your machine, has only been tested very lightly. This
can be done by configuring with --enable-framework=$HOME/Library/Frameworks.
The other two directories, "/Applications/MacPython-2.6" and /usr/local/bin, 
will then also be deposited in $HOME. This is sub-optimal for the unix tools, 
which you would want in $HOME/bin, but there is no easy way to fix this right 
now.

What do all these programs do?
===============================

"IDLE.app" is an integrated development environment for Python: editor,
debugger, etc.

"PythonLauncher.app" is a helper application that will handle things when you
double-click a .py, .pyc or .pyw file. For the first two it creates a Terminal
window and runs the scripts with the normal command-line Python. For the
latter it runs the script in the Python.app interpreter so the script can do
GUI-things. Keep the "alt" key depressed while dragging or double-clicking a
script to set runtime options. These options can be set once and for all
through PythonLauncher's preferences dialog.

"BuildApplet.app" creates an applet from a Python script. Drop the script on it
and out comes a full-featured MacOS application.  BuildApplet.app is now
deprecated and has been removed in Python 3.  As of OS X 10.8, Xcode 4 no
longer supplies the headers for the deprecated QuickDraw APIs used by
the EasyDialogs module making BuildApplet unusable as an app.  It will
not be built by the Mac/Makefile in this case.

The commandline scripts /usr/local/bin/python and pythonw can be used to run
non-GUI and GUI python scripts from the command line, respectively.

How do I create a binary distribution?
======================================

Go to the directory "Mac/OSX/BuildScript". There you'll find a script 
"build-installer.py" that does all the work. This will download and build
a number of 3rd-party libaries, configures and builds a framework Python,
installs it, creates the installer package files and then packs this in a
DMG image.

The script will build a universal binary, you'll therefore have to run this
script on Mac OS X 10.4 or later and with Xcode 2.1 or later installed.

All of this is normally done completely isolated in /tmp/_py, so it does not
use your normal build directory nor does it install into /.

Because of the way the script locates the files it needs you have to run it
from within the BuildScript directory. The script accepts a number of 
command-line arguments, run it with --help for more information.

Configure warnings
==================

The configure script sometimes emits warnings like the one below::

   configure: WARNING: libintl.h: present but cannot be compiled
   configure: WARNING: libintl.h:     check for missing prerequisite headers?
   configure: WARNING: libintl.h: see the Autoconf documentation
   configure: WARNING: libintl.h:     section "Present But Cannot Be Compiled"
   configure: WARNING: libintl.h: proceeding with the preprocessor's result
   configure: WARNING: libintl.h: in the future, the compiler will take precedence
   configure: WARNING:     ## -------------------------------------- ##
   configure: WARNING:     ## Report this to http://bugs.python.org/ ##
   configure: WARNING:     ## -------------------------------------- ##

This almost always means you are trying to build a universal binary for
Python and have libaries in ``/usr/local`` that don't contain the required
architectures. Temporarily move ``/usr/local`` aside to finish the build.


Uninstalling a framework install, including the binary installer
================================================================

Uninstalling a framework can be done by manually removing all bits that got installed.
That's true for both installations from source and installations using the binary installer.
Sadly enough OSX does not have a central uninstaller.

The main bit of a framework install is the framework itself, installed in
``/Library/Frameworks/Python.framework``. This can contain multiple versions
of Python, if you want to remove just one version you have to remove the
version-specific subdirectory: ``/Library/Frameworks/Python.framework/Versions/X.Y``.
If you do that, ensure that ``/Library/Frameworks/Python.framework/Versions/Current``
is a symlink that points to an installed version of Python.

A framework install also installs some applications in ``/Applications/Python X.Y``,

And lastly a framework installation installs files in ``/usr/local/bin``, all of
them symbolic links to files in ``/Library/Frameworks/Python.framework/Versions/X.Y/bin``.

Odds and ends
=============

Something to take note of is that the ".rsrc" files in the distribution are
not actually resource files, they're AppleSingle encoded resource files. The
macresource module and the Mac/OSX/Makefile cater for this, and create
".rsrc.df.rsrc" files on the fly that are normal datafork-based resource
files.

	Jack Jansen, Jack.Jansen@cwi.nl, 15-Jul-2004.
	Ronald Oussoren, RonaldOussoren@mac.com, 30-April-2010

Python Misc subdirectory
========================

This directory contains files that wouldn't fit in elsewhere.  Some
documents are only of historic importance.

Files found here
----------------

ACKS                    Acknowledgements
AIX-NOTES               Notes for building Python on AIX
BeOS-NOTES              Notes for building on BeOS
BeOS-setup.py           setup.py replacement for BeOS, see BeOS-NOTES
build.sh                Script to build and test latest Python from the repository
cheatsheet              Quick summary of Python by Ken Manheimer
developers.txt          A history of who got developer permissions, and why
gdbinit                 Handy stuff to put in your .gdbinit file, if you use gdb
HISTORY                 News from previous releases -- oldest last
indent.pro              GNU indent profile approximating my C style
maintainers.rst         A list of maintainers for library modules
NEWS                    News for this release (for some meaning of "this")
NEWS.help               How to edit NEWS
Porting                 Mini-FAQ on porting to new platforms
PURIFY.README           Information for Purify users
pymemcompat.h           Memory interface compatibility file.
python-config.in        Python script template for python-config
python.man              UNIX man page for the python interpreter
python-mode.el          Emacs mode for editing Python programs
python.pc.in            Package configuration info template for pkg-config
python-wing.wpr         Wing IDE project file
README                  The file you're reading now
README.coverity         Information about running Coverity's Prevent on Python
README.klocwork         Information about running Klocwork's K7 on Python
README.OpenBSD          Help for building problems on OpenBSD
README.valgrind         Information for Valgrind users, see valgrind-python.supp
RFD                     Request For Discussion about a Python newsgroup
RPM                     (Old) tools to build RPMs
setuid-prog.c           C helper program for set-uid Python scripts
SpecialBuilds.txt       Describes extra symbols you can set for debug builds
TextMate                A TextMate bundle for Python development
valgrind-python.supp    Valgrind suppression file, see README.valgrind
vgrindefs               Python configuration for vgrind (a generic pretty printer)
Vim                     Python development utilities for the Vim editor


This documentation tries to help people who intend to use Python on
AIX.

There used to be many issues with Python on AIX, but the major ones
have been corrected for version 3.2, so that Python should now work
rather well on this platform. The remaining known issues are listed in
this document.


======================================================================
			   Compiling Python
----------------------------------------------------------------------

You can compile Python with gcc or the native AIX compiler. The native
compiler used to give better performances on this system with older
versions of Python.  With Python 3.2 it may not be the case anymore,
as this compiler does not allow compiling Python with computed gotos.
Some benchmarks need to be done.

Compiling with gcc:

cd Python-3.2
CC=gcc OPT="-O2" ./configure --enable-shared
make

There are various aliases for the native compiler.  The recommended
alias for compiling Python is 'xlc_r', which provides a better level of
compatibility and handles thread initialization properly.

It is a good idea to add the '-qmaxmem=70000' option, otherwise the
compiler considers various files too complex to optimize.

Compiling with xlc:

cd Python-3.2
CC=xlc_r OPT="-O2 -qmaxmem=70000" ./configure --without-computed-gotos --enable-shared
make

Note:
On AIX 5.3 and earlier, you will also need to specify the
"--disable-ipv6" flag to configure. This has been corrected in AIX
6.1.


======================================================================
			  Memory Limitations
----------------------------------------------------------------------

Note: this section may not apply when compiling Python as a 64 bit
application.

By default on AIX each program gets one segment register for its data
segment. As each segment register covers 256 MB, a Python program that
would use more than 256MB will raise a MemoryError.  The standard
Python test suite is one such application.

To allocate more segment registers to Python, you must use the linker
option -bmaxdata or the ldedit tool to specify the number of bytes you
need in the data segment.

For example, if you want to allow 512MB of memory for Python (this is
enough for the test suite to run without MemoryErrors), you should run
the following command at the end of compilation:

ldedit -b maxdata:0x20000000 ./python

You can allow up to 2GB of memory for Python by using the value
0x80000000 for maxdata.

It is also possible to go beyond 2GB of memory by activating Large
Page Use. You should consult the IBM documentation if you need to use
this option. You can also follow the discussion of this problem
in issue 11212 at bugs.python.org.

http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=/com.ibm.aix.cmds/doc/aixcmds3/ldedit.htm


======================================================================
			     Known issues
----------------------------------------------------------------------

Those issues are currently affecting Python on AIX:

* Python has not been fully tested on AIX when compiled as a 64 bit
  application.

* issue 3526: the memory used by a Python process will never be
  released to the system. If you have a Python application on AIX that
  uses a lot of memory, you should read this issue and you may
  consider using the provided patch that implements a custom malloc
  implementation

* issue 11184: support for large files is currently broken

* issue 11185: os.wait4 does not behave correctly with option WNOHANG

* issue 1745108: there may be some problems with curses.panel

* issue 11192: test_socket fails

* issue 11190: test_locale fails

* issue 11193: test_subprocess fails

* issue 9920: minor arithmetic issues in cmath

* issue 11215: test_fileio fails

* issue 11188: test_time fails


======================================================================
		Implementation details for developers
----------------------------------------------------------------------

Python and python modules can now be built as shared libraries on AIX
as usual.

AIX shared libraries require that an "export" and "import" file be
provided at compile time to list all extern symbols which may be
shared between modules.  The "export" file (named python.exp) for the
modules and the libraries that belong to the Python core is created by
the "makexp_aix" script before performing the link of the python
binary. It lists all global symbols (exported during the link) of the
modules and the libraries that make up the python executable.

When shared library modules (.so files) are made, a second shell
script is invoked.  This script is named "ld_so_aix" and is also
provided with the distribution in the Modules subdirectory.  This
script acts as an "ld" wrapper which hides the explicit management of
"export" and "import" files; it adds the appropriate arguments (in the
appropriate order) to the link command that creates the shared module.
Among other things, it specifies that the "python.exp" file is an
"import" file for the shared module.

This mechanism should be transparent.


Coverity has a static analysis tool (Prevent) which is similar to Klocwork.
They run their tool on the Python source code (SVN head) on a daily basis.
The results are available at:

     http://scan.coverity.com/

About 20 people have access to the analysis reports.  Other
people can be added by request.

Prevent was first run on the Python 2.5 source code in March 2006.
There were originally about 100 defects reported.  Some of these
were false positives.  Over 70 issues were uncovered.

Each warning has a unique id and comments that can be made on it.
When checking in changes due to a warning, the unique id
as reported by the tool was added to the SVN commit message.

False positives were annotated so that the comments can
be reviewed and reversed if the analysis was incorrect.

Contact python-dev@python.org for more information.

=============
Emacs support
=============

If you want to edit Python code in Emacs, you should download python-mode.el
and install it somewhere on your load-path.  See the project page to download:

    https://launchpad.net/python-mode

While Emacs comes with a python.el file, it is not recommended.
python-mode.el is maintained by core Python developers and is generally
considered more Python programmer friendly.  For example, python-mode.el
includes a killer feature called `pdbtrack` which allows you to set a pdb
breakpoint in your code, run your program in an Emacs shell buffer, and do gud
style debugging when the breakpoint is hit.

python-mode.el is compatible with both GNU Emacs from the FSF, and XEmacs.

For more information and bug reporting, see the above project page.  For help,
development, or discussions, see the python-mode mailing list:

    http://mail.python.org/mailman/listinfo/python-mode


..
   Local Variables:
   mode: indented-text
   indent-tabs-mode: nil
   sentence-end-double-space: t
   fill-column: 78
   coding: utf-8
   End:


Klocwork has a static analysis tool (K7) which is similar to Coverity.
They will run their tool on the Python source code on demand.
The results are available at:

     https://opensource.klocwork.com/

Currently, only Neal Norwitz has access to the analysis reports.  Other
people can be added by request.

K7 was first run on the Python 2.5 source code in mid-July 2006.
This is after Coverity had been making their results available.
There were originally 175 defects reported.  Most of these
were false positives.  However, there were numerous real issues 
also uncovered.

Each warning has a unique id and comments that can be made on it.
When checking in changes due to a K7 report, the unique id
as reported by the tool was added to the SVN commit message.
A comment was added to the K7 warning indicating the SVN revision
in addition to any analysis.

False positives were also annotated so that the comments can
be reviewed and reversed if the analysis was incorrect.

A second run was performed on 10-Aug-2006.  The tool was tuned to remove
some false positives and perform some additional checks.  ~150 new
warnings were produced, primarily related to dereferencing NULL pointers.

Contact python-dev@python.org for more information.


2005-01-08

If you are have a problem building on OpenBSD and see output like this
while running configure:

checking curses.h presence... yes
configure: WARNING: curses.h: present but cannot be compiled
configure: WARNING: curses.h: check for missing prerequisite headers?
configure: WARNING: curses.h: see the Autoconf documentation
configure: WARNING: curses.h: section "Present But Cannot Be Compiled"
configure: WARNING: curses.h: proceeding with the preprocessor's result
configure: WARNING: curses.h: in the future, the compiler will take precedence

there is likely a problem that will prevent building python.  
If you see the messages above and are able to completely build python,
please tell python-dev@python.org indicating your version of OpenBSD
and any other relevant system configuration.

The build error that occurs while making may look something like this:

    /usr/include/sys/event.h:53: error: syntax error before "u_int"
    /usr/include/sys/event.h:55: error: syntax error before "u_short"

To fix this problem, you will probably need update Python's configure
script to disable certain options.  Search for a line that looks like:

    OpenBSD/2.* | OpenBSD/3.@<:@012345678@:>@)

If your version is not in that list, e.g., 3.9, add the version
number.  In this case, you would just need to add a 9 after the 8.
If you modify configure.ac, you will need to regenerate configure
with autoconf.

If your version is already in the list, this is not a known problem.
Please submit a bug report here:

    http://sourceforge.net/tracker/?group_id=5470&atid=105470

This document describes some caveats about the use of Valgrind with
Python.  Valgrind is used periodically by Python developers to try
to ensure there are no memory leaks or invalid memory reads/writes.

If you don't want to read about the details of using Valgrind, there
are still two things you must do to suppress the warnings.  First,
you must use a suppressions file.  One is supplied in
Misc/valgrind-python.supp.  Second, you must do one of the following:

  * Uncomment Py_USING_MEMORY_DEBUGGER in Objects/obmalloc.c,
    then rebuild Python
  * Uncomment the lines in Misc/valgrind-python.supp that
    suppress the warnings for PyObject_Free and PyObject_Realloc

If you want to use Valgrind more effectively and catch even more
memory leaks, you will need to configure python --without-pymalloc.
PyMalloc allocates a few blocks in big chunks and most object
allocations don't call malloc, they use chunks doled about by PyMalloc
from the big blocks.  This means Valgrind can't detect
many allocations (and frees), except for those that are forwarded
to the system malloc.  Note: configuring python --without-pymalloc
makes Python run much slower, especially when running under Valgrind.
You may need to run the tests in batches under Valgrind to keep
the memory usage down to allow the tests to complete.  It seems to take
about 5 times longer to run --without-pymalloc.

Apr 15, 2006:
  test_ctypes causes Valgrind 3.1.1 to fail (crash).
  test_socket_ssl should be skipped when running valgrind.
	The reason is that it purposely uses uninitialized memory.
	This causes many spurious warnings, so it's easier to just skip it.


Details:
--------
Python uses its own small-object allocation scheme on top of malloc,
called PyMalloc.

Valgrind may show some unexpected results when PyMalloc is used.
Starting with Python 2.3, PyMalloc is used by default.  You can disable
PyMalloc when configuring python by adding the --without-pymalloc option.
If you disable PyMalloc, most of the information in this document and
the supplied suppressions file will not be useful.  As discussed above,
disabling PyMalloc can catch more problems.

If you use valgrind on a default build of Python,  you will see
many errors like:

        ==6399== Use of uninitialised value of size 4
        ==6399== at 0x4A9BDE7E: PyObject_Free (obmalloc.c:711)
        ==6399== by 0x4A9B8198: dictresize (dictobject.c:477)

These are expected and not a problem.  Tim Peters explains
the situation:

        PyMalloc needs to know whether an arbitrary address is one
	that's managed by it, or is managed by the system malloc.
	The current scheme allows this to be determined in constant
	time, regardless of how many memory areas are under pymalloc's
	control.

        The memory pymalloc manages itself is in one or more "arenas",
	each a large contiguous memory area obtained from malloc.
	The base address of each arena is saved by pymalloc
	in a vector.  Each arena is carved into "pools", and a field at
	the start of each pool contains the index of that pool's arena's
	base address in that vector.

        Given an arbitrary address, pymalloc computes the pool base
	address corresponding to it, then looks at "the index" stored
	near there.  If the index read up is out of bounds for the
	vector of arena base addresses pymalloc maintains, then
	pymalloc knows for certain that this address is not under
	pymalloc's control.  Otherwise the index is in bounds, and
	pymalloc compares

            the arena base address stored at that index in the vector

        to

            the arbitrary address pymalloc is investigating

        pymalloc controls this arbitrary address if and only if it lies
        in the arena the address's pool's index claims it lies in.

        It doesn't matter whether the memory pymalloc reads up ("the
	index") is initialized.  If it's not initialized, then
	whatever trash gets read up will lead pymalloc to conclude
	(correctly) that the address isn't controlled by it, either
	because the index is out of bounds, or the index is in bounds
	but the arena it represents doesn't contain the address.

        This determination has to be made on every call to one of
	pymalloc's free/realloc entry points, so its speed is critical
	(Python allocates and frees dynamic memory at a ferocious rate
	-- everything in Python, from integers to "stack frames",
	lives in the heap).

This directory contains support file used to build RPM releases of
Python.  Its contents are maintained by Sean Reifschneider
<jafo@tummy.com>.

It is recommended that RPM builders use the python*.src.rpm file
downloaded from the "ftp.python.org:/pub/python/<version>/rpms".  These
may be more up to date than the files included in the base Python
release tar-file.

If you wish to build RPMs from the base Python release tar-file, note
that you will have to download the
"doc/<version>/html-<version>.tar.bz2"
file from python.org and place it into your "SOURCES" directory for
the build to complete.  This is the same directory that you place the
Python-2.3.1 release tar-file in.  You can then use the ".spec" file in
this directory to build RPMs.

To generate or modify mapping headers
-------------------------------------
Mapping headers are imported from CJKCodecs as pre-generated form.
If you need to tweak or add something on it, please look at tools/
subdirectory of CJKCodecs' distribution.



Notes on implmentation characteristics of each codecs
-----------------------------------------------------

1) Big5 codec

  The big5 codec maps the following characters as cp950 does rather
  than conforming Unicode.org's that maps to 0xFFFD.

    BIG5        Unicode     Description

    0xA15A      0x2574      SPACING UNDERSCORE
    0xA1C3      0xFFE3      SPACING HEAVY OVERSCORE
    0xA1C5      0x02CD      SPACING HEAVY UNDERSCORE
    0xA1FE      0xFF0F      LT DIAG UP RIGHT TO LOW LEFT
    0xA240      0xFF3C      LT DIAG UP LEFT TO LOW RIGHT
    0xA2CC      0x5341      HANGZHOU NUMERAL TEN
    0xA2CE      0x5345      HANGZHOU NUMERAL THIRTY

  Because unicode 0x5341, 0x5345, 0xFF0F, 0xFF3C is mapped to another
  big5 codes already, a roundtrip compatibility is not guaranteed for
  them.


2) cp932 codec

  To conform to Windows's real mapping, cp932 codec maps the following
  codepoints in addition of the official cp932 mapping.

    CP932     Unicode     Description

    0x80      0x80        UNDEFINED
    0xA0      0xF8F0      UNDEFINED
    0xFD      0xF8F1      UNDEFINED
    0xFE      0xF8F2      UNDEFINED
    0xFF      0xF8F3      UNDEFINED


3) euc-jisx0213 codec

  The euc-jisx0213 codec maps JIS X 0213 Plane 1 code 0x2140 into
  unicode U+FF3C instead of U+005C as on unicode.org's mapping.
  Because euc-jisx0213 has REVERSE SOLIDUS on 0x5c already and A140
  is shown as a full width character, mapping to U+FF3C can make
  more sense.

  The euc-jisx0213 codec is enabled to decode JIS X 0212 codes on
  codeset 2. Because JIS X 0212 and JIS X 0213 Plane 2 don't have
  overlapped by each other, it doesn't bother standard conformations
  (and JIS X 0213 Plane 2 is intended to use so.) On encoding
  sessions, the codec will try to encode kanji characters in this
  order:

    JIS X 0213 Plane 1 -> JIS X 0213 Plane 2 -> JIS X 0212


4) euc-jp codec

  The euc-jp codec is a compatibility instance on these points:
   - U+FF3C FULLWIDTH REVERSE SOLIDUS is mapped to EUC-JP A1C0 (vice versa)
   - U+00A5 YEN SIGN is mapped to EUC-JP 0x5c. (one way)
   - U+203E OVERLINE is mapped to EUC-JP 0x7e. (one way)


5) shift-jis codec

  The shift-jis codec is mapping 0x20-0x7e area to U+20-U+7E directly
  instead of using JIS X 0201 for compatibility. The differences are:
   - U+005C REVERSE SOLIDUS is mapped to SHIFT-JIS 0x5c.
   - U+007E TILDE is mapped to SHIFT-JIS 0x7e.
   - U+FF3C FULL-WIDTH REVERSE SOLIDUS is mapped to SHIFT-JIS 815f.


ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.3 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format). These documents are also available in other
formats from ftp://ftp.uu.net/graphics/png/documents/zlib/zdoc-index.html

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org). A usage example
of the library is given in the file example.c which also tests that the library
is working correctly. Another example is given in the file minigzip.c. The
compression library itself is composed of all source files except example.c and
minigzip.c.

To compile all files and run the test program, follow the instructions given at
the top of Makefile. In short "make test; make install" should work for most
machines. For Unix: "./configure; make test; make install". For MSDOS, use one
of the special makefiles such as Makefile.msc. For VMS, use make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version. The zlib home page is
http://www.zlib.org or http://www.gzip.org/zlib/ Before reporting a problem,
please check this site to verify that you have the latest version of zlib;
otherwise get the latest version and check whether the problem still exists or
not.

PLEASE read the zlib FAQ http://www.gzip.org/zlib/zlib_faq.html before asking
for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan. 1997
issue of  Dr. Dobb's Journal; a copy of the article is available in
http://dogma.net/markn/articles/zlibtool/zlibtool.htm

The changes made in version 1.2.3 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory "contrib".

A Java implementation of zlib is available in the Java Development Kit
http://java.sun.com/j2se/1.4.2/docs/api/java/util/zip/package-summary.html
See the zlib home page http://www.zlib.org for details.

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is in the
CPAN (Comprehensive Perl Archive Network) sites
http://www.cpan.org/modules/by-module/Compress/

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://www.python.org/doc/lib/module-zlib.html

A zlib binding for TCL written by Andreas Kupries <a.kupries@westend.com> is
availlable at http://www.oche.de/~akupries/soft/trf/trf_zip.html

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS, BEOS and by some Mac compilers.

- For PalmOs, see http://palmzlib.sourceforge.net/

- When building a shared, i.e. dynamic library on Mac OS X, the library must be
  installed before testing (do "make install" before "make test"), since the
  library location is specified in the library.


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz. The deflate
  and zlib specifications were written by L. Peter Deutsch. Thanks to all the
  people who reported problems and suggested various improvements in zlib;
  they are too numerous to cite here.

Copyright notice:

 (C) 1995-2004 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not*
receiving lengthy legal documents to sign. The sources are provided
for free but without warranty of any kind.  The library has been
entirely written by Jean-loup Gailly and Mark Adler; it does not
include third-party code.

If you redistribute modified sources, we would appreciate that you include
in the file ChangeLog history information documenting your changes. Please
read the FAQ for more information on the distribution of modified source
versions.

dlcompat for Darwin
=========================

This is dlcompat, a small library that emulates the dlopen()
interface on top of Darwin's dyld API.

dlcompat allows loading a ".dylib" library (as long as the RTLD_LOCAL 
flag isn't passed to dlopen()). It can be configured to yield a warning 
when trying to close it (dynamic libraries cannot currently be unloaded).

It automatically searches for modules in several directories when no 
absolute path is specified and the module is not found in the current 
directory.

The paths searched are those specified in the environment variables
LD_LIBRARY_PATH and DYLD_LIBRARY_PATH plus /lib, /usr/local/lib and 
/usr/lib or the path specified in the environment variable 
DYLD_FALLBACK_LIBRARY_PATH.

In the default install the behavior of dlsym is to automatically prepend
an underscore to passed in symbol names, this allows easier porting of
applications which were written specifically for ELF based lifeforms.

Installation
--------------
Type:
	./configure
	make
	sudo make install

This will compile the source file, generate both a static and shared
library called libdl and install it into /usr/local/lib. The header
file dlfcn.h will be installed in /usr/local/include.

If you want to place the files somewhere else, run

  make clean
  ./configure --prefix=<prefix>
  make
  sudo make install

where <prefix> is the hierarchy you want to install into, e.g. /usr
for /usr/lib and /usr/include (_NOT_ recommended!).

To enable debugging output (useful for me), run

  make clean
  ./configure --enable-debug
  make
  sudo make install
  
If you want old dlcompat style behavior of not prepending the underscore
on calls to dlsym then type:

  make clean
  ./configure --enable-fink
  make
  sudo make install

Usage
-------
Software that uses GNU autoconf will likely check for a library called
libdl, that's why I named it that way. For software that doesn't find
the library on its own, you must add a '-ldl' to the appropriate
Makefile (or environment) variable, usually LIBS.

If you installed dlcompat into a directory other than /usr/local/lib,
you must tell the compiler where to find it. Add '-L<prefix>/lib' to
LDFLAGS (or CFLAGS) and '-I<prefix>/include' to CPPFLAGS (or CFLAGS).

Notes
-----
If you are writing new software and plan to have Mac OX X compatibility you
should look at the dyld api's in /usr/include/mach-o/dyld.h, rather than
using dlcompat, using the native api's is the supported method of loading
dynamically on Mac OS X, if you want an small example, look at dlfcn_simple.c,
which should help get you started.

Also note that the functions in dlcompat are not thread safe, and while it is not
POSIX spec compliant, it is about as close to compliance as it is going to get though.

You can always get the latest version from opendarwin cvs:

  cvs -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od login
  cvs -z3 -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od \
               co -d dlcompat proj/dlcompat


It is hoped that this library will be useful, and as bug free as possible, if you find
any bugs please let us know about them so they can be fixed.

Please send bug reports to Peter O'Gorman <ogorman@users.sourceforge.net>

Thanks.


The files in this directory are taken from
http://www.opendarwin.org/cgi-bin/cvsweb.cgi/~checkout~/proj/dlcompat/

The LICENSE in this directory applies to these files.

Thomas Heller, Jan 2003

These files have been modified so they fall back to the system
dlfcn calls if available in libSystem.

Bob Ippolito, Feb 2006

Status
======

libffi-3.0.13 was released on March 17, 2013.  Check the libffi web
page for updates: <URL:http://sourceware.org/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

FFI stands for Foreign Function Interface.  A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms
===================

Libffi has been ported to many different platforms.
For specific configuration details and testing status, please
refer to the wiki page here:

 http://www.moxielogic.org/wiki/index.php?title=Libffi_3.0.13

At the time of release, the following basic configurations have been
tested:

|-----------------+------------------+-------------------------|
| Architecture    | Operating System | Compiler                |
|-----------------+------------------+-------------------------|
| AArch64         | Linux            | GCC                     |
| Alpha           | Linux            | GCC                     |
| Alpha           | Tru64            | GCC                     |
| ARM             | Linux            | GCC                     |
| ARM             | iOS              | GCC                     |
| AVR32           | Linux            | GCC                     |
| Blackfin        | uClinux          | GCC                     |
| HPPA            | HPUX             | GCC                     |
| IA-64           | Linux            | GCC                     |
| M68K            | FreeMiNT         | GCC                     |
| M68K            | Linux	     | GCC                     |
| M68K            | RTEMS            | GCC                     |
| Meta            | Linux            | GCC                     |
| MicroBlaze      | Linux            | GCC                     |
| MIPS            | IRIX             | GCC                     |
| MIPS            | Linux            | GCC                     |
| MIPS            | RTEMS            | GCC                     |
| MIPS64          | Linux            | GCC                     |
| Moxie		  | Bare metal	     | GCC
| PowerPC 32-bit  | AIX              | IBM XL C                |
| PowerPC 64-bit  | AIX              | IBM XL C                |
| PowerPC         | AMIGA            | GCC                     |
| PowerPC         | Linux            | GCC                     |
| PowerPC         | Mac OSX          | GCC                     |
| PowerPC         | FreeBSD          | GCC                     |
| PowerPC 64-bit  | FreeBSD          | GCC                     |
| PowerPC 64-bit  | Linux            | GCC                     |
| S390            | Linux            | GCC                     |
| S390X           | Linux            | GCC                     |
| SPARC           | Linux            | GCC                     |
| SPARC           | Solaris          | GCC                     |
| SPARC           | Solaris          | Oracle Solaris Studio C |
| SPARC64         | Linux            | GCC                     |
| SPARC64         | FreeBSD          | GCC                     |
| SPARC64         | Solaris          | Oracle Solaris Studio C |
| TILE-Gx/TILEPro | Linux            | GCC                     |
| X86             | FreeBSD          | GCC                     |
| X86             | GNU HURD         | GCC                     |
| X86             | Interix          | GCC                     |
| X86             | kFreeBSD         | GCC                     |
| X86             | Linux            | GCC                     |
| X86             | Mac OSX          | GCC                     |
| X86             | OpenBSD          | GCC                     |
| X86             | OS/2             | GCC                     |
| X86             | Solaris          | GCC                     |
| X86             | Solaris          | Oracle Solaris Studio C |
| X86             | Windows/Cygwin   | GCC                     |
| X86             | Windows/MingW    | GCC                     |
| X86-64          | FreeBSD          | GCC                     |
| X86-64          | Linux            | GCC                     |
| X86-64          | Linux/x32        | GCC                     |
| X86-64          | OpenBSD          | GCC                     |
| X86-64          | Solaris          | Oracle Solaris Studio C |
| X86-64          | Windows/MingW    | GCC                     |
| Xtensa          | Linux            | GCC                     |
|-----------------+------------------+-------------------------|

Please send additional platform test results to
libffi-discuss@sourceware.org and feel free to update the wiki page
above.

Installing libffi
=================

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

It's also possible to build libffi on Windows platforms with
Microsoft's Visual C++ compiler.  In this case, use the msvcc.sh
wrapper script during configuration like so:

path/to/configure CC=path/to/msvcc.sh LD=link CPP=\"cl -nologo -EP\"

For 64-bit Windows builds, use CC="path/to/msvcc.sh -m64".
You may also need to specify --build appropriately. When building with MSVC
under a MingW environment, you may need to remove the line in configure
that sets 'fix_srcfile_path' to a 'cygpath' command. ('cygpath' is not
present in MingW, and is not required when using MingW-style paths.)

For iOS builds, the 'libffi.xcodeproj' Xcode project is available.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make.  You can ftp GNU make from ftp.gnu.org:/pub/gnu/make .

To ensure that libffi is working as advertised, type "make check".
This will require that you have DejaGNU installed.

To install the library and header files, type "make install".


History
=======

See the ChangeLog files for details.

3.0.13 Mar-17-13
	Add Meta support.
	Add missing Moxie bits.
	Fix stack alignment bug on 32-bit x86.
	Build fix for m68000 targets.
	Build fix for soft-float Power targets.
	Fix the install dir location for some platforms when building
	  with GCC (OS X, Solaris).
	Fix Cygwin regression.

3.0.12 Feb-11-13
        Add Moxie support.
	Add AArch64 support.
	Add Blackfin support.
	Add TILE-Gx/TILEPro support.
	Add MicroBlaze support.
	Add Xtensa support.
	Add support for PaX enabled kernels with MPROTECT.
	Add support for native vendor compilers on
	  Solaris and AIX.
	Work around LLVM/GCC interoperability issue on x86_64.

3.0.11 Apr-11-12
        Lots of build fixes.
	Add Amiga newer MacOS support.
	Add support for variadic functions (ffi_prep_cif_var).
	Add Linux/x32 support.
	Add thiscall, fastcall and MSVC cdecl support on Windows.
	Add Amiga and newer MacOS support.
	Add m68k FreeMiNT support.
	Integration with iOS' xcode build tools.
	Fix Octeon and MC68881 support.
	Fix code pessimizations.

3.0.10 Aug-23-11
        Add support for Apple's iOS.
	Add support for ARM VFP ABI.
        Add RTEMS support for MIPS and M68K.
	Fix instruction cache clearing problems on
	  ARM and SPARC.
	Fix the N64 build on mips-sgi-irix6.5.
	Enable builds with Microsoft's compiler.
	Enable x86 builds with Oracle's Solaris compiler.
	Fix support for calling code compiled with Oracle's Sparc
	  Solaris compiler.
	Testsuite fixes for Tru64 Unix.
	Additional platform support.

3.0.9 Dec-31-09
        Add AVR32 and win64 ports.  Add ARM softfp support.
	Many fixes for AIX, Solaris, HP-UX, *BSD.
	Several PowerPC and x86-64 bug fixes.
	Build DLL for windows.

3.0.8 Dec-19-08
        Add *BSD, BeOS, and PA-Linux support.

3.0.7 Nov-11-08
        Fix for ppc FreeBSD.
	(thanks to Andreas Tobler)

3.0.6 Jul-17-08
        Fix for closures on sh.
	Mark the sh/sh64 stack as non-executable.
	(both thanks to Kaz Kojima)

3.0.5 Apr-3-08
        Fix libffi.pc file.
	Fix #define ARM for IcedTea users.
	Fix x86 closure bug.

3.0.4 Feb-24-08
        Fix x86 OpenBSD configury.

3.0.3 Feb-22-08
        Enable x86 OpenBSD thanks to Thomas Heller, and
	x86-64 FreeBSD thanks to BjÃ¶rn KÃ¶nig and Andreas Tobler.
	Clean up test instruction in README.

3.0.2 Feb-21-08
        Improved x86 FreeBSD support.
	Thanks to BjÃ¶rn KÃ¶nig.

3.0.1 Feb-15-08
        Fix instruction cache flushing bug on MIPS.
	Thanks to David Daney.

3.0.0 Feb-15-08
        Many changes, mostly thanks to the GCC project.
	Cygnus Solutions is now Red Hat.

  [10 years go by...]

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was originally written by Anthony Green <green@redhat.com>.

The developers of the GNU Compiler Collection project have made
innumerable valuable contributions.  See the ChangeLog file for
details.

Some of the ideas behind libffi were inspired by Gianni Mariani's free
gencall library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

Major processor architecture ports were contributed by the following
developers:

aarch64		Marcus Shawcroft, James Greenhalgh
alpha		Richard Henderson
arm		Raffaele Sena
blackfin        Alexandre Keunecke I. de Mendonca
cris		Simon Posnjak, Hans-Peter Nilsson
frv		Anthony Green
ia64		Hans Boehm
m32r		Kazuhiro Inaoka
m68k		Andreas Schwab
microblaze	Nathan Rossi
mips		Anthony Green, Casey Marshall
mips64		David Daney
moxie		Anthony Green
pa		Randolph Chung, Dave Anglin, Andreas Tobler
powerpc		Geoffrey Keating, Andreas Tobler, 
			 David Edelsohn, John Hornkvist
powerpc64	Jakub Jelinek
s390		Gerhard Tonn, Ulrich Weigand
sh		Kaz Kojima
sh64		Kaz Kojima
sparc		Anthony Green, Gordon Irlam
tile-gx/tilepro Walter Lee
x86		Anthony Green, Jon Beniston
x86-64		Bo Thorsen
xtensa		Chris Zankel

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes, documentation and
configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

Andreas Tobler has done a tremendous amount of work on the testsuite.

Alex Oliva solved the executable page problem for SElinux.

The list above is almost certainly incomplete and inaccurate.  I'm
happy to make corrections or additions upon request.

If you have a problem, or have found a bug, please send a note to the
author at green@moxielogic.com, or the project mailing list at
libffi-discuss@sourceware.org.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

The purpose is to hack the libffi sources so that they can be compiled
with MSVC, and to extend them so that they have the features I need
for ctypes.

I retrieved the libffi sources from the gcc cvs repository on
2004-01-27.  Then I did 'configure' in a 'build' subdirectory on a x86
linux system, and copied the files I found useful.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

This directory contains a slightly modified version of libffi, extracted from
the GCC source-tree.

The only modifications are those that are necessary to compile libffi using 
the Apple provided compiler and outside of the GCC source tree.

bits shared by the stringobject and unicodeobject implementations (and
possibly other modules, in a not too distant future).

the stuff in here is included into relevant places; see the individual
source files for details.

--------------------------------------------------------------------
the following defines used by the different modules:

STRINGLIB_CHAR

    the type used to hold a character (char or Py_UNICODE)

STRINGLIB_EMPTY

    a PyObject representing the empty string, only to be used if
    STRINGLIB_MUTABLE is 0

Py_ssize_t STRINGLIB_LEN(PyObject*)

    returns the length of the given string object (which must be of the
    right type)

PyObject* STRINGLIB_NEW(STRINGLIB_CHAR*, Py_ssize_t)

    creates a new string object

STRINGLIB_CHAR* STRINGLIB_STR(PyObject*)

    returns the pointer to the character data for the given string
    object (which must be of the right type)

int STRINGLIB_CHECK_EXACT(PyObject *)

    returns true if the object is an instance of our type, not a subclass

STRINGLIB_MUTABLE

    must be 0 or 1 to tell the cpp macros in stringlib code if the object
    being operated on is mutable or not


XXX Write description
XXX Dont't forget to mention upx

XXX Add pointer to this file into PC/README.txt

Example Python extension for Windows NT
=======================================

This directory contains everything needed (except for the Python
distribution!) to build a Python extension module using Microsoft VC++.
Notice that you need to use the same compiler version that was used to build 
Python itself.

The simplest way to build this example is to use the distutils script
'setup.py'.  To do this, simply execute:

  % python setup.py install

after everything builds and installs, you can test it:

  % python -c "import example; example.foo()"
  Hello, world

See setup.py for more details.  alternatively, see below for instructions on 
how to build inside the Visual Studio environment.

Visual Studio Build Instructions
================================

These are instructions how to build an extension using Visual C++.  The
instructions and project files have not been updated to the latest VC
version.  In general, it is recommended you use the 'setup.py' instructions
above.

It has been tested with VC++ 7.1 on Python 2.4.  You can also use earlier 
versions of VC to build Python extensions, but the sample VC project file 
(example.dsw in this directory) is in VC 7.1 format.

COPY THIS DIRECTORY!
--------------------
This "example_nt" directory is a subdirectory of the PC directory, in order
to keep all the PC-specific files under the same directory.  However, the
example_nt directory can't actually be used from this location.  You first
need to copy or move it up one level, so that example_nt is a direct
sibling of the PC\ and Include\ directories.  Do all your work from within
this new location -- sorry, but you'll be sorry if you don't.

OPEN THE PROJECT
----------------
From VC 7.1, use the
    File -> Open Solution...
dialog (*not* the "File -> Open..." dialog!).  Navigate to and select the
file "example.sln", in the *copy* of the example_nt directory you made
above.
Click Open.

BUILD THE EXAMPLE DLL
---------------------
In order to check that everything is set up right, try building:

1. Select a configuration.  This step is optional.  Do
       Build -> Configuration Manager... -> Active Solution Configuration
   and select either "Release" or "Debug".
   If you skip this step, you'll use the Debug configuration by default.

2. Build the DLL.  Do
       Build -> Build Solution
   This creates all intermediate and result files in a subdirectory which
   is called either Debug or Release, depending on which configuration you
   picked in the preceding step.

TESTING THE DEBUG-MODE DLL
--------------------------
Once the Debug build has succeeded, bring up a DOS box, and cd to
example_nt\Debug.  You should now be able to repeat the following session
("C>" is the DOS prompt, ">>>" is the Python prompt) (note that various
debug output from Python may not match this screen dump exactly):

    C>..\..\PCbuild\python_d
    Adding parser accelerators ...
    Done.
    Python 2.2c1+ (#28, Dec 14 2001, 18:06:39) [MSC 32 bit (Intel)] on win32
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import example
    [7052 refs]
    >>> example.foo()
    Hello, world
    [7052 refs]
    >>>

TESTING THE RELEASE-MODE DLL
----------------------------
Once the Release build has succeeded, bring up a DOS box, and cd to
example_nt\Release.  You should now be able to repeat the following session
("C>" is the DOS prompt, ">>>" is the Python prompt):

    C>..\..\PCbuild\python
    Python 2.2c1+ (#28, Dec 14 2001, 18:06:04) [MSC 32 bit (Intel)] on win32
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import example
    >>> example.foo()
    Hello, world
    >>>

Congratulations!  You've successfully built your first Python extension
module.

CREATING YOUR OWN PROJECT
-------------------------
Choose a name ("spam" is always a winner :-) and create a directory for
it.  Copy your C sources into it.  Note that the module source file name
does not necessarily have to match the module name, but the "init" function
name should match the module name -- i.e. you can only import a module
"spam" if its init function is called "initspam()", and it should call
Py_InitModule with the string "spam" as its first argument (use the minimal
example.c in this directory as a guide).  By convention, it lives in a file
called "spam.c" or "spammodule.c".  The output file should be called
"spam.dll" or "spam.pyd" (the latter is supported to avoid confusion with a
system library "spam.dll" to which your module could be a Python interface)
in Release mode, or spam_d.dll or spam_d.pyd in Debug mode.

Now your options are:

1) Copy example.sln and example.vcproj, rename them to spam.*, and edit them
by hand.

or

2) Create a brand new project; instructions are below.

In either case, copy example_nt\example.def to spam\spam.def, and edit the
new spam.def so its second line contains the string "initspam".  If you
created a new project yourself, add the file spam.def to the project now.
(This is an annoying little file with only two lines.  An alternative
approach is to forget about the .def file, and add the option
"/export:initspam" somewhere to the Link settings, by manually editing the
"Project -> Properties -> Linker -> Command Line -> Additional Options" 
box).

You are now all set to build your extension, unless it requires other
external libraries, include files, etc.  See Python's Extending and
Embedding manual for instructions on how to write an extension.


CREATING A BRAND NEW PROJECT
----------------------------
Use the
    File -> New -> Project...
dialog to create a new Project Workspace.  Select "Visual C++ Projects/Win32/
Win32 Project", enter the name ("spam"), and make sure the "Location" is 
set to parent of the spam directory you have created (which should be a direct 
subdirectory of the Python build tree, a sibling of Include and PC).  
In "Application Settings", select "DLL", and "Empty Project".  Click OK.

You should now create the file spam.def as instructed in the previous
section. Add the source files (including the .def file) to the project, 
using "Project", "Add Existing Item".

Now open the
    Project -> spam properties...
dialog.  (Impressive, isn't it? :-) You only need to change a few
settings.  Make sure "All Configurations" is selected from the "Settings
for:" dropdown list.  Select the "C/C++" tab.  Choose the "General"
category in the popup menu at the top.  Type the following text in the
entry box labeled "Addditional Include Directories:"

    ..\Include,..\PC

Then, choose the "General" category in the "Linker" tab, and enter
    ..\PCbuild
in the "Additional library Directories" box.

Now you need to add some mode-specific settings (select "Accept"
when asked to confirm your changes):

Select "Release" in the "Configuration" dropdown list.  Click the
"Link" tab, choose the "Input" Category, and append "python24.lib" to the
list in the "Additional Dependencies" box.

Select "Debug" in the "Settings for:" dropdown list, and append
"python24_d.lib" to the list in the Additional Dependencies" box.  Then
click on the C/C++ tab, select "Code Generation", and select 
"Multi-threaded Debug DLL" from the "Runtime library" dropdown list.

Select "Release" again from the "Settings for:" dropdown list.
Select "Multi-threaded DLL" from the "Use run-time library:" dropdown list.

That's all <wink>.

This is a port of Python 2.6 to OS/2 using the EMX development tools
=========================================================================

What's new since the previous release
-------------------------------------

Another day, another version...


Licenses and info about Python and EMX
--------------------------------------

Please read the file README.Python-2.6 included in this package for 
information about Python 2.6.  This file is the README file from the 
Python 2.6 source distribution available via http://www.python.org/ 
and its mirrors.  The file LICENCE.Python-2.6 is the text of the Licence 
from the Python 2.6 source distribution.

Note that the EMX package that this package depends on is released under 
the GNU General Public Licence.  Please refer to the documentation 
accompanying the EMX Runtime libraries for more information about the 
implications of this.  A copy of version 2 of the GPL is included as the 
file COPYING.gpl2.

Readline and GDBM are covered by the GNU General Public Licence.  I think 
Eberhard Mattes' porting changes to BSD DB v1.85 are also GPL'ed (BSD DB 
itself is BSD Licenced).  ncurses and expat appear to be covered by MIT 
style licences - please refer to the source distributions for more detail.  
zlib is distributable under a very free license.  GNU UFC is under the 
GNU LGPL (see file COPYING.lib).

My patches to the Python-2.x source distributions, and any other packages 
used in this port, are placed in the public domain.

This software is provided 'as-is', without any express or implied warranty.
In no event will the author be held liable for any damages arising from the 
use of the software.

I do hope however that it proves useful to someone.


Other ports
-----------

There have been ports of previous versions of Python to OS/2.

The best known would be that by Jeff Rush, most recently of version 
1.5.2.  Jeff used IBM's Visual Age C++ (v3) for his ports, and his 
patches have been included in the Python 2.6 source distribution.

Andy Zabolotny implemented a port of Python v1.5.2 using the EMX 
development tools.  His patches against the Python v1.5.2 source 
distribution have become the core of this port, and without his efforts 
this port wouldn't exist.  Andy's port also appears to have been 
compiled with his port of gcc 2.95.2 to EMX, which I have but have 
chosen not to use for the binary distribution of this port (see item 16 
of the "YOU HAVE BEEN WARNED" section below).

It is possible to have these earlier ports still usable after installing 
this port - see the README.os2emx.multiple_versions file, contributed by
Dr David Mertz, for a suggested approach to achieving this.


Software requirements
---------------------

This package requires the EMX Runtime package, available from the 
Hobbes (http://hobbes.nmsu.edu/) and LEO (http://archiv.leo.org/) 
archives of OS/2 software.  I have used EMX version 0.9d fix04 in 
developing this port.

My development system is running OS/2 v4 with fixpack 12.

3rd party software which has been linked into dynamically loaded modules:
- ncurses      (see http://dickey.his.com/ for more info, v5.2)
- GNU Readline (Kai Uwe Rommel's port available from Hobbes or LEO, v2.1)
- GNU GDBM     (Kai Uwe Rommel's port available from Hobbes or LEO, v1.7.3)
- zlib         (derived from Hung-Chi Chu's port of v1.1.3, v1.1.4)
- expat        (distributed with Python, v1.95.6)
- GNU UFC      (Kai Uwe Rommel's port available from LEO, v2.0.4)


About this port
---------------

I have attempted to make this port as complete and functional as I can, 
notwithstanding the issues in the "YOU HAVE BEEN WARNED" section below.

Core components:

Python.exe is linked as an a.out executable, ie using EMX method E1 
to compile & link the executable.  This is so that fork() works (see 
"YOU HAVE BEEN WARNED" item 1).

Python26.dll is created as a normal OMF DLL, with an OMF import 
library and module definition file.  There is also an a.out (.a) import 
library to support linking the DLL to a.out executables.  The DLL 
requires the EMX runtime DLLs.

This port has been built with complete support for multithreading.

Modules:

With the exception of modules that have a significant code size, or are 
not recommended or desired for normal use, the standard modules are now 
built into the core DLL rather than configured as dynamically loadable 
modules.  This is for both reasons of performance (startup time) and 
memory use (lots of small DLLs fragment the address space).

I haven't yet changed the building of Python's dynamically loadable 
modules over to using the DistUtils.

See "YOU HAVE BEEN WARNED" item 3 for notes about the fcntl module, and 
"YOU HAVE BEEN WARNED" item 10 for notes about the pwd and grp modules.

This port supports case sensitive module import semantics, matching 
the Windows release.  This can be deactivated by setting the PYTHONCASEOK 
environment variable (the value doesn't matter) - see "YOU HAVE BEEN WARNED" 
item 12.

Optional modules:

Where I've been able to locate the required 3rd party packages already 
ported to OS/2, I've built and included them.

These include ncurses (_curses, _curses_panel), BSD DB (bsddb185), 
GNU GDBM (gdbm, dbm), zlib (zlib), GNU Readline (readline), and GNU UFC 
(crypt).

Expat is now included in the Python release sourceball, and the pyexpat 
module is always built.

I have built these modules statically linked against the 3rd party 
libraries.  Unfortunately my attempts to use the dll version of GNU 
readline have been a dismal failure, in that when the dynamically 
linked readline module is active other modules immediately provoke a 
core dump when imported.

Only the BSD DB package (part of the BSD package distributed with EMX) 
needs source modifications to be used for this port, pertaining to use 
of errno with multithreading.

The other packages, except for ncurses and zlib, needed Makefile changes 
for multithreading support but no source changes.

The _curses_panel module is a potential problem - see "YOU HAVE BEEN 
WARNED" item 13.

Upstream source patches:

No updates to the Python 2.6 release have become available.

Eberhard Mattes' EMXFIX04 update to his EMX 0.9d tools suite includes 
bug fixes for the BSD DB library.  The bsddb module included in this 
port incorporates these fixes.

Library and other distributed Python code:

The Python standard library lives in the Lib directory.  All the standard 
library code included with the Python 2.6 source distribution is included 
in the binary archive, with the exception of the dos-8x3 and tkinter 
subdirectories which have been omitted to reduce the size of the binary 
archive - the dos-8x3 components are unnecessary duplicates and Tkinter 
is not supported by this port (yet).  All the plat-* subdirectories in the 
source distribution have also been omitted, except for the plat-os2emx 
subdirectory.

The Tools and Demo directories contain a collection of Python scripts.  
To reduce the size of the binary archive, the Demo/sgi, Demo/Tix, 
Demo/tkinter, Tools/audiopy and Tools/IDLE subdirectories have been 
omitted as not being supported by this port.  The Misc directory has 
also been omitted.

All subdirectories omitted from the binary archive can be reconstituted 
from the Python 2.6 source distribution, if desired.

Support for building Python extensions:

The Config subdirectory contains the files describing the configuration 
of the interpreter and the Makefile, import libraries for the Python DLL, 
and the module definition file used to create the Python DLL.  The 
Include subdirectory contains all the standard Python header files 
needed for building extensions.

As I don't have the Visual Age C++ compiler, I've made no attempt to 
have this port support extensions built with that compiler.


Packaging
---------

This port is packaged as follows:
- python-2.6-os2emx-bin-03????.zip  (binaries, library modules)
- python-2.6-os2emx-src-03????      (patches+makefiles for non-Python code)

As all the Python specific patches for the port are now part of the 
Python release tarball, only the patches and makefiles involved in 
building external libraries for optional extensions are included in 
the source archive.

Documentation for the Python language, as well as the Python 2.6 
source distibution, can be obtained from the Python website 
(http://www.python.org/) or the Python project pages at Sourceforge 
(http://sf.net/projects/python/).


Installation
------------

Obtain and install, as per the included instructions, the EMX runtime 
package.

Unpack this archive, preserving the subdirectories, in the root directory 
of the drive where you want Python to live.

Add the Python directory (eg C:\Python26) to the PATH and LIBPATH 
variables in CONFIG.SYS.

You should then set the PYTHONHOME and PYTHONPATH environment variables 
in CONFIG.SYS.

PYTHONHOME should be set to Python's top level directory.  PYTHONPATH 
should be set to the semicolon separated list of principal Python library 
directories.
I use:
  SET PYTHONHOME=F:/Python26
  SET PYTHONPATH=F:/Python26/Lib;F:/Python26/Lib/plat-os2emx;
                 F:/Python26/Lib/lib-dynload;F:/Python26/Lib/site-packages

NOTE!:  the PYTHONPATH setting above is linewrapped for this document - it 
should all be on one line in CONFIG.SYS!

If you wish to use the curses module, you should set the TERM and TERMINFO 
environment variables appropriately.

If you don't already have ncurses installed, I have included a copy of the 
EMX subset of the Terminfo database included with the ncurses-5.2 source 
distribution.  This can be used by setting the TERMINFO environment variable 
to the path of the Terminfo subdirectory below the Python home directory.
On my system this looks like:
  SET TERMINFO=F:/Python26/Terminfo

For the TERM environment variable, I would try one of the following:
  SET TERM=ansi
  SET TERM=os2
  SET TERM=window

You will have to reboot your system for these changes to CONFIG.SYS to take 
effect.

If you wish to compile all the included Python library modules to bytecode, 
you can change into the Python home directory and run the COMPILEALL.CMD 
batch file.

You can execute the regression tests included with the Python 2.6 source 
distribution by changing to the Python 2.6 home directory and executing the 
REGRTEST.CMD batch file.  The following tests are known to fail at this 
time:
- test_mhlib (I don't know of any port of MH to OS/2);
- test_strptime (see "YOU HAVE BEEN WARNED" item 22);
- test_time (see "YOU HAVE BEEN WARNED" item 22);
- test_posixpath (see "YOU HAVE BEEN WARNED" item 23).

Note that some of the network related tests expect the loopback interface
(interface "lo", with IP address 127.0.0.1) to be enabled, which from my
experience is not the default configuration.  Additionally, test_popen2
expects the "cat" utility (such as found in ports of the GNU tools) to
be installed.


Building from source
--------------------

With the EMX port now checked into Python's CVS repository, the build 
infrastructure is part of the Python release sourceball.

Prerequisites

First and foremost, you need an operational EMX development installation - 
EMX v0.9d with fix04 (the latest at time of writing) & the gcc 2.8.1 
compiler released by Eberhard Mattes is the recommended setup.

If you have a different version of gcc installed, see "YOU HAVE BEEN 
WARNED" item 16.

Other items of software required:-

- GNU make (I'm using v3.76.1)
- rm, cp, mkdir from the GNU file utilities package
- GNU find
- GNU sed

Procedure

0. all changes mentioned apply to files in the PC/os2emx subdirectory 
   of the Python release source tree.  make is also executed from this 
   directory, so change into this directory before proceeding.

1. decide if you need to change the location of the Python installation.
   If you wish to do this, set the value of the Makefile variable LIB_DIR 
   to the directory you wish to use for PYTHONHOME 
   (eg /usr/local/lib/python2.6).

   If you want Python to find its library without the PYTHONHOME 
   environment variable set, set the value of the Makefile variable 
   FIXED_PYHOME to "yes" (uncomment the appropriate line).

2. If you wish the Python executables (python.exe, pythonpm.exe & pgen.exe) 
   to be installed in a directory other than the PYTHONHOME directory, set 
   the value of the Makefile variable EXE_DIR to the appropriate directory.

3. If you wish the Python core DLL (python27.dll) to be installed in a 
   directory other than the directory in which the Python executables are 
   installed (by default, the PYTHONHOME directory), set the value of the 
   Makefile variable DLL_DIR to the appropriate directory.  This DLL must 
   be placed in a directory on the system's LIBPATH, or that gets set 
   with BEGINLIBPATH or ENDLIBPATH.

4. If you have installed any of the libraries that can be used to build 
   optional Python modules, set the value of the relevant HAVE_<package> 
   Makefile variable to "yes".  The Makefile currently supports:

   library               Makefile variable
   ........................................
   zlib (1.1.4)          HAVE_ZLIB
   GNU UltraFast Crypt   HAVE_UFC
   Tcl/Tk                HAVE_TCLTK (not known to work)
   GNU Readline          HAVE_GREADLINE
   BSD DB (v1.85)        HAVE_BSDDB
   ncurses               HAVE_NCURSES
   GNU gdbm              HAVE_GDBM
   libbz2                HAVE_BZ2
   OpenSSL               HAVE_OPENSSL

   Please note that you need to check that what you have installed 
   is compatible with Python's build options.  In particular, the 
   BSD DB v1.85 library needs to be rebuilt with a source patch for 
   multithread support (doesn't change the library's reentrant status 
   but allows it to be linked to Python which is multithreaded).  
   Widely available binary packages of other librarys & DLLs are 
   not built/linked with multithread support.  Beware!

   Also note that the Makefile currently expects any libraries to be 
   found with the default library search path.  You may need to add 
   -L switches to the LDFLAGS Makefile variable if you have installed 
   libraries in directories not in the default search path (which can 
   be controlled by the LIBRARY_PATH environment variable used by EMX).

5. make

   It is usually a good idea to redirect the stdout and stderr streams 
   of the make process to log files, so that you can review any messages. 

6. make test

   This runs the Python regression tests, and completion is a sign of 
   a usable build.  You should check the list of skipped modules to 
   ensure that any optional modules you selected have been built; 
   checking the list of failures against the list of known failures 
   elsewhere in this document is also prudent.

7. make install
   >>>>>> NOT YET COMPLETE <<<<<< 

8. change to a directory outside the Python source tree and start Python. 
   Check the version and build date to confirm satisfactory installation.


YOU HAVE BEEN WARNED!!
----------------------

I know about a number of nasties in this port.

1.  Eberhard Mattes, author of EMX, writes in his documentation that fork() 
is very inefficient in the OS/2 environment.  It also requires that the 
executable be linked in a.out format rather than OMF.  Use the os.exec 
and/or the os.spawn family of functions where possible.

2.  In the absence of GNU Readline, terminating the interpreter requires a 
control-Z (^Z) followed by a carriage return.  Jeff Rush documented this 
problem in his Python 1.5.2 port.  With Readline, a control-D (^D) works 
as per the standard Unix environment.

3.  EMX only has a partial implementation of fcntl().  The fcntl module 
in this port supports what EMX supports.  If fcntl is important to you, 
please review the EMX C Library Reference (included in .INF format in the 
EMXVIEW.ZIP archive as part of the complete EMX development tools suite).
Because of other side-effects I have modified the test_fcntl.py test 
script to deactivate the exercising of the missing functionality.

4.  the PyBSDDB3 module has been imported into the Python standard
library, with the intent of superceding the BSDDB 1.85 module (bsddb).
As I don't yet have a satisfactory port of Sleepcat's more recent DB
library (3.3.x/4.0.x/4.1.x), I haven't included a binary of this
module.  I have left the Python part of the PyBSDDB package in this
distribution for completeness.

5.  As a consequence of the PyBSDDB3 module being imported, the former 
BSD DB (bsddb) module, linked against the DB v1.85 library from EMX, 
has been renamed bsddb185.  The bsddb185 module will not be built by 
default on most platforms, but in the absence of a PyBSDDB3 module I 
have retained it in the EMX port.

Version 1.85 of the DB library is widely known to have bugs, although 
some patches have become available (and are incorporated into the 
included bsddb185 module).  Unless you have problems with software 
licenses which would rule out GDBM (and the dbm module because it is 
linked against the GDBM library) or need it for file format compatibility, 
you may be better off deleting it and relying on GDBM.

Any code you have which uses the v1.85 bsddb module can be modified to 
use the renamed module by changing

  import bsddb

to

  import bsddb185 as bsddb

6.  The readline module has been linked against ncurses rather than the 
termcap library supplied with EMX.

7.  I have configured this port to use "/" as the preferred path separator 
character, rather than "\" ('\\'), in line with the convention supported 
by EMX.  Backslashes are still supported of course, and still appear in 
unexpected places due to outside sources that don't get normalised.

8.  While the DistUtils components are now functional, other 
packaging/binary handling tools and utilities such as those included in
the Demo and Tools directories - freeze in particular - are unlikely to 
work.  If you do get them going, I'd like to know about your success.

9.  I haven't set out to support the [BEGIN|END]LIBPATH functionality 
supported by one of the earlier ports (Rush's??).  If it works let me know.

10. As a result of the limitations imposed by EMX's library routines, the 
standard extension module pwd only synthesises a simple passwd database, 
and the grp module cannot be supported at all.

I have written pure Python substitutes for pwd and grp, which can process 
real passwd and group files for those applications (such as MailMan) that 
require more than EMX emulates.  I have placed pwd.py and grp.py in 
Lib/plat-os2emx, which is usually before Lib/lib-dynload (which contains 
pwd.pyd) in the PYTHONPATH.  If you have become attached to what pwd.pyd 
supports, you can put Lib/lib-dynload before Lib/plat-os2emx in PYTHONPATH 
or delete/rename pwd.py & grp.py.

pwd.py & grp.py support locating their data files by looking in the 
environment for them in the following sequence:
pwd.py:  $ETC_PASSWD             (%ETC_PASSWD%)
         $ETC/passwd             (%ETC%/passwd)
         $PYTHONHOME/Etc/passwd  (%PYTHONHOME%/Etc/passwd)
grp.py:  $ETC_GROUP              (%ETC_GROUP%)
         $ETC/group              (%ETC%/group)
         $PYTHONHOME/Etc/group   (%PYTHONHOME%/Etc/group)

The ETC_PASSWD and ETC_GROUP environment variables are intended to allow 
support for multiple passwd/grp files, where other applications may not 
support as wide a variety of input variations (drive remappings, 
separators etc).

Both modules support using either the ":" character (Unix standard) or 
";" (OS/2, DOS, Windows standard) field separator character, and pwd.py 
implements the following drive letter conversions for the home_directory and 
shell fields (for the ":" separator only):
         $x  ->  x:
         x;  ->  x:

Example versions of passwd and group are in the Etc subdirectory.  The 
regression tests (test_pwd and test_grp) will fail if valid password and 
group files cannot be found, but should pass otherwise.

Be aware that Python's pwd & group modules are for reading password and 
group information only.

11. EMX's termios routines don't support all of the functionality now 
exposed by the termios module - refer to the EMX documentation to find 
out what is supported.

12. The case sensitive import semantics introduced in Python 2.1 for other 
case insensitive but case preserving file/operating systems (Windows etc), 
have been incorporated into this port, and are active by default.  Setting 
the PYTHONCASEOK environment variable (to any value) reverts to the 
previous (case insensitive) semantics.  This can be an issue with some 
file management utilities that do not preserve the case of file and
directory names.

13. Because I am statically linking ncurses, the _curses_panel 
module has potential problems arising from separate library data areas.
To avoid this, I have configured the _curses_.pyd (imported as 
"_curses_panel") to import the ncurses symbols it needs from _curses.dll 
(which is the curses module, but with a .dll extension rather than .pyd 
so that the dynamic loader can actually import the symbols from it as a 
DLL).

The site module (Lib/site.py) has code added to tweak BEGINLIBPATH so
that _curses.dll is found when _curses_panel is imported.  If you have
problems attempting to use the _curses_panel support please let me know,
and I'll have another look at this.

14. sys.platform reports "os2emx" instead of "os2".  os.name still 
reports "os2".  This change was to make it easier to distinguish between 
the VAC++ build (formerly maintained by Michael Muller) and the EMX build 
(this port), principally for DistUtils.

15. it appears that the %W substitution in the EMX strftime() routine has 
an off-by-one bug.  strftime was listed as passing the regression tests 
in previous releases, but this fact appears to have been an oversight in 
the regression test suite.  To fix this really requires a portable 
strftime routine - I'm looking into using one from FreeBSD, but its not 
ready yet.

16. I have successfully built this port with Andy Zabolotny's ports of 
pgcc 2.95 and gcc 3.2.1, in addition to EM's gcc 2.8.1.  To use the 
bsddb185 module with the gcc 3.2.1 build, I had to recompile the DB library 
with gcc 3.2.1 - I don't know why, but trying to import the module built 
against a DB library compiled with gcc 2.8.1 would result in a SYS3175 
error.

I have not attempted to compile Python with any version of gcc prior to 
v2.8.1.

This release sees the default optimisation change to 
"-O3 -fomit-frame-pointer -mprobe".  This works fine too for pgcc 2.95 
but not for gcc 3.2.1.

With gcc 3.2.1, -O3 causes 2 unexpected test failures: test_format and 
test_unicode.  Both these tests pass if -O2 is instead of -O3 with this 
compiler, and the performance difference is negligible (in contrast to 
gcc 2.8.1 and pgcc 2.95, where the performance difference between the 
2 optimisation settings approaches 10%).

17.  os.spawnv() and os.spawnve() expose EMX's library routines rather 
than use the emulation in os.py.

In order to make use of some of the features this makes available in 
the OS/2 environment, you should peruse the relevant EMX documentation 
(EMXLIB.INF in the EMXVIEW.ZIP archive accompanying the EMX archives 
on Hobbes or LEO).  Be aware that I have exposed all the "mode" options 
supported by EMX, but there are combinations that either cannot be 
practically used by/in Python or have the potential to compromise your 
system's stability.

18.  pythonpm.exe used to be just python.exe with the WINDOWAPI linker 
option set in the pythonpm.def file.  In practice, this turns out to do 
nothing useful.

I have written a replacement which wraps the Python DLL in a genuine 
Presentation Manager application.  This version actually runs the 
Python interpreter in a separate thread from the PM shell, in order 
that PythonPM has a functioning message queue as good PM apps should.
In its current state, PythonPM's window is hidden.  It can be displayed, 
although it will have no content as nothing is ever written to the 
window.  Only the "hide" button is available.  Although the code 
has support for shutting PythonPM down when the Python interpreter is 
still busy (via the "control" menu), this is not well tested and given 
comments I've come across in EMX documentation suggesting that the 
thread killing operation has problems I would suggest caution in 
relying on this capability.

PythonPM processes commandline parameters normally.  The standard input, 
output and error streams are only useful if redirected, as PythonPM's 
window is not a console in any form and so cannot accept or display 
anything.  This means that the -i option is ineffective.

Because the Python thread doesn't create its own message queue, creating 
PM Windows and performing most PM operations is not possible from within 
this thread.  How this will affect supporting PM extensions (such as 
Tkinter using a PM port of Tcl/Tk, or wxPython using the PM port of 
WxWindows) is still being researched.

Note that os.fork() _DOES_NOT_WORK_ in PythonPM - SYS3175s are the result 
of trying.  os.spawnv() _does_ work.  PythonPM passes all regression tests 
that the standard Python interpreter (python.exe) passes, with the exception 
of test_fork1 and test_socket which both attempt to use os.fork().

I very much want feedback on the performance, behaviour and utility of 
PythonPM.  I would like to add a PM console capability to it, but that 
will be a non-trivial effort.  I may be able to leverage the code in 
Illya Vaes' Tcl/Tk port, which would make it easier.

19.  os.chdir() uses EMX's _chdir2(), which supports changing both drive 
and directory at once.  Similarly, os.getcwd() uses EMX's _getcwd() 
which returns drive as well as path.

20.  pyconfig.h is installed in the Include subdirectory with all 
other include files.

21.  the default build explicitly sets the number of file handles 
available to a Python process to 250.  EMX default is 40, which is 
insufficient for the tempfile regression test (test_tempfile) which 
tries to create 100 temporary files.

This setting can be overridden via the EMXOPT environment variable:
  set EMXOPT=-h250
is equivalent to the setting currently used.  The emxbind utility (if you 
have it installed) can also be used to permanently change the setting in 
python.exe - please refer to the EMX documentation for more information.

22.  a pure python strptime module is now part of the Python standard
library, superceding a platform specific extension module. This module
leverages the strftime module, and as a result test_strptime fails
due to the EMX strftime bug in item 20 above.

23.  test_posixpath attempts to exercise various Posix path related
functionality.  Most of the sub-tests pass, but the "ismount" and
"samestat" subtests fail:
- EMX provides not satisfactory mount point emulation, so "ismount"
  cannot succeed;
- EMX documents that successive stat() calls will produce different
  results, so "samestat" cannot succeed.

test_posixpath should skip these tests on EMX.

24.  I have reports of BitTorrent not working.  It appears that the
EMX select() emulation, possibly in concert with bugs in the TCP/IP
stack, runs into problems under the stress imposed by this application.
I think it suffices to say that BitTorrent is a fair stress test of a
system's networking capability.

25.  In the absence of an EMX implementation of the link() function, I've 
implemented a crude Python emulation, in the file 
Lib/plat-os2emx/_emx_link.py.  This is imported into the os module, and 
becomes available as os.link() in the normal way.

The emulation copies the source file in binary mode, and will fail if 
disk space is exhausted. The call fails if the target already exists. 
There are no guarantees to thread safety with this emulation - beware!

The emulation was written to support a link() based file locking system 
used in GNU Mailman.

26.  AF_UNIX sockets, otherwise known as Unix domain sockets, are now
supported.  Unfortunately, there are some traps arising from the
implementation in IBM's TCP/IP stack:-
- the path name must start with '\\socket\\' ('/socket/' won't work!),
  with the length of the full path name less than 108 characters;
- unlike Unix, the socket endpoints don't exist in the filesystem;
- by default, sockets are in binary mode.

27.  As of Python 2.4, the mpz, rotor and xreadlines modules have been 
dropped from the Python source tree.

28.  The subprocess module was added to the standard library relatively
late in the 2.4 development cycle.  Unfortunately I haven't had the
round tuits to adapt the module to the EMX environment yet, and
test_subprocess has a number of failures as a result.

29.  The default stack size for threads has been 64k.  This is proving
insufficient for some codebases, such as Zope.  The thread stack size
still defaults to 64k, but this can now be increased via the stack_size()
function exposed by the threading & thread modules as well as by defining
THREAD_STACK_SIZE to an appropriate value in the Makefile (which contains
a commented out definition for 128kB thread stacks).  I have seen
references to heavy Zope/Plone usage requiring 1MB thread stacks on
FreeBSD and Linux, but doubt that for most likely usage on OS/2 that
more than 256kB is necessary.  The size of the required stacks (main 
and thread) can vary significantly depending on which version of gcc
is used along with the compiler optimisations selected.  Note that the
main thread stack size is set during linking and is currently 2MB.

... probably other issues that I've not encountered, or don't remember :-(

If you encounter other difficulties with this port, which can be 
characterised as peculiar to this port rather than to the Python release,
I would like to hear about them.  However I cannot promise to be able to do 
anything to resolve such problems.  See the Contact section below...


To do...
--------

In no particular order of apparent importance or likelihood...

- support Tkinter and/or alternative GUI (wxWindows??)


Credits
-------

In addition to people identified above, I'd like to thank:
- the BDFL, Guido van Rossum, and crew for Python;
- Dr David Mertz, for trying out a pre-release of this port;
- the Python-list/comp.lang.python community;
- John Poltorak, for input about pwd/grp.

Contact
-------

Constructive feedback, negative or positive, about this port is welcome 
and should be addressed to me at the e-mail addresses below.

I have a private mailing list for announcements of fixes & updates to 
this port.  If you wish to receive such e-mail announcments, please send 
me an e-mail requesting that you be added to this list.

Andrew MacIntyre
E-mail: andymac@bullseye.apana.org.au, or andymac@pcug.org.au
Web:    http://www.andymac.org/

28 January, 2008.

IBM VisualAge C/C++ for OS/2
============================

To build Python for OS/2, change into ./os2vacpp and issue an 'NMAKE'
command.  This will build a PYTHON15.DLL containing the set of Python
modules listed in config.c and a small PYTHON.EXE to start the
interpreter.

By changing the C compiler flag /Gd- in the makefile to /Gd+, you can
reduce the size of these by causing Python to dynamically link to the
C runtime DLLs instead of including their bulk in your binaries. 
However, this means that any system on which you run Python must have
the VAC++ compiler installed in order to have those DLLs available.

During the build process you may see a couple of harmless warnings:

  From the C Compiler, "No function prototype given for XXX", which
  comes from the use of K&R parameters within Python for portability.

  From the ILIB librarian, "Module Not Found (XXX)", which comes
  from its attempt to perform the (-+) operation, which removes and
  then adds a .OBJ to the library.  The first time a build is done,
  it obviously cannot remove what is not yet built.

This build includes support for most Python functionality as well as
TCP/IP sockets.  It omits the Posix ability to 'fork' a process but
supports threads using OS/2 native capabilities.  I have tried to
support everything possible but here are a few usage notes.


-- os.popen() Usage Warnings

With respect to my implementation of popen() under OS/2:

    import os

    fd = os.popen("pkzip.exe -@ junk.zip", 'wb')
    fd.write("file1.txt\n")
    fd.write("file2.txt\n")
    fd.write("file3.txt\n")
    fd.write("\x1a")  # Should Not Be Necessary But Is
    fd.close()

There is a bug, either in the VAC++ compiler or OS/2 itself, where the
simple closure of the write-side of a pipe -to- a process does not
send an EOF to that process.  I find I must explicitly write a
control-Z (EOF) before closing the pipe.  This is not a problem when
using popen() in read mode.

One other slight difference with my popen() is that I return None
from the close(), instead of the Unix convention of the return code
of the spawned program.  I could find no easy way to do this under
OS/2.


-- BEGINLIBPATH/ENDLIBPATH

With respect to environment variables, this OS/2 port supports the
special-to-OS/2 magic names of 'BEGINLIBPATH' and 'ENDLIBPATH' to
control where to load conventional DLLs from.  Those names are
intercepted and converted to calls on the OS/2 kernel APIs and
are inherited by child processes, whether Python-based or not.

A few new attributes have been added to the os module:

    os.meminstalled  # Count of Bytes of RAM Installed on Machine
    os.memkernel     # Count of Bytes of RAM Reserved (Non-Swappable)
    os.memvirtual    # Count of Bytes of Virtual RAM Possible
    os.timeslice     # Duration of Scheduler Timeslice, in Milliseconds
    os.maxpathlen    # Maximum Length of a Path Specification, in chars
    os.maxnamelen    # Maximum Length of a Single Dir/File Name, in chars
    os.version       # Version of OS/2 Being Run e.g. "4.00"
    os.revision      # Revision of OS/2 Being Run (usually zero)
    os.bootdrive     # Drive that System Booted From e.g. "C:"
                     # (useful to find the CONFIG.SYS used to boot with)


-- Using Python as the Default OS/2 Batch Language

Note that OS/2 supports the Unix technique of putting the special
comment line at the time of scripts e.g. "#!/usr/bin/python" in
a different syntactic form.  To do this, put your script into a file
with a .CMD extension and added 'extproc' to the top as follows:

    extproc C:\Python\Python.exe -x
    import os
    print "Hello from Python"

The '-x' option tells Python to skip the first line of the file
while processing the rest as normal Python source.


-- Suggested Environment Variable Setup

With respect to the environment variables for Python, I use the
following setup:

    Set PYTHONHOME=E:\Tau\Projects\Python;D:\DLLs
    Set PYTHONPATH=.;E:\Tau\Projects\Python\Lib; \
                     E:\Tau\Projects\Python\Lib\plat-win

The EXEC_PREFIX (optional second pathspec on PYTHONHOME) is where
you put any Python extension DLLs you may create/obtain.  There
are none provided with this release.


-- Contact Info

Jeff Rush is no longer supporting the VACPP port :-(

I don't have the VACPP compiler, so can't reliably maintain this port. 

Anyone with VACPP who can contribute patches to keep this port buildable
should upload them to the Python Patch Manager at Sourceforge and 
assign them to me for review/checkin.

Andrew MacIntyre
aimacintyre at users.sourceforge.net
August 18, 2002.

Welcome to the "PC" subdirectory of the Python distribution
***********************************************************

This "PC" subdirectory contains complete project files to make
several older PC ports of Python, as well as all the PC-specific
Python source files.  It should be located in the root of the
Python distribution, and there should be directories "Modules",
"Objects", "Python", etc. in the parent directory of this "PC"
subdirectory.  Be sure to read the documentation in the Python
distribution.

Python requires library files such as string.py to be available in
one or more library directories.  The search path of libraries is
set up when Python starts.  To see the current Python library search
path, start Python and enter "import sys" and "print sys.path".

All PC ports use this scheme to try to set up a module search path:

  1) The script location; the current directory without script.
  2) The PYTHONPATH variable, if set.
  3) For Win32 platforms (NT/95), paths specified in the Registry.
  4) Default directories lib, lib/win, lib/test, lib/tkinter;
     these are searched relative to the environment variable
     PYTHONHOME, if set, or relative to the executable and its
     ancestors, if a landmark file (Lib/string.py) is found ,
     or the current directory (not useful).
  5) The directory containing the executable.

The best installation strategy is to put the Python executable (and
DLL, for Win32 platforms) in some convenient directory such as
C:/python, and copy all library files and subdirectories (using XCOPY)
to C:/python/lib.  Then you don't need to set PYTHONPATH.  Otherwise,
set the environment variable PYTHONPATH to your Python search path.
For example,
   set PYTHONPATH=.;d:\python\lib;d:\python\lib\win;d:\python\lib\dos-8x3

There are several add-in modules to build Python programs which use
the native Windows operating environment.  The ports here just make
"QuickWin" and DOS Python versions which support a character-mode
(console) environment.  Look in www.python.org for Tkinter, PythonWin,
WPY and wxPython.

To make a Python port, start the Integrated Development Environment
(IDE) of your compiler, and read in the native "project file"
(or makefile) provided.  This will enable you to change any source
files or build settings so you can make custom builds.

pyconfig.h    An important configuration file specific to PC's.

config.c    The list of C modules to include in the Python PC
            version.  Manually edit this file to add or
            remove Python modules.

testpy.py   A Python test program.  Run this to test your
            Python port.  It should produce copious output,
	    ending in a report on how many tests were OK, how many
	    failed, and how many were skipped.  Don't worry about
	    skipped tests (these test unavailable optional features).


Additional files and subdirectories for 32-bit Windows
======================================================

python_nt.rc   Resource compiler input for python15.dll.

dl_nt.c, import_nt.c
               Additional sources used for 32-bit Windows features.

getpathp.c     Default sys.path calculations (for all PC platforms).

dllbase_nt.txt A (manually maintained) list of base addresses for
               various DLLs, to avoid run-time relocation.

example_nt     A subdirectory showing how to build an extension as a
               DLL.

Legacy support for older versions of Visual Studio
==================================================
The subdirectories VC6, VS7.1 and VS8.0 contain legacy support older
versions of Microsoft Visual Studio. See PCbuild/readme.txt.

EMX development tools for OS/2
==============================

See os2emx/readme.txt. This platform is maintained by Andrew MacIntyre.

IBM VisualAge C/C++ for OS/2
============================

See os2vacpp/readme.txt.  This platform is supported by Jeff Rush.

NOTE: Support for os2vacpp may be dropped in the near future. Please move
      to EMX.

Note for Windows 3.x and DOS users
==================================

Neither Windows 3.x nor DOS is supported any more.  The last Python
version that supported these was Python 1.5.2; the support files were
present in Python 2.0 but weren't updated, and it is not our intention
to support these platforms for Python 2.x.

Building Python using VC++ 6.0 or 5.0
-------------------------------------
This directory is used to build Python for Win32 platforms, e.g. Windows
2000 and XP.  It requires Microsoft Visual C++ 6.x or 5.x and Platform
SDK February 2003 Edition (Core SDK).
(For other Windows platforms and compilers, see ../readme.txt.)

All you need to do is open the workspace "pcbuild.dsw" in MSVC++, select
the Debug or Release setting (using Build -> Set Active Configuration...),
and build the projects.

The proper order to build subprojects:

1) pythoncore (this builds the main Python DLL and library files,
               python27.{dll, lib} in Release mode)

2) python (this builds the main Python executable,
           python.exe in Release mode)

3) the other subprojects, as desired or needed (note:  you probably don't
   want to build most of the other subprojects, unless you're building an
   entire Python distribution from scratch, or specifically making changes
   to the subsystems they implement; see SUBPROJECTS below)

When using the Debug setting, the output files have a _d added to
their name:  python27_d.dll, python_d.exe, pyexpat_d.pyd, and so on.

SUBPROJECTS
-----------
These subprojects should build out of the box.  Subprojects other than the
main ones (pythoncore, python, pythonw) generally build a DLL (renamed to
.pyd) from a specific module so that users don't have to load the code
supporting that module unless they import the module.

pythoncore
    .dll and .lib
python
    .exe
pythonw
    pythonw.exe, a variant of python.exe that doesn't pop up a DOS box
_msi
    _msi.c. You need to install Windows Installer SDK to build this module.
_socket
    socketmodule.c
_testcapi
    tests of the Python C API, run via Lib/test/test_capi.py, and
    implemented by module Modules/_testcapimodule.c
pyexpat
    Python wrapper for accelerated XML parsing, which incorporates stable
    code from the Expat project:  http://sourceforge.net/projects/expat/
select
    selectmodule.c
unicodedata
    large tables of Unicode data
winsound
    play sounds (typically .wav files) under Windows

The following subprojects will generally NOT build out of the box.  They
wrap code Python doesn't control, and you'll need to download the base
packages first and unpack them into siblings of PCbuilds's parent
directory; for example, if your PCbuild is  .......\dist\src\PCbuild\,
unpack into new subdirectories of dist\.

_tkinter
    Python wrapper for the Tk windowing system.  Requires building
    Tcl/Tk first.  Following are instructions for Tcl/Tk 8.5.2.

    Get source
    ----------
    In the dist directory, run
    svn export http://svn.python.org/projects/external/tcl-8.5.2.1 tcl8.5.2
    svn export http://svn.python.org/projects/external/tk-8.5.2.0 tk8.5.2
    svn export http://svn.python.org/projects/external/tix-8.4.3.1 tix8.4.3

    Debug Build
    -----------
    To build debug version, add DEBUG=1 to all nmake call bellow.

    Build Tcl first (done here w/ MSVC 6 on Win2K)
    ---------------
    If your environment doesn't have struct _stat64, you need to apply
    tcl852.patch in this directory to dist\tcl8.5.2\generic\tcl.h.

    cd dist\tcl8.5.2\win
    run vcvars32.bat
    nmake -f makefile.vc
    nmake -f makefile.vc INSTALLDIR=..\..\tcltk install

    XXX Should we compile with OPTS=threads?

    Optional:  run tests, via
        nmake -f makefile.vc test

        all.tcl:        Total   24242   Passed  23358   Skipped 877     Failed  7
        Sourced 137 Test Files.
        Files with failing tests: exec.test http.test io.test main.test string.test stri
        ngObj.test

    Build Tk
    --------
    cd dist\tk8.5.2\win
    nmake -f makefile.vc TCLDIR=..\..\tcl8.5.2
    nmake -f makefile.vc TCLDIR=..\..\tcl8.5.2 INSTALLDIR=..\..\tcltk install

    XXX Should we compile with OPTS=threads?

    XXX I have no idea whether "nmake -f makefile.vc test" passed or
    XXX failed.  It popped up tons of little windows, and did lots of
    XXX stuff, and nothing blew up.

    Build Tix
    ---------
    cd dist\tix8.4.3\win
    nmake -f python.mak TCL_MAJOR=8 TCL_MINOR=5 TCL_PATCH=2 MACHINE=IX86 DEBUG=0
    nmake -f python.mak TCL_MAJOR=8 TCL_MINOR=5 TCL_PATCH=2 MACHINE=IX86 DEBUG=0 INSTALL_DIR=..\..\tcltk install

bz2
    Python wrapper for the libbz2 compression library.  Homepage
        http://www.bzip.org/
    Download the source from the python.org copy into the dist
    directory:

    svn export http://svn.python.org/projects/external/bzip2-1.0.6

    And requires building bz2 first.

    cd dist\bzip2-1.0.6
    nmake -f makefile.msc

    All of this managed to build bzip2-1.0.6\libbz2.lib, which the Python
    project links in.


_bsddb
    To use the version of bsddb that Python is built with by default, invoke
    (in the dist directory)

     svn export http://svn.python.org/projects/external/db-4.7.25.0 db-4.7.25

    Then open db-4.7.25\build_windows\Berkeley_DB.dsw and build the
    "db_static" project for "Release" mode.

    Alternatively, if you want to start with the original sources,
    go to Oracle's download page:
        http://www.oracle.com/technology/software/products/berkeley-db/db/

    and download version 4.7.25.

    With or without strong cryptography? You can choose either with or
    without strong cryptography, as per the instructions below.  By
    default, Python is built and distributed WITHOUT strong crypto.

    Unpack the sources; if you downloaded the non-crypto version, rename
    the directory from db-4.7.25.NC to db-4.7.25.

    Now apply any patches that apply to your version.

    To run extensive tests, pass "-u bsddb" to regrtest.py.  test_bsddb3.py
    is then enabled.  Running in verbose mode may be helpful.

    XXX The test_bsddb3 tests don't always pass, on Windows (according to
    XXX me) or on Linux (according to Barry).  (I had much better luck
    XXX on Win2K than on Win98SE.)  The common failure mode across platforms
    XXX is
    XXX     DBAgainError: (11, 'Resource temporarily unavailable -- unable
    XXX                         to join the environment')
    XXX
    XXX and it appears timing-dependent.  On Win2K I also saw this once:
    XXX
    XXX test02_SimpleLocks (bsddb.test.test_thread.HashSimpleThreaded) ...
    XXX Exception in thread reader 1:
    XXX Traceback (most recent call last):
    XXX File "C:\Code\python\lib\threading.py", line 411, in __bootstrap
    XXX    self.run()
    XXX File "C:\Code\python\lib\threading.py", line 399, in run
    XXX    apply(self.__target, self.__args, self.__kwargs)
    XXX File "C:\Code\python\lib\bsddb\test\test_thread.py", line 268, in
    XXX                  readerThread
    XXX    rec = c.next()
    XXX DBLockDeadlockError: (-30996, 'DB_LOCK_DEADLOCK: Locker killed
    XXX                                to resolve a deadlock')
    XXX
    XXX I'm told that DBLockDeadlockError is expected at times.  It
    XXX doesn't cause a test to fail when it happens (exceptions in
    XXX threads are invisible to unittest).


_sqlite3
    Python wrapper for SQLite library.
    
    Get the source code through
    
    svn export http://svn.python.org/projects/external/sqlite-source-3.3.4
    
    To use the extension module in a Python build tree, copy sqlite3.dll into
    the PC/VC6 folder.


_ssl
    Python wrapper for the secure sockets library.

    Get the latest source code for OpenSSL from
        http://www.openssl.org

    You (probably) don't want the "engine" code.  For example, don't get
        openssl-engine-0.9.6g.tar.gz

    Unpack into the "dist" directory, retaining the folder name from
    the archive - for example, the latest stable OpenSSL will install as
        dist/openssl-1.0.0a

    You can (theoretically) use any version of OpenSSL you like - the
    build process will automatically select the latest version.

    You can install the NASM assembler from
        http://www.nasm.us/
    for x86 builds.  Put nasmw.exe anywhere in your PATH.
    Note: recent releases of nasm only have nasm.exe. Just rename it to 
    nasmw.exe.

    You can also install ActivePerl from
        http://www.activestate.com/activeperl/
    if you like to use the official sources instead of the files from 
    python's subversion repository. The svn version contains pre-build
    makefiles and assembly files.

    The MSVC project simply invokes PC/VC6/build_ssl.py to perform
    the build.  This Python script locates and builds your OpenSSL
    installation, then invokes a simple makefile to build the final .pyd.

    build_ssl.py attempts to catch the most common errors (such as not
    being able to find OpenSSL sources, or not being able to find a Perl
    that works with OpenSSL) and give a reasonable error message.
    If you have a problem that doesn't seem to be handled correctly
    (eg, you know you have ActivePerl but we can't find it), please take
    a peek at build_ssl.py and suggest patches.  Note that build_ssl.py
    should be able to be run directly from the command-line.

    build_ssl.py/MSVC isn't clever enough to clean OpenSSL - you must do
    this by hand.


YOUR OWN EXTENSION DLLs
-----------------------
If you want to create your own extension module DLL, there's an example
with easy-to-follow instructions in ../PC/example/; read the file
readme.txt there first.

Building Python using VC++ 7.1
-------------------------------------
This directory is used to build Python for Win32 platforms, e.g. Windows
95, 98 and NT.  It requires Microsoft Visual C++ 7.1
(a.k.a. Visual Studio .NET 2003).
(For other Windows platforms and compilers, see ../PC/readme.txt.)

All you need to do is open the workspace "pcbuild.sln" in MSVC++, select
the Debug or Release setting (using "Solution Configuration" from
the "Standard" toolbar"), and build the projects.

The proper order to build subprojects:

1) pythoncore (this builds the main Python DLL and library files,
               python27.{dll, lib} in Release mode)
              NOTE:  in previous releases, this subproject was
              named after the release number, e.g. python20.

2) python (this builds the main Python executable,
           python.exe in Release mode)

3) the other subprojects, as desired or needed (note:  you probably don't
   want to build most of the other subprojects, unless you're building an
   entire Python distribution from scratch, or specifically making changes
   to the subsystems they implement, or are running a Python core buildbot
   test slave; see SUBPROJECTS below)

When using the Debug setting, the output files have a _d added to
their name:  python27_d.dll, python_d.exe, parser_d.pyd, and so on.

SUBPROJECTS
-----------
These subprojects should build out of the box.  Subprojects other than the
main ones (pythoncore, python, pythonw) generally build a DLL (renamed to
.pyd) from a specific module so that users don't have to load the code
supporting that module unless they import the module.

pythoncore
    .dll and .lib
python
    .exe
pythonw
    pythonw.exe, a variant of python.exe that doesn't pop up a DOS box
_socket
    socketmodule.c
_testcapi
    tests of the Python C API, run via Lib/test/test_capi.py, and
    implemented by module Modules/_testcapimodule.c
pyexpat
    Python wrapper for accelerated XML parsing, which incorporates stable
    code from the Expat project:  http://sourceforge.net/projects/expat/
select
    selectmodule.c
unicodedata
    large tables of Unicode data
winsound
    play sounds (typically .wav files) under Windows

The following subprojects will generally NOT build out of the box.  They
wrap code Python doesn't control, and you'll need to download the base
packages first and unpack them into siblings of PC's parent
directory; for example, if this directory is ....\dist\trunk\PC\VS7.1,
unpack into new subdirectories of dist\.

_tkinter
    Python wrapper for the Tk windowing system.  Requires building
    Tcl/Tk first.  Following are instructions for Tcl/Tk 8.4.12.

    Get source
    ----------
    In the dist directory, run
    svn export http://svn.python.org/projects/external/tcl8.4.12
    svn export http://svn.python.org/projects/external/tk8.4.12
    svn export http://svn.python.org/projects/external/tix-8.4.0

    Build Tcl first (done here w/ MSVC 7.1 on Windows XP)
    ---------------
    Use "Start -> All Programs -> Microsoft Visual Studio .NET 2003
         -> Visual Studio .NET Tools -> Visual Studio .NET 2003 Command Prompt"
    to get a shell window with the correct environment settings
    cd dist\tcl8.4.12\win
    nmake -f makefile.vc
    nmake -f makefile.vc INSTALLDIR=..\..\tcltk install

    XXX Should we compile with OPTS=threads?

    Optional:  run tests, via
        nmake -f makefile.vc test

        On WinXP Pro, wholly up to date as of 30-Aug-2004:
        all.tcl:        Total   10678   Passed  9969    Skipped 709     Failed  0
        Sourced 129 Test Files.

    Build Tk
    --------
    cd dist\tk8.4.12\win
    nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12
    nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 INSTALLDIR=..\..\tcltk install

    XXX Should we compile with OPTS=threads?

    XXX Our installer copies a lot of stuff out of the Tcl/Tk install
    XXX directory.  Is all of that really needed for Python use of Tcl/Tk?

    Optional:  run tests, via
        nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 test

        On WinXP Pro, wholly up to date as of 30-Aug-2004:
        all.tcl:        Total   8420    Passed  6826    Skipped 1581    Failed  13
        Sourced 91 Test Files.
        Files with failing tests: canvImg.test scrollbar.test textWind.test winWm.test

   Built Tix
   ---------
   cd dist\tix-8.4.0\win
   nmake -f python.mak
   nmake -f python.mak install

bz2
    Python wrapper for the libbz2 compression library.  Homepage
        http://sources.redhat.com/bzip2/
    Download the source from the python.org copy into the dist
    directory:

    svn export http://svn.python.org/projects/external/bzip2-1.0.3

    A custom pre-link step in the bz2 project settings should manage to
    build bzip2-1.0.3\libbz2.lib by magic before bz2.pyd (or bz2_d.pyd) is
    linked in VS7.1\.
    However, the bz2 project is not smart enough to remove anything under
    bzip2-1.0.3\ when you do a clean, so if you want to rebuild bzip2.lib
    you need to clean up bzip2-1.0.3\ by hand.

    The build step shouldn't yield any warnings or errors, and should end
    by displaying 6 blocks each terminated with
        FC: no differences encountered

    All of this managed to build bzip2-1.0.3\libbz2.lib, which the Python
    project links in.


_bsddb
    To use the version of bsddb that Python is built with by default, invoke
    (in the dist directory)

     svn export http://svn.python.org/projects/external/db-4.4.20


    Then open a VS.NET 2003 shell, and invoke:

       devenv db-4.4.20\build_win32\Berkeley_DB.sln /build Release /project db_static

    and do that a second time for a Debug build too:

       devenv db-4.4.20\build_win32\Berkeley_DB.sln /build Debug /project db_static

    Alternatively, if you want to start with the original sources,
    go to Sleepycat's download page:
        http://www.sleepycat.com/downloads/releasehistorybdb.html

    and download version 4.4.20.

    With or without strong cryptography? You can choose either with or
    without strong cryptography, as per the instructions below.  By
    default, Python is built and distributed WITHOUT strong crypto.

    Unpack the sources; if you downloaded the non-crypto version, rename
    the directory from db-4.4.20.NC to db-4.4.20.

    Now apply any patches that apply to your version.

    Open
        dist\db-4.4.20\docs\index.html

    and follow the "Windows->Building Berkeley DB with Visual C++ .NET"
    instructions for building the Sleepycat
    software.  Note that Berkeley_DB.dsw is in the build_win32 subdirectory.
    Build the "db_static" project, for "Release" mode.

    To run extensive tests, pass "-u bsddb" to regrtest.py.  test_bsddb3.py
    is then enabled.  Running in verbose mode may be helpful.

    XXX The test_bsddb3 tests don't always pass, on Windows (according to
    XXX me) or on Linux (according to Barry).  (I had much better luck
    XXX on Win2K than on Win98SE.)  The common failure mode across platforms
    XXX is
    XXX     DBAgainError: (11, 'Resource temporarily unavailable -- unable
    XXX                         to join the environment')
    XXX
    XXX and it appears timing-dependent.  On Win2K I also saw this once:
    XXX
    XXX test02_SimpleLocks (bsddb.test.test_thread.HashSimpleThreaded) ...
    XXX Exception in thread reader 1:
    XXX Traceback (most recent call last):
    XXX File "C:\Code\python\lib\threading.py", line 411, in __bootstrap
    XXX    self.run()
    XXX File "C:\Code\python\lib\threading.py", line 399, in run
    XXX    apply(self.__target, self.__args, self.__kwargs)
    XXX File "C:\Code\python\lib\bsddb\test\test_thread.py", line 268, in
    XXX                  readerThread
    XXX    rec = c.next()
    XXX DBLockDeadlockError: (-30996, 'DB_LOCK_DEADLOCK: Locker killed
    XXX                                to resolve a deadlock')
    XXX
    XXX I'm told that DBLockDeadlockError is expected at times.  It
    XXX doesn't cause a test to fail when it happens (exceptions in
    XXX threads are invisible to unittest).

    Building for Win64:
    - open a VS.NET 2003 command prompt
    - run the SDK setenv.cmd script, passing /RETAIL and the target
      architecture (/SRV64 for Itanium, /X64 for AMD64)
    - build BerkeleyDB with the solution configuration matching the
      target ("Release IA64" for Itanium, "Release AMD64" for AMD64), e.g.
    devenv db-4.4.20\build_win32\Berkeley_DB.sln /build "Release AMD64" /project db_static /useenv

_sqlite3
    Python wrapper for SQLite library.
    
    Get the source code through
    
    svn export http://svn.python.org/projects/external/sqlite-source-3.3.4
    
    To use the extension module in a Python build tree, copy sqlite3.dll into
    the VS7.1 folder.

_ssl
    Python wrapper for the secure sockets library.

    Get the source code through

    svn export http://svn.python.org/projects/external/openssl-0.9.8a

    Alternatively, get the latest version from http://www.openssl.org.
    You can (theoretically) use any version of OpenSSL you like - the
    build process will automatically select the latest version.

    You must also install ActivePerl from
        http://www.activestate.com/Products/ActivePerl/
    as this is used by the OpenSSL build process.  Complain to them <wink>.

    The MSVC project simply invokes build_ssl.py to perform
    the build.  This Python script locates and builds your OpenSSL
    installation, then invokes a simple makefile to build the final .pyd.

    build_ssl.py attempts to catch the most common errors (such as not
    being able to find OpenSSL sources, or not being able to find a Perl
    that works with OpenSSL) and give a reasonable error message.
    If you have a problem that doesn't seem to be handled correctly
    (eg, you know you have ActivePerl but we can't find it), please take
    a peek at build_ssl.py and suggest patches.  Note that build_ssl.py
    should be able to be run directly from the command-line.

    build_ssl.py/MSVC isn't clever enough to clean OpenSSL - you must do
    this by hand.

Building for Itanium
--------------------

The project files support a ReleaseItanium configuration which creates
Win64/Itanium binaries. For this to work, you need to install the Platform
SDK, in particular the 64-bit support. This includes an Itanium compiler
(future releases of the SDK likely include an AMD64 compiler as well).
In addition, you need the Visual Studio plugin for external C compilers,
from http://sf.net/projects/vsextcomp. The plugin will wrap cl.exe, to
locate the proper target compiler, and convert compiler options
accordingly. The project files require atleast version 0.9.

Building for AMD64
------------------

The build process for the ReleaseAMD64 configuration is very similar
to the Itanium configuration; make sure you use the latest version of
vsextcomp.

Building Python Using the free MS Toolkit Compiler
--------------------------------------------------

The build process for Visual C++ can be used almost unchanged with the free MS
Toolkit Compiler. This provides a way of building Python using freely
available software.

Note that Microsoft have withdrawn the free MS Toolkit Compiler, so this can
no longer be considered a supported option. The instructions are still
correct, but you need to already have a copy of the compiler in order to use
them. Microsoft now supply Visual C++ 2008 Express Edition for free, but this
is NOT compatible with Visual C++ 7.1 (it uses a different C runtime), and so
cannot be used to build a version of Python compatible with the standard
python.org build. If you are interested in using Visual C++ 2008 Express
Edition, however, you should look at the PCBuild directory.

Requirements

    To build Python, the following tools are required:

    * The Visual C++ Toolkit Compiler
        no longer available for download - see above
    * A recent Platform SDK
        from http://www.microsoft.com/downloads/details.aspx?FamilyID=484269e2-3b89-47e3-8eb7-1f2be6d7123a
    * The .NET 1.1 SDK
        from http://www.microsoft.com/downloads/details.aspx?FamilyID=9b3a2ca6-3647-4070-9f41-a333c6b9181d

    [Does anyone have better URLs for the last 2 of these?]

    The toolkit compiler is needed as it is an optimising compiler (the
    compiler supplied with the .NET SDK is a non-optimising version). The
    platform SDK is needed to provide the Windows header files and libraries
    (the Windows 2003 Server SP1 edition, typical install, is known to work -
    other configurations or versions are probably fine as well). The .NET 1.1
    SDK is needed because it contains a version of msvcrt.dll which links to
    the msvcr71.dll CRT. Note that the .NET 2.0 SDK is NOT acceptable, as it
    references msvcr80.dll.

    All of the above items should be installed as normal.

    If you intend to build the openssl (needed for the _ssl extension) you
    will need the C runtime sources installed as part of the platform SDK.

    In addition, you will need Nant, available from
    http://nant.sourceforge.net. The 0.85 release candidate 3 version is known
    to work. This is the latest released version at the time of writing. Later
    "nightly build" versions are known NOT to work - it is not clear at
    present whether future released versions will work.

Setting up the environment

    Start a platform SDK "build environment window" from the start menu. The
    "Windows XP 32-bit retail" version is known to work.

    Add the following directories to your PATH:
        * The toolkit compiler directory
        * The SDK "Win64" binaries directory
	* The Nant directory
    Add to your INCLUDE environment variable:
        * The toolkit compiler INCLUDE directory
    Add to your LIB environment variable:
        * The toolkit compiler LIB directory
	* The .NET SDK Visual Studio 2003 VC7\lib directory

    The following commands should set things up as you need them:

        rem Set these values according to where you installed the software
        set TOOLKIT=C:\Program Files\Microsoft Visual C++ Toolkit 2003
        set SDK=C:\Program Files\Microsoft Platform SDK
        set NET=C:\Program Files\Microsoft Visual Studio .NET 2003
        set NANT=C:\Utils\Nant

        set PATH=%TOOLKIT%\bin;%PATH%;%SDK%\Bin\win64;%NANT%\bin
        set INCLUDE=%TOOLKIT%\include;%INCLUDE%
        set LIB=%TOOLKIT%\lib;%NET%\VC7\lib;%LIB%

    The "win64" directory from the SDK is added to supply executables such as
    "cvtres" and "lib", which are not available elsewhere. The versions in the
    "win64" directory are 32-bit programs, so they are fine to use here.

    That's it. To build Python (the core only, no binary extensions which
    depend on external libraries) you just need to issue the command

        nant -buildfile:python.build all

    from within the VS7.1 directory.

Extension modules

    To build those extension modules which require external libraries
    (_tkinter, bz2, _bsddb, _sqlite3, _ssl) you can follow the instructions
    for the Visual Studio build above, with a few minor modifications. These
    instructions have only been tested using the sources in the Python
    subversion repository - building from original sources should work, but
    has not been tested.

    For each extension module you wish to build, you should remove the
    associated include line from the excludeprojects section of pc.build.

    The changes required are:

    _tkinter
        The tix makefile (tix-8.4.0\win\makefile.vc) must be modified to
	remove references to TOOLS32. The relevant lines should be changed to
	read:
            cc32 = cl.exe
            link32 = link.exe
            include32 = 
	The remainder of the build instructions will work as given.

    bz2
        No changes are needed

    _bsddb
        The file db.build should be copied from the Python PCBuild directory
	to the directory db-4.4.20\build_win32.

	The file db_static.vcproj in db-4.4.20\build_win32 should be edited to
	remove the string "$(SolutionDir)" - this occurs in 2 places, only
	relevant for 64-bit builds. (The edit is required as otherwise, nant
	wants to read the solution file, which is not in a suitable form).

	The bsddb library can then be build with the command
	    nant -buildfile:db.build all
	run from the db-4.4.20\build_win32 directory.

    _sqlite3
        No changes are needed. However, in order for the tests to succeed, a
	copy of sqlite3.dll must be downloaded, and placed alongside
	python.exe.

    _ssl
        The documented build process works as written. However, it needs a
	copy of the file setargv.obj, which is not supplied in the platform
	SDK. However, the sources are available (in the crt source code). To
	build setargv.obj, proceed as follows:

        Copy setargv.c, cruntime.h and internal.h from %SDK%\src\crt to a
	temporary directory.
	Compile using "cl /c /I. /MD /D_CRTBLD setargv.c"
	Copy the resulting setargv.obj to somewhere on your LIB environment
	(%SDK%\lib is a reasonable place).

	With setargv.obj in place, the standard build process should work
	fine.

YOUR OWN EXTENSION DLLs
-----------------------
If you want to create your own extension module DLL, there's an example
with easy-to-follow instructions in ../PC/example/; read the file
readme.txt there first.

Building Python using VC++ 9.0
------------------------------

This directory is used to build Python for Win32 and x64 platforms, e.g. 
Windows 2000, XP, Vista and Windows Server 2008.  In order to build 32-bit
debug and release executables, Microsoft Visual C++ 2008 Express Edition is
required at the very least.  In order to build 64-bit debug and release
executables, Visual Studio 2008 Standard Edition is required at the very
least.  In order to build all of the above, as well as generate release builds
that make use of Profile Guided Optimisation (PG0), Visual Studio 2008
Professional Edition is required at the very least.  The official Python
releases are built with this version of Visual Studio.

For other Windows platforms and compilers, see ../PC/readme.txt.

All you need to do is open the workspace "pcbuild.sln" in Visual Studio,
select the desired combination of configuration and platform and eventually
build the solution. Unless you are going to debug a problem in the core or
you are going to create an optimized build you want to select "Release" as
configuration.

The PCbuild directory is compatible with all versions of Visual Studio from
VS C++ Express Edition over the standard edition up to the professional
edition. However the express edition does not support features like solution
folders or profile guided optimization (PGO). The missing bits and pieces
won't stop you from building Python.

The solution is configured to build the projects in the correct order. "Build
Solution" or F7 takes care of dependencies except for x64 builds. To make
cross compiling x64 builds on a 32bit OS possible the x64 builds require a 
32bit version of Python.

NOTE:
   You probably don't want to build most of the other subprojects, unless
   you're building an entire Python distribution from scratch, or
   specifically making changes to the subsystems they implement, or are
   running a Python core buildbot test slave; see SUBPROJECTS below)

When using the Debug setting, the output files have a _d added to
their name:  python30_d.dll, python_d.exe, parser_d.pyd, and so on. Both
the build and rt batch files accept a -d option for debug builds.

The 32bit builds end up in the solution folder PCbuild while the x64 builds
land in the amd64 subfolder. The PGI and PGO builds for profile guided
optimization end up in their own folders, too.

Legacy support
--------------

You can find build directories for older versions of Visual Studio and 
Visual C++ in the PC directory. The legacy build directories are no longer
actively maintained and may not work out of the box.

PC/VC6/
    Visual C++ 6.0
PC/VS7.1/
    Visual Studio 2003 (7.1)
PC/VS8.0/
    Visual Studio 2005 (8.0)


C RUNTIME
---------

Visual Studio 2008 uses version 9 of the C runtime (MSVCRT9).  The executables
are linked to a CRT "side by side" assembly which must be present on the target
machine.  This is avalible under the VC/Redist folder of your visual studio
distribution. On XP and later operating systems that support
side-by-side assemblies it is not enough to have the msvcrt90.dll present,
it has to be there as a whole assembly, that is, a folder with the .dll
and a .manifest.  Also, a check is made for the correct version.
Therefore, one should distribute this assembly with the dlls, and keep
it in the same directory.  For compatibility with older systems, one should
also set the PATH to this directory so that the dll can be found.
For more info, see the Readme in the VC/Redist folder.

SUBPROJECTS
-----------
These subprojects should build out of the box.  Subprojects other than the
main ones (pythoncore, python, pythonw) generally build a DLL (renamed to
.pyd) from a specific module so that users don't have to load the code
supporting that module unless they import the module.

pythoncore
    .dll and .lib
python
    .exe
pythonw
    pythonw.exe, a variant of python.exe that doesn't pop up a DOS box
_socket
    socketmodule.c
_testcapi
    tests of the Python C API, run via Lib/test/test_capi.py, and
    implemented by module Modules/_testcapimodule.c
pyexpat
    Python wrapper for accelerated XML parsing, which incorporates stable
    code from the Expat project:  http://sourceforge.net/projects/expat/
select
    selectmodule.c
unicodedata
    large tables of Unicode data
winsound
    play sounds (typically .wav files) under Windows

Python-controlled subprojects that wrap external projects:
_bsddb
    Wraps Berkeley DB 4.7.25, which is currently built by _bsddb.vcproj.
    project (see below).
_sqlite3
    Wraps SQLite 3.6.21, which is currently built by sqlite3.vcproj (see below).
_tkinter
    Wraps the Tk windowing system.  Unlike _bsddb and _sqlite3, there's no
    corresponding tcltk.vcproj-type project that builds Tcl/Tk from vcproj's
    within our pcbuild.sln, which means this module expects to find a
    pre-built Tcl/Tk in either ..\..\tcltk for 32-bit or ..\..\tcltk64 for
    64-bit (relative to this directory).  See below for instructions to build
    Tcl/Tk. 
bz2
    Python wrapper for the libbz2 compression library.  Homepage
        http://sources.redhat.com/bzip2/
    Download the source from the python.org copy into the dist
    directory:

    svn export http://svn.python.org/projects/external/bzip2-1.0.6

    ** NOTE: if you use the Tools\buildbot\external(-amd64).bat approach for
    obtaining external sources then you don't need to manually get the source
    above via subversion. **

    A custom pre-link step in the bz2 project settings should manage to
    build bzip2-1.0.6\libbz2.lib by magic before bz2.pyd (or bz2_d.pyd) is
    linked in PCbuild\.
    However, the bz2 project is not smart enough to remove anything under
    bzip2-1.0.6\ when you do a clean, so if you want to rebuild bzip2.lib
    you need to clean up bzip2-1.0.6\ by hand.

    All of this managed to build libbz2.lib in 
    bzip2-1.0.6\$platform-$configuration\, which the Python project links in.

_ssl
    Python wrapper for the secure sockets library.

    Get the source code through

    svn export http://svn.python.org/projects/external/openssl-0.9.8y

    ** NOTE: if you use the Tools\buildbot\external(-amd64).bat approach for
    obtaining external sources then you don't need to manually get the source
    above via subversion. **

    Alternatively, get the latest version from http://www.openssl.org.
    You can (theoretically) use any version of OpenSSL you like - the
    build process will automatically select the latest version.

    You must install the NASM assembler from
        http://nasm.sf.net
    for x86 builds.  Put nasmw.exe anywhere in your PATH.
    Note: recent releases of nasm only have nasm.exe. Just rename it to 
    nasmw.exe.

    You can also install ActivePerl from
        http://www.activestate.com/activeperl/
    if you like to use the official sources instead of the files from 
    python's subversion repository. The svn version contains pre-build
    makefiles and assembly files.

    The build process makes sure that no patented algorithms are included.
    For now RC5, MDC2 and IDEA are excluded from the build. You may have 
    to manually remove $(OBJ_D)\i_*.obj from ms\nt.mak if the build process
    complains about missing files or forbidden IDEA. Again the files provided
    in the subversion repository are already fixed.

    The MSVC project simply invokes PCBuild/build_ssl.py to perform
    the build.  This Python script locates and builds your OpenSSL
    installation, then invokes a simple makefile to build the final .pyd.

    build_ssl.py attempts to catch the most common errors (such as not
    being able to find OpenSSL sources, or not being able to find a Perl
    that works with OpenSSL) and give a reasonable error message.
    If you have a problem that doesn't seem to be handled correctly
    (eg, you know you have ActivePerl but we can't find it), please take
    a peek at build_ssl.py and suggest patches.  Note that build_ssl.py
    should be able to be run directly from the command-line.

    build_ssl.py/MSVC isn't clever enough to clean OpenSSL - you must do
    this by hand.

The subprojects above wrap external projects Python doesn't control, and as
such, a little more work is required in order to download the relevant source 
files for each project before they can be built.  The buildbots do this each
time they're built, so the easiest approach is to run either external.bat or 
external-amd64.bat in the ..\Tools\buildbot directory from ..\, i.e.:

    C:\..\svn.python.org\projects\python\trunk\PCbuild>cd ..
    C:\..\svn.python.org\projects\python\trunk>Tools\buildbot\external.bat

This extracts all the external subprojects from http://svn.python.org/external
via Subversion (so you'll need an svn.exe on your PATH) and places them in 
..\.. (relative to this directory).  The external(-amd64).bat scripts will
also build a debug build of Tcl/Tk; there aren't any equivalent batch files
for building release versions of Tcl/Tk lying around in the Tools\buildbot
directory.  If you need to build a release version of Tcl/Tk it isn't hard
though, take a look at the relevant external(-amd64).bat file and find the
two nmake lines, then call each one without the 'DEBUG=1' parameter, i.e.:

The external-amd64.bat file contains this for tcl:
    nmake -f makefile.vc COMPILERFLAGS=-DWINVER=0x0500 DEBUG=1 MACHINE=AMD64 INSTALLDIR=..\..\tcltk64 clean all install

So for a release build, you'd call it as:
    nmake -f makefile.vc COMPILERFLAGS=-DWINVER=0x0500 MACHINE=AMD64 INSTALLDIR=..\..\tcltk64 clean all install

    XXX Should we compile with OPTS=threads?
    XXX Our installer copies a lot of stuff out of the Tcl/Tk install
    XXX directory.  Is all of that really needed for Python use of Tcl/Tk?

This will be cleaned up in the future; ideally Tcl/Tk will be brought into our
pcbuild.sln as custom .vcproj files, just as we've recently done with the
_bsddb.vcproj and sqlite3.vcproj files, which will remove the need for
Tcl/Tk to be built separately via a batch file.

XXX trent.nelson 02-Apr-08:
    Having the external subprojects in ..\.. relative to this directory is a
    bit of a nuisance when you're working on py3k and trunk in parallel and
    your directory layout mimics that of Python's subversion layout, e.g.:

        C:\..\svn.python.org\projects\python\trunk
        C:\..\svn.python.org\projects\python\branches\py3k
        C:\..\svn.python.org\projects\python\branches\release25-maint

    I'd like to change things so that external subprojects are fetched from
    ..\external instead of ..\.., then provide some helper scripts or batch
    files that would set up a new ..\external directory with svn checkouts of
    the relevant branches in http://svn.python.org/projects/external/, or
    alternatively, use junctions to link ..\external with a pre-existing
    externals directory being used by another branch.  i.e. if I'm usually
    working on trunk (and have previously created trunk\external via the
    provided batch file), and want to do some work on py3k, I'd set up a
    junction as follows (using the directory structure above as an example):

        C:\..\python\trunk\external <- already exists and has built versions
                                       of the external subprojects 

        C:\..\python\branches\py3k>linkd.exe external ..\..\trunk\external
        Link created at: external

    Only a slight tweak would be needed to the buildbots such that bots
    building trunk and py3k could make use of the same facility.  (2.5.x
    builds need to be kept separate as they're using Visual Studio 7.1.)
/XXX trent.nelson 02-Apr-08

Building for Itanium
--------------------

NOTE:
Official support for Itanium builds have been dropped from the build. Please
contact us and provide patches if you are interested in Itanium builds.

The project files support a ReleaseItanium configuration which creates
Win64/Itanium binaries. For this to work, you need to install the Platform
SDK, in particular the 64-bit support. This includes an Itanium compiler
(future releases of the SDK likely include an AMD64 compiler as well).
In addition, you need the Visual Studio plugin for external C compilers,
from http://sf.net/projects/vsextcomp. The plugin will wrap cl.exe, to
locate the proper target compiler, and convert compiler options
accordingly. The project files require at least version 0.9.

Building for AMD64
------------------

The build process for AMD64 / x64 is very similar to standard builds. You just
have to set x64 as platform. In addition, the HOST_PYTHON environment variable
must point to a Python interpreter (at least 2.4), to support cross-compilation.

Building Python Using the free MS Toolkit Compiler
--------------------------------------------------

Microsoft has withdrawn the free MS Toolkit Compiler, so this can no longer
be considered a supported option. Instead you can use the free VS C++ Express
Edition.

Profile Guided Optimization
---------------------------

The solution has two configurations for PGO. The PGInstrument
configuration must be build first. The PGInstrument binaries are
lniked against a profiling library and contain extra debug
information. The PGUpdate configuration takes the profiling data and
generates optimized binaries.

The build_pgo.bat script automates the creation of optimized binaries. It
creates the PGI files, runs the unit test suite or PyBench with the PGI
python and finally creates the optimized files.

http://msdn2.microsoft.com/en-us/library/e7k32f4k(VS.90).aspx

Static library
--------------

The solution has no configuration for static libraries. However it is easy
it build a static library instead of a DLL. You simply have to set the 
"Configuration Type" to "Static Library (.lib)" and alter the preprocessor
macro "Py_ENABLE_SHARED" to "Py_NO_ENABLE_SHARED". You may also have to
change the "Runtime Library" from "Multi-threaded DLL (/MD)" to 
"Multi-threaded (/MT)".

Visual Studio properties
------------------------

The PCbuild solution makes heavy use of Visual Studio property files 
(*.vsprops). The properties can be viewed and altered in the Property
Manager (View -> Other Windows -> Property Manager).

 * debug (debug macro: _DEBUG)
 * pginstrument (PGO)
 * pgupdate (PGO)
    +-- pginstrument
 * pyd (python extension, release build)
    +-- release
    +-- pyproject
 * pyd_d (python extension, debug build)
    +-- debug
    +-- pyproject
 * pyproject (base settings for all projects, user macros like PyDllName)
 * release (release macro: NDEBUG)
 * x64 (AMD64 / x64 platform specific settings)

The pyproject propertyfile defines _WIN32 and x64 defines _WIN64 and _M_X64
although the macros are set by the compiler, too. The GUI doesn't always know
about the macros and confuse the user with false information.

YOUR OWN EXTENSION DLLs
-----------------------

If you want to create your own extension module DLL, there's an example
with easy-to-follow instructions in ../PC/example/; read the file
readme.txt there first.

This is Python version 2.7.6
============================

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013 Python Software Foundation.  All rights reserved.

Copyright (c) 2000 BeOpen.com.
All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.
All rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.
All rights reserved.


License information
-------------------

See the file "LICENSE" for information on the history of this
software, terms & conditions for usage, and a DISCLAIMER OF ALL
WARRANTIES.

This Python distribution contains no GNU General Public Licensed
(GPLed) code so it may be used in proprietary projects just like prior
Python distributions.  There are interfaces to some GNU code but these
are entirely optional.

All trademarks referenced herein are property of their respective
holders.


What's new in this release?
---------------------------

See the file "Misc/NEWS".


If you don't read instructions
------------------------------

Congratulations on getting this far. :-)

To start building right away (on UNIX): type "./configure" in the
current directory and when it finishes, type "make".  This creates an
executable "./python"; to install in /usr/local, first do "su root"
and then "make install".

The section `Build instructions' below is still recommended reading.


What is Python anyway?
----------------------

Python is an interpreted, interactive object-oriented programming
language suitable (amongst other uses) for distributed application
development, scripting, numeric computing and system testing.  Python
is often compared to Tcl, Perl, Java, JavaScript, Visual Basic or
Scheme.  To find out more about what Python can do for you, point your
browser to http://www.python.org/.


How do I learn Python?
----------------------

The official tutorial is still a good place to start; see
http://docs.python.org/ for online and downloadable versions, as well
as a list of other introductions, and reference documentation.

There's a quickly growing set of books on Python.  See
http://wiki.python.org/moin/PythonBooks for a list.


Documentation
-------------

All documentation is provided online in a variety of formats.  In
order of importance for new users: Tutorial, Library Reference,
Language Reference, Extending & Embedding, and the Python/C API.  The
Library Reference is especially of immense value since much of
Python's power is described there, including the built-in data types
and functions!

All documentation is also available online at the Python web site
(http://docs.python.org/, see below).  It is available online for occasional
reference, or can be downloaded in many formats for faster access.  The
documentation is downloadable in HTML, PostScript, PDF, LaTeX, and
reStructuredText (2.6+) formats; the LaTeX and reStructuredText versions are
primarily for documentation authors, translators, and people with special
formatting requirements.

If you would like to contribute to the development of Python, relevant
documentation is available at:

    http://docs.python.org/devguide/

For information about building Python's documentation, refer to Doc/README.txt.


Web sites
---------

New Python releases and related technologies are published at
http://www.python.org/.  Come visit us!


Newsgroups and Mailing Lists
----------------------------

Read comp.lang.python, a high-volume discussion newsgroup about
Python, or comp.lang.python.announce, a low-volume moderated newsgroup
for Python-related announcements.  These are also accessible as
mailing lists: see http://www.python.org/community/lists/ for an
overview of these and many other Python-related mailing lists.

Archives are accessible via the Google Groups Usenet archive; see
http://groups.google.com/.  The mailing lists are also archived, see
http://www.python.org/community/lists/ for details.


Bug reports
-----------

To report or search for bugs, please use the Python Bug
Tracker at http://bugs.python.org/.


Patches and contributions
-------------------------

To submit a patch or other contribution, please use the Python Patch
Manager at http://bugs.python.org/.  Guidelines
for patch submission may be found at http://www.python.org/dev/patches/.

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or python-ideas mailing lists for inital feedback. A Python
Enhancement Proposal (PEP) may be submitted if your idea gains ground. All
current PEPs, as well as guidelines for submitting a new PEP, are listed at
http://www.python.org/dev/peps/.


Questions
---------

For help, if you can't find it in the manuals or on the web site, it's
best to post to the comp.lang.python or the Python mailing list (see
above).  If you specifically don't want to involve the newsgroup or
mailing list, send questions to help@python.org (a group of volunteers
who answer questions as they can).  The newsgroup is the most
efficient way to ask public questions.


Build instructions
==================

Before you can build Python, you must first configure it.
Fortunately, the configuration and build process has been automated
for Unix and Linux installations, so all you usually have to do is
type a few commands and sit back.  There are some platforms where
things are not quite as smooth; see the platform specific notes below.
If you want to build for multiple platforms sharing the same source
tree, see the section on VPATH below.

Start by running the script "./configure", which determines your
system configuration and creates the Makefile.  (It takes a minute or
two -- please be patient!)  You may want to pass options to the
configure script -- see the section below on configuration options and
variables.  When it's done, you are ready to run make.

To build Python, you normally type "make" in the toplevel directory.
If you have changed the configuration, the Makefile may have to be
rebuilt.  In this case, you may have to run make again to correctly
build your desired target.  The interpreter executable is built in the
top level directory.

Once you have built a Python interpreter, see the subsections below on
testing and installation.  If you run into trouble, see the next
section.

Previous versions of Python used a manual configuration process that
involved editing the file Modules/Setup.  While this file still exists
and manual configuration is still supported, it is rarely needed any
more: almost all modules are automatically built as appropriate under
guidance of the setup.py script, which is run by Make after the
interpreter has been built.


Troubleshooting
---------------

See also the platform specific notes in the next section.

If you run into other trouble, see the FAQ
(http://www.python.org/doc/faq/) for hints on what can go wrong, and
how to fix it.

If you rerun the configure script with different options, remove all
object files by running "make clean" before rebuilding.  Believe it or
not, "make clean" sometimes helps to clean up other inexplicable
problems as well.  Try it before sending in a bug report!

If the configure script fails or doesn't seem to find things that
should be there, inspect the config.log file.

If you get a warning for every file about the -Olimit option being no
longer supported, you can ignore it.  There's no foolproof way to know
whether this option is needed; all we can do is test whether it is
accepted without error.  On some systems, e.g. older SGI compilers, it
is essential for performance (specifically when compiling ceval.c,
which has more basic blocks than the default limit of 1000).  If the
warning bothers you, edit the Makefile to remove "-Olimit 1500" from
the OPT variable.

If you get failures in test_long, or sys.maxint gets set to -1, you
are probably experiencing compiler bugs, usually related to
optimization.  This is a common problem with some versions of gcc, and
some vendor-supplied compilers, which can sometimes be worked around
by turning off optimization.  Consider switching to stable versions
(gcc 2.95.2, gcc 3.x, or contact your vendor.)

From Python 2.0 onward, all Python C code is ANSI C.  Compiling using
old K&R-C-only compilers is no longer possible.  ANSI C compilers are
available for all modern systems, either in the form of updated
compilers from the vendor, or one of the free compilers (gcc).

If "make install" fails mysteriously during the "compiling the library"
step, make sure that you don't have any of the PYTHONPATH or PYTHONHOME
environment variables set, as they may interfere with the newly built
executable which is compiling the library.

Unsupported systems
-------------------

A number of systems are not supported in Python 2.7 anymore. Some
support code is still present, but will be removed in later versions.
If you still need to use current Python versions on these systems,
please send a message to python-dev@python.org indicating that you
volunteer to support this system. For a more detailed discussion 
regarding no-longer-supported and resupporting platforms, as well
as a list of platforms that became or will be unsupported, see PEP 11.

More specifically, the following systems are not supported any
longer:
- SunOS 4
- DYNIX
- dgux
- Minix
- NeXT
- Irix 4 and --with-sgi-dl
- Linux 1
- Systems defining __d6_pthread_create (configure.ac)
- Systems defining PY_PTHREAD_D4, PY_PTHREAD_D6,
  or PY_PTHREAD_D7 in thread_pthread.h
- Systems using --with-dl-dld
- Systems using --without-universal-newlines
- MacOS 9
- Systems using --with-wctype-functions
- Win9x, WinME


Platform specific notes
-----------------------

(Some of these may no longer apply.  If you find you can build Python
on these platforms without the special directions mentioned here,
submit a documentation bug report to SourceForge (see Bug Reports
above) so we can remove them!)

Unix platforms: If your vendor still ships (and you still use) Berkeley DB
        1.85 you will need to edit Modules/Setup to build the bsddb185
        module and add a line to sitecustomize.py which makes it the
        default.  In Modules/Setup a line like

            bsddb185 bsddbmodule.c

        should work.  (You may need to add -I, -L or -l flags to direct the
        compiler and linker to your include files and libraries.)

XXX I think this next bit is out of date:

64-bit platforms: The modules audioop, and imageop don't work.
        The setup.py script disables them on 64-bit installations.
        Don't try to enable them in the Modules/Setup file.  They
        contain code that is quite wordsize sensitive.  (If you have a
        fix, let us know!)

Solaris: When using Sun's C compiler with threads, at least on Solaris
        2.5.1, you need to add the "-mt" compiler option (the simplest
        way is probably to specify the compiler with this option as
        the "CC" environment variable when running the configure
        script).

        When using GCC on Solaris, beware of binutils 2.13 or GCC
        versions built using it.  This mistakenly enables the
        -zcombreloc option which creates broken shared libraries on
        Solaris.  binutils 2.12 works, and the binutils maintainers
        are aware of the problem.  Binutils 2.13.1 only partially
        fixed things.  It appears that 2.13.2 solves the problem
        completely.  This problem is known to occur with Solaris 2.7
        and 2.8, but may also affect earlier and later versions of the
        OS.

        When the dynamic loader complains about errors finding shared
        libraries, such as

        ld.so.1: ./python: fatal: libstdc++.so.5: open failed:
        No such file or directory

        you need to first make sure that the library is available on
        your system. Then, you need to instruct the dynamic loader how
        to find it. You can choose any of the following strategies:

        1. When compiling Python, set LD_RUN_PATH to the directories
           containing missing libraries.
        2. When running Python, set LD_LIBRARY_PATH to these directories.
        3. Use crle(8) to extend the search path of the loader.
        4. Modify the installed GCC specs file, adding -R options into the
           *link: section.

        The complex object fails to compile on Solaris 10 with gcc 3.4 (at
        least up to 3.4.3).  To work around it, define Py_HUGE_VAL as
        HUGE_VAL(), e.g.:

          make CPPFLAGS='-D"Py_HUGE_VAL=HUGE_VAL()" -I. -I$(srcdir)/Include'
          ./python setup.py CPPFLAGS='-D"Py_HUGE_VAL=HUGE_VAL()"'

Linux:  A problem with threads and fork() was tracked down to a bug in
        the pthreads code in glibc version 2.0.5; glibc version 2.0.7
        solves the problem.  This causes the popen2 test to fail;
        problem and solution reported by Pablo Bleyer.

Red Hat Linux: Red Hat 9 built Python2.2 in UCS-4 mode and hacked
        Tcl to support it. To compile Python2.3 with Tkinter, you will
        need to pass --enable-unicode=ucs4 flag to ./configure.

        There's an executable /usr/bin/python which is Python
        1.5.2 on most older Red Hat installations; several key Red Hat tools
        require this version.  Python 2.1.x may be installed as
        /usr/bin/python2.  The Makefile installs Python as
        /usr/local/bin/python, which may or may not take precedence
        over /usr/bin/python, depending on how you have set up $PATH.

FreeBSD 3.x and probably platforms with NCurses that use libmytinfo or
        similar: When using cursesmodule, the linking is not done in
        the correct order with the defaults.  Remove "-ltermcap" from
        the readline entry in Setup, and use as curses entry: "curses
        cursesmodule.c -lmytinfo -lncurses -ltermcap" - "mytinfo" (so
        called on FreeBSD) should be the name of the auxiliary library
        required on your platform.  Normally, it would be linked
        automatically, but not necessarily in the correct order.

BSDI:   BSDI versions before 4.1 have known problems with threads,
        which can cause strange errors in a number of modules (for
        instance, the 'test_signal' test script will hang forever.)
        Turning off threads (with --with-threads=no) or upgrading to
        BSDI 4.1 solves this problem.

DEC Unix: Run configure with --with-dec-threads, or with
        --with-threads=no if no threads are desired (threads are on by
        default).  When using GCC, it is possible to get an internal
        compiler error if optimization is used.  This was reported for
        GCC 2.7.2.3 on selectmodule.c.  Manually compile the affected
        file without optimization to solve the problem.

DEC Ultrix: compile with GCC to avoid bugs in the native compiler,
        and pass SHELL=/bin/sh5 to Make when installing.

AIX:    A complete overhaul of the shared library support is now in
        place.  See Misc/AIX-NOTES for some notes on how it's done.
        (The optimizer bug reported at this place in previous releases
        has been worked around by a minimal code change.) If you get
        errors about pthread_* functions, during compile or during
        testing, try setting CC to a thread-safe (reentrant) compiler,
        like "cc_r".  For full C++ module support, set CC="xlC_r" (or
        CC="xlC" without thread support).

AIX 5.3: To build a 64-bit version with IBM's compiler, I used the
        following:

        export PATH=/usr/bin:/usr/vacpp/bin
        ./configure --with-gcc="xlc_r -q64" --with-cxx="xlC_r -q64" \
                    --disable-ipv6 AR="ar -X64"
        make

HP-UX:  When using threading, you may have to add -D_REENTRANT to the
        OPT variable in the top-level Makefile; reported by Pat Knight,
        this seems to make a difference (at least for HP-UX 10.20)
        even though pyconfig.h defines it. This seems unnecessary when
        using HP/UX 11 and later - threading seems to work "out of the
        box".

HP-UX ia64: When building on the ia64 (Itanium) platform using HP's
        compiler, some experience has shown that the compiler's
        optimiser produces a completely broken version of python
        (see http://bugs.python.org/814976). To work around this,
        edit the Makefile and remove -O from the OPT line.

        To build a 64-bit executable on an Itanium 2 system using HP's
        compiler, use these environment variables:

                CC=cc
                CXX=aCC
                BASECFLAGS="+DD64"
                LDFLAGS="+DD64 -lxnet"

        and call configure as:

                ./configure --without-gcc

        then *unset* the environment variables again before running
        make.  (At least one of these flags causes the build to fail
        if it remains set.)  You still have to edit the Makefile and
        remove -O from the OPT line.

HP PA-RISC 2.0: A recent bug report (http://bugs.python.org/546117)
        suggests that the C compiler in this 64-bit system has bugs
        in the optimizer that break Python.  Compiling without
        optimization solves the problems.

SCO:    The following apply to SCO 3 only; Python builds out of the box
        on SCO 5 (or so we've heard).

        1) Everything works much better if you add -U__STDC__ to the
        defs.  This is because all the SCO header files are broken.
        Anything that isn't mentioned in the C standard is
        conditionally excluded when __STDC__ is defined.

        2) Due to the U.S. export restrictions, SCO broke the crypt
        stuff out into a separate library, libcrypt_i.a so the LIBS
        needed be set to:

                LIBS=' -lsocket -lcrypt_i'

UnixWare: There are known bugs in the math library of the system, as well as
        problems in the handling of threads (calling fork in one
        thread may interrupt system calls in others). Therefore, test_math and
        tests involving threads will fail until those problems are fixed.

QNX:    Chris Herborth (chrish@qnx.com) writes:
        configure works best if you use GNU bash; a port is available on
        ftp.qnx.com in /usr/free.  I used the following process to build,
        test and install Python 1.5.x under QNX:

        1) CONFIG_SHELL=/usr/local/bin/bash CC=cc RANLIB=: \
            ./configure --verbose --without-gcc --with-libm=""

        2) edit Modules/Setup to activate everything that makes sense for
           your system... tested here at QNX with the following modules:

                array, audioop, binascii, cPickle, cStringIO, cmath,
                crypt, curses, errno, fcntl, gdbm, grp, imageop,
                _locale, math, md5, new, operator, parser, pcre,
                posix, pwd, readline, regex, reop,
                select, signal, socket, soundex, strop, struct,
                syslog, termios, time, timing, zlib, audioop, imageop

        3) make SHELL=/usr/local/bin/bash

           or, if you feel the need for speed:

           make SHELL=/usr/local/bin/bash OPT="-5 -Oil+nrt"

        4) make SHELL=/usr/local/bin/bash test

           Using GNU readline 2.2 seems to behave strangely, but I
           think that's a problem with my readline 2.2 port.  :-\

        5) make SHELL=/usr/local/bin/bash install

        If you get SIGSEGVs while running Python (I haven't yet, but
        I've only run small programs and the test cases), you're
        probably running out of stack; the default 32k could be a
        little tight.  To increase the stack size, edit the Makefile
        to read: LDFLAGS = -N 48k

BeOS:   See Misc/BeOS-NOTES for notes about compiling/installing
        Python on BeOS R3 or later.  Note that only the PowerPC
        platform is supported for R3; both PowerPC and x86 are
        supported for R4.

Cray T3E: Mark Hadfield (m.hadfield@niwa.co.nz) writes:
        Python can be built satisfactorily on a Cray T3E but based on
        my experience with the NIWA T3E (2002-05-22, version 2.2.1)
        there are a few bugs and gotchas. For more information see a
        thread on comp.lang.python in May 2002 entitled "Building
        Python on Cray T3E".

        1) Use Cray's cc and not gcc. The latter was reported not to
           work by Konrad Hinsen. It may work now, but it may not.

        2) To set sys.platform to something sensible, pass the
           following environment variable to the configure script:

             MACHDEP=unicosmk

        2) Run configure with option "--enable-unicode=ucs4".

        3) The Cray T3E does not support dynamic linking, so extension
           modules have to be built by adding (or uncommenting) lines
           in Modules/Setup. The minimum set of modules is

             posix, new, _sre, unicodedata

           On NIWA's vanilla T3E system the following have also been
           included successfully:

             _codecs, _locale, _socket, _symtable, _testcapi, _weakref
             array, binascii, cmath, cPickle, crypt, cStringIO, dbm
             errno, fcntl, grp, math, md5, operator, parser, pcre, pwd
             regex, rotor, select, struct, strop, syslog, termios
             time, timing, xreadlines

        4) Once the python executable and library have been built, make
           will execute setup.py, which will attempt to build remaining
           extensions and link them dynamically. Each of these attempts
           will fail but should not halt the make process. This is
           normal.

        5) Running "make test" uses a lot of resources and causes
           problems on our system. You might want to try running tests
           singly or in small groups.

SGI:    SGI's standard "make" utility (/bin/make or /usr/bin/make)
        does not check whether a command actually changed the file it
        is supposed to build.  This means that whenever you say "make"
        it will redo the link step.  The remedy is to use SGI's much
        smarter "smake" utility (/usr/sbin/smake), or GNU make.  If
        you set the first line of the Makefile to #!/usr/sbin/smake
        smake will be invoked by make (likewise for GNU make).

        WARNING: There are bugs in the optimizer of some versions of
        SGI's compilers that can cause bus errors or other strange
        behavior, especially on numerical operations.  To avoid this,
        try building with "make OPT=".

OS/2:   If you are running Warp3 or Warp4 and have IBM's VisualAge C/C++
        compiler installed, just change into the pc\os2vacpp directory
        and type NMAKE.  Threading and sockets are supported by default
        in the resulting binaries of PYTHON15.DLL and PYTHON.EXE.

Reliant UNIX: The thread support does not compile on Reliant UNIX, and
        there is a (minor) problem in the configure script for that
        platform as well.  This should be resolved in time for a
        future release.

MacOSX: The tests will crash on both 10.1 and 10.2 with SEGV in
        test_re and test_sre due to the small default stack size.  If
        you set the stack size to 2048 before doing a "make test" the
        failure can be avoided.  If you're using the tcsh or csh shells,
        use "limit stacksize 2048" and for the bash shell (the default
        as of OSX 10.3), use "ulimit -s 2048".

        On naked Darwin you may want to add the configure option
        "--disable-toolbox-glue" to disable the glue code for the Carbon
        interface modules. The modules themselves are currently only built
        if you add the --enable-framework option, see below.

        On a clean OSX /usr/local does not exist. Do a
        "sudo mkdir -m 775 /usr/local"
        before you do a make install. It is probably not a good idea to
        do "sudo make install" which installs everything as superuser,
        as this may later cause problems when installing distutils-based
        additions.

        Some people have reported problems building Python after using "fink"
        to install additional unix software. Disabling fink (remove all 
        references to /sw from your .profile or .login) should solve this.

        You may want to try the configure option "--enable-framework"
        which installs Python as a framework. The location can be set
        as argument to the --enable-framework option (default
        /Library/Frameworks). A framework install is probably needed if you
        want to use any Aqua-based GUI toolkit (whether Tkinter, wxPython,
        Carbon, Cocoa or anything else).

        You may also want to try the configure option "--enable-universalsdk"
        which builds Python as a universal binary with support for the 
        i386 and PPC architetures. This requires Xcode 2.1 or later to build.

        See Mac/README for more information on framework and 
        universal builds.

Cygwin: With recent (relative to the time of writing, 2001-12-19)
        Cygwin installations, there are problems with the interaction
        of dynamic linking and fork().  This manifests itself in build
        failures during the execution of setup.py.

        There are two workarounds that both enable Python (albeit
        without threading support) to build and pass all tests on
        NT/2000 (and most likely XP as well, though reports of testing
        on XP would be appreciated).

        The workarounds:

        (a) the band-aid fix is to link the _socket module statically
        rather than dynamically (which is the default).

        To do this, run "./configure --with-threads=no" including any
        other options you need (--prefix, etc.).  Then in Modules/Setup
        uncomment the lines:

        #SSL=/usr/local/ssl
        #_socket socketmodule.c \
        #       -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        #       -L$(SSL)/lib -lssl -lcrypto

        and remove "local/" from the SSL variable.  Finally, just run
        "make"!

        (b) The "proper" fix is to rebase the Cygwin DLLs to prevent
        base address conflicts.  Details on how to do this can be
        found in the following mail:

           http://sources.redhat.com/ml/cygwin/2001-12/msg00894.html

        It is hoped that a version of this solution will be
        incorporated into the Cygwin distribution fairly soon.

        Two additional problems:

        (1) Threading support should still be disabled due to a known
        bug in Cygwin pthreads that causes test_threadedtempfile to
        hang.

        (2) The _curses module does not build.  This is a known
        Cygwin ncurses problem that should be resolved the next time
        that this package is released.

        On older versions of Cygwin, test_poll may hang and test_strftime
        may fail.

        The situation on 9X/Me is not accurately known at present.
        Some time ago, there were reports that the following
        regression tests failed:

            test_pwd
            test_select (hang)
            test_socket

        Due to the test_select hang on 9X/Me, one should run the
        regression test using the following:

            make TESTOPTS='-l -x test_select' test

        News regarding these platforms with more recent Cygwin
        versions would be appreciated!

Windows: When executing Python scripts on the command line using file type
        associations (i.e. starting "script.py" instead of "python script.py"),
        redirects may not work unless you set a specific registry key.  See
        the Knowledge Base article <http://support.microsoft.com/kb/321788>.


Configuring the bsddb and dbm modules
-------------------------------------

Beginning with Python version 2.3, the PyBsddb package
<http://pybsddb.sf.net/> was adopted into Python as the bsddb package,
exposing a set of package-level functions which provide
backwards-compatible behavior.  Only versions 3.3 through 4.4 of
Sleepycat's libraries provide the necessary API, so older versions
aren't supported through this interface.  The old bsddb module has
been retained as bsddb185, though it is not built by default.  Users
wishing to use it will have to tweak Modules/Setup to build it.  The
dbm module will still be built against the Sleepycat libraries if
other preferred alternatives (ndbm, gdbm) are not found.

Building the sqlite3 module
---------------------------

To build the sqlite3 module, you'll need the sqlite3 or libsqlite3
packages installed, including the header files. Many modern operating
systems distribute the headers in a separate package to the library -
often it will be the same name as the main package, but with a -dev or
-devel suffix. 

The version of pysqlite2 that's including in Python needs sqlite3 3.0.8
or later. setup.py attempts to check that it can find a correct version.

Configuring threads
-------------------

As of Python 2.0, threads are enabled by default.  If you wish to
compile without threads, or if your thread support is broken, pass the
--with-threads=no switch to configure.  Unfortunately, on some
platforms, additional compiler and/or linker options are required for
threads to work properly.  Below is a table of those options,
collected by Bill Janssen.  We would love to automate this process
more, but the information below is not enough to write a patch for the
configure.ac file, so manual intervention is required.  If you patch
the configure.ac file and are confident that the patch works, please
send in the patch.  (Don't bother patching the configure script itself
-- it is regenerated each time the configure.ac file changes.)

Compiler switches for threads
.............................

The definition of _REENTRANT should be configured automatically, if
that does not work on your system, or if _REENTRANT is defined
incorrectly, please report that as a bug.

    OS/Compiler/threads                     Switches for use with threads
    (POSIX is draft 10, DCE is draft 4)     compile & link

    SunOS 5.{1-5}/{gcc,SunPro cc}/solaris   -mt
    SunOS 5.5/{gcc,SunPro cc}/POSIX         (nothing)
    DEC OSF/1 3.x/cc/DCE                    -threads
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/cc/DCE                 -threads
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/cc/POSIX               -pthread
            (butenhof@zko.dec.com)
    AIX 4.1.4/cc_r/d7                       (nothing)
            (buhrt@iquest.net)
    AIX 4.1.4/cc_r4/DCE                     (nothing)
            (buhrt@iquest.net)
    IRIX 6.2/cc/POSIX                       (nothing)
            (robertl@cwi.nl)


Linker (ld) libraries and flags for threads
...........................................

    OS/threads                          Libraries/switches for use with threads

    SunOS 5.{1-5}/solaris               -lthread
    SunOS 5.5/POSIX                     -lpthread
    DEC OSF/1 3.x/DCE                   -lpthreads -lmach -lc_r -lc
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/DCE                -lpthreads -lpthread -lmach -lexc -lc
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/POSIX              -lpthread -lmach -lexc -lc
            (butenhof@zko.dec.com)
    AIX 4.1.4/{draft7,DCE}              (nothing)
            (buhrt@iquest.net)
    IRIX 6.2/POSIX                      -lpthread
            (jph@emilia.engr.sgi.com)


Building a shared libpython
---------------------------

Starting with Python 2.3, the majority of the interpreter can be built
into a shared library, which can then be used by the interpreter
executable, and by applications embedding Python. To enable this feature,
configure with --enable-shared.

If you enable this feature, the same object files will be used to create
a static library.  In particular, the static library will contain object
files using position-independent code (PIC) on platforms where PIC flags
are needed for the shared library.


Configuring additional built-in modules
---------------------------------------

Starting with Python 2.1, the setup.py script at the top of the source
distribution attempts to detect which modules can be built and
automatically compiles them.  Autodetection doesn't always work, so
you can still customize the configuration by editing the Modules/Setup
file; but this should be considered a last resort.  The rest of this
section only applies if you decide to edit the Modules/Setup file.
You also need this to enable static linking of certain modules (which
is needed to enable profiling on some systems).

This file is initially copied from Setup.dist by the configure script;
if it does not exist yet, create it by copying Modules/Setup.dist
yourself (configure will never overwrite it).  Never edit Setup.dist
-- always edit Setup or Setup.local (see below).  Read the comments in
the file for information on what kind of edits are allowed.  When you
have edited Setup in the Modules directory, the interpreter will
automatically be rebuilt the next time you run make (in the toplevel
directory).

Many useful modules can be built on any Unix system, but some optional
modules can't be reliably autodetected.  Often the quickest way to
determine whether a particular module works or not is to see if it
will build: enable it in Setup, then if you get compilation or link
errors, disable it -- you're either missing support or need to adjust
the compilation and linking parameters for that module.

On SGI IRIX, there are modules that interface to many SGI specific
system libraries, e.g. the GL library and the audio hardware.  These
modules will not be built by the setup.py script.

In addition to the file Setup, you can also edit the file Setup.local.
(the makesetup script processes both).  You may find it more
convenient to edit Setup.local and leave Setup alone.  Then, when
installing a new Python version, you can copy your old Setup.local
file.


Setting the optimization/debugging options
------------------------------------------

If you want or need to change the optimization/debugging options for
the C compiler, assign to the OPT variable on the toplevel make
command; e.g. "make OPT=-g" will build a debugging version of Python
on most platforms.  The default is OPT=-O; a value for OPT in the
environment when the configure script is run overrides this default
(likewise for CC; and the initial value for LIBS is used as the base
set of libraries to link with).

When compiling with GCC, the default value of OPT will also include
the -Wall and -Wstrict-prototypes options.

Additional debugging code to help debug memory management problems can
be enabled by using the --with-pydebug option to the configure script.

For flags that change binary compatibility, use the EXTRA_CFLAGS
variable.


Profiling
---------

If you want C profiling turned on, the easiest way is to run configure
with the CC environment variable to the necessary compiler
invocation.  For example, on Linux, this works for profiling using
gprof(1):

    CC="gcc -pg" ./configure

Note that on Linux, gprof apparently does not work for shared
libraries.  The Makefile/Setup mechanism can be used to compile and
link most extension modules statically.


Coverage checking
-----------------

For C coverage checking using gcov, run "make coverage".  This will
build a Python binary with profiling activated, and a ".gcno" and
".gcda" file for every source file compiled with that option.  With
the built binary, now run the code whose coverage you want to check.
Then, you can see coverage statistics for each individual source file
by running gcov, e.g.

    gcov -o Modules zlibmodule

This will create a "zlibmodule.c.gcov" file in the current directory
containing coverage info for that source file.

This works only for source files statically compiled into the
executable; use the Makefile/Setup mechanism to compile and link
extension modules you want to coverage-check statically.


Testing
-------

To test the interpreter, type "make test" in the top-level directory.
This runs the test set twice (once with no compiled files, once with
the compiled files left by the previous test run).  The test set
produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.
If a message is printed about a failed test or a traceback or core
dump is produced, something is wrong.  On some Linux systems (those
that are not yet using glibc 6), test_strftime fails due to a
non-standard implementation of strftime() in the C library. Please
ignore this, or upgrade to glibc version 6.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run "make testall".

IMPORTANT: If the tests fail and you decide to mail a bug report,
*don't* include the output of "make test".  It is useless.  Run the
failing test manually, as follows:

        ./python Lib/test/regrtest.py -v test_whatever

(substituting the top of the source tree for '.' if you built in a
different directory).  This runs the test in verbose mode.


Installing
----------

To install the Python binary, library modules, shared library modules
(see below), include files, configuration files, and the manual page,
just type

        make install

This will install all platform-independent files in subdirectories of
the directory given with the --prefix option to configure or to the
`prefix' Make variable (default /usr/local).  All binary and other
platform-specific files will be installed in subdirectories if the
directory given by --exec-prefix or the `exec_prefix' Make variable
(defaults to the --prefix directory) is given.

If DESTDIR is set, it will be taken as the root directory of the
installation, and files will be installed into $(DESTDIR)$(prefix),
$(DESTDIR)$(exec_prefix), etc.

All subdirectories created will have Python's version number in their
name, e.g. the library modules are installed in
"/usr/local/lib/python<version>/" by default, where <version> is the
<major>.<minor> release number (e.g. "2.1").  The Python binary is
installed as "python<version>" and a hard link named "python" is
created.  The only file not installed with a version number in its
name is the manual page, installed as "/usr/local/man/man1/python.1"
by default.

If you want to install multiple versions of Python see the section below
entitled "Installing multiple versions".

The only thing you may have to install manually is the Python mode for
Emacs found in Misc/python-mode.el.  (But then again, more recent
versions of Emacs may already have it.)  Follow the instructions that
came with Emacs for installation of site-specific files.

On Mac OS X, if you have configured Python with --enable-framework, you
should use "make frameworkinstall" to do the installation. Note that this
installs the Python executable in a place that is not normally on your
PATH, you may want to set up a symlink in /usr/local/bin.


Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (--prefix argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using "make altinstall" contain the major and minor
version and can thus live side-by-side.  "make install" also creates
${prefix}/bin/python which refers to ${prefix}/bin/pythonX.Y.  If you intend
to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using
"make install".  Install all other versions using "make altinstall".

For example, if you want to install Python 2.5, 2.6 and 3.0 with 2.6 being
the primary version, you would execute "make install" in your 2.6 build
directory and "make altinstall" in the others.


Configuration options and variables
-----------------------------------

Some special cases are handled by passing options to the configure
script.

WARNING: if you rerun the configure script with different options, you
must run "make clean" before rebuilding.  Exceptions to this rule:
after changing --prefix or --exec-prefix, all you need to do is remove
Modules/getpath.o.

--with(out)-gcc: The configure script uses gcc (the GNU C compiler) if
        it finds it.  If you don't want this, or if this compiler is
        installed but broken on your platform, pass the option
        --without-gcc.  You can also pass "CC=cc" (or whatever the
        name of the proper C compiler is) in the environment, but the
        advantage of using --without-gcc is that this option is
        remembered by the config.status script for its --recheck
        option.

--prefix, --exec-prefix: If you want to install the binaries and the
        Python library somewhere else than in /usr/local/{bin,lib},
        you can pass the option --prefix=DIRECTORY; the interpreter
        binary will be installed as DIRECTORY/bin/python and the
        library files as DIRECTORY/lib/python/*.  If you pass
        --exec-prefix=DIRECTORY (as well) this overrides the
        installation prefix for architecture-dependent files (like the
        interpreter binary).  Note that --prefix=DIRECTORY also
        affects the default module search path (sys.path), when
        Modules/config.c is compiled.  Passing make the option
        prefix=DIRECTORY (and/or exec_prefix=DIRECTORY) overrides the
        prefix set at configuration time; this may be more convenient
        than re-running the configure script if you change your mind
        about the install prefix.

--with-readline: This option is no longer supported.  GNU
        readline is automatically enabled by setup.py when present.

--with-threads: On most Unix systems, you can now use multiple
        threads, and support for this is enabled by default.  To
        disable this, pass --with-threads=no.  If the library required
        for threads lives in a peculiar place, you can use
        --with-thread=DIRECTORY.  IMPORTANT: run "make clean" after
        changing (either enabling or disabling) this option, or you
        will get link errors!  Note: for DEC Unix use
        --with-dec-threads instead.

--with-sgi-dl: On SGI IRIX 4, dynamic loading of extension modules is
        supported by the "dl" library by Jack Jansen, which is
        ftp'able from ftp://ftp.cwi.nl/pub/dynload/dl-1.6.tar.Z.
        This is enabled (after you've ftp'ed and compiled the dl
        library) by passing --with-sgi-dl=DIRECTORY where DIRECTORY
        is the absolute pathname of the dl library.  (Don't bother on
        IRIX 5, it already has dynamic linking using SunOS style
        shared libraries.)  THIS OPTION IS UNSUPPORTED.

--with-dl-dld: Dynamic loading of modules is rumored to be supported
        on some other systems: VAX (Ultrix), Sun3 (SunOS 3.4), Sequent
        Symmetry (Dynix), and Atari ST.  This is done using a
        combination of the GNU dynamic loading package
        (ftp://ftp.cwi.nl/pub/dynload/dl-dld-1.1.tar.Z) and an
        emulation of the SGI dl library mentioned above (the emulation
        can be found at
        ftp://ftp.cwi.nl/pub/dynload/dld-3.2.3.tar.Z).  To
        enable this, ftp and compile both libraries, then call
        configure, passing it the option
        --with-dl-dld=DL_DIRECTORY,DLD_DIRECTORY where DL_DIRECTORY is
        the absolute pathname of the dl emulation library and
        DLD_DIRECTORY is the absolute pathname of the GNU dld library.
        (Don't bother on SunOS 4 or 5, they already have dynamic
        linking using shared libraries.)  THIS OPTION IS UNSUPPORTED.

--with-libm, --with-libc: It is possible to specify alternative
        versions for the Math library (default -lm) and the C library
        (default the empty string) using the options
        --with-libm=STRING and --with-libc=STRING, respectively.  For
        example, if your system requires that you pass -lc_s to the C
        compiler to use the shared C library, you can pass
        --with-libc=-lc_s. These libraries are passed after all other
        libraries, the C library last.

--with-libs='libs': Add 'libs' to the LIBS that the python interpreter
        is linked against.

--with-cxx-main=<compiler>: If you plan to use C++ extension modules,
        then -- on some platforms -- you need to compile python's main()
        function with the C++ compiler. With this option, make will use
        <compiler> to compile main() *and* to link the python executable.
        It is likely that the resulting executable depends on the C++
        runtime library of <compiler>. (The default is --without-cxx-main.)

        There are platforms that do not require you to build Python
        with a C++ compiler in order to use C++ extension modules.
        E.g., x86 Linux with ELF shared binaries and GCC 3.x, 4.x is such
        a platform. We recommend that you configure Python
        --without-cxx-main on those platforms because a mismatch
        between the C++ compiler version used to build Python and to
        build a C++ extension module is likely to cause a crash at
        runtime.

        The Python installation also stores the variable CXX that
        determines, e.g., the C++ compiler distutils calls by default
        to build C++ extensions. If you set CXX on the configure command
        line to any string of non-zero length, then configure won't
        change CXX. If you do not preset CXX but pass
        --with-cxx-main=<compiler>, then configure sets CXX=<compiler>.
        In all other cases, configure looks for a C++ compiler by
        some common names (c++, g++, gcc, CC, cxx, cc++, cl) and sets
        CXX to the first compiler it finds. If it does not find any
        C++ compiler, then it sets CXX="".

        Similarly, if you want to change the command used to link the
        python executable, then set LINKCC on the configure command line.


--with-pydebug:  Enable additional debugging code to help track down
        memory management problems.  This allows printing a list of all
        live objects when the interpreter terminates.

--with(out)-universal-newlines: enable reading of text files with
        foreign newline convention (default: enabled). In other words,
        any of \r, \n or \r\n is acceptable as end-of-line character.
        If enabled import and execfile will automatically accept any newline
        in files. Python code can open a file with open(file, 'U') to
        read it in universal newline mode. THIS OPTION IS UNSUPPORTED.

--with-tsc: Profile using the Pentium timestamping counter (TSC).

--with-system-ffi:  Build the _ctypes extension module using an ffi
        library installed on the system.

--with-dbmliborder=db1:db2:...:  Specify the order that backends for the
	dbm extension are checked. Valid value is a colon separated string
	with the backend names `ndbm', `gdbm' and `bdb'.

Building for multiple architectures (using the VPATH feature)
-------------------------------------------------------------

If your file system is shared between multiple architectures, it
usually is not necessary to make copies of the sources for each
architecture you want to support.  If the make program supports the
VPATH feature, you can create an empty build directory for each
architecture, and in each directory run the configure script (on the
appropriate machine with the appropriate options).  This creates the
necessary subdirectories and the Makefiles therein.  The Makefiles
contain a line VPATH=... which points to a directory containing the
actual sources.  (On SGI systems, use "smake -J1" instead of "make" if
you use VPATH -- don't try gnumake.)

For example, the following is all you need to build a minimal Python
in /usr/tmp/python (assuming ~guido/src/python is the toplevel
directory and you want to build in /usr/tmp/python):

        $ mkdir /usr/tmp/python
        $ cd /usr/tmp/python
        $ ~guido/src/python/configure
        [...]
        $ make
        [...]
        $

Note that configure copies the original Setup file to the build
directory if it finds no Setup file there.  This means that you can
edit the Setup file for each architecture independently.  For this
reason, subsequent changes to the original Setup file are not tracked
automatically, as they might overwrite local changes.  To force a copy
of a changed original Setup file, delete the target Setup file.  (The
makesetup script supports multiple input files, so if you want to be
fancy you can change the rules to create an empty Setup.local if it
doesn't exist and run it with arguments $(srcdir)/Setup Setup.local;
however this assumes that you only need to add modules.)

Also note that you can't use a workspace for VPATH and non VPATH builds. The
object files left behind by one version confuses the other.


Building on non-UNIX systems
----------------------------

For Windows (2000/NT/ME/98/95), assuming you have MS VC++ 7.1, the
project files are in PCbuild, the workspace is pcbuild.dsw.  See
PCbuild\readme.txt for detailed instructions.

For other non-Unix Windows compilers, in particular MS VC++ 6.0 and
for OS/2, enter the directory "PC" and read the file "readme.txt".

For the Mac, a separate source distribution will be made available,
for use with the CodeWarrior compiler.  If you are interested in Mac
development, join the PythonMac Special Interest Group
(http://www.python.org/sigs/pythonmac-sig/, or send email to
pythonmac-sig-request@python.org).

Of course, there are also binary distributions available for these
platforms -- see http://www.python.org/.

To port Python to a new non-UNIX system, you will have to fake the
effect of running the configure script manually (for Mac and PC, this
has already been done for you).  A good start is to copy the file
pyconfig.h.in to pyconfig.h and edit the latter to reflect the actual
configuration of your system.  Most symbols must simply be defined as
1 only if the corresponding feature is present and can be left alone
otherwise; however the *_t type symbols must be defined as some
variant of int if they need to be defined at all.

For all platforms, it's important that the build arrange to define the
preprocessor symbol NDEBUG on the compiler command line in a release
build of Python (else assert() calls remain in the code, hurting
release-build performance).  The Unix, Windows and Mac builds already
do this.


Miscellaneous issues
====================

Emacs mode
----------

There's an excellent Emacs editing mode for Python code; see the file
Misc/python-mode.el.  Originally written by the famous Tim Peters, it is now
maintained by the equally famous Barry Warsaw.  The latest version, along with
various other contributed Python-related Emacs goodies, is online at
http://launchpad.net/python-mode/.


Tkinter
-------

The setup.py script automatically configures this when it detects a
usable Tcl/Tk installation.  This requires Tcl/Tk version 8.0 or
higher.

For more Tkinter information, see the Tkinter Resource page:
http://www.python.org/topics/tkinter/

There are demos in the Demo/tkinter directory.

Note that there's a Python module called "Tkinter" (capital T) which
lives in Lib/lib-tk/Tkinter.py, and a C module called "_tkinter"
(lower case t and leading underscore) which lives in
Modules/_tkinter.c.  Demos and normal Tk applications import only the
Python Tkinter module -- only the latter imports the C _tkinter
module.  In order to find the C _tkinter module, it must be compiled
and linked into the Python interpreter -- the setup.py script does
this.  In order to find the Python Tkinter module, sys.path must be
set correctly -- normal installation takes care of this.


Distribution structure
----------------------

Most subdirectories have their own README files.  Most files have
comments.

Demo/           Demonstration scripts, modules and programs
Doc/            Documentation sources (reStructuredText)
Grammar/        Input for the parser generator
Include/        Public header files
LICENSE         Licensing information
Lib/            Python library modules
Mac/            Macintosh specific resources
Makefile.pre.in Source from which config.status creates the Makefile.pre
Misc/           Miscellaneous useful files
Modules/        Implementation of most built-in modules
Objects/        Implementation of most built-in object types
PC/             Files specific to PC ports (DOS, Windows, OS/2)
PCbuild/        Build directory for Microsoft Visual C++
Parser/         The parser and tokenizer and their input handling
Python/         The byte-compiler and interpreter
README          The file you're reading now
RISCOS/         Files specific to RISC OS port
Tools/          Some useful programs written in Python
pyconfig.h.in   Source from which pyconfig.h is created (GNU autoheader output)
configure       Configuration shell script (GNU autoconf output)
configure.ac    Configuration specification (input for GNU autoconf)
install-sh      Shell script used to install files
setup.py        Python script used to build extension modules

The following files will (may) be created in the toplevel directory by
the configuration and build processes:

Makefile        Build rules
Makefile.pre    Build rules before running Modules/makesetup
buildno         Keeps track of the build number
config.cache    Cache of configuration variables
pyconfig.h      Configuration header
config.log      Log from last configure run
config.status   Status from last run of the configure script
getbuildinfo.o  Object file from Modules/getbuildinfo.c
libpython<version>.a    The library archive
python          The executable interpreter
reflog.txt      Output from running the regression suite with the -R flag 
tags, TAGS      Tags files for vi and Emacs


That's all, folks!
------------------


--Guido van Rossum (home page: http://www.python.org/~guido/)

This directory contains files for the RISC OS port of Python.
For more information about RISC OS see http://www.riscos.com/ .

This port is currently being maintained by Dietmar Schwertberger,
dietmar@schwertberger.de .

On http://www.schwertberger.de you may find compiled versions and libraries
as well as RISC OS specific documentation and extensions.


==========================================================================
Compiling:

1. Extract Files from archive directory 'Python-...' to a directory named
   '!Python'.
2. Use a tool like Rename to change filenames from '*/[ch]' into '[ch].*'.
3. Create missing directories with 'amu cdirs'.
4. Build with 'amu'.

I've only tested Acorn/Norcroft C/C++ 5.30 and amu.

Python now uses the 32 bit libraries from Pace as well as the 32 bit
version of OSLib.

You will also need some additional libraries:

  DLK (patched version)
    http://www.schwertberger.de
  OSLib
    http://www.mk-net.demon.co.uk/oslib
  
  zlib (optional)
    ftp://ftp.freesoftware.com/pub/infozip/zlib/
  expat (optional)
    http://sourceforge.net/projects/expat/
    (makefile and config.h available from http://www.schwertberger.de/riscos_expat.zip

audiopy - a program to control the Solaris audio device.

Contact: Barry Warsaw
Email:   bwarsaw@python.org
Version: 1.1

Introduction

    Audiopy is a program to control the Solaris audio device, allowing
    you to choose both the input and output devices, and to set the
    output volume.  It can be run either as a standalone command-line
    script, or as a Tkinter based GUI application.

    Note that your version of Python must have been built with the
    sunaudiodev module enabled.  It is not enabled by default however!
    You will need to edit your Modules/Setup file, uncomment the
    sunaudiodev module spec line and rebuild Python.

    Using audiopy, you can select one of three possible input devices:
    the microphone, the line-in jack, or the CD in.  These choices are
    mutually exclusive; you can only have one active input device at
    any one time (this is enforced by the underlying device).  Some
    input devices may not be supported on all Solaris machines.

    You can also choose to enable any of the three possible output
    devices: the headphone jack, the speakers, or the line-out jack.
    You can enable any combination of these three devices.

    You can also set the output gain (volume) level.

Running as a GUI

    Simply start audiopy with no arguments to start it as a Tkinter
    based GUI application.  It will pop up a window with two sections:
    the top portion contains three radio buttons indicating your
    selected input device; the middle portion contains three
    checkboxes indicating your selected output devices; the bottom
    portion contains a slider that changes the output gain.

    Note the underlined characters in the button labels.  These
    indicate keyboard accelerators so that pressing Alt+character you
    can select that device.  For example, Alt-s toggles the Speaker
    device.  The Alt accelerators are the same as those you'd use in
    as the short-form command line switches (see below).

    Alt-q is also an accelerator for selecting Quit from the File
    menu.

    Unsupported devices will appear dimmed out in the GUI.  When run
    as a GUI, audiopy monitors the audio device and automatically
    updates its display if the state of the device is changed by some
    other means.  With Python versions before 1.5.2 this is done by
    occasionally polling the device, but in Python 1.5.2 no polling is
    necessary (you don't really need to know this, but I thought I'd
    plug 1.5.2 :-).
    
Running as a Command Line Program

    You can run audiopy from the command line to select any
    combination of input or output device, by using the command line
    options.  Actually, any option forces audiopy to run as a command
    line program and not display its GUI.

    Options have the general form

        --device[={0,1}]
        -d[-{0,1}]

    meaning there is both a long-form and short-form of the switch,
    where `device' or `d' is one of the following:

        (input)
            microphone -- m
            linein     -- i
            cd         -- c

        (output)
            headphones -- p
            speaker    -- s
            lineout    -- o

    When no value is given, the switch just toggles the specified
    device.  With a value, 0 turns the device off and 1 turns the
    device on.  Any other value is an error.

    For example, to turn the speakers off, turn the headphones on, and 
    toggle the cd input device, run audiopy from the command line like 
    so:

    % ./audiopy -s=0 -p=1 -c

    Audiopy understands these other command line options:

    --gain volume
    -g volume
        Sets the output volume to the specified gain level.  This must 
        be an integer between MIN_GAIN and MAX_GAIN (usually [0..255], 
        but use the -h option to find the exact values).

    --version
    -v
        Print the version number and exit

    --help
    -h
        Print a help message and exit
        


Local Variables:
indent-tabs-mode: nil
End:

BGEN -- Automatic Generation of Extension Modules
=================================================

This directory contains BGEN -- a package that helps in generating
complete source code for Python extension module.  For examples of its
use, see the Mac Python source distribution (available separately
from the Python ftp archives).  Note that BGEN is not Mac specific!

WARNING: bgen has been removed in 3.0.

Comments on building tcl/tk for AMD64 with the MS SDK compiler
==============================================================

I did have to build tcl/tk manually.

First, I had to build the nmakehlp.exe helper utility manually by executing
   cl nmakehlp.c /link bufferoverflowU.lib
in both the tcl8.4.12\win and tk8.4.12\win directories.

Second, the AMD64 compiler refuses to compile the file
tcl8.4.12\generic\tclExecute.c because it insists on using intrinsics
for the 'ceil' and 'floor' functions:

  ..\generic\tclExecute.c(394) : error C2099: initializer is not a constant
  ..\generic\tclExecute.c(398) : error C2099: initializer is not a constant

I did comment out these lines; an alternative would have been to use
the /Oi- compiler flag to disable the intrinsic functions.
The commands then used were these:

   svn export http://svn.python.org/projects/external/tcl8.4.12
   cd tcl8.4.12\win
   REM
   echo patch the tcl8.4.12\generic\tclExecute.c file
   pause 
   REM
   cl nmakehlp.c /link bufferoverflowU.lib
   nmake -f makefile.vc MACHINE=AMD64
   nmake -f makefile.vc INSTALLDIR=..\..\tcltk install
   cd ..\..
   svn export http://svn.python.org/projects/external/tk8.4.12
   cd tk8.4.12\win
   cl nmakehlp.c /link bufferoverflowU.lib
   nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 MACHINE=AMD64
   nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 INSTALLDIR=..\..\tcltk install
   cd ..\..

This directory contains support tools for the Python compiler package,
which is now part of the standard library.

compile.py	Demo that compiles a Python module into a .pyc file
		using the pure-Python compiler code.

demo.py		Prints the names of all the methods defined in a module,
		as a demonstration of walking through the abstract syntax
		tree produced by the parser.

dumppyc.py	Dumps the contents of a .pyc file, printing 
		the attributes of the code object followed by a 
		code disassembly.

regrtest.py	Runs the Python test suite using bytecode generated 
		by the pure-Python compiler code instead of the
		builtin compiler.


FAQ Wizard
----------

Author: Guido van Rossum <guido@python.org>
Version: 1.0
Date:  6 April 1998


This is a CGI program that maintains a user-editable FAQ.  It uses RCS
to keep track of changes to individual FAQ entries.  It is fully
configurable; everything you might want to change when using this
program to maintain some other FAQ than the Python FAQ is contained in
the configuration module, faqconf.py.

Note that the bulk of the code is not an executable script; it's an
importable module.  The actual script in cgi-bin is minimal.

Files:

faqw.py		executable script to be edited and installed in cgi-bin
faqwiz.py	main module, lives in same directory as FAQ entry files
faqconf.py	main configuration module
faqcust.py	additional local customization module (optional)
move-faqwiz.sh  Script to move faqwiz entries.


What's New?
-----------

Version 1.0 corrects some minor bugs and uses tab-agnostic
indentation; it is otherwise unchanged from version 0.9.0.

Version 0.9.0 uses the re module (Perl style regular expressions) for
all its regular expression needs, instead of the regex and regsub
modules (Emacs style).  This affects the syntax for regular
expressions entered by the user as search strings (with "regular
expression" checked), hence the version number jump.


Setup Information
-----------------

This assumes you are familiar with Python, with your http server, and
with running CGI scripts under your http server.  You need Python 1.5
or better.

Select a place where the Python modules that constitute the FAQ wizard
will live (the directory where you unpacked it is an obvious choice).
This will be called the SRCDIR.  This directory should not be writable
by other users of your system (since they would be able to execute
arbitrary code by invoking the FAQ wizard's CGI script).

Create a dedicated working directory, preferably one that's not
directly reachable from your http server.  This will be called the
FAQDIR.  Create a subdirectory named RCS.  Make both the working
directory and the RCS subdirectory wrld-writable.  (This is essential,
since the FAQ wizard runs as use nobody, and needs to create
additional files here!)

Edit faqconf.py to reflect your setup.  You only need to edit the top
part, up till the line of all dashes.  The comments should guide you
in your edits.  (Actually, you can also choose to add your changes to
faqcust.py and leave faqconf.py alone.  This is essential if you are
maintaining multiple FAQs; see below.)

Don't forget to edit the SECTION_TITLES variables to reflect the set
of section titles for your FAQ!

Next, edit faqw.py to reflect the pathname of your Python interpreter
and the values for SRCDIR and FAQDIR that you just chose.  Then
install faqw.py in your cgi-bin directory.  Make sure that it is
world-executable.  You should now be able to connect to the FAQ wizard
by entering the following URL in your web client (subsituting the
appropriate host and port for "your.web.server", and perhaps
specifying a different directory for "cgi-bin" if local conventions so
dictate):

	http://your.web.server/cgi-bin/faqw.py

If you are unable to get this working, check your server's error_log
file.  The documentation for Python's cgi module in the Python Library
Reference Manual gives plentyu additional information about installing
and debugging CGI scripts, including setup debugging.  This
documentation is repeated in the doc string in the cgi module; try
``import cgi; print cgi.__doc__''.

Assuming this works, you should now be able to add the first entry to
your FAQ using the FAQ wizard interface.  This creates a file
faq01.001.htp in your working directory and an RCS revision history
file faq01.001.htp,v in the RCS subdirectory.  You can now exercise
the other FAQ wizard features (search, index, whole FAQ, what's new,
roulette, and so on).


Maintaining Multiple FAQs
-------------------------

If you have multiple FAQs, you need a separate FAQDIR per FAQ, and a
different customization file per FAQ.  The easiest thing to do would
be to have the faqcust.py for each FAQ live in the FAQDIR for that
FAQ, but that creates some security concerns, since the FAQDIR must be
world writable: *if* someone who breaks into your system (or a
legitimate user) manages to edit the faqcust.py file they can get
arbitrary code to execute through the FAQ wizard.  Therefore, you will
need a more complex setup.

The best way is probably to have a directory that is only writable by
you for each FAQ, where you place the copy of faqcust.py for that FAQ,
and have a world-writable subdirectory DATA for the data.  You then
set FAQDIR to point to the DATA directory and change the faqw.py
bootstrap script to add FAQDIR/.. to sys.path (in front of SRCDIR, so
the dummy faqcust.py from SRCDIR is ignored).

--Guido van Rossum (home page: http://www.python.org/~guido/)

framer is a tool to generate boilerplate code for C extension types.

The boilerplate is generated from a specification object written in
Python.  The specification uses the class statement to describe the
extension module and any extension types it contains.  From the
specification, framer can generate all the boilerplate C code,
including function definitions, argument handling code, and type
objects.

THE FREEZE SCRIPT
=================

(Directions for Windows are at the end of this file.)


What is Freeze?
---------------

Freeze make it possible to ship arbitrary Python programs to people
who don't have Python.  The shipped file (called a "frozen" version of
your Python program) is an executable, so this only works if your
platform is compatible with that on the receiving end (this is usually
a matter of having the same major operating system revision and CPU
type).

The shipped file contains a Python interpreter and large portions of
the Python run-time.  Some measures have been taken to avoid linking
unneeded modules, but the resulting binary is usually not small.

The Python source code of your program (and of the library modules
written in Python that it uses) is not included in the binary --
instead, the compiled byte-code (the instruction stream used
internally by the interpreter) is incorporated.  This gives some
protection of your Python source code, though not much -- a
disassembler for Python byte-code is available in the standard Python
library.  At least someone running "strings" on your binary won't see
the source.


How does Freeze know which modules to include?
----------------------------------------------

Previous versions of Freeze used a pretty simple-minded algorithm to
find the modules that your program uses, essentially searching for
lines starting with the word "import".  It was pretty easy to trick it
into making mistakes, either missing valid import statements, or
mistaking string literals (e.g. doc strings) for import statements.

This has been remedied: Freeze now uses the regular Python parser to
parse the program (and all its modules) and scans the generated byte
code for IMPORT instructions.  It may still be confused -- it will not
know about calls to the __import__ built-in function, or about import
statements constructed on the fly and executed using the 'exec'
statement, and it will consider import statements even when they are
unreachable (e.g. "if 0: import foobar").

This new version of Freeze also knows about Python's new package
import mechanism, and uses exactly the same rules to find imported
modules and packages.  One exception: if you write 'from package
import *', Python will look into the __all__ variable of the package
to determine which modules are to be imported, while Freeze will do a
directory listing.

One tricky issue: Freeze assumes that the Python interpreter and
environment you're using to run Freeze is the same one that would be
used to run your program, which should also be the same whose sources
and installed files you will learn about in the next section.  In
particular, your PYTHONPATH setting should be the same as for running
your program locally.  (Tip: if the program doesn't run when you type
"python hello.py" there's little chance of getting the frozen version
to run.)


How do I use Freeze?
--------------------

Normally, you should be able to use it as follows:

	python freeze.py hello.py

where hello.py is your program and freeze.py is the main file of
Freeze (in actuality, you'll probably specify an absolute pathname
such as /usr/joe/python/Tools/freeze/freeze.py).


What do I do next?
------------------

Freeze creates a number of files: frozen.c, config.c and Makefile,
plus one file for each Python module that gets included named
M_<module>.c.  To produce the frozen version of your program, you can
simply type "make".  This should produce a binary file.  If the
filename argument to Freeze was "hello.py", the binary will be called
"hello".

Note: you can use the -o option to freeze to specify an alternative
directory where these files are created. This makes it easier to
clean up after you've shipped the frozen binary.  You should invoke
"make" in the given directory.


Freezing Tkinter programs
-------------------------

Unfortunately, it is currently not possible to freeze programs that
use Tkinter without a Tcl/Tk installation. The best way to ship a
frozen Tkinter program is to decide in advance where you are going
to place the Tcl and Tk library files in the distributed setup, and
then declare these directories in your frozen Python program using
the TCL_LIBRARY, TK_LIBRARY and TIX_LIBRARY environment variables.

For example, assume you will ship your frozen program in the directory 
<root>/bin/windows-x86 and will place your Tcl library files 
in <root>/lib/tcl8.2 and your Tk library files in <root>/lib/tk8.2. Then
placing the following lines in your frozen Python script before importing
Tkinter or Tix would set the environment correctly for Tcl/Tk/Tix:

import os
import os.path
RootDir = os.path.dirname(os.path.dirname(os.getcwd()))

import sys
if sys.platform == "win32":
   sys.path = ['', '..\\..\\lib\\python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '\\lib\\tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '\\lib\\tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '\\lib\\tix8.1'
elif sys.platform == "linux2":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'
elif sys.platform == "solaris":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'

This also adds <root>/lib/python-2.0 to your Python path
for any Python files such as _tkinter.pyd you may need.

Note that the dynamic libraries (such as tcl82.dll tk82.dll python20.dll
under Windows, or libtcl8.2.so and libtcl8.2.so under Unix) are required
at program load time, and are searched by the operating system loader
before Python can be started. Under Windows, the environment
variable PATH is consulted, and under Unix, it may be the
environment variable LD_LIBRARY_PATH and/or the system
shared library cache (ld.so). An additional preferred directory for
finding the dynamic libraries is built into the .dll or .so files at
compile time - see the LIB_RUNTIME_DIR variable in the Tcl makefile. 
The OS must find the dynamic libraries or your frozen program won't start. 
Usually I make sure that the .so or .dll files are in the same directory
as the executable, but this may not be foolproof.

A workaround to installing your Tcl library files with your frozen
executable would be possible, in which the Tcl/Tk library files are
incorporated in a frozen Python module as string literals and written
to a temporary location when the program runs; this is currently left
as an exercise for the reader.  An easier approach is to freeze the
Tcl/Tk/Tix code into the dynamic libraries using the Tcl ET code,
or the Tix Stand-Alone-Module code. Of course, you can also simply 
require that Tcl/Tk is required on the target installation, but be 
careful that the version corresponds.

There are some caveats using frozen Tkinter applications:
	Under Windows if you use the -s windows option, writing
to stdout or stderr is an error.
	The Tcl [info nameofexecutable] will be set to where the
program was frozen, not where it is run from.
	The global variables argc and argv do not exist.


A warning about shared library modules
--------------------------------------

When your Python installation uses shared library modules such as 
_tkinter.pyd, these will not be incorporated in the frozen program.
 Again, the frozen program will work when you test it, but it won't
 work when you ship it to a site without a Python installation.

Freeze prints a warning when this is the case at the end of the
freezing process:

	Warning: unknown modules remain: ...

When this occurs, the best thing to do is usually to rebuild Python
using static linking only. Or use the approach described in the previous
section to declare a library path using sys.path, and place the modules
such as _tkinter.pyd there.


Troubleshooting
---------------

If you have trouble using Freeze for a large program, it's probably
best to start playing with a really simple program first (like the file
hello.py).  If you can't get that to work there's something
fundamentally wrong -- perhaps you haven't installed Python.  To do a
proper install, you should do "make install" in the Python root
directory.


Usage under Windows 95 or NT
----------------------------

Under Windows 95 or NT, you *must* use the -p option and point it to
the top of the Python source tree.

WARNING: the resulting executable is not self-contained; it requires
the Python DLL, currently PYTHON20.DLL (it does not require the
standard library of .py files though).  It may also require one or
more extension modules loaded from .DLL or .PYD files; the module
names are printed in the warning message about remaining unknown
modules.

The driver script generates a Makefile that works with the Microsoft
command line C compiler (CL).  To compile, run "nmake"; this will
build a target "hello.exe" if the source was "hello.py".  Only the
files frozenmain.c and frozen.c are used; no config.c is generated or
used, since the standard DLL is used.

In order for this to work, you must have built Python using the VC++
(Developer Studio) 5.0 compiler.  The provided project builds
python20.lib in the subdirectory pcbuild\Release of thje Python source
tree, and this is where the generated Makefile expects it to be.  If
this is not the case, you can edit the Makefile or (probably better)
winmakemakefile.py (e.g., if you are using the 4.2 compiler, the
python20.lib file is generated in the subdirectory vc40 of the Python
source tree).

It is possible to create frozen programs that don't have a console
window, by specifying the option '-s windows'. See the Usage below.

Usage
-----

Here is a list of all of the options (taken from freeze.__doc__):

usage: freeze [options...] script [module]...

Options:
-p prefix:    This is the prefix used when you ran ``make install''
              in the Python build directory.
              (If you never ran this, freeze won't work.)
              The default is whatever sys.prefix evaluates to.
              It can also be the top directory of the Python source
              tree; then -P must point to the build tree.

-P exec_prefix: Like -p but this is the 'exec_prefix', used to
                install objects etc.  The default is whatever sys.exec_prefix
                evaluates to, or the -p argument if given.
                If -p points to the Python source tree, -P must point
                to the build tree, if different.

-e extension: A directory containing additional .o files that
              may be used to resolve modules.  This directory
              should also have a Setup file describing the .o files.
              On Windows, the name of a .INI file describing one
              or more extensions is passed.
              More than one -e option may be given.

-o dir:       Directory where the output files are created; default '.'.

-m:           Additional arguments are module names instead of filenames.

-a package=dir: Additional directories to be added to the package's
                __path__.  Used to simulate directories added by the
                package at runtime (eg, by OpenGL and win32com).
                More than one -a option may be given for each package.

-l file:      Pass the file to the linker (windows only)

-d:           Debugging mode for the module finder.

-q:           Make the module finder totally quiet.

-h:           Print this help message.

-x module     Exclude the specified module.

-i filename:  Include a file with additional command line options.  Used
              to prevent command lines growing beyond the capabilities of
              the shell/OS.  All arguments specified in filename
              are read and the -i option replaced with the parsed
              params (note - quoting args in this file is NOT supported)

-s subsystem: Specify the subsystem (For Windows only.); 
              'console' (default), 'windows', 'service' or 'com_dll'
              
-w:           Toggle Windows (NT or 95) behavior.
              (For debugging only -- on a win32 platform, win32 behavior
              is automatic.)

Arguments:

script:       The Python script to be executed by the resulting binary.

module ...:   Additional Python modules (referenced by pathname)
              that will be included in the resulting binary.  These
              may be .py or .pyc files.  If -m is specified, these are
              module names that are search in the path instead.



--Guido van Rossum (home page: http://www.python.org/~guido/)

Packaging Python as a Microsoft Installer Package (MSI)
=======================================================

Using this library, Python can be packaged as a MS-Windows
MSI file. To generate an installer package, you need
a build tree. By default, the build tree root directory
is assumed to be in "../..". This location can be changed
by adding a file config.py; see the beginning of msi.py
for additional customization options.

The packaging process assumes that binaries have been 
generated according to the instructions in PCBuild/README.txt,
and that you have either Visual Studio or the Platform SDK
installed. In addition, you need the Python COM extensions,
either from PythonWin, or from ActivePython.

To invoke the script, open a cmd.exe window which has 
cabarc.exe in its PATH (e.g. "Visual Studio .NET 2003
Command Prompt"). Then invoke

<path-to-python.exe> msi.py

If everything succeeds, pythonX.Y.Z.msi is generated
in the current directory.


________________________________________________________________________

PYBENCH - A Python Benchmark Suite
________________________________________________________________________

     Extendable suite of low-level benchmarks for measuring
          the performance of the Python implementation 
                 (interpreter, compiler or VM).

pybench is a collection of tests that provides a standardized way to
measure the performance of Python implementations. It takes a very
close look at different aspects of Python programs and let's you
decide which factors are more important to you than others, rather
than wrapping everything up in one number, like the other performance
tests do (e.g. pystone which is included in the Python Standard
Library).

pybench has been used in the past by several Python developers to
track down performance bottlenecks or to demonstrate the impact of
optimizations and new features in Python.

The command line interface for pybench is the file pybench.py. Run
this script with option '--help' to get a listing of the possible
options. Without options, pybench will simply execute the benchmark
and then print out a report to stdout.


Micro-Manual
------------

Run 'pybench.py -h' to see the help screen.  Run 'pybench.py' to run
the benchmark suite using default settings and 'pybench.py -f <file>'
to have it store the results in a file too.

It is usually a good idea to run pybench.py multiple times to see
whether the environment, timers and benchmark run-times are suitable
for doing benchmark tests. 

You can use the comparison feature of pybench.py ('pybench.py -c
<file>') to check how well the system behaves in comparison to a
reference run. 

If the differences are well below 10% for each test, then you have a
system that is good for doing benchmark testings.  Of you get random
differences of more than 10% or significant differences between the
values for minimum and average time, then you likely have some
background processes running which cause the readings to become
inconsistent. Examples include: web-browsers, email clients, RSS
readers, music players, backup programs, etc.

If you are only interested in a few tests of the whole suite, you can
use the filtering option, e.g. 'pybench.py -t string' will only
run/show the tests that have 'string' in their name.

This is the current output of pybench.py --help:

"""
------------------------------------------------------------------------
PYBENCH - a benchmark test suite for Python interpreters/compilers.
------------------------------------------------------------------------

Synopsis:
 pybench.py [option] files...

Options and default settings:
  -n arg           number of rounds (10)
  -f arg           save benchmark to file arg ()
  -c arg           compare benchmark with the one in file arg ()
  -s arg           show benchmark in file arg, then exit ()
  -w arg           set warp factor to arg (10)
  -t arg           run only tests with names matching arg ()
  -C arg           set the number of calibration runs to arg (20)
  -d               hide noise in comparisons (0)
  -v               verbose output (not recommended) (0)
  --with-gc        enable garbage collection (0)
  --with-syscheck  use default sys check interval (0)
  --timer arg      use given timer (time.time)
  -h               show this help text
  --help           show this help text
  --debug          enable debugging
  --copyright      show copyright
  --examples       show examples of usage

Version:
 2.0

The normal operation is to run the suite and display the
results. Use -f to save them for later reuse or comparisons.

Available timers:

   time.time
   time.clock
   systimes.processtime

Examples:

python2.1 pybench.py -f p21.pybench
python2.5 pybench.py -f p25.pybench
python pybench.py -s p25.pybench -c p21.pybench
"""

License
-------

See LICENSE file.


Sample output
-------------

"""
-------------------------------------------------------------------------------
PYBENCH 2.0
-------------------------------------------------------------------------------
* using Python 2.4.2
* disabled garbage collection
* system check interval set to maximum: 2147483647
* using timer: time.time

Calibrating tests. Please wait...

Running 10 round(s) of the suite at warp factor 10:

* Round 1 done in 6.388 seconds.
* Round 2 done in 6.485 seconds.
* Round 3 done in 6.786 seconds.
...
* Round 10 done in 6.546 seconds.

-------------------------------------------------------------------------------
Benchmark: 2006-06-12 12:09:25
-------------------------------------------------------------------------------

    Rounds: 10
    Warp:   10
    Timer:  time.time

    Machine Details:
       Platform ID:  Linux-2.6.8-24.19-default-x86_64-with-SuSE-9.2-x86-64
       Processor:    x86_64

    Python:
       Executable:   /usr/local/bin/python
       Version:      2.4.2
       Compiler:     GCC 3.3.4 (pre 3.3.5 20040809)
       Bits:         64bit
       Build:        Oct  1 2005 15:24:35 (#1)
       Unicode:      UCS2


Test                             minimum  average  operation  overhead
-------------------------------------------------------------------------------
          BuiltinFunctionCalls:    126ms    145ms    0.28us    0.274ms
           BuiltinMethodLookup:    124ms    130ms    0.12us    0.316ms
                 CompareFloats:    109ms    110ms    0.09us    0.361ms
         CompareFloatsIntegers:    100ms    104ms    0.12us    0.271ms
               CompareIntegers:    137ms    138ms    0.08us    0.542ms
        CompareInternedStrings:    124ms    127ms    0.08us    1.367ms
                  CompareLongs:    100ms    104ms    0.10us    0.316ms
                CompareStrings:    111ms    115ms    0.12us    0.929ms
                CompareUnicode:    108ms    128ms    0.17us    0.693ms
                 ConcatStrings:    142ms    155ms    0.31us    0.562ms
                 ConcatUnicode:    119ms    127ms    0.42us    0.384ms
               CreateInstances:    123ms    128ms    1.14us    0.367ms
            CreateNewInstances:    121ms    126ms    1.49us    0.335ms
       CreateStringsWithConcat:    130ms    135ms    0.14us    0.916ms
       CreateUnicodeWithConcat:    130ms    135ms    0.34us    0.361ms
                  DictCreation:    108ms    109ms    0.27us    0.361ms
             DictWithFloatKeys:    149ms    153ms    0.17us    0.678ms
           DictWithIntegerKeys:    124ms    126ms    0.11us    0.915ms
            DictWithStringKeys:    114ms    117ms    0.10us    0.905ms
                      ForLoops:    110ms    111ms    4.46us    0.063ms
                    IfThenElse:    118ms    119ms    0.09us    0.685ms
                   ListSlicing:    116ms    120ms    8.59us    0.103ms
                NestedForLoops:    125ms    137ms    0.09us    0.019ms
          NormalClassAttribute:    124ms    136ms    0.11us    0.457ms
       NormalInstanceAttribute:    110ms    117ms    0.10us    0.454ms
           PythonFunctionCalls:    107ms    113ms    0.34us    0.271ms
             PythonMethodCalls:    140ms    149ms    0.66us    0.141ms
                     Recursion:    156ms    166ms    3.32us    0.452ms
                  SecondImport:    112ms    118ms    1.18us    0.180ms
           SecondPackageImport:    118ms    127ms    1.27us    0.180ms
         SecondSubmoduleImport:    140ms    151ms    1.51us    0.180ms
       SimpleComplexArithmetic:    128ms    139ms    0.16us    0.361ms
        SimpleDictManipulation:    134ms    136ms    0.11us    0.452ms
         SimpleFloatArithmetic:    110ms    113ms    0.09us    0.571ms
      SimpleIntFloatArithmetic:    106ms    111ms    0.08us    0.548ms
       SimpleIntegerArithmetic:    106ms    109ms    0.08us    0.544ms
        SimpleListManipulation:    103ms    113ms    0.10us    0.587ms
          SimpleLongArithmetic:    112ms    118ms    0.18us    0.271ms
                    SmallLists:    105ms    116ms    0.17us    0.366ms
                   SmallTuples:    108ms    128ms    0.24us    0.406ms
         SpecialClassAttribute:    119ms    136ms    0.11us    0.453ms
      SpecialInstanceAttribute:    143ms    155ms    0.13us    0.454ms
                StringMappings:    115ms    121ms    0.48us    0.405ms
              StringPredicates:    120ms    129ms    0.18us    2.064ms
                 StringSlicing:    111ms    127ms    0.23us    0.781ms
                     TryExcept:    125ms    126ms    0.06us    0.681ms
                TryRaiseExcept:    133ms    137ms    2.14us    0.361ms
                  TupleSlicing:    117ms    120ms    0.46us    0.066ms
               UnicodeMappings:    156ms    160ms    4.44us    0.429ms
             UnicodePredicates:    117ms    121ms    0.22us    2.487ms
             UnicodeProperties:    115ms    153ms    0.38us    2.070ms
                UnicodeSlicing:    126ms    129ms    0.26us    0.689ms
-------------------------------------------------------------------------------
Totals:                           6283ms   6673ms
"""
________________________________________________________________________

Writing New Tests
________________________________________________________________________

pybench tests are simple modules defining one or more pybench.Test
subclasses.

Writing a test essentially boils down to providing two methods:
.test() which runs .rounds number of .operations test operations each
and .calibrate() which does the same except that it doesn't actually
execute the operations.


Here's an example:
------------------

from pybench import Test

class IntegerCounting(Test):

    # Version number of the test as float (x.yy); this is important
    # for comparisons of benchmark runs - tests with unequal version
    # number will not get compared.
    version = 1.0
    
    # The number of abstract operations done in each round of the
    # test. An operation is the basic unit of what you want to
    # measure. The benchmark will output the amount of run-time per
    # operation. Note that in order to raise the measured timings
    # significantly above noise level, it is often required to repeat
    # sets of operations more than once per test round. The measured
    # overhead per test round should be less than 1 second.
    operations = 20

    # Number of rounds to execute per test run. This should be
    # adjusted to a figure that results in a test run-time of between
    # 1-2 seconds (at warp 1).
    rounds = 100000

    def test(self):

	""" Run the test.

	    The test needs to run self.rounds executing
	    self.operations number of operations each.

        """
        # Init the test
        a = 1

        # Run test rounds
	#
        # NOTE: Use xrange() for all test loops unless you want to face
	# a 20MB process !
	#
        for i in xrange(self.rounds):

            # Repeat the operations per round to raise the run-time
            # per operation significantly above the noise level of the
            # for-loop overhead. 

	    # Execute 20 operations (a += 1):
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1

    def calibrate(self):

	""" Calibrate the test.

	    This method should execute everything that is needed to
	    setup and run the test - except for the actual operations
	    that you intend to measure. pybench uses this method to
            measure the test implementation overhead.

        """
        # Init the test
        a = 1

        # Run test rounds (without actually doing any operation)
        for i in xrange(self.rounds):

	    # Skip the actual execution of the operations, since we
	    # only want to measure the test's administration overhead.
            pass

Registering a new test module
-----------------------------

To register a test module with pybench, the classes need to be
imported into the pybench.Setup module. pybench will then scan all the
symbols defined in that module for subclasses of pybench.Test and
automatically add them to the benchmark suite.


Breaking Comparability
----------------------

If a change is made to any individual test that means it is no
longer strictly comparable with previous runs, the '.version' class
variable should be updated. Therefafter, comparisons with previous
versions of the test will list as "n/a" to reflect the change.


Version History
---------------

  2.0: rewrote parts of pybench which resulted in more repeatable
       timings:
        - made timer a parameter
        - changed the platform default timer to use high-resolution
          timers rather than process timers (which have a much lower
          resolution)
        - added option to select timer
        - added process time timer (using systimes.py)
        - changed to use min() as timing estimator (average
          is still taken as well to provide an idea of the difference)
        - garbage collection is turned off per default
        - sys check interval is set to the highest possible value
        - calibration is now a separate step and done using
          a different strategy that allows measuring the test
          overhead more accurately
        - modified the tests to each give a run-time of between
          100-200ms using warp 10
        - changed default warp factor to 10 (from 20)
        - compared results with timeit.py and confirmed measurements
        - bumped all test versions to 2.0
        - updated platform.py to the latest version
        - changed the output format a bit to make it look
          nicer
        - refactored the APIs somewhat
  1.3+: Steve Holden added the NewInstances test and the filtering 
       option during the NeedForSpeed sprint; this also triggered a long 
       discussion on how to improve benchmark timing and finally
       resulted in the release of 2.0
  1.3: initial checkin into the Python SVN repository


Have fun,
--
Marc-Andre Lemburg
mal@lemburg.com

Pynche - The PYthonically Natural Color and Hue Editor

Contact: Barry A. Warsaw
Email:   bwarsaw@python.org
Version: 1.3

Introduction

    Pynche is a color editor based largely on a similar program that I
    originally wrote back in 1987 for the Sunview window system.  That
    editor was called ICE, the Interactive Color Editor.  I'd always
    wanted to port this program to X but didn't feel like hacking X
    and C code to do it.  Fast forward many years, to where Python +
    Tkinter provides such a nice programming environment, with enough
    power, that I finally buckled down and re-implemented it.  I
    changed the name because these days, too many other systems have
    the acronym `ICE'.

    Pynche should work with any variant of Python after 1.5.2
    (e.g. 2.0.1 and 2.1.1), using Tk 8.0.x.  It's been tested on
    Solaris 2.6, Windows NT 4, and various Linux distros.  You'll want
    to be sure to have at least Tk 8.0.3 for Windows.  Also, Pynche is
    very colormap intensive, so it doesn't work very well on 8-bit
    graphics cards; 24bit+ graphics cards are so cheap these days,
    I'll probably never "fix" that.

    Pynche must find a text database of colors names in order to
    provide `nearest' color matching.  Pynche is distributed with an
    rgb.txt file from the X11R6.4 distribution for this reason, along
    with other "Web related" database (see below).  You can use a
    different file with the -d option.  The file xlicense.txt contains
    the license only for rgb.txt and both files are in the X/
    subdirectory.

    Pynche is pronounced: Pin'-chee


Running Standalone

    On Unix, start it by running the `pynche' script.  On Windows, run
    pynche.pyw to inhibit the console window.  When run from the
    command line, the following options are recognized:

    --database file
    -d file
        Alternate location of the color database file.  Without this
        option, the first valid file found will be used (see below).

    --initfile file
    -i file
        Alternate location of the persistent initialization file.  See 
        the section on Persistency below.

    --ignore
    -X
        Ignore the persistent initialization file when starting up.
        Pynche will still write the current option settings to the
        persistent init file when it quits.

    --help
    -h
        Print the help message.

    initialcolor
        a Tk color name or #rrggbb color spec to be used as the
        initially selected color.  This overrides any color saved in
        the persistent init file.  Since `#' needs to be escaped in
        many shells, it is optional in the spec (e.g. #45dd1f is the
        same as 45dd1f).


Running as a Modal Dialog

    Pynche can be run as a modal dialog, inside another application,
    say as a general color chooser.  In fact, Grail 0.6 uses Pynche
    and a future version of IDLE may as well.  Pynche supports the API
    implemented by the Tkinter standard tkColorChooser module, with a
    few changes as described below.  By importing pyColorChooser from
    the Pynche package, you can run

        pyColorChooser.askcolor()

    which will popup Pynche as a modal dialog, and return the selected 
    color.

    There are some UI differences when running as a modal
    vs. standalone.  When running as a modal, there is no "Quit" menu
    item under the "File" menu.  Instead there are "Okay" and "Cancel"
    buttons.

    When "Okay" is hit, askcolor() returns the tuple

        ((r, g, b), "name")

    where r, g, and b are red, green, and blue color values
    respectively (in the range 0 to 255).  "name" will be a color name
    from the color database if there is an exact match, otherwise it
    will be an X11 color spec of the form "#rrggbb".  Note that this
    is different than tkColorChooser, which doesn't know anything
    about color names.

    askcolor() supports the following optional keyword arguments:

        color
            the color to set as the initial selected color

        master[*]
            the master window to use as the parent of the modal
            dialog.  Without this argument, pyColorChooser will create 
            its own Tkinter.Tk instance as the master.  This may not
            be what you want.

        databasefile
            similar to the --database option, the value must be a
            file name

        initfile[*]
            similar to the --initfile option, the value must be a
            file name

        ignore[*]
            similar to the --ignore flag, the value is a boolean

        wantspec
            When this is true, the "name" field in the return tuple
            will always be a color spec of the form "#rrggbb".  It
            will not return a color name even if there is a match;
            this is so pyColorChooser can exactly match the API of
            tkColorChooser.

        [*] these arguments must be specified the first time
        askcolor() is used and cannot be changed on subsequent calls.


The Colorstrip Window

    The top part of the main Pynche window contains the "variation
    strips".  Each strip contains a number of "color chips".  The
    strips always indicate the currently selected color by a highlight
    rectangle around the selected color chip, with an arrow pointing
    to the chip.  Each arrow has an associated number giving you the
    color value along the variation's axis.  Each variation strip
    shows you the colors that are reachable from the selected color by
    varying just one axis of the color solid.

    For example, when the selected color is (in Red/Green/Blue
    notation) 127/127/127, the Red Variations strip shows you every
    color in the range 0/127/127 to 255/127/127.  Similarly for the
    green and blue axes.  You can select any color by clicking on its
    chip.  This will update the highlight rectangle and the arrow, as
    well as other displays in Pynche.

    Click on "Update while dragging" if you want Pynche to update the
    selected color while you drag along any variation strip (this will
    be a bit slower).  Click on "Hexadecimal" to display the arrow
    numbers in hex.

    There are also two shortcut buttons in this window, which
    auto-select Black (0/0/0) and White (255/255/255).


The Proof Window

    In the lower left corner of the main window you see two larger
    color chips.  The Selected chip shows you a larger version of the
    color selected in the variation strips, along with its X11 color
    specification.  The Nearest chip shows you the closest color in
    the X11 database to the selected color, giving its X11 color
    specification, and below that, its X11 color name.  When the
    Selected chip color exactly matches the Nearest chip color, you
    will see the color name appear below the color specification for
    the Selected chip.
    
    Clicking on the Nearest color chip selects that color.  Color
    distance is calculated in the 3D space of the RGB color solid and
    if more than one color name is the same distance from the selected
    color, the first one found will be chosen.

    Note that there may be more than one X11 color name for the same
    RGB value.  In that case, the first one found in the text database
    is designated the "primary" name, and this is shown under the
    Nearest chip.  The other names are "aliases" and they are visible
    in the Color List Window (see below).

    Both the color specifications and color names are selectable for
    copying and pasting into another window.


The Type-in Window

    At the lower right of the main window are three entry fields.
    Here you can type numeric values for any of the three color axes.
    Legal values are between 0 and 255, and these fields do not allow
    you to enter illegal values.  You must hit Enter or Tab to select
    the new color.

    Click on "Update while typing" if you want Pynche to select the
    color on every keystroke (well, every one that produces a legal
    value!)  Click on "Hexadecimal" to display and enter color values
    in hex.


Other Views

    There are three secondary windows which are not displayed by
    default.  You can bring these up via the "View" menu on the main
    Pynche window.


The Text Window

    The "Text Window" allows you to see what effects various colors
    have on the standard Tk text widget elements.  In the upper part
    of the window is a plain Tk text widget and here you can edit the
    text, select a region of text, etc.  Below this is a button "Track
    color changes".  When this is turned on, any colors selected in
    the other windows will change the text widget element specified in
    the radio buttons below.  When this is turned off, text widget
    elements are not affected by color selection.

    You can choose which element gets changed by color selection by
    clicking on one of the radio buttons in the bottom part of this
    window.  Text foreground and background affect the text in the
    upper part of the window.  Selection foreground and background
    affect the colors of the primary selection which is what you see
    when you click the middle button (depending on window system) and
    drag it through some text.

    The Insertion is the insertion cursor in the text window, where
    new text will be inserted as you type.  The insertion cursor only
    has a background.


The Color List Window

    The "Color List" window shows every named color in the color name
    database (this window may take a while to come up).  In the upper
    part of the window you see a scrolling list of all the color names
    in the database, in alphabetical order.  Click on any color to
    select it.  In the bottom part of the window is displayed any
    aliases for the selected color (those color names that have the
    same RGB value, but were found later in the text database).  For
    example, find the color "Black" and you'll see that its aliases
    are "gray0" and "grey0".

    If the color has no aliases you'll see "<no aliases>" here.  If you
    just want to see if a color has an alias, and do not want to select a
    color when you click on it, turn off "Update on Click".

    Note that the color list is always updated when a color is selected
    from the main window.  There's no way to turn this feature off.  If
    the selected color has no matching color name you'll see
    "<no matching color>" in the Aliases window.


The Details Window

    The "Details" window gives you more control over color selection
    than just clicking on a color chip in the main window.  The row of
    buttons along the top apply the specified increment and decrement
    amounts to the selected color.  These delta amounts are applied to
    the variation strips specified by the check boxes labeled "Move
    Sliders".  Thus if just Red and Green are selected, hitting -10
    will subtract 10 from the color value along the red and green
    variation only.  Note the message under the checkboxes; this
    indicates the primary color level being changed when more than one
    slider is tied together.  For example, if Red and Green are
    selected, you will be changing the Yellow level of the selected
    color.

    The "At Boundary" behavior determines what happens when any color
    variation hits either the lower or upper boundaries (0 or 255) as
    a result of clicking on the top row buttons:

    Stop
        When the increment or decrement would send any of the tied
        variations out of bounds, the entire delta is discarded.

    Wrap Around
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds value is wrapped
        around to the other side.  Thus if red were at 238 and +25
        were clicked, red would have the value 7.

    Preserve Distance
        When the increment or decrement would send any of the tied
        variations out of bounds, all tied variations are wrapped as
        one, so as to preserve the distance between them.  Thus if
        green and blue were tied, and green was at 238 while blue was
        at 223, and +25 were clicked, green would be at 15 and blue
        would be at 0.

    Squash
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds variation is set
        to the ceiling of 255 or floor of 0, as appropriate.  In this
        way, all tied variations are squashed to one edge or the
        other.

    The top row buttons have the following keyboard accelerators:

    -25 == Shift Left Arrow
    -10 == Control Left Arrow
     -1 == Left Arrow
     +1 == Right Arrow
    +10 == Control Right Arrow
    +25 == Shift Right Arrow


Keyboard Accelerators

    Alt-w in any secondary window dismisses the window.  In the main
    window it exits Pynche (except when running as a modal).

    Alt-q in any window exits Pynche (except when running as a modal).


Persistency

    Pynche remembers various settings of options and colors between
    invocations, storing these values in a `persistent initialization
    file'.  The actual location of this file is specified by the
    --initfile option (see above), and defaults to ~/.pynche.

    When Pynche exits, it saves these values in the init file, and
    re-reads them when it starts up.  There is no locking on this
    file, so if you run multiple instances of Pynche at a time, you
    may clobber the init file.

    The actual options stored include

    - the currently selected color

    - all settings of checkbox and radio button options in all windows

    - the contents of the text window, the current text selection and
      insertion point, and all current text widget element color
      settings.

    - the name of the color database file (but not its contents)

    You can inhibit Pynche from reading the init file by supplying the
    --ignore option on the command line.  However, you cannot suppress
    the storing of the settings in the init file on Pynche exit.  If
    you really want to do this, use /dev/null as the init file, using
    --initfile.


Color Name Database Files

    Pynche uses a color name database file to calculate the nearest
    color to the selected color, and to display in the Color List
    view.  Several files are distributed with Pynche, described
    below.  By default, the X11 color name database file is selected.
    Other files:

    html40colors.txt -- the HTML 4.0 guaranteed color names

    websafe.txt -- the 216 "Web-safe" colors that Netscape and MSIE
    guarantee will not be dithered.  These are specified in #rrggbb
    format for both values and names

    webcolors.txt -- The 140 color names that Tim Peters and his
    sister say NS and MSIE both understand (with some controversy over 
    AliceBlue).

    namedcolors.txt -- an alternative set of Netscape colors.

    You can switch between files by choosing "Load palette..." from
    the "File" menu.  This brings up a standard Tk file dialog.
    Choose the file you want and then click "Ok".  If Pynche
    understands the format in this file, it will load the database and 
    update the appropriate windows.  If not, it will bring up an error 
    dialog.


To Do

    Here's a brief list of things I want to do (some mythical day):

    - Better support for resizing the top level windows

    - More output views, e.g. color solids

    - Have the notion of a `last color selected'; this may require a
      new output view

    - Support setting the font in the text view

    - Support distutils setup.py for installation

    I'm open to suggestions!



Local Variables:
indent-tabs-mode: nil
End:

This directory contains a number of Python programs that are useful
while building or extending Python.

audiopy		Audiopy is a program to control the Solaris audio
		device, allowing you to choose both the input and
		output devices, and to set the output volume, that can
		be run either as a command-line script, or as a
		Tkinter application.

bgen		Generate complete extension modules from a
		description.  Still under development!
		WARNING: bgen has been removed in 3.0.

compiler	Tools used to maintain the compiler package in the
		standard library.

faqwiz		FAQ Wizard.
		See http://www.python.org/cgi-bin/faqw.py
		for a live example.

freeze		Create a stand-alone executable from a Python program.

gdb             Python code to be run inside gdb, to make it easier to
                debug Python itself (by David Malcolm).

i18n		Tools for internationalization. pygettext.py 
		parses Python source code and generates .pot files,
		and msgfmt.py generates a binary message catalog 
		from a catalog in text format.

pynche		A Tkinter-based color editor.

scripts         A number of useful single-file programs, e.g. tabnanny.py
                by Tim Peters, which checks for inconsistent mixing of
                tabs and spaces, and 2to3, which converts Python 2 code
                to Python 3 code.

unicode		Tools used to generate unicode database files for
		Python 2.0 (by Fredrik Lundh).

versioncheck	A tool to automate checking whether you have the latest
		version of a package (by Jack Jansen).

webchecker	A link checker for web sites.

world		Script to take a list of Internet addresses and print
		out where in the world those addresses originate from,
		based on the top-level domain country code found in
		the address. 

This directory contains a collection of executable Python scripts that
are useful while building, extending or managing Python.  Some (e.g.,
dutree or lll) are also generally useful UNIX tools.

See also the Demo/scripts directory!

analyze_dxp.py		Analyzes the result of sys.getdxp()
byext.py		Print lines/words/chars stats of files by extension
byteyears.py		Print product of a file's size and age
checkappend.py		Search for multi-argument .append() calls
checkpyc.py		Check presence and validity of ".pyc" files
classfix.py		Convert old class syntax to new
cleanfuture.py		Fix reduntant Python __future__ statements
combinerefs.py		A helper for analyzing PYTHONDUMPREFS output.
copytime.py		Copy one file's atime and mtime to another
crlf.py			Change CRLF line endings to LF (Windows to Unix)
cvsfiles.py		Print a list of files that are under CVS
db2pickle.py		Dump a database file to a pickle
diff.py			Print file diffs in context, unified, or ndiff formats
dutree.py		Format du(1) output as a tree sorted by size
eptags.py		Create Emacs TAGS file for Python modules
find_recursionlimit.py  Find the maximum recursion limit on this machine 
finddiv.py		A grep-like tool that looks for division operators
findlinksto.py		Recursively find symbolic links to a given path prefix
findnocoding.py		Find source files which need an encoding declaration
fixcid.py		Massive identifier substitution on C source files
fixdiv.py		Tool to fix division operators.
fixheader.py		Add some cpp magic to a C include file
fixnotice.py		Fix the copyright notice in source files
fixps.py		Fix Python scripts' first line (if #!)
ftpmirror.py		FTP mirror script
google.py		Open a webbrowser with Google
gprof2html.py		Transform gprof(1) output into useful HTML
h2py.py			Translate #define's into Python assignments
hotshotmain.py		Main program to run script under control of hotshot
idle			Main program to start IDLE
ifdef.py		Remove #if(n)def groups from C sources
lfcr.py			Change LF line endings to CRLF (Unix to Windows)
linktree.py		Make a copy of a tree with links to original files
lll.py			Find and list symbolic links in current directory
logmerge.py		Consolidate CVS/RCS logs read from stdin
mailerdaemon.py		parse error messages from mailer daemons (Sjoerd&Jack)
md5sum.py		Print MD5 checksums of argument files.
methfix.py		Fix old method syntax def f(self, (a1, ..., aN)):
mkreal.py		Turn a symbolic link into a real file or directory
ndiff.py		Intelligent diff between text files (Tim Peters)
nm2def.py		Create a template for PC/python_nt.def (Marc Lemburg)
objgraph.py		Print object graph from nm output on a library
parseentities.py	Utility for parsing HTML entity definitions
pathfix.py		Change #!/usr/local/bin/python into something else
pdeps.py		Print dependencies between Python modules
pickle2db.py		Load a pickle generated by db2pickle.py to a database
pindent.py		Indent Python code, giving block-closing comments
ptags.py		Create vi tags file for Python modules
pydoc			Python documentation browser.
pysource.py		Find Python source files
redemo.py		Basic regular expression demonstration facility
reindent.py		Change .py files to use 4-space indents.
rgrep.py		Reverse grep through a file (useful for big logfiles)
serve.py		Small wsgiref-based web server, used in make serve in Doc
setup.py		Install all scripts listed here
suff.py			Sort a list of files by suffix
svneol.py		Sets svn:eol-style on all files in directory
texcheck.py             Validate Python LaTeX formatting (Raymond Hettinger)
texi2html.py		Convert GNU texinfo files into HTML
treesync.py		Synchronize source trees (very ideosyncratic)
untabify.py		Replace tabs with spaces in argument files
which.py		Find a program in $PATH
xxci.py			Wrapper for rcsdiff and ci

This is versioncheck 1.0, a first stab at automatic checking of versions of
Python extension packages installed on your system.

The basic idea is that each package contains a _checkversion.py
somewhere, probably at the root level of the package. In addition, each
package maintainer makes a file available on the net, through ftp or
http, which contains the version number of the most recent distribution
and some readable text explaining the differences with previous
versions, where to download the package, etc.

The checkversions.py script walks through the installed Python tree (or
through a tree of choice), and runs each _checkversion.py script. These
scripts retrieve the current-version file over the net, compares version
numbers and tells the user about new versions of packages available.

A boilerplate for the _checkversion.py file can be found here. Replace
package name, version and the URL of the version-check file and put it in
your distribution. In stead of a single URL you can also specify a list
of URLs. Each of these will be checked in order until one is available,
this is handy for distributions that live in multiple places. Put the
primary distribution site (the most up-to-date site) before others.
The script is executed with execfile(), not imported, and the current
directory is the checkversion directory, so be careful with globals,
importing, etc.

The version-check file consists of an rfc822-style header followed by
plaintext. The only header field checked currently is
'Current-Version:', which should contain te current version and is
matched against the string contained in the _checkversion.py script.
The rest of the file is human-readable text and presented to the user if
there is a version mismatch. It should contain at the very least a URL
of either the current distribution or a webpage describing it.

Pycheckversion.py is the module that does the actual checking of versions.
It should be fine where it is, it is imported by checkversion before anything
else is done, but if imports fail you may want to move it to somewhere
along sys.path.

	Jack Jansen, CWI, 23-Dec-97.
	<jack@cwi.nl>
	

Webchecker
----------

This is a simple web tree checker, useful to find bad links in a web
tree.  It currently checks links pointing within the same subweb for
validity.  The main program is "webchecker.py".  See its doc string
(or invoke it with the option "-?") for more defails.

History:

- Jan 1997.  First release.  The module robotparser.py was written by
Skip Montanaro; the rest is original work by Guido van Rossum.

- May 1999.  Sam Bayer contributed a new version, wcnew.py, which
supports checking internal links (#spam fragments in URLs) and some
other options.

- Nov 1999.  Sam Bayer contributed patches to reintegrate wcnew.py
into webchecker.py, and corresponding mods to wcgui.py and
websucker.py.

- Mar 2004.  Chris Herborth contributed a patch to let webchecker.py
handle XHTML's 'id' attribute.

world -- Print mappings between country names and DNS country codes.

Contact: Barry Warsaw
Email:   bwarsaw@python.org

This script will take a list of Internet addresses and print out where in the
world those addresses originate from, based on the top-level domain country
code found in the address.  Addresses can be in any of the following forms:

    xx                -- just the country code or top-level domain identifier
    host.domain.xx    -- any Internet host or network name
    somebody@where.xx -- an Internet email address

If no match is found, the address is interpreted as a regular expression [*]
and a reverse lookup is attempted.  This script will search the country names
and print a list of matching entries.  You can force reverse mappings with the
`-r' flag (see below).

For example:

    %% world tz us
    tz originated from Tanzania, United Republic of
    us originated from United States

    %% world united
    united matches 6 countries:
        ae: United Arab Emirates
        uk: United Kingdom (common practice)
        um: United States Minor Outlying Islands
        us: United States
        tz: Tanzania, United Republic of
        gb: United Kingdom


 [*] Note that regular expressions must conform to Python 1.5's re.py module
 syntax.  The comparison is done with the search() method.

Country codes are maintained by the RIPE Network Coordination Centre,
in coordination with the ISO 3166 Maintenance Agency at DIN Berlin.  The
authoritative source of counry code mappings is:

    <url:ftp://info.ripe.net/iso3166-countrycodes>

The latest known change to this information was:

    Thu Aug  7 17:59:51 MET DST 1997

This script also knows about non-geographic top-level domains.

Usage: world [-d] [-p file] [-o] [-h] addr [addr ...]

    --dump
    -d
        Print mapping of all top-level domains.

    --parse file
    -p file
        Parse an iso3166-countrycodes file extracting the two letter country
        code followed by the country name.  Note that the three letter country
        codes and numbers, which are also provided in the standard format
        file, are ignored.

    --outputdict
    -o
        When used in conjunction with the `-p' option, output is in the form
        of a Python dictionary, and country names are normalized
        w.r.t. capitalization.  This makes it appropriate for cutting and
        pasting back into this file.

    --reverse
    -r
        Force reverse lookup.  In this mode the address can be any Python
        regular expression; this is matched against all country names and a
        list of matching mappings is printed.  In normal mode (e.g. without
        this flag), reverse lookup is performed on addresses if no matching
        country code is found.

    -h
    --help
        Print this message.


Local Variables:
indent-tabs-mode: nil
End:


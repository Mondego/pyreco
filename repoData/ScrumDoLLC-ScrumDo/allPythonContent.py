__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Pinax documentation build configuration file
#
# This file is execfile()d with the current directory set to its containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.

import sys, os

# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
#sys.path.append(os.path.abspath('some/directory'))

# General configuration
# ---------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.txt'

# The master toctree document.
master_doc = 'index'

# General substitutions.
project = 'Pinax'
copyright = '2010, James Tauber and Pinax Team'

# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = '0.7'
# The full version, including alpha/beta/rc tags.
release = '0.7.3'

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directories, that shouldn't be searched
# for source files recursively.
exclude_trees = [
    "external/repos",
]

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# Options for HTML output
# -----------------------

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
html_style = 'default.css'

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (within the static path) to place at the top of
# the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, the reST sources are included in the HTML build as _sources/<name>.
#html_copy_source = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'Pinaxdoc'


# Options for LaTeX output
# ------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class [howto/manual]).
latex_documents = [
  ('index', 'Pinax.tex', 'Pinax Documentation',
   'James Tauber and Pinax Team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

########NEW FILE########
__FILENAME__ = pinax-boot
#!/usr/bin/env python
## WARNING: This file is generated
#!/usr/bin/env python
"""Create a "virtual" Python installation
"""

virtualenv_version = "1.4.8"

import sys
import os
import optparse
import re
import shutil
import logging
import distutils.sysconfig
try:
    import subprocess
except ImportError, e:
    if sys.version_info <= (2, 3):
        print 'ERROR: %s' % e
        print 'ERROR: this script requires Python 2.4 or greater; or at least the subprocess module.'
        print 'If you copy subprocess.py from a newer version of Python this script will probably work'
        sys.exit(101)
    else:
        raise
try:
    set
except NameError:
    from sets import Set as set

join = os.path.join
py_version = 'python%s.%s' % (sys.version_info[0], sys.version_info[1])
is_jython = sys.platform.startswith('java')
expected_exe = is_jython and 'jython' or 'python'

REQUIRED_MODULES = ['os', 'posix', 'posixpath', 'nt', 'ntpath', 'genericpath',
                    'fnmatch', 'locale', 'encodings', 'codecs',
                    'stat', 'UserDict', 'readline', 'copy_reg', 'types',
                    're', 'sre', 'sre_parse', 'sre_constants', 'sre_compile',
                    'lib-dynload', 'config', 'zlib']

if sys.version_info[:2] >= (2, 6):
    REQUIRED_MODULES.extend(['warnings', 'linecache', '_abcoll', 'abc'])
if sys.version_info[:2] <= (2, 3):
    REQUIRED_MODULES.extend(['sets', '__future__'])

class Logger(object):

    """
    Logging object for use in command-line script.  Allows ranges of
    levels, to avoid some redundancy of displayed information.
    """

    DEBUG = logging.DEBUG
    INFO = logging.INFO
    NOTIFY = (logging.INFO+logging.WARN)/2
    WARN = WARNING = logging.WARN
    ERROR = logging.ERROR
    FATAL = logging.FATAL

    LEVELS = [DEBUG, INFO, NOTIFY, WARN, ERROR, FATAL]

    def __init__(self, consumers):
        self.consumers = consumers
        self.indent = 0
        self.in_progress = None
        self.in_progress_hanging = False

    def debug(self, msg, *args, **kw):
        self.log(self.DEBUG, msg, *args, **kw)
    def info(self, msg, *args, **kw):
        self.log(self.INFO, msg, *args, **kw)
    def notify(self, msg, *args, **kw):
        self.log(self.NOTIFY, msg, *args, **kw)
    def warn(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def error(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def fatal(self, msg, *args, **kw):
        self.log(self.FATAL, msg, *args, **kw)
    def log(self, level, msg, *args, **kw):
        if args:
            if kw:
                raise TypeError(
                    "You may give positional or keyword arguments, not both")
        args = args or kw
        rendered = None
        for consumer_level, consumer in self.consumers:
            if self.level_matches(level, consumer_level):
                if (self.in_progress_hanging
                    and consumer in (sys.stdout, sys.stderr)):
                    self.in_progress_hanging = False
                    sys.stdout.write('\n')
                    sys.stdout.flush()
                if rendered is None:
                    if args:
                        rendered = msg % args
                    else:
                        rendered = msg
                    rendered = ' '*self.indent + rendered
                if hasattr(consumer, 'write'):
                    consumer.write(rendered+'\n')
                else:
                    consumer(rendered)

    def start_progress(self, msg):
        assert not self.in_progress, (
            "Tried to start_progress(%r) while in_progress %r"
            % (msg, self.in_progress))
        if self.level_matches(self.NOTIFY, self._stdout_level()):
            sys.stdout.write(msg)
            sys.stdout.flush()
            self.in_progress_hanging = True
        else:
            self.in_progress_hanging = False
        self.in_progress = msg

    def end_progress(self, msg='done.'):
        assert self.in_progress, (
            "Tried to end_progress without start_progress")
        if self.stdout_level_matches(self.NOTIFY):
            if not self.in_progress_hanging:
                # Some message has been printed out since start_progress
                sys.stdout.write('...' + self.in_progress + msg + '\n')
                sys.stdout.flush()
            else:
                sys.stdout.write(msg + '\n')
                sys.stdout.flush()
        self.in_progress = None
        self.in_progress_hanging = False

    def show_progress(self):
        """If we are in a progress scope, and no log messages have been
        shown, write out another '.'"""
        if self.in_progress_hanging:
            sys.stdout.write('.')
            sys.stdout.flush()

    def stdout_level_matches(self, level):
        """Returns true if a message at this level will go to stdout"""
        return self.level_matches(level, self._stdout_level())

    def _stdout_level(self):
        """Returns the level that stdout runs at"""
        for level, consumer in self.consumers:
            if consumer is sys.stdout:
                return level
        return self.FATAL

    def level_matches(self, level, consumer_level):
        """
        >>> l = Logger()
        >>> l.level_matches(3, 4)
        False
        >>> l.level_matches(3, 2)
        True
        >>> l.level_matches(slice(None, 3), 3)
        False
        >>> l.level_matches(slice(None, 3), 2)
        True
        >>> l.level_matches(slice(1, 3), 1)
        True
        >>> l.level_matches(slice(2, 3), 1)
        False
        """
        if isinstance(level, slice):
            start, stop = level.start, level.stop
            if start is not None and start > consumer_level:
                return False
            if stop is not None or stop <= consumer_level:
                return False
            return True
        else:
            return level >= consumer_level

    #@classmethod
    def level_for_integer(cls, level):
        levels = cls.LEVELS
        if level < 0:
            return levels[0]
        if level >= len(levels):
            return levels[-1]
        return levels[level]

    level_for_integer = classmethod(level_for_integer)

def mkdir(path):
    if not os.path.exists(path):
        logger.info('Creating %s', path)
        os.makedirs(path)
    else:
        logger.info('Directory %s already exists', path)

def copyfile(src, dest, symlink=True):
    if not os.path.exists(src):
        # Some bad symlink in the src
        logger.warn('Cannot find file %s (bad symlink)', src)
        return
    if os.path.exists(dest):
        logger.debug('File %s already exists', dest)
        return
    if not os.path.exists(os.path.dirname(dest)):
        logger.info('Creating parent directories for %s' % os.path.dirname(dest))
        os.makedirs(os.path.dirname(dest))
    if symlink and hasattr(os, 'symlink'):
        logger.info('Symlinking %s', dest)
        os.symlink(os.path.abspath(src), dest)
    else:
        logger.info('Copying to %s', dest)
        if os.path.isdir(src):
            shutil.copytree(src, dest, True)
        else:
            shutil.copy2(src, dest)

def writefile(dest, content, overwrite=True):
    if not os.path.exists(dest):
        logger.info('Writing %s', dest)
        f = open(dest, 'wb')
        f.write(content)
        f.close()
        return
    else:
        f = open(dest, 'rb')
        c = f.read()
        f.close()
        if c != content:
            if not overwrite:
                logger.notify('File %s exists with different content; not overwriting', dest)
                return
            logger.notify('Overwriting %s with new content', dest)
            f = open(dest, 'wb')
            f.write(content)
            f.close()
        else:
            logger.info('Content %s already in place', dest)

def rmtree(dir):
    if os.path.exists(dir):
        logger.notify('Deleting tree %s', dir)
        shutil.rmtree(dir)
    else:
        logger.info('Do not need to delete %s; already gone', dir)

def make_exe(fn):
    if hasattr(os, 'chmod'):
        oldmode = os.stat(fn).st_mode & 07777
        newmode = (oldmode | 0555) & 07777
        os.chmod(fn, newmode)
        logger.info('Changed mode of %s to %s', fn, oct(newmode))

def _find_file(filename, dirs):
    for dir in dirs:
        if os.path.exists(join(dir, filename)):
            return join(dir, filename)
    return filename

def _install_req(py_executable, unzip=False, distribute=False):
    if not distribute:
        setup_fn = 'setuptools-0.6c11-py%s.egg' % sys.version[:3]
        project_name = 'setuptools'
        bootstrap_script = EZ_SETUP_PY
        source = None
    else:
        setup_fn = None
        source = 'distribute-0.6.8.tar.gz'
        project_name = 'distribute'
        bootstrap_script = DISTRIBUTE_SETUP_PY
        try:
            # check if the global Python has distribute installed or plain
            # setuptools
            import pkg_resources
            if not hasattr(pkg_resources, '_distribute'):
                location = os.path.dirname(pkg_resources.__file__)
                logger.notify("A globally installed setuptools was found (in %s)" % location)
                logger.notify("Use the --no-site-packages option to use distribute in "
                              "the virtualenv.")
        except ImportError:
            pass

    search_dirs = file_search_dirs()

    if setup_fn is not None:
        setup_fn = _find_file(setup_fn, search_dirs)

    if source is not None:
        source = _find_file(source, search_dirs)

    if is_jython and os._name == 'nt':
        # Jython's .bat sys.executable can't handle a command line
        # argument with newlines
        import tempfile
        fd, ez_setup = tempfile.mkstemp('.py')
        os.write(fd, bootstrap_script)
        os.close(fd)
        cmd = [py_executable, ez_setup]
    else:
        cmd = [py_executable, '-c', bootstrap_script]
    if unzip:
        cmd.append('--always-unzip')
    env = {}
    if logger.stdout_level_matches(logger.DEBUG):
        cmd.append('-v')

    old_chdir = os.getcwd()
    if setup_fn is not None and os.path.exists(setup_fn):
        logger.info('Using existing %s egg: %s' % (project_name, setup_fn))
        cmd.append(setup_fn)
        if os.environ.get('PYTHONPATH'):
            env['PYTHONPATH'] = setup_fn + os.path.pathsep + os.environ['PYTHONPATH']
        else:
            env['PYTHONPATH'] = setup_fn
    else:
        # the source is found, let's chdir
        if source is not None and os.path.exists(source):
            os.chdir(os.path.dirname(source))
        else:
            logger.info('No %s egg found; downloading' % project_name)
        cmd.extend(['--always-copy', '-U', project_name])
    logger.start_progress('Installing %s...' % project_name)
    logger.indent += 2
    cwd = None
    if project_name == 'distribute':
        env['DONT_PATCH_SETUPTOOLS'] = 'true'

    def _filter_ez_setup(line):
        return filter_ez_setup(line, project_name)

    if not os.access(os.getcwd(), os.W_OK):
        cwd = '/tmp'
        if source is not None and os.path.exists(source):
            # the current working dir is hostile, let's copy the
            # tarball to /tmp
            target = os.path.join(cwd, os.path.split(source)[-1])
            shutil.copy(source, target)
    try:
        call_subprocess(cmd, show_stdout=False,
                        filter_stdout=_filter_ez_setup,
                        extra_env=env,
                        cwd=cwd)
    finally:
        logger.indent -= 2
        logger.end_progress()
        if os.getcwd() != old_chdir:
            os.chdir(old_chdir)
        if is_jython and os._name == 'nt':
            os.remove(ez_setup)

def file_search_dirs():
    here = os.path.dirname(os.path.abspath(__file__))
    dirs = ['.', here,
            join(here, 'virtualenv_support')]
    if os.path.splitext(os.path.dirname(__file__))[0] != 'virtualenv':
        # Probably some boot script; just in case virtualenv is installed...
        try:
            import virtualenv
        except ImportError:
            pass
        else:
            dirs.append(os.path.join(os.path.dirname(virtualenv.__file__), 'virtualenv_support'))
    return [d for d in dirs if os.path.isdir(d)]

def install_setuptools(py_executable, unzip=False):
    _install_req(py_executable, unzip)

def install_distribute(py_executable, unzip=False):
    _install_req(py_executable, unzip, distribute=True)

_pip_re = re.compile(r'^pip-.*(zip|tar.gz|tar.bz2|tgz|tbz)$', re.I)
def install_pip(py_executable):
    filenames = []
    for dir in file_search_dirs():
        filenames.extend([join(dir, fn) for fn in os.listdir(dir)
                          if _pip_re.search(fn)])
    filenames.sort(key=lambda x: os.path.basename(x).lower())
    if not filenames:
        filename = 'pip'
    else:
        filename = filenames[-1]
    easy_install_script = 'easy_install'
    if sys.platform == 'win32':
        easy_install_script = 'easy_install-script.py'
    cmd = [py_executable, join(os.path.dirname(py_executable), easy_install_script), filename]
    if filename == 'pip':
        logger.info('Installing pip from network...')
    else:
        logger.info('Installing %s' % os.path.basename(filename))
    logger.indent += 2
    def _filter_setup(line):
        return filter_ez_setup(line, 'pip')
    try:
        call_subprocess(cmd, show_stdout=False,
                        filter_stdout=_filter_setup)
    finally:
        logger.indent -= 2

def filter_ez_setup(line, project_name='setuptools'):
    if not line.strip():
        return Logger.DEBUG
    if project_name == 'distribute':
        for prefix in ('Extracting', 'Now working', 'Installing', 'Before',
                       'Scanning', 'Setuptools', 'Egg', 'Already',
                       'running', 'writing', 'reading', 'installing',
                       'creating', 'copying', 'byte-compiling', 'removing',
                       'Processing'):
            if line.startswith(prefix):
                return Logger.DEBUG
        return Logger.DEBUG
    for prefix in ['Reading ', 'Best match', 'Processing setuptools',
                   'Copying setuptools', 'Adding setuptools',
                   'Installing ', 'Installed ']:
        if line.startswith(prefix):
            return Logger.DEBUG
    return Logger.INFO

def main():
    parser = optparse.OptionParser(
        version=virtualenv_version,
        usage="%prog [OPTIONS] DEST_DIR")

    parser.add_option(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=0,
        help="Increase verbosity")

    parser.add_option(
        '-q', '--quiet',
        action='count',
        dest='quiet',
        default=0,
        help='Decrease verbosity')

    parser.add_option(
        '-p', '--python',
        dest='python',
        metavar='PYTHON_EXE',
        help='The Python interpreter to use, e.g., --python=python2.5 will use the python2.5 '
        'interpreter to create the new environment.  The default is the interpreter that '
        'virtualenv was installed with (%s)' % sys.executable)

    parser.add_option(
        '--clear',
        dest='clear',
        action='store_true',
        help="Clear out the non-root install and start from scratch")

    parser.add_option(
        '--no-site-packages',
        dest='no_site_packages',
        action='store_true',
        help="Don't give access to the global site-packages dir to the "
             "virtual environment")

    parser.add_option(
        '--unzip-setuptools',
        dest='unzip_setuptools',
        action='store_true',
        help="Unzip Setuptools or Distribute when installing it")

    parser.add_option(
        '--relocatable',
        dest='relocatable',
        action='store_true',
        help='Make an EXISTING virtualenv environment relocatable.  '
        'This fixes up scripts and makes all .pth files relative')

    parser.add_option(
        '--distribute',
        dest='use_distribute',
        action='store_true',
        help='Use Distribute instead of Setuptools. Set environ variable'
        'VIRTUALENV_USE_DISTRIBUTE to make it the default ')

    if 'extend_parser' in globals():
        extend_parser(parser)

    options, args = parser.parse_args()

    global logger

    if 'adjust_options' in globals():
        adjust_options(options, args)

    verbosity = options.verbose - options.quiet
    logger = Logger([(Logger.level_for_integer(2-verbosity), sys.stdout)])

    if options.python and not os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        env = os.environ.copy()
        interpreter = resolve_interpreter(options.python)
        if interpreter == sys.executable:
            logger.warn('Already using interpreter %s' % interpreter)
        else:
            logger.notify('Running virtualenv with interpreter %s' % interpreter)
            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'
            file = __file__
            if file.endswith('.pyc'):
                file = file[:-1]
            os.execvpe(interpreter, [interpreter, file] + sys.argv[1:], env)

    if not args:
        print 'You must provide a DEST_DIR'
        parser.print_help()
        sys.exit(2)
    if len(args) > 1:
        print 'There must be only one argument: DEST_DIR (you gave %s)' % (
            ' '.join(args))
        parser.print_help()
        sys.exit(2)

    home_dir = args[0]

    if os.environ.get('WORKING_ENV'):
        logger.fatal('ERROR: you cannot run virtualenv while in a workingenv')
        logger.fatal('Please deactivate your workingenv, then re-run this script')
        sys.exit(3)

    if 'PYTHONHOME' in os.environ:
        logger.warn('PYTHONHOME is set.  You *must* activate the virtualenv before using it')
        del os.environ['PYTHONHOME']

    if options.relocatable:
        make_environment_relocatable(home_dir)
        return

    create_environment(home_dir, site_packages=not options.no_site_packages, clear=options.clear,
                       unzip_setuptools=options.unzip_setuptools,
                       use_distribute=options.use_distribute)
    if 'after_install' in globals():
        after_install(options, home_dir)

def call_subprocess(cmd, show_stdout=True,
                    filter_stdout=None, cwd=None,
                    raise_on_returncode=True, extra_env=None):
    cmd_parts = []
    for part in cmd:
        if len(part) > 40:
            part = part[:30]+"..."+part[-5:]
        if ' ' in part or '\n' in part or '"' in part or "'" in part:
            part = '"%s"' % part.replace('"', '\\"')
        cmd_parts.append(part)
    cmd_desc = ' '.join(cmd_parts)
    if show_stdout:
        stdout = None
    else:
        stdout = subprocess.PIPE
    logger.debug("Running command %s" % cmd_desc)
    if extra_env:
        env = os.environ.copy()
        env.update(extra_env)
    else:
        env = None
    try:
        proc = subprocess.Popen(
            cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,
            cwd=cwd, env=env)
    except Exception, e:
        logger.fatal(
            "Error %s while executing command %s" % (e, cmd_desc))
        raise
    all_output = []
    if stdout is not None:
        stdout = proc.stdout
        while 1:
            line = stdout.readline()
            if not line:
                break
            line = line.rstrip()
            all_output.append(line)
            if filter_stdout:
                level = filter_stdout(line)
                if isinstance(level, tuple):
                    level, line = level
                logger.log(level, line)
                if not logger.stdout_level_matches(level):
                    logger.show_progress()
            else:
                logger.info(line)
    else:
        proc.communicate()
    proc.wait()
    if proc.returncode:
        if raise_on_returncode:
            if all_output:
                logger.notify('Complete output from command %s:' % cmd_desc)
                logger.notify('\n'.join(all_output) + '\n----------------------------------------')
            raise OSError(
                "Command %s failed with error code %s"
                % (cmd_desc, proc.returncode))
        else:
            logger.warn(
                "Command %s had error code %s"
                % (cmd_desc, proc.returncode))


def create_environment(home_dir, site_packages=True, clear=False,
                       unzip_setuptools=False, use_distribute=False):
    """
    Creates a new environment in ``home_dir``.

    If ``site_packages`` is true (the default) then the global
    ``site-packages/`` directory will be on the path.

    If ``clear`` is true (default False) then the environment will
    first be cleared.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)

    py_executable = os.path.abspath(install_python(
        home_dir, lib_dir, inc_dir, bin_dir,
        site_packages=site_packages, clear=clear))

    install_distutils(lib_dir, home_dir)

    if use_distribute or os.environ.get('VIRTUALENV_USE_DISTRIBUTE'):
        install_distribute(py_executable, unzip=unzip_setuptools)
    else:
        install_setuptools(py_executable, unzip=unzip_setuptools)

    install_pip(py_executable)

    install_activate(home_dir, bin_dir)

def path_locations(home_dir):
    """Return the path locations for the environment (where libraries are,
    where scripts go, etc)"""
    # XXX: We'd use distutils.sysconfig.get_python_inc/lib but its
    # prefix arg is broken: http://bugs.python.org/issue3386
    if sys.platform == 'win32':
        # Windows has lots of problems with executables with spaces in
        # the name; this function will remove them (using the ~1
        # format):
        mkdir(home_dir)
        if ' ' in home_dir:
            try:
                import win32api
            except ImportError:
                print 'Error: the path "%s" has a space in it' % home_dir
                print 'To handle these kinds of paths, the win32api module must be installed:'
                print '  http://sourceforge.net/projects/pywin32/'
                sys.exit(3)
            home_dir = win32api.GetShortPathName(home_dir)
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'Scripts')
    elif is_jython:
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'bin')
    else:
        lib_dir = join(home_dir, 'lib', py_version)
        inc_dir = join(home_dir, 'include', py_version)
        bin_dir = join(home_dir, 'bin')
    return home_dir, lib_dir, inc_dir, bin_dir

def install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages, clear):
    """Install just the base environment, no distutils patches etc"""
    if sys.executable.startswith(bin_dir):
        print 'Please use the *system* python to run this script'
        return

    if clear:
        rmtree(lib_dir)
        ## FIXME: why not delete it?
        ## Maybe it should delete everything with #!/path/to/venv/python in it
        logger.notify('Not deleting %s', bin_dir)

    if hasattr(sys, 'real_prefix'):
        logger.notify('Using real prefix %r' % sys.real_prefix)
        prefix = sys.real_prefix
    else:
        prefix = sys.prefix
    mkdir(lib_dir)
    fix_lib64(lib_dir)
    stdlib_dirs = [os.path.dirname(os.__file__)]
    if sys.platform == 'win32':
        stdlib_dirs.append(join(os.path.dirname(stdlib_dirs[0]), 'DLLs'))
    elif sys.platform == 'darwin':
        stdlib_dirs.append(join(stdlib_dirs[0], 'site-packages'))
    for stdlib_dir in stdlib_dirs:
        if not os.path.isdir(stdlib_dir):
            continue
        if hasattr(os, 'symlink'):
            logger.info('Symlinking Python bootstrap modules')
        else:
            logger.info('Copying Python bootstrap modules')
        logger.indent += 2
        try:
            for fn in os.listdir(stdlib_dir):
                if fn != 'site-packages' and os.path.splitext(fn)[0] in REQUIRED_MODULES:
                    copyfile(join(stdlib_dir, fn), join(lib_dir, fn))
        finally:
            logger.indent -= 2
    mkdir(join(lib_dir, 'site-packages'))
    writefile(join(lib_dir, 'site.py'), SITE_PY)
    writefile(join(lib_dir, 'orig-prefix.txt'), prefix)
    site_packages_filename = join(lib_dir, 'no-global-site-packages.txt')
    if not site_packages:
        writefile(site_packages_filename, '')
    else:
        if os.path.exists(site_packages_filename):
            logger.info('Deleting %s' % site_packages_filename)
            os.unlink(site_packages_filename)

    stdinc_dir = join(prefix, 'include', py_version)
    if os.path.exists(stdinc_dir):
        copyfile(stdinc_dir, inc_dir)
    else:
        logger.debug('No include dir %s' % stdinc_dir)

    if sys.exec_prefix != prefix:
        if sys.platform == 'win32':
            exec_dir = join(sys.exec_prefix, 'lib')
        elif is_jython:
            exec_dir = join(sys.exec_prefix, 'Lib')
        else:
            exec_dir = join(sys.exec_prefix, 'lib', py_version)
        for fn in os.listdir(exec_dir):
            copyfile(join(exec_dir, fn), join(lib_dir, fn))

    if is_jython:
        # Jython has either jython-dev.jar and javalib/ dir, or just
        # jython.jar
        for name in 'jython-dev.jar', 'javalib', 'jython.jar':
            src = join(prefix, name)
            if os.path.exists(src):
                copyfile(src, join(home_dir, name))
        # XXX: registry should always exist after Jython 2.5rc1
        src = join(prefix, 'registry')
        if os.path.exists(src):
            copyfile(src, join(home_dir, 'registry'), symlink=False)
        copyfile(join(prefix, 'cachedir'), join(home_dir, 'cachedir'),
                 symlink=False)

    mkdir(bin_dir)
    py_executable = join(bin_dir, os.path.basename(sys.executable))
    if 'Python.framework' in prefix:
        if re.search(r'/Python(?:-32|-64)*$', py_executable):
            # The name of the python executable is not quite what
            # we want, rename it.
            py_executable = os.path.join(
                    os.path.dirname(py_executable), 'python')

    logger.notify('New %s executable in %s', expected_exe, py_executable)
    if sys.executable != py_executable:
        ## FIXME: could I just hard link?
        executable = sys.executable
        if sys.platform == 'cygwin' and os.path.exists(executable + '.exe'):
            # Cygwin misreports sys.executable sometimes
            executable += '.exe'
            py_executable += '.exe'
            logger.info('Executable actually exists in %s' % executable)
        shutil.copyfile(executable, py_executable)
        make_exe(py_executable)
        if sys.platform == 'win32' or sys.platform == 'cygwin':
            pythonw = os.path.join(os.path.dirname(sys.executable), 'pythonw.exe')
            if os.path.exists(pythonw):
                logger.info('Also created pythonw.exe')
                shutil.copyfile(pythonw, os.path.join(os.path.dirname(py_executable), 'pythonw.exe'))

    if os.path.splitext(os.path.basename(py_executable))[0] != expected_exe:
        secondary_exe = os.path.join(os.path.dirname(py_executable),
                                     expected_exe)
        py_executable_ext = os.path.splitext(py_executable)[1]
        if py_executable_ext == '.exe':
            # python2.4 gives an extension of '.4' :P
            secondary_exe += py_executable_ext
        if os.path.exists(secondary_exe):
            logger.warn('Not overwriting existing %s script %s (you must use %s)'
                        % (expected_exe, secondary_exe, py_executable))
        else:
            logger.notify('Also creating executable in %s' % secondary_exe)
            shutil.copyfile(sys.executable, secondary_exe)
            make_exe(secondary_exe)

    if 'Python.framework' in prefix:
        logger.debug('MacOSX Python framework detected')

        # Make sure we use the the embedded interpreter inside
        # the framework, even if sys.executable points to
        # the stub executable in ${sys.prefix}/bin
        # See http://groups.google.com/group/python-virtualenv/
        #                              browse_thread/thread/17cab2f85da75951
        shutil.copy(
                os.path.join(
                    prefix, 'Resources/Python.app/Contents/MacOS/%s' % os.path.basename(sys.executable)),
                py_executable)

        # Copy the framework's dylib into the virtual
        # environment
        virtual_lib = os.path.join(home_dir, '.Python')

        if os.path.exists(virtual_lib):
            os.unlink(virtual_lib)
        copyfile(
            os.path.join(prefix, 'Python'),
            virtual_lib)

        # And then change the install_name of the copied python executable
        try:
            call_subprocess(
                ["install_name_tool", "-change",
                 os.path.join(prefix, 'Python'),
                 '@executable_path/../.Python',
                 py_executable])
        except:
            logger.fatal(
                "Could not call install_name_tool -- you must have Apple's development tools installed")
            raise

        # Some tools depend on pythonX.Y being present
        py_executable_version = '%s.%s' % (
            sys.version_info[0], sys.version_info[1])
        if not py_executable.endswith(py_executable_version):
            # symlinking pythonX.Y > python
            pth = py_executable + '%s.%s' % (
                    sys.version_info[0], sys.version_info[1])
            if os.path.exists(pth):
                os.unlink(pth)
            os.symlink('python', pth)
        else:
            # reverse symlinking python -> pythonX.Y (with --python)
            pth = join(bin_dir, 'python')
            if os.path.exists(pth):
                os.unlink(pth)
            os.symlink(os.path.basename(py_executable), pth)

    if sys.platform == 'win32' and ' ' in py_executable:
        # There's a bug with subprocess on Windows when using a first
        # argument that has a space in it.  Instead we have to quote
        # the value:
        py_executable = '"%s"' % py_executable
    cmd = [py_executable, '-c', 'import sys; print sys.prefix']
    logger.info('Testing executable with %s %s "%s"' % tuple(cmd))
    proc = subprocess.Popen(cmd,
                            stdout=subprocess.PIPE)
    proc_stdout, proc_stderr = proc.communicate()
    proc_stdout = os.path.normcase(os.path.abspath(proc_stdout.strip()))
    if proc_stdout != os.path.normcase(os.path.abspath(home_dir)):
        logger.fatal(
            'ERROR: The executable %s is not functioning' % py_executable)
        logger.fatal(
            'ERROR: It thinks sys.prefix is %r (should be %r)'
            % (proc_stdout, os.path.normcase(os.path.abspath(home_dir))))
        logger.fatal(
            'ERROR: virtualenv is not compatible with this system or executable')
        if sys.platform == 'win32':
            logger.fatal(
                'Note: some Windows users have reported this error when they installed Python for "Only this user".  The problem may be resolvable if you install Python "For all users".  (See https://bugs.launchpad.net/virtualenv/+bug/352844)')
        sys.exit(100)
    else:
        logger.info('Got sys.prefix result: %r' % proc_stdout)

    pydistutils = os.path.expanduser('~/.pydistutils.cfg')
    if os.path.exists(pydistutils):
        logger.notify('Please make sure you remove any previous custom paths from '
                      'your %s file.' % pydistutils)
    ## FIXME: really this should be calculated earlier
    return py_executable

def install_activate(home_dir, bin_dir):
    if sys.platform == 'win32' or is_jython and os._name == 'nt':
        files = {'activate.bat': ACTIVATE_BAT,
                 'deactivate.bat': DEACTIVATE_BAT}
        if os.environ.get('OS') == 'Windows_NT' and os.environ.get('OSTYPE') == 'cygwin':
            files['activate'] = ACTIVATE_SH
    else:
        files = {'activate': ACTIVATE_SH}
    files['activate_this.py'] = ACTIVATE_THIS
    for name, content in files.items():
        content = content.replace('__VIRTUAL_ENV__', os.path.abspath(home_dir))
        content = content.replace('__VIRTUAL_NAME__', os.path.basename(os.path.abspath(home_dir)))
        content = content.replace('__BIN_NAME__', os.path.basename(bin_dir))
        writefile(os.path.join(bin_dir, name), content)

def install_distutils(lib_dir, home_dir):
    distutils_path = os.path.join(lib_dir, 'distutils')
    mkdir(distutils_path)
    ## FIXME: maybe this prefix setting should only be put in place if
    ## there's a local distutils.cfg with a prefix setting?
    home_dir = os.path.abspath(home_dir)
    ## FIXME: this is breaking things, removing for now:
    #distutils_cfg = DISTUTILS_CFG + "\n[install]\nprefix=%s\n" % home_dir
    writefile(os.path.join(distutils_path, '__init__.py'), DISTUTILS_INIT)
    writefile(os.path.join(distutils_path, 'distutils.cfg'), DISTUTILS_CFG, overwrite=False)

def fix_lib64(lib_dir):
    """
    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y
    instead of lib/pythonX.Y.  If this is such a platform we'll just create a
    symlink so lib64 points to lib
    """
    if [p for p in distutils.sysconfig.get_config_vars().values()
        if isinstance(p, basestring) and 'lib64' in p]:
        logger.debug('This system uses lib64; symlinking lib64 to lib')
        assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (
            "Unexpected python lib dir: %r" % lib_dir)
        lib_parent = os.path.dirname(lib_dir)
        assert os.path.basename(lib_parent) == 'lib', (
            "Unexpected parent dir: %r" % lib_parent)
        copyfile(lib_parent, os.path.join(os.path.dirname(lib_parent), 'lib64'))

def resolve_interpreter(exe):
    """
    If the executable given isn't an absolute path, search $PATH for the interpreter
    """
    if os.path.abspath(exe) != exe:
        paths = os.environ.get('PATH', '').split(os.pathsep)
        for path in paths:
            if os.path.exists(os.path.join(path, exe)):
                exe = os.path.join(path, exe)
                break
    if not os.path.exists(exe):
        logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))
        sys.exit(3)
    return exe

############################################################
## Relocating the environment:

def make_environment_relocatable(home_dir):
    """
    Makes the already-existing environment use relative paths, and takes out
    the #!-based environment selection in scripts.
    """
    activate_this = os.path.join(home_dir, 'bin', 'activate_this.py')
    if not os.path.exists(activate_this):
        logger.fatal(
            'The environment doesn\'t have a file %s -- please re-run virtualenv '
            'on this environment to update it' % activate_this)
    fixup_scripts(home_dir)
    fixup_pth_and_egg_link(home_dir)
    ## FIXME: need to fix up distutils.cfg

OK_ABS_SCRIPTS = ['python', 'python%s' % sys.version[:3],
                  'activate', 'activate.bat', 'activate_this.py']

def fixup_scripts(home_dir):
    # This is what we expect at the top of scripts:
    shebang = '#!%s/bin/python' % os.path.normcase(os.path.abspath(home_dir))
    # This is what we'll put:
    new_shebang = '#!/usr/bin/env python%s' % sys.version[:3]
    activate = "import os; activate_this=os.path.join(os.path.dirname(__file__), 'activate_this.py'); execfile(activate_this, dict(__file__=activate_this)); del os, activate_this"
    bin_dir = os.path.join(home_dir, 'bin')
    for filename in os.listdir(bin_dir):
        filename = os.path.join(bin_dir, filename)
        if not os.path.isfile(filename):
            # ignore subdirs, e.g. .svn ones.
            continue
        f = open(filename, 'rb')
        lines = f.readlines()
        f.close()
        if not lines:
            logger.warn('Script %s is an empty file' % filename)
            continue
        if not lines[0].strip().startswith(shebang):
            if os.path.basename(filename) in OK_ABS_SCRIPTS:
                logger.debug('Cannot make script %s relative' % filename)
            elif lines[0].strip() == new_shebang:
                logger.info('Script %s has already been made relative' % filename)
            else:
                logger.warn('Script %s cannot be made relative (it\'s not a normal script that starts with %s)'
                            % (filename, shebang))
            continue
        logger.notify('Making script %s relative' % filename)
        lines = [new_shebang+'\n', activate+'\n'] + lines[1:]
        f = open(filename, 'wb')
        f.writelines(lines)
        f.close()

def fixup_pth_and_egg_link(home_dir, sys_path=None):
    """Makes .pth and .egg-link files use relative paths"""
    home_dir = os.path.normcase(os.path.abspath(home_dir))
    if sys_path is None:
        sys_path = sys.path
    for path in sys_path:
        if not path:
            path = '.'
        if not os.path.isdir(path):
            continue
        path = os.path.normcase(os.path.abspath(path))
        if not path.startswith(home_dir):
            logger.debug('Skipping system (non-environment) directory %s' % path)
            continue
        for filename in os.listdir(path):
            filename = os.path.join(path, filename)
            if filename.endswith('.pth'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .pth file %s, skipping' % filename)
                else:
                    fixup_pth_file(filename)
            if filename.endswith('.egg-link'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .egg-link file %s, skipping' % filename)
                else:
                    fixup_egg_link(filename)

def fixup_pth_file(filename):
    lines = []
    prev_lines = []
    f = open(filename)
    prev_lines = f.readlines()
    f.close()
    for line in prev_lines:
        line = line.strip()
        if (not line or line.startswith('#') or line.startswith('import ')
            or os.path.abspath(line) != line):
            lines.append(line)
        else:
            new_value = make_relative_path(filename, line)
            if line != new_value:
                logger.debug('Rewriting path %s as %s (in %s)' % (line, new_value, filename))
            lines.append(new_value)
    if lines == prev_lines:
        logger.info('No changes to .pth file %s' % filename)
        return
    logger.notify('Making paths in .pth file %s relative' % filename)
    f = open(filename, 'w')
    f.write('\n'.join(lines) + '\n')
    f.close()

def fixup_egg_link(filename):
    f = open(filename)
    link = f.read().strip()
    f.close()
    if os.path.abspath(link) != link:
        logger.debug('Link in %s already relative' % filename)
        return
    new_link = make_relative_path(filename, link)
    logger.notify('Rewriting link %s in %s as %s' % (link, filename, new_link))
    f = open(filename, 'w')
    f.write(new_link)
    f.close()

def make_relative_path(source, dest, dest_is_directory=True):
    """
    Make a filename relative, where the filename is dest, and it is
    being referred to from the filename source.

        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/usr/share/another-place/src/Directory')
        '../another-place/src/Directory'
        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/home/user/src/Directory')
        '../../../home/user/src/Directory'
        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')
        './'
    """
    source = os.path.dirname(source)
    if not dest_is_directory:
        dest_filename = os.path.basename(dest)
        dest = os.path.dirname(dest)
    dest = os.path.normpath(os.path.abspath(dest))
    source = os.path.normpath(os.path.abspath(source))
    dest_parts = dest.strip(os.path.sep).split(os.path.sep)
    source_parts = source.strip(os.path.sep).split(os.path.sep)
    while dest_parts and source_parts and dest_parts[0] == source_parts[0]:
        dest_parts.pop(0)
        source_parts.pop(0)
    full_parts = ['..']*len(source_parts) + dest_parts
    if not dest_is_directory:
        full_parts.append(dest_filename)
    if not full_parts:
        # Special case for the current directory (otherwise it'd be '')
        return './'
    return os.path.sep.join(full_parts)



############################################################
## Bootstrap script creation:

def create_bootstrap_script(extra_text, python_version=''):
    """
    Creates a bootstrap script, which is like this script but with
    extend_parser, adjust_options, and after_install hooks.

    This returns a string that (written to disk of course) can be used
    as a bootstrap script with your own customizations.  The script
    will be the standard virtualenv.py script, with your extra text
    added (your extra text should be Python code).

    If you include these functions, they will be called:

    ``extend_parser(optparse_parser)``:
        You can add or remove options from the parser here.

    ``adjust_options(options, args)``:
        You can change options here, or change the args (if you accept
        different kinds of arguments, be sure you modify ``args`` so it is
        only ``[DEST_DIR]``).

    ``after_install(options, home_dir)``:

        After everything is installed, this function is called.  This
        is probably the function you are most likely to use.  An
        example would be::

            def after_install(options, home_dir):
                subprocess.call([join(home_dir, 'bin', 'easy_install'),
                                 'MyPackage'])
                subprocess.call([join(home_dir, 'bin', 'my-package-script'),
                                 'setup', home_dir])

        This example immediately installs a package, and runs a setup
        script from that package.

    If you provide something like ``python_version='2.4'`` then the
    script will start with ``#!/usr/bin/env python2.4`` instead of
    ``#!/usr/bin/env python``.  You can use this when the script must
    be run with a particular Python version.
    """
    filename = __file__
    if filename.endswith('.pyc'):
        filename = filename[:-1]
    f = open(filename, 'rb')
    content = f.read()
    f.close()
    py_exe = 'python%s' % python_version
    content = (('#!/usr/bin/env %s\n' % py_exe)
               + '## WARNING: This file is generated\n'
               + content)
    return content.replace('##EXT' 'END##', extra_text)

import os
import sys
import urllib

PINAX_GIT_LOCATION = 'git://github.com/pinax/pinax.git'
PINAX_PYPI_MIRRORS = [
    'http://pypi.pinaxproject.com',
    'http://pypi2.pinaxproject.com',
]
PINAX_MUST_HAVES = {
    'setuptools-git': ('0.3.4', 'setuptools_git-0.3.4.tar.gz'),
    'setuptools-dummy': ('0.0.3', 'setuptools_dummy-0.0.3.tar.gz'),
    'Django': ('1.0.4', 'Django-1.0.4.tar.gz'),
}

DJANGO_VERSIONS = (
    '1.0.4',
#    '1.1',
)

if sys.platform == 'win32':
    GIT_CMD = 'git.cmd'
    PIP_CMD = 'pip.exe'
    extra = {'shell': True}
else:
    GIT_CMD = 'git'
    PIP_CMD = 'pip'
    extra = {}

# Bailing if "~/.pydistutils.cfg" exists
pydistutils = os.path.expanduser('~/.pydistutils.cfg')
if os.path.exists(pydistutils):
    print "ERROR: Please make sure you remove any previous custom paths from"
    print "your %s file." % pydistutils
    sys.exit(3)

def filename_to_url(filename):
    """
    Convert a path to a file: URL.  The path will be made absolute.
    """
    filename = os.path.normcase(os.path.abspath(filename))
    drive, filename = os.path.splitdrive(filename)
    filepath = filename.split(os.path.sep)
    url = '/'.join([urllib.quote(part) for part in filepath])
    if not drive:
        url = url.lstrip('/')
    return 'file:///' + drive + url

def winpath(path):
    if sys.platform == 'win32':
        if not os.path.exists(path):
            os.makedirs(path)
        import win32api
        # get the stupid short name on Windows to prevent dying
        # because of spaces in the command name
        return win32api.GetShortPathName(path)
    return path

def resolve_command(cmd, path=None, pathext=None):
    """
    Searches the PATH for the given executable and returns the normalized path
    """
    # save the path searched for for later fallback
    searched_for_path = path
    if path is None:
        path = os.environ.get('PATH', []).split(os.pathsep)
    if isinstance(path, basestring):
        path = [path]
    # check if there are funny path extensions for executables, e.g. Windows
    if pathext is None:
        pathext = os.environ.get('PATHEXT', '.COM;.EXE;.BAT;.CMD').split(os.pathsep)
    # don't use extensions if the command ends with one of them
    for ext in pathext:
        if cmd.endswith(ext):
            pathext = ['']
            break
    # check if we find the command on PATH
    for _dir in path:
        f = os.path.join(_dir, cmd)
        for ext in pathext:
            # try without extension first
            if os.path.isfile(f):
                return os.path.realpath(f)
            # then including the extension
            fext = f + ext
            if os.path.isfile(fext):
                return os.path.realpath(fext)
    # last resort: just try the searched for path
    if searched_for_path:
        cmd = os.path.join(os.path.realpath(searched_for_path), cmd)
    if not os.path.exists(cmd):
        print "ERROR: this script requires %s." % cmd
        print "Please verify it exists because it couldn't be found."
        sys.exit(3)
    return os.path.realpath(cmd)

def extend_parser(parser):
    parser.add_option("-s", "--source",
        metavar="DIR_OR_URL", dest="pinax_source", default=PINAX_GIT_LOCATION,
        help="Location of the Pinax source to use for the installation")
    parser.add_option("-r", "--release",
        metavar="RELEASE_VERSION", dest="release", default=None,
        help="Release version of Pinax to bootstrap")
    parser.add_option("-d", "--development",
        action="store_true", dest="development",
        help="Setup development environment")
    parser.add_option("--django-version",
        metavar="DJANGO_VERSION", dest="django_version", default=None,
        help="The version of Django to be installed, e.g. --django-version=1.0.4 will install Django 1.0.4. The default is 1.0.4.")

def adjust_options(options, args):
    """
    You can change options here, or change the args (if you accept
    different kinds of arguments, be sure you modify ``args`` so it is
    only ``[DEST_DIR]``).
    """
    if options.release and options.development:
        print "ERROR: please use --development without providing a --release version."
        sys.exit(101)
    if options.django_version:
        if options.django_version not in DJANGO_VERSIONS:
            print "ERROR: this Django version is not supported."
            print "Use one of those: %s" % ", ".join(DJANGO_VERSIONS)
            sys.exit(101)
        django_tarball = 'Django-%s.tar.gz' % options.django_version
        PINAX_MUST_HAVES['django'] = (options.django_version, django_tarball)
    if not args:
        return # caller will raise error

def install_base(parent_dir, bin_dir, requirements_dir, packages):
    """
    Installs base packages from the bundled tarball if existing
    """
    packages = packages.copy() # prevent changing the global data
    
    find_links = []
    for mirror in PINAX_PYPI_MIRRORS:
        find_links.extend(['--find-links', mirror])
    # resolve path to the freshly installed pip
    pip = resolve_command(PIP_CMD, bin_dir)
    for pkg in packages:
        version, filename = packages[pkg]
        src = join(requirements_dir, 'base', filename)
        if not os.path.exists(src):
            # get it from the PyPI
            src = '%s==%s' % (pkg, version)
        logger.notify('Installing %s %s' % (pkg, version))
        call_subprocess([
            pip,
            'install',
            '--quiet',
            '--no-deps',
            '--ignore-installed',
        ] + find_links + [
            src,
        ], filter_stdout=filter_lines, show_stdout=False)
    return pip

def after_install(options, home_dir):
    this_dir = os.path.dirname(__file__)
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)
    src_dir = join(home_dir, 'src')
    parent_dir = join(this_dir, '..')

    python = resolve_command(expected_exe, bin_dir)

    requirements_dir = join(parent_dir, 'requirements')
    pip = install_base(parent_dir, bin_dir, requirements_dir, PINAX_MUST_HAVES)

    version_file = join(parent_dir, 'VERSION')
    if os.path.exists(version_file) and not options.release:
        f = open(version_file)
        version = f.read()
        f.close()
        version = "".join(version.splitlines())
        if version:
            options.release = version

    if options.development:
        logger.notify('Going to setup a Pinax development environment.')
        # For developers and other crazy HEAD lovers
        source = options.pinax_source
        if os.path.exists(source):
            # A directory was given as a source for bootstrapping
            pinax_dir = winpath(os.path.realpath(source))
            logger.notify('Using existing Pinax at %s' % source)
        else:
            # Go and get Pinax
            pinax_dir = join(src_dir, 'pinax')
            if not os.path.exists(src_dir):
                logger.info('Creating directory %s' % src_dir)
                os.makedirs(src_dir)
            git = resolve_command(GIT_CMD)
            if os.path.exists(join(pinax_dir, '.git')):
                logger.notify('Found Pinax in %s. Updating' % pinax_dir)
                call_subprocess([git, 'pull'], show_stdout=True, cwd=pinax_dir)
            else:
                logger.notify('Fetching Pinax from %s to %s' % (source, pinax_dir))
                call_subprocess([git, 'clone', source, pinax_dir],
                                show_stdout=True)
        logger.indent += 2
        try:
            logger.notify('Installing Pinax')
            call_subprocess([python, 'setup.py', 'develop', '--quiet'],
                            filter_stdout=filter_lines, show_stdout=False,
                            cwd=pinax_dir)
        finally:
            logger.indent -= 2
        logger.notify('Please follow the documentation to continue.')
    elif options.release:
        # release should *never* touch the Internet.
        logger.notify('Going to install a full Pinax %s release.' % options.release)
        release_dir = join(requirements_dir, options.release)
        requirements_file = os.path.abspath(join(release_dir, 'release.txt'))
        if not os.path.exists(requirements_file):
            print "ERROR: no requirements were found for version %s." % options.release
            sys.exit(101)
        logger.indent += 2
        try:
            logger.notify('Installing Pinax')
            call_subprocess([
                pip,
                'install',
                '--no-deps',
                '--no-index',
                '--ignore-installed',
                '--environment', home_dir,
                '--requirement', requirements_file,
                '--find-links', filename_to_url(release_dir),
            ], show_stdout=True)
        finally:
            logger.indent -= 2
        env_dir = os.path.normpath(home_dir)
        logger.notify("Please activate the newly created virtualenv by running: ")
        logger.indent += 2
        if sys.platform == "win32":
            logger.notify("%s\\Scripts\\activate.bat" % env_dir)
        else:
            logger.notify("source %s/bin/activate" % env_dir)
        logger.indent -= 2
        logger.notify("Pinax environment created successfully.")
    else:
        logger.notify("Cannot locate a VERSION file for release. You are "
            "likely not running from a release tarball. Perhaps you meant to "
            "use --development")


def filter_lines(line):
    if not line.strip():
        return Logger.DEBUG
    for prefix in ['Searching for', 'Reading ', 'Best match: ', 'Processing ',
                   'Moving ', 'Adding ', 'running ', 'writing ', 'Creating ',
                   'creating ', 'Copying ', 'warning: manifest_maker',
                   'zip_safe flag not set', 'Installed', 'Finished']:
        if line.startswith(prefix):
            return Logger.DEBUG
    for suffix in ['module references __file__', 'module references __path__',
                   'inspect.getsourcefile']:
        if line.endswith(suffix):
            return Logger.DEBUG
    return Logger.NOTIFY


##file site.py
SITE_PY = """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""".decode("base64").decode("zlib")

##file ez_setup.py
EZ_SETUP_PY = """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""".decode("base64").decode("zlib")

##file distribute_setup.py
DISTRIBUTE_SETUP_PY = """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=
""".decode("base64").decode("zlib")

##file activate.sh
ACTIVATE_SH = """
eJytVFFv2jAQfs+vuIU+QDWK+tqKB6oigdRC1bBOW1sZk1yIpWAj2yGj0/77ziFAUijStPIA2Hc+
f/7u+64Bk0QYiEWKsMiMhRlCZjCCXNgEfKMyHSLMhOzw0IoVt+jDeazVAmbcJOdeA9Yqg5BLqSzo
TIKwEAmNoU3Xnhfh9hQ0W/DbA/o0QKNBCyqNAOVKaCUXKC2suBZ8lqIpskQMz9CW4J+x8d0texo+
Tr717thDbzLw4RWuwSYoi0z3cdvdY6m7DPy1VNoWibu9TDocB4eKeCxOwvgxGYxHg/F9/xiYXfAA
0v7YAbBd6CS8ehaBLCktmmgSlRGpEVqiv+gPcBnBm0m+Qp6IMIGErxA4/VAoVIuFC9uE26L1ZSkS
QMjTlCRgFcwJAXWU/sVKu8WSk0bKo+YC4DvJRGW2DFsh52WZWqIjCM4cuRAmXM7RQE5645H7WoPT
Dl1LulgScozeUX/TC6jpbbVZ/QwG7Kn/GAzHoyPkF09r6xo9HzUxuDzWveDyoG2UeNCv4PJko8rw
FsImZRvtj572wL4QLgLSBV8qGaGxOnOewXfYGhBgGsM24cu729sutDXb9uo/HvlzExdaY0rdrxmt
Ys/63Z5Xgdr1GassGfO9koTqe7wDHxGNGw+Wi0p2h7Gb4YiNevd9xq7KtKpFd7j3inds0Q5FrBN7
LtIUYi5St1/NMi7LKdZpDhdLuwZ6FwkTmhsTUMaMR2SNdc7XLaoXFrahqQdTqtUs6Myu4YoUu6vb
guspCFm4ytsL6sNB8IFtu7UjFWlUnO00s7nhDWqssdth0Lu567OHx/H9w+TkjYWKd8ItyvlTAo+S
LxBeanVf/GmhP+rsoR8a4EwpeEpTgRgin0OPdiQZdy7CctYrLcq5XR5BhMTa5VWnk+f5xRtasvrq
gsZBx6jY5lxjh7sqnbrvnisQp1T6KNiX6fQV9m/D1GC9SvPEQ1v7g+WIrxjaMf9Js/QT5uh/ztB/
n5/b2Uk0/AXm/2MV
""".decode("base64").decode("zlib")

##file activate.bat
ACTIVATE_BAT = """
eJyFUssKgzAQvAfyD3swYH+hItSiVKlGsalQKOyhauvFHOr/U+MzFcWc9jEzO7vkVLw+EmRZUvIt
GsiCVNydED2e2YhahkgJJVUJtWwgL8qqLnJI0jhKBJiUQPsUv6/YRmJcKDkMlBGOcehOmptctgJj
e2IP4cfcjyNvFOwVp/JSdWqMygq+MthmkwHNojmfhjuRh3iAGffncsPYhpl2mm5sbY+9QzjC7ylt
sFy6LTEL3rKRcLsGicrXV++4HVz1jzN4Vta+BnsingM+nMLSiB53KfkBsnmnEA==
""".decode("base64").decode("zlib")

##file deactivate.bat
DEACTIVATE_BAT = """
eJxzSE3OyFfIT0vj4spMU0hJTcvMS01RiPf3cYkP8wwKCXX0iQ8I8vcNCFHQ4FIAguLUEgWIgK0q
FlWqXJpcICVYpGzx2BAZ4uHv5+Hv6wq1BWINXBTdKriEKkI1DhW2QAfhttcxxANiFZCBbglQSJUL
i2dASrm4rFz9XLgAwJNbyQ==
""".decode("base64").decode("zlib")

##file distutils-init.py
DISTUTILS_INIT = """
eJytVl2L6zYQffevGBKK7XavKe3bhVBo78uFSyml0IdlEVpbTtR1JCMpm6S/vjOSY0v+uO1DDbs4
0tF8nJk5sjz32jjQNpPhzd7H1ys3SqqjhcfCL1q18vgbN1YY2Kc/pQWlHXB4l8ZdeCfUO5x1c+nE
E1gNVwE1V3CxAqQDp6GVqgF3EmBd08nXLGukUfws4IDBVD13p2pYoS3rLk52ltF6hPhLS1XM4EUc
VsVYKzvBWPkE+WgmLzPZjkaUNmd6KVI3JRwWoRSLM6P98mMG+Dw4q+il8Ev07P7ATCNmRlfQ8/qN
HwVwB99Y4H0vMHAi6BWZUoEhoqXTNXdSK+A2LN6tE+fJ0E+7MhOdFSEM5lNgrJIKWXDF908wy87D
xE3UoHsxkegZTaHIHGNSSYfm+ntelpURvCnK7NEWBI/ap/b8Z1m232N2rj7B60V2DRM3B5NpaLSw
KnfwpvQVTviHOR+F88lhQyBAGlE7be6DoRNg9ldsG3218IHa6MRNU+tGBEYIggwafRk6yzsXDcVU
9Ua08kYxt+F3x12LRaQi52j0xx/ywFxrdMRqVevzmaummlIYEp0WsCAaX8cFb6buuLUTqEgQQ6/Q
04iWRoF38m/BdE8VtlBY0bURiB6KG1crpMZwc2fIjqWh+1UrkSLpWUIP8PySwLKv4qPGSVqDuMPy
dywQ+gS7L1irXVkm5pJsq3l+Ib1lMOvUrxI+/mBBY4KB+WpUtcO06RtzckNvQ6vYj1lGoZM2sdDG
fryJPYJVn/Cfka8XSqNaoLKhmOlqXMzW9+YBVp1EtIThZtOwzCRvMaARa+0xD0b2kcaJGwJsMbc7
hLUfY4vKvsCOBdvDnyfuRbzmXRdGTZgPF7oGQkJACWVD22IMQdhx0npt5S2f+pXO+OwH6d+hwiS5
7IJOjcK2emj1zBy1aONHByfAMoraw6WlrSIFTbGghqASoRCjVncYROFpXM4uYSqhGnuVeGvks4jz
cjnCoR5GnPW7KOh4maVbdFeoplgJ3wh3MSrAsv/QuMjOspnTKRl1fTYqqNisv7uTVnhF1GhoBFbp
lh+OcXN2riA5ZrYXtWxlfcDuC8U5kLoN3CCJYXGpesO6dx6rU0zGMtjU6cNlmW0Fid8Sja4ZG+Z3
fTPbyj+mZnZ2wSQK8RaT9Km0ySRuLpm0DkUUL0ra3WQ2BgGJ7v9I9SKqNKZ/IR4R28RHm+vEz5ic
nZ2IH7bfub8pU1PR3gr10W7xLTfHh6Z6bgZ7K14G7Mj/1z5J6MFo6V5e07H0Ou78dTyeI+mxKOpI
eC2KMSj6HKxd6Uudf/n886fPv+f++x1lbASlmjQuPz8OvGA0j7j2eCu/4bcW6SFeCuNJ0W1GQHI5
iwC9Ey0bjtHd9P4dPA++XxLnZDVuxvFEtlm3lf5a2c02u2LRYXHH/AOs8pIa
""".decode("base64").decode("zlib")

##file distutils.cfg
DISTUTILS_CFG = """
eJxNj00KwkAMhfc9xYNuxe4Ft57AjYiUtDO1wXSmNJnK3N5pdSEEAu8nH6lxHVlRhtDHMPATA4uH
xJ4EFmGbvfJiicSHFRzUSISMY6hq3GLCRLnIvSTnEefN0FIjw5tF0Hkk9Q5dRunBsVoyFi24aaLg
9FDOlL0FPGluf4QjcInLlxd6f6rqkgPu/5nHLg0cXCscXoozRrP51DRT3j9QNl99AP53T2Q=
""".decode("base64").decode("zlib")

##file activate_this.py
ACTIVATE_THIS = """
eJyNUk2L3DAMvftXiCxLEphmSvc2MIcu9NaWHnopwxCcRNlRN7GD7clM/n0lp5mPZQs1JLb8pKcn
WUmSPE9w9GReAM9Yt9RhFg7kSzmtoKE6ZGU0ynJ7AfIcJnuEE3Wd0nWgUQcEQWEkF466QzMCf+Ss
6dGEQqmfgtbaQIWcDxs4HdBElv7og1wBg3gmH0TMjykcrAEyAd3gkP8rMDaocMDbHBWZ9RBdVZIk
SgU3bRTwWjQrPNc4BPiue/zinHUz7DRxws/eowtkTUSyiMhKfi2y3NHMdXX0itcOpYMOh3Ww61g8
luJSDFP6tmH3ftyki2eeJ7mifrAugJ/8crReqUqztC0fC4kuGnKGxWf/snXlZb8kzXMmboW0GDod
Wut62G4hPZF5+pTO5XtiKYOuX/UL+ptcvy2ZTPKvIP1KFdeTiuuHxTXNFXYe/5+km0nmJ3r0KTxG
YSM6z23fbZ7276Tg9x5LdiuFjok7noks1sP2tWscpeRX6KaRnRuT3WnKlQQ51F3JlC2dmSvSRENd
j3wvetUDfLOjDDLPYtPwjDJb7yHYeNXyMPMLtdEQKRtl8HQrdLdX3O4YxZP7RvfcNH6ZCPMsi8td
qZvLAN7yFnoY0DSZhOUXj4WWy+tZ8190ud1tPu5Zzy2N+gOGaVfA
""".decode("base64").decode("zlib")

if __name__ == '__main__':
    main()

## TODO:
## Copy python.exe.manifest
## Monkeypatch distutils.sysconfig

########NEW FILE########
__FILENAME__ = models
from django.db import models

# Create your models here.
#Model created

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.views.generic.simple import direct_to_template

urlpatterns = patterns('',
    url(r'^$', direct_to_template, {"template": "about/about.html"}, name="about"),

    url(r'^terms/$', direct_to_template, {"template": "about/terms.html"}, name="terms"),
    url(r'^privacy/$', direct_to_template, {"template": "about/privacy.html"}, name="privacy"),
    url(r'^dmca/$', direct_to_template, {"template": "about/dmca.html"}, name="dmca"),

    url(r'^what_next/$', direct_to_template, {"template": "about/what_next.html"}, name="what_next"),
)

########NEW FILE########
__FILENAME__ = views
# Create your views here.

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from account.models import Account, OtherServiceInfo, PasswordReset

admin.site.register(Account)
admin.site.register(OtherServiceInfo)

class PasswordResetAdmin(admin.ModelAdmin):
    list_display = ('user', 'temp_key', 'timestamp', 'reset')

admin.site.register(PasswordReset, PasswordResetAdmin)

########NEW FILE########
__FILENAME__ = context_processors

from account.models import Account, AnonymousAccount

def openid(request):
    return {'openid': request.openid}

def account(request):
    if request.user.is_authenticated():
        try:
            account = Account._default_manager.get(user=request.user)
        except Account.DoesNotExist:
            account = AnonymousAccount(request)
    else:
        account = AnonymousAccount(request)
    return {'account': account}

########NEW FILE########
__FILENAME__ = forms
import re

from django import forms
from django.template.loader import render_to_string
from django.conf import settings
from django.utils.translation import ugettext_lazy as _, ugettext
from django.utils.encoding import smart_unicode
from django.utils.hashcompat import sha_constructor

from pinax.core.utils import get_send_mail
send_mail = get_send_mail()

from django.contrib.auth import authenticate, login
from django.contrib.auth.models import User
from django.contrib.sites.models import Site

from emailconfirmation.models import EmailAddress
from account.models import Account

from timezones.forms import TimeZoneField

from account.models import PasswordReset


alnum_re = re.compile(r'^\w+$')
import logging

logger = logging.getLogger(__name__)

class LoginForm(forms.Form):

    username = forms.CharField(label=_("Username"), max_length=30, widget=forms.TextInput())
    password = forms.CharField(label=_("Password"), widget=forms.PasswordInput(render_value=False))
    remember = forms.BooleanField(label=_("Remember Me"), help_text=_("If checked you will stay logged in for 3 weeks"), required=False)

    user = None

    def clean(self):
        if self._errors:
            return
        user = authenticate(username=self.cleaned_data["username"], password=self.cleaned_data["password"])
        if user:
            if user.is_active:
                self.user = user
            else:
                raise forms.ValidationError(_("This account is currently inactive."))
        else:
            raise forms.ValidationError(_("The username and/or password you specified are not correct."))
        return self.cleaned_data

    def login(self, request):
        if self.is_valid():
            login(request, self.user)
            request.user.message_set.create(message=ugettext(u"Successfully logged in as %(username)s.") % {'username': self.user.username})
            if self.cleaned_data['remember']:
                request.session.set_expiry(60 * 60 * 24 * 7 * 3)
            else:
                request.session.set_expiry(0)
            return True
        return False


class SignupForm(forms.Form):

    username = forms.CharField(label=_("Username"), max_length=30, widget=forms.TextInput())
    password1 = forms.CharField(label=_("Password"), widget=forms.PasswordInput(render_value=False))
    password2 = forms.CharField(label=_("Password (again)"), widget=forms.PasswordInput(render_value=False))

    if settings.ACCOUNT_REQUIRED_EMAIL or settings.ACCOUNT_EMAIL_VERIFICATION:
        email = forms.EmailField(
            label = _("Email"),
            required = True,
            widget = forms.TextInput()
        )
    else:
        email = forms.EmailField(
            label = _("Email (optional)"),
            required = False,
            widget = forms.TextInput()
        )

    confirmation_key = forms.CharField(max_length=40, required=False, widget=forms.HiddenInput())

    def clean_username(self):
        if not alnum_re.search(self.cleaned_data["username"]):
            raise forms.ValidationError(_("Usernames can only contain letters, numbers and underscores."))
        try:
            user = User.objects.get(username__iexact=self.cleaned_data["username"])
        except User.DoesNotExist:
            return self.cleaned_data["username"]
        raise forms.ValidationError(_("This username is already taken. Please choose another."))

    def clean(self):
        if "password1" in self.cleaned_data and "password2" in self.cleaned_data:
            if self.cleaned_data["password1"] != self.cleaned_data["password2"]:
                raise forms.ValidationError(_("You must type the same password each time."))
        return self.cleaned_data

    def save(self):
        username = self.cleaned_data["username"]
        email = self.cleaned_data["email"]
        password = self.cleaned_data["password1"]

        if self.cleaned_data["confirmation_key"]:
            from friends.models import JoinInvitation # @@@ temporary fix for issue 93
            try:
                join_invitation = JoinInvitation.objects.get(confirmation_key = self.cleaned_data["confirmation_key"])
                confirmed = True
            except JoinInvitation.DoesNotExist:
                confirmed = False
        else:
            confirmed = False

        # @@@ clean up some of the repetition below -- DRY!

        if confirmed:
            if email == join_invitation.contact.email:
                new_user = User.objects.create_user(username, email, password)
                join_invitation.accept(new_user) # should go before creation of EmailAddress below
                new_user.message_set.create(message=ugettext(u"Your email address has already been verified"))
                # already verified so can just create
                EmailAddress(user=new_user, email=email, verified=True, primary=True).save()
            else:
                new_user = User.objects.create_user(username, "", password)
                join_invitation.accept(new_user) # should go before creation of EmailAddress below
                if email:
                    new_user.message_set.create(message=ugettext(u"Confirmation email sent to %(email)s") % {'email': email})
                    EmailAddress.objects.add_email(new_user, email)
        else:
            new_user = User.objects.create_user(username, "", password)
            if email:
                new_user.message_set.create(message=ugettext(u"Confirmation email sent to %(email)s") % {'email': email})
                EmailAddress.objects.add_email(new_user, email)

        if settings.ACCOUNT_EMAIL_VERIFICATION:
            new_user.is_active = False
            new_user.save()

        return username, password # required for authenticate()


class OpenIDSignupForm(forms.Form):
    username = forms.CharField(label="Username", max_length=30, widget=forms.TextInput())

    if settings.ACCOUNT_REQUIRED_EMAIL or settings.ACCOUNT_EMAIL_VERIFICATION:
        email = forms.EmailField(
            label = _("Email"),
            required = True,
            widget = forms.TextInput()
        )
    else:
        email = forms.EmailField(
            label = _("Email (optional)"),
            required = False,
            widget = forms.TextInput()
        )

    def __init__(self, *args, **kwargs):
        # remember provided (validated!) OpenID to attach it to the new user
        # later.
        self.openid = kwargs.pop("openid", None)

        # pop these off since they are passed to this method but we can't
        # pass them to forms.Form.__init__
        kwargs.pop("reserved_usernames", [])
        kwargs.pop("no_duplicate_emails", False)

        super(OpenIDSignupForm, self).__init__(*args, **kwargs)

    def clean_username(self):
        if not alnum_re.search(self.cleaned_data["username"]):
            raise forms.ValidationError(u"Usernames can only contain letters, numbers and underscores.")
        try:
            user = User.objects.get(username__iexact=self.cleaned_data["username"])
        except User.DoesNotExist:
            return self.cleaned_data["username"]
        raise forms.ValidationError(u"This username is already taken. Please choose another.")


class UserForm(forms.Form):

    def __init__(self, user=None, *args, **kwargs):
        self.user = user
        super(UserForm, self).__init__(*args, **kwargs)


class AccountForm(UserForm):

    def __init__(self, *args, **kwargs):
        super(AccountForm, self).__init__(*args, **kwargs)
        try:
            self.account = Account.objects.get(user=self.user)
        except Account.DoesNotExist:
            self.account = Account(user=self.user)


class AddEmailForm(UserForm):

    email = forms.EmailField(label=_("Email"), required=True, widget=forms.TextInput(attrs={'size':'30'}))

    def clean_email(self):
        try:
            EmailAddress.objects.get(user=self.user, email=self.cleaned_data["email"])
        except EmailAddress.DoesNotExist:
            return self.cleaned_data["email"]
        raise forms.ValidationError(_("This email address already associated with this account."))

    def save(self):
        self.user.message_set.create(message=ugettext(u"Confirmation email sent to %(email)s") % {'email': self.cleaned_data["email"]})
        return EmailAddress.objects.add_email(self.user, self.cleaned_data["email"])


class ChangePasswordForm(UserForm):

    oldpassword = forms.CharField(label=_("Current Password"), widget=forms.PasswordInput(render_value=False))
    password1 = forms.CharField(label=_("New Password"), widget=forms.PasswordInput(render_value=False))
    password2 = forms.CharField(label=_("New Password (again)"), widget=forms.PasswordInput(render_value=False))

    def clean_oldpassword(self):
        if not self.user.check_password(self.cleaned_data.get("oldpassword")):
            raise forms.ValidationError(_("Please type your current password."))
        return self.cleaned_data["oldpassword"]

    def clean_password2(self):
        if "password1" in self.cleaned_data and "password2" in self.cleaned_data:
            if self.cleaned_data["password1"] != self.cleaned_data["password2"]:
                raise forms.ValidationError(_("You must type the same password each time."))
        return self.cleaned_data["password2"]

    def save(self):
        self.user.set_password(self.cleaned_data['password1'])
        self.user.save()
        self.user.message_set.create(message=ugettext(u"Password successfully changed."))


class SetPasswordForm(UserForm):

    password1 = forms.CharField(label=_("Password"), widget=forms.PasswordInput(render_value=False))
    password2 = forms.CharField(label=_("Password (again)"), widget=forms.PasswordInput(render_value=False))

    def clean_password2(self):
        if "password1" in self.cleaned_data and "password2" in self.cleaned_data:
            if self.cleaned_data["password1"] != self.cleaned_data["password2"]:
                raise forms.ValidationError(_("You must type the same password each time."))
        return self.cleaned_data["password2"]

    def save(self):
        self.user.set_password(self.cleaned_data["password1"])
        self.user.save()
        self.user.message_set.create(message=ugettext(u"Password successfully set."))


class ResetPasswordForm(forms.Form):

    email = forms.EmailField(label=_("Email"), required=True, widget=forms.TextInput(attrs={'size':'30'}))

    def clean_email(self):
        if EmailAddress.objects.filter(email__iexact=self.cleaned_data["email"], verified=True).count() == 0:
            raise forms.ValidationError(_("Email address not verified for any user account"))
        return self.cleaned_data["email"]

    def save(self):
        for user in User.objects.filter(email__iexact=self.cleaned_data["email"]):

            temp_key = sha_constructor("%s%s%s" % (
                settings.SECRET_KEY,
                user.email,
                settings.SECRET_KEY,
            )).hexdigest()

            # save it to the password reset model
            password_reset = PasswordReset(user=user, temp_key=temp_key)
            password_reset.save()

            current_site = Site.objects.get_current()
            domain = unicode(current_site.domain)

            #send the password reset email
            subject = _("Password reset email sent")
            message = render_to_string("account/password_reset_key_message.txt", {
                "user": user,
                "temp_key": temp_key,
                "domain": domain,
            })

            send_mail(subject, message, settings.DEFAULT_FROM_EMAIL, [user.email], priority="high")
        return self.cleaned_data["email"]


class ResetPasswordKeyForm(forms.Form):

    password1 = forms.CharField(label=_("New Password"), widget=forms.PasswordInput(render_value=False))
    password2 = forms.CharField(label=_("New Password (again)"), widget=forms.PasswordInput(render_value=False))
    temp_key = forms.CharField(widget=forms.HiddenInput)

    def clean_temp_key(self):
        temp_key = self.cleaned_data.get("temp_key")
        if PasswordReset.objects.filter(temp_key=temp_key, reset=False).count() == 0:
            raise forms.ValidationError(_("Temporary key is invalid."))
        return temp_key

    def clean_password2(self):
        if "password1" in self.cleaned_data and "password2" in self.cleaned_data:
            if self.cleaned_data["password1"] != self.cleaned_data["password2"]:
                raise forms.ValidationError(_("You must type the same password each time."))
        return self.cleaned_data["password2"]

    def save(self):
        # get the password_reset object
        temp_key = self.cleaned_data.get("temp_key")
        reset_records = PasswordReset.objects.filter(temp_key__exact=temp_key)
        if len(reset_records) > 0:
            password_reset = reset_records[0]
        else:
            raise forms.ValidationError(_("Could not find that password reset key"))

        # now set the new user password
        user = User.objects.get(passwordreset__exact=password_reset)
        user.set_password(self.cleaned_data["password1"])
        user.save()
        user.message_set.create(message=ugettext(u"Password successfully changed."))

        # change all the password reset records to this person to be true.
        for password_reset in PasswordReset.objects.filter(user=user):
            password_reset.reset = True
            password_reset.save()


class ChangeTimezoneForm(AccountForm):

    timezone = TimeZoneField(label=_("Timezone"), required=True)

    def __init__(self, *args, **kwargs):
        super(ChangeTimezoneForm, self).__init__(*args, **kwargs)
        self.initial.update({"timezone": self.account.timezone})

    def save(self):
        self.account.timezone = self.cleaned_data["timezone"]
        self.account.save()
        self.user.message_set.create(message=ugettext(u"Timezone successfully updated."))


class ChangeLanguageForm(AccountForm):

    language = forms.ChoiceField(label=_("Language"), required=True, choices=settings.LANGUAGES)

    def __init__(self, *args, **kwargs):
        super(ChangeLanguageForm, self).__init__(*args, **kwargs)
        self.initial.update({"language": self.account.language})

    def save(self):
        self.account.language = self.cleaned_data["language"]
        self.account.save()
        self.user.message_set.create(message=ugettext(u"Language successfully updated."))


# @@@ these should somehow be moved out of account or at least out of this module

from account.models import OtherServiceInfo, other_service, update_other_services

class TwitterForm(UserForm):
    username = forms.CharField(label=_("Username"), required=True)
    password = forms.CharField(label=_("Password"), required=True,
                               widget=forms.PasswordInput(render_value=False))

    def __init__(self, *args, **kwargs):
        super(TwitterForm, self).__init__(*args, **kwargs)
        self.initial.update({"username": other_service(self.user, "twitter_user")})

    def save(self):
        from microblogging.utils import get_twitter_password
        update_other_services(self.user,
            twitter_user = self.cleaned_data['username'],
            twitter_password = get_twitter_password(settings.SECRET_KEY, self.cleaned_data['password']),
        )
        self.user.message_set.create(message=ugettext(u"Successfully authenticated."))

########NEW FILE########
__FILENAME__ = middleware
import re

from django.utils.cache import patch_vary_headers
from django.utils import translation
from django.utils.http import urlquote
from django.contrib.auth import REDIRECT_FIELD_NAME
from django.conf import settings
from django.http import HttpResponseRedirect

from account.models import Account


class LocaleMiddleware(object):
    """
    This is a very simple middleware that parses a request
    and decides what translation object to install in the current
    thread context depending on the user's account. This allows pages
    to be dynamically translated to the language the user desires
    (if the language is available, of course).
    """

    def get_language_for_user(self, request):
        if request.user.is_authenticated():
            try:
                account = Account.objects.get(user=request.user)
                return account.language
            except Account.DoesNotExist:
                pass
        return translation.get_language_from_request(request)

    def process_request(self, request):
        translation.activate(self.get_language_for_user(request))
        request.LANGUAGE_CODE = translation.get_language()

    def process_response(self, request, response):
        patch_vary_headers(response, ('Accept-Language',))
        response['Content-Language'] = translation.get_language()
        translation.deactivate()
        return response


class AuthenticatedMiddleware(object):
    def __init__(self, login_url=None, redirect_field_name=REDIRECT_FIELD_NAME):
        if login_url is None:
            login_url = settings.LOGIN_URL
        self.redirect_field_name = redirect_field_name
        self.login_url = login_url
        self.exemptions = [
            r"^%s" % settings.MEDIA_URL,
            r"^%s" % settings.STATIC_URL,
            r"^%s$" % login_url,
        ] + getattr(settings, "AUTHENTICATED_EXEMPT_URLS", [])

    def process_request(self, request):
        for exemption in self.exemptions:
            if re.match(exemption, request.path):
                return None
        if not request.user.is_authenticated():
            path = urlquote(request.get_full_path())
            tup = (self.login_url, self.redirect_field_name, path)
            return HttpResponseRedirect("%s?%s=%s" % tup)

########NEW FILE########
__FILENAME__ = models
import sys

from datetime import datetime

from django.db import models
from django.conf import settings
from django.contrib.auth.models import User, AnonymousUser
from django.db.models.signals import post_save
from django.utils.translation import get_language_from_request, ugettext_lazy as _

from emailconfirmation.models import EmailAddress, EmailConfirmation

from timezones.fields import TimeZoneField
from emailconfirmation.signals import email_confirmed

class Account(models.Model):

    user = models.ForeignKey(User, unique=True, verbose_name=_('user'))

    timezone = TimeZoneField(_('timezone'))
    language = models.CharField(_('language'), max_length=10, choices=settings.LANGUAGES, default=settings.LANGUAGE_CODE)

    def __unicode__(self):
        return self.user.username


class OtherServiceInfo(models.Model):

    # eg blogrss, twitter_user, twitter_password

    user = models.ForeignKey(User, verbose_name=_('user'))
    key = models.CharField(_('Other Service Info Key'), max_length=50)
    value = models.TextField(_('Other Service Info Value'))

    class Meta:
        unique_together = [('user', 'key')]

    def __unicode__(self):
        return u"%s for %s" % (self.key, self.user)

def other_service(user, key, default_value=""):
    """
    retrieve the other service info for given key for the given user.

    return default_value ("") if no value.
    """
    try:
        value = OtherServiceInfo.objects.get(user=user, key=key).value
    except OtherServiceInfo.DoesNotExist:
        value = default_value
    return value

def update_other_services(user, **kwargs):
    """
    update the other service info for the given user using the given keyword args.

    e.g. update_other_services(user, twitter_user=..., twitter_password=...)
    """
    for key, value in kwargs.items():
        info, created = OtherServiceInfo.objects.get_or_create(user=user, key=key)
        info.value = value
        info.save()

def create_account(sender, instance=None, **kwargs):
    if instance is None:
        return
    account, created = Account.objects.get_or_create(user=instance)

post_save.connect(create_account, sender=User)


# @@@ move to emailconfirmation app?
def superuser_email_address(sender, instance=None, **kwargs):
    if instance is None:
        return
    # only run when we are in syncdb or createsuperuser to be as unobstrusive
    # as possible and reduce the risk of running at inappropriate times
    if "syncdb" in sys.argv or "createsuperuser" in sys.argv:
        defaults = {
            "user": instance,
            "verified": True,
            "primary": True,
        }
        EmailAddress.objects.get_or_create(email=instance.email, **defaults)

post_save.connect(superuser_email_address, sender=User)


class AnonymousAccount(object):
    def __init__(self, request=None):
        self.user = AnonymousUser()
        self.timezone = settings.TIME_ZONE
        if request is not None:
            self.language = get_language_from_request(request)
        else:
            self.language = settings.LANGUAGE_CODE

    def __unicode__(self):
        return "AnonymousAccount"


class PasswordReset(models.Model):

    user = models.ForeignKey(User, verbose_name=_('user'))

    temp_key = models.CharField(_('temp_key'), max_length=100)
    timestamp = models.DateTimeField(_('timestamp'), default=datetime.now)
    reset = models.BooleanField(_('reset yet?'), default=False)

    def __unicode__(self):
        return "%s (key=%s, reset=%r)" % (self.user.username, self.temp_key, self.reset)


def mark_user_active(sender, instance=None, **kwargs):
    user = kwargs.get("email_address").user
    user.is_active = True
    user.save()

email_confirmed.connect(mark_user_active, sender=EmailConfirmation)

########NEW FILE########
__FILENAME__ = openid_consumer
import urlparse

from openid import oidutil

from django.core.urlresolvers import reverse
from django.http import HttpResponseRedirect
from django.template import RequestContext
from django.shortcuts import render_to_response
from django.conf import settings

from django_openid.registration import RegistrationConsumer

from account.forms import OpenIDSignupForm
from account.utils import get_default_redirect
from account.views import login as account_login


# install our own logger that does nothing
def dummy_log(*args, **kwargs):
    return
oidutil.log = dummy_log


class PinaxConsumer(RegistrationConsumer):

    # Pinax does its own e-mail confirmation handling, but django-openid
    # wants to do its own handling of this so we turn it off in all cases
    confirm_email_addresses = False

    def on_registration_complete(self, request):
        return HttpResponseRedirect(get_default_redirect(request))

    def show_i_have_logged_you_in(self, request):
        return HttpResponseRedirect(get_default_redirect(request))

    def get_registration_form_class(self, request):
        return OpenIDSignupForm

    def do_register(self, request, *args, **kwargs):
        """
        A small wrapper around django_openid's implementation of registration
        that redirects back to a certain URL if there's no openid_url in the
        POST body.
        """

        openid_url = request.POST.get('openid_url')
        openids = request.session.get('openids')

        if not openid_url and not openids:
            return account_login(request, url_required=True, extra_context={
                "openid_login": True,
            })

        # perform OpenID login if openid_url is defined. we do this now (as
        # opposed to letting super(...).do_register() handle it) to allow
        # users who have existing OpenID association to login even when
        # ACCOUNT_OPEN_SIGNUP is turned off
        if openid_url:
            return self.start_openid_process(request,
                user_url = openid_url,
                on_complete_url = urlparse.urljoin(
                    request.path, '../register_complete/'
                ),
                trust_root = urlparse.urljoin(request.path, '..')
            )

        if not settings.ACCOUNT_OPEN_SIGNUP:
            return render_to_response("django_openid/registration_closed.html", {
            }, context_instance=RequestContext(request))

        return super(PinaxConsumer, self).do_register(request, *args, **kwargs)

    def show_already_signed_in(self, request):
        return render_to_response("django_openid/already_logged_in.html", {
        }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = openid_tags
import os

from django import template
from django.utils.safestring import mark_safe
from django.utils.translation import ugettext
from django.conf import settings

from django_openid.models import UserOpenidAssociation

try:
    any
except NameError:
    def any(seq):
        for x in seq:
            if x:
                return True
        return False

register = template.Library()

def openid_icon(openid, user):
    oid = u'%s' % openid
    matches = [u.openid == oid for u in UserOpenidAssociation.objects.filter(user=user)]
    if any(matches):
        return mark_safe(u'<img src="%s" alt="%s" />' % (
            os.path.join(settings.SSL_STATIC_URL, 'images', 'openid-icon.png'),
            ugettext('Logged in with OpenID')
        ))
    else:
        return u''
register.simple_tag(openid_icon)

########NEW FILE########
__FILENAME__ = other_service_tags
import re

from django import template
from account.models import other_service

register = template.Library()

class OtherServiceNode(template.Node):
    def __init__(self, user, key, asvar):
        self.user = user
        self.key = key
        self.asvar = asvar

    def render(self, context):
        user = self.user.resolve(context)
        key = self.key
        value = other_service(user, key)

        if self.asvar:
            context[self.asvar] = value
            return ''
        else:
            return value


@register.tag(name='other_service')
def other_service_tag(parser, token):
    bits = token.split_contents()
    if len(bits) == 3: # {% other_service user key %}
        user = parser.compile_filter(bits[1])
        key = bits[2]
        asvar = None
    elif len(bits) == 5: # {% other_service user key as var %}
        if bits[3] != "as":
            raise template.TemplateSyntaxError("3rd argument to %s should be 'as'" % bits[0])
        user = parser.compile_filter(bits[1])
        key = bits[2]
        asvar = bits[4]
    else:
        raise template.TemplateSyntaxError("wrong number of arguments to %s" % bits[0])

    return OtherServiceNode(user, key, asvar)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from account.forms import *

urlpatterns = patterns('',
    url(r'^email/$', 'account.views.email', name="acct_email"),
    url(r'^signup/$', 'account.views.signup', name="acct_signup"),
    url(r'^login/$', 'account.views.login', name="acct_login"),
    url(r'^login/openid/$', 'account.views.login', {'associate_openid': True},
        name="acct_login_openid"),
    url(r'^password_change/$', 'account.views.password_change', name="acct_passwd"),
    url(r'^password_set/$', 'account.views.password_set', name="acct_passwd_set"),
    url(r'^password_delete/$', 'account.views.password_delete', name="acct_passwd_delete"),
    url(r'^password_delete/done/$', 'django.views.generic.simple.direct_to_template', {
        "template": "account/password_delete_done.html",
    }, name="acct_passwd_delete_done"),
    url(r'^password_reset/$', 'account.views.password_reset', name="acct_passwd_reset"),
    url(r'^timezone/$', 'account.views.timezone_change', name="acct_timezone_change"),
    url(r'^other_services/$', 'account.views.other_services', name="acct_other_services"),
    url(r'^other_services/remove/$', 'account.views.other_services_remove', name="acct_other_services_remove"),

    url(r'^language/$', 'account.views.language_change', name="acct_language_change"),
    url(r'^logout/$', 'django.contrib.auth.views.logout', {"template_name": "account/logout.html"}, name="acct_logout"),

    url(r'^confirm_email/(\w+)/$', 'emailconfirmation.views.confirm_email', name="acct_confirm_email"),

    # Setting the permanent password after getting a key by email
    url(r'^password_reset_key/(\w+)/$', 'account.views.password_reset_from_key', name="acct_passwd_reset_key"),

    # ajax validation
    (r'^validate/$', 'ajax_validation.views.validate', {'form_class': SignupForm}, 'signup_form_validate'),
)

########NEW FILE########
__FILENAME__ = utils
from django.conf import settings
from django.core.urlresolvers import reverse

LOGIN_REDIRECT_URLNAME = getattr(settings, "LOGIN_REDIRECT_URLNAME", '')
import logging

logger = logging.getLogger(__name__)
def get_default_redirect(request, redirect_field_name="next",
        login_redirect_urlname=LOGIN_REDIRECT_URLNAME, default_redirect_to=None):
    """
    Returns the URL to be used in login procedures by looking at different
    values in the following order:

    - LOGIN_REDIRECT_URLNAME - the name of a URLconf entry in the settings
    - LOGIN_REDIRECT_URL - the URL in the setting
    - a REQUEST value, GET or POST, named "next" by default.
    """
    if default_redirect_to == None:
        if login_redirect_urlname:
            default_redirect_to = reverse(login_redirect_urlname)
        else:
            default_redirect_to = settings.LOGIN_REDIRECT_URL
    redirect_to = request.GET.get(redirect_field_name)
    if not redirect_to:
        redirect_to = request.POST.get(redirect_field_name)
        
    logger.debug("=> %s, %s, %s" % (default_redirect_to, redirect_to, redirect_field_name))
    
    
    
    # light security check -- make sure redirect_to isn't garabage.
    if (not redirect_to) or ("://" in redirect_to) or (" " in redirect_to):
        redirect_to = default_redirect_to
    return redirect_to

########NEW FILE########
__FILENAME__ = views
from django.conf import settings
from django.shortcuts import render_to_response
from django.http import HttpResponseRedirect, HttpResponseForbidden, Http404
from django.db.models import Q
from django.contrib.auth import authenticate
from django.contrib.auth import login as auth_login
from django.template import RequestContext
from django.utils.translation import ugettext, ugettext_lazy as _
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.db import models

from projects.limits import personal_ssl_access

from account.utils import get_default_redirect
from account.models import OtherServiceInfo
from account.forms import SignupForm, AddEmailForm, LoginForm, \
    ChangePasswordForm, SetPasswordForm, ResetPasswordForm, \
    ChangeTimezoneForm, ChangeLanguageForm, TwitterForm, ResetPasswordKeyForm
from emailconfirmation.models import EmailAddress, EmailConfirmation

import logging

logger = logging.getLogger(__name__)

association_model = models.get_model('django_openid', 'Association')
if association_model is not None:
    from django_openid.models import UserOpenidAssociation

def login(request, form_class=LoginForm, template_name="account/login.html",
          success_url=None, associate_openid=False, openid_success_url=None,
          url_required=False, extra_context=None):
    if extra_context is None:
        extra_context = {}
    if success_url is None:
        success_url = get_default_redirect(request)
    if request.method == "POST" and not url_required:
        form = form_class(request.POST)
        if form.login(request):
            if associate_openid and association_model is not None:
                for openid in request.session.get('openids', []):
                    assoc, created = UserOpenidAssociation.objects.get_or_create(
                        user=form.user, openid=openid.openid
                    )
                success_url = openid_success_url or success_url
                
            # Redirect to http or https based on account
            if personal_ssl_access.increaseAllowed(user=form.user):
                success_url = "%s%s" % (settings.SSL_BASE_URL, success_url)
                logger.debug("Subscriber %s" % success_url)
            else:
                success_url = "%s%s" % (settings.BASE_URL, success_url)                
                logger.debug("NonSubscriber %s" % success_url)
                    
            return HttpResponseRedirect(success_url)
    else:
        form = form_class()
    ctx = {
        "form": form,
        "url_required": url_required,
        "success_url": success_url
    }
    ctx.update(extra_context)
    return render_to_response(template_name, ctx,
        context_instance = RequestContext(request)
    )

def signup(request, form_class=SignupForm,
        template_name="account/signup.html", success_url=None):


    success_url = get_default_redirect(request, default_redirect_to=success_url)
        
        
    if request.method == "POST":
        form = form_class(request.POST)
        if form.is_valid():
            username, password = form.save()
            if settings.ACCOUNT_EMAIL_VERIFICATION:
                return render_to_response("account/verification_sent.html", {
                    "email": form.cleaned_data["email"],
                }, context_instance=RequestContext(request))
            else:
                user = authenticate(username=username, password=password)
                auth_login(request, user)
                return HttpResponseRedirect(success_url)
    else:
        form = form_class()
    return render_to_response(template_name, {
        "form": form,
        "success_url":success_url
    }, context_instance=RequestContext(request))

@login_required
def email(request, form_class=AddEmailForm,
        template_name="account/email.html"):
    if request.method == "POST" and request.user.is_authenticated():
        if request.POST["action"] == "add":
            add_email_form = form_class(request.user, request.POST)
            if add_email_form.is_valid():
                add_email_form.save()
                add_email_form = form_class() # @@@
        else:
            add_email_form = form_class()
            if request.POST["action"] == "send":
                email = request.POST["email"]
                try:
                    email_address = EmailAddress.objects.get(
                        user=request.user,
                        email=email,
                    )
                    request.user.message_set.create(
                        message=_("Confirmation email sent to %(email)s") % {
                            'email': email,
                        })
                    EmailConfirmation.objects.send_confirmation(email_address)
                except EmailAddress.DoesNotExist:
                    pass
            elif request.POST["action"] == "remove":
                email = request.POST["email"]
                try:
                    email_address = EmailAddress.objects.get(
                        user=request.user,
                        email=email
                    )
                    email_address.delete()
                    request.user.message_set.create(
                        message=_("Removed email address %(email)s") % {
                            'email': email,
                        })
                except EmailAddress.DoesNotExist:
                    pass
            elif request.POST["action"] == "primary":
                email = request.POST["email"]
                email_address = EmailAddress.objects.get(
                    user=request.user,
                    email=email,
                )
                email_address.set_as_primary()
    else:
        add_email_form = form_class()
    return render_to_response(template_name, {
        "add_email_form": add_email_form,
    }, context_instance=RequestContext(request))

@login_required
def password_change(request, form_class=ChangePasswordForm,
        template_name="account/password_change.html"):
    if not request.user.password:
        return HttpResponseRedirect(reverse("acct_passwd_set"))
    if request.method == "POST":
        password_change_form = form_class(request.user, request.POST)
        if password_change_form.is_valid():
            password_change_form.save()
            password_change_form = form_class(request.user)
    else:
        password_change_form = form_class(request.user)
    return render_to_response(template_name, {
        "password_change_form": password_change_form,
    }, context_instance=RequestContext(request))

@login_required
def password_set(request, form_class=SetPasswordForm,
        template_name="account/password_set.html"):
    if request.user.password:
        return HttpResponseRedirect(reverse("acct_passwd"))
    if request.method == "POST":
        password_set_form = form_class(request.user, request.POST)
        if password_set_form.is_valid():
            password_set_form.save()
            return HttpResponseRedirect(reverse("acct_passwd"))
    else:
        password_set_form = form_class(request.user)
    return render_to_response(template_name, {
        "password_set_form": password_set_form,
    }, context_instance=RequestContext(request))

@login_required
def password_delete(request, template_name="account/password_delete.html"):
    # prevent this view when openids is not present or it is empty.
    if not request.user.password or \
        (not hasattr(request, "openids") or \
            not getattr(request, "openids", None)):
        return HttpResponseForbidden()
    if request.method == "POST":
        request.user.password = u""
        request.user.save()
        return HttpResponseRedirect(reverse("acct_passwd_delete_done"))
    return render_to_response(template_name, {
    }, context_instance=RequestContext(request))

def password_reset(request, form_class=ResetPasswordForm,
        template_name="account/password_reset.html",
        template_name_done="account/password_reset_done.html"):
    if request.method == "POST":
        password_reset_form = form_class(request.POST)
        if password_reset_form.is_valid():
            email = password_reset_form.save()
            return render_to_response(template_name_done, {
                "email": email,
            }, context_instance=RequestContext(request))
    else:
        password_reset_form = form_class()

    return render_to_response(template_name, {
        "password_reset_form": password_reset_form,
    }, context_instance=RequestContext(request))

def password_reset_from_key(request, key, form_class=ResetPasswordKeyForm,
        template_name="account/password_reset_from_key.html"):
    if request.method == "POST":
        password_reset_key_form = form_class(request.POST)
        if password_reset_key_form.is_valid():
            password_reset_key_form.save()
            password_reset_key_form = None
    else:
        password_reset_key_form = form_class(initial={"temp_key": key})

    return render_to_response(template_name, {
        "form": password_reset_key_form,
    }, context_instance=RequestContext(request))

@login_required
def timezone_change(request, form_class=ChangeTimezoneForm,
        template_name="account/timezone_change.html"):
    if request.method == "POST":
        form = form_class(request.user, request.POST)
        if form.is_valid():
            form.save()
    else:
        form = form_class(request.user)
    return render_to_response(template_name, {
        "form": form,
    }, context_instance=RequestContext(request))

@login_required
def language_change(request, form_class=ChangeLanguageForm,
        template_name="account/language_change.html"):
    if request.method == "POST":
        form = form_class(request.user, request.POST)
        if form.is_valid():
            form.save()
            next = request.META.get('HTTP_REFERER', None)
            return HttpResponseRedirect(next)
    else:
        form = form_class(request.user)
    return render_to_response(template_name, {
        "form": form,
    }, context_instance=RequestContext(request))

@login_required
def other_services(request, template_name="account/other_services.html"):
    from microblogging.utils import twitter_verify_credentials
    twitter_form = TwitterForm(request.user)
    twitter_authorized = False
    if request.method == "POST":
        twitter_form = TwitterForm(request.user, request.POST)

        if request.POST['actionType'] == 'saveTwitter':
            if twitter_form.is_valid():
                from microblogging.utils import twitter_account_raw
                twitter_account = twitter_account_raw(
                    request.POST['username'], request.POST['password'])
                twitter_authorized = twitter_verify_credentials(
                    twitter_account)
                if not twitter_authorized:
                    request.user.message_set.create(
                        message=ugettext("Twitter authentication failed"))
                else:
                    twitter_form.save()
    else:
        from microblogging.utils import twitter_account_for_user
        twitter_account = twitter_account_for_user(request.user)
        twitter_authorized = twitter_verify_credentials(twitter_account)
        twitter_form = TwitterForm(request.user)
    return render_to_response(template_name, {
        "twitter_form": twitter_form,
        "twitter_authorized": twitter_authorized,
    }, context_instance=RequestContext(request))

@login_required
def other_services_remove(request):
    # TODO: this is a bit coupled.
    OtherServiceInfo.objects.filter(user=request.user).filter(
        Q(key="twitter_user") | Q(key="twitter_password")
    ).delete()
    request.user.message_set.create(message=ugettext("Removed twitter account information successfully."))
    return HttpResponseRedirect(reverse("acct_other_services"))

########NEW FILE########
__FILENAME__ = remove_activitiy_tables
from django_evolution.mutations import *
from django.db import models

MUTATIONS = [     
     DeleteModel('TextActivity'),
     DeleteModel('StoryActivity'),
     DeleteModel('CommentActivity'),
     DeleteModel('PointsChangeActivity'),
     DeleteModel('IterationActivity'),
     DeleteModel('DeletedActivity'),
     DeleteModel('Activity'),
     DeleteModel('ActivityAction'),     
]
########NEW FILE########
__FILENAME__ = feedgenerator
"""
Syndication feed generation library -- used for generating RSS, etc.

Sample usage:

>>> from django.utils import feedgenerator
>>> feed = feedgenerator.Rss201rev2Feed(
...     title=u"Poynter E-Media Tidbits",
...     link=u"http://www.poynter.org/column.asp?id=31",
...     description=u"A group weblog by the sharpest minds in online media/journalism/publishing.",
...     language=u"en",
... )
>>> feed.add_item(title="Hello", link=u"http://www.holovaty.com/test/", description="Testing.")
>>> fp = open('test.rss', 'w')
>>> feed.write(fp, 'utf-8')
>>> fp.close()

For definitions of the different versions of RSS, see:
http://diveintomark.org/archives/2004/02/04/incompatible-rss
"""

import re
import datetime
from django.utils.xmlutils import SimplerXMLGenerator
from django.utils.encoding import force_unicode, iri_to_uri

def rfc2822_date(date):
    # We do this ourselves to be timezone aware, email.Utils is not tz aware.
    if date.tzinfo:
        time_str = date.strftime('%a, %d %b %Y %H:%M:%S ')
        offset = date.tzinfo.utcoffset(date)
        timezone = (offset.days * 24 * 60) + (offset.seconds / 60)
        hour, minute = divmod(timezone, 60)
        return time_str + "%+03d%02d" % (hour, minute)
    else:
        return date.strftime('%a, %d %b %Y %H:%M:%S -0000')

def rfc3339_date(date):
    if date.tzinfo:
        time_str = date.strftime('%Y-%m-%dT%H:%M:%S')
        offset = date.tzinfo.utcoffset(date)
        timezone = (offset.days * 24 * 60) + (offset.seconds / 60)
        hour, minute = divmod(timezone, 60)
        return time_str + "%+03d:%02d" % (hour, minute)
    else:
        return date.strftime('%Y-%m-%dT%H:%M:%SZ')

def get_tag_uri(url, date):
    "Creates a TagURI. See http://diveintomark.org/archives/2004/05/28/howto-atom-id"
    tag = re.sub('^http://', '', url)
    if date is not None:
        tag = re.sub('/', ',%s:/' % date.strftime('%Y-%m-%d'), tag, 1)
    tag = re.sub('#', '/', tag)
    return u'tag:' + tag

class SyndicationFeed(object):
    "Base class for all syndication feeds. Subclasses should provide write()"
    def __init__(self, title, link, description, language=None, author_email=None,
            author_name=None, author_link=None, subtitle=None, categories=None,
            feed_url=None, feed_copyright=None, feed_guid=None, ttl=None, **kwargs):
        to_unicode = lambda s: force_unicode(s, strings_only=True)
        if categories:
            categories = [force_unicode(c) for c in categories]
        self.feed = {
            'title': to_unicode(title),
            'link': iri_to_uri(link),
            'description': to_unicode(description),
            'language': to_unicode(language),
            'author_email': to_unicode(author_email),
            'author_name': to_unicode(author_name),
            'author_link': iri_to_uri(author_link),
            'subtitle': to_unicode(subtitle),
            'categories': categories or (),
            'feed_url': iri_to_uri(feed_url),
            'feed_copyright': to_unicode(feed_copyright),
            'id': feed_guid or link,
            'ttl': ttl,
        }
        self.feed.update(kwargs)
        self.items = []

    def add_item(self, title, link, description, author_email=None,
        author_name=None, author_link=None, pubdate=None, comments=None,
        unique_id=None, enclosure=None, categories=(), item_copyright=None,
        ttl=None, **kwargs):
        """
        Adds an item to the feed. All args are expected to be Python Unicode
        objects except pubdate, which is a datetime.datetime object, and
        enclosure, which is an instance of the Enclosure class.
        """
        to_unicode = lambda s: force_unicode(s, strings_only=True)
        if categories:
            categories = [to_unicode(c) for c in categories]
        item = {
            'title': to_unicode(title),
            'link': iri_to_uri(link),
            'description': to_unicode(description),
            'author_email': to_unicode(author_email),
            'author_name': to_unicode(author_name),
            'author_link': iri_to_uri(author_link),
            'pubdate': pubdate,
            'comments': to_unicode(comments),
            'unique_id': to_unicode(unique_id),
            'enclosure': enclosure,
            'categories': categories or (),
            'item_copyright': to_unicode(item_copyright),
            'ttl': ttl,
        }
        item.update(kwargs)
        self.items.append(item)

    def num_items(self):
        return len(self.items)

    def root_attributes(self):
        """
        Return extra attributes to place on the root (i.e. feed/channel) element.
        Called from write().
        """
        return {}

    def add_root_elements(self, handler):
        """
        Add elements in the root (i.e. feed/channel) element. Called
        from write().
        """
        pass

    def item_attributes(self, item):
        """
        Return extra attributes to place on each item (i.e. item/entry) element.
        """
        return {}

    def add_item_elements(self, handler, item):
        """
        Add elements on each item (i.e. item/entry) element.
        """
        pass

    def write(self, outfile, encoding):
        """
        Outputs the feed in the given encoding to outfile, which is a file-like
        object. Subclasses should override this.
        """
        raise NotImplementedError

    def writeString(self, encoding):
        """
        Returns the feed in the given encoding as a string.
        """
        from StringIO import StringIO
        s = StringIO()
        self.write(s, encoding)
        return s.getvalue()

    def latest_post_date(self):
        """
        Returns the latest item's pubdate. If none of them have a pubdate,
        this returns the current date/time.
        """
        updates = [i['pubdate'] for i in self.items if i['pubdate'] is not None]
        if len(updates) > 0:
            updates.sort()
            return updates[-1]
        else:
            return datetime.datetime.now()

class Enclosure(object):
    "Represents an RSS enclosure"
    def __init__(self, url, length, mime_type):
        "All args are expected to be Python Unicode objects"
        self.length, self.mime_type = length, mime_type
        self.url = iri_to_uri(url)

class RssFeed(SyndicationFeed):
    mime_type = 'application/rss+xml'
    def write(self, outfile, encoding):
        handler = SimplerXMLGenerator(outfile, encoding)
        handler.startDocument()
        handler.startElement(u"rss", self.rss_attributes())
        handler.startElement(u"channel", self.root_attributes())
        self.add_root_elements(handler)
        self.write_items(handler)
        self.endChannelElement(handler)
        handler.endElement(u"rss")

    def rss_attributes(self):
        return {u"version": self._version}

    def write_items(self, handler):
        for item in self.items:
            handler.startElement(u'item', self.item_attributes(item))
            self.add_item_elements(handler, item)
            handler.endElement(u"item")

    def add_root_elements(self, handler):
        handler.addQuickElement(u"title", self.feed['title'])
        handler.addQuickElement(u"link", self.feed['link'])
        handler.addQuickElement(u"description", self.feed['description'])
        if self.feed['language'] is not None:
            handler.addQuickElement(u"language", self.feed['language'])
        for cat in self.feed['categories']:
            handler.addQuickElement(u"category", cat)
        if self.feed['feed_copyright'] is not None:
            handler.addQuickElement(u"copyright", self.feed['feed_copyright'])
        handler.addQuickElement(u"lastBuildDate", rfc2822_date(self.latest_post_date()).decode('utf-8'))
        if self.feed['ttl'] is not None:
            handler.addQuickElement(u"ttl", self.feed['ttl'])

    def endChannelElement(self, handler):
        handler.endElement(u"channel")

class RssUserland091Feed(RssFeed):
    _version = u"0.91"
    def add_item_elements(self, handler, item):
        handler.addQuickElement(u"title", item['title'])
        handler.addQuickElement(u"link", item['link'])
        if item['description'] is not None:
            handler.addQuickElement(u"description", item['description'])

class Rss201rev2Feed(RssFeed):
    # Spec: http://blogs.law.harvard.edu/tech/rss
    _version = u"2.0"
    def add_item_elements(self, handler, item):
        handler.addQuickElement(u"title", item['title'])
        handler.addQuickElement(u"link", item['link'])
        if item['description'] is not None:
            handler.addQuickElement(u"description", item['description'])

        # Author information.
        if item["author_name"] and item["author_email"]:
            handler.addQuickElement(u"author", "%s (%s)" % \
                (item['author_email'], item['author_name']))
        elif item["author_email"]:
            handler.addQuickElement(u"author", item["author_email"])
        elif item["author_name"]:
            handler.addQuickElement(u"dc:creator", item["author_name"], {"xmlns:dc": u"http://purl.org/dc/elements/1.1/"})

        if item['pubdate'] is not None:
            handler.addQuickElement(u"pubDate", rfc2822_date(item['pubdate']).decode('utf-8'))
        if item['comments'] is not None:
            handler.addQuickElement(u"comments", item['comments'])
        if item['unique_id'] is not None:
            handler.addQuickElement(u"guid", item['unique_id'], {u"isPermaLink": "false"})

        if item['ttl'] is not None:
            handler.addQuickElement(u"ttl", item['ttl'])

        # Enclosure.
        if item['enclosure'] is not None:
            handler.addQuickElement(u"enclosure", '',
                {u"url": item['enclosure'].url, u"length": item['enclosure'].length,
                    u"type": item['enclosure'].mime_type})

        # Categories.
        for cat in item['categories']:
            handler.addQuickElement(u"category", cat)

class Atom1Feed(SyndicationFeed):
    # Spec: http://atompub.org/2005/07/11/draft-ietf-atompub-format-10.html
    mime_type = 'application/atom+xml'
    ns = u"http://www.w3.org/2005/Atom"

    def write(self, outfile, encoding):
        handler = SimplerXMLGenerator(outfile, encoding)
        handler.startDocument()
        handler.startElement(u'feed', self.root_attributes())
        self.add_root_elements(handler)
        self.write_items(handler)
        handler.endElement(u"feed")

    def root_attributes(self):
        if self.feed['language'] is not None:
            return {u"xmlns": self.ns, u"xml:lang": self.feed['language']}
        else:
            return {u"xmlns": self.ns}

    def add_root_elements(self, handler):
        handler.addQuickElement(u"title", self.feed['title'])
        handler.addQuickElement(u"link", "", {u"rel": u"alternate", u"href": self.feed['link']})
        if self.feed['feed_url'] is not None:
            handler.addQuickElement(u"link", "", {u"rel": u"self", u"href": self.feed['feed_url']})
        handler.addQuickElement(u"id", self.feed['id'])
        handler.addQuickElement(u"updated", rfc3339_date(self.latest_post_date()).decode('utf-8'))
        if self.feed['author_name'] is not None:
            handler.startElement(u"author", {})
            handler.addQuickElement(u"name", self.feed['author_name'])
            if self.feed['author_email'] is not None:
                handler.addQuickElement(u"email", self.feed['author_email'])
            if self.feed['author_link'] is not None:
                handler.addQuickElement(u"uri", self.feed['author_link'])
            handler.endElement(u"author")
        if self.feed['subtitle'] is not None:
            handler.addQuickElement(u"subtitle", self.feed['subtitle'])
        for cat in self.feed['categories']:
            handler.addQuickElement(u"category", "", {u"term": cat})
        if self.feed['feed_copyright'] is not None:
            handler.addQuickElement(u"rights", self.feed['feed_copyright'])

    def write_items(self, handler):
        for item in self.items:
            handler.startElement(u"entry", self.item_attributes(item))
            self.add_item_elements(handler, item)
            handler.endElement(u"entry")

    def add_item_elements(self, handler, item):
        handler.addQuickElement(u"title", item['title'])
        handler.addQuickElement(u"link", u"", {u"href": item['link'], u"rel": u"alternate"})
        if item['pubdate'] is not None:
            handler.addQuickElement(u"updated", rfc3339_date(item['pubdate']).decode('utf-8'))

        # Author information.
        if item['author_name'] is not None:
            handler.startElement(u"author", {})
            handler.addQuickElement(u"name", item['author_name'])
            if item['author_email'] is not None:
                handler.addQuickElement(u"email", item['author_email'])
            if item['author_link'] is not None:
                handler.addQuickElement(u"uri", item['author_link'])
            handler.endElement(u"author")

        # Unique ID.
        if item['unique_id'] is not None:
            unique_id = item['unique_id']
        else:
            unique_id = get_tag_uri(item['link'], item['pubdate'])
        handler.addQuickElement(u"id", unique_id)

        # Summary.
        if item['description'] is not None:
            handler.addQuickElement(u"summary", item['description'], {u"type": u"html"})

        # Enclosure.
        if item['enclosure'] is not None:
            handler.addQuickElement(u"link", '',
                {u"rel": u"enclosure",
                 u"href": item['enclosure'].url,
                 u"length": item['enclosure'].length,
                 u"type": item['enclosure'].mime_type})

        # Categories.
        for cat in item['categories']:
            handler.addQuickElement(u"category", u"", {u"term": cat})

        # Rights.
        if item['item_copyright'] is not None:
            handler.addQuickElement(u"rights", item['item_copyright'])

# This isolates the decision of what the system default is, so calling code can
# do "feedgenerator.DefaultFeed" instead of "feedgenerator.Rss201rev2Feed".
DefaultFeed = Rss201rev2Feed

########NEW FILE########
__FILENAME__ = cleanact

from optparse import make_option
import datetime
from  django.core.management.base import BaseCommand
from activities.models import NewsItem

class Command(BaseCommand):
    option_list = BaseCommand.option_list + (
        make_option(
            '--purge', action='store_true', dest='purge', default=False,
            help='Generate evolutions to delete stale applications.'),
    )


    help = 'Purge old Activities'

    def handle(self, *app_labels, **options):
        self.purge(*app_labels, **options)

    def purge(self, *app_labels, **options):
        print 'Purging Activities'
        NewsItem.purgeOld(365)

########NEW FILE########
__FILENAME__ = daily_digest
import datetime
from django.core.management.base import BaseCommand
from django.contrib.auth.models import User
from django.template import loader, Context
from django.conf import settings
from activities.models import NewsItem

from mailer import send_mail
from django.core.mail import EmailMultiAlternatives

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *app_labels, **options):
        for user in User.objects.all():
            if user.email_subscriptions.count() > 0:
                try:
                    self.dailyDigest( user )
                except:
                    logger.error("Could not send daily digest to %s" % user)

    def dailyDigest( self, user ):
        logger.debug( "Sending daily digest to %s" % user )

        template = loader.get_template('activities/digest_header.html')
        context = Context( {"user":user, "site_name":settings.SITE_NAME } )
        body = template.render(context)
        domain = settings.BASE_URL

        email_address = user.email

        for sub in user.email_subscriptions.all():
            if not sub.project.active:
                continue
            logger.debug(sub)
            today = datetime.date.today()
            mdiff = datetime.timedelta(hours=-24)
            daterange = today + mdiff
            stories = {}
            news_items = NewsItem.objects.filter( project=sub.project, created__gte=daterange).order_by("-created")

            template = loader.get_template('activities/digest_project.html')
            context = Context( {"project":sub.project , "news_items":news_items, "domain":domain, 'email_address':email_address,"support_email":settings.CONTACT_EMAIL} )
            body = "%s %s" % (body, template.render(context))

        template = loader.get_template('activities/digest_footer.html')
        context = Context( {"user":user , "domain":domain, 'email_address':email_address,"support_email":settings.CONTACT_EMAIL} )
        body = "%s %s" % (body, template.render(context))

        subject, from_email, to = 'ScrumDo Daily Digest', 'noreply@scrumdo.com', email_address
        text_content = 'See html email...'
        html_content = body
        msg = EmailMultiAlternatives(subject, text_content, from_email, [to])
        msg.attach_alternative(html_content, "text/html")
        msg.send()


        # logger.debug(body)

########NEW FILE########
__FILENAME__ = fix_activity_types
# 
# 
# from optparse import make_option
# import datetime
# from  django.core.management.base import BaseCommand
# from activities.models import Activity
# 
# class Command(BaseCommand):
#     def handle(self, *app_labels, **options):
#         activities = Activity.objects.all()
#         for activity in activities:
#             activity = activity.mergeChildren()
#             activity.real_type = activity._get_real_type()
#             activity.save()

########NEW FILE########
__FILENAME__ = models
from django.db import models
from django.template.loader import render_to_string
from django.contrib.contenttypes.models import ContentType
from django.utils.translation import ugettext_lazy as _
from django.contrib.auth.models import  User
from django.core.cache import cache
import sys, traceback
from itertools import groupby

from scrumdo_model_utils.models import InheritanceCastModel

import datetime
import traceback
import logging

logger = logging.getLogger(__name__)

class ProjectEmailSubscription(models.Model):
    project = models.ForeignKey("projects.Project")
    user = models.ForeignKey(User,related_name="email_subscriptions")
    def __unicode__(self):
        return "Subscription: %s %s" % (self.user, self.project)

class NewsItem(models.Model):
    created = models.DateTimeField(_('created'), default=datetime.datetime.now)
    user = models.ForeignKey(User,related_name="newsItems", null=True, blank=True)
    project = models.ForeignKey("projects.Project", related_name="newsItems", null=True, blank=True)
    text = models.TextField()
    icon = models.CharField(max_length=24)
    feed_url = models.CharField(max_length=75, null=True, blank=True)
    @staticmethod
    def purgeOld(days=365):
        today = datetime.date.today()
        mdiff = datetime.timedelta(days=-days)
        date_30days_Agoago = today + mdiff
        NewsItem.objects.filter(created__lte=date_30days_Agoago).delete()
        
    class Meta:
        ordering = [ '-created' ]
########NEW FILE########
__FILENAME__ = fields
from datetime import datetime

from django.db import models
from django.conf import settings

from activities.scrumdo_model_utils import Choices

class AutoCreatedField(models.DateTimeField):
    """
    A DateTimeField that automatically populates itself at
    object creation.

    By default, sets editable=False, default=datetime.now.

    """
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('editable', False)
        kwargs.setdefault('default', datetime.now)
        super(AutoCreatedField, self).__init__(*args, **kwargs)


class AutoLastModifiedField(AutoCreatedField):
    """
    A DateTimeField that updates itself on each save() of the model.

    By default, sets editable=False and default=datetime.now.

    """
    def pre_save(self, model_instance, add):
        value = datetime.now()
        setattr(model_instance, self.attname, value)
        return value


class StatusField(models.CharField):
    """
    A CharField that looks for a ``STATUS`` class-attribute and
    automatically uses that as ``choices``. The first option in
    ``STATUS`` is set as the default.

    Also has a default max_length so you don't have to worry about
    setting that.

    Also features a ``no_check_for_status`` argument to make sure
    South can handle this field when it freezes a model.
    """
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('max_length', 100)
        self.check_for_status = not kwargs.pop('no_check_for_status', False)
        super(StatusField, self).__init__(*args, **kwargs)

    def contribute_to_class(self, cls, name):
        if not cls._meta.abstract and self.check_for_status:
            assert hasattr(cls, 'STATUS'), \
                "To use StatusField, the model '%s' must have a STATUS choices class attribute." \
                % cls.__name__
            setattr(self, '_choices', cls.STATUS)
            setattr(self, 'default', tuple(cls.STATUS)[0][0]) # sets first as default
        super(StatusField, self).contribute_to_class(cls, name)


class MonitorField(models.DateTimeField):
    """
    A DateTimeField that monitors another field on the same model and
    sets itself to the current date/time whenever the monitored field
    changes.

    """
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('default', datetime.now)
        monitor = kwargs.pop('monitor', None)
        if not monitor:
            raise TypeError(
                '%s requires a "monitor" argument' % self.__class__.__name__)
        self.monitor = monitor
        super(MonitorField, self).__init__(*args, **kwargs)

    def contribute_to_class(self, cls, name):
        self.monitor_attname = '_monitor_%s' % name
        models.signals.post_init.connect(self._save_initial, sender=cls)
        super(MonitorField, self).contribute_to_class(cls, name)

    def get_monitored_value(self, instance):
        return getattr(instance, self.monitor)

    def _save_initial(self, sender, instance, **kwargs):
        setattr(instance, self.monitor_attname,
                self.get_monitored_value(instance))

    def pre_save(self, model_instance, add):
        value = datetime.now()
        previous = getattr(model_instance, self.monitor_attname, None)
        current = self.get_monitored_value(model_instance)
        if previous != current:
            setattr(model_instance, self.attname, value)
            self._save_initial(model_instance.__class__, model_instance)
        return super(MonitorField, self).pre_save(model_instance, add)


SPLIT_MARKER = getattr(settings, 'SPLIT_MARKER', '<!-- split -->')

# the number of paragraphs after which to split if no marker
SPLIT_DEFAULT_PARAGRAPHS = getattr(settings, 'SPLIT_DEFAULT_PARAGRAPHS', 2)

_excerpt_field_name = lambda name: '_%s_excerpt' % name

def get_excerpt(content):
    excerpt = []
    default_excerpt = []
    paras_seen = 0
    for line in content.splitlines():
        if not line.strip():
            paras_seen += 1
        if paras_seen < SPLIT_DEFAULT_PARAGRAPHS:
            default_excerpt.append(line)
        if line.strip() == SPLIT_MARKER:
            return '\n'.join(excerpt)
        excerpt.append(line)

    return '\n'.join(default_excerpt)

class SplitText(object):
    def __init__(self, instance, field_name, excerpt_field_name):
        # instead of storing actual values store a reference to the instance
        # along with field names, this makes assignment possible
        self.instance = instance
        self.field_name = field_name
        self.excerpt_field_name = excerpt_field_name

    # content is read/write
    def _get_content(self):
        return self.instance.__dict__[self.field_name]
    def _set_content(self, val):
        setattr(self.instance, self.field_name, val)
    content = property(_get_content, _set_content)

    # excerpt is a read only property
    def _get_excerpt(self):
        return getattr(self.instance, self.excerpt_field_name)
    excerpt = property(_get_excerpt)

    # has_more is a boolean property
    def _get_has_more(self):
        return self.excerpt.strip() != self.content.strip()
    has_more = property(_get_has_more)

    # allows display via templates without .content necessary
    def __unicode__(self):
        return self.content

class SplitDescriptor(object):
    def __init__(self, field):
        self.field = field
        self.excerpt_field_name = _excerpt_field_name(self.field.name)

    def __get__(self, instance, owner):
        if instance is None:
            raise AttributeError('Can only be accessed via an instance.')
        content = instance.__dict__[self.field.name]
        if content is None:
            return None
        return SplitText(instance, self.field.name, self.excerpt_field_name)

    def __set__(self, obj, value):
        if isinstance(value, SplitText):
            obj.__dict__[self.field.name] = value.content
            setattr(obj, self.excerpt_field_name, value.excerpt)
        else:
            obj.__dict__[self.field.name] = value

class SplitField(models.TextField):
    def __init__(self, *args, **kwargs):
        # for South FakeORM compatibility: the frozen version of a
        # SplitField can't try to add an _excerpt field, because the
        # _excerpt field itself is frozen as well. See introspection
        # rules below.
        self.add_excerpt_field = not kwargs.pop('no_excerpt_field', False)
        super(SplitField, self).__init__(*args, **kwargs)

    def contribute_to_class(self, cls, name):
        if self.add_excerpt_field:
            excerpt_field = models.TextField(editable=False)
            excerpt_field.creation_counter = self.creation_counter+1
            cls.add_to_class(_excerpt_field_name(name), excerpt_field)
        super(SplitField, self).contribute_to_class(cls, name)
        setattr(cls, self.name, SplitDescriptor(self))

    def pre_save(self, model_instance, add):
        value = super(SplitField, self).pre_save(model_instance, add)
        excerpt = get_excerpt(value.content)
        setattr(model_instance, _excerpt_field_name(self.attname), excerpt)
        return value.content

    def value_to_string(self, obj):
        value = self._get_val_from_obj(obj)
        return value.content

    def get_prep_value(self, value):
        try:
            return value.content
        except AttributeError:
            return value


# allow South to handle these fields smoothly
try:
    from south.modelsinspector import add_introspection_rules
    # For a normal MarkupField, the add_excerpt_field attribute is
    # always True, which means no_excerpt_field arg will always be
    # True in a frozen MarkupField, which is what we want.
    add_introspection_rules(rules=[
        (
            (SplitField,),
            [],
            {'no_excerpt_field': ('add_excerpt_field', {})}
        ),
        (
            (MonitorField,),
            [],
            {'monitor': ('monitor', {})}
        ),
        (
            (StatusField,),
            [],
            {'no_check_for_status': ('check_for_status', {})}
        ),
    ], patterns=['scrumdo_model_utils\.fields\.'])
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = managers
from types import ClassType

from django.contrib.contenttypes.models import ContentType
from django.db import models
from django.db.models.fields.related import SingleRelatedObjectDescriptor
from django.db.models.manager import Manager
from django.db.models.query import QuerySet

class InheritanceQuerySet(QuerySet):
    def select_subclasses(self, *subclasses):
        if not subclasses:
            subclasses = [o for o in dir(self.model)
                          if isinstance(getattr(self.model, o), SingleRelatedObjectDescriptor)
                          and issubclass(getattr(self.model,o).related.model, self.model)]
        new_qs = self.select_related(*subclasses)
        new_qs.subclasses = subclasses
        return new_qs

    def _clone(self, klass=None, setup=False, **kwargs):
        try:
            kwargs.update({'subclasses': self.subclasses})
        except AttributeError:
            pass
        return super(InheritanceQuerySet, self)._clone(klass, setup, **kwargs)

    def iterator(self):
        iter = super(InheritanceQuerySet, self).iterator()
        if getattr(self, 'subclasses', False):
            for obj in iter:
                obj = [getattr(obj, s) for s in self.subclasses if getattr(obj, s)] or [obj]
                yield obj[0]
        else:
            for obj in iter:
                yield obj

class InheritanceManager(models.Manager):
    def get_query_set(self):
        return InheritanceQuerySet(self.model)

    def select_subclasses(self, *subclasses):
        return self.get_query_set().select_subclasses(*subclasses)


class InheritanceCastMixin(object):
    def cast(self, select_related_types=None, depth=None):
        results = tuple(self.values_list('pk', 'real_type'))
        type_to_pks = {}
        for pk, real_type_id in results:
            type_to_pks.setdefault(real_type_id, []).append(pk)
        content_types = ContentType.objects.in_bulk(type_to_pks.keys())
        pk_to_child = {}
        for real_type_id, pks in type_to_pks.iteritems():
            content_type = content_types[real_type_id]
            child_type = content_type.model_class()
            if select_related_types:
                params = select_related_types.get('all',[]) + select_related_types.get(content_type.model, [])
                children = child_type._default_manager.select_related(*params).in_bulk(pks)
            elif depth:
                children = child_type._default_manager.select_related(depth=depth).in_bulk(pks)
            else:
                children = child_type._default_manager.in_bulk(pks)
            for pk, child in children.iteritems():
                pk_to_child[pk] = child
        children = []
        # sort children into same order as parents where returned
        for pk, real_type_id in results:
            children.append(pk_to_child[pk])
        return children


class QueryManager(models.Manager):
    def __init__(self, *args, **kwargs):
        if args:
            self._q = args[0]
        else:
            self._q = models.Q(**kwargs)
        super(QueryManager, self).__init__()

    def order_by(self, *args):
        self._order_by = args
        return self

    def get_query_set(self):
        qs = super(QueryManager, self).get_query_set().filter(self._q)
        if hasattr(self, '_order_by'):
            return qs.order_by(*self._order_by)
        return qs


def manager_from(*mixins, **kwds):
    '''
    Returns a Manager instance with extra methods, also available and
    chainable on generated querysets.

    (By George Sakkis, originally posted at
    http://djangosnippets.org/snippets/2117/)

    :param mixins: Each ``mixin`` can be either a class or a function. The
        generated manager and associated queryset subclasses extend the mixin
        classes and include the mixin functions (as methods).

    :keyword queryset_cls: The base queryset class to extend from
        (``django.db.models.query.QuerySet`` by default).

    :keyword manager_cls: The base manager class to extend from
        (``django.db.models.manager.Manager`` by default).
    '''
    # collect separately the mixin classes and methods
    bases = [kwds.get('queryset_cls', QuerySet)]
    methods = {}
    for mixin in mixins:
        if isinstance(mixin, (ClassType, type)):
            bases.append(mixin)
        else:
            try: methods[mixin.__name__] = mixin
            except AttributeError:
                raise TypeError('Mixin must be class or function, not %s' %
                                mixin.__class__)
    # create the QuerySet subclass
    id = hash(mixins + tuple(kwds.iteritems()))
    new_queryset_cls = type('Queryset_%d' % id, tuple(bases), methods)
    # create the Manager subclass
    bases[0] = manager_cls = kwds.get('manager_cls', Manager)
    new_manager_cls = type('Manager_%d' % id, tuple(bases), methods)
    # and finally override new manager's get_query_set
    super_get_query_set = manager_cls.get_query_set
    def get_query_set(self):
        # first honor the super manager's get_query_set
        qs = super_get_query_set(self)
        # and then try to bless the returned queryset by reassigning it to the
        # newly created Queryset class, though this may not be feasible
        if not issubclass(new_queryset_cls, qs.__class__):
            raise TypeError('QuerySet subclass conflict: cannot determine a '
                            'unique class for queryset instance')
        qs.__class__ = new_queryset_cls
        return qs
    new_manager_cls.get_query_set = get_query_set
    return new_manager_cls()

########NEW FILE########
__FILENAME__ = models
from datetime import datetime

from django.db import models
from django.contrib.contenttypes.models import ContentType
from django.utils.translation import ugettext_lazy as _
from django.db.models.fields import FieldDoesNotExist
from django.core.exceptions import ImproperlyConfigured

from managers import manager_from, InheritanceCastMixin, \
    QueryManager
from fields import AutoCreatedField, AutoLastModifiedField, \
    StatusField, MonitorField

class InheritanceCastModel(models.Model):
    """
    An abstract base class that provides a ``real_type`` FK to ContentType.

    For use in trees of inherited models, to be able to downcast
    parent instances to their child types.

    """
    real_type = models.ForeignKey(ContentType, editable=False, null=True)

    objects = manager_from(InheritanceCastMixin)

    def save(self, *args, **kwargs):
        if not self.id:
            self.real_type = self._get_real_type()
        super(InheritanceCastModel, self).save(*args, **kwargs)

    def _get_real_type(self):
        return ContentType.objects.get_for_model(type(self))

    def cast(self):
        return self.real_type.get_object_for_this_type(pk=self.pk)

    class Meta:
        abstract = True


class TimeStampedModel(models.Model):
    """
    An abstract base class model that provides self-updating
    ``created`` and ``modified`` fields.

    """
    created = AutoCreatedField(_('created'))
    modified = AutoLastModifiedField(_('modified'))

    class Meta:
        abstract = True


class TimeFramedModel(models.Model):
    """
    An abstract base class model that provides ``start``
    and ``end`` fields to record a timeframe.

    """
    start = models.DateTimeField(_('start'), null=True, blank=True)
    end = models.DateTimeField(_('end'), null=True, blank=True)

    class Meta:
        abstract = True

class StatusModel(models.Model):
    """
    An abstract base class model with a ``status`` field that
    automatically uses a ``STATUS`` class attribute of choices, a
    ``status_changed`` date-time field that records when ``status``
    was last modified, and an automatically-added manager for each
    status that returns objects with that status only.

    """
    status = StatusField(_('status'))
    status_changed = MonitorField(_('status changed'), monitor='status')

    class Meta:
        abstract = True

def add_status_query_managers(sender, **kwargs):
    """
    Add a Querymanager for each status item dynamically.

    """
    if not issubclass(sender, StatusModel):
        return
    for value, name in getattr(sender, 'STATUS', ()):
        try:
            sender._meta.get_field(name)
            raise ImproperlyConfigured("StatusModel: Model '%s' has a field "
                                       "named '%s' which conflicts with a "
                                       "status of the same name."
                                       % (sender.__name__, name))
        except FieldDoesNotExist:
            pass
        sender.add_to_class(value, QueryManager(status=value))

def add_timeframed_query_manager(sender, **kwargs):
    """
    Add a QueryManager for a specific timeframe.

    """
    if not issubclass(sender, TimeFramedModel):
        return
    try:
        sender._meta.get_field('timeframed')
        raise ImproperlyConfigured("Model '%s' has a field named "
                                   "'timeframed' which conflicts with "
                                   "the TimeFramedModel manager."
                                   % sender.__name__)
    except FieldDoesNotExist:
        pass
    sender.add_to_class('timeframed', QueryManager(
        (models.Q(start__lte=datetime.now) | models.Q(start__isnull=True)) &
        (models.Q(end__gte=datetime.now) | models.Q(end__isnull=True))
    ))


models.signals.class_prepared.connect(add_status_query_managers)
models.signals.class_prepared.connect(add_timeframed_query_manager)

########NEW FILE########
__FILENAME__ = signal_handlers
from django.db import models
from django.template.loader import render_to_string
from django.contrib.contenttypes.models import ContentType
from django.contrib.auth.models import  User
from django.core.cache import cache
import sys, traceback
from itertools import groupby

from threadedcomments.models import ThreadedComment
from activities.utils import allinstances, instanceof
from scrumdo_model_utils.models import InheritanceCastModel
from scrum_log.models import ScrumLog

from activities.models import *
import projects.signals as signals

import datetime
import traceback
import logging

logger = logging.getLogger(__name__)


mappers = {        
    'rank':lambda val: ("Increased Rank" if val[0] > val[1] else "Decreased Rank"),
    'board_rank':lambda val: ("Increased Rank" if val[0] > val[1] else "Decreased Rank"),
    'summary':lambda val: "Summary: %s -> %s" % val,
    'detail':lambda val: "Detail: %s -> %s" % val,
    'modified': lambda val: None,
    'assignee':lambda val: "Assigned To: %s -> %s" % val,
    'points':lambda val: "Point Value: %s -> %s" % val,
    'iteration':lambda val: "Iteration: %s -> %s" % (val[0].name, val[1].name),
    'project':lambda val: "Project: %s -> %s" % (val[0].name, val[1].name),
    'status':lambda val: "Status: %s -> %s" % (val[0].name, val[1].name),
    'category':lambda val: "Category: %s -> %s" % val,
    'epic':lambda val: "Epic: %s -> %s" % val,
    'extra_1':lambda val: "%s: %s -> %s" % (project.extra_1_label, val[0], val[1]),
    'extra_2':lambda val: "%s: %s -> %s" % (project.extra_2_label, val[0], val[1]),
    'extra_3':lambda val: "%s: %s -> %s" % (project.extra_3_label, val[0], val[1]),   
}

def _translate_diffs(diffs, project):
    if diffs == None:
        return {}
    tdiffs = {}
    for k,v in diffs.iteritems():
        logger.debug("Translate diffs %s %s" % (k,v) )
        if k in mappers:
            newval = mappers[k](v)
            if newval:
                tdiffs[k] = newval
        
    return tdiffs

def _createStoryNewsItem(icon, template, **kwargs):
    try:
        story = kwargs["story"]
        user = kwargs["user"]
        diffs = kwargs.get("diffs",None)
        
        diffs = _translate_diffs(diffs, story.project)
        
        item = NewsItem(user=user, project=story.iteration.project, icon=icon )
        item.text = render_to_string("activities/%s" % template, {'user':user,'story':story, 'diffs':diffs} )
        item.save()
    except:
        logger.error("Could not create news item")
        traceback.print_exc(file=sys.stdout)    
        
def _createIterationNewsItem(icon, template, **kwargs):
    try:
        iteration = kwargs["iteration"]
        user = kwargs["user"]
        item = NewsItem(user=user, project=iteration.project, icon=icon )
        item.text = render_to_string("activities/%s" % template, {'user':user,'iteration':iteration} )
        item.save()
    except:
        logger.error("Could not create news item")
        traceback.print_exc(file=sys.stdout)
        
def _createTaskNewsItem(icon, template, **kwargs):
    try:
        task = kwargs["task"]
        user = kwargs["user"]
        diffs = kwargs.get("diffs",None)
        item = NewsItem(user=user, project=task.story.iteration.project, icon=icon )
        item.text = render_to_string("activities/%s" % template, {'user':user,'task':task, 'diffs':diffs} )
        item.save()
    except:
        logger.error("Could not create news item")
        traceback.print_exc(file=sys.stdout)    


def onStoryCreated(sender, **kwargs):
    _createStoryNewsItem("script_add","new_story.txt", **kwargs)
signals.story_created.connect( onStoryCreated , dispatch_uid="newsfeed_signal_hookup")

def onStoryUpdated(sender, **kwargs):
    _createStoryNewsItem("script_edit","edited_story.txt", **kwargs)
signals.story_updated.connect( onStoryUpdated , dispatch_uid="newsfeed_signal_hookup")

def onStoryStatusChanged(sender, **kwargs):
    _createStoryNewsItem("script_code","status_change_story.txt", **kwargs)     
signals.story_status_changed.connect( onStoryStatusChanged , dispatch_uid="newsfeed_signal_hookup")

def onStoryDeleted(sender, **kwargs):
    _createStoryNewsItem("script_delete","delete_story.txt", **kwargs)         
   
signals.story_deleted.connect( onStoryDeleted , dispatch_uid="newsfeed_signal_hookup")

def onTaskCreated(sender, **kwargs):
    _createTaskNewsItem('drive_add', 'new_task.txt', **kwargs)
signals.task_created.connect( onTaskCreated , dispatch_uid="newsfeed_signal_hookup")

def onTaskStatusChange(sender, **kwargs):
    _createTaskNewsItem('drive_go', 'status_change_task.txt', **kwargs)    
signals.task_status_changed.connect( onTaskStatusChange , dispatch_uid="newsfeed_signal_hookup")

def onTaskUpdated(sender, **kwargs):
    _createTaskNewsItem('drive_edit', 'edited_task.txt', **kwargs)    
signals.task_updated.connect( onTaskUpdated , dispatch_uid="newsfeed_signal_hookup")

def onTaskDeleted(sender, **kwargs):
    _createTaskNewsItem('drive_delete', 'delete_task.txt', **kwargs)    
signals.task_deleted.connect( onTaskDeleted , dispatch_uid="newsfeed_signal_hookup")

def onIterationCreated(sender, **kwargs):
    _createIterationNewsItem("calendar_add", "activities/new_iteration.html", **kwargs)
signals.iteration_created.connect( onIterationCreated , dispatch_uid="newsfeed_signal_hookup")

def onIterationDeleted(sender, **kwargs):
    _createIterationNewsItem("calendar_delete", "activities/delete_iteration.html", **kwargs)
signals.iteration_deleted.connect( onIterationDeleted , dispatch_uid="newsfeed_signal_hookup")



def onScrumLogPosted(sender, instance, signal, *args, **kwargs):
    try:        
        icon = "group"
        if instance.flagged:
            icon = "flag_red"
        item = NewsItem(user=instance.creator, project=instance.project, icon=icon )
        item.text = render_to_string("activities/scrumLog.txt", {'item':instance} )
        item.save()
    except:
        logger.error("Could not create news item")
        traceback.print_exc(file=sys.stdout)
models.signals.post_save.connect(onScrumLogPosted, sender=ScrumLog)


def onCommentPosted(sender, **kwargs):
    t_comment = kwargs['instance']
    from projects.models import Story
    # check if this is a comment on a story, the only kind we know how to deal with, and that its a new comment.
    if t_comment.content_type.id == ContentType.objects.get_for_model(Story).id and kwargs['created']:
        try:
            story = Story.objects.get(id=t_comment.object_id)        
            item = NewsItem(user=t_comment.user, project=story.iteration.project, icon="comment_add" )            
            item.text = render_to_string("activities/comment_on_story.txt", {'story':story,'item':t_comment} )
            item.save()
        except:
            logger.error("Could not create news item")
            traceback.print_exc(file=sys.stdout)
models.signals.post_save.connect(onCommentPosted, sender=ThreadedComment)

########NEW FILE########
__FILENAME__ = activity_tags
from django import template
from django.core.urlresolvers import reverse
from django.utils.safestring import mark_safe
from django.utils.html import escape
from django.http import HttpResponseRedirect, HttpResponse
from django.template.loader import render_to_string
from django.utils.html import escape
from activities.models import NewsItem
from organizations.models import Organization
import projects.access as access

import string

register = template.Library()

import logging

logger = logging.getLogger(__name__)

@register.inclusion_tag('activities/news_feed.html',takes_context=True)
def news_feed(context):
    user = context["user"]
    request = context["request"]
    if "organization" in context:
        organization = context["organization"]
        if organization.hasStaffAccess(user):
            # The user can see app projects in the org.
            news_items = NewsItem.objects.filter(project__organization=organization)
        else:
            # NOTE: We need to watch this query to make sure it performs ok.
            news_items = NewsItem.objects.filter(project__organization=organization, project__teams__members=user).distinct()
    else:
        project = context["project"]
        if access.has_read_access(project,user):
            news_items = NewsItem.objects.filter(project=project)
        else:
            news_items = []
        
    return {'newsitems':news_items,"request":request}
    


def iteration_name(iteration):
    if iteration.name == "Backlog":
        return "Backlog"
    else:
        if iteration.name.startswith("Iteration") or iteration.name.startswith("Sprint"):
            return iteration.name
        else:
            return "Iteration %s" % iteration.name

def iteration_uri(iteration, project):
    return reverse("iteration", args=[project.slug, iteration.id])

def iteration_link(iteration, project):
    return ("<a href='%s'>" % iteration_uri(iteration, project)) + iteration_name(iteration) + "</a>"

def story_link(s, project):
    url = iteration_uri(s.iteration, project)
    summary = escape(smart_truncate(s.summary,length=40))
    return "<a title='%s' href='%s#story_%s'> #%d %s</a>" % (escape(s.summary),url, s.id, s.local_id, summary )


def smart_truncate(content, length=100, suffix='...'):
    if len(content) <= length:
        return content
    else:
        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix

@register.filter
def absolute_url(value):    
    return string.replace(value,'href="/','href="http://www.scrumdo.com/')

@register.filter
def subscription_checkbox(project , subscription_list):
    try:
        if project.id in subscription_list:
            return "<input type=\"checkbox\" name=\"subscriptions\" value=\"%d\" checked=\"checked\"> " % project.id
        else:
            return "<input type=\"checkbox\" name=\"subscriptions\" value=\"%d\"> " % project.id
    except:
        return ""


########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *

urlpatterns = patterns('activities.views',
url(r'^subscription/$', 'activity_subscriptions', name="activity_subscriptions"),
url(r'^test/$', 'activities_test', name="activities_test"),
)

########NEW FILE########
__FILENAME__ = utils
from django.db.models import fields

def model_differences(m1, m2, excludes = [], dicts=False):
    """ this function takes two models and compares them.
    optionally takes two dicts created by running model_instance.__dict__ """
    changes = {}
    if not dicts:
        m1 = m1.__dict__
        m2 = m2.__dict__
    for k in m1:
        if m1[k] != m2[k] and not k in excludes:
            changes[k] = (m1[k], m2[k])
    return changes



def instanceof(obj, classes):
    for cl in classes:
        if isinstance(obj, cl):
            return True
    return False

def allinstances(objs, cl):
    for obj in objs:
        if not isinstance(obj, cl):
            return False
    return True

########NEW FILE########
__FILENAME__ = views
from django.contrib.auth.decorators import login_required
from django.template import RequestContext
from django.core.paginator import Paginator, InvalidPage, EmptyPage
from projects.models import ProjectMember, Project
from organizations.models import Organization
from django.shortcuts import render_to_response, get_object_or_404
from django.http import Http404
from activities.models import ProjectEmailSubscription

import logging

logger = logging.getLogger(__name__)

@login_required
def activities_test(request):
    """ this is a version of user_activities that includes closing body tags so django-toolbar works with it """
    r = user_activities(request,1)
    r.write("</body>")
    return r


@login_required
def activity_subscriptions(request):
    organizations = Organization.getOrganizationsForUser(request.user)

    subscription_objs =  ProjectEmailSubscription.objects.filter(user=request.user)
    if request.method == "POST":
        subscriptions = request.POST.getlist('subscriptions')
        subscriptions = [int(sub) for sub in subscriptions]
        # remove any unchecked...
        for old_sub in subscription_objs:
            if not old_sub.project.id in subscriptions:
                old_sub.delete()
        # add any new ones
        for new_sub in subscriptions:
            if len([ sub for sub in subscription_objs if sub.project.id==new_sub ]) == 0 :
                project = Project.objects.get(id=new_sub)
                s = ProjectEmailSubscription( user=request.user, project=project)
                s.save()
    else:
        subscriptions = [ sub.project.id for sub in subscription_objs ]

    return render_to_response("activities/email_subscription.html", { "organizations":organizations, "subscriptions":subscriptions
      }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = auth
from tastypie.authentication import ApiKeyAuthentication
from tastypie.authorization import Authorization
from api.models import UserApiKey


class ScrumDoAuthorization(Authorization):
    def __init__(self, read_q, write_q):
        """
        read_q and write_q are functions that return Q queries that check for read and write access
        respectively for the resource type in question.

        each takes the user in question as its only argument.
        """
        self.read_q = read_q
        self.write_q = write_q

    def is_authorized(self, request, object=None):
        # this method is NEVER passed an object currently as far as I can tell (somewhat misleadingly),
        # so it is useless unless you are doing class level authorization.
        return True

    def apply_limits(self, request, object_list):
        if request and hasattr(request, 'user') and self:
            if request.method == 'GET':
                return object_list.filter(self.read_q(request.user)).distinct()
            elif request.method in ['POST','PUT','DELETE']:
                return object_list.filter(self.write_q(request.user)).distinct()
        else: # if there is no user, then this is an internal call, in which case give it all back
            return object_list.all()

class ScrumDoAuthentication(ApiKeyAuthentication):
    """ Overriding to use more sensible param name for username (as username collides with built in django auth) """

    def get_key(self, user, api_key):
        """
        Attempts to find the API key for the user. Uses ``ApiKey`` by default
        but can be overridden.
        """

        try:
            key = UserApiKey.objects.get(user=user, key=api_key)
        except UserApiKey.DoesNotExist:
            return self._unauthorized()

        if key.developer_key and key.developer_key.approved:
            return True
        else:
            return self._unauthorized()

    def is_authenticated(self, request, **kwargs):
        """
        Finds the user and checks their API key.

        Should return either ``True`` if allowed, ``False`` if not or an
        ``HttpResponse`` if you need something custom.
        """
        from django.contrib.auth.models import User

        username = request.GET.get('user_name') or request.POST.get('user_name')
        api_key = request.GET.get('user_key') or request.POST.get('user_key')

        if not username or not api_key:
            return self._unauthorized()

        try:
            user = User.objects.get(username=username)
        except (User.DoesNotExist, User.MultipleObjectsReturned):
            return self._unauthorized()

        request.user = user

        return self.get_key(user, api_key)

    def get_identifier(self, request):
        """
        Provides a unique string identifier for the requestor.

        This implementation returns the user's username.
        """
        return request.REQUEST.get('user_name', 'no_user')

########NEW FILE########
__FILENAME__ = models
import uuid
from django.conf import settings
from django.contrib.auth.models import User
from django.db import models
from datetime import datetime, date
import hmac
try:
    from hashlib import sha1
except ImportError:
    import sha
    sha1 = sha.sha

def generate_key():
    # Get a random UUID.
    new_uuid = uuid.uuid4()
    # Hmac that beast.
    return hmac.new(str(new_uuid), digestmod=sha1).hexdigest()

class DeveloperApiKey(models.Model):
    developer = models.ForeignKey(User, related_name='developer_key')
    key = models.CharField(max_length=256, blank=True, default='')
    application_name = models.CharField(max_length=30)
    created = models.DateTimeField(default=datetime.now)
    approved = models.BooleanField(default=False)

    def __unicode__(self):
        return u"%s for developer %s" % (self.key, self.developer)

    def save(self, *args, **kwargs):
        if not self.key:
            self.key = generate_key()

        return super(DeveloperApiKey, self).save(*args, **kwargs)


class UserApiKey(models.Model):
    user = models.ForeignKey(User, related_name='api_key')
    key = models.CharField(max_length=256, blank=True, default='')
    developer_key = models.ForeignKey(DeveloperApiKey,related_name="user_key")

    created = models.DateTimeField(default=datetime.now)

    def __unicode__(self):
        return u"%s for %s" % (self.key, self.user)

    def save(self, *args, **kwargs):
        if not self.key:
            self.key = generate_key()

        return super(UserApiKey, self).save(*args, **kwargs)

########NEW FILE########
__FILENAME__ = resources
from django.contrib.auth.models import User
from django.contrib.auth import authenticate
from django.core.urlresolvers import reverse

from django.conf import settings

from tastypie import fields
from tastypie.resources import ModelResource
from tastypie.validation import Validation, FormValidation

from django.conf.urls.defaults import *
from django.core.urlresolvers import reverse


from django.contrib.contenttypes.models import ContentType
from django.db.models import Q
from activities.templatetags.activity_tags import absolute_url
from activities.models import NewsItem
from projects.models import Project,ProjectMember,Story,Iteration,Epic,Task
from organizations.models import Organization, Team
from threadedcomments.models import ThreadedComment

from projects.access import has_read_access, has_write_access

from projects.forms import StoryForm

from auth import ScrumDoAuthentication, ScrumDoAuthorization
from api.models import DeveloperApiKey, UserApiKey
import projects.signals as signals

class UserResource(ModelResource):
    teams = fields.ToManyField('api.resources.TeamResource', 'teams')
    projects = fields.ToManyField('api.resources.ProjectResource', 'user_projects')
    assigned_stories = fields.ToManyField('api.resources.StoryResource', 'assigned_stories', null=True)
    tasks = fields.ToManyField('api.resources.TaskResource','assigned_tasks')

    class Meta:
        queryset = User.objects.all()
        fields = ['username', 'first_name', 'last_name', 'last_login']
        detail_allowed_methods = ['get']
        list_allowed_methods = []
        authentication = ScrumDoAuthentication()

    def override_urls(self):
        return [
            url(r"^(?P<resource_name>%s)/self/$" % self._meta.resource_name, self.wrap_view('self_dispatch_detail'), name="api_self_dispatch_detail"),
        ]
    def self_dispatch_detail(self, request, **kwargs):
        self._meta.authentication.is_authenticated(request)
        kwargs['id'] = request.user.id
        return self.dispatch_detail(request, **kwargs)

    def dehydrate(self, bundle):
     # get ALL the users projects (including those discovered through teams), and set the projects to the uri's for those.
        bundle.data['projects'] = map(lambda p: reverse("api_dispatch_detail", kwargs={'resource_name':"project", 'pk': p.id, "api_name": self._meta.api_name}), ProjectMember.getProjectsForUser(User.objects.get(username=bundle.data["username"])))
        return bundle

class OrganizationResource(ModelResource):
    teams = fields.ToManyField('api.resources.TeamResource', 'teams')
    projects = fields.ToManyField('api.resources.ProjectResource', 'projects')

    class Meta:
        queryset = Organization.objects.all()
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
           lambda u: Q(id__in = u.teams.all().values('organization__id').distinct()),
           lambda u: Q(id__in = u.teams.filter(access_type="admin").values('organization__id').distinct()))


class TeamResource(ModelResource):
    members = fields.ToManyField('api.resources.UserResource', 'members')
    projects = fields.ToManyField('api.resources.ProjectResource', 'projects')
    organization = fields.ToOneField('api.resources.OrganizationResource', 'organization')

    class Meta:
        queryset = Team.objects.all()
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
          lambda u: Q(organization__id__in = u.teams.all().values('organization__id').distinct()),
          lambda u: Q(organization__id__in = u.teams.filter(access_type="admin").values('organization__id').distinct()))


# This isn't working right now as I don't know how to get comments only on a specific model (see in StoryResource for what I tried)
# class CommentResource(ModelResource):
#   story = fields.ToOneField('api.resources.StoryResource', 'content_object')
#   class Meta:
#     queryset = ThreadedComment.objects.all()
#     fields = ["comment", "date_modified", "date_submitted"]
#     filtering = {
#                 "content_type": ('exact', ContentType.objects.get_for_model(Story),),
#             }

class StoryResource(ModelResource):
    iteration = fields.ToOneField('api.resources.IterationResource', 'iteration')
    project = fields.ToOneField('api.resources.ProjectResource', 'project')
    creator = fields.ToOneField('api.resources.UserResource', 'creator')
    tasks = fields.ToManyField('api.resources.TaskResource','tasks')
    epic = fields.ToOneField('api.resources.EpicResource','epic',null=True)
    assignee = fields.ToOneField('api.resources.UserResource','assignee',null=True)

    class Meta:
        queryset = Story.objects.all()
        fields = ['id', 'summary','detail','assignee_id','points', 'iteration_id','project_id', 'status','extra_1','extra_2','extra_3', 'local_id','assignee', 'rank']
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
          lambda u: Q(project__teams__in = u.teams.all())|Q(project__member_users__in = [u]),
          lambda u: Q(project__teams__in = u.teams.filter(Q(access_type="read/write")|Q(access_type="admin")))|Q(project__member_users__in = [u]))
        #     validation = Validation() # FormValidation(form_class=StoryForm)

    def dehydrate(self, bundle):
        # cr = CommentResource()
        # bundle.data['comments'] = map(lambda c: cr.get_resource_uri(c), cr.obj_get_list(content_object__exact=bundle.obj))
        return bundle
        
    def obj_create(self, bundle, request=None, **kwargs):
        res = super(StoryResource, self).obj_create(bundle, request)
        if "pk" in kwargs.keys():
            signals.story_updated.send( sender=request, story=res.obj, user=request.user )
        else:
            signals.story_created.send( sender=request, story=res.obj, user=request.user )
        return res
        
    def obj_delete(self, request=None, **kwargs):
        story = Story.objects.get(pk=kwargs["pk"])
        signals.story_deleted.send( sender=request, story=story, user=request.user )
        super(StoryResource, self).obj_delete(request, pk=kwargs["pk"])
            
    def obj_update(self, bundle, request=None, **kwargs):
        res = super(StoryResource, self).obj_update(bundle, request)
        signals.task_updated.send( sender=request, story=res.obj, user=request.user )
        return res
            
        
        

class TaskResource(ModelResource):
    story = fields.ToOneField('api.resources.StoryResource', 'story')
    assignee = fields.ToOneField('api.resources.UserResource', 'assignee')
    
    class Meta:
        queryset = Task.objects.all()
        fields = ['id', 'summary','complete','order']
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
          lambda u: Q(story__project__teams__in = u.teams.all())|Q(story__project__member_users__in = [u]),
          lambda u: Q(story__project__teams__in = u.teams.filter(Q(access_type="read/write")|Q(access_type="admin")))|Q(story__project__member_users__in = [u]))
        
    def obj_create(self, bundle, request=None, **kwargs):
        res = super(TaskResource, self).obj_create(bundle, request)
        if "pk" in kwargs.keys():
            signals.task_updated.send( sender=request, task=res.obj, user=request.user )
        else:
            signals.task_created.send( sender=request, task=res.obj, user=request.user )
        return res
        
    def obj_delete(self, request=None, **kwargs):
        task = Task.objects.get(pk=kwargs["pk"])
        signals.task_deleted.send( sender=request, task=task, user=request.user )
        super(TaskResource, self).obj_delete(request, pk=kwargs["pk"])
            
    def obj_update(self, bundle, request=None, **kwargs):
        res = super(TaskResource, self).obj_update(bundle, request)
        signals.task_updated.send( sender=request, task=res.obj, user=request.user )
        return res



class EpicResource(ModelResource):
    project = fields.ToOneField('api.resources.ProjectResource', 'project')
    parent = fields.ToOneField('api.resources.EpicResource', 'parent', null=True)
    stories = fields.ToManyField('api.resources.StoryResource','stories')
    
    class Meta:
        queryset = Epic.objects.all()
        fields = ['id', 'summary','detail','points','project_id', 'parent_id', 'status','order','archived', 'local_id']
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
          lambda u: Q(project__teams__in = u.teams.all())|Q(project__member_users__in = [u]),
          lambda u: Q(project__teams__in = u.teams.filter(Q(access_type="read/write")|Q(access_type="admin")))|Q(project__member_users__in = [u]))


class IterationResource(ModelResource):
    stories = fields.ToManyField('api.resources.StoryResource', 'stories')
    project = fields.ToOneField('api.resources.ProjectResource', 'project')

    class Meta:
        queryset = Iteration.objects.all()
        fields = ['id','name', 'start_date','end_date','project_id']
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
          lambda u: Q(project__teams__in = u.teams.all())|Q(project__member_users__in = [u]),
          lambda u: Q(project__teams__in = u.teams.filter(Q(access_type="read/write")|Q(access_type="admin")))|Q(project__member_users__in = [u]))
        
    def obj_create(self, bundle, request=None, **kwargs):
        res = super(IterationResource, self).obj_create(bundle, request)
        if "pk" in kwargs.keys():
            signals.iteration_updated.send( sender=request, iteration=res.obj, user=request.user )
        else:
            signals.iteration_created.send( sender=request, iteration=res.obj, user=request.user )
        return res
        
    def obj_delete(self, request=None, **kwargs):
        iteration = Iteration.objects.get(pk=kwargs["pk"])
        signals.iteration_deleted.send( sender=request, iteration=iteration, user=request.user )
        super(IterationResource, self).obj_delete(request, pk=kwargs["pk"])
            
    

class ProjectResource(ModelResource):
    iterations = fields.ToManyField('api.resources.IterationResource', 'iterations')
    epics = fields.ToManyField('api.resources.EpicResource', 'epics')
    teams = fields.ToManyField('api.resources.TeamResource', 'teams')
    organization = fields.ToOneField(OrganizationResource, 'organization', null=True, full=True)
    member_users = fields.ToManyField('api.resources.UserResource', 'member_users')


    class Meta:
        queryset = Project.objects.all()
        fields = ['name', 'slug', 'creator_id','organization_id','velocity','iterations_left','get_num_stories']
        list_allowed_methods = ['get', 'post', 'put', 'delete']
        authentication = ScrumDoAuthentication()
        authorization = ScrumDoAuthorization(
           lambda u: Q(teams__in = u.teams.all())|Q(member_users__in = [u]),
           lambda u: Q(teams__in = u.teams.filter(Q(access_type="read/write")|Q(access_type="admin")))|Q(member_users__in = [u]))


class NewsItemResource(ModelResource):
    user = fields.CharField(attribute='user')
    def obj_get_list(self, request=None, **kwargs):
        """ overriding """
        return NewsItem.objects.filter(project__teams__members=request.user)
    def dehydrate_text(self, bundle):        
        return absolute_url(bundle.obj.text)[1:]
    def dehydrate_user(self, bundle):
        return str(bundle.obj.user)
    def dehydrate_icon(self, bundle):
        if "http" == settings.STATIC_URL[:4]:
            return "%simages/%s.png" % (settings.STATIC_URL,bundle.obj.icon)
        else:
            return "%s%simages/%s.png" % (settings.BASE_URL,settings.STATIC_URL,bundle.obj.icon)
    class Meta:
        fields=['created','text','icon','user']
        resource_name = "news"
        queryset = NewsItem.objects.all()
        authentication = ScrumDoAuthentication()
    
# class ActivityResource(ModelResource):
#     def obj_get_list(self, request=None, **kwargs):
#         """ overriding """
#         return Activity.getActivitiesForUser(request.user)
#        
#     line = fields.CharField(readonly=True)
#     #creator = fields.ToOneField('api.resources.UserResource', 'user', full=True)
#     #project = fields.ToOneField('api.resources.ProjectResource', 'project', full=True)
#     #action = fields.ToOneField('api.resources.ActivityActionResource', 'action', full=True)
# 
#     class Meta:
#         queryset = Activity.objects.all()
#         authentication = ScrumDoAuthentication()
#     
#     def dehydrate_line(self, bundle):
#         return bundle.obj.getPrettyActivityString()

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from tastypie.api import Api
from api.resources import *

class ScrumDoApi(Api):
    def override_urls(self):
        return [url(r'^v1/login$', "api.views.login" , name="api_login"),
                url(r'^v1/is_key_valid$', "api.views.is_key_valid" , name="is_key_valid"),
                ]

v1_api = ScrumDoApi(api_name='v1')
v1_api.register(OrganizationResource())
v1_api.register(TeamResource())
v1_api.register(ProjectResource())
v1_api.register(StoryResource())
v1_api.register(TaskResource())
v1_api.register(EpicResource())
v1_api.register(NewsItemResource())
# v1_api.register(CommentResource())
v1_api.register(IterationResource())
v1_api.register(UserResource())
# v1_api.register(ActivityResource())

urlpatterns = v1_api.urls

########NEW FILE########
__FILENAME__ = views
from django.contrib.auth import authenticate
from api.models import DeveloperApiKey, UserApiKey
from tastypie.utils.mime import determine_format, build_content_type
from tastypie.serializers import Serializer
from django.http import HttpResponse
from tastypie.http import HttpUnauthorized

def login(request, **kwargs):
    # self.method_check(request, allowed=['post'])
    # self.is_authenticated(request)
    # self.throttle_check(request)

    developer_key = request.GET.get('developer_key')
    username = request.GET.get('username')
    password = request.GET.get('password')

    if not developer_key or not username or not password:
        return HttpUnauthorized()

    try:
        dev_key = DeveloperApiKey.objects.get(key=developer_key, approved=True)
    except DeveloperApiKey.DoesNotExist:
        return HttpUnauthorized()

    user = authenticate(username=username, password=password)

    if user:
        try:
            key = UserApiKey.objects.get(user=user, developer_key=dev_key)
        except  UserApiKey.DoesNotExist:
            key = UserApiKey(user=user, developer_key=dev_key)
            key.save()

        # self.log_throttled_access(request)
        serializer = Serializer()
        desired_format = determine_format(request, serializer)
        serialized = serializer.serialize({'key' : key.key}, desired_format)
        return HttpResponse(content=serialized, content_type=build_content_type(desired_format))
    else:
        return HttpUnauthorized()

def is_key_valid(request, **kwargs):
    user_key = request.GET.get('user_key')
    try:
        user_key = UserApiKey.objects.get(key=user_key)
        key_valid = True
    except UserApiKey.DoesNotExist:
        key_valid = False
        # self.log_throttled_access(request)
    serializer = Serializer()
    desired_format = determine_format(request, serializer)
    serialized = serializer.serialize({'valid' : key_valid}, desired_format)
    return HttpResponse(content=serialized, content_type=build_content_type(desired_format))

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from avatar.models import Avatar

admin.site.register(Avatar)

########NEW FILE########
__FILENAME__ = forms
from django import forms
from django.forms import widgets
from django.utils.safestring import mark_safe

def avatar_img(avatar, size):
    if not avatar.thumbnail_exists(size):
        avatar.create_thumbnail(size)
    return mark_safe("""<img src="%s" alt="%s" width="%s" height="%s" />""" %
        (avatar.avatar_url(size), unicode(avatar), size, size))

class PrimaryAvatarForm(forms.Form):

    def __init__(self, *args, **kwargs):
        user = kwargs.pop('user')
        size = kwargs.pop('size', 80)
        super(PrimaryAvatarForm, self).__init__(*args, **kwargs)
        avatars = user.avatar_set.all()
        self.fields['choice'] = forms.ChoiceField(
            choices=[(c.id, avatar_img(c, size)) for c in user.avatar_set.all()],
            widget=widgets.RadioSelect)

class DeleteAvatarForm(forms.Form):

    def __init__(self, *args, **kwargs):
        user = kwargs.pop('user')
        size = kwargs.pop('size', 80)
        super(DeleteAvatarForm, self).__init__(*args, **kwargs)
        avatars = user.avatar_set.all()
        self.fields['choices'] = forms.MultipleChoiceField(
            choices=[(c.id, avatar_img(c, size)) for c in user.avatar_set.all()],
            widget=widgets.CheckboxSelectMultiple)

########NEW FILE########
__FILENAME__ = rebuild_avatars
from django.core.management.base import NoArgsCommand
from django.conf import settings

from avatar.models import Avatar
from avatar import AUTO_GENERATE_AVATAR_SIZES

class Command(NoArgsCommand):
    help = "Regenerates avatar thumbnails for the sizes specified in " + \
        "settings.AUTO_GENERATE_AVATAR_SIZES."

    def handle_noargs(self, **options):
        for avatar in Avatar.objects.all():
            for size in AUTO_GENERATE_AVATAR_SIZES:
                print "Rebuilding Avatar id=%s at size %s." % (avatar.id, size)
                avatar.create_thumbnail(size)

########NEW FILE########
__FILENAME__ = models
import datetime
import os.path

from django.db import models
from django.contrib.auth.models import User
from django.core.files.base import ContentFile
from django.utils.translation import ugettext as _

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

try:
    from PIL import Image
except ImportError:
    import Image

from avatar import AVATAR_STORAGE_DIR, AVATAR_RESIZE_METHOD

import logging

logger = logging.getLogger(__name__)

def avatar_file_path(instance=None, filename=None, user=None):
    user = user or instance.user
    return os.path.join(AVATAR_STORAGE_DIR, user.username, filename)

class Avatar(models.Model):
    user = models.ForeignKey(User)
    primary = models.BooleanField(default=False)
    avatar = models.ImageField(max_length=1024, upload_to=avatar_file_path, blank=True)
    date_uploaded = models.DateTimeField(default=datetime.datetime.now)

    def __unicode__(self):
        return _(u'Avatar for %s') % self.user

    def save(self, force_insert=False, force_update=False):
        if self.primary:
            avatars = Avatar.objects.filter(user=self.user, primary=True)\
                .exclude(id=self.id)
            avatars.update(primary=False)
        super(Avatar, self).save(force_insert, force_update)

    def thumbnail_exists(self, size):
        return self.avatar.storage.exists(self.avatar_name(size))

    def create_thumbnail(self, size):
        try:
            orig = self.avatar.storage.open(self.avatar.name, 'rb').read()
            image = Image.open(StringIO(orig))
        except IOError:
            return # What should we do here?  Render a "sorry, didn't work" img?
        try:
            (w, h) = image.size
            if w != size or h != size:
                if w > h:
                    diff = (w - h) / 2
                    image = image.crop((diff, 0, w - diff, h))
                else:
                    diff = (h - w) / 2
                    image = image.crop((0, diff, w, h - diff))
                image = image.resize((size, size), AVATAR_RESIZE_METHOD)
                if image.mode != "RGB":
                    image = image.convert("RGB")
                thumb = StringIO()
                image.save(thumb, "JPEG")
                thumb_file = ContentFile(thumb.getvalue())
            else:
                thumb_file = ContentFile(orig)
            thumb = self.avatar.storage.save(self.avatar_name(size), thumb_file)
        except:
            # If an interlaced PNG is uploaded, the above resizing fails, so just reuse the original.
            # Errors in other circumstances could also happen...
            thumb_file = ContentFile(orig)
            thumb = self.avatar.storage.save(self.avatar_name(size), thumb_file)

    def avatar_url(self, size):
        return self.avatar.storage.url(self.avatar_name(size))

    def avatar_name(self, size):
        return os.path.join(AVATAR_STORAGE_DIR, self.user.username,
            'resized', str(size), self.avatar.name)

########NEW FILE########
__FILENAME__ = avatar_tags
import urllib

from django import template
from django.contrib.auth.models import User
from django.utils.translation import ugettext as _
from django.utils.hashcompat import md5_constructor

from scrumdo_utils import cache

from avatar import AVATAR_DEFAULT_URL, AVATAR_GRAVATAR_BACKUP, AVATAR_GRAVATAR_DEFAULT

register = template.Library()

def avatar_url(user, size=80):
    if not isinstance(user, User):
        try:
            user = User.objects.get(username=user)
        except User.DoesNotExist:
            return AVATAR_DEFAULT_URL
    avatars = user.avatar_set.order_by('-date_uploaded')
    primary = avatars.filter(primary=True)
    if primary.count() > 0:
        avatar = primary[0]
    elif avatars.count() > 0:
        avatar = avatars[0]
    else:
        avatar = None
    if avatar is not None:
        if not avatar.thumbnail_exists(size):
            avatar.create_thumbnail(size)
        return avatar.avatar_url(size)
    else:
        if AVATAR_GRAVATAR_BACKUP:
            params = {'s': str(size)}
            if AVATAR_GRAVATAR_DEFAULT:
                params['d'] = AVATAR_GRAVATAR_DEFAULT
            return "https://secure.gravatar.com/avatar/%s/?%s" % (
                md5_constructor(user.email).hexdigest(),
                urllib.urlencode(params))
        else:
            return AVATAR_DEFAULT_URL
register.simple_tag(avatar_url)


def avatar(user, size=80):
    return real_avatar(user, size)

@cache(120)
def real_avatar(user, size):
    if not isinstance(user, User):
        try:
            user = User.objects.get(username=user)
            alt = unicode(user)
            url = avatar_url(user, size)
        except User.DoesNotExist:
            url = AVATAR_DEFAULT_URL
            alt = _("Default Avatar")
    else:
        alt = unicode(user)
        url = avatar_url(user, size)
    return """<img src="%s" alt="%s" width="%s" height="%s" />""" % (url, alt,
        size, size)
register.simple_tag(avatar)

def render_avatar(avatar, size=80):
    if not avatar.thumbnail_exists(size):
        avatar.create_thumbnail(size)
    return """<img src="%s" alt="%s" width="%s" height="%s" />""" % (
        avatar.avatar_url(size), str(avatar), size, size)
register.simple_tag(render_avatar)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import patterns, url

urlpatterns = patterns('avatar.views',
    url('^change/$', 'change', name='avatar_change'),
    url('^delete/$', 'delete', name='avatar_delete'),
)

########NEW FILE########
__FILENAME__ = views
import os.path

from avatar.models import Avatar, avatar_file_path
from avatar.forms import PrimaryAvatarForm, DeleteAvatarForm
from django.http import HttpResponseRedirect
from django.shortcuts import render_to_response
from django.template import RequestContext
from django.contrib.auth.decorators import login_required
from django.utils.translation import ugettext as _

from django.db.models import get_app
from django.core.exceptions import ImproperlyConfigured
from django.conf import settings

try:
    notification = get_app('notification')
except ImproperlyConfigured:
    notification = None

friends = False
if 'friends' in settings.INSTALLED_APPS:
    friends = True
    from friends.models import Friendship

def _get_next(request):
    """
    The part that's the least straightforward about views in this module is how they
    determine their redirects after they have finished computation.

    In short, they will try and determine the next place to go in the following order:

    1. If there is a variable named ``next`` in the *POST* parameters, the view will
    redirect to that variable's value.
    2. If there is a variable named ``next`` in the *GET* parameters, the view will
    redirect to that variable's value.
    3. If Django can determine the previous page from the HTTP headers, the view will
    redirect to that previous page.
    """
    next = request.POST.get('next', request.GET.get('next', request.META.get('HTTP_REFERER', None)))
    if not next:
        next = request.path
    return next

def change(request, extra_context={}, next_override=None):
    avatars = Avatar.objects.filter(user=request.user).order_by('-primary')
    if avatars.count() > 0:
        avatar = avatars[0]
        kwargs = {'initial': {'choice': avatar.id}}
    else:
        avatar = None
        kwargs = {}
    primary_avatar_form = PrimaryAvatarForm(request.POST or None, user=request.user, **kwargs)
    if request.method == "POST":
        updated = False
        if 'avatar' in request.FILES:
            path = avatar_file_path(user=request.user,
                filename=request.FILES['avatar'].name)
            avatar = Avatar(
                user = request.user,
                primary = True,
                avatar = path,
            )
            new_file = avatar.avatar.storage.save(path, request.FILES['avatar'])
            avatar.save()
            updated = True
            request.user.message_set.create(
                message=_("Successfully uploaded a new avatar."))
        if 'choice' in request.POST and primary_avatar_form.is_valid():
            avatar = Avatar.objects.get(id=
                primary_avatar_form.cleaned_data['choice'])
            avatar.primary = True
            avatar.save()
            updated = True
            request.user.message_set.create(
                message=_("Successfully updated your avatar."))
        if updated and notification:
            notification.send([request.user], "avatar_updated", {"user": request.user, "avatar": avatar})
            if friends:
                notification.send((x['friend'] for x in Friendship.objects.friends_for_user(request.user)), "avatar_friend_updated", {"user": request.user, "avatar": avatar})
        return HttpResponseRedirect(next_override or _get_next(request))
    return render_to_response(
        'avatar/change.html',
        extra_context,
        context_instance = RequestContext(
            request,
            { 'avatar': avatar,
              'avatars': avatars,
              'primary_avatar_form': primary_avatar_form,
              'next': next_override or _get_next(request), }
        )
    )
change = login_required(change)

def delete(request, extra_context={}, next_override=None):
    avatars = Avatar.objects.filter(user=request.user).order_by('-primary')
    if avatars.count() > 0:
        avatar = avatars[0]
    else:
        avatar = None
    delete_avatar_form = DeleteAvatarForm(request.POST or None, user=request.user)
    if request.method == 'POST':
        if delete_avatar_form.is_valid():
            ids = delete_avatar_form.cleaned_data['choices']
            if unicode(avatar.id) in ids and avatars.count() > len(ids):
                for a in avatars:
                    if unicode(a.id) not in ids:
                        a.primary = True
                        a.save()
                        notification.send([request.user], "avatar_updated", {"user": request.user, "avatar": a})
                        if friends:
                            notification.send((x['friend'] for x in Friendship.objects.friends_for_user(request.user)), "avatar_friend_updated", {"user": request.user, "avatar": a})
                        break
            Avatar.objects.filter(id__in=ids).delete()
            request.user.message_set.create(
                message=_("Successfully deleted the requested avatars."))
            return HttpResponseRedirect(next_override or _get_next(request))
    return render_to_response(
        'avatar/confirm_delete.html',
        extra_context,
        context_instance = RequestContext(
            request,
            { 'avatar': avatar,
              'avatars': avatars,
              'delete_avatar_form': delete_avatar_form,
              'next': next_override or _get_next(request), }
        )
    )
delete = login_required(delete)

########NEW FILE########
__FILENAME__ = debugsqlshell
import os
from optparse import make_option

from django.core.management.base import NoArgsCommand
from django.db.backends import util

from debug_toolbar.utils import sqlparse

class PrintQueryWrapper(util.CursorDebugWrapper):
    def execute(self, sql, params=()):
        try:
            return self.cursor.execute(sql, params)
        finally:
            raw_sql = self.db.ops.last_executed_query(self.cursor, sql, params)
            print sqlparse.format(raw_sql, reindent=True)
            print

util.CursorDebugWrapper = PrintQueryWrapper

# The rest is copy/paste from django/core/management/commands/shell.py

class Command(NoArgsCommand):
    option_list = NoArgsCommand.option_list + (
        make_option('--plain', action='store_true', dest='plain',
            help='Tells Django to use plain Python, not IPython.'),
    )
    help = "Runs a Python interactive interpreter. Tries to use IPython, if it's available."

    requires_model_validation = False

    def handle_noargs(self, **options):
        # XXX: (Temporary) workaround for ticket #1796: force early loading of all
        # models from installed apps.
        from django.db.models.loading import get_models
        loaded_models = get_models()

        use_plain = options.get('plain', False)

        try:
            if use_plain:
                # Don't bother loading IPython, because the user wants plain Python.
                raise ImportError
            import IPython
            # Explicitly pass an empty list as arguments, because otherwise IPython
            # would use sys.argv from this script.
            shell = IPython.Shell.IPShell(argv=[])
            shell.mainloop()
        except ImportError:
            import code
            # Set up a dictionary to serve as the environment for the shell, so
            # that tab completion works on objects that are imported at runtime.
            # See ticket 5082.
            imported_objects = {}
            try: # Try activating rlcompleter, because it's handy.
                import readline
            except ImportError:
                pass
            else:
                # We don't have to wrap the following import in a 'try', because
                # we already know 'readline' was imported successfully.
                import rlcompleter
                readline.set_completer(rlcompleter.Completer(imported_objects).complete)
                readline.parse_and_bind("tab:complete")

            # We want to honor both $PYTHONSTARTUP and .pythonrc.py, so follow system
            # conventions and get $PYTHONSTARTUP first then import user.
            if not use_plain:
                pythonrc = os.environ.get("PYTHONSTARTUP")
                if pythonrc and os.path.isfile(pythonrc):
                    try:
                        execfile(pythonrc)
                    except NameError:
                        pass
                # This will import .pythonrc.py as a side-effect
                import user
            code.interact(local=imported_objects)

########NEW FILE########
__FILENAME__ = middleware
"""
Debug Toolbar middleware
"""
import os

from django.conf import settings
from django.http import HttpResponseRedirect
from django.shortcuts import render_to_response
from django.utils.encoding import smart_unicode
from django.conf.urls.defaults import include, patterns

import debug_toolbar.urls
from debug_toolbar.toolbar.loader import DebugToolbar

_HTML_TYPES = ('text/html', 'application/xhtml+xml')

def replace_insensitive(string, target, replacement):
    """
    Similar to string.replace() but is case insensitive
    Code borrowed from: http://forums.devshed.com/python-programming-11/case-insensitive-string-replace-490921.html
    """
    no_case = string.lower()
    index = no_case.rfind(target.lower())
    if index >= 0:
        return string[:index] + replacement + string[index + len(target):]
    else: # no results so return the original string
        return string

class DebugToolbarMiddleware(object):
    """
    Middleware to set up Debug Toolbar on incoming request and render toolbar
    on outgoing response.
    """
    def __init__(self):
        self.debug_toolbars = {}
        self.override_url = True

        # Set method to use to decide to show toolbar
        self.show_toolbar = self._show_toolbar # default

        # The tag to attach the toolbar to
        self.tag= u'</body>'

        if hasattr(settings, 'DEBUG_TOOLBAR_CONFIG'):
            show_toolbar_callback = settings.DEBUG_TOOLBAR_CONFIG.get(
                'SHOW_TOOLBAR_CALLBACK', None)
            if show_toolbar_callback:
                self.show_toolbar = show_toolbar_callback

            tag = settings.DEBUG_TOOLBAR_CONFIG.get('TAG', None)
            if tag:
                self.tag = u'</' + tag + u'>'

    def _show_toolbar(self, request):
        x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR', None)
        if x_forwarded_for:
            remote_addr = x_forwarded_for.split(',')[0].strip()
        else:
            remote_addr = request.META.get('REMOTE_ADDR', None)
        if not remote_addr in settings.INTERNAL_IPS \
            or (request.is_ajax() and \
                not debug_toolbar.urls._PREFIX in request.path) \
                    or not settings.DEBUG:
            return False
        return True

    def process_request(self, request):
        if self.show_toolbar(request):
            if self.override_url:
                original_urlconf = getattr(request, 'urlconf', settings.ROOT_URLCONF)
                debug_toolbar.urls.urlpatterns += patterns('',
                    ('', include(original_urlconf)),
                )
                self.override_url = False
            request.urlconf = 'debug_toolbar.urls'

            self.debug_toolbars[request] = DebugToolbar(request)
            for panel in self.debug_toolbars[request].panels:
                panel.process_request(request)

    def process_view(self, request, view_func, view_args, view_kwargs):
        if request in self.debug_toolbars:
            for panel in self.debug_toolbars[request].panels:
                panel.process_view(request, view_func, view_args, view_kwargs)

    def process_response(self, request, response):
        if request not in self.debug_toolbars:
            return response
        if self.debug_toolbars[request].config['INTERCEPT_REDIRECTS']:
            if isinstance(response, HttpResponseRedirect):
                redirect_to = response.get('Location', None)
                if redirect_to:
                    cookies = response.cookies
                    response = render_to_response(
                        'debug_toolbar/redirect.html',
                        {'redirect_to': redirect_to}
                    )
                    response.cookies = cookies
        if response.status_code == 200:
            for panel in self.debug_toolbars[request].panels:
                panel.process_response(request, response)
            if response['Content-Type'].split(';')[0] in _HTML_TYPES:
                response.content = replace_insensitive(
                    smart_unicode(response.content),
                    self.tag,
                    smart_unicode(self.debug_toolbars[request].render_toolbar() + self.tag))
            if response.get('Content-Length', None):
                response['Content-Length'] = len(response.content)
        del self.debug_toolbars[request]
        return response

########NEW FILE########
__FILENAME__ = models

########NEW FILE########
__FILENAME__ = cache
import time
import inspect

from django.core import cache
from django.core.cache.backends.base import BaseCache
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

class CacheStatTracker(BaseCache):
    """A small class used to track cache calls."""
    def __init__(self, cache):
        self.cache = cache
        self.reset()

    def reset(self):
        self.calls = []
        self.hits = 0
        self.misses = 0
        self.sets = 0
        self.gets = 0
        self.get_many = 0
        self.deletes = 0
        self.total_time = 0

    def _get_func_info(self):
        stack = inspect.stack()[2]
        return (stack[1], stack[2], stack[3], stack[4])

    def get(self, key, default=None):
        t = time.time()
        value = self.cache.get(key, default)
        this_time = time.time() - t
        self.total_time += this_time * 1000
        if value is None:
            self.misses += 1
        else:
            self.hits += 1
        self.gets += 1
        self.calls.append((this_time, 'get', (key,), self._get_func_info()))
        return value

    def set(self, key, value, timeout=None):
        t = time.time()
        self.cache.set(key, value, timeout)
        this_time = time.time() - t
        self.total_time += this_time * 1000
        self.sets += 1
        self.calls.append((this_time, 'set', (key, value, timeout), self._get_func_info()))

    def delete(self, key):
        t = time.time()
        self.cache.delete(key)
        this_time = time.time() - t
        self.total_time += this_time * 1000
        self.deletes += 1
        self.calls.append((this_time, 'delete', (key,), self._get_func_info()))

    def get_many(self, keys):
        t = time.time()
        results = self.cache.get_many(keys)
        this_time = time.time() - t
        self.total_time += this_time * 1000
        self.get_many += 1
        for key, value in results.iteritems():
            if value is None:
                self.misses += 1
            else:
                self.hits += 1
        self.calls.append((this_time, 'get_many', (keys,), self._get_func_info()))

class CacheDebugPanel(DebugPanel):
    """
    Panel that displays the cache statistics.
    """
    name = 'Cache'
    has_content = True

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        # This is hackish but to prevent threading issues is somewhat needed
        if isinstance(cache.cache, CacheStatTracker):
            cache.cache.reset()
            self.cache = cache.cache
        else:
            self.cache = CacheStatTracker(cache.cache)
            cache.cache = self.cache

    def nav_title(self):
        return _('Cache: %.2fms') % self.cache.total_time

    def title(self):
        return _('Cache Usage')

    def url(self):
        return ''

    def content(self):
        context = self.context.copy()
        context.update({
            'cache_calls': len(self.cache.calls),
            'cache_time': self.cache.total_time,
            'cache': self.cache,
        })
        return render_to_string('debug_toolbar/panels/cache.html', context)

########NEW FILE########
__FILENAME__ = headers
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

class HeaderDebugPanel(DebugPanel):
    """
    A panel to display HTTP headers.
    """
    name = 'Header'
    has_content = True
    # List of headers we want to display
    header_filter = (
        'CONTENT_TYPE',
        'HTTP_ACCEPT',
        'HTTP_ACCEPT_CHARSET',
        'HTTP_ACCEPT_ENCODING',
        'HTTP_ACCEPT_LANGUAGE',
        'HTTP_CACHE_CONTROL',
        'HTTP_CONNECTION',
        'HTTP_HOST',
        'HTTP_KEEP_ALIVE',
        'HTTP_REFERER',
        'HTTP_USER_AGENT',
        'QUERY_STRING',
        'REMOTE_ADDR',
        'REMOTE_HOST',
        'REQUEST_METHOD',
        'SCRIPT_NAME',
        'SERVER_NAME',
        'SERVER_PORT',
        'SERVER_PROTOCOL',
        'SERVER_SOFTWARE',
    )

    def nav_title(self):
        return _('HTTP Headers')

    def title(self):
        return _('HTTP Headers')

    def url(self):
        return ''

    def process_request(self, request):
        self.headers = dict(
            [(k, request.META[k]) for k in self.header_filter if k in request.META]
        )

    def content(self):
        context = self.context.copy()
        context.update({
            'headers': self.headers
        })
        return render_to_string('debug_toolbar/panels/headers.html', context)

########NEW FILE########
__FILENAME__ = logger
import datetime
import logging
try:
    import threading
except ImportError:
    threading = None
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

class ThreadTrackingHandler(logging.Handler):
    def __init__(self):
        if threading is None:
            raise NotImplementedError("threading module is not available, \
                the logging panel cannot be used without it")
        logging.Handler.__init__(self)
        self.records = {} # a dictionary that maps threads to log records

    def emit(self, record):
        self.get_records().append(record)

    def get_records(self, thread=None):
        """
        Returns a list of records for the provided thread, of if none is provided,
        returns a list for the current thread.
        """
        if thread is None:
            thread = threading.currentThread()
        if thread not in self.records:
            self.records[thread] = []
        return self.records[thread]

    def clear_records(self, thread=None):
        if thread is None:
            thread = threading.currentThread()
        if thread in self.records:
            del self.records[thread]

handler = ThreadTrackingHandler()
logging.root.setLevel(logging.NOTSET)
logging.root.addHandler(handler)

class LoggingPanel(DebugPanel):
    name = 'Logging'
    has_content = True

    def process_request(self, request):
        handler.clear_records()

    def get_and_delete(self):
        records = handler.get_records()
        handler.clear_records()
        return records

    def nav_title(self):
        return _("Logging")

    def nav_subtitle(self):
        # FIXME l10n: use ngettext
        return "%s message%s" % (len(handler.get_records()), (len(handler.get_records()) == 1) and '' or 's')

    def title(self):
        return _('Log Messages')

    def url(self):
        return ''

    def content(self):
        records = []
        for record in self.get_and_delete():
            records.append({
                'message': record.getMessage(),
                'time': datetime.datetime.fromtimestamp(record.created),
                'level': record.levelname,
                'file': record.pathname,
                'line': record.lineno,
            })

        context = self.context.copy()
        context.update({'records': records})

        return render_to_string('debug_toolbar/panels/logger.html', context)

########NEW FILE########
__FILENAME__ = request_vars
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

class RequestVarsDebugPanel(DebugPanel):
    """
    A panel to display request variables (POST/GET, session, cookies).
    """
    name = 'RequestVars'
    has_content = True

    def nav_title(self):
        return _('Request Vars')

    def title(self):
        return _('Request Vars')

    def url(self):
        return ''

    def process_request(self, request):
        self.request = request

    def process_view(self, request, view_func, view_args, view_kwargs):
        self.view_func = view_func
        self.view_args = view_args
        self.view_kwargs = view_kwargs

    def content(self):
        context = self.context.copy()
        context.update({
            'get': [(k, self.request.GET.getlist(k)) for k in self.request.GET],
            'post': [(k, self.request.POST.getlist(k)) for k in self.request.POST],
            'cookies': [(k, self.request.COOKIES.get(k)) for k in self.request.COOKIES],
            'view_func': '%s.%s' % (self.view_func.__module__, self.view_func.__name__),
            'view_args': self.view_args,
            'view_kwargs': self.view_kwargs
        })
        if hasattr(self.request, 'session'):
            context.update({
                'session': [(k, self.request.session.get(k)) for k in self.request.session.iterkeys()]
            })

        return render_to_string('debug_toolbar/panels/request_vars.html', context)

########NEW FILE########
__FILENAME__ = settings_vars
from django.conf import settings
from django.template.loader import render_to_string
from django.views.debug import get_safe_settings
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel


class SettingsVarsDebugPanel(DebugPanel):
    """
    A panel to display all variables in django.conf.settings
    """
    name = 'SettingsVars'
    has_content = True

    def nav_title(self):
        return _('Settings')

    def title(self):
        return _('Settings from <code>%s</code>') % settings.SETTINGS_MODULE

    def url(self):
        return ''

    def content(self):
        context = self.context.copy()
        context.update({
            'settings': get_safe_settings(),
        })
        return render_to_string('debug_toolbar/panels/settings_vars.html', context)

########NEW FILE########
__FILENAME__ = signals
import sys

from django.conf import settings
from django.core.signals import request_started, request_finished, \
    got_request_exception
from django.db.models.signals import class_prepared, pre_init, post_init, \
    pre_save, post_save, pre_delete, post_delete, post_syncdb
from django.dispatch.dispatcher import WEAKREF_TYPES
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _

try:
    from django.db.backends.signals import connection_created
except ImportError:
    connection_created = None

from debug_toolbar.panels import DebugPanel

class SignalDebugPanel(DebugPanel):
    name = "Signals"
    has_content = True

    SIGNALS = {
        'request_started': request_started,
        'request_finished': request_finished,
        'got_request_exception': got_request_exception,
        'connection_created': connection_created,
        'class_prepared': class_prepared,
        'pre_init': pre_init,
        'post_init': post_init,
        'pre_save': pre_save,
        'post_save': post_save,
        'pre_delete': pre_delete,
        'post_delete': post_delete,
        'post_syncdb': post_syncdb,
    }

    def nav_title(self):
        return _("Signals")

    def title(self):
        return _("Signals")

    def url(self):
        return ''

    def signals(self):
        signals = self.SIGNALS.copy()
        if hasattr(settings, 'DEBUG_TOOLBAR_CONFIG'):
            extra_signals = settings.DEBUG_TOOLBAR_CONFIG.get('EXTRA_SIGNALS', [])
        else:
            extra_signals = []
        for signal in extra_signals:
            parts = signal.split('.')
            path = '.'.join(parts[:-1])
            __import__(path)
            signals[parts[-1]] = getattr(sys.modules[path], parts[-1])
        return signals
    signals = property(signals)

    def content(self):
        signals = []
        keys = self.signals.keys()
        keys.sort()
        for name in keys:
            signal = self.signals[name]
            if signal is None:
                continue
            receivers = []
            for (receiverkey, r_senderkey), receiver in signal.receivers:
                if isinstance(receiver, WEAKREF_TYPES):
                    receiver = receiver()
                if receiver is None:
                    continue
                if getattr(receiver, 'im_self', None) is not None:
                    text = "method %s on %s object" % (receiver.__name__, receiver.im_self.__class__.__name__)
                elif getattr(receiver, 'im_class', None) is not None:
                    text = "method %s on %s" % (receiver.__name__, receiver.im_class.__name__)
                else:
                    text = "function %s" % receiver.__name__
                receivers.append(text)
            signals.append((name, signal, receivers))

        context = self.context.copy()
        context.update({'signals': signals})

        return render_to_string('debug_toolbar/panels/signals.html', context)

########NEW FILE########
__FILENAME__ = sql
from datetime import datetime
import os
import sys
import SocketServer
import traceback

import django
from django.conf import settings
from django.db import connection
from django.db.backends import util
from django.views.debug import linebreak_iter
from django.template import Node
from django.template.loader import render_to_string
from django.utils import simplejson
from django.utils.encoding import force_unicode
from django.utils.hashcompat import sha_constructor
from django.utils.translation import ugettext_lazy as _

from debug_toolbar.panels import DebugPanel
from debug_toolbar.utils import sqlparse

# Figure out some paths
django_path = os.path.realpath(os.path.dirname(django.__file__))
socketserver_path = os.path.realpath(os.path.dirname(SocketServer.__file__))

# TODO:This should be set in the toolbar loader as a default and panels should
# get a copy of the toolbar object with access to its config dictionary
SQL_WARNING_THRESHOLD = getattr(settings, 'DEBUG_TOOLBAR_CONFIG', {}) \
                            .get('SQL_WARNING_THRESHOLD', 500)

def tidy_stacktrace(strace):
    """
    Clean up stacktrace and remove all entries that:
    1. Are part of Django (except contrib apps)
    2. Are part of SocketServer (used by Django's dev server)
    3. Are the last entry (which is part of our stacktracing code)
    """
    trace = []
    for s in strace[:-1]:
        s_path = os.path.realpath(s[0])
        if getattr(settings, 'DEBUG_TOOLBAR_CONFIG', {}).get('HIDE_DJANGO_SQL', True) \
            and django_path in s_path and not 'django/contrib' in s_path:
            continue
        if socketserver_path in s_path:
            continue
        trace.append((s[0], s[1], s[2], s[3]))
    return trace

def get_template_info(source, context_lines=3):
    line = 0
    upto = 0
    source_lines = []
    before = during = after = ""

    origin, (start, end) = source
    template_source = origin.reload()

    for num, next in enumerate(linebreak_iter(template_source)):
        if start >= upto and end <= next:
            line = num
            before = template_source[upto:start]
            during = template_source[start:end]
            after = template_source[end:next]
        source_lines.append((num, template_source[upto:next]))
        upto = next

    top = max(1, line - context_lines)
    bottom = min(len(source_lines), line + 1 + context_lines)

    context = []
    for num, content in source_lines[top:bottom]:
        context.append({
            'num': num,
            'content': content,
            'highlight': (num == line),
        })

    return {
        'name': origin.name,
        'context': context,
    }

class DatabaseStatTracker(util.CursorDebugWrapper):
    """
    Replacement for CursorDebugWrapper which stores additional information
    in `connection.queries`.
    """
    def execute(self, sql, params=()):
        start = datetime.now()
        try:
            return self.cursor.execute(sql, params)
        finally:
            stop = datetime.now()
            duration = ms_from_timedelta(stop - start)
            stacktrace = tidy_stacktrace(traceback.extract_stack())
            _params = ''
            try:
                _params = simplejson.dumps([force_unicode(x, strings_only=True) for x in params])
            except TypeError:
                pass # object not JSON serializable

            template_info = None
            cur_frame = sys._getframe().f_back
            try:
                while cur_frame is not None:
                    if cur_frame.f_code.co_name == 'render':
                        node = cur_frame.f_locals['self']
                        if isinstance(node, Node):
                            template_info = get_template_info(node.source)
                            break
                    cur_frame = cur_frame.f_back
            except:
                pass
            del cur_frame

            # We keep `sql` to maintain backwards compatibility
            self.db.queries.append({
                'sql': self.db.ops.last_executed_query(self.cursor, sql, params),
                'duration': duration,
                'raw_sql': sql,
                'params': _params,
                'hash': sha_constructor(settings.SECRET_KEY + sql + _params).hexdigest(),
                'stacktrace': stacktrace,
                'start_time': start,
                'stop_time': stop,
                'is_slow': (duration > SQL_WARNING_THRESHOLD),
                'is_select': sql.lower().strip().startswith('select'),
                'template_info': template_info,
            })
util.CursorDebugWrapper = DatabaseStatTracker

class SQLDebugPanel(DebugPanel):
    """
    Panel that displays information about the SQL queries run while processing
    the request.
    """
    name = 'SQL'
    has_content = True

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self._offset = len(connection.queries)
        self._sql_time = 0
        self._queries = []

    def nav_title(self):
        return _('SQL')

    def nav_subtitle(self):
        self._queries = connection.queries[self._offset:]
        self._sql_time = sum([q['duration'] for q in self._queries])
        num_queries = len(self._queries)
        # TODO l10n: use ngettext
        return "%d %s in %.2fms" % (
            num_queries,
            (num_queries == 1) and 'query' or 'queries',
            self._sql_time
        )

    def title(self):
        return _('SQL Queries')

    def url(self):
        return ''

    def content(self):
        width_ratio_tally = 0
        for query in self._queries:
            query['sql'] = reformat_sql(query['sql'])
            try:
                query['width_ratio'] = (query['duration'] / self._sql_time) * 100
            except ZeroDivisionError:
                query['width_ratio'] = 0
            query['start_offset'] = width_ratio_tally
            width_ratio_tally += query['width_ratio']

        context = self.context.copy()
        context.update({
            'queries': self._queries,
            'sql_time': self._sql_time,
            'is_mysql': settings.DATABASE_ENGINE == 'mysql',
        })

        return render_to_string('debug_toolbar/panels/sql.html', context)

def ms_from_timedelta(td):
    """
    Given a timedelta object, returns a float representing milliseconds
    """
    return (td.seconds * 1000) + (td.microseconds / 1000.0)

class BoldKeywordFilter(sqlparse.filters.Filter):
    """sqlparse filter to bold SQL keywords"""
    def process(self, stack, stream):
        """Process the token stream"""
        for token_type, value in stream:
            is_keyword = token_type in sqlparse.tokens.Keyword
            if is_keyword:
                yield sqlparse.tokens.Text, '<strong>'
            yield token_type, django.utils.html.escape(value)
            if is_keyword:
                yield sqlparse.tokens.Text, '</strong>'

def reformat_sql(sql):
    stack = sqlparse.engine.FilterStack()
    stack.preprocess.append(BoldKeywordFilter()) # add our custom filter
    stack.postprocess.append(sqlparse.filters.SerializerUnicode()) # tokens -> strings
    return ''.join(stack.run(sql))

########NEW FILE########
__FILENAME__ = template
from os.path import normpath
from pprint import pformat

from django import http
from django.conf import settings
from django.core.signals import request_started
from django.dispatch import Signal
from django.template.context import get_standard_processors
from django.template.loader import render_to_string
from django.test.signals import template_rendered
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

# Code taken and adapted from Simon Willison and Django Snippets:
# http://www.djangosnippets.org/snippets/766/

# Monkeypatch instrumented test renderer from django.test.utils - we could use
# django.test.utils.setup_test_environment for this but that would also set up
# e-mail interception, which we don't want
from django.test.utils import instrumented_test_render
from django.template import Template

if not hasattr(Template, '_render'): # Django < 1.2
    if Template.render != instrumented_test_render:
        Template.original_render = Template.render
        Template.render = instrumented_test_render
else:
    if Template._render != instrumented_test_render:
        Template.original_render = Template._render
        Template._render = instrumented_test_render

# MONSTER monkey-patch
old_template_init = Template.__init__
def new_template_init(self, template_string, origin=None, name='<Unknown Template>'):
    old_template_init(self, template_string, origin, name)
    self.origin = origin
Template.__init__ = new_template_init

class TemplateDebugPanel(DebugPanel):
    """
    A panel that lists all templates used during processing of a response.
    """
    name = 'Template'
    has_content = True

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.templates = []
        template_rendered.connect(self._store_template_info)

    def _store_template_info(self, sender, **kwargs):
        self.templates.append(kwargs)

    def nav_title(self):
        return _('Templates')

    def title(self):
        num_templates = len([t for t in self.templates
            if not t['template'].name.startswith('debug_toolbar/')])
        return _('Templates (%(num_templates)s rendered)') % {'num_templates': num_templates}

    def url(self):
        return ''

    def process_request(self, request):
        self.request = request

    def content(self):
        context_processors = dict(
            [
                ("%s.%s" % (k.__module__, k.__name__),
                    pformat(k(self.request))) for k in get_standard_processors()
            ]
        )
        template_context = []
        for template_data in self.templates:
            info = {}
            # Clean up some info about templates
            template = template_data.get('template', None)
            # Skip templates that we are generating through the debug toolbar.
            if template.name.startswith('debug_toolbar/'):
                continue
            if template.origin and template.origin.name:
                template.origin_name = template.origin.name
            else:
                template.origin_name = 'No origin'
            info['template'] = template
            # Clean up context for better readability
            if getattr(settings, 'DEBUG_TOOLBAR_CONFIG', {}).get('SHOW_TEMPLATE_CONTEXT', True):
                context_data = template_data.get('context', None)

                context_list = []
                for context_layer in context_data.dicts:
                    if hasattr(context_layer, 'items'):
                        for key, value in context_layer.items():
                            # Replace any request elements - they have a large
                            # unicode representation and the request data is
                            # already made available from the Request Vars panel.
                            if isinstance(value, http.HttpRequest):
                                context_layer[key] = '<<request>>'
                            # Replace the debugging sql_queries element. The SQL
                            # data is already made available from the SQL panel.
                            elif key == 'sql_queries' and isinstance(value, list):
                                context_layer[key] = '<<sql_queries>>'
                            # Replace LANGUAGES, which is available in i18n context processor
                            elif key == 'LANGUAGES' and isinstance(value, tuple):
                                context_layer[key] = '<<languages>>'
                    try:
                        context_list.append(pformat(context_layer))
                    except UnicodeEncodeError:
                        pass
                info['context'] = '\n'.join(context_list)
            template_context.append(info)

        context = self.context.copy()
        context.update({
            'templates': template_context,
            'template_dirs': [normpath(x) for x in settings.TEMPLATE_DIRS],
            'context_processors': context_processors,
        })

        return render_to_string('debug_toolbar/panels/templates.html', context)

########NEW FILE########
__FILENAME__ = timer
try:
    import resource
except ImportError:
    pass # Will fail on Win32 systems
import time
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from debug_toolbar.panels import DebugPanel

class TimerDebugPanel(DebugPanel):
    """
    Panel that displays the time a response took in milliseconds.
    """
    name = 'Timer'
    try: # if resource module not available, don't show content panel
        resource
    except NameError:
        has_content = False
        has_resource = False
    else:
        has_content = True
        has_resource = True

    def process_request(self, request):
        self._start_time = time.time()
        if self.has_resource:
            self._start_rusage = resource.getrusage(resource.RUSAGE_SELF)

    def process_response(self, request, response):
        self.total_time = (time.time() - self._start_time) * 1000
        if self.has_resource:
            self._end_rusage = resource.getrusage(resource.RUSAGE_SELF)

    def nav_title(self):
        return _('Time')

    def nav_subtitle(self):
        # TODO l10n
        if self.has_resource:
            utime = self._end_rusage.ru_utime - self._start_rusage.ru_utime
            stime = self._end_rusage.ru_stime - self._start_rusage.ru_stime
            return 'CPU: %0.2fms (%0.2fms)' % ((utime + stime) * 1000.0, self.total_time)
        else:
            return 'TOTAL: %0.2fms' % (self.total_time)

    def title(self):
        return _('Resource Usage')

    def url(self):
        return ''

    def _elapsed_ru(self, name):
        return getattr(self._end_rusage, name) - getattr(self._start_rusage, name)

    def content(self):

        utime = 1000 * self._elapsed_ru('ru_utime')
        stime = 1000 * self._elapsed_ru('ru_stime')
        vcsw = self._elapsed_ru('ru_nvcsw')
        ivcsw = self._elapsed_ru('ru_nivcsw')
        minflt = self._elapsed_ru('ru_minflt')
        majflt = self._elapsed_ru('ru_majflt')

# these are documented as not meaningful under Linux.  If you're running BSD
# feel free to enable them, and add any others that I hadn't gotten to before
# I noticed that I was getting nothing but zeroes and that the docs agreed. :-(
#
#        blkin = self._elapsed_ru('ru_inblock')
#        blkout = self._elapsed_ru('ru_oublock')
#        swap = self._elapsed_ru('ru_nswap')
#        rss = self._end_rusage.ru_maxrss
#        srss = self._end_rusage.ru_ixrss
#        urss = self._end_rusage.ru_idrss
#        usrss = self._end_rusage.ru_isrss

        # TODO l10n on values
        rows = (
            (_('User CPU time'), '%0.3f msec' % utime),
            (_('System CPU time'), '%0.3f msec' % stime),
            (_('Total CPU time'), '%0.3f msec' % (utime + stime)),
            (_('Elapsed time'), '%0.3f msec' % self.total_time),
            (_('Context switches'), '%d voluntary, %d involuntary' % (vcsw, ivcsw)),
#            ('Memory use', '%d max RSS, %d shared, %d unshared' % (rss, srss, urss + usrss)),
#            ('Page faults', '%d no i/o, %d requiring i/o' % (minflt, majflt)),
#            ('Disk operations', '%d in, %d out, %d swapout' % (blkin, blkout, swap)),
        )

        context = self.context.copy()
        context.update({
            'rows': rows,
        })

        return render_to_string('debug_toolbar/panels/timer.html', context)

########NEW FILE########
__FILENAME__ = version
import sys

import django
from django.conf import settings
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _

import debug_toolbar
from debug_toolbar.panels import DebugPanel


class VersionDebugPanel(DebugPanel):
    """
    Panel that displays the Django version.
    """
    name = 'Version'
    has_content = True

    def nav_title(self):
        return _('Versions')

    def nav_subtitle(self):
        return 'Django %s' % django.get_version()

    def url(self):
        return ''

    def title(self):
        return _('Versions')

    def content(self):
        versions = {}
        for app in settings.INSTALLED_APPS + ['django']:
            name = app.split('.')[-1].replace('_', ' ').capitalize()
            __import__(app)
            app = sys.modules[app]
            if hasattr(app, 'get_version'):
                get_version = app.get_version
                if callable(get_version):
                    version = get_version()
                else:
                    version = get_version
            elif hasattr(app, 'VERSION'):
                version = app.VERSION
            elif hasattr(app, '__version__'):
                version = app.__version__
            else:
                continue
            if isinstance(version, (list, tuple)):
                version = '.'.join(str(o) for o in version)
            versions[name] = version

        context = self.context.copy()
        context.update({
            'versions': versions,
            'paths': sys.path,
        })

        return render_to_string('debug_toolbar/panels/versions.html', context)

########NEW FILE########
__FILENAME__ = loader
"""
The main DebugToolbar class that loads and renders the Toolbar.
"""
from django.conf import settings
from django.template.loader import render_to_string

class DebugToolbar(object):

    def __init__(self, request):
        self.request = request
        self.panels = []
        base_url = self.request.META.get('SCRIPT_NAME', '')
        self.config = {
            'INTERCEPT_REDIRECTS': True,
            'MEDIA_URL': u'%s/__debug__/m/' % base_url
        }
        # Check if settings has a DEBUG_TOOLBAR_CONFIG and updated config
        self.config.update(getattr(settings, 'DEBUG_TOOLBAR_CONFIG', {}))
        self.template_context = {
            'BASE_URL': base_url, # for backwards compatibility
            'DEBUG_TOOLBAR_MEDIA_URL': self.config.get('MEDIA_URL'),
        }
        # Override this tuple by copying to settings.py as `DEBUG_TOOLBAR_PANELS`
        self.default_panels = (
            'debug_toolbar.panels.version.VersionDebugPanel',
            'debug_toolbar.panels.timer.TimerDebugPanel',
            'debug_toolbar.panels.settings_vars.SettingsVarsDebugPanel',
            'debug_toolbar.panels.headers.HeaderDebugPanel',
            'debug_toolbar.panels.request_vars.RequestVarsDebugPanel',
            'debug_toolbar.panels.sql.SQLDebugPanel',
            'debug_toolbar.panels.template.TemplateDebugPanel',
            #'debug_toolbar.panels.cache.CacheDebugPanel',
            'debug_toolbar.panels.signals.SignalDebugPanel',
            'debug_toolbar.panels.logger.LoggingPanel',
        )
        self.load_panels()

    def load_panels(self):
        """
        Populate debug panels
        """
        from django.conf import settings
        from django.core import exceptions

        # Check if settings has a DEBUG_TOOLBAR_PANELS, otherwise use default
        if hasattr(settings, 'DEBUG_TOOLBAR_PANELS'):
            self.default_panels = settings.DEBUG_TOOLBAR_PANELS

        for panel_path in self.default_panels:
            try:
                dot = panel_path.rindex('.')
            except ValueError:
                raise exceptions.ImproperlyConfigured, '%s isn\'t a debug panel module' % panel_path
            panel_module, panel_classname = panel_path[:dot], panel_path[dot+1:]
            try:
                mod = __import__(panel_module, {}, {}, [''])
            except ImportError, e:
                raise exceptions.ImproperlyConfigured, 'Error importing debug panel %s: "%s"' % (panel_module, e)
            try:
                panel_class = getattr(mod, panel_classname)
            except AttributeError:
                raise exceptions.ImproperlyConfigured, 'Toolbar Panel module "%s" does not define a "%s" class' % (panel_module, panel_classname)

            try:
                panel_instance = panel_class(context=self.template_context)
            except:
                raise # Bubble up problem loading panel

            self.panels.append(panel_instance)

    def render_toolbar(self):
        """
        Renders the overall Toolbar with panels inside.
        """
        context = self.template_context.copy()
        context.update({ 'panels': self.panels, })

        return render_to_string('debug_toolbar/base.html', context)

########NEW FILE########
__FILENAME__ = urls
"""
URLpatterns for the debug toolbar.

These should not be loaded explicitly; the debug toolbar middleware will patch
this into the urlconf for the request.
"""
from django.conf.urls.defaults import *
from django.conf import settings

_PREFIX = '__debug__'

urlpatterns = patterns('',
    url(r'^%s/m/(.*)$' % _PREFIX, 'debug_toolbar.views.debug_media'),
    url(r'^%s/sql_select/$' % _PREFIX, 'debug_toolbar.views.sql_select', name='sql_select'),
    url(r'^%s/sql_explain/$' % _PREFIX, 'debug_toolbar.views.sql_explain', name='sql_explain'),
    url(r'^%s/sql_profile/$' % _PREFIX, 'debug_toolbar.views.sql_profile', name='sql_profile'),
    url(r'^%s/template_source/$' % _PREFIX, 'debug_toolbar.views.template_source', name='template_source'),
)

########NEW FILE########
__FILENAME__ = filter
# -*- coding: utf-8 -*-

from debug_toolbar.utils.sqlparse import tokens as T
from debug_toolbar.utils.sqlparse.engine.grouping import Statement, Token


class TokenFilter(object):

    def __init__(self, **options):
        self.options = options

    def process(self, stack, stream):
        """Process token stream."""
        raise NotImplementedError


class StatementFilter(TokenFilter):

    def __init__(self):
        TokenFilter.__init__(self)
        self._in_declare = False
        self._in_dbldollar = False
        self._is_create = False

    def _reset(self):
        self._in_declare = False
        self._in_dbldollar = False
        self._is_create = False

    def _change_splitlevel(self, ttype, value):
        # PostgreSQL
        if (ttype == T.Name.Builtin
            and value.startswith('$') and value.endswith('$')):
            if self._in_dbldollar:
                self._in_dbldollar = False
                return -1
            else:
                self._in_dbldollar = True
                return 1
        elif self._in_dbldollar:
            return 0

        # ANSI
        if ttype is not T.Keyword:
            return 0

        unified = value.upper()

        if unified == 'DECLARE':
            self._in_declare = True
            return 1

        if unified == 'BEGIN':
            if self._in_declare:
                return 0
            return 0

        if unified == 'END':
            # Should this respect a preceeding BEGIN?
            # In CASE ... WHEN ... END this results in a split level -1.
            return -1

        if ttype is T.Keyword.DDL and unified.startswith('CREATE'):
            self._is_create = True

        if unified in ('IF', 'FOR') and self._is_create:
            return 1

        # Default
        return 0

    def process(self, stack, stream):
        splitlevel = 0
        stmt = None
        consume_ws = False
        stmt_tokens = []
        for ttype, value in stream:
            # Before appending the token
            if (consume_ws and ttype is not T.Whitespace
                and ttype is not T.Comment.Single):
                consume_ws = False
                stmt.tokens = stmt_tokens
                yield stmt
                self._reset()
                stmt = None
                splitlevel = 0
            if stmt is None:
                stmt = Statement()
                stmt_tokens = []
            splitlevel += self._change_splitlevel(ttype, value)
            # Append the token
            stmt_tokens.append(Token(ttype, value))
            # After appending the token
            if (splitlevel <= 0 and ttype is T.Punctuation
                and value == ';'):
                consume_ws = True
        if stmt is not None:
            stmt.tokens = stmt_tokens
            yield stmt

########NEW FILE########
__FILENAME__ = grouping
# -*- coding: utf-8 -*-

import itertools
import re
import types

from debug_toolbar.utils.sqlparse import tokens as T
from debug_toolbar.utils.sqlparse.sql import *



def _group_left_right(tlist, ttype, value, cls,
                      check_right=lambda t: True,
                      include_semicolon=False):
    [_group_left_right(sgroup, ttype, value, cls, check_right,
                       include_semicolon) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, cls)]
    idx = 0
    token = tlist.token_next_match(idx, ttype, value)
    while token:
        right = tlist.token_next(tlist.token_index(token))
        left = tlist.token_prev(tlist.token_index(token))
        if (right is None or not check_right(right)
            or left is None):
            token = tlist.token_next_match(tlist.token_index(token)+1,
                                           ttype, value)
        else:
            if include_semicolon:
                right = tlist.token_next_match(tlist.token_index(right),
                                               T.Punctuation, ';')
            tokens = tlist.tokens_between(left, right)[1:]
            if not isinstance(left, cls):
                new = cls([left])
                new_idx = tlist.token_index(left)
                tlist.tokens.remove(left)
                tlist.tokens.insert(new_idx, new)
                left = new
            left.tokens.extend(tokens)
            for t in tokens:
                tlist.tokens.remove(t)
            token = tlist.token_next_match(tlist.token_index(left)+1,
                                           ttype, value)

def _group_matching(tlist, start_ttype, start_value, end_ttype, end_value,
                    cls, include_semicolon=False, recurse=False):
    def _find_matching(i, tl, stt, sva, ett, eva):
        depth = 1
        for t in tl.tokens[i:]:
            if t.match(stt, sva):
                depth += 1
            elif t.match(ett, eva):
                depth -= 1
                if depth == 1:
                    return t
        return None
    [_group_matching(sgroup, start_ttype, start_value, end_ttype, end_value,
                     cls, include_semicolon) for sgroup in tlist.get_sublists()
     if recurse]
    if isinstance(tlist, cls):
        idx = 1
    else:
        idx = 0
    token = tlist.token_next_match(idx, start_ttype, start_value)
    while token:
        tidx = tlist.token_index(token)
        end = _find_matching(tidx, tlist, start_ttype, start_value,
                             end_ttype, end_value)
        if end is None:
            idx = tidx+1
        else:
            if include_semicolon:
                next_ = tlist.token_next(tlist.token_index(end))
                if next_ and next_.match(T.Punctuation, ';'):
                    end = next_
            group = tlist.group_tokens(cls, tlist.tokens_between(token, end))
            _group_matching(group, start_ttype, start_value,
                            end_ttype, end_value, cls, include_semicolon)
            idx = tlist.token_index(group)+1
        token = tlist.token_next_match(idx, start_ttype, start_value)

def group_if(tlist):
    _group_matching(tlist, T.Keyword, 'IF', T.Keyword, 'END IF', If, True)

def group_for(tlist):
    _group_matching(tlist, T.Keyword, 'FOR', T.Keyword, 'END LOOP', For, True)

def group_as(tlist):
    _group_left_right(tlist, T.Keyword, 'AS', Identifier)

def group_assignment(tlist):
    _group_left_right(tlist, T.Assignment, ':=', Assignment,
                      include_semicolon=True)

def group_comparsion(tlist):
    _group_left_right(tlist, T.Operator, None, Comparsion)


def group_case(tlist):
    _group_matching(tlist, T.Keyword, 'CASE', T.Keyword, 'END', Case,
                    include_semicolon=True, recurse=True)


def group_identifier(tlist):
    def _consume_cycle(tl, i):
        x = itertools.cycle((lambda y: y.match(T.Punctuation, '.'),
                             lambda y: y.ttype in (T.String.Symbol,
                                                   T.Name,
                                                   T.Wildcard)))
        for t in tl.tokens[i:]:
            if x.next()(t):
                yield t
            else:
                raise StopIteration

    # bottom up approach: group subgroups first
    [group_identifier(sgroup) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, Identifier)]

    # real processing
    idx = 0
    token = tlist.token_next_by_type(idx, (T.String.Symbol, T.Name))
    while token:
        identifier_tokens = [token]+list(
            _consume_cycle(tlist,
                           tlist.token_index(token)+1))
        group = tlist.group_tokens(Identifier, identifier_tokens)
        idx = tlist.token_index(group)+1
        token = tlist.token_next_by_type(idx, (T.String.Symbol, T.Name))


def group_identifier_list(tlist):
    [group_identifier_list(sgroup) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, (Identifier, IdentifierList))]
    idx = 0
    # Allowed list items
    fend1_funcs = [lambda t: isinstance(t, Identifier),
                   lambda t: t.is_whitespace(),
                   lambda t: t.ttype == T.Wildcard,
                   lambda t: t.match(T.Keyword, 'null'),
                   lambda t: t.ttype == T.Number.Integer,
                   lambda t: t.ttype == T.String.Single,
                   lambda t: isinstance(t, Comparsion),
                   ]
    tcomma = tlist.token_next_match(idx, T.Punctuation, ',')
    start = None
    while tcomma is not None:
        before = tlist.token_prev(tcomma)
        after = tlist.token_next(tcomma)
        # Check if the tokens around tcomma belong to a list
        bpassed = apassed = False
        for func in fend1_funcs:
            if before is not None and func(before):
                bpassed = True
            if after is not None and func(after):
                apassed = True
        if not bpassed or not apassed:
            # Something's wrong here, skip ahead to next ","
            start = None
            tcomma = tlist.token_next_match(tlist.token_index(tcomma)+1,
                                            T.Punctuation, ',')
        else:
            if start is None:
                start = before
            next_ = tlist.token_next(after)
            if next_ is None or not next_.match(T.Punctuation, ','):
                # Reached the end of the list
                tokens = tlist.tokens_between(start, after)
                group = tlist.group_tokens(IdentifierList, tokens)
                start = None
                tcomma = tlist.token_next_match(tlist.token_index(group)+1,
                                                T.Punctuation, ',')
            else:
                tcomma = next_


def group_parenthesis(tlist):
    _group_matching(tlist, T.Punctuation, '(', T.Punctuation, ')', Parenthesis)

def group_comments(tlist):
    [group_comments(sgroup) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, Comment)]
    idx = 0
    token = tlist.token_next_by_type(idx, T.Comment)
    while token:
        tidx = tlist.token_index(token)
        end = tlist.token_not_matching(tidx+1,
                                       [lambda t: t.ttype in T.Comment,
                                        lambda t: t.is_whitespace()])
        if end is None:
            idx = tidx + 1
        else:
            eidx = tlist.token_index(end)
            grp_tokens = tlist.tokens_between(token,
                                              tlist.token_prev(eidx, False))
            group = tlist.group_tokens(Comment, grp_tokens)
            idx = tlist.token_index(group)
        token = tlist.token_next_by_type(idx, T.Comment)

def group_where(tlist):
    [group_where(sgroup) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, Where)]
    idx = 0
    token = tlist.token_next_match(idx, T.Keyword, 'WHERE')
    stopwords = ('ORDER', 'GROUP', 'LIMIT', 'UNION')
    while token:
        tidx = tlist.token_index(token)
        end = tlist.token_next_match(tidx+1, T.Keyword, stopwords)
        if end is None:
            end = tlist.tokens[-1]
        else:
            end = tlist.tokens[tlist.token_index(end)-1]
        group = tlist.group_tokens(Where, tlist.tokens_between(token, end))
        idx = tlist.token_index(group)
        token = tlist.token_next_match(idx, T.Keyword, 'WHERE')

def group_aliased(tlist):
    [group_aliased(sgroup) for sgroup in tlist.get_sublists()
     if not isinstance(sgroup, Identifier)]
    idx = 0
    token = tlist.token_next_by_instance(idx, Identifier)
    while token:
        next_ = tlist.token_next(tlist.token_index(token))
        if next_ is not None and isinstance(next_, Identifier):
            grp = tlist.tokens_between(token, next_)[1:]
            token.tokens.extend(grp)
            for t in grp:
                tlist.tokens.remove(t)
        idx = tlist.token_index(token)+1
        token = tlist.token_next_by_instance(idx, Identifier)


def group_typecasts(tlist):
    _group_left_right(tlist, T.Punctuation, '::', Identifier)


def group(tlist):
    for func in [group_parenthesis,
                 group_comments,
                 group_where,
                 group_case,
                 group_identifier,
                 group_typecasts,
                 group_as,
                 group_aliased,
                 group_assignment,
                 group_comparsion,
                 group_identifier_list,
                 group_if,
                 group_for,]:
        func(tlist)

########NEW FILE########
__FILENAME__ = filters
# -*- coding: utf-8 -*-

import re

from debug_toolbar.utils.sqlparse.engine import grouping
from debug_toolbar.utils.sqlparse import tokens as T
from debug_toolbar.utils.sqlparse import sql


class Filter(object):

    def process(self, *args):
        raise NotImplementedError


class TokenFilter(Filter):

    def process(self, stack, stream):
        raise NotImplementedError


# FIXME: Should be removed
def rstrip(stream):
    buff = []
    for token in stream:
        if token.is_whitespace() and '\n' in token.value:
            # assuming there's only one \n in value
            before, rest = token.value.split('\n', 1)
            token.value = '\n%s' % rest
            buff = []
            yield token
        elif token.is_whitespace():
            buff.append(token)
        elif token.is_group():
            token.tokens = list(rstrip(token.tokens))
            # process group and look if it starts with a nl
            if token.tokens and token.tokens[0].is_whitespace():
                before, rest = token.tokens[0].value.split('\n', 1)
                token.tokens[0].value = '\n%s' % rest
                buff = []
            while buff:
                yield buff.pop(0)
            yield token
        else:
            while buff:
                yield buff.pop(0)
            yield token


# --------------------------
# token process

class _CaseFilter(TokenFilter):

    ttype = None

    def __init__(self, case=None):
        if case is None:
            case = 'upper'
        assert case in ['lower', 'upper', 'capitalize']
        self.convert = getattr(unicode, case)

    def process(self, stack, stream):
        for ttype, value in stream:
            if ttype in self.ttype:
                value = self.convert(value)
            yield ttype, value


class KeywordCaseFilter(_CaseFilter):
    ttype = T.Keyword


class IdentifierCaseFilter(_CaseFilter):
    ttype = (T.Name, T.String.Symbol)


# ----------------------
# statement process

class StripCommentsFilter(Filter):

    def _process(self, tlist):
        idx = 0
        clss = set([x.__class__ for x in tlist.tokens])
        while grouping.Comment in clss:
            token = tlist.token_next_by_instance(0, grouping.Comment)
            tidx = tlist.token_index(token)
            prev = tlist.token_prev(tidx, False)
            next_ = tlist.token_next(tidx, False)
            # Replace by whitespace if prev and next exist and if they're not
            # whitespaces. This doesn't apply if prev or next is a paranthesis.
            if (prev is not None and next_ is not None
                and not prev.is_whitespace() and not next_.is_whitespace()
                and not (prev.match(T.Punctuation, '(')
                         or next_.match(T.Punctuation, ')'))):
                tlist.tokens[tidx] = grouping.Token(T.Whitespace, ' ')
            else:
                tlist.tokens.pop(tidx)
            clss = set([x.__class__ for x in tlist.tokens])

    def process(self, stack, stmt):
        [self.process(stack, sgroup) for sgroup in stmt.get_sublists()]
        self._process(stmt)


class StripWhitespaceFilter(Filter):

    def _stripws(self, tlist):
        func_name = '_stripws_%s' % tlist.__class__.__name__.lower()
        func = getattr(self, func_name, self._stripws_default)
        func(tlist)

    def _stripws_default(self, tlist):
        last_was_ws = False
        for token in tlist.tokens:
            if token.is_whitespace():
                if last_was_ws:
                    token.value = ''
                else:
                    token.value = ' '
            last_was_ws = token.is_whitespace()

    def _stripws_parenthesis(self, tlist):
        if tlist.tokens[1].is_whitespace():
            tlist.tokens.pop(1)
        if tlist.tokens[-2].is_whitespace():
            tlist.tokens.pop(-2)
        self._stripws_default(tlist)

    def process(self, stack, stmt):
        [self.process(stack, sgroup) for sgroup in stmt.get_sublists()]
        self._stripws(stmt)
        if stmt.tokens[-1].is_whitespace():
            stmt.tokens.pop(-1)


class ReindentFilter(Filter):

    def __init__(self, width=2, char=' ', line_width=None):
        self.width = width
        self.char = char
        self.indent = 0
        self.offset = 0
        self.line_width = line_width
        self._curr_stmt = None
        self._last_stmt = None

    def _get_offset(self, token):
        all_ = list(self._curr_stmt.flatten())
        idx = all_.index(token)
        raw = ''.join(unicode(x) for x in all_[:idx+1])
        line = raw.splitlines()[-1]
        # Now take current offset into account and return relative offset.
        full_offset = len(line)-(len(self.char*(self.width*self.indent)))
        return full_offset - self.offset

    def nl(self):
        # TODO: newline character should be configurable
        ws = '\n'+(self.char*((self.indent*self.width)+self.offset))
        return grouping.Token(T.Whitespace, ws)

    def _split_kwds(self, tlist):
        split_words = ('FROM', 'JOIN$', 'AND', 'OR',
                       'GROUP', 'ORDER', 'UNION', 'VALUES',
                       'SET')
        idx = 0
        token = tlist.token_next_match(idx, T.Keyword, split_words,
                                       regex=True)
        while token:
            prev = tlist.token_prev(tlist.token_index(token), False)
            offset = 1
            if prev and prev.is_whitespace():
                tlist.tokens.pop(tlist.token_index(prev))
                offset += 1
            if (prev
                and isinstance(prev, sql.Comment)
                and (str(prev).endswith('\n')
                     or str(prev).endswith('\r'))):
                nl = tlist.token_next(token)
            else:
                nl = self.nl()
                tlist.insert_before(token, nl)
            token = tlist.token_next_match(tlist.token_index(nl)+offset,
                                           T.Keyword, split_words, regex=True)

    def _split_statements(self, tlist):
        idx = 0
        token = tlist.token_next_by_type(idx, (T.Keyword.DDL, T.Keyword.DML))
        while token:
            prev = tlist.token_prev(tlist.token_index(token), False)
            if prev and prev.is_whitespace():
                tlist.tokens.pop(tlist.token_index(prev))
            # only break if it's not the first token
            if prev:
                nl = self.nl()
                tlist.insert_before(token, nl)
            token = tlist.token_next_by_type(tlist.token_index(token)+1,
                                             (T.Keyword.DDL, T.Keyword.DML))

    def _process(self, tlist):
        func_name = '_process_%s' % tlist.__class__.__name__.lower()
        func = getattr(self, func_name, self._process_default)
        func(tlist)

    def _process_where(self, tlist):
        token = tlist.token_next_match(0, T.Keyword, 'WHERE')
        tlist.insert_before(token, self.nl())
        self.indent += 1
        self._process_default(tlist)
        self.indent -= 1

    def _process_parenthesis(self, tlist):
        first = tlist.token_next(0)
        indented = False
        if first and first.ttype in (T.Keyword.DML, T.Keyword.DDL):
            self.indent += 1
            tlist.tokens.insert(0, self.nl())
            indented = True
        num_offset = self._get_offset(tlist.token_next_match(0,
                                                        T.Punctuation, '('))
        self.offset += num_offset
        self._process_default(tlist, stmts=not indented)
        if indented:
            self.indent -= 1
        self.offset -= num_offset

    def _process_identifierlist(self, tlist):
        identifiers = tlist.get_identifiers()
        if len(identifiers) > 1:
            first = list(identifiers[0].flatten())[0]
            num_offset = self._get_offset(first)-len(first.value)
            self.offset += num_offset
            for token in identifiers[1:]:
                tlist.insert_before(token, self.nl())
            self.offset -= num_offset
        self._process_default(tlist)

    def _process_case(self, tlist):
        cases = tlist.get_cases()
        is_first = True
        num_offset = None
        case = tlist.tokens[0]
        outer_offset = self._get_offset(case)-len(case.value)
        self.offset += outer_offset
        for cond, value in tlist.get_cases():
            if is_first:
                is_first = False
                num_offset = self._get_offset(cond[0])-len(cond[0].value)
                self.offset += num_offset
                continue
            if cond is None:
                token = value[0]
            else:
                token = cond[0]
            tlist.insert_before(token, self.nl())
        # Line breaks on group level are done. Now let's add an offset of
        # 5 (=length of "when", "then", "else") and process subgroups.
        self.offset += 5
        self._process_default(tlist)
        self.offset -= 5
        if num_offset is not None:
            self.offset -= num_offset
        end = tlist.token_next_match(0, T.Keyword, 'END')
        tlist.insert_before(end, self.nl())
        self.offset -= outer_offset

    def _process_default(self, tlist, stmts=True, kwds=True):
        if stmts:
            self._split_statements(tlist)
        if kwds:
            self._split_kwds(tlist)
        [self._process(sgroup) for sgroup in tlist.get_sublists()]

    def process(self, stack, stmt):
        if isinstance(stmt, grouping.Statement):
            self._curr_stmt = stmt
        self._process(stmt)
        if isinstance(stmt, grouping.Statement):
            if self._last_stmt is not None:
                if self._last_stmt.to_unicode().endswith('\n'):
                    nl = '\n'
                else:
                    nl = '\n\n'
                stmt.tokens.insert(0,
                    grouping.Token(T.Whitespace, nl))
            if self._last_stmt != stmt:
                self._last_stmt = stmt


# FIXME: Doesn't work ;)
class RightMarginFilter(Filter):

    keep_together = (
#        grouping.TypeCast, grouping.Identifier, grouping.Alias,
    )

    def __init__(self, width=79):
        self.width = width
        self.line = ''

    def _process(self, stack, group, stream):
        for token in stream:
            if token.is_whitespace() and '\n' in token.value:
                if token.value.endswith('\n'):
                    self.line = ''
                else:
                    self.line = token.value.splitlines()[-1]
            elif (token.is_group()
                  and not token.__class__ in self.keep_together):
                token.tokens = self._process(stack, token, token.tokens)
            else:
                val = token.to_unicode()
                if len(self.line) + len(val) > self.width:
                    match = re.search('^ +', self.line)
                    if match is not None:
                        indent = match.group()
                    else:
                        indent = ''
                    yield grouping.Token(T.Whitespace, '\n%s' % indent)
                    self.line = indent
                self.line += val
            yield token

    def process(self, stack, group):
        return
        group.tokens = self._process(stack, group, group.tokens)


# ---------------------------
# postprocess

class SerializerUnicode(Filter):

    def process(self, stack, stmt):
        raw = stmt.to_unicode()
        add_nl = raw.endswith('\n')
        res = '\n'.join(line.rstrip() for line in raw.splitlines())
        if add_nl:
            res += '\n'
        return res


class OutputPythonFilter(Filter):

    def __init__(self, varname='sql'):
        self.varname = varname
        self.cnt = 0

    def _process(self, stream, varname, count, has_nl):
        if count > 1:
            yield grouping.Token(T.Whitespace, '\n')
        yield grouping.Token(T.Name, varname)
        yield grouping.Token(T.Whitespace, ' ')
        yield grouping.Token(T.Operator, '=')
        yield grouping.Token(T.Whitespace, ' ')
        if has_nl:
            yield grouping.Token(T.Operator, '(')
        yield grouping.Token(T.Text, "'")
        cnt = 0
        for token in stream:
            cnt += 1
            if token.is_whitespace() and '\n' in token.value:
                if cnt == 1:
                    continue
                after_lb = token.value.split('\n', 1)[1]
                yield grouping.Token(T.Text, " '")
                yield grouping.Token(T.Whitespace, '\n')
                for i in range(len(varname)+4):
                    yield grouping.Token(T.Whitespace, ' ')
                yield grouping.Token(T.Text, "'")
                if after_lb:  # it's the indendation
                    yield grouping.Token(T.Whitespace, after_lb)
                continue
            elif token.value and "'" in token.value:
                token.value = token.value.replace("'", "\\'")
            yield grouping.Token(T.Text, token.value or '')
        yield grouping.Token(T.Text, "'")
        if has_nl:
            yield grouping.Token(T.Operator, ')')

    def process(self, stack, stmt):
        self.cnt += 1
        if self.cnt > 1:
            varname = '%s%d' % (self.varname, self.cnt)
        else:
            varname = self.varname
        has_nl = len(stmt.to_unicode().strip().splitlines()) > 1
        stmt.tokens = self._process(stmt.tokens, varname, self.cnt, has_nl)
        return stmt


class OutputPHPFilter(Filter):

    def __init__(self, varname='sql'):
        self.varname = '$%s' % varname
        self.count = 0

    def _process(self, stream, varname):
        if self.count > 1:
            yield grouping.Token(T.Whitespace, '\n')
        yield grouping.Token(T.Name, varname)
        yield grouping.Token(T.Whitespace, ' ')
        yield grouping.Token(T.Operator, '=')
        yield grouping.Token(T.Whitespace, ' ')
        yield grouping.Token(T.Text, '"')
        cnt = 0
        for token in stream:
            if token.is_whitespace() and '\n' in token.value:
#                cnt += 1
#                if cnt == 1:
#                    continue
                after_lb = token.value.split('\n', 1)[1]
                yield grouping.Token(T.Text, ' "')
                yield grouping.Token(T.Operator, ';')
                yield grouping.Token(T.Whitespace, '\n')
                yield grouping.Token(T.Name, varname)
                yield grouping.Token(T.Whitespace, ' ')
                yield grouping.Token(T.Punctuation, '.')
                yield grouping.Token(T.Operator, '=')
                yield grouping.Token(T.Whitespace, ' ')
                yield grouping.Token(T.Text, '"')
                if after_lb:
                    yield grouping.Token(T.Text, after_lb)
                continue
            elif '"' in token.value:
                token.value = token.value.replace('"', '\\"')
            yield grouping.Token(T.Text, token.value)
        yield grouping.Token(T.Text, '"')
        yield grouping.Token(T.Punctuation, ';')

    def process(self, stack, stmt):
        self.count += 1
        if self.count > 1:
            varname = '%s%d' % (self.varname, self.count)
        else:
            varname = self.varname
        stmt.tokens = tuple(self._process(stmt.tokens, varname))
        return stmt

########NEW FILE########
__FILENAME__ = formatter
# Copyright (C) 2008 Andi Albrecht, albrecht.andi@gmail.com
#
# This module is part of python-sqlparse and is released under
# the BSD License: http://www.opensource.org/licenses/bsd-license.php.

"""SQL formatter"""

from debug_toolbar.utils.sqlparse import SQLParseError
from debug_toolbar.utils.sqlparse import filters


def validate_options(options):
    """Validates options."""
    kwcase = options.get('keyword_case', None)
    if kwcase not in [None, 'upper', 'lower', 'capitalize']:
        raise SQLParseError('Invalid value for keyword_case: %r' % kwcase)

    idcase = options.get('identifier_case', None)
    if idcase not in [None, 'upper', 'lower', 'capitalize']:
        raise SQLParseError('Invalid value for identifier_case: %r' % idcase)

    ofrmt = options.get('output_format', None)
    if ofrmt not in [None, 'sql', 'python', 'php']:
        raise SQLParseError('Unknown output format: %r' % ofrmt)

    strip_comments = options.get('strip_comments', False)
    if strip_comments not in [True, False]:
        raise SQLParseError('Invalid value for strip_comments: %r'
                            % strip_comments)

    strip_ws = options.get('strip_whitespace', False)
    if strip_ws not in [True, False]:
        raise SQLParseError('Invalid value for strip_whitespace: %r'
                            % strip_ws)

    reindent = options.get('reindent', False)
    if reindent not in [True, False]:
        raise SQLParseError('Invalid value for reindent: %r'
                            % reindent)
    elif reindent:
        options['strip_whitespace'] = True
    indent_tabs = options.get('indent_tabs', False)
    if indent_tabs not in [True, False]:
        raise SQLParseError('Invalid value for indent_tabs: %r' % indent_tabs)
    elif indent_tabs:
        options['indent_char'] = '\t'
    else:
        options['indent_char'] = ' '
    indent_width = options.get('indent_width', 2)
    try:
        indent_width = int(indent_width)
    except (TypeError, ValueError):
        raise SQLParseError('indent_width requires an integer')
    if indent_width < 1:
        raise SQLParseError('indent_width requires an positive integer')
    options['indent_width'] = indent_width

    right_margin = options.get('right_margin', None)
    if right_margin is not None:
        try:
            right_margin = int(right_margin)
        except (TypeError, ValueError):
            raise SQLParseError('right_margin requires an integer')
        if right_margin < 10:
            raise SQLParseError('right_margin requires an integer > 10')
    options['right_margin'] = right_margin

    return options


def build_filter_stack(stack, options):
    """Setup and return a filter stack.

    Args:
      stack: :class:`~sqlparse.filters.FilterStack` instance
      options: Dictionary with options validated by validate_options.
    """
    # Token filter
    if 'keyword_case' in options:
        stack.preprocess.append(
            filters.KeywordCaseFilter(options['keyword_case']))

    if 'identifier_case' in options:
        stack.preprocess.append(
            filters.IdentifierCaseFilter(options['identifier_case']))

    # After grouping
    if options.get('strip_comments', False):
        stack.enable_grouping()
        stack.stmtprocess.append(filters.StripCommentsFilter())

    if (options.get('strip_whitespace', False)
        or options.get('reindent', False)):
        stack.enable_grouping()
        stack.stmtprocess.append(filters.StripWhitespaceFilter())

    if options.get('reindent', False):
        stack.enable_grouping()
        stack.stmtprocess.append(
            filters.ReindentFilter(char=options['indent_char'],
                                   width=options['indent_width']))

    if options.get('right_margin', False):
        stack.enable_grouping()
        stack.stmtprocess.append(
            filters.RightMarginFilter(width=options['right_margin']))

    # Serializer
    if options.get('output_format'):
        frmt = options['output_format']
        if frmt.lower() == 'php':
            fltr = filters.OutputPHPFilter()
        elif frmt.lower() == 'python':
            fltr = filters.OutputPythonFilter()
        else:
            fltr = None
        if fltr is not None:
            stack.postprocess.append(fltr)

    return stack

########NEW FILE########
__FILENAME__ = keywords
from debug_toolbar.utils.sqlparse.tokens import *

KEYWORDS = {
    'ABORT': Keyword,
    'ABS': Keyword,
    'ABSOLUTE': Keyword,
    'ACCESS': Keyword,
    'ADA': Keyword,
    'ADD': Keyword,
    'ADMIN': Keyword,
    'AFTER': Keyword,
    'AGGREGATE': Keyword,
    'ALIAS': Keyword,
    'ALL': Keyword,
    'ALLOCATE': Keyword,
    'ANALYSE': Keyword,
    'ANALYZE': Keyword,
    'AND': Keyword,
    'ANY': Keyword,
    'ARE': Keyword,
    'AS': Keyword,
    'ASC': Keyword,
    'ASENSITIVE': Keyword,
    'ASSERTION': Keyword,
    'ASSIGNMENT': Keyword,
    'ASYMMETRIC': Keyword,
    'AT': Keyword,
    'ATOMIC': Keyword,
    'AUTHORIZATION': Keyword,
    'AVG': Keyword,

    'BACKWARD': Keyword,
    'BEFORE': Keyword,
    'BEGIN': Keyword,
    'BETWEEN': Keyword,
    'BITVAR': Keyword,
    'BIT_LENGTH': Keyword,
    'BOTH': Keyword,
    'BREADTH': Keyword,
    'BY': Keyword,

#    'C': Keyword,  # most likely this is an alias
    'CACHE': Keyword,
    'CALL': Keyword,
    'CALLED': Keyword,
    'CARDINALITY': Keyword,
    'CASCADE': Keyword,
    'CASCADED': Keyword,
    'CASE': Keyword,
    'CAST': Keyword,
    'CATALOG': Keyword,
    'CATALOG_NAME': Keyword,
    'CHAIN': Keyword,
    'CHARACTERISTICS': Keyword,
    'CHARACTER_LENGTH': Keyword,
    'CHARACTER_SET_CATALOG': Keyword,
    'CHARACTER_SET_NAME': Keyword,
    'CHARACTER_SET_SCHEMA': Keyword,
    'CHAR_LENGTH': Keyword,
    'CHECK': Keyword,
    'CHECKED': Keyword,
    'CHECKPOINT': Keyword,
    'CLASS': Keyword,
    'CLASS_ORIGIN': Keyword,
    'CLOB': Keyword,
    'CLOSE': Keyword,
    'CLUSTER': Keyword,
    'COALSECE': Keyword,
    'COBOL': Keyword,
    'COLLATE': Keyword,
    'COLLATION': Keyword,
    'COLLATION_CATALOG': Keyword,
    'COLLATION_NAME': Keyword,
    'COLLATION_SCHEMA': Keyword,
    'COLUMN': Keyword,
    'COLUMN_NAME': Keyword,
    'COMMAND_FUNCTION': Keyword,
    'COMMAND_FUNCTION_CODE': Keyword,
    'COMMENT': Keyword,
    'COMMIT': Keyword,
    'COMMITTED': Keyword,
    'COMPLETION': Keyword,
    'CONDITION_NUMBER': Keyword,
    'CONNECT': Keyword,
    'CONNECTION': Keyword,
    'CONNECTION_NAME': Keyword,
    'CONSTRAINT': Keyword,
    'CONSTRAINTS': Keyword,
    'CONSTRAINT_CATALOG': Keyword,
    'CONSTRAINT_NAME': Keyword,
    'CONSTRAINT_SCHEMA': Keyword,
    'CONSTRUCTOR': Keyword,
    'CONTAINS': Keyword,
    'CONTINUE': Keyword,
    'CONVERSION': Keyword,
    'CONVERT': Keyword,
    'COPY': Keyword,
    'CORRESPONTING': Keyword,
    'COUNT': Keyword,
    'CREATEDB': Keyword,
    'CREATEUSER': Keyword,
    'CROSS': Keyword,
    'CUBE': Keyword,
    'CURRENT': Keyword,
    'CURRENT_DATE': Keyword,
    'CURRENT_PATH': Keyword,
    'CURRENT_ROLE': Keyword,
    'CURRENT_TIME': Keyword,
    'CURRENT_TIMESTAMP': Keyword,
    'CURRENT_USER': Keyword,
    'CURSOR': Keyword,
    'CURSOR_NAME': Keyword,
    'CYCLE': Keyword,

    'DATA': Keyword,
    'DATABASE': Keyword,
    'DATETIME_INTERVAL_CODE': Keyword,
    'DATETIME_INTERVAL_PRECISION': Keyword,
    'DAY': Keyword,
    'DEALLOCATE': Keyword,
    'DECLARE': Keyword,
    'DEFAULT': Keyword,
    'DEFAULTS': Keyword,
    'DEFERRABLE': Keyword,
    'DEFERRED': Keyword,
    'DEFINED': Keyword,
    'DEFINER': Keyword,
    'DELIMITER': Keyword,
    'DELIMITERS': Keyword,
    'DEREF': Keyword,
    'DESC': Keyword,
    'DESCRIBE': Keyword,
    'DESCRIPTOR': Keyword,
    'DESTROY': Keyword,
    'DESTRUCTOR': Keyword,
    'DETERMINISTIC': Keyword,
    'DIAGNOSTICS': Keyword,
    'DICTIONARY': Keyword,
    'DISCONNECT': Keyword,
    'DISPATCH': Keyword,
    'DISTINCT': Keyword,
    'DO': Keyword,
    'DOMAIN': Keyword,
    'DYNAMIC': Keyword,
    'DYNAMIC_FUNCTION': Keyword,
    'DYNAMIC_FUNCTION_CODE': Keyword,

    'EACH': Keyword,
    'ELSE': Keyword,
    'ENCODING': Keyword,
    'ENCRYPTED': Keyword,
    'END': Keyword,
    'END-EXEC': Keyword,
    'EQUALS': Keyword,
    'ESCAPE': Keyword,
    'EVERY': Keyword,
    'EXCEPT': Keyword,
    'ESCEPTION': Keyword,
    'EXCLUDING': Keyword,
    'EXCLUSIVE': Keyword,
    'EXEC': Keyword,
    'EXECUTE': Keyword,
    'EXISTING': Keyword,
    'EXISTS': Keyword,
    'EXTERNAL': Keyword,
    'EXTRACT': Keyword,

    'FALSE': Keyword,
    'FETCH': Keyword,
    'FINAL': Keyword,
    'FIRST': Keyword,
    'FOR': Keyword,
    'FORCE': Keyword,
    'FOREIGN': Keyword,
    'FORTRAN': Keyword,
    'FORWARD': Keyword,
    'FOUND': Keyword,
    'FREE': Keyword,
    'FREEZE': Keyword,
    'FROM': Keyword,
    'FULL': Keyword,
    'FUNCTION': Keyword,

    'G': Keyword,
    'GENERAL': Keyword,
    'GENERATED': Keyword,
    'GET': Keyword,
    'GLOBAL': Keyword,
    'GO': Keyword,
    'GOTO': Keyword,
    'GRANT': Keyword,
    'GRANTED': Keyword,
    'GROUP': Keyword,
    'GROUPING': Keyword,

    'HANDLER': Keyword,
    'HAVING': Keyword,
    'HIERARCHY': Keyword,
    'HOLD': Keyword,
    'HOST': Keyword,

    'IDENTITY': Keyword,
    'IF': Keyword,
    'IGNORE': Keyword,
    'ILIKE': Keyword,
    'IMMEDIATE': Keyword,
    'IMMUTABLE': Keyword,

    'IMPLEMENTATION': Keyword,
    'IMPLICIT': Keyword,
    'IN': Keyword,
    'INCLUDING': Keyword,
    'INCREMENT': Keyword,
    'INDEX': Keyword,

    'INDITCATOR': Keyword,
    'INFIX': Keyword,
    'INHERITS': Keyword,
    'INITIALIZE': Keyword,
    'INITIALLY': Keyword,
    'INNER': Keyword,
    'INOUT': Keyword,
    'INPUT': Keyword,
    'INSENSITIVE': Keyword,
    'INSTANTIABLE': Keyword,
    'INSTEAD': Keyword,
    'INTERSECT': Keyword,
    'INTO': Keyword,
    'INVOKER': Keyword,
    'IS': Keyword,
    'ISNULL': Keyword,
    'ISOLATION': Keyword,
    'ITERATE': Keyword,

    'JOIN': Keyword,

    'K': Keyword,
    'KEY': Keyword,
    'KEY_MEMBER': Keyword,
    'KEY_TYPE': Keyword,

    'LANCOMPILER': Keyword,
    'LANGUAGE': Keyword,
    'LARGE': Keyword,
    'LAST': Keyword,
    'LATERAL': Keyword,
    'LEADING': Keyword,
    'LEFT': Keyword,
    'LENGTH': Keyword,
    'LESS': Keyword,
    'LEVEL': Keyword,
    'LIKE': Keyword,
    'LIMIT': Keyword,
    'LISTEN': Keyword,
    'LOAD': Keyword,
    'LOCAL': Keyword,
    'LOCALTIME': Keyword,
    'LOCALTIMESTAMP': Keyword,
    'LOCATION': Keyword,
    'LOCATOR': Keyword,
    'LOCK': Keyword,
    'LOWER': Keyword,

    'M': Keyword,
    'MAP': Keyword,
    'MATCH': Keyword,
    'MAX': Keyword,
    'MAXVALUE': Keyword,
    'MESSAGE_LENGTH': Keyword,
    'MESSAGE_OCTET_LENGTH': Keyword,
    'MESSAGE_TEXT': Keyword,
    'METHOD': Keyword,
    'MIN': Keyword,
    'MINUTE': Keyword,
    'MINVALUE': Keyword,
    'MOD': Keyword,
    'MODE': Keyword,
    'MODIFIES': Keyword,
    'MODIFY': Keyword,
    'MONTH': Keyword,
    'MORE': Keyword,
    'MOVE': Keyword,
    'MUMPS': Keyword,

    'NAMES': Keyword,
    'NATIONAL': Keyword,
    'NATURAL': Keyword,
    'NCHAR': Keyword,
    'NCLOB': Keyword,
    'NEW': Keyword,
    'NEXT': Keyword,
    'NO': Keyword,
    'NOCREATEDB': Keyword,
    'NOCREATEUSER': Keyword,
    'NONE': Keyword,
    'NOT': Keyword,
    'NOTHING': Keyword,
    'NOTIFY': Keyword,
    'NOTNULL': Keyword,
    'NULL': Keyword,
    'NULLABLE': Keyword,
    'NULLIF': Keyword,

    'OBJECT': Keyword,
    'OCTET_LENGTH': Keyword,
    'OF': Keyword,
    'OFF': Keyword,
    'OFFSET': Keyword,
    'OIDS': Keyword,
    'OLD': Keyword,
    'ON': Keyword,
    'ONLY': Keyword,
    'OPEN': Keyword,
    'OPERATION': Keyword,
    'OPERATOR': Keyword,
    'OPTION': Keyword,
    'OPTIONS': Keyword,
    'OR': Keyword,
    'ORDER': Keyword,
    'ORDINALITY': Keyword,
    'OUT': Keyword,
    'OUTER': Keyword,
    'OUTPUT': Keyword,
    'OVERLAPS': Keyword,
    'OVERLAY': Keyword,
    'OVERRIDING': Keyword,
    'OWNER': Keyword,

    'PAD': Keyword,
    'PARAMETER': Keyword,
    'PARAMETERS': Keyword,
    'PARAMETER_MODE': Keyword,
    'PARAMATER_NAME': Keyword,
    'PARAMATER_ORDINAL_POSITION': Keyword,
    'PARAMETER_SPECIFIC_CATALOG': Keyword,
    'PARAMETER_SPECIFIC_NAME': Keyword,
    'PARAMATER_SPECIFIC_SCHEMA': Keyword,
    'PARTIAL': Keyword,
    'PASCAL': Keyword,
    'PENDANT': Keyword,
    'PLACING': Keyword,
    'PLI': Keyword,
    'POSITION': Keyword,
    'POSTFIX': Keyword,
    'PRECISION': Keyword,
    'PREFIX': Keyword,
    'PREORDER': Keyword,
    'PREPARE': Keyword,
    'PRESERVE': Keyword,
    'PRIMARY': Keyword,
    'PRIOR': Keyword,
    'PRIVILEGES': Keyword,
    'PROCEDURAL': Keyword,
    'PROCEDURE': Keyword,
    'PUBLIC': Keyword,

    'RAISE': Keyword,
    'READ': Keyword,
    'READS': Keyword,
    'RECHECK': Keyword,
    'RECURSIVE': Keyword,
    'REF': Keyword,
    'REFERENCES': Keyword,
    'REFERENCING': Keyword,
    'REINDEX': Keyword,
    'RELATIVE': Keyword,
    'RENAME': Keyword,
    'REPEATABLE': Keyword,
    'REPLACE': Keyword,
    'RESET': Keyword,
    'RESTART': Keyword,
    'RESTRICT': Keyword,
    'RESULT': Keyword,
    'RETURN': Keyword,
    'RETURNED_LENGTH': Keyword,
    'RETURNED_OCTET_LENGTH': Keyword,
    'RETURNED_SQLSTATE': Keyword,
    'RETURNS': Keyword,
    'REVOKE': Keyword,
    'RIGHT': Keyword,
    'ROLE': Keyword,
    'ROLLBACK': Keyword,
    'ROLLUP': Keyword,
    'ROUTINE': Keyword,
    'ROUTINE_CATALOG': Keyword,
    'ROUTINE_NAME': Keyword,
    'ROUTINE_SCHEMA': Keyword,
    'ROW': Keyword,
    'ROWS': Keyword,
    'ROW_COUNT': Keyword,
    'RULE': Keyword,

    'SAVE_POINT': Keyword,
    'SCALE': Keyword,
    'SCHEMA': Keyword,
    'SCHEMA_NAME': Keyword,
    'SCOPE': Keyword,
    'SCROLL': Keyword,
    'SEARCH': Keyword,
    'SECOND': Keyword,
    'SECURITY': Keyword,
    'SELF': Keyword,
    'SENSITIVE': Keyword,
    'SERIALIZABLE': Keyword,
    'SERVER_NAME': Keyword,
    'SESSION': Keyword,
    'SESSION_USER': Keyword,
    'SETOF': Keyword,
    'SETS': Keyword,
    'SHARE': Keyword,
    'SHOW': Keyword,
    'SIMILAR': Keyword,
    'SIMPLE': Keyword,
    'SIZE': Keyword,
    'SOME': Keyword,
    'SOURCE': Keyword,
    'SPACE': Keyword,
    'SPECIFIC': Keyword,
    'SPECIFICTYPE': Keyword,
    'SPECIFIC_NAME': Keyword,
    'SQL': Keyword,
    'SQLCODE': Keyword,
    'SQLERROR': Keyword,
    'SQLEXCEPTION': Keyword,
    'SQLSTATE': Keyword,
    'SQLWARNINIG': Keyword,
    'STABLE': Keyword,
    'START': Keyword,
    'STATE': Keyword,
    'STATEMENT': Keyword,
    'STATIC': Keyword,
    'STATISTICS': Keyword,
    'STDIN': Keyword,
    'STDOUT': Keyword,
    'STORAGE': Keyword,
    'STRICT': Keyword,
    'STRUCTURE': Keyword,
    'STYPE': Keyword,
    'SUBCLASS_ORIGIN': Keyword,
    'SUBLIST': Keyword,
    'SUBSTRING': Keyword,
    'SUM': Keyword,
    'SYMMETRIC': Keyword,
    'SYSID': Keyword,
    'SYSTEM': Keyword,
    'SYSTEM_USER': Keyword,

    'TABLE': Keyword,
    'TABLE_NAME': Keyword,
    ' TEMP': Keyword,
    'TEMPLATE': Keyword,
    'TEMPORARY': Keyword,
    'TERMINATE': Keyword,
    'THAN': Keyword,
    'THEN': Keyword,
    'TIMESTAMP': Keyword,
    'TIMEZONE_HOUR': Keyword,
    'TIMEZONE_MINUTE': Keyword,
    'TO': Keyword,
    'TOAST': Keyword,
    'TRAILING': Keyword,
    'TRANSATION': Keyword,
    'TRANSACTIONS_COMMITTED': Keyword,
    'TRANSACTIONS_ROLLED_BACK': Keyword,
    'TRANSATION_ACTIVE': Keyword,
    'TRANSFORM': Keyword,
    'TRANSFORMS': Keyword,
    'TRANSLATE': Keyword,
    'TRANSLATION': Keyword,
    'TREAT': Keyword,
    'TRIGGER': Keyword,
    'TRIGGER_CATALOG': Keyword,
    'TRIGGER_NAME': Keyword,
    'TRIGGER_SCHEMA': Keyword,
    'TRIM': Keyword,
    'TRUE': Keyword,
    'TRUNCATE': Keyword,
    'TRUSTED': Keyword,
    'TYPE': Keyword,

    'UNCOMMITTED': Keyword,
    'UNDER': Keyword,
    'UNENCRYPTED': Keyword,
    'UNION': Keyword,
    'UNIQUE': Keyword,
    'UNKNOWN': Keyword,
    'UNLISTEN': Keyword,
    'UNNAMED': Keyword,
    'UNNEST': Keyword,
    'UNTIL': Keyword,
    'UPPER': Keyword,
    'USAGE': Keyword,
    'USER': Keyword,
    'USER_DEFINED_TYPE_CATALOG': Keyword,
    'USER_DEFINED_TYPE_NAME': Keyword,
    'USER_DEFINED_TYPE_SCHEMA': Keyword,
    'USING': Keyword,

    'VACUUM': Keyword,
    'VALID': Keyword,
    'VALIDATOR': Keyword,
    'VALUES': Keyword,
    'VARIABLE': Keyword,
    'VERBOSE': Keyword,
    'VERSION': Keyword,
    'VIEW': Keyword,
    'VOLATILE': Keyword,

    'WHEN': Keyword,
    'WHENEVER': Keyword,
    'WHERE': Keyword,
    'WITH': Keyword,
    'WITHOUT': Keyword,
    'WORK': Keyword,
    'WRITE': Keyword,

    'YEAR': Keyword,

    'ZONE': Keyword,


    'ARRAY': Name.Builtin,
    'BIGINT': Name.Builtin,
    'BINARY': Name.Builtin,
    'BIT': Name.Builtin,
    'BLOB': Name.Builtin,
    'BOOLEAN': Name.Builtin,
    'CHAR': Name.Builtin,
    'CHARACTER': Name.Builtin,
    'DATE': Name.Builtin,
    'DEC': Name.Builtin,
    'DECIMAL': Name.Builtin,
    'FLOAT': Name.Builtin,
    'INT': Name.Builtin,
    'INTEGER': Name.Builtin,
    'INTERVAL': Name.Builtin,
    'NUMBER': Name.Builtin,
    'NUMERIC': Name.Builtin,
    'REAL': Name.Builtin,
    'SERIAL': Name.Builtin,
    'SMALLINT': Name.Builtin,
    'VARCHAR': Name.Builtin,
    'VARYING': Name.Builtin,
    'INT8': Name.Builtin,
    'SERIAL8': Name.Builtin,
    'TEXT': Name.Builtin,
    }


KEYWORDS_COMMON = {
    'SELECT': Keyword.DML,
    'INSERT': Keyword.DML,
    'DELETE': Keyword.DML,
    'UPDATE': Keyword.DML,
    'DROP': Keyword.DDL,
    'CREATE': Keyword.DDL,
    'ALTER': Keyword.DDL,

    'WHERE': Keyword,
    'FROM': Keyword,
    'INNER': Keyword,
    'JOIN': Keyword,
    'AND': Keyword,
    'OR': Keyword,
    'LIKE': Keyword,
    'ON': Keyword,
    'IN': Keyword,
    'SET': Keyword,

    'BY': Keyword,
    'GROUP': Keyword,
    'ORDER': Keyword,
    'LEFT': Keyword,
    'OUTER': Keyword,

    'IF': Keyword,
    'END': Keyword,
    'THEN': Keyword,
    'LOOP': Keyword,
    'AS': Keyword,
    'ELSE': Keyword,
    'FOR': Keyword,

    'CASE': Keyword,
    'WHEN': Keyword,
    'MIN': Keyword,
    'MAX': Keyword,
    'DISTINCT': Keyword,

    }

########NEW FILE########
__FILENAME__ = lexer
# -*- coding: utf-8 -*-

# Copyright (C) 2008 Andi Albrecht, albrecht.andi@gmail.com
#
# This module is part of python-sqlparse and is released under
# the BSD License: http://www.opensource.org/licenses/bsd-license.php.

"""SQL Lexer"""

# This code is based on the SqlLexer in pygments.
# http://pygments.org/
# It's separated from the rest of pygments to increase performance
# and to allow some customizations.

import re

from debug_toolbar.utils.sqlparse.keywords import KEYWORDS, KEYWORDS_COMMON
from debug_toolbar.utils.sqlparse.tokens import *
from debug_toolbar.utils.sqlparse.tokens import _TokenType


class include(str):
    pass

class combined(tuple):
    """Indicates a state combined from multiple states."""

    def __new__(cls, *args):
        return tuple.__new__(cls, args)

    def __init__(self, *args):
        # tuple.__init__ doesn't do anything
        pass

def is_keyword(value):
    test = value.upper()
    return KEYWORDS_COMMON.get(test, KEYWORDS.get(test, Name)), value


def apply_filters(stream, filters, lexer=None):
    """
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """
    def _apply(filter_, stream):
        for token in filter_.filter(lexer, stream):
            yield token
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


class LexerMeta(type):
    """
    Metaclass for Lexer, creates the self._tokens attribute from
    self.tokens on the first instantiation.
    """

    def _process_state(cls, unprocessed, processed, state):
        assert type(state) is str, "wrong state name %r" % state
        assert state[0] != '#', "invalid state name %r" % state
        if state in processed:
            return processed[state]
        tokens = processed[state] = []
        rflags = cls.flags
        for tdef in unprocessed[state]:
            if isinstance(tdef, include):
                # it's a state reference
                assert tdef != state, "circular state reference %r" % state
                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
                continue

            assert type(tdef) is tuple, "wrong rule def %r" % tdef

            try:
                rex = re.compile(tdef[0], rflags).match
            except Exception, err:
                raise ValueError("uncompilable regex %r in state %r of %r: %s" %
                                 (tdef[0], state, cls, err))

            assert type(tdef[1]) is _TokenType or callable(tdef[1]), \
                   'token type must be simple type or callable, not %r' % (tdef[1],)

            if len(tdef) == 2:
                new_state = None
            else:
                tdef2 = tdef[2]
                if isinstance(tdef2, str):
                    # an existing state
                    if tdef2 == '#pop':
                        new_state = -1
                    elif tdef2 in unprocessed:
                        new_state = (tdef2,)
                    elif tdef2 == '#push':
                        new_state = tdef2
                    elif tdef2[:5] == '#pop:':
                        new_state = -int(tdef2[5:])
                    else:
                        assert False, 'unknown new state %r' % tdef2
                elif isinstance(tdef2, combined):
                    # combine a new state from existing ones
                    new_state = '_tmp_%d' % cls._tmpname
                    cls._tmpname += 1
                    itokens = []
                    for istate in tdef2:
                        assert istate != state, 'circular state ref %r' % istate
                        itokens.extend(cls._process_state(unprocessed,
                                                          processed, istate))
                    processed[new_state] = itokens
                    new_state = (new_state,)
                elif isinstance(tdef2, tuple):
                    # push more than one state
                    for state in tdef2:
                        assert (state in unprocessed or
                                state in ('#pop', '#push')), \
                               'unknown new state ' + state
                    new_state = tdef2
                else:
                    assert False, 'unknown new state def %r' % tdef2
            tokens.append((rex, tdef[1], new_state))
        return tokens

    def process_tokendef(cls):
        cls._all_tokens = {}
        cls._tmpname = 0
        processed = cls._all_tokens[cls.__name__] = {}
        #tokendefs = tokendefs or cls.tokens[name]
        for state in cls.tokens.keys():
            cls._process_state(cls.tokens, processed, state)
        return processed

    def __call__(cls, *args, **kwds):
        if not hasattr(cls, '_tokens'):
            cls._all_tokens = {}
            cls._tmpname = 0
            if hasattr(cls, 'token_variants') and cls.token_variants:
                # don't process yet
                pass
            else:
                cls._tokens = cls.process_tokendef()

        return type.__call__(cls, *args, **kwds)




class Lexer:

    __metaclass__ = LexerMeta

    encoding = 'utf-8'
    stripall = False
    stripnl = False
    tabsize = 0
    flags = re.IGNORECASE

    tokens = {
        'root': [
            (r'--.*?(\r|\n|\r\n)', Comment.Single),
            (r'(\r|\n|\r\n)', Newline),
            (r'\s+', Whitespace),
            (r'/\*', Comment.Multiline, 'multiline-comments'),
            (r':=', Assignment),
            (r'::', Punctuation),
            (r'[*]', Wildcard),
            (r"`(``|[^`])*`", Name),
            (r"(|[^])*", Name),
            (r'@[a-zA-Z_][a-zA-Z0-9_]+', Name),
            (r'[+/<>=~!@#%^&|`?^-]', Operator),
            (r'[0-9]+', Number.Integer),
            # TODO: Backslash escapes?
            (r"'(''|[^'])*'", String.Single),
            (r'"(""|[^"])*"', String.Symbol), # not a real string literal in ANSI SQL
            (r'(LEFT |RIGHT )?(INNER |OUTER )?JOIN', Keyword),
            (r'END( IF| LOOP)?', Keyword),
            (r'CREATE( OR REPLACE)?', Keyword.DDL),
            (r'[a-zA-Z_][a-zA-Z0-9_]*', is_keyword),
            (r'\$([a-zA-Z_][a-zA-Z0-9_]*)?\$', Name.Builtin),
            (r'[;:()\[\],\.]', Punctuation),
        ],
        'multiline-comments': [
            (r'/\*', Comment.Multiline, 'multiline-comments'),
            (r'\*/', Comment.Multiline, '#pop'),
            (r'[^/\*]+', Comment.Multiline),
            (r'[/*]', Comment.Multiline)
        ]
    }

    def __init__(self):
        self.filters = []

    def add_filter(self, filter_, **options):
        from sqlparse.filters import Filter
        if not isinstance(filter_, Filter):
            filter_ = filter_(**options)
        self.filters.append(filter_)

    def get_tokens(self, text, unfiltered=False):
        """
        Return an iterable of (tokentype, value) pairs generated from
        `text`. If `unfiltered` is set to `True`, the filtering mechanism
        is bypassed even if filters are defined.

        Also preprocess the text, i.e. expand tabs and strip it if
        wanted and applies registered filters.
        """
        if not isinstance(text, unicode):
            if self.encoding == 'guess':
                try:
                    text = text.decode('utf-8')
                    if text.startswith(u'\ufeff'):
                        text = text[len(u'\ufeff'):]
                except UnicodeDecodeError:
                    text = text.decode('latin1')
            elif self.encoding == 'chardet':
                try:
                    import chardet
                except ImportError:
                    raise ImportError('To enable chardet encoding guessing, '
                                      'please install the chardet library '
                                      'from http://chardet.feedparser.org/')
                enc = chardet.detect(text)
                text = text.decode(enc['encoding'])
            else:
                text = text.decode(self.encoding)
        if self.stripall:
            text = text.strip()
        elif self.stripnl:
            text = text.strip('\n')
        if self.tabsize > 0:
            text = text.expandtabs(self.tabsize)
#        if not text.endswith('\n'):
#            text += '\n'

        def streamer():
            for i, t, v in self.get_tokens_unprocessed(text):
                yield t, v
        stream = streamer()
        if not unfiltered:
            stream = apply_filters(stream, self.filters, self)
        return stream


    def get_tokens_unprocessed(self, text, stack=('root',)):
        """
        Split ``text`` into (tokentype, text) pairs.

        ``stack`` is the inital stack (default: ``['root']``)
        """
        pos = 0
        tokendefs = self._tokens
        statestack = list(stack)
        statetokens = tokendefs[statestack[-1]]
        known_names = {}
        while 1:
            for rexmatch, action, new_state in statetokens:
                m = rexmatch(text, pos)
                if m:
                    # print rex.pattern
                    value = m.group()
                    if value in known_names:
                        yield pos, known_names[value], value
                    elif type(action) is _TokenType:
                        yield pos, action, value
                    elif hasattr(action, '__call__'):
                        ttype, value = action(value)
                        known_names[value] = ttype
                        yield pos, ttype, value
                    else:
                        for item in action(self, m):
                            yield item
                    pos = m.end()
                    if new_state is not None:
                        # state transition
                        if isinstance(new_state, tuple):
                            for state in new_state:
                                if state == '#pop':
                                    statestack.pop()
                                elif state == '#push':
                                    statestack.append(statestack[-1])
                                else:
                                    statestack.append(state)
                        elif isinstance(new_state, int):
                            # pop
                            del statestack[new_state:]
                        elif new_state == '#push':
                            statestack.append(statestack[-1])
                        else:
                            assert False, "wrong state def: %r" % new_state
                        statetokens = tokendefs[statestack[-1]]
                    break
            else:
                try:
                    if text[pos] == '\n':
                        # at EOL, reset state to "root"
                        pos += 1
                        statestack = ['root']
                        statetokens = tokendefs['root']
                        yield pos, Text, u'\n'
                        continue
                    yield pos, Error, text[pos]
                    pos += 1
                except IndexError:
                    break


def tokenize(sql):
    """Tokenize sql.

    Tokenize *sql* using the :class:`Lexer` and return a 2-tuple stream
    of ``(token type, value)`` items.
    """
    lexer = Lexer()
    return lexer.get_tokens(sql)

########NEW FILE########
__FILENAME__ = sql
# -*- coding: utf-8 -*-

"""This module contains classes representing syntactical elements of SQL."""

import re
import types

from debug_toolbar.utils.sqlparse import tokens as T


class Token(object):
    """Base class for all other classes in this module.

    It represents a single token and has two instance attributes:
    ``value`` is the unchange value of the token and ``ttype`` is
    the type of the token.
    """

    __slots__ = ('value', 'ttype',)

    def __init__(self, ttype, value):
        self.value = value
        self.ttype = ttype

    def __str__(self):
        return unicode(self).encode('latin-1')

    def __repr__(self):
        short = self._get_repr_value()
        return '<%s \'%s\' at 0x%07x>' % (self._get_repr_name(),
                                          short, id(self))

    def __unicode__(self):
        return self.value or ''

    def to_unicode(self):
        """Returns a unicode representation of this object."""
        return unicode(self)

    def _get_repr_name(self):
        return str(self.ttype).split('.')[-1]

    def _get_repr_value(self):
        raw = unicode(self)
        if len(raw) > 7:
            short = raw[:6]+u'...'
        else:
            short = raw
        return re.sub('\s+', ' ', short)

    def flatten(self):
        """Resolve subgroups."""
        yield self

    def match(self, ttype, values, regex=False):
        """Checks whether the token matches the given arguments.

        *ttype* is a token type. If this token doesn't match the given token
        type.
        *values* is a list of possible values for this token. The values
        are OR'ed together so if only one of the values matches ``True``
        is returned. Except for keyword tokens the comparsion is
        case-sensitive. For convenience it's ok to pass in a single string.
        If *regex* is ``True`` (default is ``False``) the given values are
        treated as regular expressions.
        """
        type_matched = self.ttype in ttype
        if not type_matched or values is None:
            return type_matched
        if isinstance(values, basestring):
            values = set([values])
        if regex:
            if self.ttype is T.Keyword:
                values = set([re.compile(v, re.IGNORECASE) for v in values])
            else:
                values = set([re.compile(v) for v in values])
            for pattern in values:
                if pattern.search(self.value):
                    return True
            return False
        else:
            if self.ttype is T.Keyword:
                values = set([v.upper() for v in values])
                return self.value.upper() in values
            else:
                return self.value in values

    def is_group(self):
        """Returns ``True`` if this object has children."""
        return False

    def is_whitespace(self):
        """Return ``True`` if this token is a whitespace token."""
        return self.ttype and self.ttype in T.Whitespace


class TokenList(Token):
    """A group of tokens.

    It has an additional instance attribute ``tokens`` which holds a
    list of child-tokens.
    """

    __slots__ = ('value', 'ttype', 'tokens')

    def __init__(self, tokens=None):
        if tokens is None:
            tokens = []
        self.tokens = tokens
        Token.__init__(self, None, None)

    def __unicode__(self):
        return ''.join(unicode(x) for x in self.flatten())

    def __str__(self):
        return unicode(self).encode('latin-1')

    def _get_repr_name(self):
        return self.__class__.__name__

    ## def _pprint_tree(self, max_depth=None, depth=0):
    ##     """Pretty-print the object tree."""
    ##     indent = ' '*(depth*2)
    ##     for token in self.tokens:
    ##         if token.is_group():
    ##             pre = ' | '
    ##         else:
    ##             pre = ' | '
    ##         print '%s%s%s \'%s\'' % (indent, pre, token._get_repr_name(),
    ##                                  token._get_repr_value())
    ##         if (token.is_group() and max_depth is not None
    ##             and depth < max_depth):
    ##             token._pprint_tree(max_depth, depth+1)

    def flatten(self):
        """Generator yielding ungrouped tokens.

        This method is recursively called for all child tokens.
        """
        for token in self.tokens:
            if isinstance(token, TokenList):
                for item in token.flatten():
                    yield item
            else:
                yield token

    def is_group(self):
        return True

    def get_sublists(self):
        return [x for x in self.tokens if isinstance(x, TokenList)]

    def token_first(self, ignore_whitespace=True):
        """Returns the first child token.

        If *ignore_whitespace* is ``True`` (the default), whitespace
        tokens are ignored.
        """
        for token in self.tokens:
            if ignore_whitespace and token.is_whitespace():
                continue
            return token
        return None

    def token_next_by_instance(self, idx, clss):
        """Returns the next token matching a class.

        *idx* is where to start searching in the list of child tokens.
        *clss* is a list of classes the token should be an instance of.

        If no matching token can be found ``None`` is returned.
        """
        if isinstance(clss, (list, tuple)):
            clss = (clss,)
        if isinstance(clss, tuple):
            clss = tuple(clss)
        for token in self.tokens[idx:]:
            if isinstance(token, clss):
                return token
        return None

    def token_next_by_type(self, idx, ttypes):
        """Returns next matching token by it's token type."""
        if not isinstance(ttypes, (list, tuple)):
            ttypes = [ttypes]
        for token in self.tokens[idx:]:
            if token.ttype in ttypes:
                return token
        return None

    def token_next_match(self, idx, ttype, value, regex=False):
        """Returns next token where it's ``match`` method returns ``True``."""
        if type(idx) != types.IntType:
            idx = self.token_index(idx)
        for token in self.tokens[idx:]:
            if token.match(ttype, value, regex):
                return token
        return None

    def token_not_matching(self, idx, funcs):
        for token in self.tokens[idx:]:
            passed = False
            for func in funcs:
                if func(token):
                    passed = True
                    break
            if not passed:
                return token
        return None

    def token_matching(self, idx, funcs):
        for token in self.tokens[idx:]:
            for i, func in enumerate(funcs):
                if func(token):
                    return token
        return None

    def token_prev(self, idx, skip_ws=True):
        """Returns the previous token relative to *idx*.

        If *skip_ws* is ``True`` (the default) whitespace tokens are ignored.
        ``None`` is returned if there's no previous token.
        """
        if idx is None:
            return None
        if not isinstance(idx, int):
            idx = self.token_index(idx)
        while idx != 0:
            idx -= 1
            if self.tokens[idx].is_whitespace() and skip_ws:
                continue
            return self.tokens[idx]

    def token_next(self, idx, skip_ws=True):
        """Returns the next token relative to *idx*.

        If *skip_ws* is ``True`` (the default) whitespace tokens are ignored.
        ``None`` is returned if there's no next token.
        """
        if idx is None:
            return None
        if not isinstance(idx, int):
            idx = self.token_index(idx)
        while idx < len(self.tokens)-1:
            idx += 1
            if self.tokens[idx].is_whitespace() and skip_ws:
                continue
            return self.tokens[idx]

    def token_index(self, token):
        """Return list index of token."""
        return self.tokens.index(token)

    def tokens_between(self, start, end, exclude_end=False):
        """Return all tokens between (and including) start and end.

        If *exclude_end* is ``True`` (default is ``False``) the end token
        is included too.
        """
        if exclude_end:
            offset = 0
        else:
            offset = 1
        return self.tokens[self.token_index(start):self.token_index(end)+offset]

    def group_tokens(self, grp_cls, tokens):
        """Replace tokens by an instance of *grp_cls*."""
        idx = self.token_index(tokens[0])
        for t in tokens:
            self.tokens.remove(t)
        grp = grp_cls(tokens)
        self.tokens.insert(idx, grp)
        return grp

    def insert_before(self, where, token):
        """Inserts *token* before *where*."""
        self.tokens.insert(self.token_index(where), token)


class Statement(TokenList):
    """Represents a SQL statement."""

    __slots__ = ('value', 'ttype', 'tokens')

    def get_type(self):
        """Returns the type of a statement.

        The returned value is a string holding an upper-cased reprint of
        the first DML or DDL keyword. If the first token in this group
        isn't a DML or DDL keyword "UNKNOWN" is returned.
        """
        first_token = self.token_first()
        if first_token.ttype in (T.Keyword.DML, T.Keyword.DDL):
            return first_token.value.upper()
        else:
            return 'UNKNOWN'


class Identifier(TokenList):
    """Represents an identifier.

    Identifiers may have aliases or typecasts.
    """

    __slots__ = ('value', 'ttype', 'tokens')

    def has_alias(self):
        """Returns ``True`` if an alias is present."""
        return self.get_alias() is not None

    def get_alias(self):
        """Returns the alias for this identifier or ``None``."""
        kw = self.token_next_match(0, T.Keyword, 'AS')
        if kw is not None:
            alias = self.token_next(self.token_index(kw))
            if alias is None:
                return None
        else:
            next_ = self.token_next(0)
            if next_ is None or not isinstance(next_, Identifier):
                return None
            alias = next_
        if isinstance(alias, Identifier):
            return alias.get_name()
        else:
            return alias.to_unicode()

    def get_name(self):
        """Returns the name of this identifier.

        This is either it's alias or it's real name. The returned valued can
        be considered as the name under which the object corresponding to
        this identifier is known within the current statement.
        """
        alias = self.get_alias()
        if alias is not None:
            return alias
        return self.get_real_name()

    def get_real_name(self):
        """Returns the real name (object name) of this identifier."""
        # a.b
        dot = self.token_next_match(0, T.Punctuation, '.')
        if dot is None:
            return self.token_next_by_type(0, T.Name).value
        else:
            next_ = self.token_next_by_type(self.token_index(dot),
                                            (T.Name, T.Wildcard))
            if next_ is None:  # invalid identifier, e.g. "a."
                return None
            return next_.value

    def get_parent_name(self):
        """Return name of the parent object if any.

        A parent object is identified by the first occuring dot.
        """
        dot = self.token_next_match(0, T.Punctuation, '.')
        if dot is None:
            return None
        prev_ = self.token_prev(self.token_index(dot))
        if prev_ is None:  # something must be verry wrong here..
            return None
        return prev_.value

    def is_wildcard(self):
        """Return ``True`` if this identifier contains a wildcard."""
        token = self.token_next_by_type(0, T.Wildcard)
        return token is not None

    def get_typecast(self):
        """Returns the typecast or ``None`` of this object as a string."""
        marker = self.token_next_match(0, T.Punctuation, '::')
        if marker is None:
            return None
        next_ = self.token_next(self.token_index(marker), False)
        if next_ is None:
            return None
        return next_.to_unicode()


class IdentifierList(TokenList):
    """A list of :class:`~sqlparse.sql.Identifier`\'s."""

    __slots__ = ('value', 'ttype', 'tokens')

    def get_identifiers(self):
        """Returns the identifiers.

        Whitespaces and punctuations are not included in this list.
        """
        return [x for x in self.tokens
                if not x.is_whitespace() and not x.match(T.Punctuation, ',')]


class Parenthesis(TokenList):
    """Tokens between parenthesis."""
    __slots__ = ('value', 'ttype', 'tokens')


class Assignment(TokenList):
    """An assignment like 'var := val;'"""
    __slots__ = ('value', 'ttype', 'tokens')

class If(TokenList):
    """An 'if' clause with possible 'else if' or 'else' parts."""
    __slots__ = ('value', 'ttype', 'tokens')

class For(TokenList):
    """A 'FOR' loop."""
    __slots__ = ('value', 'ttype', 'tokens')

class Comparsion(TokenList):
    """A comparsion used for example in WHERE clauses."""
    __slots__ = ('value', 'ttype', 'tokens')

class Comment(TokenList):
    """A comment."""
    __slots__ = ('value', 'ttype', 'tokens')

class Where(TokenList):
    """A WHERE clause."""
    __slots__ = ('value', 'ttype', 'tokens')


class Case(TokenList):
    """A CASE statement with one or more WHEN and possibly an ELSE part."""

    __slots__ = ('value', 'ttype', 'tokens')

    def get_cases(self):
        """Returns a list of 2-tuples (condition, value).

        If an ELSE exists condition is None.
        """
        ret = []
        in_condition = in_value = False
        for token in self.tokens:
            if token.match(T.Keyword, 'WHEN'):
                ret.append(([], []))
                in_condition = True
                in_value = False
            elif token.match(T.Keyword, 'ELSE'):
                ret.append((None, []))
                in_condition = False
                in_value = True
            elif token.match(T.Keyword, 'THEN'):
                in_condition = False
                in_value = True
            elif token.match(T.Keyword, 'END'):
                in_condition = False
                in_value = False
            if in_condition:
                ret[-1][0].append(token)
            elif in_value:
                ret[-1][1].append(token)
        return ret

########NEW FILE########
__FILENAME__ = tokens
# Copyright (C) 2008 Andi Albrecht, albrecht.andi@gmail.com
#
# This module is part of python-sqlparse and is released under
# the BSD License: http://www.opensource.org/licenses/bsd-license.php.

# The Token implementation is based on pygment's token system written
# by Georg Brandl.
# http://pygments.org/

"""Tokens"""

try:
    set
except NameError:
    from sets import Set as set


class _TokenType(tuple):
    parent = None

    def split(self):
        buf = []
        node = self
        while node is not None:
            buf.append(node)
            node = node.parent
        buf.reverse()
        return buf

    def __init__(self, *args):
        # no need to call super.__init__
        self.subtypes = set()

    def __contains__(self, val):
        return self is val or (
            type(val) is self.__class__ and
            val[:len(self)] == self
        )

    def __getattr__(self, val):
        if not val or not val[0].isupper():
            return tuple.__getattribute__(self, val)
        new = _TokenType(self + (val,))
        setattr(self, val, new)
        self.subtypes.add(new)
        new.parent = self
        return new

    def __hash__(self):
        return hash(tuple(self))

    def __repr__(self):
        return 'Token' + (self and '.' or '') + '.'.join(self)


Token       = _TokenType()

# Special token types
Text        = Token.Text
Whitespace  = Text.Whitespace
Newline     = Whitespace.Newline
Error       = Token.Error
# Text that doesn't belong to this lexer (e.g. HTML in PHP)
Other       = Token.Other

# Common token types for source code
Keyword     = Token.Keyword
Name        = Token.Name
Literal     = Token.Literal
String      = Literal.String
Number      = Literal.Number
Punctuation = Token.Punctuation
Operator    = Token.Operator
Wildcard    = Token.Wildcard
Comment     = Token.Comment
Assignment  = Token.Assignement

# Generic types for non-source code
Generic     = Token.Generic

# String and some others are not direct childs of Token.
# alias them:
Token.Token = Token
Token.String = String
Token.Number = Number

# SQL specific tokens
DML = Keyword.DML
DDL = Keyword.DDL
Command = Keyword.Command

Group = Token.Group
Group.Parenthesis = Token.Group.Parenthesis
Group.Comment = Token.Group.Comment
Group.Where = Token.Group.Where


def is_token_subtype(ttype, other):
    """
    Return True if ``ttype`` is a subtype of ``other``.

    exists for backwards compatibility. use ``ttype in other`` now.
    """
    return ttype in other


def string_to_tokentype(s):
    """
    Convert a string into a token type::

        >>> string_to_token('String.Double')
        Token.Literal.String.Double
        >>> string_to_token('Token.Literal.Number')
        Token.Literal.Number
        >>> string_to_token('')
        Token

    Tokens that are already tokens are returned unchanged:

        >>> string_to_token(String)
        Token.Literal.String
    """
    if isinstance(s, _TokenType):
        return s
    if not s:
        return Token
    node = Token
    for item in s.split('.'):
        node = getattr(node, item)
    return node

########NEW FILE########
__FILENAME__ = views
"""
Helper views for the debug toolbar. These are dynamically installed when the
debug toolbar is displayed, and typically can do Bad Things, so hooking up these
views in any other way is generally not advised.
"""

import os
import django.views.static
from django.conf import settings
from django.db import connection
from django.http import HttpResponseBadRequest
from django.shortcuts import render_to_response
from django.utils import simplejson
from django.utils.hashcompat import sha_constructor

class InvalidSQLError(Exception):
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return repr(self.value)

def debug_media(request, path):
    root = getattr(settings, 'DEBUG_TOOLBAR_MEDIA_ROOT', None)
    if root is None:
        parent = os.path.abspath(os.path.dirname(__file__))
        root = os.path.join(parent, 'media', 'debug_toolbar')
    return django.views.static.serve(request, path, root)

def sql_select(request):
    """
    Returns the output of the SQL SELECT statement.

    Expected GET variables:
        sql: urlencoded sql with positional arguments
        params: JSON encoded parameter values
        duration: time for SQL to execute passed in from toolbar just for redisplay
        hash: the hash of (secret + sql + params) for tamper checking
    """
    from debug_toolbar.panels.sql import reformat_sql
    sql = request.GET.get('sql', '')
    params = request.GET.get('params', '')
    hash = sha_constructor(settings.SECRET_KEY + sql + params).hexdigest()
    if hash != request.GET.get('hash', ''):
        return HttpResponseBadRequest('Tamper alert') # SQL Tampering alert
    if sql.lower().strip().startswith('select'):
        params = simplejson.loads(params)
        cursor = connection.cursor()
        cursor.execute(sql, params)
        headers = [d[0] for d in cursor.description]
        result = cursor.fetchall()
        cursor.close()
        context = {
            'result': result,
            'sql': reformat_sql(cursor.db.ops.last_executed_query(cursor, sql, params)),
            'duration': request.GET.get('duration', 0.0),
            'headers': headers,
        }
        return render_to_response('debug_toolbar/panels/sql_select.html', context)
    raise InvalidSQLError("Only 'select' queries are allowed.")

def sql_explain(request):
    """
    Returns the output of the SQL EXPLAIN on the given query.

    Expected GET variables:
        sql: urlencoded sql with positional arguments
        params: JSON encoded parameter values
        duration: time for SQL to execute passed in from toolbar just for redisplay
        hash: the hash of (secret + sql + params) for tamper checking
    """
    from debug_toolbar.panels.sql import reformat_sql
    sql = request.GET.get('sql', '')
    params = request.GET.get('params', '')
    hash = sha_constructor(settings.SECRET_KEY + sql + params).hexdigest()
    if hash != request.GET.get('hash', ''):
        return HttpResponseBadRequest('Tamper alert') # SQL Tampering alert
    if sql.lower().strip().startswith('select'):
        params = simplejson.loads(params)
        cursor = connection.cursor()

        if settings.DATABASE_ENGINE == "sqlite3":
            # SQLite's EXPLAIN dumps the low-level opcodes generated for a query;
            # EXPLAIN QUERY PLAN dumps a more human-readable summary
            # See http://www.sqlite.org/lang_explain.html for details
            cursor.execute("EXPLAIN QUERY PLAN %s" % (sql,), params)
        else:
            cursor.execute("EXPLAIN %s" % (sql,), params)

        headers = [d[0] for d in cursor.description]
        result = cursor.fetchall()
        cursor.close()
        context = {
            'result': result,
            'sql': reformat_sql(cursor.db.ops.last_executed_query(cursor, sql, params)),
            'duration': request.GET.get('duration', 0.0),
            'headers': headers,
        }
        return render_to_response('debug_toolbar/panels/sql_explain.html', context)
    raise InvalidSQLError("Only 'select' queries are allowed.")

def sql_profile(request):
    """
    Returns the output of running the SQL and getting the profiling statistics.

    Expected GET variables:
        sql: urlencoded sql with positional arguments
        params: JSON encoded parameter values
        duration: time for SQL to execute passed in from toolbar just for redisplay
        hash: the hash of (secret + sql + params) for tamper checking
    """
    from debug_toolbar.panels.sql import reformat_sql
    sql = request.GET.get('sql', '')
    params = request.GET.get('params', '')
    hash = sha_constructor(settings.SECRET_KEY + sql + params).hexdigest()
    if hash != request.GET.get('hash', ''):
        return HttpResponseBadRequest('Tamper alert') # SQL Tampering alert
    if sql.lower().strip().startswith('select'):
        params = simplejson.loads(params)
        cursor = connection.cursor()
        result = None
        headers = None
        result_error = None
        try:
            cursor.execute("SET PROFILING=1") # Enable profiling
            cursor.execute(sql, params) # Execute SELECT
            cursor.execute("SET PROFILING=0") # Disable profiling
            # The Query ID should always be 1 here but I'll subselect to get the last one just in case...
            cursor.execute("SELECT * FROM information_schema.profiling WHERE query_id=(SELECT query_id FROM information_schema.profiling ORDER BY query_id DESC LIMIT 1)")
            headers = [d[0] for d in cursor.description]
            result = cursor.fetchall()
        except:
            result_error = "Profiling is either not available or not supported by your database."
        cursor.close()
        context = {
            'result': result,
            'result_error': result_error,
            'sql': reformat_sql(cursor.db.ops.last_executed_query(cursor, sql, params)),
            'duration': request.GET.get('duration', 0.0),
            'headers': headers,
        }
        return render_to_response('debug_toolbar/panels/sql_profile.html', context)
    raise InvalidSQLError("Only 'select' queries are allowed.")

def template_source(request):
    """
    Return the source of a template, syntax-highlighted by Pygments if
    it's available.
    """
    from django.template import TemplateDoesNotExist
    from django.utils.safestring import mark_safe
    from django.conf import settings

    template_name = request.GET.get('template', None)
    if template_name is None:
        return HttpResponseBadRequest('"template" key is required')

    try: # Django 1.2 ...
        from django.template.loader import find_template_loader, make_origin
        loaders = []
        for loader_name in settings.TEMPLATE_LOADERS:
            loader = find_template_loader(loader_name)
            if loader is not None:
                loaders.append(loader)
        for loader in loaders:
            try:
                source, display_name = loader.load_template_source(template_name)
                origin = make_origin(display_name, loader, template_name, settings.TEMPLATE_DIRS)
                break
            except TemplateDoesNotExist:
                source = "Template Does Not Exist: %s" % (template_name,)
    except (ImportError, AttributeError): # Django 1.1 ...
        from django.template.loader import find_template_source
        source, origin = find_template_source(template_name)

    try:
        from pygments import highlight
        from pygments.lexers import HtmlDjangoLexer
        from pygments.formatters import HtmlFormatter

        source = highlight(source, HtmlDjangoLexer(), HtmlFormatter())
        source = mark_safe(source)
        source.pygmentized = True
    except ImportError:
        pass

    return render_to_response('debug_toolbar/panels/template_source.html', {
        'source': source,
        'template_name': template_name
    })

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from api.models import DeveloperApiKey, UserApiKey

class DeveloperKeyAdmin(admin.ModelAdmin):
    list_display = ('approved','developer', 'application_name', 'key')
    search_fields = ('application_name',)

admin.site.register(DeveloperApiKey , DeveloperKeyAdmin)

admin.site.register(UserApiKey)

########NEW FILE########
__FILENAME__ = forms
from django import forms
from api.models import DeveloperApiKey

class DeveloperApiKeyForm(forms.ModelForm):
    class Meta:
        model = DeveloperApiKey
        fields = ('application_name',)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.views.generic.simple import direct_to_template

urlpatterns = patterns('developer.views',
 url(r'^$', 'home', name='developer_home'),
 url(r'^crossdomain.xml$', direct_to_template, {'template': 'developer/crossdomain.xml'}),
 url(r'^apply$', 'apply', name="developer_apply"),
 url(r'^user_keys$', 'user_keys', name='developer_user_keys'),
 url(r'^(?P<name>\w+)/print', 'docs_print', name="developer_docs_print"),
 url(r'^(?P<name>\w+)/(?P<page>\w+)', 'docs_page', name="developer_docs_page"),
)

########NEW FILE########
__FILENAME__ = views
from forms import DeveloperApiKeyForm
from api.models import DeveloperApiKey, UserApiKey
from django.shortcuts import render_to_response
from django.http import HttpResponseRedirect
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.http import Http404
from django.template import RequestContext,TemplateDoesNotExist
from django.template.loader import get_template

from os.path import isfile, join

@login_required
def home(request):
    keys = DeveloperApiKey.objects.filter(developer = request.user)
    return render_to_response("developer/index.html", {
      "keys" : keys,
      "name": "api",
      "pages": docs["api"]
    }, context_instance=RequestContext(request))

@login_required
def apply(request):
    form = DeveloperApiKeyForm(request.POST or None)

    if request.method == 'POST':
        if form.is_valid(): # All validation rules pass
            dev_key = form.save( commit=False )
            dev_key.developer = request.user
            dev_key.approved = True
            dev_key.save()
            request.user.message_set.create(message="Created developer key.")
            return HttpResponseRedirect(reverse("developer_home"))

    return render_to_response("developer/apply.html", {
               "form": form,
               "name": "api",
               "pages": docs["api"]
               }, context_instance=RequestContext(request))


@login_required
def user_keys(request):
    if request.method == 'POST' and request.POST.get("key_id"):
        k = UserApiKey.objects.get(pk=request.POST.get("key_id"))
        appname = k.developer_key.application_name
        k.delete()
        request.user.message_set.create(message=("Deleted developer key for application %s." % appname))

    keys = UserApiKey.objects.filter(user = request.user)
    return render_to_response("developer/user_keys.html", {
      "keys" : keys,
    }, context_instance=RequestContext(request))

# syntax is name_of_tutorial: {url_of_page: name_of_page...
# this must match the templates found in tutorial/ . ie tutorial/name_of_tutorial/url_of_page.html
docs = { "api":[("start","Getting Started"),
                ("permissions", "Permissions"),
                ("traversing", "Traversing Data"),
                ("updating", "Updating, deleting"),
                ("reference", "API Reference"),
                ],
              }

def list_has_key(lis,key):
    for k,v in lis:
        if k == key:
            return True
    return False

def list_get(lis, key):
    for k,v in lis:
        if k == key:
            return (k,v)
    return None

def docs_page(request, name, page="start"):
    def mk_path(page):
        return join("developer", name, page+".html")
    if not docs.has_key(name) or not list_has_key(docs[name], page) :
        raise Http404

    return render_to_response("developer/base.html", {
            "page_template": mk_path(page),
            "name": name,
            "this": list_get(docs[name], page),
            "pages": docs[name],
    }, context_instance=RequestContext(request))

def docs_print(request, name):
    if not docs.has_key(name):
        raise Http404

    return render_to_response("developer/print.html", {
            "page_templates" : map(lambda (x,y): join("developer", name, x + ".html"), docs[name]),
            "name":name,
            "this": ("print", "Print Version"),
            "pages": docs[name],
            }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from django_evolution.models import Version, Evolution

admin.site.register(Version)
admin.site.register(Evolution)

########NEW FILE########
__FILENAME__ = common
import django
from django.core.management import color
from django.db import connection as default_connection
from django.db.backends.util import truncate_name
import copy


class BaseEvolutionOperations(object):
    connection = None

    def __init__(self, connection = default_connection):
        self.connection = connection

    def quote_sql_param(self, param):
        "Add protective quoting around an SQL string parameter"
        if isinstance(param, basestring):
            return u"'%s'" % unicode(param).replace(u"'",ur"\'")
        else:
            return param

    def rename_table(self, model, old_db_tablename, db_tablename):
        if old_db_tablename == db_tablename:
            # No Operation
            return []

        style = color.no_style()
        qn = self.connection.ops.quote_name
        max_name_length = self.connection.ops.max_name_length()
        creation = self.connection.creation

        sql = []
        refs = {}
        models = []

        for field in model._meta.local_many_to_many:
            if (field.rel and
                field.rel.through and
                field.rel.through._meta.db_table == old_db_tablename):

                through = field.rel.through

                for m2m_field in through._meta.local_fields:
                    if m2m_field.rel and m2m_field.rel.to == model:
                        models.append(m2m_field.rel.to)
                        refs.setdefault(m2m_field.rel.to, []).append(
                            (through, m2m_field))

        remove_refs = refs.copy()

        for relto in models:
            sql.extend(creation.sql_remove_table_constraints(relto, remove_refs,
                                                             style))
        params = (qn(old_db_tablename), qn(db_tablename))
        sql.append('ALTER TABLE %s RENAME TO %s;' % params)

        for relto in models:
            for rel_class, f in refs[relto]:
                if rel_class._meta.db_table == old_db_tablename:
                    rel_class._meta.db_table = db_tablename

                rel_class._meta.db_table = \
                    truncate_name(rel_class._meta.db_table, max_name_length)

            sql.extend(creation.sql_for_pending_references(relto, style, refs))

        return sql

    def delete_column(self, model, f):
        qn = self.connection.ops.quote_name
        params = (qn(model._meta.db_table), qn(f.column))

        return ['ALTER TABLE %s DROP COLUMN %s CASCADE;' % params]

    def delete_table(self, table_name):
        qn = self.connection.ops.quote_name
        return ['DROP TABLE %s;' % qn(table_name)]

    def add_m2m_table(self, model, f):
        style = color.no_style()
        creation = self.connection.creation

        if f.rel.through:
            references = {}
            pending_references = {}

            sql, references = creation.sql_create_model(f.rel.through, style)

            for refto, refs in references.items():
                pending_references.setdefault(refto, []).extend(refs)
                sql.extend(creation.sql_for_pending_references(
                    refto, style, pending_references))

            sql.extend(creation.sql_for_pending_references(
                f.rel.through, style, pending_references))
        else:
            sql = creation.sql_for_many_to_many_field(model, f, style)

        return sql

    def add_column(self, model, f, initial):
        qn = self.connection.ops.quote_name

        if f.rel:
            # it is a foreign key field
            # NOT NULL REFERENCES "django_evolution_addbasemodel" ("id") DEFERRABLE INITIALLY DEFERRED
            # ALTER TABLE <tablename> ADD COLUMN <column name> NULL REFERENCES <tablename1> ("<colname>") DEFERRABLE INITIALLY DEFERRED
            related_model = f.rel.to
            related_table = related_model._meta.db_table
            related_pk_col = related_model._meta.pk.name
            constraints = ['%sNULL' % (not f.null and 'NOT ' or '')]
            if f.unique or f.primary_key:
                constraints.append('UNIQUE')
            params = (qn(model._meta.db_table), qn(f.column), f.db_type(), ' '.join(constraints),
                qn(related_table), qn(related_pk_col), self.connection.ops.deferrable_sql())
            output = ['ALTER TABLE %s ADD COLUMN %s %s %s REFERENCES %s (%s) %s;' % params]
        else:
            null_constraints = '%sNULL' % (not f.null and 'NOT ' or '')
            if f.unique or f.primary_key:
                unique_constraints = 'UNIQUE'
            else:
                unique_constraints = ''

            # At this point, initial can only be None if null=True, otherwise it is
            # a user callable or the default AddFieldInitialCallback which will shortly raise an exception.
            if initial is not None:
                params = (qn(model._meta.db_table), qn(f.column), f.db_type(), unique_constraints)
                output = ['ALTER TABLE %s ADD COLUMN %s %s %s;' % params]

                if callable(initial):
                    params = (qn(model._meta.db_table), qn(f.column), initial(), qn(f.column))
                    output.append('UPDATE %s SET %s = %s WHERE %s IS NULL;' % params)
                else:
                    params = (qn(model._meta.db_table), qn(f.column), qn(f.column))
                    output.append(('UPDATE %s SET %s = %%s WHERE %s IS NULL;' % params, (initial,)))

                if not f.null:
                    # Only put this sql statement if the column cannot be null.
                    output.append(self.set_field_null(model, f, f.null))
            else:
                params = (qn(model._meta.db_table), qn(f.column), f.db_type(),' '.join([null_constraints, unique_constraints]))
                output = ['ALTER TABLE %s ADD COLUMN %s %s %s;' % params]
        return output

    def set_field_null(self, model, f, null):
        qn = self.connection.ops.quote_name
        params = (qn(model._meta.db_table), qn(f.column),)
        if null:
            return 'ALTER TABLE %s ALTER COLUMN %s DROP NOT NULL;' % params
        else:
            return 'ALTER TABLE %s ALTER COLUMN %s SET NOT NULL;' % params

    def create_index(self, model, f):
        "Returns the CREATE INDEX SQL statements."
        style = color.no_style()

        return self.connection.creation.sql_indexes_for_field(model, f, style)

    def drop_index(self, model, f):
        qn = self.connection.ops.quote_name
        index_name = self.get_index_name(model, f)
        max_length = self.connection.ops.max_name_length()

        return ['DROP INDEX %s;' % qn(truncate_name(index_name, max_length))]

    def get_index_name(self, model, f):
        if django.VERSION >= (1, 2):
            colname = self.connection.creation._digest(f.column)
        else:
            colname = f.column

        return '%s_%s' % (model._meta.db_table, colname)

    def change_null(self, model, field_name, new_null_attr, initial=None):
        qn = self.connection.ops.quote_name
        opts = model._meta
        f = opts.get_field(field_name)
        output = []
        if new_null_attr:
            # Setting null to True
            opts = model._meta
            params = (qn(opts.db_table), qn(f.column),)
            output.append(self.set_field_null(model, f, new_null_attr))
        else:
            if initial is not None:
                output = []
                if callable(initial):
                    params = (qn(opts.db_table), qn(f.column), initial(), qn(f.column))
                    output.append('UPDATE %s SET %s = %s WHERE %s IS NULL;' % params)
                else:
                    params = (qn(opts.db_table), qn(f.column), qn(f.column))
                    output.append(('UPDATE %s SET %s = %%s WHERE %s IS NULL;' % params, (initial,)))
            output.append(self.set_field_null(model, f, new_null_attr))

        return output

    def change_max_length(self, model, field_name, new_max_length, initial=None):
        qn = self.connection.ops.quote_name
        opts = model._meta
        f = opts.get_field(field_name)
        f.max_length = new_max_length
        params = (qn(opts.db_table), qn(f.column), f.db_type(), qn(f.column), f.db_type())
        return ['ALTER TABLE %s ALTER COLUMN %s TYPE %s USING CAST(%s as %s);' % params]

    def change_db_column(self, model, field_name, new_db_column, initial=None):
        opts = model._meta
        old_field = opts.get_field(field_name)
        new_field = copy.copy(old_field)
        new_field.column = new_db_column
        return self.rename_column(opts, old_field, new_field)

    def change_db_table(self, model, old_db_tablename, new_db_tablename):
        return self.rename_table(model, old_db_tablename, new_db_tablename)

    def change_db_index(self, model, field_name, new_db_index, initial=None):
        f = model._meta.get_field(field_name)
        f.db_index = new_db_index
        if new_db_index:
            return self.create_index(model, f)
        else:
            return self.drop_index(model, f)

    def change_unique(self, model, field_name, new_unique_value, initial=None):
        qn = self.connection.ops.quote_name
        opts = model._meta
        f = opts.get_field(field_name)
        constraint_name = truncate_name('%s_%s_key' % (opts.db_table, f.column),
                                        self.connection.ops.max_name_length())

        if new_unique_value:
            params = (qn(opts.db_table), constraint_name, qn(f.column),)
            return ['ALTER TABLE %s ADD CONSTRAINT %s UNIQUE(%s);' % params]
        else:
            params = (qn(opts.db_table), constraint_name,)
            return ['ALTER TABLE %s DROP CONSTRAINT %s;' % params]

########NEW FILE########
__FILENAME__ = mysql
from django.core.management import color

from common import BaseEvolutionOperations

class EvolutionOperations(BaseEvolutionOperations):
    def rename_column(self, opts, old_field, f):
        if old_field.column == f.column:
            # No Operation
            return []

        qn = self.connection.ops.quote_name
        style = color.no_style()

        ###
        col_type = f.db_type()
        tablespace = f.db_tablespace or opts.db_tablespace
        if col_type is None:
            # Skip ManyToManyFields, because they're not represented as
            # database columns in this table.
            return []
        # Make the definition (e.g. 'foo VARCHAR(30)') for this field.
        field_output = [style.SQL_FIELD(qn(f.column)),
            style.SQL_COLTYPE(col_type)]
        field_output.append(style.SQL_KEYWORD('%sNULL' % (not f.null and 'NOT ' or '')))
        if f.primary_key:
            field_output.append(style.SQL_KEYWORD('PRIMARY KEY'))
        if f.unique:
            field_output.append(style.SQL_KEYWORD('UNIQUE'))
        if tablespace and self.connection.features.supports_tablespaces and (f.unique or f.primary_key) and self.connection.features.autoindexes_primary_keys:
            # We must specify the index tablespace inline, because we
            # won't be generating a CREATE INDEX statement for this field.
            field_output.append(self.connection.ops.tablespace_sql(tablespace, inline=True))
        if f.rel:
            field_output.append(style.SQL_KEYWORD('REFERENCES') + ' ' + \
                style.SQL_TABLE(qn(f.rel.to._meta.db_table)) + ' (' + \
                style.SQL_FIELD(qn(f.rel.to._meta.get_field(f.rel.field_name).column)) + ')' +
                self.connection.ops.deferrable_sql()
            )

        params = (qn(opts.db_table), qn(old_field.column), ' '.join(field_output))
        return ['ALTER TABLE %s CHANGE COLUMN %s %s;' % params]

    def set_field_null(self, model, f, null):
        qn = self.connection.ops.quote_name
        params = (qn(model._meta.db_table), qn(f.column), f.db_type())
        if null:
            return 'ALTER TABLE %s MODIFY COLUMN %s %s DEFAULT NULL;' % params
        else:
            return 'ALTER TABLE %s MODIFY COLUMN %s %s NOT NULL;' % params

    def change_max_length(self, model, field_name, new_max_length, initial=None):
        qn = self.connection.ops.quote_name
        opts = model._meta
        f = opts.get_field(field_name)
        f.max_length = new_max_length
        params = {
            'table': qn(opts.db_table),
            'column': qn(f.column),
            'length': f.max_length,
            'type': f.db_type()
        }
        return ['UPDATE %(table)s SET %(column)s=LEFT(%(column)s,%(length)d);' % params,
                'ALTER TABLE %(table)s MODIFY COLUMN %(column)s %(type)s;' % params]

    def drop_index(self, model, f):
        qn = self.connection.ops.quote_name
        params = (qn(self.get_index_name(model, f)), qn(model._meta.db_table))
        return ['DROP INDEX %s ON %s;' % params]

    def change_unique(self, model, field_name, new_unique_value, initial=None):
        qn = self.connection.ops.quote_name
        opts = model._meta
        f = opts.get_field(field_name)
        constraint_name = '%s' % (f.column,)
        if new_unique_value:
            params = (constraint_name, qn(opts.db_table), qn(f.column),)
            return ['CREATE UNIQUE INDEX %s ON %s(%s);' % params]
        else:
            params = (constraint_name, qn(opts.db_table))
            return ['DROP INDEX %s ON %s;' % params]

    def rename_table(self, model, old_db_tablename, db_tablename):
        if old_db_tablename == db_tablename:
            return []

        qn = self.connection.ops.quote_name
        params = (qn(old_db_tablename), qn(db_tablename))
        return ['RENAME TABLE %s TO %s;' % params]

########NEW FILE########
__FILENAME__ = mysql_old
# MySQL_old behaviour is identical to mysql base
from mysql import *

########NEW FILE########
__FILENAME__ = postgresql
from django.core.management import color
from django.db.backends.util import truncate_name

from common import BaseEvolutionOperations


class EvolutionOperations(BaseEvolutionOperations):
    def rename_column(self, opts, old_field, new_field):
        if old_field.column == new_field.column:
            # No Operation
            return []

        style = color.no_style()
        qn = self.connection.ops.quote_name
        max_name_length = self.connection.ops.max_name_length()
        creation = self.connection.creation
        sql = []
        refs = {}
        models = []

        if old_field.primary_key:
            for field in opts.local_many_to_many:
                if field.rel and field.rel.through:
                    through = field.rel.through

                    for m2m_f in through._meta.local_fields:
                        if (m2m_f.rel and
                            m2m_f.rel.to._meta.db_table == opts.db_table and
                            m2m_f.rel.field_name == old_field.column):

                            models.append(m2m_f.rel.to)
                            refs.setdefault(m2m_f.rel.to, []).append(
                                (through, m2m_f))

            remove_refs = refs.copy()

            for relto in models:
                sql.extend(creation.sql_remove_table_constraints(
                    relto, remove_refs, style))

        params = (qn(opts.db_table),
                  truncate_name(qn(old_field.column), max_name_length),
                  truncate_name(qn(new_field.column), max_name_length))
        sql.append('ALTER TABLE %s RENAME COLUMN %s TO %s;' % params)

        if old_field.primary_key:
            for relto in models:
                for rel_class, f in refs[relto]:
                    f.rel.field_name = new_field.column

                del relto._meta._fields[old_field.name]
                relto._meta._fields[new_field.name] = new_field

                sql.extend(creation.sql_for_pending_references(
                    relto, style, refs))

        return sql

    def get_index_name(self, model, f):
        # By default, Django 1.2 will use a digest hash for the column name.
        # The PostgreSQL support, however, uses the column name itself.
        return '%s_%s' % (model._meta.db_table, f.column)

########NEW FILE########
__FILENAME__ = postgresql_psycopg2
# Psycopg2 behaviour is identical to Psycopg1
from postgresql import *

########NEW FILE########
__FILENAME__ = sqlite3
from django.core.management import color
from django.db import models

from common import BaseEvolutionOperations

TEMP_TABLE_NAME = 'TEMP_TABLE'

class EvolutionOperations(BaseEvolutionOperations):
    def delete_column(self, model, f):
        output = []

        field_list = [field for field in model._meta.local_fields
                        if f.name != field.name # Remove the field to be deleted
                        and field.db_type() is not None] # and any Generic fields
        table_name = model._meta.db_table

        output.extend(self.create_temp_table(field_list))
        output.extend(self.copy_to_temp_table(table_name, field_list))
        output.extend(self.delete_table(table_name))
        output.extend(self.create_table(table_name, field_list))
        output.extend(self.copy_from_temp_table(table_name, field_list))
        output.extend(self.delete_table(TEMP_TABLE_NAME))

        return output

    def copy_to_temp_table(self, source_table_name, original_field_list,
                           new_field_list=None):
        qn = self.connection.ops.quote_name

        source_columns = self.column_names(original_field_list)

        if new_field_list:
            temp_columns = self.column_names(new_field_list)
        else:
            temp_columns = source_columns

        return ['INSERT INTO %s (%s) SELECT %s FROM %s;' %
                (qn(TEMP_TABLE_NAME), temp_columns, source_columns,
                 qn(source_table_name))]

    def copy_from_temp_table(self, dest_table_name, field_list):
        qn = self.connection.ops.quote_name
        params = {
            'dest_table_name': qn(dest_table_name),
            'temp_table': qn(TEMP_TABLE_NAME),
            'column_names': self.column_names(field_list),
        }

        return ['INSERT INTO %(dest_table_name)s (%(column_names)s) SELECT %(column_names)s FROM %(temp_table)s;' % params]

    def column_names(self, field_list):
        qn = self.connection.ops.quote_name
        columns = []

        for field in field_list:
            if not isinstance(field, models.ManyToManyField):
                columns.append(qn(field.column))

        return ', '.join(columns)

    def insert_to_temp_table(self, field, initial):
        qn = self.connection.ops.quote_name

        # At this point, initial can only be None if null=True, otherwise it is
        # a user callable or the default AddFieldInitialCallback which will shortly raise an exception.
        if initial is None:
            return []

        params = {
            'table_name': qn(TEMP_TABLE_NAME),
            'column_name': qn(field.column),
        }

        if callable(initial):
            params['value'] = initial()
            return ["UPDATE %(table_name)s SET %(column_name)s = %(value)s;" % params]
        else:
            return [("UPDATE %(table_name)s SET %(column_name)s = %%s;" % params, (initial,))]


    def create_temp_table(self, field_list):
        return self.create_table(TEMP_TABLE_NAME, field_list, True, False)

    def create_indexes_for_table(self, table_name, field_list):
        class FakeMeta(object):
            def __init__(self, table_name, field_list):
                self.db_table = table_name
                self.local_fields = field_list
                self.fields = field_list # Required for Pre QS-RF support
                self.db_tablespace = None
                self.managed = True
                self.proxy = False

        class FakeModel(object):
            def __init__(self, table_name, field_list):
                self._meta = FakeMeta(table_name, field_list)

        style = color.no_style()
        return self.connection.creation.sql_indexes_for_model(FakeModel(table_name, field_list), style)

    def create_table(self, table_name, field_list, temporary=False, create_index=True):
        qn = self.connection.ops.quote_name
        output = []

        create = ['CREATE']
        if temporary:
            create.append('TEMPORARY')
        create.append('TABLE %s' % qn(table_name))
        output = [' '.join(create)]
        output.append('(')
        columns = []
        for field in field_list:
            if not models.ManyToManyField == field.__class__:
                column_name = qn(field.column)
                column_type = field.db_type()
                params = [column_name, column_type]

                # Always use null if this is a temporary table. It may be
                # used to create a new field (which will be null while data is
                # copied across from the old table).
                if temporary or field.null:
                    params.append('NULL')
                else:
                    params.append('NOT NULL')

                if field.unique:
                    params.append('UNIQUE')

                if field.primary_key:
                    params.append('PRIMARY KEY')

                columns.append(' '.join(params))

        output.append(', '.join(columns))
        output.append(');')
        output = [''.join(output)]

        if create_index:
            output.extend(self.create_indexes_for_table(table_name, field_list))

        return output

    def rename_column(self, opts, old_field, new_field):
        if old_field.column == new_field.column:
            # No Operation
            return []

        original_fields = opts.local_fields
        new_fields = []
        for f in original_fields:
            if f.db_type() is not None: # Ignore Generic Fields
                if f.name == old_field.name:
                    new_fields.append(new_field)
                else:
                    new_fields.append(f)

        table_name = opts.db_table
        output = []
        output.extend(self.create_temp_table(new_fields))
        output.extend(self.copy_to_temp_table(table_name, original_fields,
                                              new_fields))
        output.extend(self.delete_table(table_name))
        output.extend(self.create_table(table_name, new_fields))
        output.extend(self.copy_from_temp_table(table_name, new_fields))
        output.extend(self.delete_table(TEMP_TABLE_NAME))

        return output

    def add_column(self, model, f, initial):
        output = []
        table_name = model._meta.db_table
        original_fields = [field for field in model._meta.local_fields if field.db_type() is not None]
        new_fields = list(original_fields)
        new_fields.append(f)

        output.extend(self.create_temp_table(new_fields))
        output.extend(self.copy_to_temp_table(table_name, original_fields))
        output.extend(self.insert_to_temp_table(f, initial))
        output.extend(self.delete_table(table_name))
        output.extend(self.create_table(table_name, new_fields, create_index=False))
        output.extend(self.copy_from_temp_table(table_name, new_fields))
        output.extend(self.delete_table(TEMP_TABLE_NAME))
        return output

    def change_null(self, model, field_name, new_null_attr, initial=None):
        return self.change_attribute(model, field_name, 'null', new_null_attr, initial)

    def change_max_length(self, model, field_name, new_max_length, initial=None):
        return self.change_attribute(model, field_name, 'max_length', new_max_length, initial)

    def change_unique(self, model, field_name, new_unique_value, initial=None):
        return self.change_attribute(model, field_name, '_unique', new_unique_value, initial)

    def change_attribute(self, model, field_name, attr_name, new_attr_value, initial=None):
        output = []
        opts = model._meta
        table_name = opts.db_table
        setattr(opts.get_field(field_name), attr_name, new_attr_value)
        fields = [f for f in opts.local_fields if f.db_type() is not None]

        output.extend(self.create_temp_table(fields))
        output.extend(self.copy_to_temp_table(table_name, fields))
        output.extend(self.insert_to_temp_table(opts.get_field(field_name), initial))
        output.extend(self.delete_table(table_name))
        output.extend(self.create_table(table_name, fields, create_index=False))
        output.extend(self.copy_from_temp_table(table_name, fields))
        output.extend(self.delete_table(TEMP_TABLE_NAME))
        return output

########NEW FILE########
__FILENAME__ = diff
from django.db import models
from django.db.models.fields.related import *

from django_evolution import EvolutionException
from django_evolution.mutations import DeleteField, AddField, DeleteModel, ChangeField
from django_evolution.signature import ATTRIBUTE_DEFAULTS

try:
    set
except ImportError:
    from sets import Set as set #Python 2.3 Fallback


class NullFieldInitialCallback(object):
    def __init__(self, app, model, field):
        self.app = app
        self.model = model
        self.field = field

    def __repr__(self):
        return '<<USER VALUE REQUIRED>>'

    def __call__(self):
        raise EvolutionException(
            "Cannot use hinted evolution: AddField or ChangeField mutation "
            "for '%s.%s' in '%s' requires user-specified initial value."
            % (self.model, self.field, self.app))


def get_initial_value(app_label, model_name, field_name):
    """Derive an initial value for a field.

    If a default has been provided on the field definition or the field allows
    for an empty string, that value will be used. Otherwise, a placeholder
    callable will be used. This callable cannot actually be used in an
    evolution, but will indicate that user input is required.
    """
    model = models.get_model(app_label, model_name)
    field = model._meta.get_field(field_name)

    if field and (field.has_default() or
                  (field.empty_strings_allowed and field.blank)):
        return field.get_default()

    return NullFieldInitialCallback(app_label, model_name, field_name)


class Diff(object):
    """
    A diff between two model signatures.

    The resulting diff is contained in two attributes:

    self.changed = {
        app_label: {
            'changed': {
                model_name : {
                    'added': [ list of added field names ]
                    'deleted': [ list of deleted field names ]
                    'changed': {
                        field: [ list of modified property names ]
                    }
                }
            'deleted': [ list of deleted model names ]
        }
    }
    self.deleted = {
        app_label: [ list of models in deleted app ]
    }
    """
    def __init__(self, original, current):
        self.original_sig = original
        self.current_sig = current

        self.changed = {}
        self.deleted = {}

        if self.original_sig.get('__version__', 1) != 1:
            raise EvolutionException(
                "Unknown version identifier in original signature: %s",
                self.original_sig['__version__'])

        if self.current_sig.get('__version__', 1) != 1:
            raise EvolutionException(
                "Unknown version identifier in target signature: %s",
                self.current_sig['__version__'])

        for app_name, old_app_sig in original.items():
            if app_name == '__version__':
                # Ignore the __version__ tag
                continue

            new_app_sig = self.current_sig.get(app_name, None)

            if new_app_sig is None:
                # App has been deleted
                self.deleted[app_name] = old_app_sig.keys()
                continue

            for model_name, old_model_sig in old_app_sig.items():
                new_model_sig = new_app_sig.get(model_name, None)

                if new_model_sig is None:
                    # Model has been deleted
                    self.changed.setdefault(app_name,
                        {}).setdefault('deleted',
                        []).append(model_name)
                    continue

                old_fields = old_model_sig['fields']
                new_fields = new_model_sig['fields']

                # Look for deleted or modified fields
                for field_name, old_field_data in old_fields.items():
                    new_field_data = new_fields.get(field_name, None)

                    if new_field_data is None:
                        # Field has been deleted
                        self.changed.setdefault(app_name,
                            {}).setdefault('changed',
                            {}).setdefault(model_name,
                            {}).setdefault('deleted',
                            []).append(field_name)
                        continue

                    properties = set(old_field_data.keys())
                    properties.update(new_field_data.keys())

                    for prop in properties:
                        old_value = old_field_data.get(prop,
                            ATTRIBUTE_DEFAULTS.get(prop, None))
                        new_value = new_field_data.get(prop,
                            ATTRIBUTE_DEFAULTS.get(prop, None))

                        if old_value != new_value:
                            try:
                                if (prop == 'field_type' and
                                    (old_value().get_internal_type() ==
                                     new_value().get_internal_type())):
                                    continue
                            except TypeError:
                                pass

                            # Field has been changed
                            self.changed.setdefault(app_name,
                                {}).setdefault('changed',
                                {}).setdefault(model_name,
                                {}).setdefault('changed',
                                {}).setdefault(field_name,[]).append(prop)

                # Look for added fields
                new_fields = new_model_sig['fields']

                for field_name, new_field_data in new_fields.items():
                    old_field_data = old_fields.get(field_name, None)

                    if old_field_data is None:
                        self.changed.setdefault(app_name,
                            {}).setdefault('changed',
                            {}).setdefault(model_name,
                            {}).setdefault('added',
                            []).append(field_name)

    def is_empty(self, ignore_apps=True):
        """Is this an empty diff? i.e., is the source and target the same?

        Set 'ignore_apps=False' if you wish to ignore changes caused by
        deleted applications. This is used when you don't purge deleted
        applications during an evolve.
        """
        if ignore_apps:
            return not self.changed
        else:
            return not self.deleted and not self.changed

    def __str__(self):
        "Output an application signature diff in a human-readable format"
        lines = []

        for app_label in self.deleted:
            lines.append('The application %s has been deleted' % app_label)

        for app_label, app_changes in self.changed.items():
            for model_name in app_changes.get('deleted', {}):
                lines.append('The model %s.%s has been deleted'
                             % (app_label, model_name))

            for model_name, change in app_changes.get('changed', {}).items():
                lines.append('In model %s.%s:' % (app_label, model_name))

                for field_name in change.get('added',[]):
                    lines.append("    Field '%s' has been added" % field_name)

                for field_name in change.get('deleted',[]):
                    lines.append("    Field '%s' has been deleted" % field_name)

                for field_name,field_change in change.get('changed',{}).items():
                    lines.append("    In field '%s':" % field_name)

                    for prop in field_change:
                        lines.append("        Property '%s' has changed" % prop)

        return '\n'.join(lines)

    def evolution(self):
        "Generate an evolution that would neutralize the diff"
        mutations = {}

        for app_label, app_changes in self.changed.items():
            for model_name, change in app_changes.get('changed', {}).items():
                for field_name in change.get('added',{}):
                    field_sig = self.current_sig[app_label][model_name]['fields'][field_name]
                    add_params = [(key,field_sig[key])
                                    for key in field_sig.keys()
                                    if key in ATTRIBUTE_DEFAULTS.keys()]
                    add_params.append(('field_type', field_sig['field_type']))

                    if (field_sig['field_type'] != models.ManyToManyField and
                        not field_sig.get('null', ATTRIBUTE_DEFAULTS['null'])):
                        add_params.append(
                            ('initial',
                             get_initial_value(app_label, model_name,
                                               field_name)))

                    if 'related_model' in field_sig:
                        add_params.append(('related_model',
                                           '%s' % field_sig['related_model']))

                    mutations.setdefault(app_label,[]).append(
                        AddField(model_name, field_name, **dict(add_params)))

                for field_name in change.get('deleted',[]):
                    mutations.setdefault(app_label,[]).append(
                        DeleteField(model_name, field_name))

                for field_name,field_change in change.get('changed',{}).items():
                    changed_attrs = {}
                    current_field_sig = self.current_sig[app_label][model_name]['fields'][field_name]

                    for prop in field_change:
                        if prop == 'related_model':
                            changed_attrs[prop] = current_field_sig[prop]
                        else:
                            changed_attrs[prop] = \
                                current_field_sig.get(prop,
                                                      ATTRIBUTE_DEFAULTS[prop])

                    if (changed_attrs.has_key('null') and
                        current_field_sig['field_type'] !=
                            models.ManyToManyField and
                        not current_field_sig.get('null',
                                                  ATTRIBUTE_DEFAULTS['null'])):
                        changed_attrs['initial'] = \
                            get_initial_value(app_label, model_name, field_name)

                    mutations.setdefault(app_label,[]).append(
                        ChangeField(model_name, field_name, **changed_attrs))

            for model_name in app_changes.get('deleted',{}):
                mutations.setdefault(app_label,[]).append(
                    DeleteModel(model_name))

        return mutations

########NEW FILE########
__FILENAME__ = evolve
import os

from django_evolution import EvolutionException, is_multi_db
from django_evolution.models import Evolution
from django_evolution.mutations import SQLMutation


def get_evolution_sequence(app):
    "Obtain the full evolution sequence for an application"
    try:
        app_name = '.'.join(app.__name__.split('.')[:-1])
        evolution_module = __import__(app_name + '.evolutions',{},{},[''])
        return evolution_module.SEQUENCE
    except:
        return []


def get_unapplied_evolutions(app, database):
    "Obtain the list of unapplied evolutions for an application"
    sequence = get_evolution_sequence(app)
    app_label = app.__name__.split('.')[-2]

    evolutions = Evolution.objects.filter(app_label=app_label)

    if is_multi_db():
        evolutions = evolutions.using(database)

    applied = [evo.label for evo in evolutions]

    return [seq for seq in sequence if seq not in applied]


def get_mutations(app, evolution_labels, database):
    """
    Obtain the list of mutations described by the named evolutions.
    """
    # For each item in the evolution sequence. Check each item to see if it is
    # a python file or an sql file.
    try:
        app_name = '.'.join(app.__name__.split('.')[:-1])
        evolution_module = __import__(app_name + '.evolutions', {}, {}, [''])
    except ImportError:
        return []

    mutations = []

    for label in evolution_labels:
        directory_name = os.path.dirname(evolution_module.__file__)

        # The first element is used for compatibility purposes.
        filenames = [
            os.path.join(directory_name, label + '.sql'),
            os.path.join(directory_name, "%s_%s.sql" % (database, label)),
        ]

        found = False

        for filename in filenames:
            if os.path.exists(filename):
                sql = []
                sql_file = open(filename)

                for line in sql_file:
                    sql.append(line)

                mutations.append(SQLMutation(label, sql))

                found = True
                break

        if not found:
            try:
                module_name = [evolution_module.__name__, label]
                module = __import__('.'.join(module_name),
                                    {}, {}, [module_name]);
                mutations.extend(module.MUTATIONS)
            except ImportError:
                raise EvolutionException(
                    'Error: Failed to find an SQL or Python evolution named %s'
                    % label)

    return mutations

########NEW FILE########
__FILENAME__ = evolve
from optparse import make_option
import sys
try:
    import cPickle as pickle
except ImportError:
    import pickle as pickle

from django.conf import settings
from django.core.exceptions import ImproperlyConfigured
from django.core.management.base import BaseCommand, CommandError
from django.db.models import get_apps, get_app
from django.db import connection, transaction

from django_evolution import CannotSimulate, EvolutionException, is_multi_db
from django_evolution.diff import Diff
from django_evolution.evolve import get_unapplied_evolutions, get_mutations
from django_evolution.models import Version, Evolution
from django_evolution.mutations import DeleteApplication
from django_evolution.signature import create_project_sig
from django_evolution.utils import write_sql, execute_sql

class Command(BaseCommand):
    option_list = BaseCommand.option_list + (
        make_option(
            '--noinput', action='store_false', dest='interactive', default=True,
            help='Tells Django to NOT prompt the user for input of any kind.'),
        make_option(
            '--hint', action='store_true', dest='hint', default=False,
            help='Generate an evolution script that would update the app.'),
        make_option(
            '--purge', action='store_true', dest='purge', default=False,
            help='Generate evolutions to delete stale applications.'),
        make_option(
            '--sql', action='store_true', dest='compile_sql',
            default=False,
            help='Compile a Django evolution script into SQL.'),
        make_option(
            '-x', '--execute', action='store_true', dest='execute',
            default=False,
            help='Apply the evolution to the database.'),
        make_option(
            '--database', action='store', dest='database',
            help='Nominates a database to synchronize.'),
    )

    if '--verbosity' not in [opt.get_opt_string()
                             for opt in BaseCommand.option_list]:
        option_list += make_option('-v', '--verbosity', action='store',
                                   dest='verbosity', default='1',
            type='choice', choices=['0', '1', '2'],
            help='Verbosity level; 0=minimal output, 1=normal output, '
                 '2=all output'),

    help = 'Evolve the models in a Django project.'
    args = '<appname appname ...>'

    requires_model_validation = False

    def handle(self, *app_labels, **options):
        self.evolve(*app_labels, **options)

    def evolve(self, *app_labels, **options):
        verbosity = int(options['verbosity'])
        interactive = options['interactive']
        execute = options['execute']
        compile_sql = options['compile_sql']
        hint = options['hint']
        purge = options['purge']
        database = options['database']

        if not database and is_multi_db():
            from django.db.utils import DEFAULT_DB_ALIAS
            database = DEFAULT_DB_ALIAS

        using_args = {}

        if is_multi_db():
            using_args['using'] = database

        # Use the list of all apps, unless app labels are specified.
        if app_labels:
            if execute:
                raise CommandError('Cannot specify an application name when '
                                   'executing evolutions.')
            try:
                app_list = [get_app(app_label) for app_label in app_labels]
            except (ImproperlyConfigured, ImportError), e:
                raise CommandError("%s. Are you sure your INSTALLED_APPS "
                                   "setting is correct?" % e)
        else:
            app_list = get_apps()

        # Iterate over all applications running the mutations
        evolution_required = False
        simulated = True
        sql = []
        new_evolutions = []

        current_proj_sig = create_project_sig(database)
        current_signature = pickle.dumps(current_proj_sig)

        try:
            if is_multi_db():
                latest_version = Version.objects.using(database).latest('when')
            else:
                latest_version = Version.objects.latest('when')

            database_sig = pickle.loads(str(latest_version.signature))
            diff = Diff(database_sig, current_proj_sig)
        except Evolution.DoesNotExist:
            raise CommandError("Can't evolve yet. Need to set an "
                               "evolution baseline.")

        try:
            for app in app_list:
                app_label = app.__name__.split('.')[-2]
                if hint:
                    evolutions = []
                    hinted_evolution = diff.evolution()
                    temp_mutations = hinted_evolution.get(app_label, [])
                else:
                    evolutions = get_unapplied_evolutions(app, database)
                    temp_mutations = get_mutations(app, evolutions, database)

                mutations = [
                    mutation for mutation in temp_mutations
                    if mutation.is_mutable(app_label, database_sig,
                                           database)
                ]

                if mutations:
                    app_sql = ['-- Evolve application %s' % app_label]
                    evolution_required = True

                    for mutation in mutations:
                        # Only compile SQL if we want to show it
                        if compile_sql or execute:
                            app_sql.extend(
                                mutation.mutate(app_label, database_sig,
                                                database))

                        # Now run the simulation, which will modify the
                        # signatures
                        try:
                            mutation.simulate(app_label, database_sig, database)
                        except CannotSimulate:
                            simulated = False

                    new_evolutions.extend(
                        Evolution(app_label=app_label, label=label)
                        for label in evolutions)

                    if not execute:
                        if compile_sql:
                            write_sql(app_sql, database)
                        else:
                            print '#----- Evolution for %s' % app_label
                            print 'from django_evolution.mutations import *'
                            print 'from django.db import models'
                            print
                            print 'MUTATIONS = ['
                            print '   ',
                            print ',\n    '.join(unicode(m) for m in mutations)
                            print ']'
                            print '#----------------------'

                    sql.extend(app_sql)
                else:
                    if verbosity > 1:
                        print 'Application %s is up to date' % app_label

            # Process the purged applications if requested to do so.
            if purge:
                if diff.deleted:
                    evolution_required = True
                    delete_app = DeleteApplication()
                    purge_sql = []

                    for app_label in diff.deleted:
                        if delete_app.is_mutable(app_label, database_sig,
                                                 database):
                            if compile_sql or execute:
                                purge_sql.append('-- Purge application %s'
                                                 % app_label)
                                purge_sql.extend(
                                    delete_app.mutate(app_label, database_sig,
                                                      database))
                            delete_app.simulate(app_label, database_sig,
                                                database)

                    if not execute:
                        if compile_sql:
                            write_sql(purge_sql, database)
                        else:
                            print 'The following application(s) can be purged:'

                            for app_label in diff.deleted:
                                print '    ', app_label

                            print

                    sql.extend(purge_sql)
                else:
                    if verbosity > 1:
                        print 'No applications need to be purged.'

        except EvolutionException, e:
            raise CommandError(str(e))

        if simulated:
            diff = Diff(database_sig, current_proj_sig)

            if not diff.is_empty(not purge):
                if hint:
                    print self.style.ERROR(
                        'Your models contain changes that Django Evolution '
                        'cannot resolve automatically.')
                    print 'This is probably due to a currently unimplemented ' \
                          'mutation type.'
                    print 'You will need to manually construct a mutation ' \
                          'to resolve the remaining changes.'
                else:
                    print self.style.ERROR(
                        'The stored evolutions do not completely resolve '
                        'all model changes.')
                    print 'Run `./manage.py evolve --hint` to see a ' \
                          'suggestion for the changes required.'
                print
                print 'The following are the changes that could ' \
                      'not be resolved:'
                print diff

                raise CommandError('Your models contain changes that Django '
                                   'Evolution cannot resolve automatically.')
        else:
            print self.style.NOTICE(
                'Evolution could not be simulated, possibly due to raw '
                'SQL mutations')

        if evolution_required:
            if execute:
                # Now that we've worked out the mutations required,
                # and we know they simulate OK, run the evolutions
                if interactive:
                    confirm = raw_input("""
You have requested a database evolution. This will alter tables
and data currently in the %r database, and may result in
IRREVERSABLE DATA LOSS. Evolutions should be *thoroughly* reviewed
prior to execution.

Are you sure you want to execute the evolutions?

Type 'yes' to continue, or 'no' to cancel: """ % database)
                else:
                    confirm = 'yes'

                if is_multi_db():
                    from django.db import connections

                if confirm.lower() == 'yes':
                    # Begin Transaction
                    transaction.enter_transaction_management(**using_args)
                    transaction.managed(flag=True, **using_args)

                    if is_multi_db():
                        cursor = connections[database].cursor()
                    else:
                        cursor = connection.cursor()

                    try:
                        # Perform the SQL
                        execute_sql(cursor, sql)

                        # Now update the evolution table
                        version = Version(signature=current_signature)
                        version.save(**using_args)

                        for evolution in new_evolutions:
                            evolution.version = version
                            evolution.save(**using_args)

                        transaction.commit(**using_args)
                    except Exception, ex:
                        transaction.rollback(**using_args)
                        raise CommandError('Error applying evolution: %s'
                                           % str(ex))

                    transaction.leave_transaction_management(**using_args)

                    if verbosity > 0:
                        print 'Evolution successful.'
                else:
                    print self.style.ERROR('Evolution cancelled.')
            elif not compile_sql:
                if verbosity > 0:
                    if simulated:
                        print "Trial evolution successful."
                        print "Run './manage.py evolve %s--execute' to apply evolution." % (hint and '--hint ' or '')
        elif verbosity > 0:
            print 'No evolution required.'

########NEW FILE########
__FILENAME__ = models
from datetime import datetime

from django.db import models


class Version(models.Model):
    signature = models.TextField()
    when = models.DateTimeField(default=datetime.now)

    class Meta:
        ordering = ('-when',)
        db_table = 'django_project_version'

    def __unicode__(self):
        if not self.evolutions.count():
            return u'Hinted version, updated on %s' % self.when

        return u'Stored version, updated on %s' % self.when


class Evolution(models.Model):
    version = models.ForeignKey(Version, related_name='evolutions')
    app_label = models.CharField(max_length=200)
    label = models.CharField(max_length=100)

    class Meta:
        db_table = 'django_evolution'

    def __unicode__(self):
        return u"Evolution %s, applied to %s" % (self.label, self.app_label)

########NEW FILE########
__FILENAME__ = mutations
import copy

from django.db.models.fields import *
from django.db.models.fields.related import *
from django.db import models
from django.utils.datastructures import SortedDict
from django.utils.functional import curry

from django_evolution.signature import ATTRIBUTE_DEFAULTS
from django_evolution import CannotSimulate, SimulationFailure, EvolutionNotImplementedError, is_multi_db
from django_evolution.db import EvolutionOperationsMulti

FK_INTEGER_TYPES = [
    'AutoField', 'PositiveIntegerField', 'PositiveSmallIntegerField'
]

if is_multi_db():
    from django.db import router


def create_field(proj_sig, field_name, field_type, field_attrs, parent_model):
    """
    Create an instance of a field from a field signature. This is useful for
    accessing all the database property mechanisms built into fields.
    """
    # related_model isn't a valid field attribute, so it must be removed
    # prior to instantiating the field, but it must be restored
    # to keep the signature consistent.
    related_model = field_attrs.pop('related_model', None)

    if related_model:
        related_app_name, related_model_name = related_model.split('.')
        related_model_sig = proj_sig[related_app_name][related_model_name]
        to = MockModel(proj_sig, related_app_name, related_model_name,
                       related_model_sig, stub=True)

        field = field_type(to, name=field_name, **field_attrs)
        field_attrs['related_model'] = related_model
    else:
        field = field_type(name=field_name, **field_attrs)

    if field_type == ManyToManyField and parent_model is not None:
        # Starting in Django 1.2, a ManyToManyField must have a through
        # model defined. This will be set internally to an auto-created
        # model if one isn't specified. We have to fake that model.
        through_model = field_attrs.get('through_model', None)
        through_model_sig = None

        if through_model:
            through_app_name, through_model_name = through_model.split('.')
            through_model_sig = proj_sig[through_app_name][through_model_name]
        elif hasattr(field, '_get_m2m_attr'):
            # Django >= 1.2
            to = field.rel.to._meta.object_name.lower()

            if (field.rel.to == RECURSIVE_RELATIONSHIP_CONSTANT or
                to == parent_model._meta.object_name.lower()):
                from_ = 'from_%s' % to
                to = 'to_%s' % to
            else:
                from_ = parent_model._meta.object_name.lower()

            # This corresponds to the signature in
            # related.create_many_to_many_intermediary_model
            through_app_name = parent_model.app_name
            through_model_name = '%s_%s' % (parent_model._meta.object_name,
                                            field.name)
            through_model = '%s.%s' % (through_app_name, through_model_name)

            fields = SortedDict()
            fields['id'] = {
                'field_type': AutoField,
                'primary_key': True,
            }

            fields[from_] = {
                'field_type': ForeignKey,
                'related_model': '%s.%s' % (parent_model.app_name,
                                            parent_model._meta.object_name),
                'related_name': '%s+' % through_model_name,
            }

            fields[to] = {
                'field_type': ForeignKey,
                'related_model': related_model,
                'related_name': '%s+' % through_model_name,
            }

            through_model_sig = {
                'meta': {
                    'db_table': field._get_m2m_db_table(parent_model._meta),
                    'managed': True,
                    'auto_created': True,
                    'app_label': through_app_name,
                    'unique_together': ((from_, to),),
                    'pk_column': 'id',
                },
                'fields': fields,
            }

            field.auto_created = True

        if through_model_sig:
            through = MockModel(proj_sig, through_app_name, through_model_name,
                                through_model_sig)
            field.rel.through = through

        field.m2m_db_table = curry(field._get_m2m_db_table, parent_model._meta)
        field.set_attributes_from_rel()

    field.set_attributes_from_name(field_name)

    return field


class MockMeta(object):
    """
    A mockup of a models Options object, based on the model signature.

    The stub argument is used to circumvent recursive relationships. If
    'stub' is provided, the constructed model will only be a stub -
    it will only have a primary key field.
    """
    def __init__(self, proj_sig, app_name, model_name, model_sig):
        self.object_name = model_name
        self.app_label = app_name
        self.meta = {
            'order_with_respect_to': None,
            'has_auto_field': None,
            'db_tablespace': None,
        }
        self.meta.update(model_sig['meta'])
        self._fields = SortedDict()
        self._many_to_many = SortedDict()
        self.abstract = False
        self.managed = True
        self.proxy = False
        self._model_sig = model_sig
        self._proj_sig = proj_sig

    def setup_fields(self, model, stub=False):
        for field_name, field_sig in self._model_sig['fields'].items():
            if not stub or field_sig.get('primary_key', False):
                field_type = field_sig.pop('field_type')
                field = create_field(self._proj_sig, field_name, field_type,
                                     field_sig, model)

                if AutoField == type(field):
                    self.meta['has_auto_field'] = True
                    self.meta['auto_field'] = field

                field_sig['field_type'] = field_type

                if ManyToManyField == type(field):
                    self._many_to_many[field.name] = field
                else:
                    self._fields[field.name] = field

                field.set_attributes_from_name(field_name)
                if field_sig.get('primary_key', False):
                    self.pk = field

    def __getattr__(self, name):
        return self.meta[name]

    def get_field(self, name):
        try:
            return self._fields[name]
        except KeyError:
            try:
                return self._many_to_many[name]
            except KeyError:
                raise FieldDoesNotExist('%s has no field named %r' %
                                        (self.object_name, name))

    def get_field_by_name(self, name):
        return (self.get_field(name), None, True, None)

    def get_fields(self):
        return self._fields.values()

    def get_many_to_many_fields(self):
        return self._many_to_many.values()

    fields = property(fget=get_fields)
    local_fields = property(fget=get_fields)
    local_many_to_many = property(fget=get_many_to_many_fields)


class MockModel(object):
    """
    A mockup of a model object, providing sufficient detail
    to derive database column and table names using the standard
    Django fields.
    """
    def __init__(self, proj_sig, app_name, model_name, model_sig, stub=False):
        self.app_name = app_name
        self.model_name = model_name
        self._meta = MockMeta(proj_sig, app_name, model_name, model_sig)
        self._meta.setup_fields(self, stub)

    def __eq__(self, other):
        # For our purposes, we don't want to appear equal to "self".
        # Really, Django 1.2 should be checking if this is a string before
        # doing this comparison,
        return (isinstance(other, MockModel) and
                self.app_name == other.app_name and
                self.model_name == other.model_name)


class MockRelated(object):
    """
    A mockup of django.db.models.related.RelatedObject, providing
    sufficient detail to derive database column and table names using
    the standard Django fields.
    """
    def __init__(self, related_model, model, field):
        self.parent_model = related_model
        self.model = model
        self.opts = model._meta
        self.field = field
        self.name = '%s:%s' % (model.app_name, model.model_name)
        self.var_name = model.model_name.lower()


class BaseMutation:
    def __init__(self):
        pass

    def mutate(self, app_label, proj_sig, target_database = None):
        """
        Performs the mutation on the database. Database changes will occur
        after this function is invoked.
        """
        raise NotImplementedError()

    def simulate(self, app_label, proj_sig, target_database = None):
        """
        Performs a simulation of the mutation to be performed. The purpose of
        the simulate function is to ensure that after all mutations have occured
        the database will emerge in a state consistent with the currently loaded
        models file.
        """
        raise NotImplementedError()

    def is_mutable(self, app_label, proj_sig, database):
        """
        test if the current mutation could be applied to the given database
        """
        return False


class MonoBaseMutation(BaseMutation):
    # introducting model_name at this stage will prevent subclasses to be
    # cross databases
    def __init__(self, model_name = None):
        BaseMutation.__init__(self)
        self.model_name = model_name

    def evolver(self, model):
        db_name = None

        if is_multi_db():
            db_name = router.db_for_write(model)

        return EvolutionOperationsMulti(db_name).get_evolver()

    def is_mutable(self, app_label, proj_sig, database):
        if is_multi_db():
            app_sig = proj_sig[app_label]
            model_sig = app_sig[self.model_name]
            model = MockModel(proj_sig, app_label, self.model_name, model_sig)
            db_name = router.db_for_write(model)
            return db_name and db_name == database
        else:
            return True


class SQLMutation(BaseMutation):
    def __init__(self, tag, sql, update_func=None):
        self.tag = tag
        self.sql = sql
        self.update_func = update_func

    def __str__(self):
        return "SQLMutation('%s')" % self.tag

    def simulate(self, app_label, proj_sig, database=None):
        """SQL mutations cannot be simulated unless an update function is
        provided"""

        if callable(self.update_func):
            self.update_func(app_label, proj_sig)
        else:
            raise CannotSimulate('Cannot simulate SQLMutations')

    def mutate(self, app_label, proj_sig, database=None):
        "The mutation of an SQL mutation returns the raw SQL"
        return self.sql

    def is_mutable(self, app_label, proj_sig, database):
        return True


class DeleteField(MonoBaseMutation):
    def __init__(self, model_name, field_name):
        MonoBaseMutation.__init__(self, model_name)
        self.field_name = field_name

    def __str__(self):
        return "DeleteField('%s', '%s')" % (self.model_name, self.field_name)

    def simulate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]

        # If the field was used in the unique_together attribute, update it.
        unique_together = model_sig['meta']['unique_together']
        unique_together_list = []

        for ut_index in range(0, len(unique_together), 1):
            ut = unique_together[ut_index]
            unique_together_fields = []

            for field_name_index in range(0, len(ut), 1):
                field_name = ut[field_name_index]

                if not field_name == self.field_name:
                    unique_together_fields.append(field_name)

            unique_together_list.append(tuple(unique_together_fields))

        model_sig['meta']['unique_together'] = tuple(unique_together_list)

        if model_sig['fields'][self.field_name].get('primary_key',False):
            raise SimulationFailure('Cannot delete a primary key.')

        # Simulate the deletion of the field.
        try:
            model_sig['fields'].pop(self.field_name)
        except KeyError:
            raise SimulationFailure('Cannot find the field named "%s".'
                                    % self.field_name)

    def mutate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]
        field_sig = model_sig['fields'][self.field_name]

        model = MockModel(proj_sig, app_label, self.model_name, model_sig)

        # Temporarily remove field_type from the field signature
        # so that we can create a field
        field_type = field_sig.pop('field_type')
        field = create_field(proj_sig, self.field_name, field_type, field_sig,
                             model)
        field_sig['field_type'] = field_type

        evolver = self.evolver(model)

        if field_type == models.ManyToManyField:
            sql_statements = \
                evolver.delete_table(field._get_m2m_db_table(model._meta))
        else:
            sql_statements = evolver.delete_column(model, field)

        return sql_statements


class AddField(MonoBaseMutation):
    def __init__(self, model_name, field_name, field_type,
                 initial=None, **kwargs):
        MonoBaseMutation.__init__(self, model_name)
        self.field_name = field_name
        self.field_type = field_type
        self.field_attrs = kwargs
        self.initial = initial

    def __str__(self):
        params = (self.model_name, self.field_name, self.field_type.__name__)
        str_output = ["'%s', '%s', models.%s" % params]

        if self.initial is not None:
            str_output.append('initial=%s' % repr(self.initial))

        for key,value in self.field_attrs.items():
            str_output.append("%s=%s" % (key,repr(value)))

        return 'AddField(' + ', '.join(str_output) + ')'

    def simulate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]

        if self.field_name in model_sig['fields']:
            raise SimulationFailure(
                "Model '%s.%s' already has a field named '%s'"
                % (app_label, self.model_name, self.field_name))

        if (self.field_type != models.ManyToManyField and
            not self.field_attrs.get('null', ATTRIBUTE_DEFAULTS['null'])
            and self.initial is None):
            raise SimulationFailure(
                "Cannot create new column '%s' on '%s.%s' without a "
                "non-null initial value."
                % (self.field_name, app_label, self.model_name))

        model_sig['fields'][self.field_name] = {
            'field_type': self.field_type,
        }

        model_sig['fields'][self.field_name].update(self.field_attrs)

    def mutate(self, app_label, proj_sig, database=None):
        if self.field_type == models.ManyToManyField:
            return self.add_m2m_table(app_label, proj_sig)
        else:
            return self.add_column(app_label, proj_sig)

    def add_column(self, app_label, proj_sig):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]

        model = MockModel(proj_sig, app_label, self.model_name, model_sig)
        field = create_field(proj_sig, self.field_name, self.field_type,
                             self.field_attrs, model)

        evolver = self.evolver(model)

        sql_statements = evolver.add_column(model, field, self.initial)

        # Create SQL index if necessary
        sql_statements.extend(evolver.create_index(model, field))

        return sql_statements

    def add_m2m_table(self, app_label, proj_sig):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]

        model = MockModel(proj_sig, app_label, self.model_name, model_sig)

        field = create_field(proj_sig, self.field_name, self.field_type,
                             self.field_attrs, model)

        related_app_label, related_model_name = \
            self.field_attrs['related_model'].split('.')
        related_sig = proj_sig[related_app_label][related_model_name]
        related_model = MockModel(proj_sig, related_app_label,
                                  related_model_name, related_sig)
        related = MockRelated(related_model, model, field)

        if hasattr(field, '_get_m2m_column_name'):
            # Django < 1.2
            field.m2m_column_name = curry(field._get_m2m_column_name, related)
            field.m2m_reverse_name = curry(field._get_m2m_reverse_name, related)
        else:
            # Django >= 1.2
            field.m2m_column_name = curry(field._get_m2m_attr,
                                          related, 'column')
            field.m2m_reverse_name = curry(field._get_m2m_reverse_attr,
                                           related, 'column')

        sql_statements = self.evolver(model).add_m2m_table(model, field)

        return sql_statements


class RenameField(MonoBaseMutation):
    def __init__(self, model_name, old_field_name, new_field_name,
                 db_column=None, db_table=None):
        MonoBaseMutation.__init__(self, model_name)
        self.old_field_name = old_field_name
        self.new_field_name = new_field_name
        self.db_column = db_column
        self.db_table = db_table

    def __str__(self):
        params = "'%s', '%s', '%s'" % (self.model_name, self.old_field_name,
                                       self.new_field_name)

        if self.db_column:
            params = params + ", db_column='%s'" % (self.db_column)
        if self.db_table:
            params = params + ", db_table='%s'" % (self.db_table)

        return "RenameField(%s)" % params

    def simulate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]
        field_dict = model_sig['fields']
        field_sig = field_dict[self.old_field_name]

        if models.ManyToManyField == field_sig['field_type']:
            if self.db_table:
                field_sig['db_table'] = self.db_table
            else:
                field_sig.pop('db_table',None)
        elif self.db_column:
            field_sig['db_column'] = self.db_column
        else:
            # db_column and db_table were not specified (or not specified for
            # the appropriate field types). Clear the old value if one was set.
            # This amounts to resetting the column or table name to the Django
            # default name
            field_sig.pop('db_column', None)

        field_dict[self.new_field_name] = field_dict.pop(self.old_field_name)

    def mutate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]
        old_field_sig = model_sig['fields'][self.old_field_name]

        # Temporarily remove the field type so that we can create mock field
        # instances
        field_type = old_field_sig.pop('field_type')

        # Duplicate the old field sig, and apply the table/column changes
        new_field_sig = copy.copy(old_field_sig)

        if models.ManyToManyField == field_type:
            if self.db_table:
                new_field_sig['db_table'] = self.db_table
            else:
                new_field_sig.pop('db_table', None)
        elif self.db_column:
            new_field_sig['db_column'] = self.db_column
        else:
            new_field_sig.pop('db_column', None)

        # Create the mock field instances.
        old_field = create_field(proj_sig, self.old_field_name, field_type,
                                 old_field_sig, None)
        new_field = create_field(proj_sig, self.new_field_name, field_type,
                                 new_field_sig, None)

        # Restore the field type to the signature
        old_field_sig['field_type'] = field_type

        model = MockModel(proj_sig, app_label, self.model_name, model_sig)

        if models.ManyToManyField == field_type:
            old_m2m_table = old_field._get_m2m_db_table(model._meta)
            new_m2m_table = new_field._get_m2m_db_table(model._meta)

            return self.evolver(model).rename_table(model, old_m2m_table,
                                                    new_m2m_table)
        else:
            return self.evolver(model).rename_column(model._meta, old_field,
                                                     new_field)


class ChangeField(MonoBaseMutation):
    def __init__(self, model_name, field_name, initial=None, **kwargs):
        MonoBaseMutation.__init__(self, model_name)
        self.field_name = field_name
        self.field_attrs = kwargs
        self.initial = initial

    def __str__(self):
        params = (self.model_name, self.field_name)
        str_output = ["'%s', '%s'" % params]

        str_output.append('initial=%s' % repr(self.initial))

        for attr_name, attr_value in self.field_attrs.items():
            if str == type(attr_value):
                str_attr_value = "'%s'" % attr_value
            else:
                str_attr_value = str(attr_value)

            str_output.append('%s=%s' % (attr_name, str_attr_value,))

        return 'ChangeField(' + ', '.join(str_output) + ')'

    def simulate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]
        field_sig = model_sig['fields'][self.field_name]

        # Catch for no-op changes.
        for field_attr, attr_value in self.field_attrs.items():
            field_sig[field_attr] = attr_value

        if ('null' in self.field_attrs and
            field_sig['field_type'] != models.ManyToManyField and
            not self.field_attrs['null'] and
            self.initial is None):
            raise SimulationFailure(
                "Cannot change column '%s' on '%s.%s' without a "
                "non-null initial value."
                % (self.field_name, app_label, self.model_name))

    def mutate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]
        old_field_sig = model_sig['fields'][self.field_name]
        model = MockModel(proj_sig, app_label, self.model_name, model_sig)

        sql_statements = []

        for field_attr, attr_value in self.field_attrs.items():
            old_field_attr = old_field_sig.get(field_attr,
                                               ATTRIBUTE_DEFAULTS[field_attr])

            # Avoid useless SQL commands if nothing has changed.
            if not old_field_attr == attr_value:
                try:
                    evolver_func = getattr(self.evolver(model),
                                           'change_%s' % field_attr)
                    if field_attr == 'null':
                        sql_statements.extend(
                            evolver_func(model, self.field_name, attr_value,
                            self.initial))
                    elif field_attr == 'db_table':
                        sql_statements.extend(
                            evolver_func(model, old_field_attr, attr_value))
                    else:
                        sql_statements.extend(
                            evolver_func(model, self.field_name, attr_value))
                except AttributeError:
                    raise EvolutionNotImplementedError(
                        "ChangeField does not support modifying the '%s' "
                        "attribute on '%s.%s'."
                        % (field_attr, self.model_name, self.field_name))

        return sql_statements


class DeleteModel(MonoBaseMutation):
    def __init__(self, model_name):
        MonoBaseMutation.__init__(self, model_name)

    def __str__(self):
        return "DeleteModel(%r)" % self.model_name

    def simulate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]

        # Simulate the deletion of the model.
        del app_sig[self.model_name]

    def mutate(self, app_label, proj_sig, database=None):
        app_sig = proj_sig[app_label]
        model_sig = app_sig[self.model_name]

        sql_statements = []
        model = MockModel(proj_sig, app_label, self.model_name, model_sig)

        # Remove any many to many tables.
        for field_name, field_sig in model_sig['fields'].items():
            if field_sig['field_type'] == models.ManyToManyField:
                field = model._meta.get_field(field_name)
                m2m_table = field._get_m2m_db_table(model._meta)
                sql_statements += self.evolver(model).delete_table(m2m_table)

        # Remove the table itself.
        sql_statements += self.evolver(model).delete_table(model._meta.db_table)

        return sql_statements


class DeleteApplication(BaseMutation):
    def __str__(self):
        return 'DeleteApplication()'

    def simulate(self, app_label, proj_sig, database=None):
        if database:
            app_sig = proj_sig[app_label]

            # Simulate the deletion of the models.
            for model_name in app_sig.keys():
                mutation = DeleteModel(model_name)

                if mutation.is_mutable(app_label, proj_sig, database):
                    del app_sig[self.model_name]

    def mutate(self, app_label, proj_sig, database=None):
        sql_statements = []

        # This test will introduce a regression, but we can't afford to remove
        # all models at a same time if they aren't owned by the same database
        if database:
            app_sig = proj_sig[app_label]

            for model_name in app_sig.keys():
                mutation = DeleteModel(model_name)

                if mutation.is_mutable(app_label, proj_sig, database):
                    sql_statements.extend(mutation.mutate(app_label, proj_sig))

        return sql_statements

    def is_mutable(self, app_label, proj_sig, database):
        # the test is done in the mutate method above. We can return True
        return True

########NEW FILE########
__FILENAME__ = signature
from django.db.models import get_apps, get_models
from django.db.models.fields.related import *
from django.conf import global_settings
from django.contrib.contenttypes import generic
from django.utils.datastructures import SortedDict
from django_evolution import is_multi_db

if is_multi_db():
    from django.db import router


ATTRIBUTE_DEFAULTS = {
    # Common to all fields
    'primary_key': False,
    'max_length' : None,
    'unique' : False,
    'null' : False,
    'db_index' : False,
    'db_column' : None,
    'db_tablespace' : global_settings.DEFAULT_TABLESPACE,
    'rel': None,
    # Decimal Field
    'max_digits' : None,
    'decimal_places' : None,
    # ManyToManyField
    'db_table': None
}

# r7790 modified the unique attribute of the meta model to be
# a property that combined an underlying _unique attribute with
# the primary key attribute. We need the underlying property,
# but we don't want to affect old signatures (plus the
# underscore is ugly :-).
ATTRIBUTE_ALIASES = {
    'unique': '_unique'
}

def create_field_sig(field):
    field_sig = {
        'field_type': field.__class__,
    }

    for attrib in ATTRIBUTE_DEFAULTS.keys():
        alias = ATTRIBUTE_ALIASES.get(attrib, attrib)
        if hasattr(field,alias):
            value = getattr(field,alias)
            if isinstance(field, ForeignKey):
                if attrib == 'db_index':
                    default = True
                else:
                    default = ATTRIBUTE_DEFAULTS[attrib]
            else:
                default = ATTRIBUTE_DEFAULTS[attrib]
            # only store non-default values
            if default != value:
                field_sig[attrib] = value

    rel = field_sig.pop('rel', None)

    if rel:
        field_sig['related_model'] = '.'.join([rel.to._meta.app_label,
                                               rel.to._meta.object_name])

    return field_sig

def create_model_sig(model):
    model_sig = {
        'meta': {
            'unique_together': model._meta.unique_together,
            'db_tablespace': model._meta.db_tablespace,
            'db_table': model._meta.db_table,
            'pk_column': model._meta.pk.column,
        },
        'fields': {},
    }

    for field in model._meta.local_fields + model._meta.local_many_to_many:
        # Special case - don't generate a signature for generic relations
        if not isinstance(field, generic.GenericRelation):
            model_sig['fields'][field.name] = create_field_sig(field)

    return model_sig

def create_app_sig(app, database):
    """
    Creates a dictionary representation of the models in a given app.
    Only those attributes that are interesting from a schema-evolution
    perspective are included.
    """
    app_sig = SortedDict()

    for model in get_models(app):
        # only include those who want to be syncdb
        if not is_multi_db() or router.allow_syncdb(database, model):
            app_sig[model._meta.object_name] = create_model_sig(model)

    return app_sig

def create_project_sig(database):
    """
    Create a dictionary representation of the apps in a given project.
    """
    proj_sig = {
        '__version__': 1,
    }

    for app in get_apps():
        proj_sig[app.__name__.split('.')[-2]] = create_app_sig(app, database)

    return proj_sig

########NEW FILE########
__FILENAME__ = add_field
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
# The AddField tests will aim to test the following usecases:
# Field resulting in a new database column.
# Field resulting in a new database column with a non-default name.
# Field resulting in a new database column in a table with a non-default name.
# Primary key field.
# Indexed field
# Unique field.
# Null field
#
# Foreign Key field.
# M2M field between models with default table names.
# M2M field between models with non-default table names.
# M2M field between self
>>> from datetime import datetime

>>> from django.db import models

>>> from django_evolution.mutations import AddField, DeleteField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django_evolution import signature
>>> from django_evolution import models as test_app

>>> import copy

>>> class AddSequenceFieldInitial(object):
...     def __init__(self, suffix):
...         self.suffix = suffix
...
...     def __call__(self):
...         from django.db import connection
...         qn = connection.ops.quote_name
...         return qn('int_field')

>>> class AddAnchor1(models.Model):
...     value = models.IntegerField()

>>> class AddAnchor2(models.Model):
...     value = models.IntegerField()
...     class Meta:
...         db_table = 'custom_add_anchor_table'

>>> class AddBaseModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()

>>> class CustomTableModel(models.Model):
...     value = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'custom_table_name'

# Store the base signatures
>>> anchors = (
...     ('AddAnchor1', AddAnchor1),
...     ('AddAnchor2', AddAnchor2)
... )

>>> custom_model = ('CustomTableModel', CustomTableModel)
>>> custom = register_models(custom_model)
>>> custom_table_sig = test_proj_sig(custom_model)

>>> test_model = ('TestModel', AddBaseModel)
>>> start = register_models(*anchors)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, *anchors)

# Add non-null field with non-callable initial value
>>> class AddNonNullColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField()

>>> end = register_models(('TestModel', AddNonNullColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddNonNullColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] #AddNonNullColumnModel
["AddField('TestModel', 'added_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>)"]

# Evolution won't run as-is
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
EvolutionException: Cannot use hinted evolution: AddField or ChangeField mutation for 'TestModel.added_field' in 'tests' requires user-specified initial value.

# First try without an initial value. This will fail
>>> evolution = [AddField('TestModel', 'added_field', models.IntegerField)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot create new column 'added_field' on 'tests.TestModel' without a non-null initial value.

# Now try with an explicitly null initial value. This will also fail
>>> evolution = [AddField('TestModel', 'added_field', models.IntegerField, initial=None)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot create new column 'added_field' on 'tests.TestModel' without a non-null initial value.

# Now try with a good initial value
>>> evolution = [AddField('TestModel', 'added_field', models.IntegerField, initial=1)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddNonNullNonCallableColumnModel
%(AddNonNullNonCallableColumnModel)s

# Now try with a good callable initial value
>>> evolution = [AddField('TestModel', 'added_field', models.IntegerField, initial=AddSequenceFieldInitial('AddNonNullCallableColumnModel'))]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddNonNullCallableColumnModel
%(AddNonNullCallableColumnModel)s

# Add nullable column with initial data
>>> class AddNullColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(null=True)

>>> end = register_models(('TestModel',AddNullColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddNullColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] #AddNullColumnModel
["AddField('TestModel', 'added_field', models.IntegerField, null=True)"]

>>> evolution = [AddField('TestModel', 'added_field', models.IntegerField, initial=1, null=True)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddNullColumnWithInitialColumnModel
%(AddNullColumnWithInitialColumnModel)s

# Add a field that requires string-form initial data
>>> class AddStringColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.CharField(max_length=10)

>>> end = register_models(('TestModel',AddStringColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddStringColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] # AddStringColumnModel
["AddField('TestModel', 'added_field', models.CharField, initial=<<USER VALUE REQUIRED>>, max_length=10)"]

>>> evolution = [AddField('TestModel', 'added_field', models.CharField, initial="abc's xyz", max_length=10)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddStringColumnModel
%(AddStringColumnModel)s

# Add a string field that allows empty strings as initial values
>>> class AddBlankStringColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.CharField(max_length=10, blank=True)

>>> end = register_models(('TestModel',AddBlankStringColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddBlankStringColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] # AddBlankStringColumnModel
["AddField('TestModel', 'added_field', models.CharField, initial='', max_length=10)"]

# Add a field that requires date-form initial data
>>> class AddDateColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.DateTimeField()

>>> end = register_models(('TestModel',AddDateColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddDateColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] # AddDateColumnModel
["AddField('TestModel', 'added_field', models.DateTimeField, initial=<<USER VALUE REQUIRED>>)"]

>>> new_date = datetime(2007,12,13,16,42,0)
>>> evolution = [AddField('TestModel', 'added_field', models.DateTimeField, initial=new_date)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddDateColumnModel
%(AddDateColumnModel)s

# Add column with default value
>>> class AddDefaultColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(default=42)

>>> end = register_models(('TestModel',AddDefaultColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddDefaultColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] #AddDefaultColumnModel
["AddField('TestModel', 'added_field', models.IntegerField, initial=42)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddDefaultColumnModel
%(AddDefaultColumnModel)s

# Add column with an empty string as the default value
>>> class AddEmptyStringDefaultColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.CharField(max_length=20, default='')

>>> end = register_models(('TestModel',AddEmptyStringDefaultColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddEmptyStringDefaultColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] #AddEmptyStringDefaultColumnModel
["AddField('TestModel', 'added_field', models.CharField, initial=u'', max_length=20)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddEmptyStringDefaultColumnModel
%(AddEmptyStringDefaultColumnModel)s


# Null field
>>> class AddNullColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(null=True)

>>> end = register_models(('TestModel', AddNullColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', AddNullColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] #AddNullColumnModel
["AddField('TestModel', 'added_field', models.IntegerField, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddNullColumnModel
%(AddNullColumnModel)s

# Field resulting in a new database column with a non-default name.
>>> class NonDefaultColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     add_field = models.IntegerField(db_column='non-default_column', null=True)

>>> end = register_models(('TestModel',NonDefaultColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',NonDefaultColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'add_field', models.IntegerField, null=True, db_column='non-default_column')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #NonDefaultColumnModel
%(NonDefaultColumnModel)s

# Field resulting in a new database column in a table with a non-default name.
>>> class AddColumnCustomTableModel(models.Model):
...     value = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     added_field = models.IntegerField(null=True)
...     class Meta:
...         db_table = 'custom_table_name'

>>> end = register_models(('CustomTableModel',AddColumnCustomTableModel))
>>> end_sig = test_proj_sig(('CustomTableModel',AddColumnCustomTableModel))
>>> d = Diff(custom_table_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('CustomTableModel', 'added_field', models.IntegerField, null=True)"]

>>> test_sig = copy.deepcopy(custom_table_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(custom, end, test_sql) #AddColumnCustomTableModel
%(AddColumnCustomTableModel)s

# Add Primary key field.
# Delete of old Primary Key is prohibited.
>>> class AddPrimaryKeyModel(models.Model):
...     my_primary_key = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()

>>> end = register_models(('TestModel', AddPrimaryKeyModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddPrimaryKeyModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'my_primary_key', models.AutoField, initial=<<USER VALUE REQUIRED>>, primary_key=True)", "DeleteField('TestModel', 'id')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []

>>> for mutation in [AddField('TestModel', 'my_primary_key', models.AutoField, initial=AddSequenceFieldInitial('AddPrimaryKeyModel'), primary_key=True), DeleteField('TestModel', 'id')]:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot delete a primary key.

# Indexed field
>>> class AddIndexedColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     add_field = models.IntegerField(db_index=True, null=True)

>>> end = register_models(('TestModel',AddIndexedColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddIndexedColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'add_field', models.IntegerField, null=True, db_index=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, debug=False) #AddIndexedColumnModel
%(AddIndexedColumnModel)s

# Unique field.
>>> class AddUniqueColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(unique=True, null=True)

>>> end = register_models(('TestModel',AddUniqueColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddUniqueColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.IntegerField, unique=True, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddUniqueColumnModel
%(AddUniqueColumnModel)s

# Unique indexed field.
>>> class AddUniqueIndexedModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(unique=True, db_index=True, null=True)

>>> end = register_models(('TestModel',AddUniqueIndexedModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddUniqueIndexedModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.IntegerField, unique=True, null=True, db_index=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddUniqueIndexedModel
%(AddUniqueIndexedModel)s

Foreign Key field.
>>> class AddForeignKeyModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.ForeignKey(AddAnchor1, null=True)

>>> end = register_models(('TestModel',AddForeignKeyModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddForeignKeyModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.ForeignKey, null=True, related_model='tests.AddAnchor1')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddForeignKeyModel
%(AddForeignKeyModel)s

# M2M field between models with default table names.
>>> class AddM2MDatabaseTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.ManyToManyField(AddAnchor1)

>>> end = register_models(('TestModel',AddM2MDatabaseTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',AddM2MDatabaseTableModel), *anchors)
>>> end_sig['tests'][AddAnchor1.__name__] = signature.create_model_sig(AddAnchor1)
>>> anchor_sig = copy.deepcopy(start_sig)
>>> anchor_sig['tests'][AddAnchor1.__name__] = signature.create_model_sig(AddAnchor1)
>>> d = Diff(anchor_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.ManyToManyField, related_model='tests.AddAnchor1')"]

>>> test_sig = copy.deepcopy(anchor_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddManyToManyDatabaseTableModel
%(AddManyToManyDatabaseTableModel)s

# M2M field between models with non-default table names.
>>> class AddM2MNonDefaultDatabaseTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.ManyToManyField(AddAnchor2)

>>> end = register_models(('TestModel', AddM2MNonDefaultDatabaseTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', AddM2MNonDefaultDatabaseTableModel), *anchors)
>>> end_sig['tests'][AddAnchor2.__name__] = signature.create_model_sig(AddAnchor2)
>>> anchor_sig = copy.deepcopy(start_sig)
>>> anchor_sig['tests'][AddAnchor2.__name__] = signature.create_model_sig(AddAnchor2)
>>> d = Diff(anchor_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.ManyToManyField, related_model='tests.AddAnchor2')"]

>>> test_sig = copy.deepcopy(anchor_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddManyToManyNonDefaultDatabaseTableModel
%(AddManyToManyNonDefaultDatabaseTableModel)s

# M2M field between self
# Need to find a better way to do this.
>>> class AddM2MSelfDatabaseTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.ManyToManyField('AddM2MSelfDatabaseTableModel')

>>> end = register_models(('TestModel', AddM2MSelfDatabaseTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', AddM2MSelfDatabaseTableModel), *anchors)

>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'added_field', models.ManyToManyField, related_model='tests.TestModel')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #AddManyToManySelf
%(AddManyToManySelf)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('add_field')

########NEW FILE########
__FILENAME__ = change_field
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models

>>> from django_evolution.mutations import ChangeField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff

>>> import copy

# Use Cases:
# Setting a null constraint
# -- without an initial value
# -- with a null initial value
# -- with a good initial value (constant)
# -- with a good initial value (callable)
# Removing a null constraint
# Invoking a no-op change field
# Changing the max_length of a character field
# -- increasing the max_length
# -- decreasing the max_length
# Renaming a column
# Changing the db_table of a many to many relationship
# Adding an index
# Removing an index
# Adding a unique constraint
# Removing a unique constraint
# Redundant attributes. (Some attribute have changed, while others haven't but are specified anyway.)
# Changing more than one attribute at a time (on different fields)
# Changing more than one attribute at a time (on one field)


### This one is a bit dubious because changing the primary key of a model will mean
### that all referenced foreign keys and M2M relationships need to be updated
# Adding a primary key constraint
# Removing a Primary Key (Changing the primary key column)



# Options that apply to all fields:
# DB related options
# null
# db_column
# db_index
# db_tablespace (Ignored)
# primary_key
# unique
# db_table (only for many to many relationships)
# -- CharField
# max_length

# Non-DB options
# blank
# core
# default
# editable
# help_text
# radio_admin
# unique_for_date
# unique_for_month
# unique_for_year
# validator_list

# I don't know yet
# choices

>>> class ChangeSequenceFieldInitial(object):
...     def __init__(self, suffix):
...         self.suffix = suffix
...
...     def __call__(self):
...         from django.db import connection
...         qn = connection.ops.quote_name
...         return qn('char_field')

# Now, a useful test model we can use for evaluating diffs
>>> class ChangeAnchor1(models.Model):
...     value = models.IntegerField()

>>> class ChangeBaseModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

# Store the base signatures
>>> anchors = [('ChangeAnchor1', ChangeAnchor1)]
>>> test_model = ('TestModel', ChangeBaseModel)

>>> start = register_models(*anchors)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, *anchors)

# Setting a null constraint without an initial value
>>> class SetNotNullChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=False)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', SetNotNullChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', SetNotNullChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field1':
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # SetNotNullChangeModel
["ChangeField('TestModel', 'char_field1', initial=<<USER VALUE REQUIRED>>, null=False)"]

# Without an initial value
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot change column 'char_field1' on 'tests.TestModel' without a non-null initial value.

# With a null initial value
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial=None)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot change column 'char_field1' on 'tests.TestModel' without a non-null initial value.

# With a good initial value (constant)
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial="abc's xyz")]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # SetNotNullChangedModelWithConstant
%(SetNotNullChangeModelWithConstant)s

# With a good initial value (callable)
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial=ChangeSequenceFieldInitial('SetNotNullChangeModel'))]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql)
%(SetNotNullChangeModelWithCallable)s

# Removing a null constraint
>>> class SetNullChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', SetNullChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', SetNullChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # SetNullChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # SetNullChangeModel
%(SetNullChangeModel)s

# Removing a null constraint
>>> class NoOpChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', NoOpChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', NoOpChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
<BLANKLINE>

>>> evolution = [ChangeField('TestModel', 'char_field1', null=True)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # NoOpChangeModel
%(NoOpChangeModel)s

# Increasing the max_length of a character field
>>> class IncreasingMaxLengthChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=45)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', IncreasingMaxLengthChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', IncreasingMaxLengthChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # IncreasingMaxLengthChangeModel
["ChangeField('TestModel', 'char_field', initial=None, max_length=45)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # IncreasingMaxLengthChangeModel
%(IncreasingMaxLengthChangeModel)s

# Decreasing the max_length of a character field
>>> class DecreasingMaxLengthChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=1)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', DecreasingMaxLengthChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', DecreasingMaxLengthChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # DecreasingMaxLengthChangeModel
["ChangeField('TestModel', 'char_field', initial=None, max_length=1)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # DecreasingMaxLengthChangeModel
%(DecreasingMaxLengthChangeModel)s

# Renaming a column
>>> class DBColumnChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='customised_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', DBColumnChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', DBColumnChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field':
        Property 'db_column' has changed

>>> print [str(e) for e in d.evolution()['tests']] # DBColumnChangeModel
["ChangeField('TestModel', 'int_field', initial=None, db_column='customised_db_column')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # DBColumnChangeModel
%(DBColumnChangeModel)s

# Changing the db_table of a many to many relationship
>>> class M2MDBTableChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='custom_m2m_db_table_name')

>>> end = register_models(('TestModel', M2MDBTableChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', M2MDBTableChangeModel), *anchors)

>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'm2m_field1':
        Property 'db_table' has changed

>>> print [str(e) for e in d.evolution()['tests']] # M2MDBTableChangeModel
["ChangeField('TestModel', 'm2m_field1', initial=None, db_table='custom_m2m_db_table_name')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # M2MDBTableChangeModel
%(M2MDBTableChangeModel)s

# Adding an index
>>> class AddDBIndexChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=True)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', AddDBIndexChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', AddDBIndexChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field2':
        Property 'db_index' has changed

>>> print [str(e) for e in d.evolution()['tests']] # AddDBIndexChangeModel
["ChangeField('TestModel', 'int_field2', initial=None, db_index=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # AddDBIndexChangeModel
%(AddDBIndexChangeModel)s

# Removing an index
>>> class RemoveDBIndexChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=False)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', RemoveDBIndexChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', RemoveDBIndexChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field1':
        Property 'db_index' has changed

>>> print [str(e) for e in d.evolution()['tests']] # RemoveDBIndexChangeModel
["ChangeField('TestModel', 'int_field1', initial=None, db_index=False)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # RemoveDBIndexChangeModel
%(RemoveDBIndexChangeModel)s

# Adding a unique constraint
>>> class AddUniqueChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=True)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', AddUniqueChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', AddUniqueChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field4':
        Property 'unique' has changed

>>> print [str(e) for e in d.evolution()['tests']] # AddUniqueChangeModel
["ChangeField('TestModel', 'int_field4', initial=None, unique=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # AddUniqueChangeModel
%(AddUniqueChangeModel)s

# Remove a unique constraint
>>> class RemoveUniqueChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=False)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', RemoveUniqueChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', RemoveUniqueChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field3':
        Property 'unique' has changed

>>> print [str(e) for e in d.evolution()['tests']] # RemoveUniqueChangeModel
["ChangeField('TestModel', 'int_field3', initial=None, unique=False)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # RemoveUniqueChangeModel
%(RemoveUniqueChangeModel)s

# Changing more than one attribute at a time (on different fields)
>>> class MultiAttrChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column2')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=35)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', MultiAttrChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', MultiAttrChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'null' has changed
    In field 'int_field':
        Property 'db_column' has changed
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # MultiAttrChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, null=True)", "ChangeField('TestModel', 'int_field', initial=None, db_column='custom_db_column2')", "ChangeField('TestModel', 'char_field', initial=None, max_length=35)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # MultiAttrChangeModel
%(MultiAttrChangeModel)s

# Changing more than one attribute at a time (on one fields)
>>> class MultiAttrSingleFieldChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=35, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', MultiAttrSingleFieldChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', MultiAttrSingleFieldChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'max_length' has changed
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # MultiAttrSingleFieldChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, max_length=35, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # MultiAttrSingleFieldChangeModel
%(MultiAttrSingleFieldChangeModel)s

# Redundant attributes. (Some attribute have changed, while others haven't but are specified anyway.)
>>> class RedundantAttrsChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column3')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=35)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', RedundantAttrsChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', RedundantAttrsChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> evolutions = [
...     ChangeField("TestModel", "char_field2", initial=None, null=True, max_length=30),
...     ChangeField("TestModel", "int_field", initial=None, db_column="custom_db_column3", primary_key=False, unique=False, db_index=False),
...     ChangeField("TestModel", "char_field", initial=None, max_length=35),
... ]

>>> for mutation in evolutions:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # RedundantAttrsChangeModel
%(RedundantAttrsChangeModel)s

# Change field type to another type with same internal_type
>>> class MyIntegerField(models.IntegerField):
...     def get_internal_type(self):
...         return 'IntegerField'

>>> class MinorFieldTypeChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = MyIntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='change_field_non-default_m2m_table')

>>> end = register_models(('TestModel', MinorFieldTypeChangeModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', MinorFieldTypeChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)

>>> d.is_empty()
True

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('change_field')

########NEW FILE########
__FILENAME__ = mysql
from django.db import connection
from django.db.models.options import Options

# This is not a great check, but it's from the same version as auto-created
# tables (Django 1.2), so we use it.
digest_index_names = hasattr(Options({}), 'auto_created')


def generate_index_name(table, column):
    if digest_index_names:
        column = connection.creation._digest(column)

    return '%s_%s' % (table, column)


add_field = {
    'AddNonNullNonCallableColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer ;',
            'UPDATE `tests_testmodel` SET `added_field` = 1 WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` integer NOT NULL;',
        ]),
    'AddNonNullCallableColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer ;',
            'UPDATE `tests_testmodel` SET `added_field` = `int_field` WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` integer NOT NULL;',
        ]),
    'AddNullColumnWithInitialColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer ;',
            'UPDATE `tests_testmodel` SET `added_field` = 1 WHERE `added_field` IS NULL;',
        ]),
    'AddStringColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` varchar(10) ;',
            'UPDATE `tests_testmodel` SET `added_field` = \'abc\\\'s xyz\' WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` varchar(10) NOT NULL;',
        ]),
    'AddDateColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` datetime ;',
            'UPDATE `tests_testmodel` SET `added_field` = 2007-12-13 16:42:00 WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` datetime NOT NULL;',
        ]),
    'AddDefaultColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer ;',
            'UPDATE `tests_testmodel` SET `added_field` = 42 WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` integer NOT NULL;',
        ]),
    'AddEmptyStringDefaultColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` varchar(20) ;',
            'UPDATE `tests_testmodel` SET `added_field` = \'\' WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `added_field` varchar(20) NOT NULL;',
        ]),
    'AddNullColumnModel':
        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer NULL ;',
    'NonDefaultColumnModel':
        'ALTER TABLE `tests_testmodel` ADD COLUMN `non-default_column` integer NULL ;',
    'AddColumnCustomTableModel':
        'ALTER TABLE `custom_table_name` ADD COLUMN `added_field` integer NULL ;',
    'AddIndexedColumnModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `add_field` integer NULL ;',
            'CREATE INDEX `%s` ON `tests_testmodel` (`add_field`);'
            % generate_index_name('tests_testmodel', 'add_field')
        ]),
    'AddUniqueColumnModel':
        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer NULL UNIQUE;',
    'AddUniqueIndexedModel':
        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field` integer NULL UNIQUE;',
    'AddForeignKeyModel':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field_id` integer NULL REFERENCES `tests_addanchor1` (`id`) ;',
            'CREATE INDEX `%s` ON `tests_testmodel` (`added_field_id`);'
            % generate_index_name('tests_testmodel', 'added_field_id')
        ]),
    'AddManyToManyDatabaseTableModel':
        '\n'.join([
            'CREATE TABLE `tests_testmodel_added_field` (',
            '    `id` integer AUTO_INCREMENT NOT NULL PRIMARY KEY,',
            '    `testmodel_id` integer NOT NULL,',
            '    `addanchor1_id` integer NOT NULL,',
            '    UNIQUE (`testmodel_id`, `addanchor1_id`)',
            ')',
            ';',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `testmodel_id_refs_id_ed159e33` FOREIGN KEY (`testmodel_id`) REFERENCES `tests_testmodel` (`id`);',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `addanchor1_id_refs_id_7efbb240` FOREIGN KEY (`addanchor1_id`) REFERENCES `tests_addanchor1` (`id`);'
        ]),
     'AddManyToManyNonDefaultDatabaseTableModel':
        '\n'.join([
            'CREATE TABLE `tests_testmodel_added_field` (',
            '    `id` integer AUTO_INCREMENT NOT NULL PRIMARY KEY,',
            '    `testmodel_id` integer NOT NULL,',
            '    `addanchor2_id` integer NOT NULL,',
            '    UNIQUE (`testmodel_id`, `addanchor2_id`)',
            ')',
            ';',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `testmodel_id_refs_id_ed159e33` FOREIGN KEY (`testmodel_id`) REFERENCES `tests_testmodel` (`id`);',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `addanchor2_id_refs_id_ec3e2588` FOREIGN KEY (`addanchor2_id`) REFERENCES `custom_add_anchor_table` (`id`);'
        ]),
     'AddManyToManySelf':
        '\n'.join([
            'CREATE TABLE `tests_testmodel_added_field` (',
            '    `id` integer AUTO_INCREMENT NOT NULL PRIMARY KEY,',
            '    `from_testmodel_id` integer NOT NULL,',
            '    `to_testmodel_id` integer NOT NULL,',
            '    UNIQUE (`from_testmodel_id`, `to_testmodel_id`)',
            ')',
            ';',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `from_testmodel_id_refs_id_ed159e33` FOREIGN KEY (`from_testmodel_id`) REFERENCES `tests_testmodel` (`id`);',
            'ALTER TABLE `tests_testmodel_added_field` ADD CONSTRAINT `to_testmodel_id_refs_id_ed159e33` FOREIGN KEY (`to_testmodel_id`) REFERENCES `tests_testmodel` (`id`);'
        ]),
}

delete_field = {
    'DefaultNamedColumnModel':
        'ALTER TABLE `tests_testmodel` DROP COLUMN `int_field` CASCADE;',
    'NonDefaultNamedColumnModel':
        'ALTER TABLE `tests_testmodel` DROP COLUMN `non-default_db_column` CASCADE;',
    'ConstrainedColumnModel':
        'ALTER TABLE `tests_testmodel` DROP COLUMN `int_field3` CASCADE;',
    'DefaultManyToManyModel':
        'DROP TABLE `tests_testmodel_m2m_field1`;',
    'NonDefaultManyToManyModel':
        'DROP TABLE `non-default_m2m_table`;',
    'DeleteForeignKeyModel':
        'ALTER TABLE `tests_testmodel` DROP COLUMN `fk_field1_id` CASCADE;',
    'DeleteColumnCustomTableModel':
        'ALTER TABLE `custom_table_name` DROP COLUMN `value` CASCADE;',
}

change_field = {
    "SetNotNullChangeModelWithConstant":
        '\n'.join([
            'UPDATE `tests_testmodel` SET `char_field1` = \'abc\\\'s xyz\' WHERE `char_field1` IS NULL;',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field1` varchar(25) NOT NULL;',
        ]),
    "SetNotNullChangeModelWithCallable":
            '\n'.join([
                'UPDATE `tests_testmodel` SET `char_field1` = `char_field` WHERE `char_field1` IS NULL;',
                'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field1` varchar(25) NOT NULL;',
            ]),
    "SetNullChangeModel": 'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field2` varchar(30) DEFAULT NULL;',
    "NoOpChangeModel": '',
    'IncreasingMaxLengthChangeModel':
            '\n'.join([
                'UPDATE `tests_testmodel` SET `char_field`=LEFT(`char_field`,45);',
                'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field` varchar(45);',
            ]),
    'DecreasingMaxLengthChangeModel':
            '\n'.join([
                'UPDATE `tests_testmodel` SET `char_field`=LEFT(`char_field`,1);',
                'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field` varchar(1);',
            ]),
    "DBColumnChangeModel": 'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_column` `customised_db_column` integer NOT NULL;',
    "M2MDBTableChangeModel": 'RENAME TABLE `change_field_non-default_m2m_table` TO `custom_m2m_db_table_name`;',
    "AddDBIndexChangeModel": 'CREATE INDEX `%s` ON `tests_testmodel` (`int_field2`);'
        % generate_index_name('tests_testmodel', 'int_field2'),
    "RemoveDBIndexChangeModel": 'DROP INDEX `%s` ON `tests_testmodel`;'
        % generate_index_name('tests_testmodel', 'int_field1'),
    "AddUniqueChangeModel": 'CREATE UNIQUE INDEX int_field4 ON `tests_testmodel`(`int_field4`);',
    "RemoveUniqueChangeModel": 'DROP INDEX int_field3 ON `tests_testmodel`;',
    "MultiAttrChangeModel":
        '\n'.join([
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field2` varchar(30) DEFAULT NULL;',
            'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_column` `custom_db_column2` integer NOT NULL;',
            'UPDATE `tests_testmodel` SET `char_field`=LEFT(`char_field`,35);',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field` varchar(35);',
        ]),
    "MultiAttrSingleFieldChangeModel":
        '\n'.join([
            'UPDATE `tests_testmodel` SET `char_field2`=LEFT(`char_field2`,35);',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field2` varchar(35);',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field2` varchar(35) DEFAULT NULL;',
        ]),
    "RedundantAttrsChangeModel":
        '\n'.join([
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field2` varchar(30) DEFAULT NULL;',
            'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_column` `custom_db_column3` integer NOT NULL;',
            'UPDATE `tests_testmodel` SET `char_field`=LEFT(`char_field`,35);',
            'ALTER TABLE `tests_testmodel` MODIFY COLUMN `char_field` varchar(35);',
        ]),
}

delete_model = {
    'BasicModel':
        'DROP TABLE `tests_basicmodel`;',
    'BasicWithM2MModel':
        '\n'.join([
            'DROP TABLE `tests_basicwithm2mmodel_m2m`;',
            'DROP TABLE `tests_basicwithm2mmodel`;'
        ]),
    'CustomTableModel':
        'DROP TABLE `custom_table_name`;',
    'CustomTableWithM2MModel':
        '\n'.join([
            'DROP TABLE `another_custom_table_name_m2m`;',
            'DROP TABLE `another_custom_table_name`;'
        ]),
}

delete_application = {
    'DeleteApplication':
        '\n'.join([
            'DROP TABLE `tests_appdeleteanchor1`;',
            'DROP TABLE `app_delete_custom_add_anchor_table`;',
            'DROP TABLE `tests_testmodel_anchor_m2m`;',
            'DROP TABLE `tests_testmodel`;',
            'DROP TABLE `app_delete_custom_table_name`;',
        ]),
}

rename_field = {
    'RenameColumnModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `int_field` `renamed_field` integer NOT NULL;',
    'RenameColumnWithTableNameModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `int_field` `renamed_field` integer NOT NULL;',
    'RenamePrimaryKeyColumnModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `id` `my_pk_id`;',
    'RenameForeignKeyColumnModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `fk_field_id` `renamed_field_id` integer NOT NULL;',
    'RenameNonDefaultColumnNameModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_col_name` `renamed_field` integer NOT NULL;',
    'RenameNonDefaultColumnNameToNonDefaultNameModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_col_name` `non-default_column_name` integer NOT NULL;',
    'RenameNonDefaultColumnNameToNonDefaultNameAndTableModel':
        'ALTER TABLE `tests_testmodel` CHANGE COLUMN `custom_db_col_name` `non-default_column_name2` integer NOT NULL;',
    'RenameColumnCustomTableModel':
        'ALTER TABLE `custom_rename_table_name` CHANGE COLUMN `value` `renamed_field` integer NOT NULL;',
    'RenameManyToManyTableModel':
        'RENAME TABLE `tests_testmodel_m2m_field` TO `tests_testmodel_renamed_field`;',
    'RenameManyToManyTableWithColumnNameModel':
        'RENAME TABLE `tests_testmodel_m2m_field` TO `tests_testmodel_renamed_field`;',
    'RenameNonDefaultManyToManyTableModel':
        'RENAME TABLE `non-default_db_table` TO `tests_testmodel_renamed_field`;',
}


sql_mutation = {
    'SQLMutationSequence': """[
...    SQLMutation('first-two-fields', [
...        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field1` integer NULL;',
...        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field2` integer NULL;'
...    ], update_first_two),
...    SQLMutation('third-field', [
...        'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field3` integer NULL;',
...    ], update_third)]
""",
    'SQLMutationOutput':
        '\n'.join([
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field1` integer NULL;',
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field2` integer NULL;',
            'ALTER TABLE `tests_testmodel` ADD COLUMN `added_field3` integer NULL;',
        ]),
}

generics = {
    'DeleteColumnModel': "ALTER TABLE `tests_testmodel` DROP COLUMN `char_field` CASCADE;"
}

inheritance = {
    'AddToChildModel':
        '\n'.join([
            'ALTER TABLE `tests_childmodel` ADD COLUMN `added_field` integer ;',
            'UPDATE `tests_childmodel` SET `added_field` = 42 WHERE `added_field` IS NULL;',
            'ALTER TABLE `tests_childmodel` MODIFY COLUMN `added_field` integer NOT NULL;',
        ]),
    'DeleteFromChildModel':
        'ALTER TABLE `tests_childmodel` DROP COLUMN `int_field` CASCADE;',
}

########NEW FILE########
__FILENAME__ = mysql_old
# MySQL_old behaviour is identical to mysql base
from mysql import *

########NEW FILE########
__FILENAME__ = postgresql
from django.db.models.options import Options

autocreate_through_tables = hasattr(Options({}), 'auto_created')


add_field = {
    'AddNonNullNonCallableColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer ;',
            'UPDATE "tests_testmodel" SET "added_field" = 1 WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddNonNullCallableColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer ;',
            'UPDATE "tests_testmodel" SET "added_field" = "int_field" WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddNullColumnWithInitialColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer ;',
            'UPDATE "tests_testmodel" SET "added_field" = 1 WHERE "added_field" IS NULL;',
        ]),
    'AddStringColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" varchar(10) ;',
            'UPDATE "tests_testmodel" SET "added_field" = \'abc\\\'s xyz\' WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddDateColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" timestamp with time zone ;',
            'UPDATE "tests_testmodel" SET "added_field" = 2007-12-13 16:42:00 WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddDefaultColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer ;',
            'UPDATE "tests_testmodel" SET "added_field" = 42 WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddEmptyStringDefaultColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" varchar(20) ;',
            'UPDATE "tests_testmodel" SET "added_field" = \'\' WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'AddNullColumnModel':
        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer NULL ;',
    'NonDefaultColumnModel':
        'ALTER TABLE "tests_testmodel" ADD COLUMN "non-default_column" integer NULL ;',
    'AddColumnCustomTableModel':
        'ALTER TABLE "custom_table_name" ADD COLUMN "added_field" integer NULL ;',
    'AddIndexedColumnModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "add_field" integer NULL ;',
            'CREATE INDEX "tests_testmodel_add_field" ON "tests_testmodel" ("add_field");'
        ]),
    'AddUniqueColumnModel':
        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer NULL UNIQUE;',
    'AddUniqueIndexedModel':
        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field" integer NULL UNIQUE;',
    'AddForeignKeyModel':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field_id" integer NULL REFERENCES "tests_addanchor1" ("id")  DEFERRABLE INITIALLY DEFERRED;',
            'CREATE INDEX "tests_testmodel_added_field_id" ON "tests_testmodel" ("added_field_id");'
        ]),
}

if autocreate_through_tables:
    add_field.update({
        'AddManyToManyDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL,',
                '    "addanchor1_id" integer NOT NULL,',
                '    UNIQUE ("testmodel_id", "addanchor1_id")',
                ')',
                ';',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "testmodel_id_refs_id_ed159e33" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "addanchor1_id_refs_id_7efbb240" FOREIGN KEY ("addanchor1_id") REFERENCES "tests_addanchor1" ("id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
         'AddManyToManyNonDefaultDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL,',
                '    "addanchor2_id" integer NOT NULL,',
                '    UNIQUE ("testmodel_id", "addanchor2_id")',
                ')',
                ';',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "testmodel_id_refs_id_ed159e33" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "addanchor2_id_refs_id_ec3e2588" FOREIGN KEY ("addanchor2_id") REFERENCES "custom_add_anchor_table" ("id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
         'AddManyToManySelf':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "from_testmodel_id" integer NOT NULL,',
                '    "to_testmodel_id" integer NOT NULL,',
                '    UNIQUE ("from_testmodel_id", "to_testmodel_id")',
                ')',
                ';',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "from_testmodel_id_refs_id_ed159e33" FOREIGN KEY ("from_testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
                'ALTER TABLE "tests_testmodel_added_field" ADD CONSTRAINT "to_testmodel_id_refs_id_ed159e33" FOREIGN KEY ("to_testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
    })
else:
    add_field.update({
        'AddManyToManyDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    "addanchor1_id" integer NOT NULL REFERENCES "tests_addanchor1" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    UNIQUE ("testmodel_id", "addanchor1_id")',
                ')',
                ';'
            ]),
         'AddManyToManyNonDefaultDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    "addanchor2_id" integer NOT NULL REFERENCES "custom_add_anchor_table" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    UNIQUE ("testmodel_id", "addanchor2_id")',
                ')',
                ';'
            ]),
         'AddManyToManySelf':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" serial NOT NULL PRIMARY KEY,',
                '    "from_testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    "to_testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED,',
                '    UNIQUE ("from_testmodel_id", "to_testmodel_id")',
                ')',
                ';'
            ]),
    })

delete_field = {
    'DefaultNamedColumnModel':
        'ALTER TABLE "tests_testmodel" DROP COLUMN "int_field" CASCADE;',
    'NonDefaultNamedColumnModel':
        'ALTER TABLE "tests_testmodel" DROP COLUMN "non-default_db_column" CASCADE;',
    'ConstrainedColumnModel':
        'ALTER TABLE "tests_testmodel" DROP COLUMN "int_field3" CASCADE;',
    'DefaultManyToManyModel':
        'DROP TABLE "tests_testmodel_m2m_field1";',
    'NonDefaultManyToManyModel':
        'DROP TABLE "non-default_m2m_table";',
    'DeleteForeignKeyModel':
        'ALTER TABLE "tests_testmodel" DROP COLUMN "fk_field1_id" CASCADE;',
    'DeleteColumnCustomTableModel':
        'ALTER TABLE "custom_table_name" DROP COLUMN "value" CASCADE;',
}

change_field = {
    "SetNotNullChangeModelWithConstant":
        '\n'.join([
            'UPDATE "tests_testmodel" SET "char_field1" = \'abc\\\'s xyz\' WHERE "char_field1" IS NULL;',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field1" SET NOT NULL;',
        ]),
    "SetNotNullChangeModelWithCallable":
            '\n'.join([
                'UPDATE "tests_testmodel" SET "char_field1" = "char_field" WHERE "char_field1" IS NULL;',
                'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field1" SET NOT NULL;',
            ]),
    "SetNullChangeModel": 'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field2" DROP NOT NULL;',
    "NoOpChangeModel": '',
    "IncreasingMaxLengthChangeModel": 'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field" TYPE varchar(45) USING CAST("char_field" as varchar(45));',
    "DecreasingMaxLengthChangeModel": 'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field" TYPE varchar(1) USING CAST("char_field" as varchar(1));',
    "DBColumnChangeModel": 'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_column" TO "customised_db_column";',
    "AddDBIndexChangeModel": 'CREATE INDEX "tests_testmodel_int_field2" ON "tests_testmodel" ("int_field2");',
    "RemoveDBIndexChangeModel": 'DROP INDEX "tests_testmodel_int_field1";',
    "AddUniqueChangeModel": 'ALTER TABLE "tests_testmodel" ADD CONSTRAINT tests_testmodel_int_field4_key UNIQUE("int_field4");',
    "RemoveUniqueChangeModel": 'ALTER TABLE "tests_testmodel" DROP CONSTRAINT tests_testmodel_int_field3_key;',
    "MultiAttrChangeModel":
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field2" DROP NOT NULL;',
            'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_column" TO "custom_db_column2";',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field" TYPE varchar(35) USING CAST("char_field" as varchar(35));',
        ]),
    "MultiAttrSingleFieldChangeModel":
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field2" TYPE varchar(35) USING CAST("char_field2" as varchar(35));',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field2" DROP NOT NULL;',
        ]),
    "RedundantAttrsChangeModel":
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field2" DROP NOT NULL;',
            'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_column" TO "custom_db_column3";',
            'ALTER TABLE "tests_testmodel" ALTER COLUMN "char_field" TYPE varchar(35) USING CAST("char_field" as varchar(35));',
        ]),
}

if autocreate_through_tables:
    change_field.update({
        "M2MDBTableChangeModel":
            '\n'.join([
                'ALTER TABLE "change_field_non-default_m2m_table" DROP CONSTRAINT "testmodel_id_refs_my_id_5d3392f8";',
                'ALTER TABLE "change_field_non-default_m2m_table" RENAME TO "custom_m2m_db_table_name";',
                'ALTER TABLE "custom_m2m_db_table_name" ADD CONSTRAINT "testmodel_id_refs_my_id_a31f0c6f" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("my_id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
    })
else:
    change_field.update({
        "M2MDBTableChangeModel": 'ALTER TABLE "change_field_non-default_m2m_table" RENAME TO "custom_m2m_db_table_name";',
    })

delete_model = {
    'BasicModel':
        'DROP TABLE "tests_basicmodel";',
    'BasicWithM2MModel':
        '\n'.join([
            'DROP TABLE "tests_basicwithm2mmodel_m2m";',
            'DROP TABLE "tests_basicwithm2mmodel";'
        ]),
    'CustomTableModel':
        'DROP TABLE "custom_table_name";',
    'CustomTableWithM2MModel':
        '\n'.join([
            'DROP TABLE "another_custom_table_name_m2m";',
            'DROP TABLE "another_custom_table_name";'
        ]),
}

delete_application = {
    'DeleteApplication':
        '\n'.join([
            'DROP TABLE "tests_appdeleteanchor1";',
            'DROP TABLE "app_delete_custom_add_anchor_table";',
            'DROP TABLE "tests_testmodel_anchor_m2m";',
            'DROP TABLE "tests_testmodel";',
            'DROP TABLE "app_delete_custom_table_name";',
        ]),
}

rename_field = {
    'RenameColumnModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "int_field" TO "renamed_field";',
    'RenameColumnWithTableNameModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "int_field" TO "renamed_field";',
    'RenameForeignKeyColumnModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "fk_field_id" TO "renamed_field_id";',
    'RenameNonDefaultColumnNameModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_col_name" TO "renamed_field";',
    'RenameNonDefaultColumnNameToNonDefaultNameModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_col_name" TO "non-default_column_name";',
    'RenameNonDefaultColumnNameToNonDefaultNameAndTableModel':
        'ALTER TABLE "tests_testmodel" RENAME COLUMN "custom_db_col_name" TO "non-default_column_name2";',
    'RenameColumnCustomTableModel':
        'ALTER TABLE "custom_rename_table_name" RENAME COLUMN "value" TO "renamed_field";',
    'RenameNonDefaultManyToManyTableModel':
        'ALTER TABLE "non-default_db_table" RENAME TO "tests_testmodel_renamed_field";',
}

sql_mutation = {
    'SQLMutationSequence': """[
...    SQLMutation('first-two-fields', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field1" integer NULL;',
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field2" integer NULL;'
...    ], update_first_two),
...    SQLMutation('third-field', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field3" integer NULL;',
...    ], update_third)]
""",
    'SQLMutationOutput':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field1" integer NULL;',
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field2" integer NULL;',
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field3" integer NULL;',
        ]),
}

if autocreate_through_tables:
    rename_field.update({
        'RenamePrimaryKeyColumnModel':
            '\n'.join([
                'ALTER TABLE "non-default_db_table" DROP CONSTRAINT "testmodel_id_refs_id_eeae318e";',
                'ALTER TABLE "tests_testmodel_m2m_field" DROP CONSTRAINT "testmodel_id_refs_id_ba77d38d";',
                'ALTER TABLE "tests_testmodel" RENAME COLUMN "id" TO "my_pk_id";',
                'ALTER TABLE "non-default_db_table" ADD CONSTRAINT "testmodel_id_refs_my_pk_id_eeae318e" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("my_pk_id") DEFERRABLE INITIALLY DEFERRED;',
                'ALTER TABLE "tests_testmodel_m2m_field" ADD CONSTRAINT "testmodel_id_refs_my_pk_id_ba77d38d" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("my_pk_id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
        'RenameManyToManyTableModel':
            '\n'.join([
                'ALTER TABLE "tests_testmodel_m2m_field" DROP CONSTRAINT "testmodel_id_refs_id_ba77d38d";',
                'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
                'ALTER TABLE "tests_testmodel_renamed_field" ADD CONSTRAINT "testmodel_id_refs_id_f50a5e5d" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
        'RenameManyToManyTableWithColumnNameModel':
            '\n'.join([
                'ALTER TABLE "tests_testmodel_m2m_field" DROP CONSTRAINT "testmodel_id_refs_id_ba77d38d";',
                'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
                'ALTER TABLE "tests_testmodel_renamed_field" ADD CONSTRAINT "testmodel_id_refs_id_f50a5e5d" FOREIGN KEY ("testmodel_id") REFERENCES "tests_testmodel" ("id") DEFERRABLE INITIALLY DEFERRED;',
            ]),
    })
else:
    rename_field.update({
        'RenamePrimaryKeyColumnModel':
            'ALTER TABLE "tests_testmodel" RENAME COLUMN "id" TO "my_pk_id";',
        'RenameManyToManyTableModel':
            'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
        'RenameManyToManyTableWithColumnNameModel':
            'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
    })

generics = {
    'DeleteColumnModel': 'ALTER TABLE "tests_testmodel" DROP COLUMN "char_field" CASCADE;'
}

inheritance = {
    'AddToChildModel':
        '\n'.join([
            'ALTER TABLE "tests_childmodel" ADD COLUMN "added_field" integer ;',
            'UPDATE "tests_childmodel" SET "added_field" = 42 WHERE "added_field" IS NULL;',
            'ALTER TABLE "tests_childmodel" ALTER COLUMN "added_field" SET NOT NULL;',
        ]),
    'DeleteFromChildModel':
        'ALTER TABLE "tests_childmodel" DROP COLUMN "int_field" CASCADE;',
}

########NEW FILE########
__FILENAME__ = postgresql_psycopg2
# Psycopg2 behaviour is identical to Psycopg1
from postgresql import *

########NEW FILE########
__FILENAME__ = sqlite3
from django.db import connection
from django.db.models.options import Options


autocreate_through_tables = hasattr(Options({}), 'auto_created')

# This is not a great check, but it's from the same version as auto-created
# tables (Django 1.2), so we use it.
digest_index_names = hasattr(Options({}), 'auto_created')


def generate_index_name(table, column):
    if digest_index_names:
        column = connection.creation._digest(column)

    return '%s_%s' % (table, column)


add_field = {
    'AddNonNullNonCallableColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = 1;',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddNonNullCallableColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = "int_field";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddNullColumnWithInitialColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = 1;',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddStringColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" varchar(10) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = \'abc\\\'s xyz\';',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" varchar(10) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddDateColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" datetime NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = 2007-12-13 16:42:00;',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" datetime NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddDefaultColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = 42;',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddEmptyStringDefaultColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" varchar(20) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = \'\';',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" varchar(20) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddNullColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'NonDefaultColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "non-default_column" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "non-default_column" integer NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "non-default_column") SELECT "int_field", "id", "char_field", "non-default_column" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddColumnCustomTableModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("id" integer NULL UNIQUE PRIMARY KEY, "value" integer NULL, "alt_value" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("id", "value", "alt_value") SELECT "id", "value", "alt_value" FROM "custom_table_name";',
            'DROP TABLE "custom_table_name";',
            'CREATE TABLE "custom_table_name"("id" integer NOT NULL UNIQUE PRIMARY KEY, "value" integer NOT NULL, "alt_value" varchar(20) NOT NULL, "added_field" integer NULL);',
            'INSERT INTO "custom_table_name" ("id", "value", "alt_value", "added_field") SELECT "id", "value", "alt_value", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddIndexedColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "add_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "add_field" integer NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "add_field") SELECT "int_field", "id", "char_field", "add_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE INDEX "%s" ON "tests_testmodel" ("add_field");'
            % generate_index_name('tests_testmodel', 'add_field'),
        ]),
    'AddUniqueColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL UNIQUE);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NULL UNIQUE);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddUniqueIndexedModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL UNIQUE);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NULL UNIQUE);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'AddForeignKeyModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field_id" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field_id" integer NULL);',
            'INSERT INTO "tests_testmodel" ("int_field", "id", "char_field", "added_field_id") SELECT "int_field", "id", "char_field", "added_field_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE INDEX "%s" ON "tests_testmodel" ("added_field_id");'
            % generate_index_name('tests_testmodel', 'added_field_id'),
        ]),
}


if autocreate_through_tables:
    add_field.update({
        'AddManyToManyDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL,',
                '    "addanchor1_id" integer NOT NULL,',
                '    UNIQUE ("testmodel_id", "addanchor1_id")',
                ')',
                ';',
            ]),
         'AddManyToManyNonDefaultDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL,',
                '    "addanchor2_id" integer NOT NULL,',
                '    UNIQUE ("testmodel_id", "addanchor2_id")',
                ')',
                ';',
            ]),
         'AddManyToManySelf':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "from_testmodel_id" integer NOT NULL,',
                '    "to_testmodel_id" integer NOT NULL,',
                '    UNIQUE ("from_testmodel_id", "to_testmodel_id")',
                ')',
                ';',
            ]),
    })
else:
    add_field.update({
        'AddManyToManyDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id"),',
                '    "addanchor1_id" integer NOT NULL REFERENCES "tests_addanchor1" ("id"),',
                '    UNIQUE ("testmodel_id", "addanchor1_id")',
                ')',
                ';',
            ]),
         'AddManyToManyNonDefaultDatabaseTableModel':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id"),',
                '    "addanchor2_id" integer NOT NULL REFERENCES "custom_add_anchor_table" ("id"),',
                '    UNIQUE ("testmodel_id", "addanchor2_id")',
                ')',
                ';',
            ]),
         'AddManyToManySelf':
            '\n'.join([
                'CREATE TABLE "tests_testmodel_added_field" (',
                '    "id" integer NOT NULL PRIMARY KEY,',
                '    "from_testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id"),',
                '    "to_testmodel_id" integer NOT NULL REFERENCES "tests_testmodel" ("id"),',
                '    UNIQUE ("from_testmodel_id", "to_testmodel_id")',
                ')',
                ';',
            ]),
    })

delete_field = {
    'DefaultNamedColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("non-default_db_column" integer NULL, "int_field3" integer NULL UNIQUE, "fk_field1_id" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("non-default_db_column", "int_field3", "fk_field1_id", "char_field", "my_id") SELECT "non-default_db_column", "int_field3", "fk_field1_id", "char_field", "my_id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("non-default_db_column" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "fk_field1_id" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field1_id");'
            % generate_index_name('tests_testmodel', 'fk_field1_id'),
            'INSERT INTO "tests_testmodel" ("non-default_db_column", "int_field3", "fk_field1_id", "char_field", "my_id") SELECT "non-default_db_column", "int_field3", "fk_field1_id", "char_field", "my_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'NonDefaultNamedColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "int_field3" integer NULL UNIQUE, "fk_field1_id" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "int_field3", "fk_field1_id", "char_field", "my_id") SELECT "int_field", "int_field3", "fk_field1_id", "char_field", "my_id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "fk_field1_id" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field1_id");'
            % generate_index_name('tests_testmodel', 'fk_field1_id'),
            'INSERT INTO "tests_testmodel" ("int_field", "int_field3", "fk_field1_id", "char_field", "my_id") SELECT "int_field", "int_field3", "fk_field1_id", "char_field", "my_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'ConstrainedColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "non-default_db_column" integer NULL, "fk_field1_id" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "non-default_db_column", "fk_field1_id", "char_field", "my_id") SELECT "int_field", "non-default_db_column", "fk_field1_id", "char_field", "my_id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "non-default_db_column" integer NOT NULL, "fk_field1_id" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field1_id");'
            % generate_index_name('tests_testmodel', 'fk_field1_id'),
            'INSERT INTO "tests_testmodel" ("int_field", "non-default_db_column", "fk_field1_id", "char_field", "my_id") SELECT "int_field", "non-default_db_column", "fk_field1_id", "char_field", "my_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'DefaultManyToManyModel':
        'DROP TABLE "tests_testmodel_m2m_field1";',
    'NonDefaultManyToManyModel':
        'DROP TABLE "non-default_m2m_table";',
    'DeleteForeignKeyModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "non-default_db_column" integer NULL, "int_field3" integer NULL UNIQUE, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "non-default_db_column", "int_field3", "char_field", "my_id") SELECT "int_field", "non-default_db_column", "int_field3", "char_field", "my_id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "non-default_db_column" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "tests_testmodel" ("int_field", "non-default_db_column", "int_field3", "char_field", "my_id") SELECT "int_field", "non-default_db_column", "int_field3", "char_field", "my_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'DeleteColumnCustomTableModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("id" integer NULL UNIQUE PRIMARY KEY, "alt_value" varchar(20) NULL);',
            'INSERT INTO "TEMP_TABLE" ("id", "alt_value") SELECT "id", "alt_value" FROM "custom_table_name";',
            'DROP TABLE "custom_table_name";',
            'CREATE TABLE "custom_table_name"("id" integer NOT NULL UNIQUE PRIMARY KEY, "alt_value" varchar(20) NOT NULL);',
            'INSERT INTO "custom_table_name" ("id", "alt_value") SELECT "id", "alt_value" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
}

change_field = {
    "SetNotNullChangeModelWithConstant":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "char_field1" = \'abc\\\'s xyz\';',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NOT NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "SetNotNullChangeModelWithCallable":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "char_field1" = "char_field";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NOT NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "SetNullChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "NoOpChangeModel": '',
    "IncreasingMaxLengthChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(45) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(45) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "DecreasingMaxLengthChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(1) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(1) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "DBColumnChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "customised_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "customised_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "M2MDBTableChangeModel": 'ALTER TABLE "change_field_non-default_m2m_table" RENAME TO "custom_m2m_db_table_name";',
    "AddDBIndexChangeModel": 'CREATE INDEX "%s" ON "tests_testmodel" ("int_field2");'
        % generate_index_name('tests_testmodel', 'int_field2'),
    "RemoveDBIndexChangeModel": 'DROP INDEX "%s";'
        % generate_index_name('tests_testmodel', 'int_field1'),
    "AddUniqueChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL UNIQUE, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL UNIQUE, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "RemoveUniqueChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "MultiAttrChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column2" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column2" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column2" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(35) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column2" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(35) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "MultiAttrSingleFieldChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "RedundantAttrsChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column3" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column3" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column3" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(35) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column3" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(35) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
}

multi_db = {
    "SetNotNullChangeModelWithConstant":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "char_field1" = \'abc\\\'s xyz\';',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NOT NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "SetNotNullChangeModelWithCallable":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'UPDATE "TEMP_TABLE" SET "char_field1" = "char_field";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NOT NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "SetNullChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "NoOpChangeModel": '',
    "IncreasingMaxLengthChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(45) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(45) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "DecreasingMaxLengthChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(1) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(1) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "DBColumnChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "customised_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "customised_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "customised_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "M2MDBTableChangeModel": 'ALTER TABLE "multi_db_non-default_m2m_table" RENAME TO "custom_m2m_db_table_name";',
    "AddDBIndexChangeModel": 'CREATE INDEX "%s" ON "tests_testmodel" ("int_field2");'
        % generate_index_name('tests_testmodel', 'int_field2'),
    "RemoveDBIndexChangeModel": 'DROP INDEX "%s";'
        % generate_index_name('tests_testmodel', 'int_field1'),
    "AddUniqueChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL UNIQUE, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL UNIQUE, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "RemoveUniqueChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "MultiAttrChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column2" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column2" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column2" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(35) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column2" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(35) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column2", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "MultiAttrSingleFieldChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NOT NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(35) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    "RedundantAttrsChangeModel":
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column3" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(20) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column3" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(20) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("int_field1");'
            % generate_index_name('tests_testmodel', 'int_field1'),
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field4" integer NULL, "custom_db_column3" integer NULL, "int_field1" integer NULL, "int_field2" integer NULL, "int_field3" integer NULL UNIQUE, "alt_pk" integer NULL, "char_field" varchar(35) NULL, "my_id" integer NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field4" integer NOT NULL, "custom_db_column3" integer NOT NULL, "int_field1" integer NOT NULL, "int_field2" integer NOT NULL, "int_field3" integer NOT NULL UNIQUE, "alt_pk" integer NOT NULL, "char_field" varchar(35) NOT NULL, "my_id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field1" varchar(25) NULL, "char_field2" varchar(30) NULL);',
            'INSERT INTO "tests_testmodel" ("int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2") SELECT "int_field4", "custom_db_column3", "int_field1", "int_field2", "int_field3", "alt_pk", "char_field", "my_id", "char_field1", "char_field2" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
}

delete_model = {
    'BasicModel':
        'DROP TABLE "tests_basicmodel";',
    'BasicWithM2MModel':
        '\n'.join([
            'DROP TABLE "tests_basicwithm2mmodel_m2m";',
            'DROP TABLE "tests_basicwithm2mmodel";'
        ]),
    'CustomTableModel':
        'DROP TABLE "custom_table_name";',
    'CustomTableWithM2MModel':
        '\n'.join([
            'DROP TABLE "another_custom_table_name_m2m";',
            'DROP TABLE "another_custom_table_name";'
        ]),
}

delete_application = {
    'DeleteApplication':
        '\n'.join([
            'DROP TABLE "tests_appdeleteanchor1";',
            'DROP TABLE "app_delete_custom_add_anchor_table";',
            'DROP TABLE "tests_testmodel_anchor_m2m";',
            'DROP TABLE "tests_testmodel";',
            'DROP TABLE "app_delete_custom_table_name";',
        ]),
    'DeleteApplicationWithoutDatabase': "",
}

rename_field = {
    'RenameColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("renamed_field" integer NULL, "char_field" varchar(20) NULL, "custom_db_col_name" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("renamed_field" integer NOT NULL, "char_field" varchar(20) NOT NULL, "custom_db_col_name" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameColumnWithTableNameModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("renamed_field" integer NULL, "char_field" varchar(20) NULL, "custom_db_col_name" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("renamed_field" integer NOT NULL, "char_field" varchar(20) NOT NULL, "custom_db_col_name" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "renamed_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenamePrimaryKeyColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "char_field" varchar(20) NULL, "custom_db_col_name" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "my_pk_id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "my_pk_id") SELECT "int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "char_field" varchar(20) NOT NULL, "custom_db_col_name" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "my_pk_id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "my_pk_id") SELECT "int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "fk_field_id", "my_pk_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameForeignKeyColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "char_field" varchar(20) NULL, "custom_db_col_name" integer NULL, "custom_db_col_name_indexed" integer NULL, "renamed_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("renamed_field", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "custom_db_col_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("int_field" integer NOT NULL, "char_field" varchar(20) NOT NULL, "custom_db_col_name" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "renamed_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("renamed_field_id");'
            % generate_index_name('tests_testmodel', 'renamed_field_id'),
            'INSERT INTO "tests_testmodel" ("int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "renamed_field_id", "id") SELECT "int_field", "char_field", "custom_db_col_name", "custom_db_col_name_indexed", "renamed_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameNonDefaultColumnNameModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("renamed_field" integer NULL, "char_field" varchar(20) NULL, "int_field" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("renamed_field", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "custom_db_col_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("renamed_field" integer NOT NULL, "char_field" varchar(20) NOT NULL, "int_field" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("renamed_field", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "renamed_field", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameNonDefaultColumnNameToNonDefaultNameModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("non-default_column_name" integer NULL, "char_field" varchar(20) NULL, "int_field" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("non-default_column_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "custom_db_col_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("non-default_column_name" integer NOT NULL, "char_field" varchar(20) NOT NULL, "int_field" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("non-default_column_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "non-default_column_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameNonDefaultColumnNameToNonDefaultNameAndTableModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("non-default_column_name2" integer NULL, "char_field" varchar(20) NULL, "int_field" integer NULL, "custom_db_col_name_indexed" integer NULL, "fk_field_id" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY);',
            'INSERT INTO "TEMP_TABLE" ("non-default_column_name2", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "custom_db_col_name", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("non-default_column_name2" integer NOT NULL, "char_field" varchar(20) NOT NULL, "int_field" integer NOT NULL, "custom_db_col_name_indexed" integer NOT NULL, "fk_field_id" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("custom_db_col_name_indexed");'
            % generate_index_name('tests_testmodel', 'custom_db_col_name_indexed'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("fk_field_id");'
            % generate_index_name('tests_testmodel', 'fk_field_id'),
            'INSERT INTO "tests_testmodel" ("non-default_column_name2", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id") SELECT "non-default_column_name2", "char_field", "int_field", "custom_db_col_name_indexed", "fk_field_id", "id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameColumnCustomTableModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("id" integer NULL UNIQUE PRIMARY KEY, "renamed_field" integer NULL, "alt_value" varchar(20) NULL);',
            'INSERT INTO "TEMP_TABLE" ("id", "renamed_field", "alt_value") SELECT "id", "value", "alt_value" FROM "custom_rename_table_name";',
            'DROP TABLE "custom_rename_table_name";',
            'CREATE TABLE "custom_rename_table_name"("id" integer NOT NULL UNIQUE PRIMARY KEY, "renamed_field" integer NOT NULL, "alt_value" varchar(20) NOT NULL);',
            'INSERT INTO "custom_rename_table_name" ("id", "renamed_field", "alt_value") SELECT "id", "renamed_field", "alt_value" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'RenameManyToManyTableModel':
        'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
    'RenameManyToManyTableWithColumnNameModel':
        'ALTER TABLE "tests_testmodel_m2m_field" RENAME TO "tests_testmodel_renamed_field";',
    'RenameNonDefaultManyToManyTableModel':
        'ALTER TABLE "non-default_db_table" RENAME TO "tests_testmodel_renamed_field";',
}

sql_mutation = {
    'SQLMutationSequence': """[
...    SQLMutation('first-two-fields', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field1" integer NULL;',
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field2" integer NULL;'
...    ], update_first_two),
...    SQLMutation('third-field', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field3" integer NULL;',
...    ], update_third)]
""",
    'SQLMutationOutput':
        '\n'.join([
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field1" integer NULL;',
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field2" integer NULL;',
            'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field3" integer NULL;',
        ]),
}

generics = {
    'DeleteColumnModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("object_id" integer unsigned NULL, "int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "content_type_id" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("object_id", "int_field", "id", "content_type_id") SELECT "object_id", "int_field", "id", "content_type_id" FROM "tests_testmodel";',
            'DROP TABLE "tests_testmodel";',
            'CREATE TABLE "tests_testmodel"("object_id" integer unsigned NOT NULL, "int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "content_type_id" integer NOT NULL);',
            'CREATE INDEX "%s" ON "tests_testmodel" ("object_id");'
            % generate_index_name('tests_testmodel', 'object_id'),
            'CREATE INDEX "%s" ON "tests_testmodel" ("content_type_id");'
            % generate_index_name('tests_testmodel', 'content_type_id'),
            'INSERT INTO "tests_testmodel" ("object_id", "int_field", "id", "content_type_id") SELECT "object_id", "int_field", "id", "content_type_id" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ])
}

inheritance = {
    'AddToChildModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("int_field" integer NULL, "id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL, "added_field" integer NULL);',
            'INSERT INTO "TEMP_TABLE" ("int_field", "id", "char_field") SELECT "int_field", "id", "char_field" FROM "tests_childmodel";',
            'UPDATE "TEMP_TABLE" SET "added_field" = 42;',
            'DROP TABLE "tests_childmodel";',
            'CREATE TABLE "tests_childmodel"("int_field" integer NOT NULL, "id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL, "added_field" integer NOT NULL);',
            'INSERT INTO "tests_childmodel" ("int_field", "id", "char_field", "added_field") SELECT "int_field", "id", "char_field", "added_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";',
        ]),
    'DeleteFromChildModel':
        '\n'.join([
            'CREATE TEMPORARY TABLE "TEMP_TABLE"("id" integer NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NULL);',
            'INSERT INTO "TEMP_TABLE" ("id", "char_field") SELECT "id", "char_field" FROM "tests_childmodel";',
            'DROP TABLE "tests_childmodel";',
            'CREATE TABLE "tests_childmodel"("id" integer NOT NULL UNIQUE PRIMARY KEY, "char_field" varchar(20) NOT NULL);',
            'INSERT INTO "tests_childmodel" ("id", "char_field") SELECT "id", "char_field" FROM "TEMP_TABLE";',
            'DROP TABLE "TEMP_TABLE";'
        ])
}

########NEW FILE########
__FILENAME__ = delete_app
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from datetime import datetime
>>> from pprint import PrettyPrinter

>>> from django.db import models

>>> from django_evolution.mutations import AddField, DeleteField, DeleteApplication
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django_evolution import signature
>>> from django_evolution import models as test_app

>>> import copy

>>> class AppDeleteAnchor1(models.Model):
...     value = models.IntegerField()

>>> class AppDeleteAnchor2(models.Model):
...     value = models.IntegerField()
...     class Meta:
...         db_table = 'app_delete_custom_add_anchor_table'

>>> class AppDeleteBaseModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     anchor_fk = models.ForeignKey(AppDeleteAnchor1)
...     anchor_m2m = models.ManyToManyField(AppDeleteAnchor2)

>>> class AppDeleteCustomTableModel(models.Model):
...     value = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'app_delete_custom_table_name'

# Store the base signatures, and populate the app cache

>>> anchors = [('AppDeleteAnchor1', AppDeleteAnchor1), ('AppDeleteAnchor2',AppDeleteAnchor2)]
>>> test_model = [('TestModel', AppDeleteBaseModel)]
>>> custom_model = [('CustomTestModel', AppDeleteCustomTableModel)]
>>> all_models = []
>>> all_models.extend(anchors)
>>> all_models.extend(test_model)
>>> all_models.extend(custom_model)
>>> start = register_models(*all_models)
>>> start_sig = test_proj_sig(*all_models)

# Copy the base signature, and delete the tests app.
>>> deleted_app_sig = copy.deepcopy(start_sig)
>>> deleted_app_sig = deleted_app_sig.pop('tests')

>>> d = Diff(start_sig, deleted_app_sig)
>>> print d.deleted
{'tests': ['AppDeleteAnchor1', 'AppDeleteAnchor2', 'TestModel', 'CustomTestModel']}

>>> test_sig = copy.deepcopy(start_sig)

>>> test_sql = []
>>> delete_app = DeleteApplication()
>>> for app_label in d.deleted.keys():
...     test_sql.append(delete_app.mutate(app_label, test_sig))
...     delete_app.simulate(app_label, test_sig)

>>> Diff(test_sig, deleted_app_sig).is_empty(ignore_apps=True)
True

>>> for sql_list in test_sql:
...     for sql in sql_list:
...         print sql
%(DeleteApplicationWithoutDatabase)s

>>> test_sql = []
>>> delete_app = DeleteApplication()
>>> for app_label in d.deleted.keys():
...     test_sql.append(delete_app.mutate(app_label, test_sig, 'default'))
...     delete_app.simulate(app_label, test_sig)

>>> Diff(test_sig, deleted_app_sig).is_empty(ignore_apps=True)
True

>>> for sql_list in test_sql:
...     for sql in sql_list:
...         print sql
%(DeleteApplication)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('delete_application')

########NEW FILE########
__FILENAME__ = delete_field
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models

>>> from django_evolution.mutations import DeleteField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff

>>> import copy

# All Fields
# db index (ignored for now)
# db tablespace (ignored for now)
# db column
# primary key
# unique

# M2M Fields
# to field
# db table

# Model Meta
# db table
# db tablespace (ignored for now)
# unique together (ignored for now)

# Now, a useful test model we can use for evaluating diffs
>>> class DeleteAnchor1(models.Model):
...     value = models.IntegerField()
>>> class DeleteAnchor2(models.Model):
...     value = models.IntegerField()
>>> class DeleteAnchor3(models.Model):
...     value = models.IntegerField()
>>> class DeleteAnchor4(models.Model):
...     value = models.IntegerField()

>>> class DeleteBaseModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     int_field3 = models.IntegerField(unique=True)
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> class CustomTableModel(models.Model):
...     value = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'custom_table_name'

# Store the base signatures
>>> anchors = (
...     ('DeleteAnchor1', DeleteAnchor1),
...     ('DeleteAnchor2', DeleteAnchor2),
...     ('DeleteAnchor3', DeleteAnchor3),
...     ('DeleteAnchor4', DeleteAnchor4),
... )

>>> custom_model = ('CustomTableModel', CustomTableModel)
>>> custom = register_models(custom_model)
>>> custom_sig = test_proj_sig(custom_model)

>>> test_model = ('TestModel', DeleteBaseModel)
>>> start = register_models(*anchors)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, *anchors)

# Deleting a default named column
>>> class DefaultNamedColumnModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     int_field3 = models.IntegerField(unique=True)
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> end = register_models(('TestModel', DefaultNamedColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', DefaultNamedColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'int_field')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #DefaultNamedColumnModel
%(DefaultNamedColumnModel)s

# Deleting a non-default named column
>>> class NonDefaultNamedColumnModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field3 = models.IntegerField(unique=True)
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> end = register_models(('TestModel', NonDefaultNamedColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', NonDefaultNamedColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'int_field2')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #NonDefaultNamedColumnModel
%(NonDefaultNamedColumnModel)s

# Deleting a column with database constraints (unique)
# TODO: Verify that the produced SQL is actually correct
# -- BK
>>> class ConstrainedColumnModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> end = register_models(('TestModel', ConstrainedColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', ConstrainedColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'int_field3')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #ConstrainedColumnModel
%(ConstrainedColumnModel)s

# Deleting a default m2m
>>> class DefaultM2MModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     int_field3 = models.IntegerField(unique=True)
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> end = register_models(('TestModel', DefaultM2MModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', DefaultM2MModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'm2m_field1')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #DefaultManyToManyModel
%(DefaultManyToManyModel)s

# Deleting a m2m stored in a non-default table
>>> class NonDefaultM2MModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     int_field3 = models.IntegerField(unique=True)
...     fk_field1 = models.ForeignKey(DeleteAnchor1)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)

>>> end = register_models(('TestModel', NonDefaultM2MModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', NonDefaultM2MModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'm2m_field2')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #NonDefaultManyToManyModel
%(NonDefaultManyToManyModel)s

# Delete a foreign key
>>> class DeleteForeignKeyModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field2 = models.IntegerField(db_column='non-default_db_column')
...     int_field3 = models.IntegerField(unique=True)
...     m2m_field1 = models.ManyToManyField(DeleteAnchor3)
...     m2m_field2 = models.ManyToManyField(DeleteAnchor4, db_table='non-default_m2m_table')

>>> end = register_models(('TestModel', DeleteForeignKeyModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', DeleteForeignKeyModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'fk_field1')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #DeleteForeignKeyModel
%(DeleteForeignKeyModel)s

# Deleting a column from a non-default table
>>> class DeleteColumnCustomTableModel(models.Model):
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'custom_table_name'

>>> end = register_models(('CustomTableModel', DeleteColumnCustomTableModel))
>>> end_sig = test_proj_sig(('CustomTableModel', DeleteColumnCustomTableModel))
>>> d = Diff(custom_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('CustomTableModel', 'value')"]

>>> test_sig = copy.deepcopy(custom_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(custom, end, test_sql) #DeleteColumnCustomTableModel
%(DeleteColumnCustomTableModel)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('delete_field')

########NEW FILE########
__FILENAME__ = delete_model
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models

>>> from django_evolution.mutations import DeleteModel
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff

>>> import copy

# Now, a useful test model we can use for evaluating diffs
>>> class DeleteModelAnchor(models.Model):
...     value = models.IntegerField()
>>> class BasicModel(models.Model):
...     value = models.IntegerField()
>>> class BasicWithM2MModel(models.Model):
...     value = models.IntegerField()
...     m2m = models.ManyToManyField(DeleteModelAnchor)
>>> class CustomTableModel(models.Model):
...     value = models.IntegerField()
...     class Meta:
...         db_table = 'custom_table_name'
>>> class CustomTableWithM2MModel(models.Model):
...     value = models.IntegerField()
...     m2m = models.ManyToManyField(DeleteModelAnchor)
...     class Meta:
...         db_table = 'another_custom_table_name'

# Store the base signature
>>> base_models = (
...     ('DeleteModelAnchor', DeleteModelAnchor),
...     ('BasicModel', BasicModel),
...     ('BasicWithM2MModel', BasicWithM2MModel),
...     ('CustomTableModel', CustomTableModel),
...     ('CustomTableWithM2MModel', CustomTableWithM2MModel),
... )

>>> start = register_models(*base_models)
>>> start_sig = test_proj_sig(*base_models)

# Delete a Model
>>> end_sig = copy.deepcopy(start_sig)
>>> _ = end_sig['tests'].pop('BasicModel')
>>> end = copy.deepcopy(start)
>>> _ = end.pop('basicmodel')

>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteModel('BasicModel')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #BasicModel
%(BasicModel)s

# Delete a model with an m2m field
>>> end_sig = copy.deepcopy(start_sig)
>>> _ = end_sig['tests'].pop('BasicWithM2MModel')
>>> end = copy.deepcopy(start)
>>> _ = end.pop('basicwithm2mmodel')

>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteModel('BasicWithM2MModel')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # BasicWithM2MModels
%(BasicWithM2MModel)s

# Delete a model with a custom table name
>>> end_sig = copy.deepcopy(start_sig)
>>> _ = end_sig['tests'].pop('CustomTableModel')
>>> end = copy.deepcopy(start)
>>> _ = end.pop('customtablemodel')

>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteModel('CustomTableModel')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #CustomTableModel
%(CustomTableModel)s

# Delete a model with a custom table name and an m2m field
>>> end_sig = copy.deepcopy(start_sig)
>>> _ = end_sig['tests'].pop('CustomTableWithM2MModel')

>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteModel('CustomTableWithM2MModel')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #CustomTableWithM2MModel
%(CustomTableWithM2MModel)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('delete_model')

########NEW FILE########
__FILENAME__ = generics
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models

>>> from django_evolution.mutations import DeleteField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django.contrib.contenttypes import generic
>>> from django.contrib.contenttypes.models import ContentType

>>> import copy

# Now, a useful test model we can use for evaluating diffs
>>> class GenericAnchor(models.Model):
...     value = models.IntegerField()
...     # Host a generic key here, too
...     content_type = models.ForeignKey(ContentType)
...     object_id = models.PositiveIntegerField(db_index=True)
...     content_object = generic.GenericForeignKey('content_type','object_id')

>>> class GenericBaseModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     # Plus a generic foreign key - the Generic itself should be ignored
...     content_type = models.ForeignKey(ContentType)
...     object_id = models.PositiveIntegerField(db_index=True)
...     content_object = generic.GenericForeignKey('content_type','object_id')
...     # Plus a generic relation, which should be ignored
...     generic = generic.GenericRelation(GenericAnchor)

# Store the base signatures
>>> anchor = ('Anchor', GenericAnchor)
>>> content_type = ('contenttypes.ContentType', ContentType)
>>> test_model = ('TestModel', GenericBaseModel)
>>> start = register_models(anchor)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, content_type, anchor)

# Delete a column
>>> class DeleteColumnModel(models.Model):
...     int_field = models.IntegerField()
...     # Plus a generic foreign key - the Generic itself should be ignored
...     content_type = models.ForeignKey(ContentType)
...     object_id = models.PositiveIntegerField(db_index=True)
...     content_object = generic.GenericForeignKey('content_type','object_id')
...     # Plus a generic relation, which should be ignored
...     generic = generic.GenericRelation(GenericAnchor)

>>> end = register_models(('TestModel', DeleteColumnModel), anchor)
>>> end_sig = test_proj_sig(('TestModel', DeleteColumnModel), content_type, anchor)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'char_field')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #DeleteColumnModel
%(DeleteColumnModel)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('generics')

########NEW FILE########
__FILENAME__ = inheritance
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from datetime import datetime

>>> from django.db import models

>>> from django_evolution.mutations import AddField, DeleteField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django_evolution import signature
>>> from django_evolution import models as test_app

>>> import copy

>>> class ParentModel(models.Model):
...     parent_field = models.CharField(max_length=20)
...     other_field = models.IntegerField()

>>> class ChildModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()

# Store the base signatures
>>> parent_model = ('ParentModel', ParentModel)
>>> parent = register_models(parent_model)
>>> parent_table_sig = test_proj_sig(parent_model)

>>> test_model = ('ChildModel', ChildModel)
>>> start = register_models(test_model)
>>> start_sig = test_proj_sig(test_model, parent_model)

# Add field to child model
>>> class AddToChildModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field = models.IntegerField(default=42)

>>> end = register_models(('ChildModel', AddToChildModel), parent_model)
>>> end_sig = test_proj_sig(('ChildModel',AddToChildModel), parent_model)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] # AddToChildModel
["AddField('ChildModel', 'added_field', models.IntegerField, initial=42)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # AddToChildModel
%(AddToChildModel)s

# Delete field from child model
>>> class AddToChildModel(models.Model):
...     char_field = models.CharField(max_length=20)

>>> end = register_models(('ChildModel', AddToChildModel), parent_model)
>>> end_sig = test_proj_sig(('ChildModel',AddToChildModel), parent_model)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']] # DeleteFromChildModel
["DeleteField('ChildModel', 'int_field')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) # DeleteFromChildModel
%(DeleteFromChildModel)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('inheritance')

########NEW FILE########
__FILENAME__ = models
# This module is used as a placeholder for the registration of test models.
# It is intentionally empty; individual tests create and register models
# that will appear to Django as if they are in this module.

########NEW FILE########
__FILENAME__ = multi_db
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models

>>> from django_evolution.mutations import ChangeField
>>> from django_evolution.tests.utils import test_proj_sig_multi, execute_test_sql, register_models_multi, deregister_models
>>> from django_evolution.diff import Diff

>>> import copy

# Use Cases:
# Setting a null constraint
# -- without an initial value
# -- with a null initial value
# -- with a good initial value (constant)
# -- with a good initial value (callable)
# Removing a null constraint
# Invoking a no-op change field
# Changing the max_length of a character field
# -- increasing the max_length
# -- decreasing the max_length
# Renaming a column
# Changing the db_table of a many to many relationship
# Adding an index
# Removing an index
# Adding a unique constraint
# Removing a unique constraint
# Redundant attributes. (Some attribute have changed, while others haven't but are specified anyway.)
# Changing more than one attribute at a time (on different fields)
# Changing more than one attribute at a time (on one field)


### This one is a bit dubious because changing the primary key of a model will mean
### that all referenced foreign keys and M2M relationships need to be updated
# Adding a primary key constraint
# Removing a Primary Key (Changing the primary key column)



# Options that apply to all fields:
# DB related options
# null
# db_column
# db_index
# db_tablespace (Ignored)
# primary_key
# unique
# db_table (only for many to many relationships)
# -- CharField
# max_length

# Non-DB options
# blank
# core
# default
# editable
# help_text
# radio_admin
# unique_for_date
# unique_for_month
# unique_for_year
# validator_list

# I don't know yet
# choices

>>> class ChangeSequenceFieldInitial(object):
...     def __init__(self, suffix):
...         self.suffix = suffix
...
...     def __call__(self):
...         from django.db import connections
...         qn = connections['db_multi'].ops.quote_name
...         return qn('char_field')

# Now, a useful test model we can use for evaluating diffs
>>> class ChangeAnchor1(models.Model):
...     value = models.IntegerField()

>>> class ChangeBaseModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

# Store the base signatures
>>> anchors = [('ChangeAnchor1', ChangeAnchor1)]
>>> test_model = ('TestModel', ChangeBaseModel)

>>> start = register_models_multi('tests', 'db_multi', *anchors)
>>> start.update(register_models_multi('tests', 'db_multi', test_model))
>>> start_sig = test_proj_sig_multi('tests', test_model, *anchors)

# Setting a null constraint without an initial value
>>> class SetNotNullChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=False)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', SetNotNullChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', SetNotNullChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field1':
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # SetNotNullChangeModel
["ChangeField('TestModel', 'char_field1', initial=<<USER VALUE REQUIRED>>, null=False)"]

# Without an initial value
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot change column 'char_field1' on 'tests.TestModel' without a non-null initial value.

# With a null initial value
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial=None)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
SimulationFailure: Cannot change column 'char_field1' on 'tests.TestModel' without a non-null initial value.

# With a good initial value (constant)
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial="abc's xyz")]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # SetNotNullChangedModelWithConstant
%(SetNotNullChangeModelWithConstant)s

# With a good initial value (callable)
>>> evolution = [ChangeField('TestModel', 'char_field1', null=False, initial=ChangeSequenceFieldInitial('SetNotNullChangeModel'))]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # SetNotNullChangeModelWithCallable
%(SetNotNullChangeModelWithCallable)s

# Removing a null constraint
>>> class SetNullChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', SetNullChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', SetNullChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # SetNullChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # SetNullChangeModel
%(SetNullChangeModel)s

# Removing a null constraint
>>> class NoOpChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', NoOpChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', NoOpChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
<BLANKLINE>

>>> evolution = [ChangeField('TestModel', 'char_field1', null=True)]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # NoOpChangeModel
%(NoOpChangeModel)s

# Increasing the max_length of a character field
>>> class IncreasingMaxLengthChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=45)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', IncreasingMaxLengthChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', IncreasingMaxLengthChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # IncreasingMaxLengthChangeModel
["ChangeField('TestModel', 'char_field', initial=None, max_length=45)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # IncreasingMaxLengthChangeModel
%(IncreasingMaxLengthChangeModel)s

# Decreasing the max_length of a character field
>>> class DecreasingMaxLengthChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=1)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', DecreasingMaxLengthChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', DecreasingMaxLengthChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # DecreasingMaxLengthChangeModel
["ChangeField('TestModel', 'char_field', initial=None, max_length=1)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # DecreasingMaxLengthChangeModel
%(DecreasingMaxLengthChangeModel)s

# Renaming a column
>>> class DBColumnChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='customised_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', DBColumnChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', DBColumnChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field':
        Property 'db_column' has changed

>>> print [str(e) for e in d.evolution()['tests']] # DBColumnChangeModel
["ChangeField('TestModel', 'int_field', initial=None, db_column='customised_db_column')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # DBColumnChangeModel
%(DBColumnChangeModel)s

# Changing the db_table of a many to many relationship
>>> class M2MDBTableChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='custom_m2m_db_table_name')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', M2MDBTableChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', M2MDBTableChangeModel), *anchors)

>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'm2m_field1':
        Property 'db_table' has changed

>>> print [str(e) for e in d.evolution()['tests']] # M2MDBTableChangeModel
["ChangeField('TestModel', 'm2m_field1', initial=None, db_table='custom_m2m_db_table_name')"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # M2MDBTableChangeModel
%(M2MDBTableChangeModel)s

# Adding an index
>>> class AddDBIndexChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=True)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', AddDBIndexChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', AddDBIndexChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field2':
        Property 'db_index' has changed

>>> print [str(e) for e in d.evolution()['tests']] # AddDBIndexChangeModel
["ChangeField('TestModel', 'int_field2', initial=None, db_index=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # AddDBIndexChangeModel
%(AddDBIndexChangeModel)s

# Removing an index
>>> class RemoveDBIndexChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=False)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', RemoveDBIndexChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', RemoveDBIndexChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field1':
        Property 'db_index' has changed

>>> print [str(e) for e in d.evolution()['tests']] # RemoveDBIndexChangeModel
["ChangeField('TestModel', 'int_field1', initial=None, db_index=False)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # RemoveDBIndexChangeModel
%(RemoveDBIndexChangeModel)s

# Adding a unique constraint
>>> class AddUniqueChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=True)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', AddUniqueChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', AddUniqueChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field4':
        Property 'unique' has changed

>>> print [str(e) for e in d.evolution()['tests']] # AddUniqueChangeModel
["ChangeField('TestModel', 'int_field4', initial=None, unique=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # AddUniqueChangeModel
%(AddUniqueChangeModel)s

# Remove a unique constraint
>>> class RemoveUniqueChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=False)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', RemoveUniqueChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', RemoveUniqueChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'int_field3':
        Property 'unique' has changed

>>> print [str(e) for e in d.evolution()['tests']] # RemoveUniqueChangeModel
["ChangeField('TestModel', 'int_field3', initial=None, unique=False)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # RemoveUniqueChangeModel
%(RemoveUniqueChangeModel)s

# Changing more than one attribute at a time (on different fields)
>>> class MultiAttrChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column2')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=35)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', MultiAttrChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', MultiAttrChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'null' has changed
    In field 'int_field':
        Property 'db_column' has changed
    In field 'char_field':
        Property 'max_length' has changed

>>> print [str(e) for e in d.evolution()['tests']] # MultiAttrChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, null=True)", "ChangeField('TestModel', 'int_field', initial=None, db_column='custom_db_column2')", "ChangeField('TestModel', 'char_field', initial=None, max_length=35)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # MultiAttrChangeModel
%(MultiAttrChangeModel)s

# Changing more than one attribute at a time (on one fields)
>>> class MultiAttrSingleFieldChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=35, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', MultiAttrSingleFieldChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', MultiAttrSingleFieldChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print d
In model tests.TestModel:
    In field 'char_field2':
        Property 'max_length' has changed
        Property 'null' has changed

>>> print [str(e) for e in d.evolution()['tests']] # MultiAttrSingleFieldChangeModel
["ChangeField('TestModel', 'char_field2', initial=None, max_length=35, null=True)"]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in d.evolution()['tests']:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # MultiAttrSingleFieldChangeModel
%(MultiAttrSingleFieldChangeModel)s

# Redundant attributes. (Some attribute have changed, while others haven't but are specified anyway.)
>>> class RedundantAttrsChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column3')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = models.IntegerField(unique=False)
...     char_field = models.CharField(max_length=35)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=True)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', RedundantAttrsChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', RedundantAttrsChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> evolutions = [
...     ChangeField("TestModel", "char_field2", initial=None, null=True, max_length=30),
...     ChangeField("TestModel", "int_field", initial=None, db_column="custom_db_column3", primary_key=False, unique=False, db_index=False),
...     ChangeField("TestModel", "char_field", initial=None, max_length=35),
... ]

>>> for mutation in evolutions:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql, database='db_multi', app_label='tests') # RedundantAttrsChangeModel
%(RedundantAttrsChangeModel)s

# Change field type to another type with same internal_type
>>> class MyIntegerField(models.IntegerField):
...     def get_internal_type(self):
...         return 'IntegerField'

>>> class MinorFieldTypeChangeModel(models.Model):
...     my_id = models.AutoField(primary_key=True)
...     alt_pk = models.IntegerField()
...     int_field = models.IntegerField(db_column='custom_db_column')
...     int_field1 = models.IntegerField(db_index=True)
...     int_field2 = models.IntegerField(db_index=False)
...     int_field3 = models.IntegerField(unique=True)
...     int_field4 = MyIntegerField(unique=False)
...     char_field = models.CharField(max_length=20)
...     char_field1 = models.CharField(max_length=25, null=True)
...     char_field2 = models.CharField(max_length=30, null=False)
...     m2m_field1 = models.ManyToManyField(ChangeAnchor1, db_table='multi_db_non-default_m2m_table')

>>> end = register_models_multi('tests', 'db_multi', ('TestModel', MinorFieldTypeChangeModel), *anchors)
>>> end_sig = test_proj_sig_multi('tests', ('TestModel', MinorFieldTypeChangeModel), *anchors)
>>> d = Diff(start_sig, end_sig)

>>> d.is_empty()
True

# Clean up after the applications that were installed
>>> deregister_models('tests')

""" % test_sql_mapping('multi_db', db_name='db_multi')

########NEW FILE########
__FILENAME__ = ordering
tests = r"""
>>> from django.db import models

>>> from django_evolution.tests.utils import test_proj_sig, register_models, deregister_models
>>> from django_evolution.diff import Diff

>>> import copy

>>> class Case41Anchor(models.Model):
...     value = models.IntegerField()

>>> class Case41Model(models.Model):
...     value = models.IntegerField()
...     ref = models.ForeignKey(Case41Anchor)

# Store the base signatures
>>> anchors = (
...     ('Case41Anchor', Case41Anchor),
... )

>>> test_model = ('TestModel', Case41Model)
>>> start = register_models(*anchors)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, *anchors)

# Regression case 41: If deleteing a model and a foreign key to that model,
# The key deletion needs to happen before the model deletion.

# Delete the foreign key...
>>> class UpdatedCase41Model(models.Model):
...     value = models.IntegerField()

>>> end = register_models(('TestModel', UpdatedCase41Model), *anchors)
>>> end_sig = test_proj_sig(('TestModel',UpdatedCase41Model), *anchors)

# ... And also delete the model that was being referenced
>>> _ = end_sig['tests'].pop('Case41Anchor')

# The evolution sequence needs
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["DeleteField('TestModel', 'ref')", "DeleteModel('Case41Anchor')"]

# Clean up after the applications that were installed
>>> deregister_models()

"""

########NEW FILE########
__FILENAME__ = rename_field
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
# Rename a database column (done)
# RenameField with a specified db table for a field other than a M2MField is allowed (but will be ignored) (done)
# Rename a primary key database column (done)
# Rename a foreign key database column (done)

# Rename a database column with a non-default name to a default name (done)
# Rename a database column with a non-default name to a different non-default name (done)
# RenameField with a specified db column and db table is allowed (but one will be ignored) (done)

# Rename a database column in a non-default table (done)

# Rename an indexed database column (Redundant, Not explicitly tested)
# Rename a database column with null constraints (Redundant, Not explicitly tested)

# Rename a M2M database table (done)
# RenameField with a specified db column for a M2MField is allowed (but will be ignored) (done)
# Rename a M2M non-default database table to a default name (done)

>>> from django.db import models
>>> from django_evolution.mutations import RenameField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django_evolution import signature
>>> from django_evolution import models as test_app

>>> import copy

>>> class RenameAnchor1(models.Model):
...     value = models.IntegerField()

>>> class RenameAnchor2(models.Model):
...     value = models.IntegerField()
...     class Meta:
...         db_table = 'custom_rename_anchor_table'

>>> class RenameAnchor3(models.Model):
...     value = models.IntegerField()

>>> class RenameBaseModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> class CustomRenameTableModel(models.Model):
...     value = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'custom_rename_table_name'

# Store the base signatures
>>> anchors = [
...     ('RenameAnchor1', RenameAnchor1),
...     ('RenameAnchor2', RenameAnchor2),
...     ('RenameAnchor3',RenameAnchor3)
... ]
>>> test_model = ('TestModel', RenameBaseModel)
>>> custom_model = ('CustomTableModel', CustomRenameTableModel)

>>> custom = register_models(custom_model)
>>> custom_table_sig = test_proj_sig(custom_model)

>>> start = register_models(*anchors)
>>> start.update(register_models(test_model))
>>> start_sig = test_proj_sig(test_model, *anchors)

# Rename a database column
>>> class RenameColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     renamed_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel', RenameColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>)", "DeleteField('TestModel', 'int_field')"]

>>> evolution = [RenameField('TestModel', 'int_field', 'renamed_field')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameColumnModel
%(RenameColumnModel)s

# RenameField with a specified db table for a field other than a M2MField is allowed (but will be ignored) (done)
>>> class RenameColumnWithTableNameModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     renamed_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameColumnWithTableNameModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameColumnWithTableNameModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>)", "DeleteField('TestModel', 'int_field')"]

>>> evolution = [RenameField('TestModel', 'int_field', 'renamed_field', db_table='ignored_db-table')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameColumnWithTableNameModel
%(RenameColumnWithTableNameModel)s

# Rename a primary key database column
>>> class RenamePrimaryKeyColumnModel(models.Model):
...     my_pk_id = models.AutoField(primary_key=True)
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenamePrimaryKeyColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenamePrimaryKeyColumnModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'my_pk_id', models.AutoField, initial=<<USER VALUE REQUIRED>>, primary_key=True)", "DeleteField('TestModel', 'id')"]

>>> evolution = [RenameField('TestModel', 'id', 'my_pk_id')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenamePrimaryKeyColumnModel
%(RenamePrimaryKeyColumnModel)s

# Rename a foreign key database column
>>> class RenameForeignKeyColumnModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     renamed_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameForeignKeyColumnModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameForeignKeyColumnModel), *anchors)
>>> start_sig = copy.deepcopy(start_sig)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.ForeignKey, initial=<<USER VALUE REQUIRED>>, related_model='tests.RenameAnchor1')", "DeleteField('TestModel', 'fk_field')"]

>>> evolution = [RenameField('TestModel', 'fk_field', 'renamed_field')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

# FIXME!! This test doesn't work on Postgres
#>>> execute_test_sql(start, end, test_sql) #RenameForeignKeyColumnModel
#%(RenameForeignKeyColumnModel)s

# Rename a database column with a non-default name
>>> class RenameNonDefaultColumnNameModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     renamed_field = models.IntegerField()
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameNonDefaultColumnNameModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameNonDefaultColumnNameModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>)", "DeleteField('TestModel', 'int_field_named')"]

>>> evolution = [RenameField('TestModel', 'int_field_named', 'renamed_field')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameNonDefaultColumnNameModel
%(RenameNonDefaultColumnNameModel)s

# Rename a database column with a non-default name to a different non-default name
>>> class RenameNonDefaultColumnNameToNonDefaultNameModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     renamed_field = models.IntegerField(db_column='non-default_column_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameNonDefaultColumnNameToNonDefaultNameModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameNonDefaultColumnNameToNonDefaultNameModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>, db_column='non-default_column_name')", "DeleteField('TestModel', 'int_field_named')"]

>>> evolution = [RenameField('TestModel', 'int_field_named', 'renamed_field', db_column='non-default_column_name')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameNonDefaultColumnNameToNonDefaultNameModel
%(RenameNonDefaultColumnNameToNonDefaultNameModel)s

# RenameField with a specified db column and db table is allowed (but one will be ignored)
>>> class RenameNonDefaultColumnNameToNonDefaultNameAndTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     renamed_field = models.IntegerField(db_column='non-default_column_name2')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameNonDefaultColumnNameToNonDefaultNameAndTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameNonDefaultColumnNameToNonDefaultNameAndTableModel), *anchors)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>, db_column='non-default_column_name2')", "DeleteField('TestModel', 'int_field_named')"]

>>> evolution = [RenameField('TestModel', 'int_field_named', 'renamed_field', db_column='non-default_column_name2', db_table='custom_ignored_db-table')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameNonDefaultColumnNameToNonDefaultNameAndTableModel
%(RenameNonDefaultColumnNameToNonDefaultNameAndTableModel)s

# Rename a database column in a non-default table
# Rename a database column
>>> class RenameColumnCustomTableModel(models.Model):
...     renamed_field = models.IntegerField()
...     alt_value = models.CharField(max_length=20)
...     class Meta:
...         db_table = 'custom_rename_table_name'

>>> end = register_models(('CustomTableModel', RenameColumnCustomTableModel))
>>> end_sig = test_proj_sig(('CustomTableModel',RenameColumnCustomTableModel))
>>> d = Diff(custom_table_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('CustomTableModel', 'renamed_field', models.IntegerField, initial=<<USER VALUE REQUIRED>>)", "DeleteField('CustomTableModel', 'value')"]

>>> evolution = [RenameField('CustomTableModel', 'value', 'renamed_field')]
>>> test_sig = copy.deepcopy(custom_table_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(custom, end, test_sql) #RenameColumnCustomTableModel
%(RenameColumnCustomTableModel)s

# Rename a M2M database table
>>> class RenameM2MTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     renamed_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameM2MTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameM2MTableModel), *anchors)
>>> start_sig = copy.deepcopy(start_sig)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.ManyToManyField, related_model='tests.RenameAnchor2')", "DeleteField('TestModel', 'm2m_field')"]

>>> evolution = [RenameField('TestModel', 'm2m_field', 'renamed_field')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True
>>> execute_test_sql(start, end, test_sql) #RenameManyToManyTableModel
%(RenameManyToManyTableModel)s

# RenameField with a specified db column for a M2MField is allowed (but will be ignored)
>>> class RenameM2MTableWithColumnNameModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     renamed_field = models.ManyToManyField(RenameAnchor2)
...     m2m_field_named = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameM2MTableWithColumnNameModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameM2MTableWithColumnNameModel), *anchors)
>>> start_sig = copy.deepcopy(start_sig)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.ManyToManyField, related_model='tests.RenameAnchor2')", "DeleteField('TestModel', 'm2m_field')"]

>>> evolution = [RenameField('TestModel', 'm2m_field', 'renamed_field', db_column='ignored_db-column')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #RenameManyToManyTableWithColumnNameModel
%(RenameManyToManyTableWithColumnNameModel)s

# Rename a M2M non-default database table to a default name
>>> class RenameNonDefaultM2MTableModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     int_field_named = models.IntegerField(db_column='custom_db_col_name')
...     int_field_named_indexed = models.IntegerField(db_column='custom_db_col_name_indexed', db_index=True)
...     fk_field = models.ForeignKey(RenameAnchor1)
...     m2m_field = models.ManyToManyField(RenameAnchor2)
...     renamed_field = models.ManyToManyField(RenameAnchor3, db_table='non-default_db_table')

>>> end = register_models(('TestModel', RenameNonDefaultM2MTableModel), *anchors)
>>> end_sig = test_proj_sig(('TestModel',RenameNonDefaultM2MTableModel), *anchors)
>>> start_sig = copy.deepcopy(start_sig)
>>> d = Diff(start_sig, end_sig)
>>> print [str(e) for e in d.evolution()['tests']]
["AddField('TestModel', 'renamed_field', models.ManyToManyField, db_table='non-default_db_table', related_model='tests.RenameAnchor3')", "DeleteField('TestModel', 'm2m_field_named')"]

>>> evolution = [RenameField('TestModel', 'm2m_field_named', 'renamed_field')]
>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in evolution:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
False

# FIXME!! This test fails under Postgres
#>>> execute_test_sql(start, end, test_sql) #RenameNonDefaultManyToManyTableModel
#%(RenameNonDefaultManyToManyTableModel)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('rename_field')

########NEW FILE########
__FILENAME__ = signature

tests = r"""
>>> from django.db import models
>>> from django_evolution import signature
>>> from django_evolution.diff import Diff
>>> from django_evolution.tests.utils import test_proj_sig, register_models, deregister_models
>>> from pprint import pprint
>>> from django.contrib.contenttypes import generic
>>> from django.contrib.contenttypes.models import ContentType

# First, a model that has one of everything so we can validate all cases for a signature
>>> class Anchor1(models.Model):
...     value = models.IntegerField()
>>> class Anchor2(models.Model):
...     value = models.IntegerField()
>>> class Anchor3(models.Model):
...     value = models.IntegerField()
...     # Host a generic key here, too
...     content_type = models.ForeignKey(ContentType)
...     object_id = models.PositiveIntegerField(db_index=True)
...     content_object = generic.GenericForeignKey('content_type','object_id')

>>> anchors = [('Anchor1', Anchor1),('Anchor2', Anchor2),('Anchor3', Anchor3)]

>>> class SigModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     null_field = models.IntegerField(null=True, db_column='size_column')
...     id_card = models.IntegerField(unique=True, db_index=True)
...     dec_field = models.DecimalField(max_digits=10, decimal_places=4)
...     ref1 = models.ForeignKey(Anchor1)
...     ref2 = models.ForeignKey(Anchor1, related_name='other_sigmodel')
...     ref3 = models.ForeignKey(Anchor2, db_column='value', db_index=True)
...     ref4 = models.ForeignKey('self')
...     ref5 = models.ManyToManyField(Anchor3)
...     ref6 = models.ManyToManyField(Anchor3, related_name='other_sigmodel')
...     ref7 = models.ManyToManyField('self')
...     # Plus a generic foreign key - the Generic itself should be ignored
...     content_type = models.ForeignKey(ContentType)
...     object_id = models.PositiveIntegerField(db_index=True)
...     content_object = generic.GenericForeignKey('content_type','object_id')
...     # Plus a generic relation, which should be ignored
...     generic = generic.GenericRelation(Anchor3)

>>> class ParentModel(models.Model):
...     parent_field = models.CharField(max_length=20)

>>> class ChildModel(ParentModel):
...     child_field = models.CharField(max_length=20)

# Store the base signatures
>>> base_cache = register_models(('Anchor1', Anchor1), ('Anchor2', Anchor2), ('Anchor3', Anchor3), ('TestModel', SigModel), ('ParentModel',ParentModel), ('ChildModel',ChildModel))

# You can create a model signature for a model
>>> pprint(signature.create_model_sig(SigModel))
{'fields': {'char_field': {'field_type': <class 'django.db.models.fields.CharField'>,
                           'max_length': 20},
            'content_type': {'field_type': <class 'django.db.models.fields.related.ForeignKey'>,
                             'related_model': 'contenttypes.ContentType'},
            'dec_field': {'decimal_places': 4,
                          'field_type': <class 'django.db.models.fields.DecimalField'>,
                          'max_digits': 10},
            'id': {'field_type': <class 'django.db.models.fields.AutoField'>,
                   'primary_key': True},
            'id_card': {'db_index': True,
                        'field_type': <class 'django.db.models.fields.IntegerField'>,
                        'unique': True},
            'int_field': {'field_type': <class 'django.db.models.fields.IntegerField'>},
            'null_field': {'db_column': 'size_column',
                           'field_type': <class 'django.db.models.fields.IntegerField'>,
                           'null': True},
            'object_id': {'db_index': True,
                          'field_type': <class 'django.db.models.fields.PositiveIntegerField'>},
            'ref1': {'field_type': <class 'django.db.models.fields.related.ForeignKey'>,
                     'related_model': 'tests.Anchor1'},
            'ref2': {'field_type': <class 'django.db.models.fields.related.ForeignKey'>,
                     'related_model': 'tests.Anchor1'},
            'ref3': {'db_column': 'value',
                     'field_type': <class 'django.db.models.fields.related.ForeignKey'>,
                     'related_model': 'tests.Anchor2'},
            'ref4': {'field_type': <class 'django.db.models.fields.related.ForeignKey'>,
                     'related_model': 'tests.TestModel'},
            'ref5': {'field_type': <class 'django.db.models.fields.related.ManyToManyField'>,
                     'related_model': 'tests.Anchor3'},
            'ref6': {'field_type': <class 'django.db.models.fields.related.ManyToManyField'>,
                     'related_model': 'tests.Anchor3'},
            'ref7': {'field_type': <class 'django.db.models.fields.related.ManyToManyField'>,
                     'related_model': 'tests.TestModel'}},
 'meta': {'db_table': 'tests_testmodel',
          'db_tablespace': '',
          'pk_column': 'id',
          'unique_together': []}}

>>> pprint(signature.create_model_sig(ChildModel))
{'fields': {'child_field': {'field_type': <class 'django.db.models.fields.CharField'>,
                            'max_length': 20},
            'parentmodel_ptr': {'field_type': <class 'django.db.models.fields.related.OneToOneField'>,
                                'primary_key': True,
                                'related_model': 'tests.ParentModel',
                                'unique': True}},
 'meta': {'db_table': 'tests_childmodel',
          'db_tablespace': '',
          'pk_column': 'parentmodel_ptr_id',
          'unique_together': []}}

# Now, a useful test model we can use for evaluating diffs
>>> class BaseModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor1)
>>> start = register_models(('TestModel', BaseModel), *anchors)

>>> start_sig = test_proj_sig(('TestModel', BaseModel), *anchors)

# An identical model gives an empty Diff
>>> class TestModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', TestModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',TestModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
True
>>> d.evolution()
{}

# Adding a field gives a non-empty diff
>>> class AddFieldModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor1)
...     date_of_birth = models.DateField()

>>> end = register_models(('TestModel', AddFieldModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',AddFieldModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False
>>> print [str(e) for e in d.evolution()['tests']] # Add Field
["AddField('TestModel', 'date_of_birth', models.DateField, initial=<<USER VALUE REQUIRED>>)"]

# Deleting a field gives a non-empty diff
>>> class DeleteFieldModel(models.Model):
...     name = models.CharField(max_length=20)
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', DeleteFieldModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',DeleteFieldModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False
>>> print [str(e) for e in d.evolution()['tests']] # Delete Field
["DeleteField('TestModel', 'age')"]

# Renaming a field is caught as 2 diffs
# (For the moment - long term, this should hint as a Rename)
>>> class RenameFieldModel(models.Model):
...     full_name = models.CharField(max_length=20)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', RenameFieldModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',RenameFieldModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False
>>> print [str(e) for e in d.evolution()['tests']] # Rename Field
["AddField('TestModel', 'full_name', models.CharField, initial=<<USER VALUE REQUIRED>>, max_length=20)", "DeleteField('TestModel', 'name')"]

# Adding a property to a field which was not present in the original Model
>>> class AddPropertyModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField(null=True)
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', AddPropertyModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',AddPropertyModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False

>>> print [str(e) for e in d.evolution()['tests']] # Change Field - add property
["ChangeField('TestModel', 'age', initial=None, null=True)"]

# Since we can't check the evolutions, check the diff instead
>>> print d
In model tests.TestModel:
    In field 'age':
        Property 'null' has changed

# Adding a property of a field which was not present in the original Model, but
# is now set to the default for that property.
>>> class AddDefaultPropertyModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField(null=False)
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', AddDefaultPropertyModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',AddDefaultPropertyModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
True
>>> print d.evolution()
{}

# Changing a property of a field
>>> class ChangePropertyModel(models.Model):
...     name = models.CharField(max_length=30)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor1)

>>> end = register_models(('TestModel', ChangePropertyModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',ChangePropertyModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False

>>> print [str(e) for e in d.evolution()['tests']] # Change Field - change property
["ChangeField('TestModel', 'name', initial=None, max_length=30)"]

# Since we can't check the evolutions, check the diff instead
>>> print d
In model tests.TestModel:
    In field 'name':
        Property 'max_length' has changed

# Changing the model that a ForeignKey references
>>> class ChangeFKModel(models.Model):
...     name = models.CharField(max_length=20)
...     age = models.IntegerField()
...     ref = models.ForeignKey(Anchor2)

>>> end = register_models(('TestModel', ChangeFKModel), *anchors)
>>> test_sig = test_proj_sig(('TestModel',ChangeFKModel), *anchors)
>>> d = Diff(start_sig, test_sig)
>>> d.is_empty()
False

>>> print [str(e) for e in d.evolution()['tests']] # Change Field - change property
["ChangeField('TestModel', 'ref', initial=None, related_model='tests.Anchor2')"]

# Clean up after the applications that were installed
>>> deregister_models()

"""

########NEW FILE########
__FILENAME__ = sql_mutation
from django_evolution.tests.utils import test_sql_mapping

tests = r"""
>>> from django.db import models
>>> from django_evolution.mutations import SQLMutation

>>> from django.db import models

>>> from django_evolution.mutations import AddField
>>> from django_evolution.tests.utils import test_proj_sig, execute_test_sql, register_models, deregister_models
>>> from django_evolution.diff import Diff
>>> from django_evolution import signature
>>> from django_evolution import models as test_app

>>> import copy

>>> class SQLBaseModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()

# Store the base signatures
>>> start = register_models(('TestModel', SQLBaseModel))
>>> start_sig = test_proj_sig(('TestModel', SQLBaseModel))

# Add 3 Fields resulting in new database columns.
>>> class SQLMutationModel(models.Model):
...     char_field = models.CharField(max_length=20)
...     int_field = models.IntegerField()
...     added_field1 = models.IntegerField(null=True)
...     added_field2 = models.IntegerField(null=True)
...     added_field3 = models.IntegerField(null=True)
>>> end = register_models(('TestModel', SQLMutationModel))
>>> end_sig = test_proj_sig(('TestModel',SQLMutationModel))
>>> d = Diff(start_sig, end_sig)

# Add the fields using SQLMutations
>>> sequence = [
...    SQLMutation('first-two-fields', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field1" integer NULL;',
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field2" integer NULL;'
...    ]),
...    SQLMutation('third-field', [
...        'ALTER TABLE "tests_testmodel" ADD COLUMN "added_field3" integer NULL;',
...    ])]

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in sequence:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)
Traceback (most recent call last):
...
CannotSimulate: Cannot simulate SQLMutations

# Redefine the sequence with update functions.
>>> def update_first_two(app_label, proj_sig):
...     app_sig = proj_sig[app_label]
...     model_sig = app_sig['TestModel']
...     model_sig['fields']['added_field1'] = {
...         'field_type': models.IntegerField,
...         'null': True
...     }
...     model_sig['fields']['added_field2'] = {
...         'field_type': models.IntegerField,
...         'null': True
...     }

>>> def update_third(app_label, proj_sig):
...     app_sig = proj_sig[app_label]
...     model_sig = app_sig['TestModel']
...     model_sig['fields']['added_field3'] = {
...         'field_type': models.IntegerField,
...         'null': True
...     }

>>> sequence = %(SQLMutationSequence)s

>>> test_sig = copy.deepcopy(start_sig)
>>> test_sql = []
>>> for mutation in sequence:
...     test_sql.extend(mutation.mutate('tests', test_sig))
...     mutation.simulate('tests', test_sig)

>>> Diff(test_sig, end_sig).is_empty()
True

>>> execute_test_sql(start, end, test_sql) #SQLMutationOutput
%(SQLMutationOutput)s

# Clean up after the applications that were installed
>>> deregister_models()

""" % test_sql_mapping('sql_mutation')

########NEW FILE########
__FILENAME__ = utils
from datetime import datetime
from django.core.management import sql
from django.core.management.color import no_style
from django.db import connection, transaction, settings, models
from django.db.backends.util import truncate_name
from django.db.models.loading import cache
from django.utils.datastructures import SortedDict
from django.utils.functional import curry
from django_evolution import signature, is_multi_db
from django_evolution.tests import models as evo_test
from django_evolution.utils import write_sql, execute_sql
import copy

if is_multi_db():
    from django.db import connections
    from django.db.utils import DEFAULT_DB_ALIAS


DEFAULT_TEST_ATTRIBUTE_VALUES = {
    models.CharField: 'TestCharField',
    models.IntegerField: '123',
    models.AutoField: None,
    models.DateTimeField: datetime.now(),
    models.PositiveIntegerField: '42'
}


def wrap_sql_func(func, evo_test, style, db_name=None):
    if is_multi_db():
        return func(evo_test, style, connections[db_name or DEFAULT_DB_ALIAS])
    else:
        return func(evo_test, style)

# Wrap the sql.* functions to work with the multi-db support
sql_create = curry(wrap_sql_func, sql.sql_create)
sql_indexes = curry(wrap_sql_func, sql.sql_indexes)
sql_delete = curry(wrap_sql_func, sql.sql_delete)


def _register_models(app_label='tests', db_name='default', *models):
    app_cache = SortedDict()

    my_connection = connection

    if is_multi_db():
        my_connection = connections[db_name or DEFAULT_DB_ALIAS]

    max_name_length = my_connection.ops.max_name_length()

    for name, model in reversed(models):
        if model._meta.module_name in cache.app_models['django_evolution']:
            del cache.app_models['django_evolution'][model._meta.module_name]

            orig_db_table = model._meta.db_table
            orig_object_name = model._meta.object_name
            orig_module_name = model._meta.module_name

            generated_db_table = truncate_name(
                '%s_%s' % (model._meta.app_label, model._meta.module_name),
                max_name_length)

            if orig_db_table.startswith(generated_db_table):
                model._meta.db_table = '%s_%s' % (app_label, name.lower())

            model._meta.db_table = truncate_name(model._meta.db_table,
                                                 max_name_length)
            model._meta.app_label = app_label
            model._meta.object_name = name
            model._meta.module_name = name.lower()

            add_app_test_model(model, app_label=app_label)

            for field in model._meta.local_many_to_many:
                if not field.rel.through:
                    continue

                through = field.rel.through

                generated_db_table = truncate_name(
                    '%s_%s' % (orig_db_table, field.name),
                    max_name_length)

                if through._meta.db_table == generated_db_table:
                    through._meta.app_label = app_label

                    # Transform the 'through' table information only
                    # if we've transformed the parent db_table.
                    if model._meta.db_table != orig_db_table:
                        through._meta.db_table = \
                            '%s_%s' % (model._meta.db_table, field.name)

                        through._meta.object_name = \
                            through._meta.object_name.replace(
                                orig_object_name,
                                model._meta.object_name)

                        through._meta.module_name = \
                            through._meta.module_name.replace(
                                orig_module_name,
                                model._meta.module_name)

                through._meta.db_table = \
                    truncate_name(through._meta.db_table, max_name_length)

                for field in through._meta.local_fields:
                    if field.rel and field.rel.to:
                        column = field.column

                        if (column.startswith(orig_module_name) or
                            column.startswith('to_%s' % orig_module_name) or
                            column.startswith('from_%s' % orig_module_name)):

                            field.column = column.replace(
                                orig_module_name,
                                model._meta.module_name)

                if (through._meta.module_name in
                    cache.app_models['django_evolution']):
                    del cache.app_models['django_evolution'][
                        through._meta.module_name]

                app_cache[through._meta.module_name] = through
                add_app_test_model(through, app_label=app_label)

        app_cache[model._meta.module_name] = model

    return app_cache


def register_models(*models):
    return _register_models('tests', 'default', *models)


def register_models_multi(app_label, db_name, *models):
    return _register_models(app_label, db_name, *models)


def _test_proj_sig(app_label, *models, **kwargs):
    "Generate a dummy project signature based around a single model"
    version = kwargs.get('version', 1)
    proj_sig = {
        app_label: SortedDict(),
        '__version__': version,
    }

    # Compute the project siguature
    for full_name, model in models:
        parts = full_name.split('.')

        if len(parts) == 1:
            name = parts[0]
            app = app_label
        else:
            app, name = parts

        proj_sig.setdefault(app, SortedDict())[name] = \
            signature.create_model_sig(model)

    return proj_sig


def test_proj_sig(*models, **kwargs):
    return _test_proj_sig('tests', *models, **kwargs)


def test_proj_sig_multi(app_label, *models, **kwargs):
    return _test_proj_sig(app_label, *models, **kwargs)


def execute_transaction(sql, output=False, database='default'):
    "A transaction wrapper for executing a list of SQL statements"
    my_connection = connection
    using_args = {}

    if is_multi_db():
        if not database:
            database = DEFAULT_DB_ALIAS

        my_connection = connections[database]
        using_args['using'] = database

    try:
        # Begin Transaction
        transaction.enter_transaction_management(**using_args)
        transaction.managed(True, **using_args)

        cursor = my_connection.cursor()

        # Perform the SQL
        if output:
            write_sql(sql, database)

        execute_sql(cursor, sql)

        transaction.commit(**using_args)
        transaction.leave_transaction_management(**using_args)
    except Exception:
        transaction.rollback(**using_args)
        raise


def execute_test_sql(start, end, sql, debug=False, app_label='tests',
                     database='default'):
    """
    Execute a test SQL sequence. This method also creates and destroys the
    database tables required by the models registered against the test
    application.

    start and end are the start- and end-point states of the application cache.

    sql is the list of sql statements to execute.

    cleanup is a list of extra sql statements required to clean up. This is
    primarily for any extra m2m tables that were added during a test that won't
    be cleaned up by Django's sql_delete() implementation.

    debug is a helper flag. It displays the ALL the SQL that would be executed,
    (including setup and teardown SQL), and executes the Django-derived
    setup/teardown SQL.
    """
    # Set up the initial state of the app cache
    set_app_test_models(copy.deepcopy(start), app_label=app_label)

    # Install the initial tables and indicies
    style = no_style()
    execute_transaction(sql_create(evo_test, style, database),
                        output=debug, database=database)
    execute_transaction(sql_indexes(evo_test, style, database),
                        output=debug, database=database)
    create_test_data(models.get_models(evo_test), database)

    # Set the app cache to the end state
    set_app_test_models(copy.deepcopy(end), app_label=app_label)

    try:
        # Execute the test sql
        if debug:
            write_sql(sql, database)
        else:
            execute_transaction(sql, output=True, database=database)
    finally:
        # Cleanup the apps.
        if debug:
            print sql_delete(evo_test, style, database)
        else:
            execute_transaction(sql_delete(evo_test, style, database),
                                output=debug, database=database)

def create_test_data(app_models, database):
    deferred_models = []
    deferred_fields = {}

    using_args = {}

    if is_multi_db():
        using_args['using'] = database

    for model in app_models:
        params = {}
        deferred = False
        for field in model._meta.fields:
            if not deferred:
                if type(field) in (models.ForeignKey, models.ManyToManyField):
                    related_model = field.rel.to

                    related_q = related_model.objects.all()

                    if is_multi_db():
                        related_q = related_q.using(database)

                    if related_q.count():
                        related_instance = related_q[0]
                    else:
                        if field.null == False:
                            # Field cannot be null yet the related object
                            # hasn't been created yet Defer the creation of
                            # this model
                            deferred = True
                            deferred_models.append(model)
                        else:
                            # Field cannot be set yet but null is acceptable
                            # for the moment
                            deferred_fields[type(model)] = \
                                deferred_fields.get(type(model),
                                                    []).append(field)
                            related_instance = None

                    if not deferred:
                        if type(field) == models.ForeignKey:
                            params[field.name] = related_instance
                        else:
                            params[field.name] = [related_instance]
                else:
                    params[field.name] = \
                        DEFAULT_TEST_ATTRIBUTE_VALUES[type(field)]

        if not deferred:
            model(**params).save(**using_args)

    # Create all deferred models.
    if deferred_models:
        create_test_data(deferred_models, database)

    # All models should be created (Not all deferred fields have been populated
    # yet) Populate deferred fields that we know about.  Here lies untested
    # code!
    if deferred_fields:
        for model, field_list in deferred_fields.items():
            for field in field_list:
                related_model = field.rel.to
                related_instance = related_model.objects.using(database)[0]

                if type(field) == models.ForeignKey:
                    setattr(model, field.name, related_instance)
                else:
                    getattr(model, field.name).add(related_instance,
                                                   **using_args)

            model.save(**using_args)


def test_sql_mapping(test_field_name, db_name='default'):
    if is_multi_db():
        engine = settings.DATABASES[db_name]['ENGINE'].split('.')[-1]
    else:
        engine = settings.DATABASE_ENGINE

    sql_for_engine = __import__('django_evolution.tests.db.%s' % (engine),
                                {}, {}, [''])

    return getattr(sql_for_engine, test_field_name)


def deregister_models(app_label='tests'):
    "Clear the test section of the app cache"
    del cache.app_models[app_label]
    clear_models_cache()


def clear_models_cache():
    """Clears the Django models cache.

    This cache is used in Django >= 1.2 to quickly return results from
    cache.get_models(). It needs to be cleared when modifying the model
    registry.
    """
    if hasattr(cache, '_get_models_cache'):
        # On Django 1.2, we need to clear this cache when unregistering models.
        cache._get_models_cache.clear()


def set_app_test_models(models, app_label):
    """Sets the list of models in the Django test models registry."""
    cache.app_models[app_label] = models
    clear_models_cache()


def add_app_test_model(model, app_label):
    """Adds a model to the Django test models registry."""
    key = model._meta.object_name.lower()
    cache.app_models.setdefault(app_label, SortedDict())[key] = model
    clear_models_cache()

########NEW FILE########
__FILENAME__ = utils
from django_evolution.db import EvolutionOperationsMulti

def write_sql(sql, database):
    "Output a list of SQL statements, unrolling parameters as required"
    qp = EvolutionOperationsMulti(database).get_evolver().quote_sql_param

    for statement in sql:
        if isinstance(statement, tuple):
            print unicode(statement[0] % tuple(qp(s) for s in statement[1]))
        else:
            print unicode(statement)


def execute_sql(cursor, sql):
    """
    Execute a list of SQL statements on the provided cursor, unrolling
    parameters as required
    """
    for statement in sql:
        if isinstance(statement, tuple):
            if not statement[0].startswith('--'):
                cursor.execute(*statement)
        else:
            if not statement.startswith('--'):
                cursor.execute(statement)

########NEW FILE########
__FILENAME__ = interfaces
from django.contrib.sites.models import Site
from django.core.exceptions import ObjectDoesNotExist
from extras.models import ExtraConfiguration
import base64
import pickle



class ScrumdoExtra:
    """ Interface that 'Extras' for the scrumdo site must implement.  Extras usually
        implement a connection to a third party service. """

    def getName(self):
        "Returns a user-friendly version of the name of this extra.  Generally should be just a couple words long."
        raise NotImplementedError("ScrumdoExtra subclasses must implement getName()")


    def getLogo(self):
        "Returns a URL to a logo that can be used on the config page."
        raise NotImplementedError("ScrumdoExtra subclasses must implement getLogo()")

    def getSlug(self):
        """ Returns a version of the name consisting of only letters, numbers, or dashes
            Max length, 25 chars    """
        raise NotImplementedError("ScrumdoExtra subclasses must implement getSlug()")


    def getDescription(self):
        " Returns a user-friendly description of this extra.  This text will be passed through a Markdown filter when displayed to the user. "
        raise NotImplementedError("ScrumdoExtra subclasses must implement getDescription()")

    # TODO (performance) - Probably want to memcache the configurations for a short period of time.
    def getConfiguration(self, project_slug ):
        """ Gets a configuration object (usually a dictionary) from the ExtraConfiguration table. """
        try:
            config = ExtraConfiguration.objects.get( extra_slug=self.getSlug(), project_slug=project_slug)
        except ObjectDoesNotExist:
            return {}
        return pickle.loads( base64.decodestring(config.configuration_pickle) )

    def isPremium(self):
        return False

    def saveConfiguration(self, project_slug, configuration_object ):
        """ Saves a configuration object (usually a dictionary) to the ExtraConfiguration table.
            configuration_object must be pickleable """

        try:
            config = ExtraConfiguration.objects.get( extra_slug=self.getSlug(), project_slug=project_slug)
        except ObjectDoesNotExist:
            config = ExtraConfiguration( project_slug=project_slug, extra_slug=self.getSlug() )

        config.configuration_pickle = base64.encodestring( pickle.dumps( configuration_object ) )
        config.save()




class ScrumdoProjectExtra( ScrumdoExtra ):
    "Base class for extras that should be associated with a project.  "


    def associate( self, project):
        "called when an extra is first associated with a project."
        pass

    def unassociate( self, project):
        "called when an extra is removed from a project."
        pass

    def getShortStatus( self,  project ):
        """ Should return a string representing the current status that this extra has for a given project.
            Examples: 'Successfully synchronize on 1/1/2010' or 'Syncronization last failed' or 'Everything OK' """
        raise NotImplementedError("ScrumdoProjectExtra subclasses must implement getShortStatus()")

    def getExtraActions( self, project, **kwargs):
        """ Should return a list of tupples with a label, url, and silk icon that represent actions that a user can manually
            invoke for this extra. Example: ('Syncronize','/blah/blah/syncronize','') """
        return []
    
    def getExtraStoryActions(self, project, story):
        """ Should return a list of tupples with a label, url, silk icon, that represent actions that a user can manually
            invoke for this extra on a story. Example: ('Syncronize','/blah/blah/syncronize','') """
        return []

    def doProjectdoProjectConfiguration( self, request, project, stage=""):
        """ Should return a django style response that handles any configuration that this extra may need. 
            Stage is an optional parameter that you can use for multi step configuration sequences.  It's
            taken from the last bit of the URL /extras/extra-slug/project-slug/configure/STAGE """
        raise NotImplementedError("ScrumdoProjectExtra subclasses must implement doProjectConfiguration()")

    def initialSync( self, project):
        """ Does whatever needs doing for an initial sync of the project.
            An extra's configuration should add this event to the queue when
            it's ready.  """
        pass

    def pullProject( self, project ):
        """ Should cause a full pull syncronization of this extra from whatever external source
            there is.  This will be called on a scheduled basis for all active projects.  The project
            parameter be an apps.projects.models.Project object.    """
        pass

    # Not yet implemented...
    # def externalHook( self, request ):
    #   "Every extra gets a URL that external services can POST to.  This should handle those requests."
    #   pass


    def storyUpdated( self, project, story ):
        "Called when a story is updated in a project that this extra is associated with."
        pass

    def storyDeleted( self, project, external_id):
        """Called when a story is deleted in a project that this extra is associated with.
           Note: the ScrumDo story has already been deleted by the time this method is called. """
        pass

    def storyCreated( self, project, story):
        "Called when a story is created in a project that this extra is associated with."
        pass

    def storyStatusChange( self, project, story):
        "Called when a story's status has changed in a project that this extra is associated with."
        pass

    def getExtraHookURL( self, project ):
        """ Returns where an external site can post to, to give this extra information.
            You probably don't want to change this in subclasses. """

        current_site = Site.objects.get_current()
        return "http://" + current_site.domain + "/extras/" + self.getSlug() + "/project/" +  project.slug + "/hook"

    def storyImported(self, project, story):
        "Occurs when a user imports a story from the story queue"
        pass

    def taskUpdated(self, project, task):
        pass

    def taskDeleted(self, project, external_id):
        pass

    def taskCreated(self, project, task):
        pass

    def taskStatusChange(self, project, task):
        pass
    






    
    
    
########NEW FILE########
__FILENAME__ = manager
from django.conf import settings
import logging
import sys, traceback

import projects.signals as project_signals
import projects.limits as project_limits
import extras.signals as extras_signals

from models import ProjectExtraMapping, ExtraConfiguration, SyncronizationQueue , ExternalStoryMapping, StoryQueue, ExternalTaskMapping

logger = logging.getLogger(__name__)

class ExtrasManager:
    """
      Class to manage our list of extras.
    """

    def getExtras(self):
        return self.extras.values()

    def getExtra( self, slug ):
        return self.extras[ slug ]

    def queueSyncAction( self, extra_slug, project, action, **kwargs):
        logger.debug("Queuing a syncronization action %d, %s, %s" % (action,project.slug,extra_slug))
        
        existing = SyncronizationQueue.objects.filter(project=project, extra_slug=extra_slug, action=action, story=kwargs.get("story",None), task=kwargs.get("task",None))
        if existing.count() > 0:
            logger.debug("Skipping this duplicate sync action")
            return
        
        queueObject = SyncronizationQueue( project=project, extra_slug=extra_slug, action=action)
        queueObject.story = kwargs.get("story",None)
        queueObject.task = kwargs.get("task",None)
        if queueObject.task == None:
            try:
                mapping = ExternalStoryMapping.objects.get(story=queueObject.story,extra_slug=extra_slug)
                queueObject.external_id = mapping.external_id
            except:
                pass
        else:
            try:
                mapping = ExternalTaskMapping.objects.get(task=queueObject.task,extra_slug=extra_slug)
                queueObject.external_id = mapping.external_id
            except:
                pass
        queueObject.save()

    def queueSyncActions(self, project, action, **kwargs):
        for mapping in project.extras.all():
            self.queueSyncAction( mapping.extra_slug, project, action, **kwargs)

    def getExtraConfigs( self , project):
        extras = self.extras.values();
        rv = []
        for extra in extras:
            if extra.isPremium() and project.organization==None and not project_limits.personal_extra_limit.increaseAllowed(project=project):
                continue
            if extra.isPremium() and project.organization!=None and not project_limits.org_extra_limit.increaseAllowed(organization=project.organization):
                continue
            config = {}
            config["extra"] = extra
            config["enabled"] = self.is_extra_enabled(project, extra.getSlug() )
            config["status"] = extra.getShortStatus( project )
            rv.append( config )
        return rv

    def syncronizeExtra(self, extra_slug, project ):
        extra = self.getExtra( extra_slug )
        extra.pullProject( project )

    def enableExtra( self, project, extra_slug ):
        config = ProjectExtraMapping( project=project, extra_slug=extra_slug )
        config.save()
        extra = self.getExtra(extra_slug)
        extra.associate( project )

    def disableExtra( self, project, extra_slug ):
        mappings = ProjectExtraMapping.objects.filter( project=project, extra_slug=extra_slug)
        for mapping in mappings:
            mapping.delete()

        configs = ExtraConfiguration.objects.filter( extra_slug=extra_slug, project_slug=project.slug)
        for config in configs:
            config.delete()

        story_mappings = ExternalStoryMapping.objects.filter( story__project=project, extra_slug=extra_slug )
        for story_mapping in story_mappings:
            story_mapping.delete()

        story_queue = StoryQueue.objects.filter( project=project, extra_slug=extra_slug)
        for queue_item in story_queue:
            queue_item.delete()

        extra = self.getExtra(extra_slug)
        extra.unassociate( project )


    def is_extra_enabled( self, project, extra_slug ):
        return ProjectExtraMapping.objects.filter( project=project, extra_slug=extra_slug).count() > 0

    def onStoryUpdated(self, sender, **kwargs):
        story = kwargs["story"]
        logger.debug("extras.ExtrasManager::onStoryUpdated(story=%d)" % ( story.id) )
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_STORY_UPDATED , story=story)

    def onStoryStatusChanged(self, sender, **kwargs):
        story = kwargs["story"]
        logger.debug("extras.ExtrasManager::onStoryStatusChanged(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_STORY_STATUS_CHANGED , story=story)

    def onStoryDeleted(self, sender, **kwargs):
        story = kwargs["story"]
        logger.debug("extras.ExtrasManager::onStoryDeleted(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_STORY_DELETED , story=story)

    def onStoryCreated(self, sender, **kwargs):
        story = kwargs["story"]
        logger.debug("extras.ExtrasManager::onStoryCreated(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_STORY_CREATED , story=story)

    def onTaskUpdated(self, sender, **kwargs):
        task = kwargs["task"]
        story = task.story
        logger.debug("extras.ExtrasManager::onTaskUpdated(story=%d)" % ( story.id) )
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_TASK_UPDATED , story=story, task=task)

    def onTaskStatusChanged(self, sender, **kwargs):
        task = kwargs["task"]
        story = task.story
        logger.debug("extras.ExtrasManager::onTaskStatusChanged(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_TASK_STATUS_CHANGED , story=story, task=task)

    def onTaskDeleted(self, sender, **kwargs):
        task = kwargs["task"]
        story = task.story
        logger.debug("extras.ExtrasManager::onTaskDeleted(project=%s, task=%s)" % (story.project.slug, task))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_TASK_DELETED , story=story, task=task)

    def onTaskCreated(self, sender, **kwargs):
        task = kwargs["task"]
        story = task.story
        logger.debug("extras.ExtrasManager::onTaskCreated(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_TASK_CREATED , story=story, task=task)

    def onStoryImported(self, sender, **kwargs):
        story = kwargs["story"]
        logger.debug("extras.ExtrasManager::onStoryCreated(project=%s, story=%d)" % (story.project.slug, story.id))
        self.queueSyncActions( story.project, SyncronizationQueue.ACTION_STORY_IMPORTED , story=story)


    def __init__(self, extras_settings):
        manager = self
        self.extras = {}
        project_signals.story_updated.connect(self.onStoryUpdated, dispatch_uid="extra_signal_hookup")
        project_signals.story_status_changed.connect(self.onStoryStatusChanged, dispatch_uid="extra_signal_hookup")
        project_signals.story_deleted.connect(self.onStoryDeleted, dispatch_uid="extra_signal_hookup")
        project_signals.story_created.connect(self.onStoryCreated, dispatch_uid="extra_signal_hookup")


        project_signals.task_updated.connect(self.onTaskUpdated, dispatch_uid="extra_signal_hookup")
        project_signals.task_status_changed.connect(self.onTaskStatusChanged, dispatch_uid="extra_signal_hookup")
        project_signals.task_deleted.connect(self.onTaskDeleted, dispatch_uid="extra_signal_hookup")
        project_signals.task_created.connect(self.onTaskCreated, dispatch_uid="extra_signal_hookup")

        extras_signals.story_imported.connect( self.onStoryImported, dispatch_uid="extra_signal_hookup")

        for extra in extras_settings:
            extra_class = get_class( extra )
            extra = extra_class()
            extra.manager = self
            self.extras[ extra.getSlug() ] = extra

def get_class( kls ):
    return getattr(__import__( kls , {}, {}, ['Plugin']), 'Plugin')


manager = ExtrasManager( settings.SCRUMDO_EXTRAS )

########NEW FILE########
__FILENAME__ = models
from datetime import datetime, date
from django.db import models
import time

from django.core.urlresolvers import reverse
from projects.models import Project, Iteration, Story, Task, STATUS_CHOICES

import json

# Determines which extras are active for which projects.
# The configuration stores a json object of configuration options to make storing that sort of
# data easier on the extra.
class ProjectExtraMapping( models.Model ):
    project = models.ForeignKey(Project, related_name="extras")
    extra_slug = models.CharField( "extra_slug" , max_length=25)
    configuration = models.TextField( )


# When an extra imports stories, it should put them into the story queue instead of directly into Story objects.
# this way, the user will be presented with a list of stories that they can choose to use or not from all of their
# external sources.
#
# After a story is imported, the extra should feel free to directly update it on syncronizations.
class StoryQueue( models.Model ):
    project = models.ForeignKey(Project, related_name="story_queue")
    extra_slug = models.CharField( "extra_slug" , max_length=25)

    external_id = models.CharField( max_length=40)
    external_url = models.CharField( max_length=256, blank=True , null=True)
    imported_on = models.DateTimeField( default=datetime.now)
    modified = models.DateTimeField( default=datetime.now)

    summary = models.TextField( )
    detail = models.TextField( blank=True )
    points = models.CharField('points', max_length=3, default="?" ,blank=True)
    status = models.IntegerField( max_length=2, choices=STATUS_CHOICES, default=1 )
    extra_1 = models.TextField( blank=True , null=True)
    extra_2 = models.TextField( blank=True , null=True)
    extra_3 = models.TextField( blank=True , null=True)
    external_extra = models.CharField( max_length=512 , blank=True, null=True )
    archived = models.BooleanField( default=False )
        


class SyncronizationQueue( models.Model ):
    ACTION_SYNC_REMOTE = 1
    ACTION_STORY_UPDATED = 2
    ACTION_STORY_DELETED = 3
    ACTION_STORY_CREATED = 4
    ACTION_INITIAL_SYNC = 5
    ACTION_STORY_STATUS_CHANGED = 6
    ACTION_TASK_UPDATED = 7
    ACTION_TASK_DELETED = 8
    ACTION_TASK_CREATED = 9
    ACTION_TASK_STATUS_CHANGED = 10
    ACTION_STORY_IMPORTED = 11

    ACTION_CHOICES = (
        (1, "SYNC_REMOTE"),
        (2, "STORY_UPDATED"),
        (3, "STORY_DELETED"),
        (4, "STORY_CREATED"),
        (5, "INITIAL_SYNC"),
        (6, "ACTION_STORY_STATUS_CHANGED"),
        (7, "ACTION_TASK_UPDATED"),
        (8, "ACTION_TASK_DELETED"),
        (9, "ACTION_TASK_CREATED"),
        (10, "ACTION_TASK_STATUS_CHANGED"),
        (11, "ACTION_STORY_IMPORTED")   )

    project = models.ForeignKey(Project)
    story = models.ForeignKey(Story, null=True, related_name="sync_queue")
    task = models.ForeignKey(Task, null=True, related_name="sync_queue")
    extra_slug = models.CharField(  max_length=25)
    action = models.IntegerField( max_length=2, choices=ACTION_CHOICES )
    queue_date = models.DateTimeField( default=datetime.now)
    external_id = models.CharField( max_length=40 , null=True)


class ExternalStoryMapping( models.Model ):
    """ When a story is related to external, third party sites, this gives a way of associating a reference to that other site.
        For instance, we store the GitHub Issue ID for any story that was imported from GitHub issues, or exported to it.
        This way, we can map them back and forth for syncronization purposes.
    """
    story = models.ForeignKey(Story, related_name="external_links")
    external_id = models.CharField( max_length=40 )
    external_url = models.CharField( max_length=256, blank=True , null=True)
    extra_slug = models.CharField( max_length=20 )
    external_extra = models.CharField( max_length=512 , blank=True, null=True )

class ExternalTaskMapping( models.Model ):
    """ When a task is related to external, third party sites, this gives a way of associating a reference to that other site."""
    task = models.ForeignKey(Task, related_name="external_links")
    external_id = models.CharField( max_length=40 )
    external_url = models.CharField( max_length=256, blank=True , null=True)
    extra_slug = models.CharField( max_length=20 )


class ExtraConfiguration( models.Model ):
    extra_slug = models.CharField( "extra_slug" , max_length=25)
    project_slug = models.CharField( "project_slug" , max_length=55)
    configuration_pickle = models.TextField( blank=True )

########NEW FILE########
__FILENAME__ = plugin
from extras.interfaces import ScrumdoProjectExtra
from django.conf import settings
from django.shortcuts import render_to_response
from django.template import RequestContext

import logging

class ExampleExtra( ScrumdoProjectExtra ):
    def getName(self):
        return "Example Extra"


    def getSlug(self):
        "Returns a version of the name consisting of only letters, numbers, or dashes"
        return "example"

    def getLogo(self):
        return settings.SSL_STATIC_URL + "extras/example-logo.png"


    def getDescription(self):
        "Returns a user-friendly description of this extra.  This text will be passed through a Markdown filter when displayed to the user."
        return "This Extra demonstrates the bare minimal required to get a ScrumDo Project based extra working."


    def doProjectConfiguration( self, request, project, stage=""):
        "Should return a django style response that handles any configuration that this extra may need."
        return render_to_response("extras/example/configure.html", {
            "extra":self,
          }, context_instance=RequestContext(request))


    def getShortStatus(self, project):
        return "Example status!"

    def associate( self, project):
        logging.info("Associated example extra with " + project.slug )


    def unassociate( self, project):
        logging.info("Unassociated example extra with " + project.slug )

Plugin = ExampleExtra

########NEW FILE########
__FILENAME__ = forms
from django import forms
from django.utils.translation import ugettext_lazy as _
import re

class GitHubIssuesConfig(forms.Form):
    username = forms.CharField(max_length=100, help_text = _("Your GitHub username."))
    password = forms.CharField(widget=forms.PasswordInput, label=_("API Token"), help_text = _("Find it on GitHub under Account Settings->Account Admin.") )
    repository = forms.CharField(help_text = _("Use the user/repository or organization/repository format.  Example: ScrumDo-Dev-Group/ScrumDo"))
    upload = forms.BooleanField(label=_("Upload Stories"),help_text = _("Should ScrumDo stories be created as GitHub issues?"), required=False)
    download = forms.BooleanField(label=_("Download Issues"),help_text = _("Should GitHub issues be created as ScrumDo stories?"), required=False)
    delete = forms.BooleanField(label=_("Delete Issues"),help_text = _("Should GitHub issues be deleted when ScrumDo issues are?"), required=False)

    def clean_repository(self):
        data = self.cleaned_data['repository']
        data = re.sub('\s', "", data)
        return data

########NEW FILE########
__FILENAME__ = client
from extras.plugins.github_issues.github2.request import GithubRequest
from extras.plugins.github_issues.github2.issues import Issues
from extras.plugins.github_issues.github2.repositories import Repositories
from extras.plugins.github_issues.github2.users import Users
from extras.plugins.github_issues.github2.commits import Commits


class Github(object):

    def __init__(self, username=None, api_token=None, debug=False,
        requests_per_second=None, access_token=None):
        """
        An interface to GitHub's API:
            http://develop.github.com/

        Params:
            `username` is your own GitHub username.

            `api_token` can be found here (while logged in as that user):
                https://github.com/account

            `access_token` can be used when no ``username`` and/or ``api_token``
                is used.  The ``access_token`` is the OAuth access token that is
                received after successful OAuth authentication.

            `requests_per_second` is a float indicating the API rate limit
                you're operating under (1 per second per GitHub at the moment),
                or None to disable delays.

                The default is to disable delays (for backwards compatibility).
        """

        self.debug = debug
        self.request = GithubRequest(username=username, api_token=api_token,
                                     debug=self.debug,
                                     requests_per_second=requests_per_second,
                                     access_token=access_token)
        self.issues = Issues(self.request)
        self.users = Users(self.request)
        self.repos = Repositories(self.request)
        self.commits = Commits(self.request)

    def project_for_user_repo(self, user, repo):
        return "/".join([user, repo])

    def get_blob_info(self, project, tree_sha, path):
        blob = self.request.get("blob/show", project, tree_sha, path)
        return blob.get("blob")

    def get_tree(self, project, tree_sha):
        tree = self.request.get("tree/show", project, tree_sha)
        return tree.get("tree", [])

    def get_network_meta(self, project):
        return self.request.raw_request("/".join([self.request.github_url,
                                                  project,
                                                  "network_meta"]), {})

    def get_network_data(self, project, nethash, start=None, end=None):
        return self.request.raw_request("/".join([self.request.github_url,
                                                  project,
                                                  "network_data_chunk"]),
                                                  {"nethash": nethash,
                                                   "start": start,
                                                   "end": end})

########NEW FILE########
__FILENAME__ = commits
from extras.plugins.github_issues.github2.core import BaseData, GithubCommand, Attribute, DateAttribute


class Commit(BaseData):
    message = Attribute("Commit message.")
    parents = Attribute("List of parents for this commit.")
    url = Attribute("Canonical URL for this commit.")
    author = Attribute("Author metadata (dict with name/email.)")
    id = Attribute("Commit ID.")
    committed_date = DateAttribute("Date committed.", format="commit")
    authored_date = DateAttribute("Date authored.", format="commit")
    tree = Attribute("Tree SHA for this commit.")
    committer = Attribute("Comitter metadata (dict with name/email.)")

    added = Attribute("(If present) Datastructure representing what's been "
                      "added since last commit.")
    removed = Attribute("(if present) Datastructure representing what's been "
                        "removed since last commit.")
    modified = Attribute("(If present) Datastructure representing what's "
                         "been modified since last commit.")

    def __repr__(self):
        return "<Commit: %s %s>" % (self.id, self.message[:64])


class Commits(GithubCommand):
    domain = "commits"

    def list(self, project, branch="master", file=None):
        return self.get_values("list", project, branch, file,
                               filter="commits", datatype=Commit)

    def show(self, project, sha):
        return self.get_value("show", project, sha,
                              filter="commit", datatype=Commit)

########NEW FILE########
__FILENAME__ = core
from datetime import datetime

GITHUB_TIMEZONE = "-0700"
GITHUB_DATE_FORMAT = "%Y/%m/%d %H:%M:%S"
#2009-03-21T18:01:48-07:00
COMMIT_DATE_FORMAT = "%Y-%m-%dT%H:%M:%S"


def ghdate_to_datetime(github_date):
    date_without_tz = " ".join(github_date.strip().split()[:2])
    return datetime.strptime(date_without_tz, GITHUB_DATE_FORMAT)


def datetime_to_ghdate(datetime_):
    date_without_tz = datetime_.strftime(GITHUB_DATE_FORMAT)
    return " ".join([date_without_tz, GITHUB_TIMEZONE])


def commitdate_to_datetime(commit_date):
    date_without_tz = commit_date[:-6]
    return datetime.strptime(date_without_tz, COMMIT_DATE_FORMAT)


def datetime_to_commitdate(datetime_):
    date_without_tz = datetime_.strftime(COMMIT_DATE_FORMAT)
    return "".join([date_without_tz, GITHUB_TIMEZONE])


class GithubCommand(object):

    def __init__(self, request):
        self.request = request

    def make_request(self, command, *args, **kwargs):
        filter = kwargs.get("filter")
        post_data = kwargs.get("post_data") or {}
        method = kwargs.get("method", "GET")
        if post_data or method.upper() == "POST":
            response = self.request.post(self.domain, command, *args,
                                         **post_data)
        else:
            response = self.request.get(self.domain, command, *args)
        if filter:
            return response[filter]
        return response

    def get_value(self, *args, **kwargs):
        datatype = kwargs.pop("datatype", None)
        value = self.make_request(*args, **kwargs)
        if datatype:
            # unicode keys are not accepted as kwargs by python, see:
            #http://mail-archives.apache.org/mod_mbox/qpid-dev/200609.mbox/%3C1159389941.4505.10.camel@localhost.localdomain%3E
            # So we make a local dict with the same keys but as strings:
            return datatype(**dict((str(k), v) for (k, v) in value.iteritems()))
        return value

    def get_values(self, *args, **kwargs):
        datatype = kwargs.pop("datatype", None)
        values = self.make_request(*args, **kwargs)
        if datatype:
            # Same as above, unicode keys will blow up in **args, so we need to
            # create a new 'values' dict with string keys
            return [datatype(**dict((str(k), v) for (k, v) in value.iteritems()))
                    for value in values]
        else:
            return values


def doc_generator(docstring, attributes):
    docstring = docstring or ""

    def section(title):
        return "\n".join([title, "-" * len(title)])

    def bullet(title, text):
        return """    *``%s``*\n      %s\n""" % (title, text)

    a = section("Attributes")
    b = "\n".join([bullet(attr_name, attr.help)
                    for attr_name, attr in attributes.items()])
    return "\n".join([docstring, a, b])


class Attribute(object):

    def __init__(self, help):
        self.help = help

    def to_python(self, value):
        return value

    from_python = to_python


class DateAttribute(Attribute):
    format = "github"
    converter_for_format = {
        "github": {
            "to": ghdate_to_datetime,
            "from": datetime_to_ghdate,
        },
        "commit": {
            "to": commitdate_to_datetime,
            "from": datetime_to_commitdate,
        },
    }

    def __init__(self, *args, **kwargs):
        self.format = kwargs.pop("format", self.format)
        super(DateAttribute, self).__init__(*args, **kwargs)

    def to_python(self, value):
        if value and not isinstance(value, datetime):
            return self.converter_for_format[self.format]["to"](value)
        return value

    def from_python(self, value):
        if value and isinstance(value, datetime):
            return self.converter_for_format[self.format]["from"](value)
        return value


class BaseDataType(type):

    def __new__(cls, name, bases, attrs):
        super_new = super(BaseDataType, cls).__new__

        _meta = dict([(attr_name, attr_value)
                        for attr_name, attr_value in attrs.items()
                            if isinstance(attr_value, Attribute)])
        attrs["_meta"] = _meta
        attributes = _meta.keys()
        attrs.update(dict([(attr_name, None)
                        for attr_name in attributes]))

        def _contribute_method(name, func):
            func.func_name = name
            attrs[name] = func

        def constructor(self, **kwargs):
            for attr_name, attr_value in kwargs.items():
                attr = self._meta.get(attr_name)
                if attr:
                    setattr(self, attr_name, attr.to_python(attr_value))
                else:
                    setattr(self, attr_name, attr_value)

        _contribute_method("__init__", constructor)

        def to_dict(self):
            _meta = self._meta
            dict_ = vars(self)
            return dict([(attr_name, _meta[attr_name].from_python(attr_value))
                            for attr_name, attr_value in dict_.items()])
        # I don't understand what this is trying to do.
        # whatever it was meant to do is broken and is breaking the ability to call "vars" on instantiations, which is breaking all kindsa shit. -AS
        #_contribute_method("__dict__", to_dict)

        def iterate(self):
            not_empty = lambda e: e[1] is not None  # AS I *think* this is what was intended.
            return iter(filter(not_empty, vars(self).items()))
        _contribute_method("__iter__", iterate)

        result_cls = super_new(cls, name, bases, attrs)
        result_cls.__doc__ = doc_generator(result_cls.__doc__, _meta)
        return result_cls

    def contribute_method_to_cls(cls, name, func):
        func.func_name = name
        return func


class BaseData(object):
    __metaclass__ = BaseDataType

########NEW FILE########
__FILENAME__ = issues
import urllib

from extras.plugins.github_issues.github2.core import GithubCommand, BaseData, Attribute, DateAttribute


class Issue(BaseData):
    position = Attribute("The position of this issue in a list.")
    number = Attribute("The issue number (unique for project).")
    votes = Attribute("Number of votes for this issue.")
    body = Attribute("The full description for this issue.")
    title = Attribute("Issue title.")
    user = Attribute("The username of the user that created this issue.")
    state = Attribute("State of this issue. Can be ``open`` or ``closed``.")
    labels = Attribute("Labels associated with this issue.")
    created_at = DateAttribute("The date this issue was created.")
    closed_at = DateAttribute("The date this issue was closed.")
    updated_at = DateAttribute("The date when this issue was last updated.")

    def __repr__(self):
        return "<Issue: %s>" % self.title.encode('utf-8')


class Comment(BaseData):
    created_at = DateAttribute("The date this comment was created.")
    updated_at = DateAttribute("The date when this comment was last updated.")
    body = Attribute("The full text of this comment.")
    id = Attribute("The comment id.")
    user = Attribute("The username of the user that created this comment.")

    def __repr__(self):
        return "<Comment: %s>" % self.body


class Issues(GithubCommand):
    domain = "issues"

    def search(self, project, term, state="open"):
        """Get all issues for project that match term with given state.

        ``project`` is a string with the project owner username and repository
        name separated by ``/`` (e.g. ``ask/pygithub2``).
        ``term`` is a string to search issues for.
        ``state`` can be either ``open`` or ``closed``.
        """
        return self.get_values("search", project, state,
                               urllib.quote_plus(term), filter="issues",
                               datatype=Issue)

    def list(self, project, state="open"):
        """Get all issues for project' with state'.

        ``project`` is a string with the project owner username and repository
        name separated by ``/`` (e.g. ``ask/pygithub2``).
        ``state`` can be either ``open`` or ``closed``.
        """
        return self.get_values("list", project, state, filter="issues",
                               datatype=Issue)

    def show(self, project, number):
        """Get all the data for issue by issue-number."""
        return self.get_value("show", project, str(number),
                              filter="issue", datatype=Issue)

    def open(self, project, title, body):
        """Open up a new issue."""
        issue_data = {"title": title, "body": body}
        return self.get_value("open", project, post_data=issue_data,
                              filter="issue", datatype=Issue)

    def close(self, project, number):
        return self.get_value("close", project, str(number), filter="issue",
                              datatype=Issue)

    def reopen(self, project, number):
        return self.get_value("reopen", project, str(number), filter="issue",
                              datatype=Issue)

    def edit(self, project, number, title, body):
        issue_data = {"title": title, "body": body}
        return self.get_value("edit", project, str(number),
                              post_data=issue_data, filter="issue",
                              datatype=Issue)

    def add_label(self, project, number, label):
        return self.make_request("label/add", project, label, str(number),
                                 filter="labels")

    def remove_label(self, project, number, label):
        return self.make_request("label/remove", project, label, str(number),
                                 filter="labels")

    def comment(self, project, number, comment):
        """Comment on an issue."""
        comment_data = {'comment': comment}
        return self.make_request("comment", project, str(number),
                                 post_data=comment_data,
                                 filter='comment')

    def comments(self, project, number):
        """View comments on an issue."""
        return self.get_values("comments", project, str(number),
                               filter="comments", datatype=Comment)

########NEW FILE########
__FILENAME__ = repositories
from extras.plugins.github_issues.github2.core import BaseData, GithubCommand, Attribute, DateAttribute

class Repository(BaseData):
    name = Attribute("Name of repository.")
    description = Attribute("Repository description.")
    forks = Attribute("Number of forks of this repository.")
    watchers = Attribute("Number of people watching this repository.")
    private = Attribute("If True, the repository is private.")
    url = Attribute("Canonical URL to this repository")
    fork = Attribute("If True, this is a fork of another repository.")
    owner = Attribute("Username of the user owning this repository.")
    homepage = Attribute("Homepage for this project.")
    open_issues = Attribute("List of open issues for this repository.")
    created_at = DateAttribute("Datetime the repository was created.")
    pushed_at = DateAttribute("Datetime of the last push to this repository")
    has_downloads = Attribute("If True, this repository has downloads.")
    has_wiki = Attribute("If True, this repository has a wiki.")
    has_issues = Attribute("If True, this repository has an issue tracker.")

    def _project(self):
        return self.owner + "/" + self.name
    project = property(_project)

    def __repr__(self):
        return "<Repository: %s>" % (self._project())


class Repositories(GithubCommand):
    domain = "repos"

    def search(self, query):
        return self.make_request("search", query, filter="repositories")

    def show(self, project):
        return self.get_value("show", project, filter="repository",
                              datatype=Repository)
    def pushable(self):
        """
        Return a list of repos you can push to that are not your own.
        """
        return self.get_values("pushable", filter="repositories", datatype=Repository)


    def list(self, for_user=None):
        """Return a list of all repositories for a user.

        If no user is given, repositoris for the currently logged in user are
        returned.
        """
        for_user = for_user or self.request.username
        return self.get_values("show", for_user, filter="repositories",
                               datatype=Repository)

    def watch(self, project):
        return self.make_request("watch", project)

    def unwatch(self, project):
        return self.make_request("unwatch", project)

    def fork(self, project):
        return self.get_value("fork", project, filter="repository",
                              datatype=Repository)

    def create(self, name, description=None, homepage=None, public=True):
        repo_data = {"name": name, "description": description,
                     "homepage": homepage, "public": str(int(public))}
        return self.get_value("create", post_data=repo_data,
                              filter="repository", datatype=Repository)

    def delete(self, name):
        return self.make_request("delete", name)

    def set_private(self, repo_name):
        return self.make_request("set/private", repo_name)

    def set_public(self, repo_name):
        return self.make_request("set/public", repo_name)

    def list_collaborators(self, project):
        """Lists all the collaborators in a project (user/repro)."""
        return self.make_request("show", project, "collaborators",
                                 filter="collaborators")

    def add_collaborator(self, repo_name, username):
        """Adds an add_collaborator to a repro.

        Do not prefix repro_name with the user owning the repro like you
        do in list_collaborators()"""
        return self.make_request("collaborators", repo_name, "add", username)

    def remove_collaborator(self, repo_name, username):
        """Removes an add_collaborator from a repro.

        Do not prefix repro_name with the user owning the repro like you
        do in list_collaborators()"""
        return self.make_request("collaborators", repo_name, "remove",
                                 username, method="POST")

    def network(self, project):
        return self.make_request("show", project, "network", filter="network")

    def languages(self, project):
        return self.make_request("show", project, "languages",
                                 filter="languages")

    def tags(self, project):
        return self.make_request("show", project, "tags", filter="tags")

    def branches(self, project):
        return self.make_request("show", project, "branches",
                                 filter="branches")

    def watchers(self, project):
        return self.make_request("show", project, "watchers",
                                 filter="watchers")

    def watching(self, for_user=None):
        """Lists all the repos a user is watching."""
        for_user = for_user or self.request.username
        return self.get_values("watched", for_user, filter="repositories",
                               datatype=Repository)

    def list_contributors(self, project):
        """Lists all the contributors in a project (user/repo)."""
        return self.make_request("show", project, "contributors",
                           filter="contributors")

########NEW FILE########
__FILENAME__ = request
import datetime
import sys
import time
import httplib
try:
    import json as simplejson  # For Python 2.6
except ImportError:
    import simplejson
from urlparse import urlparse, urlunparse
try:
    from urlparse import parse_qs
except ImportError:
    from cgi import parse_qs
from urllib import urlencode

GITHUB_URL = "https://github.com"

URL_PREFIX = "https://github.com/api/v2/json"


class GithubError(Exception):
    """An error occured when making a request to the Github API."""


class GithubRequest(object):
    github_url = GITHUB_URL
    url_format = "%(github_url)s/api/%(api_version)s/%(api_format)s"
    api_version = "v2"
    api_format = "json"
    GithubError = GithubError

    connector_for_scheme = {
        "http": httplib.HTTPConnection,
        "https": httplib.HTTPSConnection,
    }

    def __init__(self, username=None, api_token=None, url_prefix=None,
            debug=False, requests_per_second=None, access_token=None):
        """
        Make an API request.
        """
        self.username = username
        self.api_token = api_token
        self.access_token = access_token
        self.url_prefix = url_prefix
        self.debug = debug
        if requests_per_second is None:
            self.delay = 0
        else:
            self.delay = 1.0 / requests_per_second
        self.last_request = datetime.datetime(1900, 1, 1)
        if not self.url_prefix:
            self.url_prefix = self.url_format % {
                "github_url": self.github_url,
                "api_version": self.api_version,
                "api_format": self.api_format,
            }

    def encode_authentication_data(self, extra_post_data):
        if self.access_token:
            post_data = {"access_token": self.access_token}
        elif self.username and self.api_token:
            post_data = {"login": self.username,
                         "token": self.api_token}
        else:
            post_data = {}
        post_data.update(extra_post_data)
        return urlencode(dict([k, v.encode('utf-8')] for k, v in post_data.items()))

    def get(self, *path_components):
        path_components = filter(None, path_components)
        return self.make_request("/".join(path_components))

    def post(self, *path_components, **extra_post_data):
        path_components = filter(None, path_components)
        return self.make_request("/".join(path_components), extra_post_data,
            method="POST")

    def make_request(self, path, extra_post_data=None, method="GET"):
        if self.delay:
            since_last = (datetime.datetime.now() - self.last_request)
            since_last_in_seconds = (since_last.days * 24 * 60 * 60) + since_last.seconds + (since_last.microseconds/1000000.0)
            if since_last_in_seconds < self.delay:
                duration = self.delay - since_last_in_seconds
                if self.debug:
                    sys.stderr.write("delaying API call %s\n" % duration)
                time.sleep(duration)

        extra_post_data = extra_post_data or {}
        url = "/".join([self.url_prefix, path])
        result = self.raw_request(url, extra_post_data, method=method)

        if self.delay:
            self.last_request = datetime.datetime.now()
        return result

    def raw_request(self, url, extra_post_data, method="GET"):
        scheme, netloc, path, params, query, fragment = urlparse(url)
        hostname = netloc.split(':')[0]
        post_data = None
        headers = self.http_headers
        headers["Accept"] = "text/html"
        method = method.upper()
        if extra_post_data or method == "POST":
            post_data = self.encode_authentication_data(extra_post_data)
            headers["Content-Length"] = str(len(post_data))
        else:
            path = urlunparse((scheme, netloc, path, params,
                self.encode_authentication_data(parse_qs(query)),
                fragment))
        connector = self.connector_for_scheme[scheme]
        connection = connector(hostname)
        connection.request(method, path, post_data, headers)
        response = connection.getresponse()
        response_text = response.read()
        if self.debug:
            sys.stderr.write("URL:[%s] POST_DATA:%s RESPONSE_TEXT: [%s]\n" % (
                                path, post_data, response_text))
        if response.status >= 400:
            raise RuntimeError("unexpected response from github.com %d: %r" % (
                               response.status, response_text))
        json = simplejson.loads(response_text)
        if json.get("error"):
            raise self.GithubError(json["error"][0]["error"])

        return json

    @property
    def http_headers(self):
        return {"User-Agent": "pygithub2 v1",
                "Accept-Encoding": "application/json"}

########NEW FILE########
__FILENAME__ = users
from extras.plugins.github_issues.github2.core import BaseData, GithubCommand, Attribute
import urllib


class User(BaseData):
    id = Attribute("The user id")
    login = Attribute("The login username")
    name = Attribute("The users full name")
    company = Attribute("Name of the company the user is associated with")
    location = Attribute("Location of the user")
    email = Attribute("The users e-mail address")
    blog = Attribute("The users blog")
    following_count = Attribute("Number of other users the user is following")
    followers_count = Attribute("Number of users following this user")
    public_gist_count = Attribute(
                            "Number of active public gists owned by the user")
    public_repo_count = Attribute(
                        "Number of active repositories owned by the user")
    total_private_repo_count = Attribute("Number of private repositories")
    collaborators = Attribute("Number of collaborators")
    disk_usage = Attribute("Currently used disk space")
    owned_private_repo_count = Attribute("Number of privately owned repos")
    private_gist_count = Attribute(
        "Number of private gists owned by the user")
    plan = Attribute("Current active github plan")

    def is_authenticated(self):
        return self.plan is not None

    def __repr__(self):
        return "<User: %s>" % (self.login)


class Users(GithubCommand):
    domain = "user"

    def search(self, query):
        return self.get_values("search", urllib.quote_plus(query),
                               filter="users", datatype=User)

    def search_by_email(self, query):
        return self.get_value("email", query, filter="user", datatype=User)

    def show(self, username):
        return self.get_value("show", username, filter="user", datatype=User)

    def followers(self, username):
        return self.make_request("show", username, "followers", filter="users")

    def following(self, username):
        return self.make_request("show", username, "following", filter="users")

    def follow(self, other_user):
        return self.make_request("follow", other_user)

    def unfollow(self, other_user):
        return self.make_request("unfollow", other_user)

########NEW FILE########
__FILENAME__ = plugin
from extras.interfaces import ScrumdoProjectExtra
from django.template import RequestContext
from django.http import HttpResponseRedirect
from django.shortcuts import render_to_response
from django.core.urlresolvers import reverse
from django.conf import settings
from django.utils.translation import ugettext_lazy as _

import datetime
import forms
import logging

import sys, traceback

from extras.models import StoryQueue, SyncronizationQueue, ExternalStoryMapping
from extras.plugins.github_issues.github2.client import Github

from projects.models import Story

logger = logging.getLogger(__name__)

class Plugin( ScrumdoProjectExtra ):
    """ This Extra allows you to syncronize your GitHub issues with your ScrumDo stories. """

    def getName(self):
        "Friendly name to display in the configuration options to the user."
        return "GitHub Issues"

    def getLogo(self):
        return settings.SSL_STATIC_URL + "extras/github-logo.png"

    def getSlug(self):
        "Returns a version of the name consisting of only letters, numbers, or dashes"
        return "github_issues"

    def getDescription(self):
        "Returns a user-friendly description of this extra.  This text will be passed through a Markdown filter when displayed to the user."
        return "Create ScrumDo stories for any open GitHub issue.  Push ScrumDo stories to GitHub issues."

    def doProjectConfiguration( self, request, project, stage=""):
        """Handles a request to do configuration for the github_issues extra.
           This displays a form asking for credentials / repository information,
           then saves that with the saveConfiguration() api in ScrumdoProjectExtra base
           class.  After a successful configuration, we redirect back to the extras page.
           (Should each extra be responsible for that?)"""

        # The super class has a helper getConfiguration method that will return a dict of options.
        configuration = self.getConfiguration( project.slug )
        if request.method == "POST":
            form = forms.GitHubIssuesConfig( request.POST )
            if form.is_valid():
                configuration = form.cleaned_data
                configuration["status"] = "Configuration Saved"

                # The super class has a helper saveConfiguration method that will save a
                # dict of options for later retrieval by getConfiguration
                self.saveConfiguration( project.slug, configuration )

                if configuration.get("upload"):
                    # Need to queue an intial action to upload our project.
                    self.manager.queueSyncAction(self.getSlug(), project, SyncronizationQueue.ACTION_INITIAL_SYNC)

                return HttpResponseRedirect(reverse("project_extras_url",kwargs={'project_slug':project.slug}))
        else:
            form = forms.GitHubIssuesConfig(initial=configuration)
        return render_to_response("extras/github_issues/configure.html", {
            "project":project,
            "extra":self,
            "form":form
          }, context_instance=RequestContext(request))


    def storyDeleted( self, project, external_id, **kwargs):
        """Called when a story is deleted in a project that this extra is associated with.  """
        configuration = self.getConfiguration( project.slug )

        if not configuration.get('delete'):
            return # Not configured to delete stories.

        repository = configuration.get('repository')
        logging.debug("Attempting to delete GitHub issue %s" % external_id)
        github = kwargs.get( "github", Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1) )
        issue = github.issues.close( repository, external_id )


    def storyCreated( self, project, story, **kwargs):
        """ Called when a new ScrumDo story is created. This plugin creates a GitHub issue if the upload option is enabled. """

        if self._getExternalLink( story ) != None:
            # Already uploaded
            return

        configuration = self.getConfiguration( project.slug )

        if not configuration.get('upload'):
            # Not configured to upload new stories.
            return

        repository = configuration.get('repository')
        logging.debug("Attempting to create GitHub issue for story %d" % story.id)
        github = kwargs.get( "github", Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1) )
        issue = github.issues.open( repository, story.summary, story.detail )
        link = ExternalStoryMapping( story=story,
                                     extra_slug=self.getSlug(),
                                     external_id=issue.number,
                                     external_url="https://github.com/%s/issues/#issue/%d" % (repository, issue.number) )
        link.save()
        logging.info("GitHub issue #%d created for story %d" % (issue.number, story.id) )

    def associate( self, project):
        "called when an extra is first associated with a project."
        logger.info("Project associated with GitHubIssuesExtra")

    def unassociate( self, project):
        "called when an extra is removed from a project."
        logger.info("Project unassociated with GitHubIssuesExtra")

    def getShortStatus(self, project):
        try:
            configuration = self.getConfiguration( project.slug )
            return configuration.get("status")
        except:
            return "Not configured"


    def pullProject( self, project ):
        """ Pulls any new GitHub issues to Scrumdo, updates any existing
            Scrumdo stories that were associated with GitHub issues. """
        logging.debug("GitHubIssues::pullProject starting up.")
        configuration = self.getConfiguration( project.slug )

        # The configuration view sets a download flag to true/false depending on user input.
        if not configuration.get("download"):
            logging.debug("Not set to download stories, aborting.")
            return

        try:
            logging.debug("Retrieving remote issues")
            github = Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1)
            issues = github.issues.list(configuration.get('repository'), state="open")
        except:
            logging.warn("Failed to retrieve remote GitHub issues for project %s" % project.slug)
            configuration["status"] = "Failed to load your GitHub issues"
            self.saveConfiguration( project.slug, configuration )
            return

        logging.debug("Retrieved %d GitHub issues" % len(issues) )

        queue_stories = StoryQueue.objects.filter( project=project, extra_slug=self.getSlug() )
        logging.debug("%d stories already sitting in the story queue" % len(queue_stories) )
        project_stories = self._getStoriesInProjectAssociatedWithExtra( project )
        logging.debug("%d stories in the project associated with GitHub issues" % len(project_stories) )

        for issue in issues:
            story = self._getStory( issue.number, queue_stories, project_stories)
            if story == None:
                self._createStoryForIssue( issue, project , configuration.get('repository'))
            else:
                if story.summary != issue.title or story.detail != issue.body :
                    logging.debug("Updating story %d." % story.id )
                    story.summary=issue.title
                    story.detail=issue.body
                    story.save()



        configuration["status"] = "Synchronized with GitHub on " + str( datetime.date.today()  )
        logging.debug("pullProject complete, saving configuration.")
        self.saveConfiguration( project.slug, configuration )

    # def getExtraActions( self, project):
    #     return [("Synchronize",
    #              "%s?syncronize=now" % reverse("configure_extra_url",kwargs={'project_slug':project.slug,'extra_slug':self.getSlug()}),
    #              'arrow_refresh')]

    def initialSync( self, project):
        logging.debug("Performing initial GitHub issues syncronization.")
        configuration = self.getConfiguration( project.slug )
        if configuration.get('upload'):
            try:
                self._initialUpload( project, configuration)
            except:
                logging.debug("Failed to upload stories.")
                traceback.print_exc(file=sys.stdout)
                configuration["status"] = "Syncronization failed to upload stories to GitHub on " + str( datetime.date.today()  )
                self.saveConfiguration( project.slug, configuration )
                return

        self.pullProject(project)

    def storyUpdated( self, project, story , **kwargs):
        "Called when a story is updated in a project that this extra is associated with."
        logging.debug("GitHub issues::storyUpdated")
        configuration = self.getConfiguration( project.slug )

        # I think we should update stories if an external link exists, no matter how it got there.
        # # The configuration view sets a download flag to true/false depending on user input.
        # if not configuration.get("upload"):
        #   logging.debug("Not set to upload stories, aborting.")
        #   return

        link = self._getExternalLink( story )

        if link == None:
            logging.debug("Story not associated with external story, aborting.")
            return

        # Grab the github client passed in kwargs, if none, create one.
        github = kwargs.get( "github", Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1) )
        github.issues.edit( configuration.get('repository'), link.external_id, story.summary, story.detail )


    def storyStatusChange( self, project, story, **kwargs):
        logging.debug("GitHub issues::storyStatusChange")
        configuration = self.getConfiguration( project.slug )

        # The configuration view sets a download flag to true/false depending on user input.
        if not configuration.get("upload"):
            logging.debug("Not set to upload stories, aborting.")
            return

        link = self._getExternalLink( story )

        if link == None:
            logging.debug("Story not associated with external story, aborting.")
            return
        # Grab the github client passed in kwargs, if none, create one.
        github = kwargs.get( "github", Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1) )

        if story.status == Story.STATUS_DONE:
            github.issues.close( configuration.get('repository'), link.external_id )
        else:
            # All other scrumdo statuses map to open on github issues.
            github.issues.reopen( configuration.get('repository'), link.external_id )



    def _createStoryForIssue( self, issue, project , repository):
        logging.debug("Attempting to create new StoryQueue object for issue %d" % issue.number )
        try:
            story = StoryQueue.objects.get( project=project, external_id=issue.number)
            # Odd, it already exists!
            story.summary=issue.title
            story.detail=issue.body
        except StoryQueue.DoesNotExist:
            story = StoryQueue(project=project,
                             extra_slug=self.getSlug(),
                             external_id=issue.number,
                             external_url="https://github.com/%s/issues/#issue/%d" % (repository, issue.number),
                             summary=issue.title,
                             detail=issue.body )
        story.save()

    def _getStoriesInProjectAssociatedWithExtra(self, project):
        rv = []
        for story in project.stories.all():
            if self._getExternalLink( story ) != None:
                rv.append( story )

        return rv

    def _getStory( self, external_id, queue_stories, project_stories ):
        """ Pass in an external id, list of stories in the queue, and a list of stories in the project, and will return the story if the it exists in either list. """
        story = self._getStoryFromQueue( external_id, queue_stories)
        if story != None:
            return story
        return self._getStoryFromProject( external_id, project_stories )

    def _initialUpload(self, project, configuration):
        configuration = self.getConfiguration( project.slug )
        github = Github(username=configuration.get('username'), api_token=configuration.get('password'),requests_per_second=1)
        for story in project.stories.all():
            self.storyCreated(project, story, github=github)

    # TODO - is this general enough to bump up to the super class?
    def _getExternalLink( self, story ):
        """ Searches for the ExternalStoryMapping that is associated with this extra and returns it.
            returns None if it's not found. """
        for link in story.external_links.all():
            if link.extra_slug == self.getSlug():
                return link
        return None

    def _getStoryFromProject(self, external_id, project_stories ):
        """ Returns the story from the list with the given external ID for this extra. """
        for project_story in project_stories:
            for link in project_story.external_links.all():
                if link.extra_slug == self.getSlug() and str(link.external_id)==str(external_id):
                    return project_story
        return None

    def _getStoryFromQueue(self, external_id, queue_stories ):
        """ Returns the story from the list of StoryQueue objects with the given external id. """
        for queue_story in queue_stories:
            if queue_story.extra_slug == self.getSlug() and int(queue_story.external_id)==external_id:
                return queue_story
        return None

########NEW FILE########
__FILENAME__ = signals
import django.dispatch


story_imported = django.dispatch.Signal(providing_args=["story","user"])

########NEW FILE########
__FILENAME__ = extras_tags
# From: https://github.com/darkpixel/irontickets/blob/master/irontickets/templatetags/ifloaded.py

from django.conf import settings
from django import template
from django.template import NodeList

from extras.manager import manager as extras_manager

register = template.Library()

import logging

logger = logging.getLogger(__name__)

@register.filter(name='extra_name')
def extra_name( slug ):
    extra = extras_manager.getExtra( slug )
    return extra.getName()

@register.inclusion_tag('extras/extras_buttons.html')
def extra_buttons(extra_slug, projectOrIteration):
    try:
        if hasattr(projectOrIteration,"project"):
            project = projectOrIteration.project
            iteration = projectOrIteration
        else:
            project = projectOrIteration
            iteration = None
        extra = extras_manager.getExtra( extra_slug )

        return {'extra':extra, 'project':project, 'actions':extra.getExtraActions(project, iteration=iteration)}
    except:
        return {}


@register.inclusion_tag('extras/extras_story_buttons.html')
def extra_story_buttons(extra_slug, story):
    extra = extras_manager.getExtra( extra_slug )
    return {'extra':extra, 'actions':extra.getExtraStoryActions(story.project, story)}


@register.tag(name='ifloaded')
def do_ifloaded(parser, token):
    bits = token.split_contents()[1:]
    var = bits[0]
    nodelist_true = parser.parse(('else', 'endifloaded'))
    token = parser.next_token()
    if token.contents == 'else':
        nodelist_false = parser.parse(('endifloaded',))
        parser.delete_first_token()
    else:
        nodelist_false = NodeList()
    return IfLoadedNode(var, nodelist_true, nodelist_false)



class IfLoadedNode(template.Node):
    def __init__(self, var, nodelist_true, nodelist_false=None):
        self.nodelist_true, self.nodelist_false = nodelist_true, nodelist_false
        self.var = var

    def __repr__(self):
        return '<IfLoaded node>'

    def __iter__(self):
        for node in self.nodelist_true:
            yield node
        for node in self.nodelist_false:
            yield node

    def get_nodes_by_type(self, nodetype):
        nodes = []
        if isinstance(self, nodetype):
            nodes.append(self)
        nodes.extend(self.nodelist_true.get_nodes_by_type(nodetype))
        nodes.extend(self.nodelist_false.get_nodes_by_type(nodetype))
        return nodes

    def render(self, context):
        for app in settings.INSTALLED_APPS:
            if str(app) == str(self.var):
                return self.nodelist_true.render(context)
        return self.nodelist_false.render(context)

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.views.generic.simple import direct_to_template


urlpatterns = patterns('extras.views',
    url(r'^(?P<extra_slug>[-\w]+)/(?P<project_slug>[-\w]+)/hook$', "project_extra_callback", name="projct_extra_callback_url"),
    url(r'^(?P<extra_slug>[-\w]+)/(?P<project_slug>[-\w]+)/enable$', "enable_extra", name="enable_extra_url"),
    url(r'^(?P<extra_slug>[-\w]+)/(?P<project_slug>[-\w]+)/disable$', "disable_extra", name="disable_extra_url"),
    url(r'^(?P<extra_slug>[-\w]*)/(?P<project_slug>[-\w]*)/configure$', "configure_extra", name="configure_extra_url"),    
    url(r'^(?P<extra_slug>[-\w]+)/(?P<project_slug>[-\w]+)/configure/(?P<stage>[-\w]+)$', "configure_extra", name="configure_extra_with_stage"),
    url(r'^(?P<extra_slug>[-\w]+)/(?P<project_slug>[-\w]+)/sync$', "syncronize_extra", name="syncronize_extra_url"),


    url(r'^(?P<project_slug>[-\w]+)$', "project_extras", name="project_extras_url"),
    url(r'^queue/(?P<project_slug>[-\w]+)/import$', "import_stories", name="import_stories_url"),
    url(r'^queue/(?P<project_slug>[-\w]+)/ignore$', "ignore_stories", name="ignore_stories_url"),
    url(r'^queue/(?P<project_slug>[-\w]+)/stories/$', "queued_stories", name="queued_stories_url_no_page", kwargs={"page":1}),        
    url(r'^queue/(?P<project_slug>[-\w]+)/stories/(?P<page>[0-9]+)$', "queued_stories", name="queued_stories_url"),        
    url(r'^queue/(?P<project_slug>[-\w]+)$', "story_queue", name="story_queue_url"),
    
)

########NEW FILE########
__FILENAME__ = views
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponseRedirect
from django.contrib.auth.decorators import login_required
from django.http import HttpResponse
from django.core.urlresolvers import reverse
from django.core.exceptions import PermissionDenied
from django.core import serializers
from django.core.paginator import Paginator, InvalidPage

from manager import manager
from projects.access import *
from projects.models import Story, Iteration
import logging

from models import StoryQueue, ExternalStoryMapping
logger = logging.getLogger(__name__)

import signals
import json


def project_extra_callback(request, extra_slug, project_slug):
    pass



@login_required
def import_stories(request, project_slug):
    logger.debug("Adding story from StoryQueue")
    project = get_object_or_404( Project, slug=project_slug )
    write_access_or_403(project, request.user )
    queue_ids = str(request.POST.get("stories","")).split(",")
    iteration = project.get_default_iteration()
    count = 0
    
    for queue_id in queue_ids:    
        try:
            story = StoryQueue.objects.get(id=queue_id)
        except:
            continue

        if story.project != project:
            # Shenanigans here!
            raise PermissionDenied()

        new_story = Story(project=project, rank=0,
                          local_id=project.getNextId(),
                          summary=story.summary,
                          detail=story.detail,
                          extra_1=story.extra_1,
                          extra_2=story.extra_2,
                          extra_3=story.extra_3,
                          status=story.status,
                          points=story.points,
                          iteration = iteration,
                          creator = request.user
                          )
        new_story.save()
        logger.debug("Added story %d" % new_story.id)
        mapping = ExternalStoryMapping(story=new_story,
                                       external_id=story.external_id,
                                       external_url=story.external_url,
                                       extra_slug=story.extra_slug)
        mapping.save()
        story.delete() # delete the story queue object since we just imported it.
        signals.story_imported.send( sender=request, story=new_story, story_queue=story, user=request.user )
        count+=1
    
    return HttpResponse( "%d imported" % count )


@login_required
def ignore_stories(request, project_slug):
    logger.debug("Ignoring story from StoryQueue")
    project = get_object_or_404( Project, slug=project_slug )
    write_access_or_403(project, request.user )
    queue_ids = str(request.POST.get("stories","")).split(",")
    count = 0

    for queue_id in queue_ids:    
        try:
            story = StoryQueue.objects.get(id=queue_id)
        except:
            continue

        if story.project != project:
            # Shenanigans here!
            raise PermissionDenied()

        story.archived = not story.archived
        story.save()
        count+=1

    return HttpResponse( "%d ignored" % count )

@login_required
def queued_stories(request, project_slug, page):
    project = get_object_or_404( Project, slug=project_slug )
    write_access_or_403(project, request.user )    
    queue = project.story_queue.all().order_by("-imported_on")
    page = int(page) 
    
    if request.GET.get("archived","false") == "false":
        queue = queue.filter(archived=False)
    
    p = Paginator(queue, 20)
    if page > p.num_pages:
        page = p.num_pages
    if page < 1:
        page = 1
    pg = p.page( page )
    stories = pg.object_list
    
    
    return render_to_response("extras/story_queue_list.html", {
        "project":project,
        "stories":stories,
        "page_num": page,
        "total_pages": p.num_pages
      }, context_instance=RequestContext(request))
    
        
@login_required    
def story_queue( request, project_slug):
    project = get_object_or_404( Project, slug=project_slug )
    write_access_or_403(project, request.user )
    return render_to_response("extras/story_queue.html", {
        "project":project,
      }, context_instance=RequestContext(request))

@login_required
def syncronize_extra(request, project_slug, extra_slug):
    project = get_object_or_404( Project, slug=project_slug )
    admin_access_or_403(project, request.user )
    manager.syncronizeExtra( extra_slug, project )
    return HttpResponse("OK")

@login_required
def enable_extra( request, project_slug, extra_slug):
    project = get_object_or_404( Project, slug=project_slug )
    admin_access_or_403(project, request.user )
    manager.enableExtra(project,extra_slug)
    return HttpResponseRedirect(reverse("configure_extra_url",kwargs={'project_slug':project_slug, "extra_slug":extra_slug}))

@login_required
def disable_extra( request, project_slug, extra_slug):
    project = get_object_or_404( Project, slug=project_slug )
    admin_access_or_403(project, request.user )
    manager.disableExtra(project,extra_slug)
    return HttpResponseRedirect(reverse("project_extras_url",kwargs={'project_slug':project_slug}))

@login_required
def configure_extra( request, project_slug, extra_slug, stage=""):
    project = get_object_or_404( Project, slug=project_slug )
    admin_access_or_403(project, request.user )
    extra = manager.getExtra( extra_slug )
    if extra != None:
        return extra.doProjectConfiguration(request, project, stage)
    return HttpResponseRedirect(reverse("project_extras_url",kwargs={'project_slug':project_slug}))

@login_required
def project_extras(request, project_slug):
    project = get_object_or_404( Project, slug=project_slug )
    admin_access_or_403(project, request.user )
    return render_to_response("extras/project_extras.html", {
        "project":project,
        "extras":manager.getExtraConfigs(project)
      }, context_instance=RequestContext(request))

def project_extra_options( request, extra_slug, project_slug):
    pass

########NEW FILE########
__FILENAME__ = models
import sys

from datetime import datetime
from projects.models import *
from django.db import models
from django.conf import settings
from django.contrib.auth.models import User, AnonymousUser
from django.db.models.signals import post_save
from django.utils.translation import get_language_from_request, ugettext_lazy as _



class Favorite(models.Model):
    TYPE_CHOICES = ( (0,"Story"),(1,"Project"),(2,"Iteration"),(3,"Epic") )
    user = models.ForeignKey(User, verbose_name=_('user'))
    favorite_type = models.IntegerField( choices=TYPE_CHOICES )
    
    # I really didn't want to use generic fields nor have 4 types of favorites, there's
    # going to be a ton of these and I'll need to index by all of them.
    story     = models.ForeignKey(Story, null=True, blank=True)
    epic      = models.ForeignKey(Epic, null=True, blank=True)
    iteration = models.ForeignKey(Iteration, null=True, blank=True)
    project   = models.ForeignKey(Project, null=True, blank=True)
    
    @staticmethod
    def setFavorite( fav_type, target_id, user, favorite):
        target = Favorite.getTarget(fav_type, target_id)
        existing = Favorite.getTargetFilter(target, Favorite.objects.filter(user=user) )
        if favorite and len(existing) == 0:
            # Favorite does not exist, but we want it            
            f = Favorite(user=user, favorite_type=fav_type)
            f.setTarget(target)
            f.save()
            return
        if (not favorite) and len(existing) > 0:
            # Favorite exists, get rid of it
            for favorite in existing:
                favorite.delete()
            
        
    
    @staticmethod
    def getFavorite( user, target ):
        q = Favorite.objects.filter(user=user)
        q = Favorite.getTargetFilter(target, q)
        items = q.all()
        if len(items) == 0:
            return None
        if len(items) > 1:
            logger.error("Found multiple favorites for %s / %s" % (user, target))
        return items[0]

    @staticmethod
    def getTarget( fav_type, target_id ):
        if fav_type == 0:
            return Story.objects.get(id=target_id)
        if fav_type == 3:
            return Epic.objects.get(id=target_id)
        if fav_type == 2:
            return Iteration.objects.get(id=target_id)
        if fav_type == 1:
            return Project.objects.get(id=target_id)
    
    @staticmethod
    def getTargetType( target ):
        if isinstance(target, Story):
            return 0
        if isinstance(target, Epic):
            return 3
        if isinstance(target, Iteration):
            return 2
        if isinstance(target, Project):
            return 1

    @staticmethod
    def getTargetFilter(target, q):
        if isinstance(target, Story):
            return q.filter(story=target)
        if isinstance(target, Epic):
            return q.filter(epic=target)
        if isinstance(target, Iteration):
            return q.filter(iteration=target)
        if isinstance(target, Project):
            return q.filter(project=target)
        
    def setTarget(self, target):
        if isinstance(target, Story):
            self.story = target
        if isinstance(target, Epic):
            self.epic = target
        if isinstance(target, Iteration):
            self.iteration = target
        if isinstance(target, Project):
            self.project = target

########NEW FILE########
__FILENAME__ = favorites_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from datetime import date
from favorites.models import Favorite
from django.template.defaultfilters import stringfilter
from django.conf import settings
import re
import logging

register = template.Library()
logger = logging.getLogger(__name__)




@register.inclusion_tag("favorites/favorite_snippet.html", takes_context=True)
def favorite(context, target):
    fav_type = Favorite.getTargetType(target)
    fav = Favorite.getFavorite( context['request'].user, target )    
    return {'fav_type':fav_type, 'favorite':fav, 'target':target, 'STATIC_URL':settings.SSL_STATIC_URL}

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from account.forms import *

urlpatterns = patterns('',
    url(r'^add/(?P<favorite_type>[0-9]+)/(?P<favorite_id>[0-9]+)$', 'favorites.views.add', name="add_favorite"),
    url(r'^remove/(?P<favorite_type>[0-9]+)/(?P<favorite_id>[0-9]+)$', 'favorites.views.remove', name="remove_favorite"),
)

########NEW FILE########
__FILENAME__ = views
from favorites.models import Favorite
from django.contrib.auth.decorators import login_required
from django.http import HttpResponse

@login_required
def add(request, favorite_type, favorite_id):    
    Favorite.setFavorite( int(favorite_type), favorite_id, request.user, True)
    return HttpResponse("OK")
    
@login_required
def remove(request, favorite_type, favorite_id):
    Favorite.setFavorite( int(favorite_type), favorite_id, request.user, False)
    return HttpResponse("OK")    
########NEW FILE########
__FILENAME__ = admin
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.contrib import admin
from organizations.models import *
import settings

if not "subscription" in settings.INSTALLED_APPS:
    admin.site.register(Organization)

class TeamAdmin(admin.ModelAdmin):
    list_display = ('name', 'organization')
    search_fields = ('name',)

admin.site.register(Team , TeamAdmin)
admin.site.register(TeamInvite)
########NEW FILE########
__FILENAME__ = forms
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import forms
from django.conf import settings
from django.contrib.auth.models import User
from django.utils.translation import ugettext_lazy as _

from organizations.models import Organization, Team
from django.forms.extras.widgets import SelectDateWidget

from projects.limits import org_user_limit

class TeamForm(forms.ModelForm):
    class Meta:
        model = Team
        fields = ('name', 'access_type' )


class UpdateOrganizationForm(forms.ModelForm):

    class Meta:
        model = Organization
        fields = ('name',  'description' )



class OrganizationForm(forms.ModelForm):
    slug = forms.SlugField(max_length=20,
        help_text = _("a short version of the name consisting only of letters, numbers, underscores and hyphens."),
        error_message = _("This value must contain only letters, numbers, underscores and hyphens."))

    def clean_slug(self):
        if Organization.objects.filter(slug__iexact=self.cleaned_data["slug"]).count() > 0:
            raise forms.ValidationError(_("A Organization already exists with that slug."))
        return self.cleaned_data["slug"].lower()

    class Meta:
        model = Organization
        fields = ('name', 'slug' )



class AddUserForm(forms.Form):

    recipient = forms.CharField(label=_(u"User"))

    def __init__(self, *args, **kwargs):
        self.team = kwargs.pop("team")
        super(AddUserForm, self).__init__(*args, **kwargs)

    def clean_recipient(self):
        try:
            user = User.objects.get(username__exact=self.cleaned_data['recipient'])
        except User.DoesNotExist:
            raise forms.ValidationError(_("There is no user with this username."))

        if user in self.team.members.all():
            raise forms.ValidationError(_("User is already a member of this team."))

        if not org_user_limit.increaseAllowed(userToAdd=user, organization=self.team.organization):
            raise forms.ValidationError(_("Upgrade your account to add more users."))

        return self.cleaned_data['recipient']

    def save(self, user):
        new_member = User.objects.get(username__exact=self.cleaned_data['recipient'])
        self.team.members.add( new_member )
        self.team.save()
        user.message_set.create(message="Added %s to team" % new_member)

########NEW FILE########
__FILENAME__ = import_export
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA



import StringIO
import csv
import re

from xml.dom.minidom import Document, parse
from django.http import HttpResponse

import projects.xlwt as xlwt
ezxf = xlwt.easyxf

from projects.import_export import _getHeaders
from projects.models import Story

import logging
import re

logger = logging.getLogger(__name__)
heading_xf = ezxf('font: bold on; align: wrap on, vert centre, horiz center')

def cleanWorksheetName( name ):
    invalidchars = "[]*/\?:=;"
    tmp = name
    for char in invalidchars:
        tmp = tmp.replace(char,"")
    tmp = tmp[:31]
    return tmp

def export_organization( organization, project_ids=None):
    response = HttpResponse( mimetype="Application/vnd.ms-excel")
    response['Content-Disposition'] = 'attachment; filename=organization.xls'
    w = xlwt.Workbook(encoding='utf8')
    projects_ws = w.add_sheet( "Projects" )
    iterations_ws = w.add_sheet( "Iterations" )

    category_ws = w.add_sheet( "Categories" )
    ic_ws = w.add_sheet( "Iterations x Categories" )

    tags_ws = w.add_sheet( "Tags" )
    it_ws = w.add_sheet( "Iterations x Tags" )

    date_xf = xlwt.XFStyle()
    date_xf.num_format_str = 'MM/dd/YYYY'

    _write_headers( projects_ws, [("Project",250),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60) ] )
    _write_headers( tags_ws, [("Project",250),("Tag",100),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60) ] )
    _write_headers( iterations_ws, [("Project",250),("Iteration",100),("Start",80),("End",80),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60),("Starting Points", 60), ("Max Points",60) ] )
    _write_headers( it_ws, [("Project",250),("Iteration",100),("Tag",110),("Start",80),("End",80),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60)] )
    _write_headers( category_ws, [("Project",250),("Category",100),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60)] )
    _write_headers( ic_ws, [("Project",250),("Iteration",100),("Category",110),("Start",80),("End",80),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60) ] )


    project_row = 1
    tags_row = 1
    categories_row = 1
    iteration_categories_row = 1
    iterations_row = 1
    iteration_tags_row = 1
    for project in organization.projects.filter(active=True):
        if project_ids:
            if project.id not in project_ids:
                continue
        
        story_headers = _getHeaders( project )
        project_ws = w.add_sheet( cleanWorksheetName(project.name) )

        for idx,header in enumerate(story_headers):
            project_ws.write(0,idx,header[1],heading_xf)
            project_ws.col(idx).width = 37*header[0]

        for idx, story in enumerate(project.stories.all().order_by("iteration","rank")):
            for hidx, header in enumerate(story_headers):
                f = header[2]
                project_ws.write(1+idx,hidx, f(story), header[3] )

        stories = project.stories.all()
        completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
        projects_ws.write(project_row,0, project.name )
        projects_ws.write(project_row,1, len(stories) )
        projects_ws.write(project_row,2, len(completed_stories) )
        projects_ws.write(project_row,3, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        projects_ws.write(project_row,4, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

        it_ws.write(iteration_tags_row,0, project.name )
        it_ws.write(iteration_tags_row,5, len(stories) )
        it_ws.write(iteration_tags_row,6, len(completed_stories) )
        it_ws.write(iteration_tags_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        it_ws.write(iteration_tags_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

        ic_ws.write(iteration_categories_row,0, project.name )
        ic_ws.write(iteration_categories_row,5, len(stories) )
        ic_ws.write(iteration_categories_row,6, len(completed_stories) )
        ic_ws.write(iteration_categories_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        ic_ws.write(iteration_categories_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

        iterations_ws.write(iterations_row,0, project.name )
        iterations_ws.write(iterations_row,4, len(stories) )
        iterations_ws.write(iterations_row,5, len(completed_stories) )
        iterations_ws.write(iterations_row,6, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        iterations_ws.write(iterations_row,7, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
        

        tags_ws.write(tags_row,0, project.name )
        tags_ws.write(tags_row,2, len(stories) )
        tags_ws.write(tags_row,3, len(completed_stories) )
        tags_ws.write(tags_row,4, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        tags_ws.write(tags_row,5, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

        category_ws.write(categories_row,0, project.name )
        category_ws.write(categories_row,2, len(stories) )
        category_ws.write(categories_row,3, len(completed_stories) )
        category_ws.write(categories_row,4, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
        category_ws.write(categories_row,5, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

        project_row += 1
        tags_row += 1
        categories_row += 1
        iteration_categories_row += 1
        iterations_row += 1
        iteration_tags_row += 1

        tags = {}
        categories = {}


        # Summarize the stories in each tag/category
        for story in stories:
            if story.category in categories:
                categories[story.category].append(story)
            else:
                categories[story.category] = [story]

            for tag in story.story_tags.all():
                tagname = tag.name
                if tagname in tags:
                    tags[tagname].append( story )
                else:
                    tags[tagname] = [story]

        # Write out the tag sheet
        for tag in tags :
            stories = tags[tag]
            completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
            tags_ws.write(tags_row,1,tag)
            tags_ws.write(tags_row,2, len(stories) )
            tags_ws.write(tags_row,3, len(completed_stories) )
            tags_ws.write(tags_row,4, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            tags_ws.write(tags_row,5, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
            tags_row += 1

        # Write out the category sheet
        for category in categories:
            stories = categories[category]
            completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
            category_ws.write(categories_row,1,category)
            category_ws.write(categories_row,2, len(stories) )
            category_ws.write(categories_row,3, len(completed_stories) )
            category_ws.write(categories_row,4, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            category_ws.write(categories_row,5, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
            categories_row += 1

        for iteration in project.iterations.all():
            itags = {}
            icategories = {}
            stories = iteration.stories.all()

            # Summarize the stories in categories / tags
            for story in stories:
                if story.category in icategories:
                    icategories[story.category].append(story)
                else:
                    icategories[story.category] = [story]
                for tag in story.story_tags.all():
                    tagname = tag.name
                    if tagname in itags:
                        itags[tagname].append( story )
                    else:
                        itags[tagname] = [story]
            completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
            iterations_ws.write(iterations_row,1, iteration.name)
            iterations_ws.write(iterations_row,2, iteration.start_date , date_xf)
            iterations_ws.write(iterations_row,3, iteration.end_date , date_xf)
            iterations_ws.write(iterations_row,4, len(stories) )
            iterations_ws.write(iterations_row,5, len(completed_stories) )
            iterations_ws.write(iterations_row,6, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            iterations_ws.write(iterations_row,7, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
            iterations_ws.write(iterations_row,8, iteration.starting_points()  )
            iterations_ws.write(iterations_row,9, iteration.max_points() )

            ic_ws.write(iteration_categories_row,1, iteration.name)
            ic_ws.write(iteration_categories_row,3, iteration.start_date , date_xf)
            ic_ws.write(iteration_categories_row,4, iteration.end_date , date_xf)
            ic_ws.write(iteration_categories_row,5, len(stories) )
            ic_ws.write(iteration_categories_row,6, len(completed_stories) )
            ic_ws.write(iteration_categories_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            ic_ws.write(iteration_categories_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )

            it_ws.write(iteration_tags_row,1, iteration.name)
            it_ws.write(iteration_tags_row,3, iteration.start_date , date_xf)
            it_ws.write(iteration_tags_row,4, iteration.end_date , date_xf)
            it_ws.write(iteration_tags_row,5, len(stories) )
            it_ws.write(iteration_tags_row,6, len(completed_stories) )
            it_ws.write(iteration_tags_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            it_ws.write(iteration_tags_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
            
            iterations_row += 1
            iteration_tags_row += 1
            iteration_categories_row += 1
            for category in icategories:
                stories = icategories[category]
                completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
                ic_ws.write(iteration_categories_row,2,category)
                ic_ws.write(iteration_categories_row,5, len(stories) )
                ic_ws.write(iteration_categories_row,6, len(completed_stories) )
                ic_ws.write(iteration_categories_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
                ic_ws.write(iteration_categories_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
                iteration_categories_row += 1

            for tag in itags :
                stories = itags[tag]
                completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
                it_ws.write(iteration_tags_row,2,tag)
                it_ws.write(iteration_tags_row,5, len(stories) )
                it_ws.write(iteration_tags_row,6, len(completed_stories) )
                it_ws.write(iteration_tags_row,7, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
                it_ws.write(iteration_tags_row,8, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
                iteration_tags_row += 1





    w.save(response)
    return response

def _write_headers(ws, headers):

    for idx,header in enumerate( headers ):
        ws.write(0,idx,header[0],heading_xf)
        ws.col(idx).width = 37*header[1]

########NEW FILE########
__FILENAME__ = models
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.core.urlresolvers import reverse
from django.contrib.auth.models import  User
from django.utils.translation import ugettext_lazy as _
from django.db import models

import datetime

from django.core.exceptions import ObjectDoesNotExist




class Team(models.Model):
    ACCESS_CHOICES = [
        ('read', 'Read Only'),
        ('write', 'Read / Write'),
        ('admin', 'Administrator'),
        ('staff', 'Staff Member'),]
    members = models.ManyToManyField(User, verbose_name=_('members'), related_name="teams")
    projects = models.ManyToManyField("projects.Project", verbose_name=_('projects'), related_name="teams")

    organization = models.ForeignKey('Organization', related_name="teams")

    name = models.CharField( max_length=65 )
    access_type = models.CharField( max_length=25 , default="read", choices=ACCESS_CHOICES)

    class Meta:
        ordering = ["name"]
    
    def access_description(self):
        access = [ entry[1] for entry in Team.ACCESS_CHOICES if entry[0]==self.access_type ]
        if len(access) == 1:
            return access[0]
        return "Read Only"

    def hasMember(self, user):
        return self.members.filter(id=user.id).count() > 0
        
    def __unicode__(self):
        return "[%s] %s" % (self.organization.name, self.name)


class TeamInvite(models.Model):
    email_address = models.CharField(max_length=60)
    team = models.ForeignKey(Team, related_name="invites")
    key = models.CharField(max_length=8)


# Was going to make these pinax groups, but didn't want to bring over the slug based urls for teams, and turns out organizations don't
# actually have members if we go the team route.

class Organization(models.Model):
    name = models.CharField( max_length=65 )
    slug = models.SlugField(_('slug'), unique=True)
    creator = models.ForeignKey(User, verbose_name=_('creator'), related_name="organizations_created")
    created = models.DateTimeField(_('created'), default=datetime.datetime.now)
    description = models.TextField(_('description'),  null=True, default="")
    source = models.CharField(max_length=100, default="", blank=True)

    def activeProjects(self):
        return self.projects.filter(active=True);

    # Returns all organizations
    @staticmethod
    def getOrganizationsForUser( user ):
        return Organization.objects.filter( teams__members = user ).distinct().order_by("name")

    # Returns all organizations the user has admin rights to.
    @staticmethod
    def getAdminOrganizationsForUser( user ):
        return Organization.objects.filter( teams__access_type="admin", teams__members = user ).distinct().order_by("name")

    def getOwnersGroup(self):
        teams = self.teams.filter( access_type="admin" )
        if len(teams) > 0:
            return teams[0]
        return None

    def projectsByCategory(self):
        return self.projects.all().order_by('-active','category','name')

    def hasStaffAccess( self, user ):
        if user.is_staff:
            return True
        if self.creator == user:
            return True
        return (self.teams.filter( access_type="staff", members=user ).count() > 0)

    def hasReadAccess( self, user ):
        if user.is_staff:
            return True
        if self.creator == user:
            return True
        return (self.teams.filter( members=user ).count() > 0)

    def memberCount(self):
        members = []
        for team in self.teams.all():
            for member in team.members.all():
                if members.count(member) == 0:
                    members.append(member)
        return len(members)

    def get_url_kwargs(self):
        return {'organization_slug': self.slug}

    def __unicode__(self):
        return "%s - %s" % (self.slug, self.name)

    def get_absolute_url(self):
        return reverse("organization_detail", kwargs={"organization_slug":self.slug})

########NEW FILE########
__FILENAME__ = signals
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


import django.dispatch

organization_created = django.dispatch.Signal(providing_args=["organization"])
organization_deleted = django.dispatch.Signal(providing_args=["organization"])

########NEW FILE########
__FILENAME__ = team_models
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

from django.core.urlresolvers import reverse
from django.contrib.auth.models import  User
from django.utils.translation import ugettext_lazy as _
from django.db import models

from django.core.exceptions import ObjectDoesNotExist

from projects.models import Project

from organizations.models import Organization

########NEW FILE########
__FILENAME__ = team_views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

from django.shortcuts import render_to_response, get_object_or_404
from django.template.loader import render_to_string
from django.template import RequestContext
from django.http import HttpResponseRedirect, HttpResponseForbidden, HttpResponse
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.utils.translation import ugettext_lazy as _
from django.http import HttpResponse
from django.core import serializers
from django.conf import settings
from django.core.exceptions import PermissionDenied
from django.forms.fields import email_re
import random
import string

from projects.models import Project
from organizations.forms import *
from organizations.models import *
from pinax.core.utils import get_send_mail
from pinax.core.utils import get_send_mail
send_mail = get_send_mail()



def _isAdmin( user, organization ):
    return Organization.objects.filter( teams__members = user , teams__access_type="admin", teams__organization=organization).count() > 0

@login_required
def team_add_project(request, organization_slug, team_id ):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    team = get_object_or_404(Team, id=team_id)
    if team.organization != organization:
        raise PermissionDenied() # Shenanigans
    project_id = request.POST.get("project")
    project = get_object_or_404(Project, id=project_id)    
    if project.organization != organization:
        raise PermissionDenied() # Shenanigans
    team.projects.add(project)
    return team_detail(request, organization_slug, team_id)
    
    
@login_required
def team_invite_accept(request, key):
    invite = get_object_or_404(TeamInvite, key=key)
    team = invite.team
    team.members.add( request.user )
    invite.delete()
    return HttpResponseRedirect(reverse("organization_detail", kwargs={"organization_slug":team.organization.slug} ))
    
@login_required
def team_invite(request, organization_slug, team_id ):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    team = get_object_or_404(Team, id=team_id)
    if team.organization != organization:
        raise PermissionDenied() # Shenanigans!
    userinput = request.POST.get("invitee")
    
    users = User.objects.filter(username__iexact=userinput)
    if len(users) == 0:
        users = User.objects.filter(email__iexact=userinput)
    
    if len(users) > 0:        
        new_member = users[0]
        team.members.add( new_member )
        team.save()
        return team_detail(request, organization_slug, team_id)
    
    if email_re.match( userinput ):
        key = ''.join(random.choice(string.letters + string.digits) for i in xrange(8))
        invite = TeamInvite(email_address=userinput, team=team, key=key)
        invite.save()
        
        subject = _("Invtation to ScrumDo team")
        message = render_to_string("organizations/team_invite_email.txt", {
            "invite": invite,
            "inviter": request.user
        })

        send_mail(subject, message, settings.DEFAULT_FROM_EMAIL, [userinput])
        
    return team_detail(request, organization_slug, team_id)
        


@login_required
def team_remove_project(request, organization_slug, team_id, project_id):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    team = get_object_or_404(Team, id=team_id)
    if team.organization != organization:
        raise PermissionDenied() # Shenanigans!
    project = Project.objects.get(id=project_id) 
    team.projects.remove(project)
    return HttpResponse("OK")

@login_required    
def team_remove_member(request, organization_slug, team_id, member_id):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    team = get_object_or_404(Team, id=team_id)
    if team.organization != organization:
        raise PermissionDenied() # Shenanigans!
    
    user = User.objects.filter(id=member_id)[0]
    if user == request.user and team.access_type=="staff":
        return HttpResponse("Can't remove yourself from the staff group.")
    else:
        team.members.remove(user);
        team.save()
    return HttpResponse("OK")
    
@login_required   
def team_summary(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    organizations = Organization.getOrganizationsForUser( request.user )
    if organization.hasStaffAccess(request.user):
        teams = organization.teams.all().order_by("name")
    else:
        teams = organization.teams.filter(members=request.user).order_by("name")

    return render_to_response("organizations/organization_teams.html", {
        "organization": organization,
        "organizations": organizations,
        "teams": teams
      }, context_instance=RequestContext(request))

@login_required   
def team_detail(request, organization_slug, team_id):
    organization = get_object_or_404(Organization, slug=organization_slug)    
    team = get_object_or_404(Team, id=team_id)
    
    if not (organization.hasStaffAccess( request.user ) or team.hasMember(request.user) ):
        raise PermissionDenied()
    

    if team.organization != organization:
        raise PermissionDenied() # Shenanigans!
        
    organizations = Organization.getOrganizationsForUser( request.user )
    
    return render_to_response("organizations/organization_team.html", {
        "team": team,
        "organization": organization,
        "organizations": organizations,
      }, context_instance=RequestContext(request))

@login_required   
def team_delete(request, organization_slug, team_id):
    organization = get_object_or_404(Organization, slug=organization_slug)
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    team = get_object_or_404(Team, id=team_id)
    if team.organization != organization:
        raise PermissionDenied()
    team.delete()
    return HttpResponse("Deleted")

@login_required   
def team_create(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)

    if request.method == 'POST': # If the form has been submitted...
        form = TeamForm( request.POST)
        if form.is_valid(): # All validation rules pass
            team = form.save( commit=False )
            team.organization = organization
            team.save()

            request.user.message_set.create(message="Team Created.")
            return HttpResponseRedirect(reverse("team_summary",  kwargs={'organization_slug':organization.slug}))
    else:
        form = TeamForm()

    organizations = Organization.getOrganizationsForUser( request.user )

    return render_to_response("organizations/create_team.html", {
        "form": form,
        "organization": organization,
        "organizations": organizations,
      }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = organizations_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from projects.forms import ProjectForm
from projects.models import Story
from projects.access import has_write_access, has_admin_access, has_read_access
from django.template.defaultfilters import stringfilter
import re
register = template.Library()



@register.tag(name="isorgstaff")
def isorgstaff( parser, token):
    tag_name, organization = token.split_contents()
    nodelist = parser.parse(('endisorgstaff',))
    parser.delete_first_token()
    return IsAdminNode(nodelist, organization)

class IsAdminNode(template.Node):
    def __init__(self, nodelist, organization):
        self.nodelist = nodelist
        self.organization = organization
    def render(self, context):
        if context[self.organization].hasStaffAccess(context["request"].user):
            output = self.nodelist.render(context)
            return output
        else:
            return ""



@register.tag(name="teammember")
def teammember( parser, token):
    tag_name, team = token.split_contents()
    nodelist = parser.parse(('endteammember',))
    parser.delete_first_token()
    return IsAdminNode(nodelist, team)

class IsTeamMemberNode(template.Node):
    def __init__(self, nodelist, team):
        self.nodelist = nodelist
        self.team = team
    def render(self, context):
        if context[self.team].hasMember( context["request"].user ):
            output = self.nodelist.render(context)
            return output
        else:
            return ""
            

########NEW FILE########
__FILENAME__ = urls
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

from django.conf.urls.defaults import *

from organizations.models import Organization, Team


# from groups.bridge import ContentBridge
#
#
# bridge = ContentBridge(Project, 'projects')

urlpatterns = patterns('organizations.views',
    url(r'^debug/$', "team_debug"),
    url(r'^create/$', 'organization_create', name="organization_create"),
    url(r'^(?P<organization_slug>[-\w]+)/projects$', 'organization_projects', name="organization_projects"),
    url(r'^(?P<organization_slug>[-\w]+)/dashboard$', 'organization_dashboard', name="organization_detail"),
    url(r'^(?P<organization_slug>[-\w]+)/dashboard$', 'organization_dashboard', name="organization_dashboard"),
    url(r'^(?P<organization_slug>[-\w]+)/edit$', 'organization_edit', name="organization_edit"),
    url(r'^(?P<organization_slug>[-\w]+)/export$', 'export_organization', name="export_organization"),
    url(r'^(?P<organization_slug>[-\w]+)/delete$', 'delete_organization', name="delete_organization"),
    url(r'^(?P<organization_slug>[-\w]+)/favorite_all$', 'favorite_all', name="favorite_all"),
)


urlpatterns += patterns('organizations.team_views',
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)/remove/(?P<member_id>[0-9]+)$', 'team_remove_member', name="team_remove_member"),
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)/remove_project/(?P<project_id>[0-9]+)$', 'team_remove_project', name="team_remove_project"),
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)$', 'team_detail', name="team_detail"),
   url(r'^(?P<organization_slug>[-\w]+)/teams$', 'team_summary', name="team_summary"),
   url(r'^(?P<organization_slug>[-\w]+)/team/create$', 'team_create', name="team_create"),  
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)/invite$', 'team_invite', name="team_invite"),   
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)/add_project$', 'team_add_project', name="team_add_project"),   
   url(r'^(?P<organization_slug>[-\w]+)/team/(?P<team_id>[0-9]+)/delete$', 'team_delete', name="team_delete"),   
   url(r'^accept/(?P<key>[-\w]+)$', 'team_invite_accept', name="team_invite_accept"),   
   
)
# http://www.scrumdo.com/organization/accept/bSg2iEct
########NEW FILE########
__FILENAME__ = views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponseRedirect, HttpResponseForbidden, HttpResponse
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.utils.translation import ugettext_lazy as _
from django.http import HttpResponse
from django.core import serializers
from django.conf import settings
from django.core.exceptions import PermissionDenied

from organizations.forms import *
from organizations.models import *
from activities.models import NewsItem
from projects.models import Project
from favorites.models import *
import projects.access as access
import organizations.signals as signals
import organizations.import_export as import_export

import re
import logging

logger = logging.getLogger(__name__)

@login_required
def organization_dashboard(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    # Organization.objects.filter(slug=organization_slug).select_related('subscription')[0]
        
    if organization.projects.count() == 0:
        return HttpResponseRedirect( reverse("organization_projects",kwargs={"organization_slug":organization_slug}))
    
    organizations = Organization.getOrganizationsForUser( request.user )
    
    favorite_projects = Favorite.objects.filter(user=request.user, project__organization=organization).select_related('project').order_by("-project__active","project__category","project__name")
    favorite_projects = [fav.project for fav in favorite_projects]
    
    stories = Story.getAssignedStories(request.user, organization)


    if not organization.hasReadAccess( request.user ):
        raise PermissionDenied()
    
    return render_to_response("organizations/organization_dashboard.html", {
        "organization": organization,
        "organizations": organizations,
        "favorite_projects": favorite_projects,
        "your_stories": stories,
        "return_type":"queue"
        # "members": members,
        # "projects": projects
      }, context_instance=RequestContext(request))
    
@login_required
def organization_projects(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    organizations = Organization.getOrganizationsForUser( request.user )

    if not organization.hasReadAccess( request.user ):
        raise PermissionDenied()

    teams = []
    for team in organization.teams.all():
        teams.append((team, AddUserForm(team=team)))

    # this used to live in team_views.py, but since the individual teams pages
    # have been consolodated, this post is now from the organization overview page
    if request.method == "POST":
        if not organization.hasStaffAccess( request.user ):
            raise PermissionDenied()
        action = request.POST.get("action")
        team_id = request.POST.get("team_id")
        team = get_object_or_404(Team, id=team_id)
        if action == "addMember":
            adduser_form = AddUserForm(request.POST, team=team)
            if adduser_form.is_valid():
                adduser_form.save(request.user)
                adduser_form=AddUserForm(team=team)
        elif action == "addProject":
            if not request.POST.get("project",None):
                request.user.message_set.create(message="Please select a project to add to this team.")
            else:
                project = get_object_or_404( Project, id=request.POST.get("project") )
                team.projects.add(project)
                team.save()
        elif action == "removeProject":
            project = Project.objects.filter(id=request.POST.get("project_id"))[0]
            team.projects.remove(project)
            team.save()
        elif action == "removeUser":
            user = User.objects.filter(id=request.POST.get("user_id"))[0]
            if user == request.user and team.access_type=="staff":
                request.user.message_set.create(message="Can't remove yourself from the staff group.")
            else:
                team.members.remove(user);
                team.save()

    members = []
    users = []
    member_count = 1

    # for team in organization.teams.all():
    #     for user in team.members.all():
    #         if not user in users:                
    #             users.append(user)
    #             members.append("#%d %s (Team %s)" % (member_count,  user, team.name))
    #             member_count+=1

    projects = organization.projects.all().order_by("-active","category","name")

    # for project in projects:
    #     for member in project.members.all():
    #         if (not member.user in users) and (member.user != project.creator):
    #             users.append(member.user)
    #             members.append("#%d %s (Project %s)" % (member_count, member.user, project.name))
    #             member_count+=1

    return render_to_response("organizations/organization_projects.html", {
        "organization": organization,
        "organization_teams": teams,
        "organizations": organizations,
        "members": members,
        "projects": projects
      }, context_instance=RequestContext(request))


# This is an ajax popup form for editing details of an organization.
@login_required
def organization_edit( request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    if request.method == "POST" and organization.creator == request.user:
        form = UpdateOrganizationForm(request.POST, instance=organization)
        if form.is_valid():
            request.user.message_set.create(message="Organization Updated")
            form.save()
        else:
            request.user.message_set.create(message="Could not update organization.")

        return HttpResponseRedirect( reverse("organization_detail",kwargs={"organization_slug":organization.slug}))

    form = UpdateOrganizationForm(instance=organization)
    organizations = Organization.getOrganizationsForUser( request.user )
    return render_to_response("organizations/organization_form.html", {
        "organization": organization,
        "organizations": organizations,
        "form":form
      }, context_instance=RequestContext(request))

def handle_organization_create( form , request, projects):
    organization = form.save( commit=False )
    organization.creator = request.user
    organization.save()


    member_team = Team(organization = organization, name="Members", access_type="write")
    member_team.save()
    
    staff_team = Team(organization = organization, name="Staff", access_type="staff")
    staff_team.save()
    staff_team.members.add(request.user)
    
    
    request.user.message_set.create(message="Organization Created.")
    
    try:
        # Store where this user came from via the google analytics tracking cookie.  Example values:
        # 183024036.1310791887.2.2.utmcsr=freshmeat.net|utmccn=(referral)|utmcmd=referral|utmcct=/projects/scrumdo; 
        # 183024036.1310842365.1.2.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=scrumdo%20harvest
        # 183024036.1311098060.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)
        # Adwords:
        # 37276784.1312040997.1.1.utmgclid=CPCFncq1qaoCFYne4AodhCvMWw|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=scrum
        
        cookie = request.COOKIES.get("__utmz")
        if re.search("utmgclid", cookie) == None:
            # Referrer based source
            source = re.search("utmcsr=([^|]+)",cookie).group(1)
            mode = re.search("utmcmd=([^|]+)",cookie).group(1)
            organization.source = "%s / %s" % (source, mode)
        else:
            # Adwords based source?
            source = re.search("utmgclid=([^|]+)",cookie).group(1)
            mode = re.search("utmctr=([^|]+)",cookie).group(1)
            organization.source = "Adwords / %s / %s" % (source, mode)
                
        organization.save()        
    except:
        organization.source = ""
        
    signals.organization_created.send( sender=request, organization=organization )    

    for project in projects:
        if request.POST.get("move_project_%d" % project.id):
            _move_project_to_organization(project, organization, member_team)
            # member_team.projects.add( project )

    return organization

@login_required
def organization_create(request):
    projects = Project.objects.filter(creator=request.user)

    if request.method == 'POST': # If the form has been submitted...
        form = OrganizationForm( request.POST)

        if form.is_valid(): # All validation rules pass
            organization = handle_organization_create(form, request, projects )
            return HttpResponseRedirect(reverse("organization_detail",  kwargs={'organization_slug':organization.slug}))
    else:
        form = OrganizationForm()

    organizations = Organization.getOrganizationsForUser( request.user )

    return render_to_response("organizations/create_organization.html", {
        "organizations": organizations,
        "form": form,
        "projects": projects
      }, context_instance=RequestContext(request))

def _move_project_to_organization(project, organization, member_team):
    project.teams.clear()
    project.organization = organization
    member_team.projects.add(project)

    for membership in project.members.all():
        member = membership.user
        if member != project.creator:
            member_team.members.add( member )
            membership.delete()
    member_team.save()
    project.save()

@login_required
def favorite_all(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    for project in organization.projects.all():
        if project.active and access.has_read_access(project, request.user):
            Favorite.setFavorite( 1, project.id, request.user, True)
    return HttpResponseRedirect( reverse("organization_detail",kwargs={"organization_slug":organization_slug}))
    
@login_required
def team_debug(request):
    read_orgs = Organization.getOrganizationsForUser(request.user)
    admin_orgs = Organization.getAdminOrganizationsForUser(request.user)
    # write_orgs = Organization.getReadWriteOrganizationsForUser(request.user)
    return render_to_response("organizations/team_debug.html", {
        "read_orgs":read_orgs,
        "admin_orgs":admin_orgs,
      }, context_instance=RequestContext(request))

@login_required
def export_organization(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    if not organization.hasReadAccess( request.user ):
        raise PermissionDenied()

    organizations = Organization.getOrganizationsForUser( request.user )

    if request.method == "POST":
        projects = []
        for key, value in request.POST.iteritems():
            m = re.match("proj_([0-9]+)",key)
            if m and value:
                projects.append(int(m.group(1)) )
        logger.debug(projects)
        return import_export.export_organization( organization, project_ids=projects)


    return render_to_response("organizations/organization_export.html", {
        "organization":organization,
        "organizations":organizations,
      }, context_instance=RequestContext(request))
    


@login_required
def delete_organization(request, organization_slug):
    organization = get_object_or_404(Organization, slug=organization_slug)
    if not organization.hasStaffAccess( request.user ):
        raise PermissionDenied()
    signals.organization_deleted.send( sender=request, organization=organization )
    for project in organization.projects.all():
        project.organization = None
        project.save()
    for team in organization.teams.all():
        team.delete()
    organization.delete()
    return HttpResponse("Deleted")

########NEW FILE########
__FILENAME__ = access
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from projects.models import Project
from organizations.models import Organization, Team

from django.core.exceptions import PermissionDenied

from django.core.cache import cache

CACHE_PERMISSION_SECONDS = 15

def admin_access_or_403(project,user, ignore_active=False):
    if not has_admin_access(project, user, ignore_active=ignore_active):
        raise PermissionDenied()

def read_access_or_403(project,user):
    if not has_read_access(project, user):
        raise PermissionDenied()

def write_access_or_403(project,user):
    if not has_write_access(project, user):
        raise PermissionDenied()

def has_staff_access(organization, user):
    try:
        # if user.is_staff:
        #     return True
        key = cache_key(organization, user, "staff")
        cached_value = cache.get(key)
        if cached_value == None:
            access = organization.teams.filter(access_type="staff",members = user).count() > 0
            cache.set(key, access, CACHE_PERMISSION_SECONDS)
            return access
        else:
            return cached_value
    except:
        return False

def has_admin_access( project, user , ignore_active=False):
    try:
        if not ignore_active:
            if not project.active:
                return False
        key = cache_key(project, user, "admin")
        cached_value = cache.get(key)
        if cached_value == None:
            access = real_has_admin_access(project,user)
            cache.set(key, access, CACHE_PERMISSION_SECONDS)
            return access
        else:
            return cached_value
    except:
        return False

def real_has_admin_access(project, user):
    if user.is_staff:
        return True
    if project.creator == user: 
        return True
    if has_staff_access(project.organization, user):
        return True    
    return Organization.objects.filter( teams__members = user , teams__access_type="admin", teams__projects=project).count() > 0            

def has_write_access( project, user ):
    try:
        if not project.active:
            return False
        key = cache_key(project, user, "write")
        cached_value = cache.get(key)
        if cached_value == None:
            access = real_has_write_access(project,user)
            cache.set(key, access, CACHE_PERMISSION_SECONDS)
            return access
        else:
            return cached_value
    except:
        return False

def real_has_write_access( project, user ):
    if user.is_staff:
        return True
    if has_admin_access( project, user):
        return True
    if project.members.filter(user__id=user.id).count() > 0:
        return True
    return Organization.objects.filter( teams__members = user , teams__access_type="write", teams__projects=project).count() > 0

def has_read_access( project, user ):
    try:
        if not project.private:
        # A public project!
            return True

        if user.is_staff:
            return True

        key = cache_key(project, user, "read")
        cached_value = cache.get(key)
        if cached_value == None:
            access = real_has_read_access(project,user)
            cache.set(key, access, CACHE_PERMISSION_SECONDS)
            return access
        else:
            return cached_value
    except:
        return False


def real_has_read_access(project,user):
    if has_admin_access( project, user):
        return True
    if has_write_access(project, user):
        return True
    return Organization.objects.filter( teams__members = user , teams__projects=project).count() > 0

def cache_key( project, user, acc_type):
    return "acc_%s_%s_%d" % (acc_type, project.slug, user.id)

########NEW FILE########
__FILENAME__ = admin
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.contrib import admin
from projects.models import *
from django.conf import settings


if not "subscription" in settings.INSTALLED_APPS:
    class ProjectAdmin(admin.ModelAdmin):
        list_display = ('name', 'slug', 'creator', 'created')
    admin.site.register(Story)
    admin.site.register(Project, ProjectAdmin)


class TaskAdmin(admin.ModelAdmin):
    list_display = ('story', 'summary', 'complete')
    search_fields = ('summary',)

admin.site.register(Iteration)
admin.site.register(SiteStats)
admin.site.register(Epic)
admin.site.register(StoryTag)
admin.site.register(Task, TaskAdmin)

########NEW FILE########
__FILENAME__ = calculation
from datetime import date, timedelta
from apps.projects.models import Project, Iteration, Story, PointsLog
from django.core.management.base import BaseCommand, CommandError

from projects.limits import on_demand_velocity

import logging

logger = logging.getLogger(__name__)

def onDemandCalculateVelocity( project ):
    if not on_demand_velocity.increaseAllowed(project=project):
        return
    calculateProject( project )

def calculatePoints( stories, epics ):
    points_total = 0;
    points_claimed = 0;
    for story in stories:
        try:
            story_points = story.points_value()
            points_total += story_points
            if( story.status == Story.STATUS_DONE):
                points_claimed += story_points
        except ValueError:
            pass # probably ? or infinity

    for epic in epics:
        try:
            points_total += epic.normalized_points_value()
        except ValueError:
            pass


    return (points_total, points_claimed)

def calculateProjectVelocity( project, total_project_points ):
    today = date.today()
    # Loop through all completed iterations and gather info
    iteration_points = []
    for iteration in project.iterations.filter( end_date__lte=today):
        if not iteration.default_iteration and iteration.include_in_velocity:
            points = 0
            for story in iteration.stories.all():
                if story.status == Story.STATUS_DONE:
                    try:
                        points += story.points_value()
                    except ValueError:
                        pass # probably ? or infinity
            iteration_points.append( points )

    velocity = 0
    if project.velocity_type == project.VELOCITY_TYPE_AVERAGE:
        velocity = calculateAverage( iteration_points )
    elif project.velocity_type == project.VELOCITY_TYPE_AVERAGE_5:
        velocity = calculateAverageLastN( iteration_points , 5)
    elif project.velocity_type == project.VELOCITY_TYPE_AVERAGE_3:
        velocity = calculateAverageLastN( iteration_points , 3)
    else:
        velocity = calculateMedian( iteration_points )

    project.velocity = velocity
    if project.velocity > 0:
        project.iterations_left = int( total_project_points / project.velocity)

    project.save();


def calculateMedian( iteration_points ):
    if len(iteration_points) == 0:
        return 0
    sorted_list = sorted( iteration_points )
    return sorted_list[ int(len( iteration_points ) / 2 ) ]



def calculateAverageLastN( iteration_points , n):
    if len(iteration_points) <= n:
        return calculateAverage( iteration_points )
    total = sum( iteration_points[-n:] )
    return total / n



def calculateAverage( iteration_points ):
    if len( iteration_points ) == 0:
        return 0
    total = sum( iteration_points )
    #print "%d / %d" % (total, len(iteration_points))
    return total / len( iteration_points )


def logPoints( related_object, points_claimed, points_total ):
    today = date.today()
    # logger.info("%s %d %f" % (related_object, points_claimed, points_total) )
    try:
        log = related_object.points_log.get( date=today )
        log.points_claimed=points_claimed
        log.points_total=points_total
        log.save()
        # logger.debug("logPoints found a previous record.")
    except:
        log = PointsLog( points_claimed=points_claimed, points_total=points_total, related_object=related_object)
        log.save()
        # logger.debug("logPoints created a new record.")

def calculateProject( project ):
    if not project.active:
        return
    stories = project.stories.all()
    epics = project.epics.all().exclude(archived=True)
    points = calculatePoints( stories, epics )

    total_project_points = points[0]

    today = date.today()

    logPoints(project, points[1], points[0])
    # log = PointsLog( points_claimed=points[1], points_total=points[0], related_object=project)
    # log.save();


    yesterday = today - timedelta( days=1 )
    tomorrow = today +  timedelta( days=1 )
    points_total = 0

    calculateProjectVelocity( project , total_project_points)

    for iteration in project.iterations.filter( start_date__lte=tomorrow, end_date__gte=yesterday):
        if( iteration != project.get_default_iteration() ):
            points = calculatePoints( iteration.stories.all(), [] );
            if points[0] > 0:  # only logging active iterations with stuff in them
                logPoints(iteration, points[1], points[0])
                # log = PointsLog( points_claimed=points[1], points_total=points[0], related_object=iteration)
                # log.save();

########NEW FILE########
__FILENAME__ = context_processors
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.conf import settings # import the settings file

import logging

logger = logging.getLogger(__name__)

def projects_constants(request):

    # AWS Does it this way:
    forwarded_ssl = ('HTTP_X_FORWARDED_SSL' in request.META) 
    
    # Other proxy's do it this way:
    if 'HTTP_X_FORWARDED_PROTO' in request.META:
        forwarded_ssl = (request.META['HTTP_X_FORWARDED_PROTO'] == 'https')
                    
    # is_secure for direct ssl request
    if request.is_secure() or forwarded_ssl:
        static_url = settings.SSL_STATIC_URL
    else:
        static_url = settings.STATIC_URL

    # return the value you want as a dictionnary. you may add multiple values in there.
    return {'GOOGLE_ANALYTICS': settings.GOOGLE_ANALYTICS,
            'GOOGLE_ANALYTICS_ACCOUNT': settings.GOOGLE_ANALYTICS_ACCOUNT ,
            'BASE_URL': settings.BASE_URL,
            'SUPPORT_URL': settings.SUPPORT_URL,
            'STATIC_URL':static_url }

########NEW FILE########
__FILENAME__ = forms
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import forms
from django.conf import settings
from django.contrib.auth.models import User
from django.utils.translation import ugettext_lazy as _

from projects.models import Project, ProjectMember, Iteration, Story, Task, Epic
from django.forms.extras.widgets import SelectDateWidget

from projects.limits import userIncreasedAlowed


if "notification" in settings.INSTALLED_APPS:
    from notification import models as notification
else:
    notification = None

# @@@ we should have auto slugs, even if suggested and overrideable

class IterationForm(forms.ModelForm):
    def __init__(self,  *args, **kwargs):
        super(IterationForm, self).__init__(*args, **kwargs)
        self.fields['include_in_velocity'].label = "Include In Velocity Calculations"

    class Meta:
        model = Iteration
        fields = ('name', 'start_date', 'end_date', 'include_in_velocity')



class ProjectOptionsForm(forms.ModelForm):
    def __init__(self, *args, **kwargs):
        super(ProjectOptionsForm, self).__init__(*args, **kwargs)
        self.fields["name"].widget.attrs["style"] = 'width:595px;'
        self.fields["categories"].widget.attrs["style"] = 'width:595px;'
        self.fields["description"].widget.attrs["style"] = 'width:600px; height:75px;'



    class Meta:
        model = Project
        fields = ('category','categories','velocity_type','point_scale_type', 'use_extra_1', 'use_extra_2', 'use_extra_3', 'use_tasks', 'use_assignee', 'extra_1_label', 'extra_2_label', 'extra_3_label','name', 'description' )

class TaskForm( forms.ModelForm ):
    def __init__(self, project, *args, **kwargs):
        super(TaskForm, self).__init__(*args, **kwargs)
        members = project.all_member_choices()
        members.insert(0,("","Nobody"))
        self.fields["assignee"].choices = members
        self.fields["summary"].widget = forms.widgets.TextInput(attrs={'size':'50'})
    class Meta:
        model = Task
        fields = ('complete','summary','assignee')

class AddStoryForm( forms.ModelForm ):
    RANK_CHOICES = (
        ('0', 'Top'),
        ('1', 'Middle'),
        ('2', 'Bottom') )
    project = None
    tags = forms.CharField( required=False )
    general_rank = forms.CharField( required=False, widget=forms.RadioSelect(choices=RANK_CHOICES), initial=2)
    def __init__(self, project, *args, **kwargs):
        super(AddStoryForm, self).__init__(*args, **kwargs)
        self.fields["points"].choices = project.getPointScale()
        self.fields["points"].widget = forms.RadioSelect(choices=project.getPointScale())
        self.fields["summary"].widget = forms.widgets.Textarea(attrs={'rows':1, 'cols':50})
        self.fields["summary"].widget.size = 200
        members = project.all_member_choices()
        members.insert(0,("","---------"))
        self.fields["assignee"].choices = members
        self.fields["extra_1"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        self.fields["extra_2"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        self.fields["extra_3"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        
        epics = [ (epic.id, ("#E%d %s" % (epic.local_id, epic.summary))[:150] ) for epic in project.epics.all().order_by("local_id") ]
        epics.insert(0,("","----------") )
        self.fields["epic"].choices=epics
        self.fields["epic"].required = False
        
        if project.categories:
            choices = [(c.strip(),c.strip()) for c in project.categories.split(",")]
            choices.insert(0,("","----------"))
            self.fields["category"] = forms.ChoiceField(choices=choices, required=False)
        self.fields["assignee"].required = False
        self.fields["extra_1"].required = False
        self.fields["extra_2"].required = False
        self.fields["extra_3"].required = False
        self.fields["tags"].initial = self.instance.tags
    def save(self,  **kwargs):
        self.instance.tags = self.cleaned_data["tags"]
        return super(AddStoryForm, self).save(**kwargs)
    class Meta:
        model = Story
        fields = ('summary', 'detail', 'category', 'tags', 'points' , 'extra_1','extra_2','extra_3','assignee','epic')

class EpicForm( forms.ModelForm ):
    def __init__(self, project,  *args, **kwargs):
        super(EpicForm, self).__init__(*args, **kwargs)
        self.project = project

        self.fields["summary"].widget = forms.TextInput()
        self.fields["summary"].widget.attrs['size'] = 60
        
        epics = [ (epic.id, ("#E%d %s" % (epic.local_id, epic.summary)[:150]) ) for epic in project.epics.all().order_by("local_id") ]
        epics.insert(0,("","----------") )
        self.fields["parent"].choices=epics
        self.fields["parent"].required = False        
        

    def clean_points(self):
        try:
            self.cleaned_data["points"] = float(self.cleaned_data["points"])
            if self.cleaned_data["points"] > 9999 :
                raise forms.ValidationError("Points must be less than 10,000")
        except:
            self.cleaned_data["points"] = "?"
        return self.cleaned_data["points"]

    def save(self,  **kwargs):
        self.instance.project = self.project
        if not self.instance.local_id:
            self.instance.local_id = self.project.getNextEpicId()
        archived = self.cleaned_data["archived"]
        for epic in self.instance.children.exclude(archived=archived):
            epic.archived = archived
            epic.save()
            
        return super(EpicForm, self).save(**kwargs)

    class Meta:
        model = Epic
        fields = ('summary','detail','points','parent', 'archived')


class StoryForm( forms.ModelForm ):
    RANK_CHOICES = (
        ('0', 'Top'),
        ('1', 'Middle'),
        ('2', 'Bottom') )
    project = None
    tags = forms.CharField( required=False )
    def __init__(self, project, *args, **kwargs):
        super(StoryForm, self).__init__(*args, **kwargs)
        self.fields["points"].choices = project.getPointScale()
        self.fields["points"].widget = forms.RadioSelect(choices=project.getPointScale())
        self.fields["summary"].widget = forms.widgets.Textarea(attrs={'rows':1, 'cols':50})
        self.fields["summary"].widget.size = 200
        members = project.all_member_choices()
        members.insert(0,("","---------"))
        self.fields["assignee"].choices = members
        
        epics = [ (epic.id, "#E%d %s" % (epic.local_id, epic.summary) ) for epic in project.epics.exclude(archived=True).order_by("local_id") ]
        epics.insert(0,("","----------") )
        self.fields["epic"].choices=epics
        self.fields["epic"].required = False
        
        self.fields["detail"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        self.fields["extra_1"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        self.fields["extra_2"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        self.fields["extra_3"].widget = forms.widgets.Textarea(attrs={'rows':3, 'cols':50})
        if project.categories:
            choices = [(c.strip(),c.strip()) for c in project.categories.split(",")]
            choices.insert(0,("",""))
            self.fields["category"] = forms.ChoiceField(choices=choices, required=False)
        self.fields["assignee"].required = False
        self.fields["extra_1"].required = False
        self.fields["extra_2"].required = False
        self.fields["extra_3"].required = False
        self.fields["tags"].initial = self.instance.tags
    def save(self,  **kwargs):
        self.instance.tags = self.cleaned_data["tags"]
        return super(StoryForm, self).save(**kwargs)
    class Meta:
        model = Story
        fields = ('summary', 'detail', 'tags', 'points' , 'category', 'extra_1','extra_2','extra_3','assignee', 'epic')


class ProjectForm(forms.ModelForm):

    slug = forms.SlugField(max_length=20,
        help_text = _("a short version of the name consisting only of letters, numbers, underscores and hyphens."),
        error_message = _("This value must contain only letters, numbers, underscores and hyphens."))

    def clean_slug(self):
        if Project.objects.filter(slug__iexact=self.cleaned_data["slug"]).count() > 0:
            raise forms.ValidationError(_("A project already exists with that slug."))
        return self.cleaned_data["slug"].lower()

    # def clean_name(self):
    #     if Project.objects.filter(name__iexact=self.cleaned_data["name"]).count() > 0:
    #         raise forms.ValidationError(_("A project already exists with that name."))
    #     return self.cleaned_data["name"]

    class Meta:
        model = Project
        fields = ('name', 'slug', 'description')


# @@@ is this the right approach, to have two forms where creation and update fields differ?

class ProjectUpdateForm(forms.ModelForm):

    def clean_name(self):
        if Project.objects.filter(name__iexact=self.cleaned_data["name"]).count() > 0:
            if self.cleaned_data["name"] == self.instance.name:
                pass # same instance
            else:
                raise forms.ValidationError(_("A project already exists with that name."))
        return self.cleaned_data["name"]

    class Meta:
        model = Project
        fields = ('name', 'description')

class FindStoryForm(forms.Form):
    text = forms.CharField(label="Search For")

class IterationImportForm(forms.Form):
    import_file  = forms.FileField(required=False)

class IterationImportFormWithUnlock(forms.Form):
    import_file  = forms.FileField(required=False)
    unlock_iteration = forms.BooleanField( required=False, help_text = _("Unlocking the iteration allows users with the appropriate access to edit stories in this iteration.") )

class UnlockForm(forms.Form):
    unlock_iteration = forms.BooleanField( required=False, help_text = _("Unlocking the iteration allows users with the appropriate access to edit stories in this iteration.") )

class ExportForm(forms.Form):
    format = forms.ChoiceField(choices=(("xls","Excel"),("csv","Comma Seperated Value (CSV)"),("xml","XML") ) )
    lock_iteration = forms.BooleanField( required=False, help_text = _("Locking the iteration prevents anyone from editing any stories in it until the iteration is unlocked.") )
    file_name = forms.CharField( required=False, help_text = _("Name of the file when saved to disk.") )

class ExportProjectForm(forms.Form):
    #format = forms.ChoiceField(initial="sheet", choices=(("sheet","Export each iteration as a seperate sheet."),("combined","Export one combined sheet.") ) , widget=forms.RadioSelect() )
    file_name = forms.CharField( required=False, help_text = _("Name of the file when saved to disk.") )



class AddUserForm(forms.Form):

    recipient = forms.CharField(label=_(u"Username or Email Address"))

    def __init__(self, *args, **kwargs):
        self.project = kwargs.pop("project")
        self.user = kwargs.pop("user")
        super(AddUserForm, self).__init__(*args, **kwargs)

    def clean_recipient(self):
        try:
            user = User.objects.get(username__exact=self.cleaned_data['recipient'])
        except User.DoesNotExist:
            raise forms.ValidationError(_("There is no user with this username."))

        if ProjectMember.objects.filter(project=self.project, user=user).count() > 0:
            raise forms.ValidationError(_("User is already a member of this project."))

        if not userIncreasedAlowed(self.project, self.user, user):
            raise forms.ValidationError(_("Upgrade your account to add more users."))

        return self.cleaned_data['recipient']

    def save(self, user):
        new_member = User.objects.get(username__exact=self.cleaned_data['recipient'])
        project_member = ProjectMember(project=self.project, user=new_member)
        project_member.save()
        self.project.members.add(project_member)
        if notification:
            notification.send(self.project.member_users.all(), "projects_new_member", {"new_member": new_member, "project": self.project})
            notification.send([new_member], "projects_added_as_member", {"adder": user, "project": self.project})
        user.message_set.create(message="added %s to project" % new_member)

########NEW FILE########
__FILENAME__ = import_export
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA



import StringIO
import csv
import re

from xml.dom.minidom import Document, parse
from django.http import HttpResponse
import codecs
import cStringIO
import xlwt
from xlrd import open_workbook

from projects.models import Story, STATUS_CHOICES, STATUS_REVERSE
import logging

logger = logging.getLogger(__name__)

ezxf = xlwt.easyxf

def exportIteration(iteration, format, file_name=None ):
    """ Exports an iteration, format should be xls, xml or csv. """
    logger.info("Exporting iteration %s %d" % (iteration.project.slug, iteration.id) )
    if format.lower() == "xls" :
        return _exportExcel( iteration, file_name )
    elif format.lower() == "xml":
        return _exportXML( iteration, file_name )
    else:
        return _exportCSV( iteration, file_name )


def importIteration(iteration, file , user):
    """ Imports data to an iteration.  Both updating and creating stories is supported.
        file can be either Excel, XML, or CSV """
    m = re.search('\.(\S+)', file.name)

    if m.group(1).lower() == "xml" :
        return _importXMLIteration(iteration, file, user)
    elif m.group(1).lower() == "xls" :
        return _importExcelIteration(iteration, file, user)
    elif m.group(1).lower() == "xlsx" :
        logger.info("Tried to import xlsx file :(")
        user.message_set.create(message="Please save your file as an .xls Excel file before importing.")
        return False
    else:
        # Assume CSV, hope for the best.
        return _importCSVIteration(iteration, file, user)


def exportProject( project, file_name=None ):
    if not file_name:
        file_name = "project"
    response = HttpResponse( mimetype="Application/vnd.ms-excel")
    response['Content-Disposition'] = 'attachment; filename=%s.xls'%file_name 
    stories = project.stories.all().order_by("iteration","rank")
    w = xlwt.Workbook(encoding='utf8')
    ws = w.add_sheet( "All Stories" )
    headers = _getHeaders(project)
    iter_format = ezxf('align: wrap on, vert top')
    headers = [(100,"Iteration", lambda story: story.iteration.name , iter_format , None)] + headers

    heading_xf = ezxf('font: bold on; align: wrap on, vert centre, horiz center')
    date_xf = xlwt.XFStyle()
    date_xf.num_format_str = 'MM/dd/YYYY'

    # Write out a header row.
    for idx,header in enumerate(headers):
        ws.write(0,idx,header[1],heading_xf)
        ws.col(idx).width = 37*header[0]

    # Write out the first sheet of all the stories
    for idx, story in enumerate(stories):
        for hidx, header in enumerate(headers):
            f = header[2]
            ws.write(1+idx,hidx, f(story), header[3] )

    headers = _getHeaders(project)

    # Create the iteration sheet (it gets filled in below the tags sheet)
    iteration_ws = w.add_sheet("Iterations")
    for idx,header in enumerate( [("Iteration",150),("Start",100),("End",100),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60),("Starting Points", 60), ("Max Points",60) ] ):
        iteration_ws.write(0,idx,header[0],heading_xf)
        iteration_ws.col(idx).width = 37*header[1]

    # Collect tags
    tags = {}
    for story in stories:
        for tag in story.story_tags.all():
            tagname = tag.name
            if tagname in tags:
                tags[tagname].append( story )
            else:
                tags[tagname] = [story]

    # Write out the tags sheet.
    if len( tags ) > 0:
        ws = w.add_sheet( "Tags" )
        for idx,header in enumerate( [("Tag",150),("Stories",50),("Stories Claimed",60),("Points",50),("Points Claimed",60)] ):
            ws.write(0,idx,header[0],heading_xf)
            ws.col(idx).width = 37*header[1]
        for idx,tag in enumerate( tags ):
            stories = tags[tag]
            completed_stories = [story for story in stories if story.status==Story.STATUS_DONE ]
            ws.write(idx+1,0,tag)
            ws.write(idx+1,1, len(stories) )
            ws.write(idx+1,2, len(completed_stories) )
            ws.write(idx+1,3, reduce( lambda total,story: total+story.points_value(), stories, 0 ) )
            ws.write(idx+1,4, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )


    # Write data to the iteration sheet, plus create one sheet per iteration.
    for itIdx, iteration in enumerate(project.iterations.all()):
        iteration_stories = iteration.stories.all()
        completed_stories = [story for story in iteration_stories if story.status==Story.STATUS_DONE ]
        iteration_ws.write(itIdx+1, 0, iteration.name )
        iteration_ws.write(itIdx+1, 1, iteration.start_date , date_xf)
        iteration_ws.write(itIdx+1, 2, iteration.end_date , date_xf)
        iteration_ws.write(itIdx+1, 3, len(iteration_stories) )
        iteration_ws.write(itIdx+1, 4, len(completed_stories) )
        iteration_ws.write(itIdx+1, 5, reduce( lambda total,story: total+story.points_value(), iteration_stories, 0 ) )
        iteration_ws.write(itIdx+1, 6, reduce( lambda total,story: total+story.points_value(), completed_stories, 0 ) )
        iteration_ws.write(itIdx+1, 7, iteration.starting_points()  )
        iteration_ws.write(itIdx+1, 8, iteration.max_points() )
        ws = w.add_sheet( cleanWorksheetName(iteration.name) )

        for idx,header in enumerate(headers):
            ws.write(0,idx,header[1],heading_xf)
            ws.col(idx).width = 37*header[0]
        for idx, story in enumerate(iteration_stories):
            for hidx, header in enumerate(headers):
                f = header[2]
                ws.write(1+idx,hidx, f(story), header[3] )


    w.save(response)
    return response


def _getHeaders( project ):
    """Returns an array of tupples with info on columns.
        (target width, title, function to get the data from a story, excel output format, function to assign the value to a story)
    """
    # There's some excel-specific data mixed in here that doesn't entirely fit, but I'm leaving it for now.
    wrap_xf = ezxf('align: wrap on, vert top')
    numeric_xf = ezxf('align: wrap on, vert top, horiz right')

    # Some methods to define how imported field values are set in a story.
    # This is one place we can do any logic to clean up the data.
    def intOrString( value ):
        try:
            if int(value) == float(value):
                return int(float(value))
        except:
            pass
        return value
    def setId(story,value):
        pass
    def setSummary(story,value):
        story.summary=unicode(value)
    def setDetail(story,value):
        story.detail=unicode(value)
    def setPoints(story,value):
        try:
            story.points="%g" % value
        except ValueError:
            if value == "":
                story.points = "?"
            else:
                story.points = str(value)


    def setStatus(story,value):
        try:
            story.status = STATUS_REVERSE[value]
        except:
            pass # Ignore invalid statuses?
    def setAssignee(story,value):
        member = story.project.get_member_by_username(value)
        story.assignee = member

    def setRank(story,value):
        try:
            story.rank = int(value)
        except:
            story.rank = story.iteration.stories.count()
    def setExtra1(story,value):
        story.extra_1 = unicode(value)
    def setExtra2(story,value):
        story.extra_2 = unicode(value)
    def setExtra3(story,value):
        story.extra_3 = unicode(value)
    def setTags( story, value ):
        story.tags = unicode(value)
    def setCategory( story, value ):
        story.category = unicode(value)

    headers = [ (50,"Story ID", lambda story: story.local_id ,numeric_xf, setId),
               (350,"Summary", lambda story: story.summary,wrap_xf, setSummary),
               (300,"Detail", lambda story: story.detail ,wrap_xf, setDetail),
               (50,"Points", lambda story: int(story.points) if story.points.isdigit() else story.points, numeric_xf, setPoints),
               (70,"Status", lambda story: STATUS_CHOICES[story.status-1][1] ,wrap_xf, setStatus),
               (50,"Rank", lambda story: story.rank,numeric_xf ,  setRank),
               (80,"Tags", lambda story: story.tags,wrap_xf,  setTags),
               (80,"Category", lambda story: story.category,wrap_xf, setCategory) ]


    # And some optional columns that depend on project settings:

    if project.use_assignee:
        headers.insert(6, (70,"Assignee", lambda story:  story.assignee.username if story.assignee is not None else "" ,wrap_xf, setAssignee))

    if project.use_extra_1:
        headers.append((200,project.extra_1_label, lambda story: story.extra_1,wrap_xf,  setExtra1))

    if project.use_extra_2:
        headers.append( (200,project.extra_2_label, lambda story: story.extra_2,wrap_xf,  setExtra2) )

    if project.use_extra_3:
        headers.append( (200,project.extra_3_label, lambda story: story.extra_3,wrap_xf,  setExtra3) )

    return headers


def _exportExcel( iteration, file_name=None ):
    """ Exports the stories in an iteration as an excel sheet. """
    if not file_name:
        file_name = "iteration"
    response = HttpResponse( mimetype="Application/vnd.ms-excel")
    response['Content-Disposition'] = 'attachment; filename=%s.xls'%file_name
    stories = iteration.stories.all().order_by("rank")
    w = xlwt.Workbook()
    ws = w.add_sheet('Iteration Export')
    headers = _getHeaders(iteration.project)
    heading_xf = ezxf('font: bold on; align: wrap on, vert centre, horiz center')

    # Write out a header row.
    for idx,header in enumerate(headers):
        logger.debug(header[1])
        ws.write(0,idx,header[1],heading_xf)
        ws.col(idx).width = 37*header[0]

    # Write out all the data.
    for idx, story in enumerate(stories):

        for hidx, header in enumerate(headers):
            f = header[2]
            ws.write(1+idx,hidx, f(story), header[3] )


    w.save(response)
    return response



def _exportXML( iteration, file_name=None  ):
    """ Exports the stories in an iteration as XML """
    if not file_name:
        file_name = "iteration"
    stories = iteration.stories.all().order_by("rank")
    doc = Document()
    iteration_node = doc.createElement("iteration")
    doc.appendChild(iteration_node)

    headers = _getHeaders(iteration.project)

    for idx, story in enumerate(stories):
        row = []
        story_node = doc.createElement("story")
        iteration_node.appendChild( story_node )
        for hidx, header in enumerate(headers):
            f = header[2]
            story_node.setAttribute(_toXMLNodeName(header[1]), unicode(f(story)).replace("\n"," ").replace("\r",""))
            # TODO (Future Enhancement): There's a bug in the minidom implementation that doesn't convert newlines to their entities inside attributes,
            #      and there's no good work-around I can find without monkey patching minidom itself.
            #      We should generally recommend people stick to excel or CSV files.

    response = HttpResponse(doc.toprettyxml(indent="  "), mimetype="text/xml")
    response['Content-Disposition'] = 'attachment; filename=%s.xml'%file_name
    return response

def _toXMLNodeName( name ):
    return re.sub('[^a-zA-Z0-9_-]',"",name.replace(" ","_").lower())

def _exportCSV( iteration, file_name=None ):
    """ Exports the stories in an iteration as CSV """
    if not file_name:
        file_name = "iteration"
    response =  HttpResponse( mimetype="text/csv")
    response['Content-Disposition'] = 'attachment; filename=%s.csv'%file_name
    stories = iteration.stories.all().order_by("rank")

    writer = UnicodeWriter(response)
    #csv.writer(response, delimiter=',' ,  quoting=csv.QUOTE_ALL, escapechar='\\')

    headers = _getHeaders(iteration.project)
    row = []
    for idx,header in enumerate(headers):
        row.append(header[1])

    writer.writerow( row )

    for idx, story in enumerate(stories):
        row = []
        for hidx, header in enumerate(headers):
            f = header[2]
            row.append( f(story) )
        writer.writerow( row )

    return response

class UnicodeWriter:
    """
    A CSV writer which will write rows to CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
        # Redirect output to a queue
        self.queue = cStringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        self.writer.writerow([ unicode(s).encode("utf-8") for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode("utf-8")
        # ... and reencode it into the target encoding
        data = self.encoder.encode(data)
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)



def _importData( data, iteration , user):
    """ Imports data from a python object to an iteration.
        The idea here is that all the import mechanisms (CSV, XML, XLS) can translate
        their input to a python object hierarchy, and they all can pass that off to
        this method to do the actual import so we only have to write the sync-code
        once.

        data should be an array of python dict like objects, where the keys are
        the names from the getHeaders call, and the values are the user's input.
        """
    imported = 0
    failed = 0
    for row in data:
        if _importSingleRow(row, iteration, user):
            imported += 1
        else:
            failed += 1
    logger.info("Imported %d records, failed on %d" % (imported,failed))
    if failed == 0:
        user.message_set.create(message="Imported %d records." % imported )
    else:
        user.message_set.create(message="Imported %d records, failed on %d" % (imported,failed))
    return (imported,failed)


def _getFieldFromImportData( data, field_name ):
    """ This method returns a value for a given field.  Generally, it's used for translating user data
        into values suitable for a story.  """
    # TODO (Future Enhancement) - Right now, we do only exact matches, we might want a more intelligent
    #        search scheme to accept a wider variety of import formats.
    #        For instance, case insensitive, ignore whitespace, whatever.

    rv = data.get(field_name)
    if( rv == None ):
        # If we didn't find one, lets try an alternative naming...
        rv = data.get( _toXMLNodeName(field_name) )

    return rv;


def _importSingleRow( row, iteration, user):
    try:
        local_id = _getFieldFromImportData( row, "Story ID" )
        story = None
        if local_id != None:
            try:
                story = Story.objects.get( project=iteration.project, local_id=int(local_id) )
                logger.debug("Found story to update (%d)" % int(local_id) )
            except:
                # Story didn't exist already, so we'll be making a new one
                # This is a little dangerous if there was a story id set, since we'll now be ignoring
                # that and that might not be what the user intended.
                story = Story(project=iteration.project, iteration=iteration, local_id=iteration.project.getNextId() )
                story.creator = user
                logger.debug("Creating new story to import into.")


        # A user could move rows from one iteration export to another, so set it here. It'll probably be rare to actually happen.
        story.iteration = iteration

        headers = _getHeaders(iteration.project)
        for header in headers:
            value = _getFieldFromImportData( row, header[1] )
            if value != None:
                try:
                    f = header[4]  # This should be a method capable of setting the property
                    f(story, value)
                    # logger.debug("Setting %s to %s" % (header[1],value) )
                except:
                    logger.info("Failed to set %s to %s, ignoring." % (header[1], unicode(value) ) )

        story.save()
        return True
    except Exception as e:
        logger.debug("Failed to import a record. %s" % e)
        return False



def _importExcelIteration(iteration, file, user):
    try:
        workbook = open_workbook(file_contents=file.read())
    except:
        workbook = open_workbook(file_contents=file.read(), encoding_override="cp1252")
    sheet = workbook.sheets()[0]
    count = 0
    headers = _getHeaders( iteration.project )
    import_data = []
    for row in range(1,sheet.nrows):
        rowData = {}
        count += 1
        for col in range( sheet.ncols  ):
            header = sheet.cell(0,col).value
            val = sheet.cell(row,col).value
            rowData[header] = val
        import_data.append( rowData )
    logger.info("Found %d rows in an excel sheet " % count)
    _importData( import_data , iteration, user)



def _importXMLIteration(iteration, file, user):
    xml = parse( file )
    import_data = []
    count = 0
    for story_node in xml.getElementsByTagName("story"):
        attrs = story_node.attributes
        import_row = {}
        for attrName in attrs.keys():
            import_row[attrName] = attrs[attrName].nodeValue
        count += 1
        import_data.append(import_row)
    logger.info("Found %d rows in an XML file" % count)
    _importData( import_data, iteration, user )




def _importCSVIteration(iteration, file, user):
    import_file = csv.reader( file , delimiter=',' ,  quoting=csv.QUOTE_ALL, escapechar='\\' )
    try:
        headers = None
        import_data = []
        count = 0
        for row in import_file:
            try:
                if headers == None:
                    headers = row
                else:
                    import_row = {}
                    for idx,header in enumerate(headers):
                        import_row[header] = row[idx]
                        #logger.debug("Import field %s as %s"%(header, row[idx]) )
                    count += 1
                    import_data.append( import_row )
            except:
                logger.warn("Failed to import CSV row")
    except:
        logger.info("Failed to import CSV file")
    logger.info("Found %d rows in a CSV file" % count)
    _importData( import_data, iteration, user )

def cleanWorksheetName( name ):
    invalidchars = "[]*/\?:=;"
    tmp = name
    for char in invalidchars:
        tmp = tmp.replace(char,"")
    tmp = tmp[:31]
    return tmp

########NEW FILE########
__FILENAME__ = iteration_views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponseRedirect, HttpResponseForbidden
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.utils.datastructures import SortedDict
from django.utils.translation import ugettext_lazy as _
from django.http import HttpResponse
from django.core import serializers
from django.core.exceptions import PermissionDenied
import datetime

from projects.calculation import onDemandCalculateVelocity
import projects.signals as signals
from projects.models import Project, ProjectMember, Iteration, Story
from projects.forms import *
from projects.access import *
import projects.import_export as import_export

from story_views import handleAddStory

import logging



logger = logging.getLogger(__name__)

@login_required
def iteration(request, group_slug, iteration_id):
    project = get_object_or_404( Project, slug=group_slug )
    read_access_or_403(project,request.user)
    iteration = get_object_or_404( Iteration, id=iteration_id )

    if request.method == 'POST': # If the form has been submitted...
        write_access_or_403(project,request.user)
        form = IterationForm( request.POST, instance=iteration)
        if form.is_valid(): # All validation rules pass
            iteration = form.save(  )
            request.user.message_set.create(message="Iteration Details Saved.")
    else:
        form = IterationForm( instance=iteration )

    today = datetime.date.today()
    daysLeft = None
    try:
        if iteration.start_date <= today and iteration.end_date >= today:
            daysLeft = (iteration.end_date - today).days
    except:
        pass

    add_story_form = handleAddStory(request, project)

    return render_to_response("projects/iteration.html", {
        "iteration": iteration,
        "iterationinfo": True,
        "project" : project,
        "iteration_form": form,
        'daysLeft': daysLeft,
        'add_story_form': add_story_form,
        "current_view":"iteration_page"
      }, context_instance=RequestContext(request))



# 
# def backlog(request, project, iteration, form):
#     today = datetime.date.today()
# 
#     if request.method == 'POST' and request.POST.get("action") == "addEpic": # If the form has been submitted...
#         write_access_or_403(project,request.user)
#         add_epic_form = EpicForm( project, iteration, request.POST)
#         if add_epic_form.is_valid(): # All validation rules pass
#             epic = add_epic_form.save()
#             request.user.message_set.create(message="Epic %d created" % epic.local_id)
#             add_epic_form = EpicForm(project, iteration)
#         logger.debug(add_epic_form.__dict__)
#         show_epic = True
#     else:
#         add_epic_form = EpicForm(project, iteration)
#         show_epic = False
# 
#     add_story_form = handleAddStory(request, project)
# 
#     return render_to_response("projects/backlog.html", {
#         "iteration": iteration,
#         "iterationinfo": True,
#         "project" : project,
#         "epics": iteration.epics.all(),
#         "iteration_form": form,
#         'add_story_form': add_story_form,
#         "add_epic_form": add_epic_form,
#         "current_view":"backlog_page",
#         "organization":project.organization,
#         "show_epic":show_epic
#       }, context_instance=RequestContext(request))



@login_required
def iteration_create(request, group_slug=None):
    project = get_object_or_404(Project, slug=group_slug)
    write_access_or_403(project,request.user)
    is_member = project.user_is_member(request.user)

    if request.method == 'POST': # If the form has been submitted...
        form = IterationForm(request.POST) # A form bound to the POST data
        if form.is_valid():
            iteration = form.save(commit=False)
            iteration.project = project
            iteration.save()
            action = "created"
            signals.iteration_created.send(user=request.user, iteration=iteration, sender=request)
            # iteration.activity_signal.send(sender=iteration, user=request.user, action=action, project=project)
            request.user.message_set.create(message="Iteration created.")
            return HttpResponseRedirect( reverse('project_detail', kwargs={'group_slug':project.slug}) ) # Redirect after POST

    else:
        form = IterationForm() # An unbound form

    return render_to_response('projects/new_iteration.html', { 'project':project, 'form': form,  }, context_instance=RequestContext(request))


# Deletes an iteration.  No further confirmation, but it will only do it if the iteration
# is empty. Also, some confirmation client-side.
@login_required
def delete_iteration( request, group_slug, iteration_id ):
    iteration = get_object_or_404( Iteration, id=iteration_id )

    if iteration.default_iteration:
        request.user.message_set.create(message="You can not delete your default iteration.")
        return HttpResponseRedirect( reverse('iteration', kwargs={'group_slug':iteration.project.slug, 'iteration_id':iteration.id }) ) #redirect to same iteration+display msg., as delete failed
        

    if request.method == "POST":
        write_access_or_403(iteration.project,request.user)
        stories = Story.objects.filter(iteration = iteration)
        if not stories:            
            # iteration.activity_signal.send(sender=Iteration, news=request.user.username + " deleted\"" +iteration.name + "\" in project\"" +iteration.project.name, user=request.user,action="deleted" ,object=iteration.name, story = None, context=iteration.project.slug)
            signals.iteration_deleted.send(user=request.user, iteration=iteration, sender=request)
            iteration.delete()
            request.user.message_set.create(message="Iteration deleted.")
            return HttpResponseRedirect( reverse('project_detail', kwargs={'group_slug':iteration.project.slug}) ) # Redirect after POST, to the only logical place
        else:
            request.user.message_set.create(message="Iteration not empty, could not be deleted.")
    return HttpResponseRedirect( reverse('iteration', kwargs={'group_slug':iteration.project.slug, 'iteration_id':iteration.id }) ) #redirect to same iteration+display msg., as delete failed


@login_required
def unlock_iteration(request, group_slug, iteration_id):
    project = get_object_or_404(Project, slug=group_slug)
    iteration = get_object_or_404(Iteration, id=iteration_id)
    write_access_or_403(project,request.user)
    if request.method == "POST":
        form = UnlockForm(request.POST)
        if form.is_valid():
            unlock = form.cleaned_data["unlock_iteration"]
            if unlock:
                iteration.locked = False
                iteration.save()
        return HttpResponseRedirect( reverse('iteration', kwargs={'group_slug':project.slug, 'iteration_id':iteration.id}) )
    else:
        form = UnlockForm()
    return render_to_response('projects/unlock_iteration.html', { 'project':project, 'iteration':iteration, 'form': form,  }, context_instance=RequestContext(request))



@login_required
def iteration_import(request, group_slug, iteration_id):
    project = get_object_or_404(Project, slug=group_slug)
    iteration = get_object_or_404(Iteration, id=iteration_id)

    if iteration.locked:
        form_class = IterationImportFormWithUnlock
    else:
        form_class = IterationImportForm

    write_access_or_403(project,request.user)
    if request.method == "POST":
        form = form_class(request.POST)
        import_file = request.FILES.get("import_file",None)
        if form.is_valid() and import_file != None:
            unlock = form.cleaned_data.get("unlock_iteration",False)
            if unlock:
                iteration.locked = False
                iteration.save()
            status = import_export.importIteration(iteration, import_file, request.user )
            onDemandCalculateVelocity( project )
        return HttpResponseRedirect( reverse('iteration', kwargs={'group_slug':project.slug, 'iteration_id':iteration.id}) )
    else:
        form = form_class(  )

    return render_to_response('projects/import_options.html', { 'project':project, 'iteration':iteration, 'form': form,  }, context_instance=RequestContext(request))

@login_required
def iteration_report(request, group_slug, iteration_id):
    project = get_object_or_404(Project, slug=group_slug)
    iteration = get_object_or_404(Iteration, id=iteration_id)
    read_access_or_403(project,request.user)
    if iteration.project != project:
        raise PermissionDenied()
    return render_to_response('projects/iteration_report.html', { 'project':project, 'iteration':iteration  }, context_instance=RequestContext(request))

@login_required
def iteration_stats(request, group_slug, iteration_id):
    project = get_object_or_404( Project, slug=group_slug )
    admin_access_or_403(project, request.user, ignore_active=True)
    iteration = get_object_or_404( Iteration, id=iteration_id )
    return render_to_response("projects/iteration_stats.html", {"project":project, "iteration":iteration}, context_instance=RequestContext(request))


@login_required
def iteration_export(request, group_slug, iteration_id):
    project = get_object_or_404(Project, slug=group_slug)
    iteration = get_object_or_404(Iteration, id=iteration_id)
    write_access_or_403(project,request.user)
    if iteration.project != project:
        raise PermissionDenied()

    if request.method == "POST":
        form = ExportForm(request.POST)
        if form.is_valid():
            format = form.cleaned_data["format"]
            lock = form.cleaned_data["lock_iteration"]
            file_name = form.cleaned_data["file_name"]
            if lock:
                iteration.locked = True
                iteration.save()
            return import_export.exportIteration(iteration, format, file_name)
    else:
        form = ExportForm(initial={'file_name':u'iteration'})

    return render_to_response('projects/export_options.html', { 'project':project, 'iteration':iteration, 'form': form,  }, context_instance=RequestContext(request))


@login_required
def scrum_board(request, group_slug, iteration_id):
    project = get_object_or_404(Project, slug=group_slug)
    iteration = get_object_or_404(Iteration, id=iteration_id)
    read_access_or_403(project,request.user)
    if iteration.project != project:
        raise PermissionDenied()
    add_story_form = handleAddStory(request, project)
    return render_to_response('projects/scrum_board.html', { 'project':project, 'iteration':iteration, "add_story_form":add_story_form  }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = limits

class Limit(object) :

    def addHandler(self, handler ):
        self.handlers.append( handler )

    def stats(self, **kwargs ):
        for handler in self.handlers:
            if handler.canHandle( **kwargs ):
                return handler.stats( **kwargs )
        return (0,0,False)


    def increaseAllowed(self, **kwargs ):
        for handler in self.handlers:
            if handler.canHandle( **kwargs ):
                return handler.increaseAllowed( **kwargs )
        return True

    def __init__(self, *args, **kwargs):
        self.providing_args = kwargs.pop("providing_args")
        self.handlers = []
        super(self.__class__, self).__init__(*args, **kwargs)



# Limits related to a personal account:
personal_project_limit      = Limit( providing_args=["user"] )
personal_user_limit         = Limit( providing_args=["user","userToAdd"] )
personal_storage_limit      = Limit( providing_args=["user"] )
personal_extra_limit        = Limit( providing_args=["project"] )
personal_email_limit        = Limit( providing_args=["project"] )
personal_ssl_access         = Limit( providing_args=["user"] )

# Limits related to an organization:
org_project_limit      = Limit( providing_args=["organization"] )
org_user_limit         = Limit( providing_args=["organization","userToAdd"] )
org_storage_limit      = Limit( providing_args=["organization"] )
org_extra_limit        = Limit( providing_args=["organization"] )
org_email_limit        = Limit( providing_args=["organization"] )

on_demand_velocity     = Limit( providing_args=["project"] )

# Helper methods:
def userIncreasedAlowed( project, user, userToAdd):
    if project.organization:
        return org_user_limit.increaseAllowed(userToAdd=userToAdd, organization=project.organization)
    else:
        return personal_user_limit.increaseAllowed(userToAdd=userToAdd, user=user)

########NEW FILE########
__FILENAME__ = burnup_chart
#!/usr/bin/env python
from datetime import date, timedelta
from apps.projects.models import Project, Iteration, Story, PointsLog
from django.core.management.base import BaseCommand, CommandError

from projects.calculation import calculateProject

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        projects = Project.objects.filter(active=True)
        for project in projects:
            try:
                calculateProject( project )
            except:
                logger.error("Could not calculate project %s" % project.slug)

########NEW FILE########
__FILENAME__ = create_staff_teams
#!/usr/bin/env python

# creates a staff team in every organization

from apps.projects.models import Project, Iteration, Story, PointsLog, SiteStats
from apps.extras.models import *
from mailer.models import Message
from django.contrib.auth.models import User
from django.conf import settings
from django.core.management.base import BaseCommand, CommandError

from organizations.models import *
from projects.models import Project

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        for organization in Organization.objects.all():
            if organization.teams.filter(access_type="staff").count() > 0:
                continue
            logger.debug("Creating staff team for %s" % organization.slug)
            team = Team(organization=organization,name="Staff",access_type="staff")
            team.save()
            team.members.add(organization.creator)
########NEW FILE########
__FILENAME__ = depersonify
#!/usr/bin/env python

from apps.projects.models import Project, Iteration, Story, PointsLog, SiteStats
from apps.extras.models import *
from mailer.models import Message
from django.contrib.auth.models import User
from django.conf import settings
from django.core.management.base import BaseCommand, CommandError

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        if not settings.DEBUG:
            logger.error("Can only run this on sites in debug mode (for safety!)")
            return

        confirm = raw_input("We're about to devestate this DB making it completely non-production worthy and destroy user data.  Type yes to continue.\n > ")
        if confirm != "yes":
            return

        logger.info("Resetting user email addresses to %s and passwords to klug" % settings.CONTACT_EMAIL)
        for user in User.objects.all():
            user.email = settings.CONTACT_EMAIL
            user.password = "sha1$d4f5f$2ccff0e66fe0090095add4d3e47343aa6b8009b7"
            user.save()

        logger.info("Removing project<->extras mappings")
        ExternalStoryMapping.objects.all().delete()
        ProjectExtraMapping.objects.all().delete()
        ExternalTaskMapping.objects.all().delete()
        ExtraConfiguration.objects.all().delete()
        SyncronizationQueue.objects.all().delete()
        Message.objects.all().delete()

########NEW FILE########
__FILENAME__ = extras_sync
#!/usr/bin/env python

# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA



# This script handles running the extra's syncronization logic.
# You should set this up on some sort of scheduled basis.  The
# scrumdo.com website runs it once a minute.

import sys
from datetime import date, timedelta

from apps.projects.models import Project, Iteration, Story, PointsLog
from apps.extras.models import ProjectExtraMapping, SyncronizationQueue
from apps.extras.manager import manager

import logging
import sys, traceback
import getopt
import urllib2

from django.core.management.base import BaseCommand, CommandError

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class Command(BaseCommand):
    args = "pull"
    def handle(self, *args, **options):
        options, remainder = getopt.getopt(sys.argv[1:], 'p', ['pull'])

        for arg in args:
            if arg == "pull":
                if len(remainder) == 3:
                    setUpPullQueue( project_slug=remainder[2] )
                else:
                    setUpPullQueue()
                    return
        processQueue()


def setUpPullQueue( **kwargs ):
    logging.info("Setting up queue to pull all projects next time.")
    project_slug = kwargs.get("project_slug",None)
    mappings = ProjectExtraMapping.objects.all()
    for mapping in mappings:
        if project_slug==None or project_slug==mapping.project.slug:
            qItem = SyncronizationQueue(project=mapping.project, extra_slug=mapping.extra_slug, action=SyncronizationQueue.ACTION_SYNC_REMOTE)
            qItem.save()


def processQueue():
    queue = SyncronizationQueue.objects.all()
    for queueItem in queue:
    # In case a second invocation of this script occurs while it's running, we don't want
    # to re-process any items...
        queueItem.delete()
        pass

    for queueItem in queue:
        try:
            project = queueItem.project
            if not project.active:
                continue

            logger.info("== Synchronizing %s / %s / %d" % (project.slug, queueItem.extra_slug, queueItem.action) )
            story = queueItem.story
            task = queueItem.task
            action = queueItem.action
            extra_slug = queueItem.extra_slug
            action = queueItem.action
            external_id = queueItem.external_id

            extra = manager.getExtra( extra_slug )

            if action == SyncronizationQueue.ACTION_SYNC_REMOTE:
                extra.pullProject(project)
            elif action == SyncronizationQueue.ACTION_STORY_UPDATED:
                extra.storyUpdated(project,story)
            elif action == SyncronizationQueue.ACTION_STORY_DELETED:
                extra.storyDeleted(project,external_id)
            elif action == SyncronizationQueue.ACTION_STORY_CREATED:
                extra.storyCreated(project,story)
            elif action == SyncronizationQueue.ACTION_STORY_STATUS_CHANGED:
                extra.storyStatusChange(project, story)
            elif action == SyncronizationQueue.ACTION_INITIAL_SYNC:
                extra.initialSync( project )
            elif action == SyncronizationQueue.ACTION_TASK_UPDATED:
                extra.taskUpdated(project, task)
            elif action == SyncronizationQueue.ACTION_TASK_DELETED:
                extra.taskDeleted(project, external_id)
            elif action == SyncronizationQueue.ACTION_TASK_CREATED:
                extra.taskCreated(project, task)
            elif action == SyncronizationQueue.ACTION_TASK_STATUS_CHANGED:
                extra.taskStatusChange(project, task)
            elif action == SyncronizationQueue.ACTION_STORY_IMPORTED:
                extra.storyImported(project, story)

            logger.info("Success.")
        except RuntimeError:
            logger.error("RuntimeError occured while processing a syncronization queue item.")
            traceback.print_exc(file=sys.stdout)
        except urllib2.HTTPError as e:
            logger.error("HTTP Error %d occured while processing a syncronization queue item." % e.code)
            logger.error(e.headers)
            logger.error(e.read())
            traceback.print_exc(file=sys.stdout)
        except:
            logger.error("Error occured while processing a syncronization queue item.")
            traceback.print_exc(file=sys.stdout)




if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = organize_projects
#!/usr/bin/env python

# creates a new organization for every project that doesn't have one

from apps.projects.models import Project, Iteration, Story, PointsLog, SiteStats
from apps.extras.models import *
from mailer.models import Message
from django.contrib.auth.models import User
from django.conf import settings
from django.core.management.base import BaseCommand, CommandError

from organizations.models import *
from projects.models import Project

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        user_org_map = {}
        projects = Project.objects.filter(organization=None)
        logger.debug("%d projects" % projects.count() )
        for project in projects:
            if project.creator.username in user_org_map:
                logger.debug("Found organization for %s" % project.creator.username)
                organization = user_org_map[ project.creator.username ]
            else:
                logger.debug("Creating organization for %s" % project.creator.username)
                # organization = user_org_map[ project.creator.username ]
                
                # name = models.CharField( max_length=65 )
                #   slug = models.SlugField(_('slug'), unique=True)
                #   creator = models.ForeignKey(User, verbose_name=_('creator'), related_name="organizations_created")
                #   created = models.DateTimeField(_('created'), default=datetime.datetime.now)
                #   description = models.TextField(_('description'),  null=True, default="")
                #   source = models.CharField(max_length=100, default="", blank=True)
                
                organization = Organization(name="%s's Default Organization" % project.creator.username,
                                            slug="def_org_%s" % project.creator.id,
                                            creator=project.creator,
                                            description="Auto generated organization for %s" % project.creator.username,
                                            source = "" )
                organization.save()
                default_team = Team(organization = organization, name="Owners", access_type="admin")
                default_team.save()

                default_team.members.add(project.creator)

                member_team = Team(organization = organization, name="Members", access_type="write")
                member_team.save()
                
                user_org_map[ project.creator.username ] = organization
                
            project.organization = organization
            for team in organization.teams.all():
                team.projects.add(project)
            project.save()
########NEW FILE########
__FILENAME__ = owners_to_staff
#!/usr/bin/env python

# Moves the users from the admin/owners team into staff.

from apps.projects.models import Project, Iteration, Story, PointsLog, SiteStats
from apps.extras.models import *
from mailer.models import Message
from django.contrib.auth.models import User
from django.conf import settings
from django.core.management.base import BaseCommand, CommandError

from organizations.models import *
from projects.models import Project
import sys, traceback

import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        for organization in Organization.objects.all():
            try:
                staff = organization.teams.get(access_type="staff")
                admin = organization.teams.get(access_type="admin", name="Owners")
                staff_members = staff.members.all()
                for admin_member in admin.members.all():
                    if admin_member in staff_members:
                        continue
                    staff.members.add(admin_member)
                    logger.debug("Adding %s to %s/%s" % (admin_member, staff, organization)  )
                    pass
            except:
                logger.debug("Could not update %s" % organization)
                traceback.print_exc(file=sys.stdout)
                continue
                

########NEW FILE########
__FILENAME__ = randomize_tokens
#!/usr/bin/env python

# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA



# This script handles running the extra's syncronization logic.
# You should set this up on some sort of scheduled basis.  The
# scrumdo.com website runs it once a minute.

import sys
from datetime import date, timedelta

from apps.projects.models import Project, Iteration, Story, PointsLog
from apps.extras.models import ProjectExtraMapping, SyncronizationQueue
from apps.extras.manager import manager
import string
import logging
import sys, traceback
import getopt
import random
from django.core.management.base import BaseCommand, CommandError

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    def handle(self, *args, **options):
        for project in Project.objects.all():
            project.token = "".join(random.sample(string.lowercase + string.digits, 7))
            project.save()

########NEW FILE########
__FILENAME__ = site_stats
#!/usr/bin/env python

from apps.projects.models import Project, Iteration, Story, PointsLog, SiteStats
from django.contrib.auth.models import User

from django.core.management.base import BaseCommand, CommandError


class Command(BaseCommand):
    def handle(self, *args, **options):
        stats = SiteStats()
        stats.project_count = Project.objects.count()
        stats.user_count = User.objects.count()
        stats.story_count = Story.objects.count()
        stats.save()

########NEW FILE########
__FILENAME__ = test_access
#!/usr/bin/env python
import sys
from datetime import date, timedelta

from os.path import abspath, dirname, join

from django.core.management.base import BaseCommand, CommandError

from projects.access import *
from projects.models import *
from organizations.models import *

from django.contrib.auth.models import User


class Command(BaseCommand):
    def handle(self, *args, **options):
        for project in Project.objects.all():
            print "Project %s " % project.slug
            for user in User.objects.all():
                if has_admin_access(project, user):
                    print "  Admin:  %s" % user.username
                if has_write_access(project, user):
                    print "  Write:  %s" % user.username
                if has_read_access( project, user):
                    print "  Read:  %s" % user.username

########NEW FILE########
__FILENAME__ = management
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.conf import settings
from django.db.models import signals
from django.utils.translation import ugettext_noop as _

if "notification" in settings.INSTALLED_APPS:
    from notification import models as notification

    def create_notice_types(app, created_models, verbosity, **kwargs):
        notification.create_notice_type("projects_new_member", _("New Project Member"), _("a project you are a member of has a new member"), default=1)
        notification.create_notice_type("projects_created_new_member", _("New Member Of Project You Created"), _("a project you created has a new member"), default=2)
        notification.create_notice_type("projects_new_project", _("New Project Created"), _("a new project has been created"), default=1)
        notification.create_notice_type("projects_added_as_member", _("Added as a Project Member"), _("you have become member of a project"), default=1)

    signals.post_syncdb.connect(create_notice_types, sender=notification)
else:
    print "Skipping creation of NoticeTypes as notification app not found"

########NEW FILE########
__FILENAME__ = models
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from datetime import datetime, date, timedelta
import time
from tagging.fields import TagField
from tagging.models import Tag
import tagging
import re
import random
import string
from itertools import groupby

from django.core.urlresolvers import reverse
from django.contrib.auth.models import  User
from django.utils.translation import ugettext_lazy as _
from django.db import models
from groups.base import Group
from django.contrib.contenttypes import generic
from django.contrib.contenttypes.models import ContentType
from django.core.exceptions import ObjectDoesNotExist

from organizations.models import Organization, Team

import django.dispatch
import projects.signals as signals

STATUS_TODO = 1
STATUS_DOING = 2
STATUS_REVIEWING = 3
STATUS_DONE = 4
STATUS_CHOICES = ( (1, "TODO"), (2, "In Progress"),  (3, "Reviewing"), (4, "Done")   )
STATUS_REVERSE = {"TODO":STATUS_TODO, "Doing":STATUS_DOING, "In Progress":STATUS_DOING,  "Reviewing":STATUS_REVIEWING,  "Done":STATUS_DONE }

import logging

logger = logging.getLogger(__name__)

class SiteStats( models.Model ):
    user_count = models.IntegerField()
    project_count = models.IntegerField()
    story_count = models.IntegerField()
    date = models.DateField( auto_now=True )
    def __unicode__(self):
        return "%s %d/%d/%d" % (self.date, self.project_count, self.story_count, self.user_count)

class PointsLog( models.Model ):
    date = models.DateField( auto_now=True )
    points_claimed = models.IntegerField()
    points_total = models.IntegerField()
    content_type = models.ForeignKey(ContentType)
    object_id = models.PositiveIntegerField()
    related_object = generic.GenericForeignKey('content_type', 'object_id')

    def timestamp(self):
        return int((time.mktime(self.date.timetuple()) - time.timezone)*1000)
    class Meta:
        ordering = ["date"]



class Project(Group):
    POINT_CHOICES_FIBO = ( ('?', '?'), ('0', '0'), ('0.5','0.5'), ('1', '1'),  ('2', '2'),  ('3', '3'),  ('5', '5'), ('8', '8'), ('13', '13'), ('20', '20'), ('40', '40'), ('100', '100'), ('Inf', 'Infinite') )
    POINT_CHOICES_MINIMAL = ( ('?', '?'), ('0', '0'),  ('1', '1'),  ('2', '2'),  ('3', '3'),  ('4', '4'), ('5', '5') )
    POINT_CHOICES_MAX = ( ('?', '?'), ('0', '0'), ('0.5','0.5'), ('1', '1'),  ('2', '2'),  ('3', '3'),   ('4', '4'), ('5', '5'),  ('6', '6'),  ('7', '7'), ('8', '8'),  ('9', '9'),  ('10', '10'), ('15', '15'), ('25', '25'), ('50', '50'), ('100', '100'), ('Inf', 'Infinite') )
    POINT_CHOICES_SIZES = ( ('?', '?'), ('1', 'XS'), ('5', 'S'), ('10','M'), ('15', 'L'),  ('25', 'XL')  )
    POINT_RANGES = [POINT_CHOICES_FIBO, POINT_CHOICES_MINIMAL, POINT_CHOICES_MAX, POINT_CHOICES_SIZES]

    VELOCITY_TYPE_AVERAGE = 0
    VELOCITY_TYPE_AVERAGE_5 = 1
    VELOCITY_TYPE_MEDIAN = 2
    VELOCITY_TYPE_AVERAGE_3 = 3

    active = models.BooleanField( default=True )
    member_users = models.ManyToManyField(User, through="ProjectMember", related_name="user_projects", verbose_name=_('member_users'))
    # private means only members can see the project
    private = models.BooleanField(_('private'), default=True)
    points_log = generic.GenericRelation( PointsLog )
    current_iterations = None
    default_iteration = None
    use_assignee = models.BooleanField( default=False )
    use_tasks = models.BooleanField( default=False )
    use_extra_1 = models.BooleanField( default=False )
    use_extra_2 = models.BooleanField( default=False )
    use_extra_3 = models.BooleanField( default=False )
    extra_1_label = models.CharField(  max_length=25, blank=True, null=True)
    extra_2_label = models.CharField(  max_length=25, blank=True, null=True)
    extra_3_label = models.CharField(  max_length=25, blank=True, null=True)
    velocity_type = models.PositiveIntegerField( default=1 )
    point_scale_type = models.PositiveIntegerField( default=0 )
    velocity = models.PositiveIntegerField( null=True )
    velocity_iteration_span = models.PositiveIntegerField( null=True )
    iterations_left = models.PositiveIntegerField( null=True )
    organization = models.ForeignKey(Organization,related_name="projects", null=True, blank=True)
    category = models.CharField( max_length=25, blank=True, null=True, default="")
    categories = models.CharField(max_length=1024, blank=True, null=True)
    token = models.CharField(max_length=7, default=lambda: "".join(random.sample(string.lowercase + string.digits, 7)))

    class Meta:
        ordering = ['-active','name']


    def getCategoryList( self ):
        if self.categories:
            return [c.strip() for c in self.categories.split(",")]
        else:
            return []

    def getPointScale( self ):
        return self.POINT_RANGES[ self.point_scale_type ]

    def getNextEpicId( self ):
        if self.epics.count() == 0:
            return 1
        return self.epics.order_by('-local_id')[0].local_id + 1

    def getNextId( self ):
        if self.stories.count() == 0:
            return 1
        return self.stories.order_by('-local_id')[0].local_id + 1

    def all_member_choices(self):
        members = self.all_members()
        choices = []
        for member in members:
            choices.append([member.id, member.username])
        choices = sorted(choices, key=lambda user: user[1].lower() )
        return choices


    def all_members(self):
        members = []
        for membership in self.members.all():
            members.append( membership.user )

        for team in self.teams.all():
            for member in team.members.all():
                if not member in members:
                    members.append(member)
        return members

    def get_member_by_username(self, username):
        members = self.all_members()
        for member in members:
            if member.username == username:
                return member
        return None

    def hasReadAccess( self, user ):
        if self.creator == user:
            return True
        return Organization.objects.filter( teams__members__user = user , teams__access_type="read", teams__projects__project=self).count() > 0

    def hasWriteAccess( self, user ):
        if self.creator == user:
            return True
        return Organization.objects.filter( teams__members__user = user , teams__access_type__ne="read", teams__projects__project=self).count() > 0


    def get_default_iteration( self ):
        if self.default_iteration == None:
            iterations = Iteration.objects.filter( project=self, default_iteration=True)
            if len(iterations) == 0:
                self.default_iteration = self.iterations.all()[0]  # Shouldn't really happen, but just in case.
            else:
                self.default_iteration = iterations[0]
        return self.default_iteration

    def get_current_iterations(self):
        if self.current_iterations == None:
            today = date.today
            self.current_iterations = self.iterations.filter( start_date__lte=today, end_date__gte=today)
        return self.current_iterations

    def get_absolute_url(self):
        return reverse('project_detail', kwargs={'group_slug': self.slug})

    def member_queryset(self):
        return self.member_users.all()

    def user_is_member(self, user):
        if ProjectMember.objects.filter(project=self, user=user).count() > 0: # @@@ is there a better way?
            return True
        else:
            return False

    def get_num_stories(self):
        return Story.objects.filter(project=self).count()

    def get_num_iterations(self):
        return Iteration.objects.filter(project=self).count()

    def get_url_kwargs(self):
        return {'group_slug': self.slug}
    
    def unique_tags(self):
        all_tags = self.tags.all().order_by("name")
        tags = []
        for tag in all_tags:
            if len([t for t in tags if t.name==tag.name]) == 0: #remove duplicates
                tags.append(tag) 
        return tags
        
    def get_iterations(self):
        if self.get_num_iterations <= 15:
            return self.iterations.all()
        else:
            return self.iterations.filter(  models.Q(default_iteration = True) |  # We always show the backlog.
                                            models.Q(start_date = None) | models.Q( end_date = None) | # And we show iterations without dates on either end
                                            models.Q(end_date__gt = datetime.today() - timedelta(days=30), end_date__lte = datetime.today()) | # We show past iterations within 30 days
                                            models.Q(start_date__gte = datetime.today(), start_date__lt =  datetime.today() + timedelta(days=30)) | # and future iterations within 30 days
                                            models.Q(start_date__lte = datetime.today(), end_date__gte =  datetime.today() ) # And current iterations too
                                            )
            
    def get_iterations_all(self):
        return self.iterations.all()
            
    def show_more(self):
        return self.get_num_iterations() > 15
    


class Iteration( models.Model):
    name = models.CharField( "name" , max_length=100)
    detail = models.TextField(_('detail'), blank=True)
    start_date = models.DateField( blank=True, null=True )
    end_date = models.DateField( blank=True, null=True )
    project = models.ForeignKey(Project, related_name="iterations")
    default_iteration = models.BooleanField( default=False )
    points_log = generic.GenericRelation( PointsLog )
    locked = models.BooleanField( default=False )

    include_in_velocity = models.BooleanField(_('include_in_velocity'), default=True)

    def isCurrent(self):
        today = date.today()
        return self.start_date <= today and self.end_date >= today

    def total_points(self) :
        return sum( map( lambda story: story.points_value(), self.stories.all() ) )

    def completed_points(self) :
        return sum( map( lambda story: (story.points_value() if story.status==Story.STATUS_DONE else 0), self.stories.all() ) )
        
    def max_points(self):
        logs = self.points_log.all()
        if len(logs) == 0:
            return None
        return reduce(lambda x,y: max(x,y.points_total), logs, 0 )
        
    def starting_points(self):
        for p in self.points_log.all():
            if p.date == self.start_date:
                return p.points_total
        return None
        

    def daysLeft(self):
        try:
            today = date.today()
            if self.start_date <= today and self.end_date >= today:
                return (self.end_date - today).days
        except:
            pass
        return None


    def stats():
        points = 0
        stories = 0
        for story in stories:
            stories += 1
            points += story.points
        return (stories, points)

    def get_absolute_url(self):
        return reverse('iteration', kwargs={'group_slug': self.project.slug, 'iteration_id': self.id})

    class Meta:
        ordering = ["-default_iteration","end_date"]

    def __unicode__(self):
        return "%s / %s" % (self.project.name, self.name)

class Epic(models.Model):
    """Represents an epic in your backlog."""
    local_id = models.IntegerField()
    summary = models.TextField()
    parent = models.ForeignKey('self', related_name="children", null=True, verbose_name="Parent Epic", help_text="What epic does this one belong within?", )
    detail = models.TextField(blank=True)
    points = models.CharField('points', max_length=4, default="?", blank=True, help_text="Rough size of this epic (including size of sub-epics or stories).  Enter ? to specify no sizing.")
    project = models.ForeignKey( Project , related_name="epics")
    status = models.IntegerField( max_length=2, choices=STATUS_CHOICES, default=1 )
    order = models.IntegerField( max_length=5, default=5000)
    archived = models.BooleanField( default=False, help_text="Archived epics are generally hidden and their points don't count towards the project." )
    
    def save(self, *args, **kwargs):
        if self.parent == self:
            self.parent = None
        super(Epic, self).save(*args, **kwargs) 

    def stories_by_rank(self):
        return self.stories.all().order_by("rank")

    def short_name(self):
        if self.parent_id:
            return "%s / #E%d" % (self.parent.short_name(), self.local_id)
        return "#E%d" % (self.local_id)

    def full_name(self):
        if self.parent_id:
            return "%s / #E%d %s" % (self.parent.full_name(), self.local_id, self.summary)
        return "#E%d %s" % (self.local_id,self.summary)

    def normalized_points_value(self):
        "Returns the point value of this epic, minus the point value of the stories within it, minimum of 0"
        pv = self.points_value()
        for story in self.stories.all():
            pv -= story.points_value()
        for epic in self.children.all():
            pv -= epic.normalized_points_value()
        return max(pv, 0)

    def points_value(self):
        if self.points.lower() == "inf" :
            return 0
        try:
            return float(self.points)
        except:
            return 0

    def getPointsLabel(self):
        result = filter( lambda v: v[0]==self.points, Project.POINT_RANGES[ self.project.point_scale_type ] )
        if len(result) > 0:
            return result[0][1]
        return self.points

    def __unicode__(self):
        if self.local_id == None:
            local_id = -1
        else:
            local_id = self.local_id
        return u"Epic %d %s" % (local_id, self.summary)
    class Meta:
        ordering = [ 'order' ]




class Story( models.Model ):
    STATUS_TODO = 1
    STATUS_DOING = 2
    STATUS_REVIEWING = 3
    STATUS_DONE = 4
    rank = models.IntegerField()
    board_rank = models.IntegerField(default=0)
    summary = models.TextField( )
    local_id = models.IntegerField()
    detail = models.TextField( blank=True )
    creator = models.ForeignKey(User, related_name="created_stories", verbose_name=_('creator'))
    created = models.DateTimeField(_('created'), default=datetime.now)
    modified = models.DateTimeField(_('modified'), default=datetime.now)
    assignee = models.ForeignKey(User, related_name="assigned_stories", verbose_name=_('assignee'), null=True, blank=True)
    points = models.CharField('points', max_length=3, default="?", blank=True)
    iteration = models.ForeignKey( Iteration , related_name="stories")
    project = models.ForeignKey( Project , related_name="stories")
    status = models.IntegerField( max_length=2, choices=STATUS_CHOICES, default=1 )
    category = models.CharField(max_length=25, blank=True, null=True)
    extra_1 = models.TextField( blank=True , null=True)
    extra_2 = models.TextField( blank=True , null=True)
    extra_3 = models.TextField( blank=True , null=True)
    epic = models.ForeignKey(Epic, null=True, blank=True, related_name="stories")

    tags_to_delete = []
    tags_to_add = []

    @staticmethod
    def getAssignedStories(user, organization):
        projects = ProjectMember.getProjectsForUser(user,organization=organization)
        assigned_stories = []
        for project in projects:
            if project.active and project.organization==organization:
                project_stories = []
                iterations = project.get_current_iterations()
                for iteration in iterations:
                    project_stories = project_stories + list(iteration.stories.filter(assignee=user).exclude(status=4).select_related())
                    project_stories = project_stories + list(iteration.stories.filter(tasks__assignee=user).exclude(status=4).select_related())
                if len(project_stories) > 0:
                    assigned_stories = assigned_stories + [(project, list(set(project_stories)) )]
        return assigned_stories
    


    def story_tags_full(self):
        "Helper function to return queryset of taggings with the tag object preloaded"
        return self.story_tags.all().select_related("tag")

    def statusText(self):
        return STATUS_CHOICES[ self.status - 1][1]

    def getPointsLabel(self):
        result = filter( lambda v: v[0]==self.points, self.getPointScale() )
        if len(result) > 0:
            return result[0][1]
        return self.points

    def getPointScale( self ):
        return Project.POINT_RANGES[ self.project.point_scale_type ]

    def points_value(self):
        # the float() method understands inf!
        if self.points.lower() == "inf" :
            return 0

        try:
            return float(self.points)
        except:
            return 0

    def getExternalLink(self, extra_slug):
        try:
            link = self.external_links.get( extra_slug="basecamp" )
        except:
            return None
        return link

    @property
    def tags(self):
        r = ""
        for tag in self.story_tags.all():
            if len(r) > 0:
                r = r + ", "
            r = r + tag.name
        return r

    @tags.setter
    def tags(self, value):
        #print "TAGS SET " + value
        input_tags = re.split('[, ]+', value)
        self.tags_to_delete = []
        self.tags_to_add = []
        # First, find all the tags we need to add.
        for input_tag in input_tags:
            found = False
            for saved_tag in self.story_tags.all():
                if saved_tag.name == input_tag:
                    found = True
            if not found :
                self.tags_to_add.append( input_tag )
        # Next, find the tags we have to delete
        for saved_tag in self.story_tags.all():
            found = False
            for input_tag in input_tags:
                if saved_tag.name == input_tag:
                    found = True
            if not found :
                self.tags_to_delete.append( saved_tag )
    def __unicode__(self):
        return "[%s/#%d] %s" % (self.project.name, self.local_id, self.summary)


    @models.permalink
    def get_absolute_url(self):
        return ('story_permalink', [str(self.id)])
    # def get_absolute_url(self):
    #     return (self.iteration.get_absolute_url() + "#story_" + str(self.id))



def tag_callback(sender, instance, **kwargs):

    for tag_to_delete in instance.tags_to_delete:
        tag_to_delete.delete()
    for tag_to_add in instance.tags_to_add:
        tag_to_add = tag_to_add.strip()
        if len(tag_to_add) == 0:
            continue
        tag = None
        try:
            tags = StoryTag.objects.filter( project=instance.project, name=tag_to_add)
            if tags.len() > 0:
                tag = tags[0]
        except:
            pass

        if tag == None:
            tag = StoryTag( project=instance.project, name=tag_to_add)
            tag.save()

        tagging = StoryTagging( tag=tag, story=instance)
        tagging.save()
    instance.tags_to_delete = []
    instance.tags_to_add = []

models.signals.post_save.connect(tag_callback, sender=Story)

class Task( models.Model ):
    story = models.ForeignKey(Story, related_name="tasks")
    summary = models.TextField(blank=True)
    assignee = models.ForeignKey(User, related_name="assigned_tasks", verbose_name=_('assignee'), null=True, blank=True)
    complete = models.BooleanField(default=False)
    order = models.PositiveIntegerField( default=0 )

    def getExternalLink(self, extra_slug):
        try:
            link = self.external_links.get( extra_slug="basecamp" )
        except:
            return None
        return link

    def __unicode__(self):
        return "[%s/#%d] Task: %s" % (self.story.project.name, self.story.local_id, self.summary)
    class Meta:
        ordering = [ 'order' ]


class StoryTag( models.Model ):
    project = models.ForeignKey( Project , related_name="tags")
    name = models.CharField('name', max_length=32 )
    def __unicode__(self):
        return "[%s] %s" % ( self.project.name, self.name)

class StoryTagging( models.Model ):
    tag = models.ForeignKey( StoryTag , related_name="stories")
    story = models.ForeignKey( Story , related_name="story_tags")
    @property
    def name(self):
        return self.tag.name



class ProjectMember(models.Model):
    project = models.ForeignKey(Project, related_name="members", verbose_name=_('project'))
    user = models.ForeignKey(User, related_name="projects", verbose_name=_('user'))

    away = models.BooleanField(_('away'), default=False)
    away_message = models.CharField(_('away_message'), max_length=500)
    away_since = models.DateTimeField(_('away since'), default=datetime.now)

    def __str__(self):
        return "ProjectMember: %s " % self.user.username

    @staticmethod
    def getProjectsForUser(user, organization=None):
        """ This gets all a user's projects, including ones they have access to via teams. """
        query = ProjectMember.objects.filter(user=user)
        if organization:
            query = query.filter(project__organization=organization)
        user_projects = [pm.project for pm in query.select_related()]
        
        team_query = Team.objects.filter(members=user)
        if organization:
            team_query = team_query.filter(organization=organization)
        team_projects = [team.projects.select_related('organization') for team in team_query]
        for project_list in team_projects:
            user_projects = user_projects + list(project_list)
        return list(set(user_projects))

########NEW FILE########
__FILENAME__ = search_indexes
import datetime
from haystack.indexes import *
from haystack import site
from projects.models import Story
import traceback
import logging

logger = logging.getLogger(__name__)

class StoryIndex(RealTimeSearchIndex):
    text = CharField(document=True, use_template=True)
    project_id = IntegerField( model_attr='project_id' )
    iteration_id = IntegerField( model_attr='iteration_id' )
    local_id = IntegerField( model_attr='local_id' )
    user_id = IntegerField( model_attr='assignee_id' , null=True)
    numeric_points = IntegerField( model_attr='points_value' )
    created = DateField(model_attr='created')
    status = IntegerField(model_attr='status')
    rank = IntegerField(model_attr='rank')
    tags = CharField(model_attr='tags')
    category = CharField(model_attr='category', null=True)
    def prepare(self, object):
        try:
            self.prepared_data = super(StoryIndex, self).prepare(object)
        except Exception as e:
            logger.error("Story Index super failed to run! %s" % e)
            traceback.print_exc()


        fields = ('user_id','category','status','numeric_points','tags')
        for field in fields:
            if not field in self.prepared_data:
                self.prepared_data[field] = ""

        return self.prepared_data

site.register(Story, StoryIndex)

########NEW FILE########
__FILENAME__ = signals
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


import django.dispatch

# Signal dispatched when a story is edited. (Not including status changes, see next signal)
# user = who did it, story = the story that changed, diffs=dict of what fields changed
story_updated = django.dispatch.Signal(providing_args=["story","user","diffs"])

# Signal dispatched when a story's status is changed.
# user = who did it, story = the story that changed.
story_status_changed = django.dispatch.Signal(providing_args=["story","user"])

# Signal dispatched when a story is deleted
# user = who did it, story = the story that changed.
story_deleted = django.dispatch.Signal(providing_args=["story","user"])

# Signal dispatched when a story is created
# user = who did it, story = the story that changed.
story_created = django.dispatch.Signal(providing_args=["story","user"])

# Signal dispatched when a new task is created.
task_created = django.dispatch.Signal(providing_args=["task","user"])

# Signal dispatched when the status (done/not done) of a task changed.
task_status_changed = django.dispatch.Signal(providing_args=["task","user"])

# Signal dispatched when a task is edited
task_updated = django.dispatch.Signal(providing_args=["task","user"])

# Signal dispatched when a task is deleted.
# Note: it's already been deleted when this is dispatched.
task_deleted = django.dispatch.Signal(providing_args=["task","user"])

# Dispatched when an iteration is created.
iteration_created = django.dispatch.Signal(providing_args=["iteration","user"])

# Dispatched when an iteration is deleted.
iteration_deleted = django.dispatch.Signal(providing_args=["iteration","user"])
########NEW FILE########
__FILENAME__ = story_views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
import sys
import urllib
import re
import logging

from django.conf import settings
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.core.urlresolvers import reverse
from django.core import serializers
from django.core.exceptions import PermissionDenied
from django.core.paginator import Paginator, InvalidPage, EmptyPage
from django.http import HttpResponse, HttpResponseRedirect, HttpResponseForbidden
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.utils.datastructures import SortedDict
from django.utils.translation import ugettext_lazy as _

from haystack.query import SearchQuerySet
from xlrd import open_workbook

from projects.models import Project, ProjectMember, Iteration, Story, STATUS_REVERSE, Epic
from projects.forms import *
from projects.access import *
from projects.calculation import onDemandCalculateVelocity
import activities.utils as utils
import projects.signals as signals


logger = logging.getLogger(__name__)


@login_required
def story_permalink(request, story_id):
    "A permalink for a story.  No matter where it goes this should work"
    story = get_object_or_404( Story, id=story_id )
    read_access_or_403(story.project,request.user)
    return HttpResponseRedirect( "%s#story_%d" % (reverse("iteration", kwargs={'group_slug':story.project.slug,'iteration_id':story.iteration.id}),story.id) )
        
# View called via ajax on the iteration or iteration planning pages.  Meant to set the status of
# a story, and then return an html snippet that can be replaced on the page with the new status
@login_required
def set_story_status( request, group_slug, story_id, status):
    story = get_object_or_404( Story, id=story_id )
    write_access_or_403(story.project,request.user)
    if story.status != status:
        # the status was actually changes
        story.status = status;
        story.save();
        signals.story_status_changed.send( sender=request, story=story, user=request.user )
        # statuses = [None, "TODO", "In Progress", "Reviewing", "Done"]
        # story.activity_signal.send(sender=story, user=request.user, story=story, action="changed status", status=statuses[status], project=story.project)
        onDemandCalculateVelocity( story.project )

    organization = _organizationOrNone( story.project )

    if( request.POST.get("return_type","mini") == "mini"):
        return render_to_response("stories/single_mini_story.html", {
            "story": story,
            "return_type": "mini",
            "project": story.project,
            "organization": organization
          }, context_instance=RequestContext(request))
    if( request.POST.get("return_type","mini") == "queue"):
        return render_to_response("stories/single_queue_story.html", {
            "story": story,
            "return_type": "queue",
            "project": story.project,
            "organization": organization
      }, context_instance=RequestContext(request))
    return render_to_response("stories/single_block_story.html", {
            "story": story,
            "return_type": "block",
            "project": story.project,
            "organization": organization
      }, context_instance=RequestContext(request))



@login_required
def story_comments( request, story_id ):
    logger.debug("Retrieving comments for story")
    story = get_object_or_404( Story, id=story_id )
    read_access_or_403( story.project, request.user )
    return render_to_response("stories/story_comments.html", {
        "story": story,
      }, context_instance=RequestContext(request))

# Deletes a story.  No further confirmation, don't post here if you want your story.
# We're doing some confirmation client-side.  Sometimes called via ajax, sometimes
# a normal request.  If a normal request, there should be a GET param specifying where
# to send us to next.
@login_required
def delete_story( request, group_slug, story_id ):
    logger.debug("Deleting story %s" % str(story_id))
    if request.method == "POST":
        story = get_object_or_404( Story, id=story_id )
        write_access_or_403(story.project,request.user)
        signals.story_deleted.send( sender=request, story=story, user=request.user )
        # story.activity_signal.send(sender=story, user=request.user, story=story, action="deleted", project=story.project)
        story.sync_queue.clear()
        story.delete()
        onDemandCalculateVelocity( story.project )

        redirTo = request.GET.get("redirectTo", "")
        if redirTo:
            return HttpResponseRedirect(redirTo);
        else:
            return HttpResponse("OK");
    else:
        return HttpResponse("FAIL");

# Request handler for the scrum board ajax calls
@login_required
def scrum_board( request, group_slug, story_id):
    status_map = {"todo":Story.STATUS_TODO, "doing":Story.STATUS_DOING, "reviewing":Story.STATUS_REVIEWING, "done":Story.STATUS_DONE};
    story = get_object_or_404( Story, id=story_id )
    project = get_object_or_404( Project, slug=group_slug )
    if request.method == 'POST':
        if request.POST.get("status",None) == None:
            return HttpResponse("FAILED")
        target_status = status_map[request.POST.get("status")]
        reorderStory( story, request.POST.get("before"), request.POST.get("after"), story.iteration, field_name="board_rank")
        if story.status != target_status:
            story.status = target_status
            story.save()            
            signals.story_status_changed.send( sender=request, story=story, user=request.user)            
            onDemandCalculateVelocity( story.project )
        else:
            story.save()
    return HttpResponse("OK")



@login_required
def reorder_epic( request, group_slug, epic_id):
    epic = get_object_or_404( Epic, id=epic_id )
    project = get_object_or_404( Project, slug=group_slug )
    if epic.project != project:
        raise PermissionDenied()
    write_access_or_403(project,request.user)
    if request.method == 'POST':
        rank = 0
        target_iteration = request.POST["iteration"]

        try:
            iteration = get_object_or_404( Iteration, id=target_iteration )
        except:
            iteration = story.iteration

        if request.POST.get("action","") == "reorder" :
            reorderEpic( epic, request.POST.get("before"), request.POST.get("after"), iteration)            

        epic.iteration = iteration
        epic.save()
        
        return HttpResponse("OK")
    return  HttpResponse("Fail")


def _deleteEpic( epic ):
    for story in epic.stories.all():
        story.epic = None
        story.save()
    for epic in epic.children.all():
        epic.parent = None
        epic.save()
    epic.delete()    
    

@login_required
def delete_epic(request,  epic_id):
    epic = get_object_or_404(Epic, id=epic_id)
    project = epic.project
    write_access_or_403(project, request.user)
    if request.method == 'POST': # If the form has been submitted...        
        _deleteEpic( epic )
        return HttpResponse("OK")
    

@login_required
def edit_epic(request,  epic_id):
    epic = get_object_or_404(Epic, id=epic_id)
    project = epic.project
    write_access_or_403(project, request.user)
    
    if request.method == 'POST': # If the form has been submitted...        
        form = EpicForm( project, request.POST, instance=epic)
        if form.is_valid(): # All validation rules pass
            epic = form.save()
            onDemandCalculateVelocity(project)
            return HttpResponse("OK")
    else:
        form = EpicForm( project,  instance=epic)
    
    return render_to_response("projects/epic_edit.html", {
        "form":form,
        "epic":epic,
        "project":project
      }, context_instance=RequestContext(request))

# This is the request handler that gets called from the story_list and iteraqtion pages when the user drags & drops a story to a
# new ranking or a new iteration.  It should have two post variables, index and iteration
@login_required
def reorder_story( request, group_slug, story_id):
    story = get_object_or_404( Story, id=story_id )
    project = get_object_or_404( Project, slug=group_slug )
    write_access_or_403(project,request.user)
    if request.method == 'POST':
        old_story = story.__dict__.copy()
        
        rank = 0
        target_iteration = request.POST.get("iteration","")

        try:
            iteration = get_object_or_404( Iteration, id=target_iteration )
        except:
            iteration = story.iteration

        if request.POST.get("action","") == "reorder" :
            old_story = story.__dict__.copy()
            reorderStory( story, request.POST.get("before"), request.POST.get("after"), iteration)            
            diffs = utils.model_differences(old_story, story.__dict__, dicts=True)
            signals.story_updated.send( sender=request, story=story, user=request.user, diffs=diffs )            
            # story.activity_signal.send(sender=story, user=request.user, story=story, action="reordered", project=project)

        if request.POST.get("epic","-1") != "-1":
            epic_id = request.POST.get("epic")
            epic = Epic.objects.get(id=epic_id)
            if epic and epic.project == project:
                story.epic = epic
        elif request.POST.get("epic","") == "-1":
            # Explicitly moving it out of any epic
            story.epic = None


        story.iteration = iteration
        story.save()
        diffs = utils.model_differences(old_story, story.__dict__, dicts=True)

        return HttpResponse("OK")
    return  HttpResponse("Fail")

def reorderEpic(epic, before_id, after_id, iteration, field_name="order"):
    "Reorders an epic between two others."
    epic_rank_before = 0
    epic_rank_after = 999999 # max value of the DB field

    # If there is a epic that should be before this one, grab it's rank
    try:
        epic_before = Epic.objects.get( id=before_id )
        epic_rank_before = epic_before.__dict__[field_name]
    except:
        pass

    # If  there is a epic that should be after this one, grab it's rank.
    try:
        epic_after = Epic.objects.get( id=after_id )
        epic_rank_after = epic_after.__dict__[field_name]
    except:
        pass

    diff = abs(epic_rank_after - epic_rank_before)
    try:
        if diff > 1:
            # It fits between them            
            epic.__dict__[field_name] = round(diff/2) + epic_rank_before
            logger.debug("Reordering epic fit %d %d %d" % (epic_rank_before, epic.__dict__[field_name], epic_rank_after) )
            return
    except:
        pass # do an emergency re-order below if things are falling out of bounds.

    # It doesn't fit!  reorder everything!
    epics = iteration.epics.all().order_by(field_name)

    rank = 10
    for other_epic in epics:
        if other_epic.__dict__[field_name] == epic_rank_after:
            # We should always have a epic_rank_after if we get here.
            epic.__dict__[field_name] = rank
            rank += 20
        other_epic.__dict__[field_name] = rank
        other_epic.save()
        rank += 20




def reorderStory( story, before_id, after_id, iteration, field_name="rank"):
    "Reorders a story between two others."
    story_rank_before = 0
    story_rank_after = 999999 # max value of the DB field

    # If there is a story that should be before this one, grab it's rank
    try:
        story_before = Story.objects.get( id=before_id )
        story_rank_before = story_before.__dict__[field_name]
    except:
        pass

    # If  there is a story that should be after this one, grab it's rank.
    try:
        story_after = Story.objects.get( id=after_id )
        story_rank_after = story_after.__dict__[field_name]
    except:
        pass

    diff = abs(story_rank_after - story_rank_before)
    # logger.debug("Before %d , after %d, diff %d" % (story_rank_after, story_rank_before, diff) )
    try:
        if diff > 1:
            # It fits between them
            # logger.debug(round(diff/2) )
            story.__dict__[field_name] = round(diff/2) + story_rank_before
            logger.debug("Reordering fit %d %d %d" % (story_rank_before, story.__dict__[field_name], story_rank_after) )
            return
    except:
        pass # do an emergency re-order below if things are falling out of bounds.

    # It doesn't fit!  reorder everything!
    stories = iteration.stories.all().order_by(field_name)

    rank = 10
    for other_story in stories:
        if other_story.__dict__[field_name] == story_rank_after:
            # We should always have a story_rank_after if we get here.
            story.__dict__[field_name] = rank
            rank += 20
        other_story.__dict__[field_name] = rank
        other_story.save()
        rank += 20





# On the iteration planning page, this renders one story view.  Generally called
# via ajax.
@login_required
def mini_story( request, group_slug, story_id):
    story = get_object_or_404( Story, id=story_id )
    read_access_or_403(story.project,request.user)
    return render_to_response("stories/single_mini_story.html", {
        "story": story,
      }, context_instance=RequestContext(request))


def _calculate_rank( iteration, general_rank ):
    """ calculates the rank a new story should have for a project based off of 3 general rankings.
    0=top, 1=middle, 2=bottom
    TODO (Improvement) - I'd like to re-think how ranking is done for both initial and adjustments of ranks.
    """
    try:
        stories = iteration.stories.all().order_by("rank")
        story_count = len(stories)
        print "%d %s" % (story_count, [story.rank for story in stories])
        if story_count == 0:
            return 10

        if( general_rank == 0): # top
            return int(stories[0].rank / 2)

        if( general_rank == 1): # middle
            if story_count < 2:
                return 10
            s1_rank = stories[ int(story_count/2) - 1 ].rank
            s2_rank = stories[ int(story_count/2) ].rank
            return int( (s1_rank + s2_rank) / 2 )

        return stories[ story_count - 1 ].rank + 10
    except:
        return 10

@login_required
def story_block(request, story_id):
    story = get_object_or_404( Story, id=story_id )
    read_access_or_403( story.project, request.user )
    organization = _organizationOrNone( story.project )
    template = "stories/single_block_story.html"
    if request.GET.get("story_type") == "scrumboard":
        template = "stories/single_scrum_board_story.html"
    return render_to_response(template, {
        "story": story,
        "return_type": "block",
        "project": story.project,
        "organization": organization
      }, context_instance=RequestContext(request))

# Returns the edit-story form, with minimal html wrapper.  This is useful for displaying within
# a facebox popup.
# One place it's used is on the iteration page when you click the magnifying glass for a story.
@login_required
def story_edit(request, group_slug, story_id):
    story = get_object_or_404( Story, id=story_id )
    project = get_object_or_404( Project, slug=group_slug )
    return_type = request.GET.get("return_type","mini")

    if request.method == 'POST': # If the form has been submitted...
        old_story = story.__dict__.copy()
        write_access_or_403(project,request.user)
        form = StoryForm( project, request.POST, project, instance=story) # A form bound to the POST data

        if form.is_valid(): # All validation rules pass
            story = form.save(commit=False)
            

            if request.POST.get("category_name","") != "":
                try:
                    category_name = request.POST.get("category_name")
                    category_name = category_name.replace(",","").strip()
                    category_name = category_name[:50]
                    if not category_name in project.getCategoryList():
                        project.categories = "%s, %s" % (project.categories, category_name)
                        if len(project.categories) <= 1024:
                            project.save()
                        else:
                            request.user.message_set.create(message="Too many categories")
                    story.category = category_name
                except:
                    pass # no category to use
                
            story.save()
            
            diffs = utils.model_differences(old_story, story.__dict__, dicts=True)
            activities = 0

            signals.story_updated.send( sender=request, story=story, diffs=diffs, user=request.user )
            onDemandCalculateVelocity( project )

        organization = _organizationOrNone( project )
        if( request.POST['return_type'] == 'mini'):
            return render_to_response("stories/single_mini_story.html", {
                "story": story,
                "return_type": return_type,
              }, context_instance=RequestContext(request))
        if( request.POST['return_type'] == 'scrumboard'):
            return render_to_response("stories/single_scrum_board_story.html", {
                "story": story,
                "return_type": return_type,
                "project": story.project,
                "organization": organization
              }, context_instance=RequestContext(request))
        if( request.POST['return_type'] == 'block'):
            return render_to_response("stories/single_block_story.html", {
                "story": story,
                "return_type": return_type,
                "project": story.project,
                "organization": organization
              }, context_instance=RequestContext(request))
        if( request.POST['return_type'] == 'queue'):
            return render_to_response("stories/single_queue_story.html", {
                "story": story,
                'project': story.project,
                "organization": organization,
                "return_type": return_type,
              }, context_instance=RequestContext(request))

    else:
        read_access_or_403(project,request.user)
        form = StoryForm(project, instance=story )

    tags = project.unique_tags()

    return   render_to_response("stories/story.html", {
        "story": story,
        "form": form,
        "project": project,
        "return_type": return_type
      }, context_instance=RequestContext(request))

@login_required
def stories_scrum_board(request, group_slug, iteration_id, status):
    project = get_object_or_404(Project, slug=group_slug)
    read_access_or_403(project,request.user)
    iteration = get_object_or_404(Iteration, id=iteration_id, project=project)


    stories = iteration.stories.select_related('project', 'project__organization','project__organization__subscription',  'iteration','iteration__project',).filter(status=STATUS_REVERSE[status]).order_by("board_rank")

    return render_to_response("stories/scrum_board_story_list.html", {
    "stories": stories,
    "project":project
    }, context_instance=RequestContext(request))


@login_required
def epic(request, epic_id):
    "Returns a snippet of html suitable for use in replacing an epic block on the backlog pages."
    epic = get_object_or_404(Epic, id=epic_id)
    project = epic.project
    read_access_or_403(project,request.user)
    organization = _organizationOrNone( project )
    return render_to_response("projects/epic.html", {
    "epic": epic,
    "organization":organization,
    "project":project
    }, context_instance=RequestContext(request))

# Returns the stories for a given iteration as an html snippet.  The iteration planning page uses this
# uplon load, and then also upon filtering by the user
@login_required
def stories_iteration(request, group_slug, iteration_id, page=1):
    page = int(page)
    project = get_object_or_404(Project, slug=group_slug)
    read_access_or_403(project,request.user)
    iteration = get_object_or_404(Iteration, id=iteration_id, project=project)

    order_by = request.GET.get("order_by","rank")
    display_type = request.GET.get("display_type","mini")
    text_search = request.GET.get("search","").strip()
    tags_search = request.GET.get("tags","").strip()
    category = request.GET.get("category","").strip()
    clrbtn = request.GET.get("clearButton",'')
    only_assigned = request.GET.get("only_assigned", False)
    backlog_mode = request.GET.get("backlog_mode", False)
    paged = "True" == request.GET.get("paged", "True")

    if request.GET.get("clearButton") == "Clear Filter":
        text_search = ""
        tags_search = ""
        category =""

    if only_assigned == "False":
        only_assigned = False

    # Store the query string, so it can be passed back for subsequent page requests.
    query_string = urllib.urlencode( {   'order_by':order_by,
                                         'display_type':display_type,
                                         'search':text_search.encode('utf-8'),
                                         'tags':tags_search.encode('utf-8'),
                                         'category':category.encode('utf-8'),
                                         'only_assigned':only_assigned,
                                         'clearButton':clrbtn
                                         })
    has_next = False
    if text_search == "":
        # Don't need to consult our solr search engine.
        has_next, stories = _getStoriesNoTextSearch( iteration, order_by, tags_search, category, only_assigned, request.user, paged, page, backlog_mode)
    else:
        # we need some fancy-schmancy searching
        stories = _getStoriesWithTextSearch( iteration, text_search, order_by, tags_search, category, only_assigned, request.user, backlog_mode)

    organization = _organizationOrNone( project )

    return render_to_response("stories/mini_story_list.html", {
      "stories": stories,
      "project":project,
      "return_type":display_type,
      "display_type": display_type,
      "load_next_page": has_next ,
      "next_page_num": page+1,
      "next_page_query_string":query_string,
      "iteration_id": iteration.id,
      "organization": organization
    }, context_instance=RequestContext(request))

def _organizationOrNone(project):
    try:
        organization = project.organization
    except Organization.DoesNotExist:
        organization = None
    return organization

def _getStoriesWithTextSearch( iteration, text_search, order_by, tags_search, category, only_assigned, user, backlog_mode):
    search_results = SearchQuerySet().filter(project_id=iteration.project.id).filter(iteration_id=iteration.id).filter(content=text_search).models(Story).order_by(order_by).load_all()
    if tags_search != "":
        search_results = search_results.filter(tags=tags_search)
    if category != "":
        search_results = search_results.filter(category=category)
    if only_assigned:
        search_results = search_results.filter(user_id=user.id)
    stories = [ result.object for result in search_results]
    return stories

def _getStoriesNoTextSearch( iteration, order_by, tags_search, category, only_assigned, user, paged, page, backlog_mode):
    tags_list = re.split('[, ]+', tags_search)

    stories = iteration.stories

    if order_by != "numeric_points":
        stories = stories.order_by(order_by)
    else:
        # Tried a few things here... CAST(points as SIGNED) in the order_by clause would have been preferred, but I couldn't get that through
        # the ORM.  Secondary, I tried craeting a custom column assigned to that, but it caused the query to fail.  The 0+string is a bit
        # of a mysql specific hack to convert a string to a number.
        stories = stories.extra(select={'numeric_points': '0+points'}).order_by(order_by)

    if tags_search:
        stories = stories.filter(story_tags__tag__name__in=tags_list).distinct().order_by(order_by)
    if only_assigned:
        stories = stories.filter(assignee=user)
    if category:
        stories = stories.filter(category=category)
    if backlog_mode:
        stories = stories.filter(epic=None)

    stories = stories.select_related('project', 'project__organization','project__organization__subscription', 'iteration',)

    if paged:
        paginator = Paginator(stories, 50)
        page_obj = paginator.page(page)
        has_next = page_obj.has_next()
        stories = page_obj.object_list
    else:
        has_next = False

    return (has_next, stories)

@login_required
def ajax_add_story( request, group_slug):
    project = get_object_or_404(Project, slug=group_slug)

    if request.method == "POST" and request.POST.get("action") == "addStory":
        form = AddStoryForm(project, request.POST) # A form bound to the POST data
        if form.is_valid(): # All validation rules pass
            story = _handleAddStoryInternal( form , project, request)
            return render_to_response("stories/story_added.html", {"story": story}, context_instance=RequestContext(request))

    # A story wasn't created...
    return HttpResponse("")




def _handleAddStoryInternal( form , project, request):
    story = form.save( commit=False )
    story.local_id = project.getNextId()
    story.project = project
    story.creator = request.user
    iteration_id = request.POST.get("iteration",None)
    epic_id = request.POST.get("epic",None)

    if iteration_id != None:
        iteration = get_object_or_404(Iteration, id=iteration_id)
        if iteration.project != project:
            raise PermissionDenied() # Shenanigans!
        story.iteration = iteration
    else:
        story.iteration = project.get_default_iteration()

    if epic_id != None and epic_id != "":
        epic = Epic.objects.get(id=epic_id)
        if epic.project != project:
            raise PermissionDenied() # Shenanigans!
        story.epic = epic

    try:
        general_rank = int(form.cleaned_data['general_rank'])
    except:
        general_rank = 2 # bottom



    if request.POST.get("category_name") != "":
        category_name = request.POST.get("category_name","")
        category_name = category_name.replace(",","").strip()
        category_name = category_name[:50]
        if not category_name in project.getCategoryList():            
            project.categories = "%s, %s" % (project.categories, category_name)
            if len(project.categories) <= 1024:
                project.save()
            else:
                request.user.message_set.create(message="Too many categories")                        
        story.category = category_name
        

    story.rank = _calculate_rank( story.iteration, general_rank )
    # logger.info("New Story %s" % story.summary)
    story.save()
    if story.points_value() > 0:
        onDemandCalculateVelocity( project )
    signals.story_created.send( sender=request, story=story, user=request.user )
    # story.activity_signal.send(sender=story, user=request.user, story=story, action="created", project=project)
    request.user.message_set.create(message="Story #%d created." % story.local_id )
    return story


def handleAddStory( request , project ):
    """ Handles the add story form.
        Various views have an add story form on them.  This method handles that,
        and returns a new StoryForm object the view can use. """
    if request.method == "POST" and request.POST.get("action") == "addStory":
        form = AddStoryForm(project, request.POST) # A form bound to the POST data
        if form.is_valid(): # All validation rules pass
            story = _handleAddStoryInternal( form , project, request)
        else:
            return form
    return AddStoryForm( project )


# The iteration planning tool.  It can also handle the add story form.
# TODO (cleanup): We should factor out the add story form functionality
# TODO (cleanup): We should rename this method, and likely rename the URL that points at it as well.
@login_required
def stories(request, group_slug):
    project = get_object_or_404(Project, slug=group_slug)
    write_access_or_403(project,request.user)

    form = handleAddStory(request, project )

    return render_to_response("stories/iteration_planning.html", {
      "add_story_form": form,
      "project": project,
      "return_type":"mini",
      "current_view":"iteration_planning",
      "default_iteration_id": int(request.GET.get("iteration","-1"))

    }, context_instance=RequestContext(request))



def pretty_print_story(request, group_slug, story_id):
    """Returns an html snippet that we use for a read-only full view of the story.  Right now, this is used
       when you mouse-hover over the eye icon for a story on an iteration page.  """
    story = get_object_or_404(Story, id=story_id)
    read_access_or_403( story.project, request.user )

    return render_to_response("stories/single_story_read_only.html", {
        "story": story
    }, context_instance=RequestContext(request))

def ajax_add_epic(request, group_slug):
    project = get_object_or_404(Project, slug=group_slug)
    write_access_or_403(project,request.user)
    if request.method == 'POST': # If the form has been submitted...        
        form = EpicForm( project, request.POST)
        if form.is_valid(): # All validation rules pass
            epic = form.save()
            onDemandCalculateVelocity(project)
            return HttpResponse(epic.id)
    return HttpResponse("")
########NEW FILE########
__FILENAME__ = tags_views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.conf import settings
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponse, HttpResponseRedirect, HttpResponseForbidden
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext


from projects.models import Project, Story, StoryTag, StoryTagging
from projects.access import *
from story_views import handleAddStory

def tag_detail(request, group_slug, tag_name):
    project = get_object_or_404( Project, slug=group_slug )
    
    read_access_or_403(project,request.user)
    tags_list = StoryTag.objects.filter( project=project, name=tag_name )

    stories = []
    for tags in tags_list:
        # TODO: there's a bug somewhere causing duplicate tags to be created
        stories += [ tagging.story for tagging in tags.stories.all() ]
    
    stories = sorted(stories, key=lambda story: story.rank)
    add_story_form = handleAddStory(request, project)

    if len(tags_list) == 0:
        tag = None
    else:
        tag = tags_list[0]

    return render_to_response("projects/tag_page.html", {
        "tag": tag,
        "stories":stories,
        "organization":_organizationOrNone(project),
        "project" : project,
        'add_story_form': add_story_form,
        "current_view":"tags_view"
      }, context_instance=RequestContext(request))

def _organizationOrNone(project):
  try:
      organization = project.organization
  except Organization.DoesNotExist:
      organization = None
  return organization

########NEW FILE########
__FILENAME__ = task_views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponse, HttpResponseRedirect, HttpResponseForbidden, HttpResponseNotAllowed
from django.core.urlresolvers import reverse
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.utils.translation import ugettext_lazy as _


from django.conf import settings

import re


from projects.models import Task, Story
from projects.forms import TaskForm
from projects.access import *
import projects.signals as signals

import logging

logger = logging.getLogger(__name__)

# View called via ajax on the iteration or iteration planning pages.  Meant to add one new task
@login_required
def create_task( request ):
    if request.method != "POST":
        return HttpResponseNotAllowed('Only POST allowed')
    story_id = request.POST.get("story_id")
    story = get_object_or_404( Story, id=story_id )
    assignee = None
    assignee_id = request.POST.get("assignee")
    if assignee_id != "-1":
        assignee = User.objects.get(id=assignee_id)

    write_access_or_403( story.project, request.user )
    task_text = request.POST.get("summary")
    logger.debug("Adding task to story %d %s" % (story.id, task_text) )

    if story.tasks.count() > 0:
        order = story.tasks.order_by("-order")[0].order + 1
    else:
        order = 0

    task = Task(story=story, summary=task_text, assignee=assignee, order=order)
    task.save()
    signals.task_created.send( sender=request, task=task, user=request.user )
    return HttpResponse("OK")

@login_required
def set_task_status( request, task_id ):
    if request.method != "POST":
        return HttpResponseNotAllowed('Only POST allowed')
    status = request.POST.get("status", None)
    if not status:
        return HttpResponse("FAIL")
    task = get_object_or_404(Task, id=task_id)
    write_access_or_403( task.story.project, request.user )
    task.complete = (status == "done")
    task.save()
    signals.task_status_changed.send( sender=request, task=task, user=request.user )
    return HttpResponse("OK")

@login_required
def delete_task( request, task_id ):
    if request.method != "POST":
        return HttpResponseNotAllowed('Only POST allowed')
    task = get_object_or_404(Task, id=task_id)
    write_access_or_403( task.story.project, request.user )
    story = task.story
    signals.task_deleted.send( sender=request, task=task, user=request.user )
    task.sync_queue.clear()
    task.delete()

    return HttpResponse("OK")

@login_required
def edit_task(request, task_id):
    task = get_object_or_404(Task, id=task_id)
    write_access_or_403( task.story.project, request.user )

    if request.method == "POST":
        form = TaskForm( task.story.project , request.POST, instance=task)
        signals.task_updated.send( sender=request, task=task, user=request.user )
        form.save()
        return HttpResponse("OK")
    else:
        form = TaskForm( task.story.project , instance=task)
        return render_to_response("tasks/edit.html", {
          "task": task,
          "form": form
        }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = iteration_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from projects.forms import ProjectForm

register = template.Library()

# Spits out the iteration name as a link prefixed with the correct icon
@register.inclusion_tag('projects/iteration_name.html', takes_context=True)
def iteration_name(context, iteration):
    return {'iteration': iteration, 'request': context['request']}


# Spits out the iteration icon
@register.inclusion_tag('projects/iteration_icon.html', takes_context=True)
def iteration_icon(context, iteration):
    return {'iteration': iteration, 'request': context['request']}

########NEW FILE########
__FILENAME__ = projects_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from datetime import date, time, datetime, timedelta

from projects.forms import ProjectForm
from projects.models import Story
from projects.access import has_write_access, has_admin_access, has_read_access
from projects.util import reduce_burndown_data
from projects.limits import personal_email_limit, org_email_limit
from django.template.defaultfilters import stringfilter
from django.conf import settings
from django.db import models
import traceback
import urllib



import re
register = template.Library()

urlfinder = re.compile('((?:https|http):\/\/[^\s<>]+)')
import logging

logger = logging.getLogger(__name__)



@register.simple_tag
def silk(name):
    return """<img class="silk_icon" src="%spinax/images/silk/icons/%s.png" />""" % (settings.SSL_STATIC_URL, name)


@register.filter("google_chart_url")
def google_chart_url(iteration_or_project):
    return _google_chart(iteration_or_project)

@register.filter("tiny_google_chart_url")
def tiny_google_chart_url(iteration_or_project):
    return _google_chart(iteration_or_project, project_size="200x50", iteration_size="200x50", label_axis="", bg_color="f6f6f6", axis_color="f6f6f6", title=True)

def _google_chart(iteration_or_project, project_size="550x120", iteration_size="550x80", label_axis="y", bg_color="ffffff", axis_color="444444", title=False):
    """Returns a URL for either a project or an iteration that corresponds to the burn up chart generated by google charts.
       The url will be to an image format. If no chart can be drawn, a 1x1 image is returned.  This should be used for quick
       summary charts, not detailed pretty ones.  We only use it in emails right now.  """
    try:
        total_points = []
        claimed_points = []
        max_val = 0
        claimed_dates = []
        claimed_values = []
        total_dates = []
        total_values = []

        # Chart Size...
        if hasattr(iteration_or_project,"slug"):
            size = project_size
            # Project charts are bigger than iteration charts.
        else:
            size = iteration_size

        # Gather up all the points_log entries.
        for log in iteration_or_project.points_log.all():
            total_points.append( [log.timestamp(), log.points_total] )
            claimed_points.append( [log.timestamp(), log.points_claimed] )
            if log.points_total > max_val:
                max_val = log.points_total

        # If we don't have enough points to draw a chart, bail.
        if len(total_points) <= 1:
            return "https://chart.googleapis.com/chart?cht=lxy&chs=1x1"

        # Remove redundant data in chart data.
        total_points = reduce_burndown_data(total_points)
        claimed_points = reduce_burndown_data(claimed_points)

        # Some helper values.
        start_date = total_points[0][0]        
        end_date = total_points[-1][0]
        
        
        start_date_s = date.fromtimestamp( start_date/1000 ).strftime('%Y-%m-%d')
        
        try:
            end_date_s = iteration_or_project.end_date.strftime('%Y-%m-%d')
            end_date = int(datetime.combine(iteration_or_project.end_date, time()).strftime("%s")) * 1000
            # logger.debug("!!!!")
            # logger.debug(end_date)
        except:                    
            end_date_s = date.fromtimestamp( end_date/1000 ).strftime('%Y-%m-%d')

        
        # logger.debug("END DATE" % end_date_s)            
        date_range = end_date - start_date

        # Create the entries for the total points series.
        for piece in total_points:
            total_dates.append( _googleChartValue(piece[0], start_date, end_date) )
            total_values.append( _googleChartValue( piece[1] ,0, max_val) )

        # Create the entries for the claimed points series.
        for piece in claimed_points:
            claimed_dates.append( _googleChartValue(piece[0], start_date, end_date) )
            claimed_values.append( _googleChartValue( piece[1] ,0, max_val) )

        if title:
            title_snippet = "chtt=%s&chts=000000,8&" %  urllib.quote(iteration_or_project.name)
        else:
            title_snippet = ""
        
        # Put it all together in google chart format.  (Docs: http://code.google.com/apis/chart/)
        data = "https://chart.googleapis.com/chart?%schf=bg,s,%s&chxr=0,0,%d&cht=lxy&chs=%s&chd=s:%s,%s,%s,%s&chxt=%s,x&chxs=0,%s,8,0,lt&chxl=1:|%s|%s&chco=9ED147,30B6EB&chm=B,eef5fb,1,0,0|B,99CBB0,0,0,0" % ( title_snippet, bg_color, max_val,size,"".join(claimed_dates), "".join(claimed_values), "".join(total_dates), "".join(total_values), label_axis, axis_color, start_date_s, end_date_s )
        #logger.debug(data)
        return data
    except:
        return "https://chart.googleapis.com/chart?cht=lxy&chs=1x1"


def _googleChartValue(val, min_val, max_val):
    """ Google charts can encode values in a 62 value range using alpha numeric characters.  This
        method does that for a given value, and a given range (min/max) of values """
    codes = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789"
    percent = (val-min_val) / float(max_val - min_val)
    new_val = int( 61 * percent )
    return codes[ new_val ]

@register.filter("urlify2")
def urlify2(value):
    return urlfinder.sub(r'<a target="_blank" href="\1">\1</a>', value)

@register.filter("name_or_username")
def name_or_username(user):
    if user.first_name and user.last_name:
        return "%s %s" % (user.first_name, user.last_name)
    return user.username

@register.filter("probable_email")
def probable_email(user):
    """ Returns what is probably the user's email address.  Use this to get the address of a user who never
        actually verified it. """
    if len(user.email) > 0:
        return user.email
    addrs = user.emailaddress_set.all()
    for email in addrs:
        if email.verified:
            return email.email

    # no verified, no primary emails...
    if len(addrs) > 0:
        return addrs[0].email

    return ""

@register.filter
def gt(a, b):
    return a > b

@stringfilter
def link_stories(value, project):
    """ Creates links to stories in a body of text.
        Example: 'Story #4' would open up the edit window for story with local_id #4 """
    def replaceLink( value ):
        try:
            local_id = value.group(1)
            story = Story.objects.get( project=project, local_id=int(local_id) )
            return "<a class='storyLink' onclick=\"openOverlay( '/projects/project/%s/story/%d?return_type=block' ); return false;\" >%s</a>" % (project.slug, story.id, value.group(0))
        except:
            return value.group(0)

    return re.sub(r'[sS]tory #([0-9]+)', replaceLink , value)

link_stories.is_safe=True
register.filter('link_stories', link_stories)



@register.inclusion_tag("projects/iteration_list.html", takes_context=True)
def show_iterations(context, project):        
    request = context['request']
    show_more = False
    if (project.iterations.count() <= 15) or (request.GET.get("more","false")=="true"):
        # If less than 15 iterations, show them all
        iterations = project.iterations.all()
    else:
        # If too many, be selective
        show_more = True
        iterations = project.iterations.filter(  models.Q(default_iteration = True) |  # We always show the backlog.
                                        models.Q(start_date = None) | models.Q( end_date = None) | # And we show iterations without dates on either end
                                        models.Q(end_date__gt = datetime.today() - timedelta(days=30), end_date__lte = datetime.today()) | # We show past iterations within 30 days
                                        models.Q(start_date__gte = datetime.today(), start_date__lt =  datetime.today() + timedelta(days=30)) | # and future iterations within 30 days
                                        models.Q(start_date__lte = datetime.today(), end_date__gte =  datetime.today() ) # And current iterations too
                                        )
    return {'iterations':iterations, 'show_more':show_more, 'project':project, 'request':context['request']}
    



@register.inclusion_tag("projects/project_item.html", takes_context=True)
def show_project(context, project):
    return {'project': project, 'request': context['request']}

# @@@ should move these next two as they aren't particularly project-specific

@register.simple_tag
def clear_search_url(request):
    getvars = request.GET.copy()
    if 'search' in getvars:
        del getvars['search']
    if len(getvars.keys()) > 0:
        return "%s?%s" % (request.path, getvars.urlencode())
    else:
        return request.path

@register.simple_tag
def persist_getvars(request):
    getvars = request.GET.copy()
    if len(getvars.keys()) > 0:
        return "?%s" % getvars.urlencode()
    return ''

@register.tag(name="notlocked")
def isNotLocked(parser, token):
    tag_name, story = token.split_contents()
    nodelist = parser.parse(('endnotlocked',))
    parser.delete_first_token()
    return NotLockedNode(nodelist, story)

class NotLockedNode(template.Node):
    def __init__(self, nodelist, story):
        self.nodelist = nodelist
        self.story = story
    def render(self, context):
        if not context[self.story].iteration.locked:
            output = self.nodelist.render(context)
            return output
        else:
            return ""

@register.tag(name="archived")
def isArchived(parser, token):
    tag_name, project = token.split_contents()
    nodelist = parser.parse(('endarchived',))
    parser.delete_first_token()
    return ArchivedNode(nodelist, project)

class ArchivedNode(template.Node):
    def __init__(self, nodelist, project):
        self.nodelist = nodelist
        self.project = project
    def render(self, context):
        try:
            if not context[self.project].active:
                output = self.nodelist.render(context)
                return output
            else:
                return ""
        except:
            return ""



@register.tag(name="locked")
def istLocked(parser, token):
    tag_name, story = token.split_contents()
    nodelist = parser.parse(('endlocked',))
    parser.delete_first_token()
    return LockedNode(nodelist, story)

class LockedNode(template.Node):
    def __init__(self, nodelist, story):
        self.nodelist = nodelist
        self.story = story
    def render(self, context):
        if context[self.story].iteration.locked:
            output = self.nodelist.render(context)
            return output
        else:
            return ""

@register.tag(name="isadmin")
def isadmin( parser, token):
    tag_name, project = token.split_contents()
    nodelist = parser.parse(('endisadmin',))
    parser.delete_first_token()
    return IsAdminNode(nodelist, project)

class IsAdminNode(template.Node):
    def __init__(self, nodelist, project):
        self.nodelist = nodelist
        self.project = project
    def render(self, context):
        if has_admin_access(context[self.project], context["request"].user):
            output = self.nodelist.render(context)
            return output
        else:
            return ""

@register.tag(name="canemail")
def can_email(parser, token):
    tag_name, project = token.split_contents()
    nodelist = parser.parse(('endcanemail',))
    parser.delete_first_token()
    return CanEmailNode(nodelist, project)

class CanEmailNode(template.Node):
    def __init__(self, nodelist, project):
        self.nodelist = nodelist
        self.project = project
    def render(self, context):
        access = True
        project = context[self.project]
        if project.organization:
            access = org_email_limit.increaseAllowed(organization=project.organization)
        else:
            access = personal_email_limit.increaseAllowed(project=project)

        if access:
            output = self.nodelist.render(context)
            return output
        else:
            return ""

@register.tag(name="canwrite")
def canwrite( parser, token):
    tag_name, project = token.split_contents()
    nodelist = parser.parse(('endcanwrite',))
    parser.delete_first_token()
    return CanWriteNode(nodelist, project)

class CanWriteNode(template.Node):
    def __init__(self, nodelist, project):
        self.nodelist = nodelist
        self.project = project
    def render(self, context):
        if has_write_access(context[self.project], context["request"].user):
            output = self.nodelist.render(context)
            return output
        else:
            return ""


@register.tag(name="canread")
def canread( parser, token):
    tag_name, project = token.split_contents()
    nodelist = parser.parse(('endcanread',))
    parser.delete_first_token()
    return CanReadNode(nodelist, project)

class CanReadNode(template.Node):
    def __init__(self, nodelist, project):
        self.nodelist = nodelist
        self.project = project
    def render(self, context):
        if has_read_access(context[self.project], context["request"].user):
            output = self.nodelist.render(context)
            return output
        else:
            return ""

########NEW FILE########
__FILENAME__ = sitetips_tags
from django import template

register = template.Library()

@register.simple_tag
def tip():
    return ""

########NEW FILE########
__FILENAME__ = story_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from projects.models import Story
from django.utils.safestring import mark_safe
import re
register = template.Library()


@register.filter
def show_points(story):
    points = story.getPointsLabel()
    if points == "Infinite":
        return mark_safe("&infin;")
    else:
        return mark_safe(points)

@register.simple_tag
def summary_view( story, user):
    if story.assignee != user:    
        return '<div style="display:inline" class="project disabled_project">%s  %s</div>' % ( story.summary, story.detail)
    else:
        return '%s  %s' % (story.summary, story.detail)

########NEW FILE########
__FILENAME__ = tasks_tags
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django import template
from projects.models import Story
import re
register = template.Library()

@register.simple_tag
def task_counts( story):
    total_tasks = story.tasks.count()
    complete_tasks = story.tasks.filter(complete=True).count()
    if complete_tasks > 0:
        return "%d/%d" % (complete_tasks, total_tasks)
    elif total_tasks > 0:
        return "%d" % (total_tasks)
    else:
        return ""

@register.simple_tag
def my_tasks( story, user):
    total_tasks = story.tasks.filter(assignee=user)
    return_str = [""]
    if len(total_tasks) > 0:
        return_str.append("<ul>")
        for t in total_tasks:
            css_class = "struck_out" if t.complete else ""
            return_str.append("<li class='%s'>%s</li>" % (css_class,t.summary) )
        return_str.append("</ul>")
    return "".join(return_str)
            
########NEW FILE########
__FILENAME__ = urls
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.conf.urls.defaults import *

from projects.models import Project

# from groups.bridge import ContentBridge
#
#
# bridge = ContentBridge(Project, 'projects')

urlpatterns = patterns('projects.views',
    # url(r'^$', 'projects', name="project_list"),
    url(r'^create/$', 'create', name="project_create"),
    url(r'^your_projects/$', 'your_projects', name="your_projects"),
    url(r'^project/(?P<group_slug>[-\w]+)/$', 'project', name="project_detail"),
    # url(r'^project/(?P<group_slug>[-\w]+)/add_category$', 'add_category', name="add_category"),
    url(r'^project/(?P<group_slug>[-\w]+)/activate/$', 'activate', name="project_activate"),
    url(r'^project/(?P<group_slug>[-\w]+)/delete/$', 'delete', name="project_delete"),
    url(r'^project/(?P<group_slug>[-\w]+)/admin$', 'project_admin', name="project_admin"),
    url(r'^project/(?P<group_slug>[-\w]+)/fix_local_id$', 'fix_local_id', name="fix_local_id"),
    url(r'^project/(?P<group_slug>[-\w]+)/history$', 'project_history', name="project_history"),
    url(r'^project/(?P<group_slug>[-\w]+)/test_data/(?P<count>[0-9]+)', 'test_data'),
    url(r'^project/(?P<group_slug>[-\w]+)/(?P<iteration_id>[0-9]+)/burndown$', 'iteration_burndown'),
    url(r'^project/(?P<group_slug>[-\w]+)/burndown$', 'project_burndown'),
    url(r'^project/(?P<group_slug>[-\w]+)/remove_user$', 'remove_user', name="remove_user"),
    url(r'^project/(?P<group_slug>[-\w]+)/project_prediction$', 'project_prediction', name="project_prediction"),
    url(r'^project/(?P<group_slug>[-\w]+)/export$', 'export_project', name="export_project"),
    url(r'^project/(?P<group_slug>[-\w]+)/epics$', 'epics', name="epics"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration_list$', 'iteration_list', name="iteration_list"),
    url(r'^project/(?P<group_slug>[-\w]+)/all_iterations$', 'all_iterations', name="all_iterations"),
    url(r'^project/(?P<group_slug>[-\w]+)/search$', 'search_project', name="search_project"),
    

    # url(r'^project/(?P<group_slug>[-\w]+)/burnup_chart.png$', 'burnup_chart', name="burnup_chart"),

)

urlpatterns += patterns('projects.tags_views',
    url(r'^project/(?P<group_slug>[-\w]+)/tag/(?P<tag_name>.+)$', 'tag_detail', name="tag_detail"),
)

urlpatterns += patterns('projects.iteration_views',
    url(r'^project/(?P<group_slug>[-\w]+)/iteration_create$', 'iteration_create', name="iteration_create"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/delete', 'delete_iteration', name="delete_iteration"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/unlock$', 'unlock_iteration', name="unlock_iteration"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/import$', 'iteration_import', name="iteration_import"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/export$', 'iteration_export', name="iteration_export"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/report$', 'iteration_report', name="iteration_report"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/board$', 'scrum_board', name="scrum_board"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)/iteration_stats$', 'iteration_stats', name="iteration_stats"),
    url(r'^project/(?P<group_slug>[-\w]+)/iteration/(?P<iteration_id>[0-9]+)$', 'iteration', name="iteration"),

)


urlpatterns += patterns('projects.story_views',
    url(r'^story_permalink/(?P<story_id>[0-9]+)$', 'story_permalink', name="story_permalink"),
    url(r'^project/(?P<group_slug>[-\w]+)/stories/$', 'stories', name="stories"),
    url(r'^project/(?P<group_slug>[-\w]+)/stories/createAsync$', 'ajax_add_story', name="ajax_add_story"),
    url(r'^project/(?P<group_slug>[-\w]+)/epics/createAsync$', 'ajax_add_epic', name="ajax_add_epic"),
    url(r'^epic/(?P<epic_id>[0-9]+)$', 'epic', name="epic"),
    url(r'^epic/(?P<epic_id>[0-9]+)/edit', 'edit_epic', name="edit_epic"),
    url(r'^epic/(?P<epic_id>[0-9]+)/delete', 'delete_epic', name="delete_epic"),    
    url(r'^project/(?P<group_slug>[-\w]+)/stories/(?P<iteration_id>[0-9]+)/board/(?P<status>[-\w]+)$', 'stories_scrum_board', name="stories_scrum_board"),
    url(r'^project/(?P<group_slug>[-\w]+)/stories/(?P<iteration_id>[0-9]+)/(?P<page>[0-9]+)$', 'stories_iteration', name="stories_iteration"),
    url(r'^story/(?P<story_id>[0-9]+)/comments', 'story_comments'),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/pretty', 'pretty_print_story'),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/set_todo', 'set_story_status', {'status':1}),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/set_doing', 'set_story_status', {'status':2}),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/set_reviewing', 'set_story_status', {'status':3}),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/set_done', 'set_story_status', {'status':4}),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/scrum_board', 'scrum_board'),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/reorder', 'reorder_story'),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)/delete', 'delete_story', name="delete_story"),    
    url(r'^project/(?P<group_slug>[-\w]+)/epic/(?P<epic_id>[0-9]+)/reorder', 'reorder_epic'),
    url(r'^story/(?P<story_id>[0-9]+)', 'story_block', name="story_block"),
    url(r'^project/(?P<group_slug>[-\w]+)/story/(?P<story_id>[0-9]+)', 'story_edit', name="story_form"),
    url(r'^project/(?P<group_slug>[-\w]+)/mini_story/(?P<story_id>[0-9]+)', 'mini_story'),
)

urlpatterns += patterns('projects.task_views',
    url(r'^task/create$', 'create_task', name="create_task"),
    url(r'^task/(?P<task_id>[0-9]+)/set_status$', 'set_task_status', name="set_task_status"),
    url(r'^task/(?P<task_id>[0-9]+)/delete$', 'delete_task', name="delete_task"),
    url(r'^task/(?P<task_id>[0-9]+)/edit$', 'edit_task', name="edit_task"),
)

# urlpatterns += bridge.include_urls('tasks.urls', r'^project/(?P<group_slug>[-\w]+)/tasks/')

########NEW FILE########
__FILENAME__ = util
def reduce_burndown_data( data ):
    """Takes a list of datapoints for a burnup chart and if there are more than 30, removes any redundant points.
       Redundant is when a point's two neighbors are equal to itself so it would just be a marker on a straight line.
       (I guess points along a straight sloped line could be considered redundant, but we don't remove those)
       The middle 15 is considered redundant here: [5,6,10,10,15,15,15,20]
       The middle 4 threes are considered redundant here: [1,2,3,3,3,3,3,3,5]
    """
    if len(data) < 30:
        return data

    subset = data[1:-1] # Subset of data that never includes first/last
    remove = []
    for idx,item in enumerate( subset ):
                # idx = index before this item in data
                # idx+1 = this item in data
                # idx+2 = next item in data
        last_val = data[ idx ][1]
        next_val = data[ idx+2 ][1]

        if item[1]==last_val and item[1]==next_val:
            # don't need this item!
            remove.append(idx+1)
            # logger.debug("Can remove %d" % (idx+1))

    remove.reverse()
    for remove_index in remove:
        del data[remove_index:(remove_index+1)]
    return data

########NEW FILE########
__FILENAME__ = views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library;  if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA


from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponseRedirect
from django.core.urlresolvers import reverse
from django.core.paginator import Paginator, InvalidPage
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.utils.datastructures import SortedDict
from django.utils.translation import ugettext_lazy as _
from django.http import HttpResponse
from django.core import serializers

from haystack.query import SearchQuerySet

from util import reduce_burndown_data

import json
import datetime
import math
import logging
import time

from projects.calculation import onDemandCalculateVelocity
from projects.limits import org_project_limit, personal_project_limit

logger = logging.getLogger(__name__)


from xlrd import open_workbook

from django.conf import settings

if "notification" in settings.INSTALLED_APPS:
    from notification import models as notification
else:
    notification = None

from projects.access import *
from projects.models import Project, ProjectMember, Iteration, Story
from projects.forms import *
from projects.import_export import exportProject
from organizations.models import Organization

import datetime

from story_views import handleAddStory

TOPIC_COUNT_SQL = """
SELECT COUNT(*)
FROM topics_topic
WHERE
    topics_topic.object_id = projects_project.id AND
    topics_topic.content_type_id = %s
"""
MEMBER_COUNT_SQL = """
SELECT COUNT(*)
FROM projects_projectmember
WHERE projects_projectmember.project_id = projects_project.id
"""


# The homepage of the site...
# For logged in users:
#   If they have a single organization, it redirects them there.
#   If they have multiple organizations, it prompts to select one
#   If they have no organization, it suggests they create one or ask to join one.
def home( request ):
    my_projects = [];
    member_projects = [];
    organizations = [];
    next_page = False

    if request.user.is_authenticated():
        organizations = Organization.getOrganizationsForUser(request.user)
        if len(organizations) == 1:
            return HttpResponseRedirect(organizations[0].get_absolute_url())

        return render_to_response("homepage.html", {
           "my_organizations": organizations,
           "now": datetime.datetime.now()
          }, context_instance=RequestContext(request))
    else:
        return render_to_response("unauthenticated_homepage.html", context_instance=RequestContext(request))

def usage(request):
    if request.user.is_anonymous():
        organizations = None
    else:
        organizations = Organization.getOrganizationsForUser( request.user )

    return render_to_response("usage_restrictions.html", {"organizations":organizations}, context_instance=RequestContext(request))

def remove_user( request, group_slug ):
    project = get_object_or_404( Project, slug=group_slug )
    admin_access_or_403(project, request.user )
    user_id = int(request.POST.get("user_id"))
    # logger.debug("Removing user %d from project %s" % (user_id, group_slug))
    membership = project.members.get( user__id=user_id )
    membership.delete()
    return HttpResponse("ok")

@login_required
def search_project(request, group_slug):
    project = get_object_or_404( Project, slug=group_slug )
    search_terms = request.GET.get("q","")
    read_access_or_403(project, request.user )
    if search_terms == "":
        search_results = SearchQuerySet().filter(project_id=project.id).models(Story).order_by("rank").load_all()
    else:
        search_results = SearchQuerySet().filter(project_id=project.id).filter(content=search_terms).models(Story).order_by("rank").load_all()
    organization = _organizationOrNone(project)
    return render_to_response("projects/search_results.html", 
                              {
                                "project":project,
                                "search_terms":search_terms,
                                "organization":organization,
                                "search_results":search_results
                              },
                              context_instance=RequestContext(request))

    

@login_required
def epics(request, group_slug):
    project = get_object_or_404( Project, slug=group_slug )
    archived = request.GET.get("show_archived","false") == "true"
    read_access_or_403(project, request.user )
    epics = project.epics.filter(parent__isnull=True)
    if not archived:
        epics = epics.filter(archived=False)
    first_time = len(epics) == 0
    epics_list = _flattenEpics(epics)
    organization = _organizationOrNone(project)
    add_story_form = handleAddStory(request, project)
    add_epic_form = EpicForm(project)

    return render_to_response("projects/epics.html", 
                              {
                                "project":project,
                                "epic_list":epics_list,
                                "organization":organization,
                                "add_story_form":add_story_form,
                                "add_epic_form":add_epic_form,
                                "archived":archived,
                                "first_time":first_time
                              },
                              context_instance=RequestContext(request))

def _flattenEpics(epics):
    """You need to flatten out a tree structure before passing it to the Django template engine, because
       templates can't recurse."""

    yield 'in'

    for epic in epics:
      yield epic
      subepics = epic.children.all()
      if len(subepics):
          epic.leaf=False
          for x in _flattenEpics(subepics):
              yield x
      else:
          epic.leaf=True
    yield 'out'

@login_required
def all_iterations(request, group_slug):
    project = get_object_or_404( Project, slug=group_slug )
    read_access_or_403(project, request.user)
    iterations = project.iterations.all()
    return render_to_response("projects/all_iterations.html", {"iterations":iterations, "project":project}, context_instance=RequestContext(request))
    
@login_required
def activate( request, group_slug ):
    project = get_object_or_404( Project, slug=group_slug )
    admin_access_or_403(project, request.user, ignore_active=True)
    project.active = True
    project.save()
    return HttpResponseRedirect( reverse("project_detail", kwargs={"group_slug":project.slug} ) )

@login_required
def iteration_list(request, group_slug):
    project = get_object_or_404( Project, slug=group_slug )
    admin_access_or_403(project, request.user, ignore_active=True)
    return render_to_response("projects/iteration_list_ajax.html", {"project":project}, context_instance=RequestContext(request))


# The project admin page, this is where you can change the title, description, etc. of a project.
@login_required
def project_admin( request, group_slug ):
    project = get_object_or_404( Project, slug=group_slug )

    admin_access_or_403(project, request.user )

    form = ProjectOptionsForm(instance=project)
    adduser_form = AddUserForm(project=project, user=request.user)

    if request.method == 'POST': # If one of the three forms on the page has been submitted...
        if request.POST.get("action") == "updateProject":
            form = ProjectOptionsForm( request.POST, instance=project)
            if form.is_valid(): # All validation rules pass
                form.save()
                request.user.message_set.create(message="Project options Saved.")
                return HttpResponseRedirect(reverse("project_detail",kwargs={'group_slug':project.slug}))
        if request.POST.get("action") == "moveToOrganization":
            organization = get_object_or_404( Organization, id=request.POST.get("organization_id",""))
            if project.organization:
                for team in project.organization.teams.all():
                    if project in team.projects.all():
                        team.projects.remove(project)                
            project.organization = organization
            project.save()
            owners = organization.getOwnersGroup()
            if owners:
                owners.projects.add(project)
            request.user.message_set.create(message="Project moved to organization")
            return HttpResponseRedirect(reverse("organization_detail",kwargs={'organization_slug':organization.slug}))
        # if request.POST.get("action") == "removeFromOrganization":
        #     if request.POST.get("remove") == "on":
        #         for team in project.organization.teams.all():
        #             if project in team.projects.all():
        #                 team.projects.remove(project)
        #         project.organization = None
        #         project.save()
        #         request.user.message_set.create(message="Project removed from organization")
        if request.POST.get("action") == "add":
            write_access_or_403(project, request.user )
            adduser_form = AddUserForm(request.POST, project=project, user=request.user)
            if adduser_form.is_valid():
                adduser_form.save(request.user)
                return HttpResponseRedirect( reverse("project_admin", kwargs={"group_slug":project.slug} ) )
        if request.POST.get("action") == "archiveProject":
            project.active = False
            project.save()
            return HttpResponseRedirect( reverse("project_detail", kwargs={"group_slug":project.slug} ) )






    organizations = Organization.getAdminOrganizationsForUser(request.user)

    return render_to_response("projects/project_admin.html", {
        "project": project,
        "form": form,
        "adduser_form": adduser_form,
        "organizations": organizations
      }, context_instance=RequestContext(request))


# Returns a JSON feed for a given project/iteration that can be transformed with some javascript
# into a burn up chart.
@login_required
def iteration_burndown(request, group_slug, iteration_id):
    project = get_object_or_404( Project, slug=group_slug )
    read_access_or_403(project, request.user )
    iteration = get_object_or_404( Iteration, id=iteration_id )
    if iteration.start_date:
        start_date_timestamp = int((time.mktime(iteration.start_date.timetuple()) - time.timezone)*1000)
    if iteration.end_date:
        end_date_timestamp = int((time.mktime(iteration.end_date.timetuple()) - time.timezone)*1000)
    has_startdate_data = False
    has_enddate_data = False
    highest_total_points = 0
    last_total_points = 0

    total_points = [];
    claimed_points = [];

    for log in iteration.points_log.all():
        
        if iteration.start_date:
            if log.timestamp() != start_date_timestamp and not has_startdate_data :
                total_points.append( [start_date_timestamp, 0])
                claimed_points.append( [start_date_timestamp, 0])
        has_startdate_data = True
        last_total_points = log.points_total
        if iteration.end_date:
            if log.timestamp() == end_date_timestamp :
                has_enddate_data = True
        if highest_total_points < log.points_total:
            highest_total_points = log.points_total
        total_points.append( [log.timestamp(), log.points_total] );
        claimed_points.append( [log.timestamp(), log.points_claimed] );
    if not has_enddate_data and iteration.end_date:
        total_points.append( [end_date_timestamp, highest_total_points])
        
    if iteration.start_date:
        total_points = filter(lambda x:x[0]>=start_date_timestamp, total_points)
        claimed_points = filter(lambda x:x[0]>=start_date_timestamp, claimed_points)
    if iteration.end_date:
        total_points = filter(lambda x:x[0]<=end_date_timestamp, total_points)
        claimed_points = filter(lambda x:x[0]<=end_date_timestamp, claimed_points)

    total_stats = { "label":"Total Points", "data":reduce_burndown_data(total_points)}
    claimed_stats = { "label":"Claimed Points", "data":reduce_burndown_data(claimed_points)}

    json_serializer = serializers.get_serializer("json")()
    result = json.dumps([total_stats,claimed_stats])
    return HttpResponse(result) #, mimetype='application/json'


# Returns a JSON feed for a given project that can be transformed with some javascript
# into a burn up chart.
@login_required
def project_burndown(request, group_slug):
    project = get_object_or_404( Project, slug=group_slug )
    read_access_or_403(project, request.user )
    total_points = [];
    claimed_points = [];

    for log in project.points_log.all():
        total_points.append( [log.timestamp(), log.points_total] );
        claimed_points.append( [log.timestamp(), log.points_claimed] );

    total_stats = { "label":"Total Points", "data":reduce_burndown_data(total_points)}
    claimed_stats = { "label":"Claimed Points", "data":reduce_burndown_data(claimed_points)}


    json_serializer = serializers.get_serializer("json")()
    result = json.dumps([ total_stats , claimed_stats ])
    return HttpResponse(result) #, mimetype='application/json'




# The project history page, which lets you see a burn up chart for each past iteration.
@login_required
def project_history( request, group_slug ):
    project = get_object_or_404( Project, slug=group_slug )
    read_access_or_403(project, request.user )

    return render_to_response("projects/project_history.html", {
        "project": project,
      }, context_instance=RequestContext(request))

# Form and handler for creating a new project.
@login_required
def create(request, form_class=ProjectForm, template_name="projects/create.html"):
    project_form = form_class(request.POST or None)
    admin_organizations = Organization.getOrganizationsForUser( request.user ) # The user can create projects in organizations the user is an admin in.

    if project_form.is_valid():
        project = project_form.save(commit=False)
        project.creator = request.user
        org_id = request.POST.get("organization","none")
        organization = None
        if org_id != "none":
            organization = Organization.objects.filter( id=org_id )[0]

        creationAllowed = True

        if organization:
            creationAllowed = org_project_limit.increaseAllowed(organization=organization)
        else:
            creationAllowed = personal_project_limit.increaseAllowed(user=request.user)

        if creationAllowed:

            project.save()

            if organization != None:
                if organization in admin_organizations: # make sure the specified organization is in the list of admin orgs, if not silently ignore it.
                    addProjectToOrganization(project, organization)

            # We better make the user a member of their own project.
            project_member = ProjectMember(project=project, user=request.user)
            project.members.add(project_member)
            project_member.save()

            # And lets make the default backlog iteration with no start/end dates.
            default_iteration = Iteration( name='Backlog', detail='', default_iteration=True, project=project)
            project.iterations.add(default_iteration)
            default_iteration.save()

            request.user.message_set.create(message="Project Created")
            # Finished successfully creating a project, send the user to that page.
            return HttpResponseRedirect(project.get_absolute_url())
        else:
            return HttpResponseRedirect( reverse("usage") )

    # If they got here from the organziation page, there will be an org get-param set stating what organization it was from.
    # we need that here so it's pre-selected in the form.
    organization = None
    if request.GET.get("org","") != "":
        organization = Organization.objects.filter(id=request.GET.get("org",""))[0]

    return render_to_response(template_name, {
        "project_form": project_form,
        "admin_organizations":admin_organizations,
        "organization":organization
    }, context_instance=RequestContext(request))


def addProjectToOrganization( project, organization):
    logger.info("Adding project %s to organization %s" % (project.slug, organization.slug))
    project.organization = organization
    project.save()
    admin_teams = organization.teams.filter(access_type="staff")
    admin_team = admin_teams[0]
    if not admin_team:
    # This really shouldn't happen, there's no way to delete that default admin team.
        logger.error('Organization %s has no admin team' % organization.slug )
        return
    admin_team.projects.add( project )
    admin_team.save()



# A debug view to generate test data.
@login_required
def test_data(request, group_slug, count):
    project = get_object_or_404(Project, slug=group_slug)
    admin_access_or_403(project, request.user )
    count = int(count)
    story_count = project.stories.all().count()
    for i in range(count) :
        story = Story( rank=i + story_count,
                       summary="Test story #" + str(story_count+i),
                       local_id=i + story_count,
                       detail="Test story detail data",
                       creator=request.user,
                       points=5,
                       iteration=project.get_default_iteration(),
                       project=project);
        story.save();
    return HttpResponse("OK")

# Got rid of this view, just show your_projects now
@login_required
def projects(request, template_name="projects/projects.html"):
    return your_projects(request, template_name)

# A debug/tech-support view that re-numbers all of the stories for a project.
@login_required
def fix_local_id(request, group_slug=None):
    project = get_object_or_404(Project, slug=group_slug)
    admin_access_or_403(project, request.user )
    id = 1
    for story in project.stories.all().order_by("local_id"):
        story.local_id = id
        story.save()
        id = id + 1
    return HttpResponse("OK")

@login_required
def delete(request, group_slug=None, redirect_url=None):
    project = get_object_or_404(Project, slug=group_slug)
    admin_access_or_403(project, request.user )
    if not redirect_url:
        redirect_url = reverse('home')


    project.delete()
    request.user.message_set.create(message=_("Project %(project_name)s deleted.") % {"project_name": project.name})

    return HttpResponseRedirect(redirect_url)


@login_required
def your_projects(request, template_name="projects/your_projects.html"):
    projects = Project.objects.filter(member_users=request.user).order_by("name")
    content_type = ContentType.objects.get_for_model(Project)

    projects = projects.extra(select=SortedDict([
        ('member_count', MEMBER_COUNT_SQL),
        ('topic_count', TOPIC_COUNT_SQL),
        ]), select_params=(content_type.id,))

    return render_to_response(template_name, {
        "projects": projects,
    }, context_instance=RequestContext(request))




@login_required
def project(request, group_slug=None, form_class=ProjectUpdateForm, adduser_form_class=AddUserForm,
        template_name="projects/project.html"):
    project = get_object_or_404(Project, slug=group_slug)
    read_access_or_403(project, request.user )
    if not request.user.is_authenticated():
        is_member = False
    else:
        is_member = project.user_is_member(request.user)

    action = request.POST.get("action")
    if action == "update":
        write_access_or_403(project, request.user )
        project_form = form_class(request.POST, instance=project)
        if project_form.is_valid():
            project = project_form.save()
    else:
        project_form = form_class(instance=project)

    add_story_form = handleAddStory(request, project)


    return render_to_response(template_name, {
        "project_form": project_form,
        "add_story_form": add_story_form,
        "project": project,
        "group": project, # @@@ this should be the only context var for the project
        "is_member": is_member,
        "current_view":"project_page",
    }, context_instance=RequestContext(request))

# @login_required
# def burnup_chart(request, group_slug):
#     project = get_object_or_404(Project, slug=group_slug)
#     read_access_or_403(project, request.user )
#     # CairoPlot
#     # data = CairoPlot.dot_line_plot('dotline1_dots', [(1,2),(2,3),(3,3)], 400, 300, axis = True, grid = True, dots = True)
#     response = HttpResponse(data, mimetype="image/png")
#     return response




# Drives the prediction page
# example: /projects/project/sch-r180-dashboards/project_prediction
@login_required
def project_prediction(request, group_slug):
    project = get_object_or_404(Project, slug=group_slug)
    read_access_or_403(project, request.user )
    stories = project.get_default_iteration().stories.exclude(status=Story.STATUS_DONE).order_by("rank")

    # there is a form on the page with velocity, iteration length, and carry over.
    # IF they're filled out, we should override the default values.
    requested_velocity = request.GET.get("velocity","x");
    if requested_velocity.isdigit():
        velocity = int( requested_velocity )
    else:
        velocity = project.velocity
    if velocity == 0 or velocity == None:
        # A default velocity just in case this project doesn't have one yet.
        velocity = 25
    requested_length = request.GET.get("iteration_length","x")
    if requested_length.isdigit():
        iteration_length = int( requested_length )
    else:
        iteration_length = 14
    carry_over = (request.GET.get("carry_over","x") == "on")


    points_left = velocity
    temp_stories = []
    predictions = []
    iteration_number = project.iterations.count()
    start_date = datetime.date.today()
    current_iterations = project.get_current_iterations()
    unsized_stories = []
    total_points = 0

    # Find when the last scheduled iteration ends.
    for iteration in current_iterations:
        if iteration.end_date > start_date:
            start_date = iteration.end_date



    # loop through the stories, and try to put them into a predicted iteration.
    for story in stories:
        if story.points_value() > points_left:
            record_prediction(predictions,temp_stories,iteration_number,start_date,iteration_length,points_left)
            iteration_number += 1
            if carry_over:
                points_left += velocity
            else:
                points_left = velocity
            temp_stories = []
            start_date += datetime.timedelta(days=iteration_length)
        if not story.points.isdigit():
            unsized_stories.append(story)
        points_left -= int(story.points_value())
        total_points += story.points_value()
        temp_stories.append(story)

    record_prediction(predictions,temp_stories,iteration_number,start_date,iteration_length,points_left)

    return render_to_response("projects/project_prediction.html", {
        "project": project,
        "predictions": predictions,
        "velocity":velocity,
        "total_points":total_points,
        "ideal_iterations":int(math.ceil(total_points/velocity)),
        "predicted_iterations":len(predictions),
        "unsized_stories":unsized_stories,
        "carry_over": carry_over,
        "iteration_length":iteration_length
    }, context_instance=RequestContext(request))


def record_prediction(predictions, stories,iteration_number,start_date,iteration_length,points_left):
    points = 0
    for story in stories:
        points += story.points_value()
    predictions.append( { "carried":points_left, "stories":stories , "points":points, "num":iteration_number, "start":start_date, "end":(start_date +  datetime.timedelta(days=(iteration_length-1))) } )

@login_required
def export_project(request, group_slug):
    project = get_object_or_404(Project, slug=group_slug)
    read_access_or_403(project, request.user )
    #return exportProject( project )
    if request.method == "POST":
        form = ExportProjectForm( request.POST )
        if form.is_valid():
            return exportProject( project, form.cleaned_data["file_name"])
        else:
            return HttpResponseRedirect(reverse("project_detail",kwargs={'group_slug':project.slug}))
    else:
        form = ExportProjectForm(initial={'file_name':u'project'})
    return render_to_response("projects/project_export_options.html", { "project":project, "form":form }, context_instance=RequestContext(request))

# @login_required
# def add_category(request, group_slug):
#     project = get_object_or_404(Project, slug=group_slug)
#     read_access_or_403(project, request.user )    
#     name = request.POST.get("category_name")
#     name = name.replace(",","").strip()
#     if not name in project.getCategoryList():
#         project.categories = "%s, %s" % (project.categories, name)
#         project.save()
#     return HttpResponse(name)

def _organizationOrNone(project):
    try:
        organization = project.organization
    except Organization.DoesNotExist:
        organization = None
    return organization

########NEW FILE########
__FILENAME__ = biffh
# -*- coding: cp1252 -*-

##
# Support module for the xlrd package.
#
# <p>Portions copyright  2005-2008 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under a BSD-style licence.</p>
##

# 2008-02-10 SJM BIFF2 BLANK record
# 2008-02-08 SJM Preparation for Excel 2.0 support
# 2008-02-02 SJM Added suffixes (_B2, _B2_ONLY, etc) on record names for biff_dump & biff_count
# 2007-12-04 SJM Added support for Excel 2.x (BIFF2) files.
# 2007-09-08 SJM Avoid crash when zero-length Unicode string missing options byte.
# 2007-04-22 SJM Remove experimental "trimming" facility.

DEBUG = 0

from struct import unpack
import sys
from timemachine import *

class XLRDError(Exception):
    pass

##
# Parent of almost all other classes in the package. Defines a common "dump" method
# for debugging.

class BaseObject(object):

    _repr_these = []

    ##
    # @param f open file object, to which the dump is written
    # @param header text to write before the dump
    # @param footer text to write after the dump
    # @param indent number of leading spaces (for recursive calls)

    def dump(self, f=None, header=None, footer=None, indent=0):
        if f is None:
            f = sys.stderr
        alist = self.__dict__.items()
        alist.sort()
        pad = " " * indent
        if header is not None: print >> f, header
        list_type = type([])
        dict_type = type({})
        for attr, value in alist:
            if getattr(value, 'dump', None) and attr != 'book':
                value.dump(f,
                    header="%s%s (%s object):" % (pad, attr, value.__class__.__name__),
                    indent=indent+4)
            elif attr not in self._repr_these and (
                isinstance(value, list_type) or isinstance(value, dict_type)
                ):
                print >> f, "%s%s: %s, len = %d" % (pad, attr, type(value), len(value))
            else:
                print >> f, "%s%s: %r" % (pad, attr, value)
        if footer is not None: print >> f, footer

FUN, FDT, FNU, FGE, FTX = range(5) # unknown, date, number, general, text
DATEFORMAT = FDT
NUMBERFORMAT = FNU

(
    XL_CELL_EMPTY,
    XL_CELL_TEXT,
    XL_CELL_NUMBER,
    XL_CELL_DATE,
    XL_CELL_BOOLEAN,
    XL_CELL_ERROR,
    XL_CELL_BLANK, # for use in debugging, gathering stats, etc
) = range(7)

biff_text_from_num = {
    0:  "(not BIFF)",
    20: "2.0",
    21: "2.1",
    30: "3",
    40: "4S",
    45: "4W",
    50: "5",
    70: "7",
    80: "8",
    85: "8X",
    }

##
# <p>This dictionary can be used to produce a text version of the internal codes
# that Excel uses for error cells. Here are its contents:
# <pre>
# 0x00: '#NULL!',  # Intersection of two cell ranges is empty
# 0x07: '#DIV/0!', # Division by zero
# 0x0F: '#VALUE!', # Wrong type of operand
# 0x17: '#REF!',   # Illegal or deleted cell reference
# 0x1D: '#NAME?',  # Wrong function or range name
# 0x24: '#NUM!',   # Value range overflow
# 0x2A: '#N/A!',   # Argument or function not available
# </pre></p>

error_text_from_code = {
    0x00: '#NULL!',  # Intersection of two cell ranges is empty
    0x07: '#DIV/0!', # Division by zero
    0x0F: '#VALUE!', # Wrong type of operand
    0x17: '#REF!',   # Illegal or deleted cell reference
    0x1D: '#NAME?',  # Wrong function or range name
    0x24: '#NUM!',   # Value range overflow
    0x2A: '#N/A!',   # Argument or function not available
}

BIFF_FIRST_UNICODE = 80

XL_WORKBOOK_GLOBALS = WBKBLOBAL = 0x5
XL_WORKBOOK_GLOBALS_4W = 0x100
XL_WORKSHEET = WRKSHEET = 0x10

XL_BOUNDSHEET_WORKSHEET = 0x00
XL_BOUNDSHEET_CHART     = 0x02
XL_BOUNDSHEET_VB_MODULE = 0x06

# XL_RK2 = 0x7e
XL_ARRAY  = 0x0221
XL_ARRAY2 = 0x0021
XL_BLANK = 0x0201
XL_BLANK_B2 = 0x01
XL_BOF = 0x809
XL_BOOLERR = 0x205
XL_BOOLERR_B2 = 0x5
XL_BOUNDSHEET = 0x85
XL_BUILTINFMTCOUNT = 0x56
XL_CF = 0x01B1
XL_CODEPAGE = 0x42
XL_COLINFO = 0x7D
XL_COLUMNDEFAULT = 0x20 # BIFF2 only
XL_COLWIDTH = 0x24 # BIFF2 only
XL_CONDFMT = 0x01B0
XL_CONTINUE = 0x3c
XL_COUNTRY = 0x8C
XL_DATEMODE = 0x22
XL_DEFAULTROWHEIGHT = 0x0225
XL_DEFCOLWIDTH = 0x55
XL_DIMENSION = 0x200
XL_DIMENSION2 = 0x0
XL_EFONT = 0x45
XL_EOF = 0x0a
XL_EXTERNNAME = 0x23
XL_EXTERNSHEET = 0x17
XL_EXTSST = 0xff
XL_FEAT11 = 0x872
XL_FILEPASS = 0x2f
XL_FONT = 0x31
XL_FONT_B3B4 = 0x231
XL_FORMAT = 0x41e
XL_FORMAT2 = 0x1E # BIFF2, BIFF3
XL_FORMULA = 0x6
XL_FORMULA3 = 0x206
XL_FORMULA4 = 0x406
XL_GCW = 0xab
XL_INDEX = 0x20b
XL_INTEGER = 0x2 # BIFF2 only
XL_IXFE = 0x44 # BIFF2 only
XL_LABEL = 0x204
XL_LABEL_B2 = 0x04
XL_LABELRANGES = 0x15f
XL_LABELSST = 0xfd
XL_MERGEDCELLS = 0xE5
XL_MSO_DRAWING = 0x00EC
XL_MSO_DRAWING_GROUP = 0x00EB
XL_MSO_DRAWING_SELECTION = 0x00ED
XL_MULRK = 0xbd
XL_MULBLANK = 0xbe
XL_NAME = 0x18
XL_NOTE = 0x1c
XL_NUMBER = 0x203
XL_NUMBER_B2 = 0x3
XL_OBJ = 0x5D
XL_PALETTE = 0x92
XL_RK = 0x27e
XL_ROW = 0x208
XL_ROW_B2 = 0x08
XL_RSTRING = 0xd6
XL_SHEETHDR = 0x8F # BIFF4W only
XL_SHEETSOFFSET = 0x8E # BIFF4W only
XL_SHRFMLA = 0x04bc
XL_SST = 0xfc
XL_STANDARDWIDTH = 0x99
XL_STRING = 0x207
XL_STRING_B2 = 0x7
XL_STYLE = 0x293
XL_SUPBOOK = 0x1AE
XL_TABLEOP = 0x236
XL_TABLEOP2 = 0x37
XL_TABLEOP_B2 = 0x36
XL_TXO = 0x1b6
XL_UNCALCED = 0x5e
XL_UNKNOWN = 0xffff
XL_WINDOW2 = 0x023E
XL_WRITEACCESS = 0x5C
XL_XF = 0xe0
XL_XF2 = 0x0043 # BIFF2 version of XF record
XL_XF3 = 0x0243 # BIFF3 version of XF record
XL_XF4 = 0x0443 # BIFF4 version of XF record

boflen = {0x0809: 8, 0x0409: 6, 0x0209: 6, 0x0009: 4}
bofcodes = (0x0809, 0x0409, 0x0209, 0x0009)

XL_FORMULA_OPCODES = (0x0006, 0x0406, 0x0206)

_cell_opcode_list = [
    XL_BOOLERR,
    XL_FORMULA,
    XL_FORMULA3,
    XL_FORMULA4,
    XL_LABEL,
    XL_LABELSST,
    XL_MULRK,
    XL_NUMBER,
    XL_RK,
    XL_RSTRING,
    ]
_cell_opcode_dict = {}
for _cell_opcode in _cell_opcode_list:
    _cell_opcode_dict[_cell_opcode] = 1
is_cell_opcode = _cell_opcode_dict.has_key

# def fprintf(f, fmt, *vargs): f.write(fmt % vargs)

def fprintf(f, fmt, *vargs):
    if fmt.endswith('\n'):
        print >> f, fmt[:-1] % vargs
    else:
        print >> f, fmt % vargs,

def upkbits(tgt_obj, src, manifest, local_setattr=setattr):
    for n, mask, attr in manifest:
        local_setattr(tgt_obj, attr, (src & mask) >> n)

def upkbitsL(tgt_obj, src, manifest, local_setattr=setattr, local_int=int):
    for n, mask, attr in manifest:
        local_setattr(tgt_obj, attr, local_int((src & mask) >> n))

def unpack_string(data, pos, encoding, lenlen=1):
    nchars = unpack('<' + 'BH'[lenlen-1], data[pos:pos+lenlen])[0]
    pos += lenlen
    return unicode(data[pos:pos+nchars], encoding)

def unpack_string_update_pos(data, pos, encoding, lenlen=1, known_len=None):
    if known_len is not None:
        # On a NAME record, the length byte is detached from the front of the string.
        nchars = known_len
    else:
        nchars = unpack('<' + 'BH'[lenlen-1], data[pos:pos+lenlen])[0]
        pos += lenlen
    newpos = pos + nchars
    return (unicode(data[pos:newpos], encoding), newpos)

def unpack_unicode(data, pos, lenlen=2):
    "Return unicode_strg"
    nchars = unpack('<' + 'BH'[lenlen-1], data[pos:pos+lenlen])[0]
    if not nchars:
        # Ambiguous whether 0-length string should have an "options" byte.
        # Avoid crash if missing.
        return u""
    pos += lenlen
    options = ord(data[pos])
    pos += 1
    # phonetic = options & 0x04
    # richtext = options & 0x08
    if options & 0x08:
        # rt = unpack('<H', data[pos:pos+2])[0] # unused
        pos += 2
    if options & 0x04:
        # sz = unpack('<i', data[pos:pos+4])[0] # unused
        pos += 4
    if options & 0x01:
        # Uncompressed UTF-16-LE
        rawstrg = data[pos:pos+2*nchars]
        # if DEBUG: print "nchars=%d pos=%d rawstrg=%r" % (nchars, pos, rawstrg)
        strg = unicode(rawstrg, 'utf_16_le')
        # pos += 2*nchars
    else:
        # Note: this is COMPRESSED (not ASCII!) encoding!!!
        # Merely returning the raw bytes would work OK 99.99% of the time
        # if the local codepage was cp1252 -- however this would rapidly go pear-shaped
        # for other codepages so we grit our Anglocentric teeth and return Unicode :-)

        strg = unicode(data[pos:pos+nchars], "latin_1")
        # pos += nchars
    # if richtext:
    #     pos += 4 * rt
    # if phonetic:
    #     pos += sz
    # return (strg, pos)
    return strg

def unpack_unicode_update_pos(data, pos, lenlen=2, known_len=None):
    "Return (unicode_strg, updated value of pos)"
    if known_len is not None:
        # On a NAME record, the length byte is detached from the front of the string.
        nchars = known_len
    else:
        nchars = unpack('<' + 'BH'[lenlen-1], data[pos:pos+lenlen])[0]
        pos += lenlen
    if not nchars and not data[pos:]:
        # Zero-length string with no options byte
        return (u"", pos)
    options = ord(data[pos])
    pos += 1
    phonetic = options & 0x04
    richtext = options & 0x08
    if richtext:
        rt = unpack('<H', data[pos:pos+2])[0]
        pos += 2
    if phonetic:
        sz = unpack('<i', data[pos:pos+4])[0]
        pos += 4
    if options & 0x01:
        # Uncompressed UTF-16-LE
        strg = unicode(data[pos:pos+2*nchars], 'utf_16_le')
        pos += 2*nchars
    else:
        # Note: this is COMPRESSED (not ASCII!) encoding!!!
        strg = unicode(data[pos:pos+nchars], "latin_1")
        pos += nchars
    if richtext:
        pos += 4 * rt
    if phonetic:
        pos += sz
    return (strg, pos)

def unpack_cell_range_address_list_update_pos(
    output_list, data, pos, biff_version, addr_size=6):
    # output_list is updated in situ
    if biff_version < 80:
        assert addr_size == 6
    else:
        assert addr_size in (6, 8)
    n, = unpack("<H", data[pos:pos+2])
    pos += 2
    if n:
        if addr_size == 6:
            fmt = "<HHBB"
        else:
            fmt = "<HHHH"
        for _unused in xrange(n):
            ra, rb, ca, cb = unpack(fmt, data[pos:pos+addr_size])
            output_list.append((ra, rb+1, ca, cb+1))
            pos += addr_size
    return pos

_brecstrg = """\
0000 DIMENSIONS_B2
0001 BLANK_B2
0002 INTEGER_B2_ONLY
0003 NUMBER_B2
0004 LABEL_B2
0005 BOOLERR_B2
0006 FORMULA
0007 STRING_B2
0008 ROW_B2
0009 BOF_B2
000A EOF
000B INDEX_B2_ONLY
000C CALCCOUNT
000D CALCMODE
000E PRECISION
000F REFMODE
0010 DELTA
0011 ITERATION
0012 PROTECT
0013 PASSWORD
0014 HEADER
0015 FOOTER
0016 EXTERNCOUNT
0017 EXTERNSHEET
0018 NAME_B2,5+
0019 WINDOWPROTECT
001A VERTICALPAGEBREAKS
001B HORIZONTALPAGEBREAKS
001C NOTE
001D SELECTION
001E FORMAT_B2-3
001F BUILTINFMTCOUNT_B2
0020 COLUMNDEFAULT_B2_ONLY
0021 ARRAY_B2_ONLY
0022 DATEMODE
0023 EXTERNNAME
0024 COLWIDTH_B2_ONLY
0025 DEFAULTROWHEIGHT_B2_ONLY
0026 LEFTMARGIN
0027 RIGHTMARGIN
0028 TOPMARGIN
0029 BOTTOMMARGIN
002A PRINTHEADERS
002B PRINTGRIDLINES
002F FILEPASS
0031 FONT
0032 FONT2_B2_ONLY
0036 TABLEOP_B2
0037 TABLEOP2_B2
003C CONTINUE
003D WINDOW1
003E WINDOW2_B2
0040 BACKUP
0041 PANE
0042 CODEPAGE
0043 XF_B2
0044 IXFE_B2_ONLY
0045 EFONT_B2_ONLY
004D PLS
0051 DCONREF
0055 DEFCOLWIDTH
0056 BUILTINFMTCOUNT_B3-4
0059 XCT
005A CRN
005B FILESHARING
005C WRITEACCESS
005D OBJECT
005E UNCALCED
005F SAVERECALC
0063 OBJECTPROTECT
007D COLINFO
007E RK2_mythical_?
0080 GUTS
0081 WSBOOL
0082 GRIDSET
0083 HCENTER
0084 VCENTER
0085 BOUNDSHEET
0086 WRITEPROT
008C COUNTRY
008D HIDEOBJ
008E SHEETSOFFSET
008F SHEETHDR
0090 SORT
0092 PALETTE
0099 STANDARDWIDTH
009B FILTERMODE
009C FNGROUPCOUNT
009D AUTOFILTERINFO
009E AUTOFILTER
00A0 SCL
00A1 SETUP
00AB GCW
00BD MULRK
00BE MULBLANK
00C1 MMS
00D6 RSTRING
00D7 DBCELL
00DA BOOKBOOL
00DD SCENPROTECT
00E0 XF
00E1 INTERFACEHDR
00E2 INTERFACEEND
00E5 MERGEDCELLS
00E9 BITMAP
00EB MSO_DRAWING_GROUP
00EC MSO_DRAWING
00ED MSO_DRAWING_SELECTION
00EF PHONETIC
00FC SST
00FD LABELSST
00FF EXTSST
013D TABID
015F LABELRANGES
0160 USESELFS
0161 DSF
01AE SUPBOOK
01AF PROTECTIONREV4
01B0 CONDFMT
01B1 CF
01B2 DVAL
01B6 TXO
01B7 REFRESHALL
01B8 HLINK
01BC PASSWORDREV4
01BE DV
01C0 XL9FILE
01C1 RECALCID
0200 DIMENSIONS
0201 BLANK
0203 NUMBER
0204 LABEL
0205 BOOLERR
0206 FORMULA_B3
0207 STRING
0208 ROW
0209 BOF
020B INDEX_B3+
0218 NAME
0221 ARRAY
0223 EXTERNNAME_B3-4
0225 DEFAULTROWHEIGHT
0231 FONT_B3B4
0236 TABLEOP
023E WINDOW2
0243 XF_B3
027E RK
0293 STYLE
0406 FORMULA_B4
0409 BOF
041E FORMAT
0443 XF_B4
04BC SHRFMLA
0800 QUICKTIP
0809 BOF
0862 SHEETLAYOUT
0867 SHEETPROTECTION
0868 RANGEPROTECTION
"""

biff_rec_name_dict = {}
for _buff in _brecstrg.splitlines():
    _numh, _name = _buff.split()
    biff_rec_name_dict[int(_numh, 16)] = _name
del _buff, _name, _brecstrg

def hex_char_dump(strg, ofs, dlen, base=0, fout=sys.stdout, unnumbered=False):
    endpos = min(ofs + dlen, len(strg))
    pos = ofs
    numbered = not unnumbered
    num_prefix = ''
    while pos < endpos:
        endsub = min(pos + 16, endpos)
        substrg = strg[pos:endsub]
        lensub = endsub - pos
        if lensub <= 0 or lensub != len(substrg):
            fprintf(
                sys.stdout,
                '??? hex_char_dump: ofs=%d dlen=%d base=%d -> endpos=%d pos=%d endsub=%d substrg=%r\n',
                ofs, dlen, base, endpos, pos, endsub, substrg)
            break
        hexd = ''.join(["%02x " % ord(c) for c in substrg])
        chard = ''
        for c in substrg:
            if c == '\0':
                c = '~'
            elif not (' ' <= c <= '~'):
                c = '?'
            chard += c
        if numbered:
            num_prefix = "%5d: " %  (base+pos-ofs)
        fprintf(fout, "%s     %-48s %s\n", num_prefix, hexd, chard)
        pos = endsub

def biff_dump(mem, stream_offset, stream_len, base=0, fout=sys.stdout, unnumbered=False):
    pos = stream_offset
    stream_end = stream_offset + stream_len
    adj = base - stream_offset
    dummies = 0
    numbered = not unnumbered
    num_prefix = ''
    while stream_end - pos >= 4:
        rc, length = unpack('<HH', mem[pos:pos+4])
        if rc == 0 and length == 0:
            if mem[pos:] == '\0' * (stream_end - pos):
                dummies = stream_end - pos
                savpos = pos
                pos = stream_end
                break
            if dummies:
                dummies += 4
            else:
                savpos = pos
                dummies = 4
            pos += 4
        else:
            if dummies:
                if numbered:
                    num_prefix =  "%5d: " % (adj + savpos)
                fprintf(fout, "%s---- %d zero bytes skipped ----\n", num_prefix, dummies)
                dummies = 0
            recname = biff_rec_name_dict.get(rc, '<UNKNOWN>')
            if numbered:
                num_prefix = "%5d: " % (adj + pos)
            fprintf(fout, "%s%04x %s len = %04x (%d)\n", num_prefix, rc, recname, length, length)
            pos += 4
            hex_char_dump(mem, pos, length, adj+pos, fout, unnumbered)
            pos += length
    if dummies:
        if numbered:
            num_prefix =  "%5d: " % (adj + savpos)
        fprintf(fout, "%s---- %d zero bytes skipped ----\n", num_prefix, dummies)
    if pos < stream_end:
        if numbered:
            num_prefix = "%5d: " % (adj + pos)
        fprintf(fout, "%s---- Misc bytes at end ----\n", num_prefix)
        hex_char_dump(mem, pos, stream_end-pos, adj + pos, fout, unnumbered)
    elif pos > stream_end:
        fprintf(fout, "Last dumped record has length (%d) that is too large\n", length)

def biff_count_records(mem, stream_offset, stream_len, fout=sys.stdout):
    pos = stream_offset
    stream_end = stream_offset + stream_len
    tally = {}
    while stream_end - pos >= 4:
        rc, length = unpack('<HH', mem[pos:pos+4])
        if rc == 0 and length == 0:
            if mem[pos:] == '\0' * (stream_end - pos):
                break
            recname = "<Dummy (zero)>"
        else:
            recname = biff_rec_name_dict.get(rc, None)
            if recname is None:
                recname = "Unknown_0x%04X" % rc
        if tally.has_key(recname):
            tally[recname] += 1
        else:
            tally[recname] = 1
        pos += length + 4
    slist = tally.items()
    slist.sort()
    for recname, count in slist:
        print >> fout, "%8d %s" % (count, recname)

encoding_from_codepage = {
    1200 : 'utf_16_le',
    10000: 'mac_roman',
    10006: 'mac_greek', # guess
    10007: 'mac_cyrillic', # guess
    10029: 'mac_latin2', # guess
    10079: 'mac_iceland', # guess
    10081: 'mac_turkish', # guess
    32768: 'mac_roman',
    32769: 'cp1252',
    }
# some more guessing, for Indic scripts
# codepage 57000 range:
# 2 Devanagari [0]
# 3 Bengali [1]
# 4 Tamil [5]
# 5 Telegu [6]
# 6 Assamese [1] c.f. Bengali
# 7 Oriya [4]
# 8 Kannada [7]
# 9 Malayalam [8]
# 10 Gujarati [3]
# 11 Gurmukhi [2]

########NEW FILE########
__FILENAME__ = compdoc
# -*- coding: cp1252 -*-

##
# Implements the minimal functionality required
# to extract a "Workbook" or "Book" stream (as one big string)
# from an OLE2 Compound Document file.
# <p>Copyright  2005-2008 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under a BSD-style licence.</p>
##

# No part of the content of this file was derived from the works of David Giffin.

# 2008-11-04 SJM Avoid assertion error when -1 used instead of -2 for first_SID of empty SCSS [Frank Hoffsuemmer]
# 2007-09-08 SJM Warning message if sector sizes are extremely large.
# 2007-05-07 SJM Meaningful exception instead of IndexError if a SAT (sector allocation table) is corrupted.
# 2007-04-22 SJM Missing "<" in a struct.unpack call => can't open files on bigendian platforms.


import sys
from struct import unpack
from timemachine import *

##
# Magic cookie that should appear in the first 8 bytes of the file.
SIGNATURE = "\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"

EOCSID = -2
FREESID = -1
SATSID = -3
MSATSID = -4

class CompDocError(Exception):
    pass

class DirNode(object):

    def __init__(self, DID, dent, DEBUG=0):
        # dent is the 128-byte directory entry
        self.DID = DID
        # (cbufsize, self.etype, self.colour, self.left_DID, self.right_DID,
        # self.root_DID,
        # self.first_SID,
        # self.tot_size) = \
        #     unpack('<HBBiii16x4x8x8xii4x', dent[64:128])
        (cbufsize, self.etype, self.colour, self.left_DID, self.right_DID,
        self.root_DID) = \
            unpack('<HBBiii', dent[64:80])
        (self.first_SID, self.tot_size) = \
            unpack('<ii', dent[116:124])
        if cbufsize == 0:
            self.name = u''
        else:
            self.name = unicode(dent[0:cbufsize-2], 'utf_16_le') # omit the trailing U+0000
        self.children = [] # filled in later
        self.parent = -1 # indicates orphan; fixed up later
        self.tsinfo = unpack('<IIII', dent[100:116])
        if DEBUG:
            self.dump(DEBUG)

    def dump(self, DEBUG=1):
        print "DID=%d name=%r etype=%d DIDs(left=%d right=%d root=%d parent=%d kids=%r) first_SID=%d tot_size=%d" \
            % (self.DID, self.name, self.etype, self.left_DID,
            self.right_DID, self.root_DID, self.parent, self.children, self.first_SID, self.tot_size)
        if DEBUG == 2:
            # cre_lo, cre_hi, mod_lo, mod_hi = tsinfo
            print "timestamp info", self.tsinfo

def _build_family_tree(dirlist, parent_DID, child_DID):
    if child_DID < 0: return
    _build_family_tree(dirlist, parent_DID, dirlist[child_DID].left_DID)
    dirlist[parent_DID].children.append(child_DID)
    dirlist[child_DID].parent = parent_DID
    _build_family_tree(dirlist, parent_DID, dirlist[child_DID].right_DID)
    if dirlist[child_DID].etype == 1: # storage
        _build_family_tree(dirlist, child_DID, dirlist[child_DID].root_DID)

##
# Compound document handler.
# @param mem The raw contents of the file, as a string, or as an mmap.mmap() object. The
# only operation it needs to support is slicing.

class CompDoc(object):

    def __init__(self, mem, logfile=sys.stdout, DEBUG=0):
        self.logfile = logfile
        if mem[0:8] != SIGNATURE:
            raise CompDocError('Not an OLE2 compound document')
        if mem[28:30] != '\xFE\xFF':
            raise CompDocError('Expected "little-endian" marker, found %r' % mem[28:30])
        revision, version = unpack('<HH', mem[24:28])
        if DEBUG:
            print >> logfile, "\nCompDoc format: version=0x%04x revision=0x%04x" % (version, revision)
        self.mem = mem
        ssz, sssz = unpack('<HH', mem[30:34])
        if ssz > 20: # allows for 2**20 bytes i.e. 1MB
            print >> logfile, \
                "WARNING: sector size (2**%d) is preposterous; assuming 512 and continuing ..." \
                % ssz
            ssz = 9
        if sssz > ssz:
            print >> logfile, \
                "WARNING: short stream sector size (2**%d) is preposterous; assuming 64 and continuing ..." \
                % sssz
            sssz = 6
        self.sec_size = sec_size = 1 << ssz
        self.short_sec_size = 1 << sssz
        (
            SAT_tot_secs, self.dir_first_sec_sid, _unused, self.min_size_std_stream,
            SSAT_first_sec_sid, SSAT_tot_secs,
            MSAT_first_sec_sid, MSAT_tot_secs,
        # ) = unpack('<ii4xiiiii', mem[44:76])
        ) = unpack('<iiiiiiii', mem[44:76])
        mem_data_len = len(mem) - 512
        mem_data_secs, left_over = divmod(mem_data_len, sec_size)
        if left_over:
            #### raise CompDocError("Not a whole number of sectors")
            print >> logfile, \
                "WARNING *** file size (%d) not 512 + multiple of sector size (%d)" \
                % (len(mem), sec_size)
        if DEBUG:
            print >> logfile, 'sec sizes', ssz, sssz, sec_size, self.short_sec_size
            print >> logfile, "mem data: %d bytes == %d sectors" % (mem_data_len, mem_data_secs)
            print >> logfile, "SAT_tot_secs=%d, dir_first_sec_sid=%d, min_size_std_stream=%d" \
                % (SAT_tot_secs, self.dir_first_sec_sid, self.min_size_std_stream,)
            print >> logfile, "SSAT_first_sec_sid=%d, SSAT_tot_secs=%d" % (SSAT_first_sec_sid, SSAT_tot_secs,)
            print >> logfile, "MSAT_first_sec_sid=%d, MSAT_tot_secs=%d" % (MSAT_first_sec_sid, MSAT_tot_secs,)
        nent = int_floor_div(sec_size, 4) # number of SID entries in a sector
        fmt = "<%di" % nent
        trunc_warned = 0
        #
        # === build the MSAT ===
        #
        MSAT = list(unpack('<109i', mem[76:512]))
        sid = MSAT_first_sec_sid
        while sid >= 0:
            if sid >= mem_data_secs:
                raise CompDocError(
                    "MSAT extension: accessing sector %d but only %d in file" % (sid, mem_data_secs)
                    )
            offset = 512 + sec_size * sid
            news = list(unpack(fmt, mem[offset:offset+sec_size]))
            sid = news.pop()
            MSAT.extend(news)
        if DEBUG:
            print >> logfile, "MSAT: len =", len(MSAT)
            print >> logfile, MSAT
        #
        # === build the SAT ===
        #
        self.SAT = []
        for msid in MSAT:
            if msid == FREESID: continue
            if msid >= mem_data_secs:
                if not trunc_warned:
                    print >> logfile, "WARNING *** File is truncated, or OLE2 MSAT is corrupt!!"
                    print >> logfile, \
                        "INFO: Trying to access sector %d but only %d available" \
                        % (msid, mem_data_secs)
                    trunc_warned = 1
                continue
            offset = 512 + sec_size * msid
            news = list(unpack(fmt, mem[offset:offset+sec_size]))
            self.SAT.extend(news)
        if DEBUG:
            print >> logfile, "SAT: len =", len(self.SAT)
            print >> logfile, self.SAT
            # print >> logfile, "SAT ",
            # for i, s in enumerate(self.SAT):
                # print >> logfile, "entry: %4d offset: %6d, next entry: %4d" % (i, 512 + sec_size * i, s)
                # print >> logfile, "%d:%d " % (i, s),
            print

        # === build the directory ===
        #
        dbytes = self._get_stream(
            self.mem, 512, self.SAT, self.sec_size, self.dir_first_sec_sid,
            name="directory")
        dirlist = []
        did = -1
        for pos in xrange(0, len(dbytes), 128):
            did += 1
            dirlist.append(DirNode(did, dbytes[pos:pos+128], 0))
        self.dirlist = dirlist
        _build_family_tree(dirlist, 0, dirlist[0].root_DID) # and stand well back ...
        if DEBUG:
            for d in dirlist:
                d.dump(DEBUG)
        #
        # === get the SSCS ===
        #
        sscs_dir = self.dirlist[0]
        assert sscs_dir.etype == 5 # root entry
        if sscs_dir.first_SID < 0 and sscs_dir.tot_size == 0:
            # Problem reported by Frank Hoffsuemmer: some software was
            # writing -1 instead of -2 (EOCSID) for the first_SID
            # when the SCCS was empty. Not having EOCSID caused assertion
            # failure in _get_stream.
            # Solution: avoid calling _get_stream in any case when the
            # SCSS appears to be empty.
            self.SSCS = ""
        else:
            self.SSCS = self._get_stream(
                self.mem, 512, self.SAT, sec_size, sscs_dir.first_SID,
                sscs_dir.tot_size, name="SSCS")
        # if DEBUG: print >> logfile, "SSCS", repr(self.SSCS)
        #
        # === build the SSAT ===
        #
        self.SSAT = []
        if SSAT_tot_secs > 0 and sscs_dir.tot_size == 0:
            print >> logfile, \
                "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero"
        if sscs_dir.tot_size > 0:
            sid = SSAT_first_sec_sid
            nsecs = SSAT_tot_secs
            while sid >= 0 and nsecs > 0:
                nsecs -= 1
                start_pos = 512 + sid * sec_size
                news = list(unpack(fmt, mem[start_pos:start_pos+sec_size]))
                self.SSAT.extend(news)
                sid = self.SAT[sid]
            # assert SSAT_tot_secs == 0 or sid == EOCSID
            if DEBUG: print >> logfile, "SSAT last sid %d; remaining sectors %d" % (sid, nsecs)
            assert nsecs == 0 and sid == EOCSID
        if DEBUG: print >> logfile, "SSAT", self.SSAT

    def _get_stream(self, mem, base, sat, sec_size, start_sid, size=None, name=''):
        # print >> self.logfile, "_get_stream", base, sec_size, start_sid, size
        sectors = []
        s = start_sid
        if size is None:
            # nothing to check against
            while s >= 0:
                start_pos = base + s * sec_size
                sectors.append(mem[start_pos:start_pos+sec_size])
                try:
                    s = sat[s]
                except IndexError:
                    raise CompDocError(
                        "OLE2 stream %r: sector allocation table invalid entry (%d)" %
                        (name, s)
                        )
            assert s == EOCSID
        else:
            todo = size
            while s >= 0:
                start_pos = base + s * sec_size
                grab = sec_size
                if grab > todo:
                    grab = todo
                todo -= grab
                sectors.append(mem[start_pos:start_pos+grab])
                try:
                    s = sat[s]
                except IndexError:
                    raise CompDocError(
                        "OLE2 stream %r: sector allocation table invalid entry (%d)" %
                        (name, s)
                        )
            assert s == EOCSID
            if todo != 0:
                print >> self.logfile, \
                    "WARNING *** OLE2 stream %r: expected size %d, actual size %d" \
                    % (name, size, size - todo)
        return ''.join(sectors)

    def _dir_search(self, path, storage_DID=0):
        # Return matching DirNode instance, or None
        head = path[0]
        tail = path[1:]
        dl = self.dirlist
        for child in dl[storage_DID].children:
            if dl[child].name.lower() == head.lower():
                et = dl[child].etype
                if et == 2:
                    return dl[child]
                if et == 1:
                    if not tail:
                        raise CompDocError("Requested component is a 'storage'")
                    return self._dir_search(tail, child)
                dl[child].dump(1)
                raise CompDocError("Requested stream is not a 'user stream'")
        return None

    ##
    # Interrogate the compound document's directory; return the stream as a string if found, otherwise
    # return None.
    # @param qname Name of the desired stream e.g. u'Workbook'. Should be in Unicode or convertible thereto.

    def get_named_stream(self, qname):
        d = self._dir_search(qname.split("/"))
        if d is None:
            return None
        if d.tot_size >= self.min_size_std_stream:
            return self._get_stream(
                self.mem, 512, self.SAT, self.sec_size, d.first_SID,
                d.tot_size, name=qname)
        else:
            return self._get_stream(
                self.SSCS, 0, self.SSAT, self.short_sec_size, d.first_SID,
                d.tot_size, name=qname + " (from SSCS)")

    ##
    # Interrogate the compound document's directory.
    # If the named stream is not found, (None, 0, 0) will be returned.
    # If the named stream is found and is contiguous within the original byte sequence ("mem")
    # used when the document was opened,
    # then (mem, offset_to_start_of_stream, length_of_stream) is returned.
    # Otherwise a new string is built from the fragments and (new_string, 0, length_of_stream) is returned.
    # @param qname Name of the desired stream e.g. u'Workbook'. Should be in Unicode or convertible thereto.

    def locate_named_stream(self, qname):
        d = self._dir_search(qname.split("/"))
        if d is None:
            return (None, 0, 0)
        if d.tot_size >= self.min_size_std_stream:
            return self._locate_stream(self.mem, 512, self.SAT, self.sec_size, d.first_SID, d.tot_size)
        else:
            return (
                self._get_stream(
                    self.SSCS, 0, self.SSAT, self.short_sec_size, d.first_SID,
                    d.tot_size, qname + " (from SSCS)"),
                0,
                d.tot_size
                )
        return (None, 0, 0) # not found

    def _locate_stream(self, mem, base, sat, sec_size, start_sid, size):
        # print >> self.logfile, "_locate_stream", base, sec_size, start_sid, size
        s = start_sid
        if s < 0:
            raise CompDocError("_locate_stream: start_sid (%d) is -ve" % start_sid)
        p = -99 # dummy previous SID
        start_pos = -9999
        end_pos = -8888
        slices = []
        while s >= 0:
            if s == p+1:
                # contiguous sectors
                end_pos += sec_size
            else:
                # start new slice
                if p >= 0:
                    # not first time
                    slices.append((start_pos, end_pos))
                start_pos = base + s * sec_size
                end_pos = start_pos + sec_size
            p = s
            s = sat[s]
        assert s == EOCSID
        # print >> self.logfile, len(slices) + 1, "slices"
        if not slices:
            # The stream is contiguous ... just what we like!
            return (mem, start_pos, size)
        slices.append((start_pos, end_pos))
        return (''.join([mem[start_pos:end_pos] for start_pos, end_pos in slices]), 0, size)

# ==========================================================================================

########NEW FILE########
__FILENAME__ = formatting
# -*- coding:cp1252 -*-

##
# Module for formatting information.
#
# <p>Copyright  2005-2008 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>Copyright  2005-2009 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under
# a BSD-style licence.</p>
##

# No part of the content of this file was derived from the works of David Giffin.

# 2009-05-31 SJM Fixed problem with non-zero reserved bits in some STYLE records in Mac Excel files
# 2008-08-03 SJM Ignore PALETTE record when Book.formatting_info is false
# 2008-08-03 SJM Tolerate up to 4 bytes trailing junk on PALETTE record
# 2008-05-10 SJM Do some XF checks only when Book.formatting_info is true
# 2008-02-08 SJM Preparation for Excel 2.0 support
# 2008-02-03 SJM Another tweak to is_date_format_string()
# 2007-12-04 SJM Added support for Excel 2.x (BIFF2) files.
# 2007-10-13 SJM Warning: style XF whose parent XF index != 0xFFF
# 2007-09-08 SJM Work around corrupt STYLE record
# 2007-07-11 SJM Allow for BIFF2/3-style FORMAT record in BIFF4/8 file

DEBUG = 0
import copy, re
from timemachine import *
from biffh import BaseObject, unpack_unicode, unpack_string, \
    upkbits, upkbitsL, fprintf, \
    FUN, FDT, FNU, FGE, FTX, XL_CELL_NUMBER, XL_CELL_DATE, \
    XL_FORMAT, XL_FORMAT2, \
    XLRDError
from struct import unpack

excel_default_palette_b5 = (
    (  0,   0,   0), (255, 255, 255), (255,   0,   0), (  0, 255,   0),
    (  0,   0, 255), (255, 255,   0), (255,   0, 255), (  0, 255, 255),
    (128,   0,   0), (  0, 128,   0), (  0,   0, 128), (128, 128,   0),
    (128,   0, 128), (  0, 128, 128), (192, 192, 192), (128, 128, 128),
    (153, 153, 255), (153,  51, 102), (255, 255, 204), (204, 255, 255),
    (102,   0, 102), (255, 128, 128), (  0, 102, 204), (204, 204, 255),
    (  0,   0, 128), (255,   0, 255), (255, 255,   0), (  0, 255, 255),
    (128,   0, 128), (128,   0,   0), (  0, 128, 128), (  0,   0, 255),
    (  0, 204, 255), (204, 255, 255), (204, 255, 204), (255, 255, 153),
    (153, 204, 255), (255, 153, 204), (204, 153, 255), (227, 227, 227),
    ( 51, 102, 255), ( 51, 204, 204), (153, 204,   0), (255, 204,   0),
    (255, 153,   0), (255, 102,   0), (102, 102, 153), (150, 150, 150),
    (  0,  51, 102), ( 51, 153, 102), (  0,  51,   0), ( 51,  51,   0),
    (153,  51,   0), (153,  51, 102), ( 51,  51, 153), ( 51,  51,  51),
    )

excel_default_palette_b2 = excel_default_palette_b5[:16]

# Following two tables borrowed from Gnumeric 1.4 source.
excel_default_palette_b5_gnumeric_14 = (
    #### dodgy; didn't match Excel results
    (  0,  0,  0), (255,255,255), (255,  0,  0), (  0,255,  0),
    (  0,  0,255), (255,255,  0), (255,  0,255), (  0,255,255),
    (128,  0,  0), (  0,128,  0), (  0,  0,128), (128,128,  0),
    (128,  0,128), (  0,128,128), (192,192,192), (128,128,128),
    (128,128,255), (128, 32, 96), (255,255,192), (160,224,224),
    ( 96,  0,128), (255,128,128), (  0,128,192), (192,192,255),
    (  0,  0,128), (255,  0,255), (255,255,  0), (  0,255,255),
    (128,  0,128), (128,  0,  0), (  0,128,128), (  0,  0,255),
    (  0,204,255), (105,255,255), (204,255,204), (255,255,153),
    (166,202,240), (204,156,204), (204,153,255), (227,227,227),
    ( 51,102,255), ( 51,204,204), ( 51,153, 51), (153,153, 51),
    (153,102, 51), (153,102,102), (102,102,153), (150,150,150),
    ( 51, 51,204), ( 51,102,102), (  0, 51,  0), ( 51, 51,  0),
    (102, 51,  0), (153, 51,102), ( 51, 51,153), ( 66, 66, 66),
    )
excel_default_palette_b8 = ( # (red, green, blue)
    (  0,  0,  0), (255,255,255), (255,  0,  0), (  0,255,  0),
    (  0,  0,255), (255,255,  0), (255,  0,255), (  0,255,255),
    (128,  0,  0), (  0,128,  0), (  0,  0,128), (128,128,  0),
    (128,  0,128), (  0,128,128), (192,192,192), (128,128,128),
    (153,153,255), (153, 51,102), (255,255,204), (204,255,255),
    (102,  0,102), (255,128,128), (  0,102,204), (204,204,255),
    (  0,  0,128), (255,  0,255), (255,255,  0), (  0,255,255),
    (128,  0,128), (128,  0,  0), (  0,128,128), (  0,  0,255),
    (  0,204,255), (204,255,255), (204,255,204), (255,255,153),
    (153,204,255), (255,153,204), (204,153,255), (255,204,153),
    ( 51,102,255), ( 51,204,204), (153,204,  0), (255,204,  0),
    (255,153,  0), (255,102,  0), (102,102,153), (150,150,150),
    (  0, 51,102), ( 51,153,102), (  0, 51,  0), ( 51, 51,  0),
    (153, 51,  0), (153, 51,102), ( 51, 51,153), ( 51, 51, 51),
    )

default_palette = {
    80: excel_default_palette_b8,
    70: excel_default_palette_b5,
    50: excel_default_palette_b5,
    45: excel_default_palette_b2,
    40: excel_default_palette_b2,
    30: excel_default_palette_b2,
    21: excel_default_palette_b2,
    20: excel_default_palette_b2,
    }

"""
00H = Normal
01H = RowLevel_lv (see next field)
02H = ColLevel_lv (see next field)
03H = Comma
04H = Currency
05H = Percent
06H = Comma [0] (BIFF4-BIFF8)
07H = Currency [0] (BIFF4-BIFF8)
08H = Hyperlink (BIFF8)
09H = Followed Hyperlink (BIFF8)
"""
built_in_style_names = [
    "Normal",
    "RowLevel_",
    "ColLevel_",
    "Comma",
    "Currency",
    "Percent",
    "Comma [0]",
    "Currency [0]",
    "Hyperlink",
    "Followed Hyperlink",
    ]

def initialise_colour_map(book):
    book.colour_map = {}
    book.colour_indexes_used = {}
    if not book.formatting_info:
        return
    # Add the 8 invariant colours
    for i in xrange(8):
        book.colour_map[i] = excel_default_palette_b8[i]
    # Add the default palette depending on the version
    dpal = default_palette[book.biff_version]
    ndpal = len(dpal)
    for i in xrange(ndpal):
        book.colour_map[i+8] = dpal[i]
    # Add the specials -- None means the RGB value is not known
    # System window text colour for border lines
    book.colour_map[ndpal+8] = None
    # System window background colour for pattern background
    book.colour_map[ndpal+8+1] = None #
    for ci in (
        0x51, # System ToolTip text colour (used in note objects)
        0x7FFF, # 32767, system window text colour for fonts
        ):
        book.colour_map[ci] = None

def nearest_colour_index(colour_map, rgb, debug=0):
    # General purpose function. Uses Euclidean distance.
    # So far used only for pre-BIFF8 WINDOW2 record.
    # Doesn't have to be fast.
    # Doesn't have to be fancy.
    best_metric = 3 * 256 * 256
    best_colourx = 0
    for colourx, cand_rgb in colour_map.items():
        if cand_rgb is None:
            continue
        metric = 0
        for v1, v2 in zip(rgb, cand_rgb):
            metric += (v1 - v2) * (v1 - v2)
        if metric < best_metric:
            best_metric = metric
            best_colourx = colourx
            if metric == 0:
                break
    if debug:
        print "nearest_colour_index for %r is %r -> %r; best_metric is %d" \
            % (rgb, best_colourx, colour_map[best_colourx], best_metric)
    return best_colourx

##
# This mixin class exists solely so that Format, Font, and XF.... objects
# can be compared by value of their attributes.
class EqNeAttrs(object):

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        return self.__dict__ != other.__dict__

##
# An Excel "font" contains the details of not only what is normally
# considered a font, but also several other display attributes.
# Items correspond to those in the Excel UI's Format/Cells/Font tab.
# <br /> -- New in version 0.6.1
class Font(BaseObject, EqNeAttrs):
    ##
    # 1 = Characters are bold. Redundant; see "weight" attribute.
    bold = 0
    ##
    # Values: 0 = ANSI Latin, 1 = System default, 2 = Symbol,
    # 77 = Apple Roman,
    # 128 = ANSI Japanese Shift-JIS,
    # 129 = ANSI Korean (Hangul),
    # 130 = ANSI Korean (Johab),
    # 134 = ANSI Chinese Simplified GBK,
    # 136 = ANSI Chinese Traditional BIG5,
    # 161 = ANSI Greek,
    # 162 = ANSI Turkish,
    # 163 = ANSI Vietnamese,
    # 177 = ANSI Hebrew,
    # 178 = ANSI Arabic,
    # 186 = ANSI Baltic,
    # 204 = ANSI Cyrillic,
    # 222 = ANSI Thai,
    # 238 = ANSI Latin II (Central European),
    # 255 = OEM Latin I
    character_set = 0
    ##
    # An explanation of "colour index" is given in the Formatting
    # section at the start of this document.
    colour_index = 0
    ##
    # 1 = Superscript, 2 = Subscript.
    escapement = 0
    ##
    # 0 = None (unknown or don't care)<br />
    # 1 = Roman (variable width, serifed)<br />
    # 2 = Swiss (variable width, sans-serifed)<br />
    # 3 = Modern (fixed width, serifed or sans-serifed)<br />
    # 4 = Script (cursive)<br />
    # 5 = Decorative (specialised, for example Old English, Fraktur)
    family = 0
    ##
    # The 0-based index used to refer to this Font() instance.
    # Note that index 4 is never used; xlrd supplies a dummy place-holder.
    font_index = 0
    ##
    # Height of the font (in twips). A twip = 1/20 of a point.
    height = 0
    ##
    # 1 = Characters are italic.
    italic = 0
    ##
    # The name of the font. Example: u"Arial"
    name = u""
    ##
    # 1 = Characters are struck out.
    struck_out = 0
    ##
    # 0 = None<br />
    # 1 = Single;  0x21 (33) = Single accounting<br />
    # 2 = Double;  0x22 (34) = Double accounting
    underline_type = 0
    ##
    # 1 = Characters are underlined. Redundant; see "underline_type" attribute.
    underlined = 0
    ##
    # Font weight (100-1000). Standard values are 400 for normal text
    # and 700 for bold text.
    weight = 400
    ##
    # 1 = Font is outline style (Macintosh only)
    outline = 0
    ##
    # 1 = Font is shadow style (Macintosh only)
    shadow = 0

    # No methods ...

def handle_efont(book, data): # BIFF2 only
    if not book.formatting_info:
        return
    book.font_list[-1].colour_index = unpack('<H', data)[0]

def handle_font(book, data):
    if not book.formatting_info:
        return
    if not book.encoding:
        book.derive_encoding()
    blah = DEBUG or book.verbosity >= 2
    bv = book.biff_version
    k = len(book.font_list)
    if k == 4:
        f = Font()
        f.name = u'Dummy Font'
        f.font_index = k
        book.font_list.append(f)
        k += 1
    f = Font()
    f.font_index = k
    book.font_list.append(f)
    if bv >= 50:
        (
            f.height, option_flags, f.colour_index, f.weight,
            f.escapement_type, f.underline_type, f.family,
            f.character_set,
        ) = unpack('<HHHHHBBB', data[0:13])
        f.bold = option_flags & 1
        f.italic = (option_flags & 2) >> 1
        f.underlined = (option_flags & 4) >> 2
        f.struck_out = (option_flags & 8) >> 3
        f.outline = (option_flags & 16) >> 4
        f.shadow = (option_flags & 32) >> 5
        if bv >= 80:
            f.name = unpack_unicode(data, 14, lenlen=1)
        else:
            f.name = unpack_string(data, 14, book.encoding, lenlen=1)
    elif bv >= 30:
        f.height, option_flags, f.colour_index = unpack('<HHH', data[0:6])
        f.bold = option_flags & 1
        f.italic = (option_flags & 2) >> 1
        f.underlined = (option_flags & 4) >> 2
        f.struck_out = (option_flags & 8) >> 3
        f.outline = (option_flags & 16) >> 4
        f.shadow = (option_flags & 32) >> 5
        f.name = unpack_string(data, 6, book.encoding, lenlen=1)
        # Now cook up the remaining attributes ...
        f.weight = [400, 700][f.bold]
        f.escapement_type = 0 # None
        f.underline_type = f.underlined # None or Single
        f.family = 0 # Unknown / don't care
        f.character_set = 1 # System default (0 means "ANSI Latin")
    else: # BIFF2
        f.height, option_flags = unpack('<HH', data[0:4])
        f.colour_index = 0x7FFF # "system window text colour"
        f.bold = option_flags & 1
        f.italic = (option_flags & 2) >> 1
        f.underlined = (option_flags & 4) >> 2
        f.struck_out = (option_flags & 8) >> 3
        f.outline = 0
        f.shadow = 0
        f.name = unpack_string(data, 4, book.encoding, lenlen=1)
        # Now cook up the remaining attributes ...
        f.weight = [400, 700][f.bold]
        f.escapement_type = 0 # None
        f.underline_type = f.underlined # None or Single
        f.family = 0 # Unknown / don't care
        f.character_set = 1 # System default (0 means "ANSI Latin")
    if blah:
        f.dump(
            book.logfile,
            header="--- handle_font: font[%d] ---" % f.font_index,
            footer="-------------------",
            )

# === "Number formats" ===

##
# "Number format" information from a FORMAT record.
# <br /> -- New in version 0.6.1
class Format(BaseObject, EqNeAttrs):
    ##
    # The key into Book.format_map
    format_key = 0
    ##
    # A classification that has been inferred from the format string.
    # Currently, this is used only to distinguish between numbers and dates.
    # <br />Values:
    # <br />FUN = 0 # unknown
    # <br />FDT = 1 # date
    # <br />FNU = 2 # number
    # <br />FGE = 3 # general
    # <br />FTX = 4 # text
    type = FUN
    ##
    # The format string
    format_str = u''

    def __init__(self, format_key, ty, format_str):
        self.format_key = format_key
        self.type = ty
        self.format_str = format_str

std_format_strings = {
    # "std" == "standard for US English locale"
    # #### TODO ... a lot of work to tailor these to the user's locale.
    # See e.g. gnumeric-1.x.y/src/formats.c
    0x00: "General",
    0x01: "0",
    0x02: "0.00",
    0x03: "#,##0",
    0x04: "#,##0.00",
    0x05: "$#,##0_);($#,##0)",
    0x06: "$#,##0_);[Red]($#,##0)",
    0x07: "$#,##0.00_);($#,##0.00)",
    0x08: "$#,##0.00_);[Red]($#,##0.00)",
    0x09: "0%",
    0x0a: "0.00%",
    0x0b: "0.00E+00",
    0x0c: "# ?/?",
    0x0d: "# ??/??",
    0x0e: "m/d/yy",
    0x0f: "d-mmm-yy",
    0x10: "d-mmm",
    0x11: "mmm-yy",
    0x12: "h:mm AM/PM",
    0x13: "h:mm:ss AM/PM",
    0x14: "h:mm",
    0x15: "h:mm:ss",
    0x16: "m/d/yy h:mm",
    0x25: "#,##0_);(#,##0)",
    0x26: "#,##0_);[Red](#,##0)",
    0x27: "#,##0.00_);(#,##0.00)",
    0x28: "#,##0.00_);[Red](#,##0.00)",
    0x29: "_(* #,##0_);_(* (#,##0);_(* \"-\"_);_(@_)",
    0x2a: "_($* #,##0_);_($* (#,##0);_($* \"-\"_);_(@_)",
    0x2b: "_(* #,##0.00_);_(* (#,##0.00);_(* \"-\"??_);_(@_)",
    0x2c: "_($* #,##0.00_);_($* (#,##0.00);_($* \"-\"??_);_(@_)",
    0x2d: "mm:ss",
    0x2e: "[h]:mm:ss",
    0x2f: "mm:ss.0",
    0x30: "##0.0E+0",
    0x31: "@",
    }

fmt_code_ranges = [ # both-inclusive ranges of "standard" format codes
    # Source: the openoffice.org doc't
    ( 0,  0, FGE),
    ( 1, 13, FNU),
    (14, 22, FDT),
    #### (27, 36, FDT), # Japanese dates -- not sure of reliability of this
    (37, 44, FNU),
    (45, 47, FDT),
    (48, 48, FNU),
    (49, 49, FTX),
    ####(50, 58, FDT), # Japanese dates -- but Gnumeric assumes
                       # built-in formats finish at 49, not at 163
    ]

std_format_code_types = {}
for lo, hi, ty in fmt_code_ranges:
    for x in xrange(lo, hi+1):
        std_format_code_types[x] = ty
del lo, hi, ty, x

date_chars = u'ymdhs' # year, month/minute, day, hour, second
date_char_dict = {}
for _c in date_chars + date_chars.upper():
    date_char_dict[_c] = 5
del _c, date_chars

skip_char_dict = {}
for _c in u'$-+/(): ':
    skip_char_dict[_c] = 1

num_char_dict = {
    u'0': 5,
    u'#': 5,
    u'?': 5,
    }

non_date_formats = {
    u'0.00E+00':1,
    u'##0.0E+0':1,
    u'General' :1,
    u'GENERAL' :1, # OOo Calc 1.1.4 does this.
    u'general' :1,  # pyExcelerator 0.6.3 does this.
    u'@'       :1,
    }

fmt_bracketed_sub = re.compile(r'\[[^]]*\]').sub

# Boolean format strings (actual cases)
# u'"Yes";"Yes";"No"'
# u'"True";"True";"False"'
# u'"On";"On";"Off"'

def is_date_format_string(book, fmt):
    # Heuristics:
    # Ignore "text" and [stuff in square brackets (aarrgghh -- see below)].
    # Handle backslashed-escaped chars properly.
    # E.g. hh\hmm\mss\s should produce a display like 23h59m59s
    # Date formats have one or more of ymdhs (caseless) in them.
    # Numeric formats have # and 0.
    # N.B. u'General"."' hence get rid of "text" first.
    # TODO: Find where formats are interpreted in Gnumeric
    # TODO: u'[h]\\ \\h\\o\\u\\r\\s' ([h] means don't care about hours > 23)
    state = 0
    s = ''
    ignorable = skip_char_dict.has_key
    for c in fmt:
        if state == 0:
            if c == u'"':
                state = 1
            elif c in ur"\_*":
                state = 2
            elif ignorable(c):
                pass
            else:
                s += c
        elif state == 1:
            if c == u'"':
                state = 0
        elif state == 2:
            # Ignore char after backslash, underscore or asterisk
            state = 0
        assert 0 <= state <= 2
    if book.verbosity >= 4:
        print "is_date_format_string: reduced format is %r" % s
    s = fmt_bracketed_sub('', s)
    if non_date_formats.has_key(s):
        return False
    state = 0
    separator = ";"
    got_sep = 0
    date_count = num_count = 0
    for c in s:
        if date_char_dict.has_key(c):
            date_count += date_char_dict[c]
        elif num_char_dict.has_key(c):
            num_count += num_char_dict[c]
        elif c == separator:
            got_sep = 1
    # print num_count, date_count, repr(fmt)
    if date_count and not num_count:
        return True
    if num_count and not date_count:
        return False
    if date_count:
        fprintf(book.logfile,
            'WARNING *** is_date_format: ambiguous d=%d n=%d fmt=%r\n',
            date_count, num_count, fmt)
    elif not got_sep:
        fprintf(book.logfile,
            "WARNING *** format %r produces constant result\n",
            fmt)
    return date_count > num_count

def handle_format(self, data, rectype=XL_FORMAT):
    DEBUG = 0
    bv = self.biff_version
    if rectype == XL_FORMAT2:
        bv = min(bv, 30)
    if not self.encoding:
        self.derive_encoding()
    strpos = 2
    if bv >= 50:
        fmtkey = unpack('<H', data[0:2])[0]
    else:
        fmtkey = self.actualfmtcount
        if bv <= 30:
            strpos = 0
    self.actualfmtcount += 1
    if bv >= 80:
        unistrg = unpack_unicode(data, 2)
    else:
        unistrg = unpack_string(data, strpos, self.encoding, lenlen=1)
    blah = DEBUG or self.verbosity >= 3
    if blah:
        fprintf(self.logfile,
            "FORMAT: count=%d fmtkey=0x%04x (%d) s=%r\n",
            self.actualfmtcount, fmtkey, fmtkey, unistrg)
    is_date_s = self.is_date_format_string(unistrg)
    ty = [FGE, FDT][is_date_s]
    if not(fmtkey > 163 or bv < 50):
        # user_defined if fmtkey > 163
        # N.B. Gnumeric incorrectly starts these at 50 instead of 164 :-(
        # if earlier than BIFF 5, standard info is useless
        std_ty = std_format_code_types.get(fmtkey, FUN)
        # print "std ty", std_ty
        is_date_c = std_ty == FDT
        if 0 < fmtkey < 50 and (is_date_c ^ is_date_s):
            DEBUG = 2
            fprintf(self.logfile,
                "WARNING *** Conflict between "
                "std format key %d and its format string %r\n",
                fmtkey, unistrg)
    if DEBUG == 2:
        fprintf(self.logfile,
            "ty: %d; is_date_c: %r; is_date_s: %r; fmt_strg: %r",
            ty, is_date_c, is_date_s, unistrg)
    fmtobj = Format(fmtkey, ty, unistrg)
    if blah:
        fmtobj.dump(self.logfile,
            header="--- handle_format [%d] ---" % (self.actualfmtcount-1, ))
    self.format_map[fmtkey] = fmtobj
    self.format_list.append(fmtobj)

# =============================================================================

def handle_palette(book, data):
    if not book.formatting_info:
        return
    blah = DEBUG or book.verbosity >= 2
    n_colours, = unpack('<H', data[:2])
    expected_n_colours = (16, 56)[book.biff_version >= 50]
    if ((DEBUG or book.verbosity >= 1)
    and n_colours != expected_n_colours):
        fprintf(book.logfile,
            "NOTE *** Expected %d colours in PALETTE record, found %d\n",
            expected_n_colours, n_colours)
    elif blah:
        fprintf(book.logfile,
            "PALETTE record with %d colours\n", n_colours)
    fmt = '<xx%di' % n_colours # use i to avoid long integers
    expected_size = 4 * n_colours + 2
    actual_size = len(data)
    tolerance = 4
    if not expected_size <= actual_size <= expected_size + tolerance:
        raise XLRDError('PALETTE record: expected size %d, actual size %d' % (expected_size, actual_size))
    colours = unpack(fmt, data[:expected_size])
    assert book.palette_record == [] # There should be only 1 PALETTE record
    # a colour will be 0xbbggrr
    # IOW, red is at the little end
    for i in xrange(n_colours):
        c = colours[i]
        red   =  c        & 0xff
        green = (c >>  8) & 0xff
        blue  = (c >> 16) & 0xff
        old_rgb = book.colour_map[8+i]
        new_rgb = (red, green, blue)
        book.palette_record.append(new_rgb)
        book.colour_map[8+i] = new_rgb
        if blah:
            if new_rgb != old_rgb:
                print >> book.logfile, "%2d: %r -> %r" % (i, old_rgb, new_rgb)

def palette_epilogue(book):
    # Check colour indexes in fonts etc.
    # This must be done here as FONT records
    # come *before* the PALETTE record :-(
    for font in book.font_list:
        if font.font_index == 4: # the missing font record
            continue
        cx = font.colour_index
        if cx == 0x7fff: # system window text colour
            continue
        if book.colour_map.has_key(cx):
            book.colour_indexes_used[cx] = 1
        else:
            print "Size of colour table:", len(book.colour_map)
            print >> book.logfile, \
                "*** Font #%d (%r): colour index 0x%04x is unknown" \
                % (font.font_index, font.name, cx)
    if book.verbosity >= 1:
        used = book.colour_indexes_used.keys()
        used.sort()
        print >> book.logfile, "\nColour indexes used:\n%r\n" % used

def handle_style(book, data):
    blah = DEBUG or book.verbosity >= 2
    bv = book.biff_version
    flag_and_xfx, built_in_id, level = unpack('<HBB', data[:4])
    xf_index = flag_and_xfx & 0x0fff
    if (data == "\0\0\0\0"
    and "Normal" not in book.style_name_map):
        # Erroneous record (doesn't have built-in bit set).
        # Example file supplied by Jeff Bell.
        built_in = 1
        built_in_id = 0
        xf_index = 0
        name = "Normal"
        level = 255
    elif flag_and_xfx & 0x8000:
        # built-in style
        built_in = 1
        name = built_in_style_names[built_in_id]
        if 1 <= built_in_id <= 2:
            name += str(level + 1)
    else:
        # user-defined style
        if bv >= 80:
            name = unpack_unicode(data, 2, lenlen=2)
        else:
            name = unpack_string(data, 2, book.encoding, lenlen=1)
        if blah and not name:
            print >> book.logfile, \
                "WARNING *** A user-defined style has a zero-length name"
        built_in = 0
        built_in_id = 0
        level = 0
    book.style_name_map[name] = (built_in, xf_index)
    if blah:
        print >> book.logfile, \
            "STYLE: built_in=%d xf_index=%d built_in_id=%d level=%d name=%r" \
            % (built_in, xf_index, built_in_id, level, name)

def check_colour_indexes_in_obj(book, obj, orig_index):
    alist = obj.__dict__.items()
    alist.sort()
    for attr, nobj in alist:
        if hasattr(nobj, 'dump'):
            check_colour_indexes_in_obj(book, nobj, orig_index)
        elif attr.find('colour_index') >= 0:
            if book.colour_map.has_key(nobj):
                book.colour_indexes_used[nobj] = 1
                continue
            oname = obj.__class__.__name__
            print >> book.logfile, \
                "*** xf #%d : %s.%s =  0x%04x (unknown)" \
                % (orig_index, oname, attr, nobj)

def handle_xf(self, data):
    ### self is a Book instance
    # DEBUG = 0
    blah = DEBUG or self.verbosity >= 3
    bv = self.biff_version
    xf = XF()
    xf.alignment = XFAlignment()
    xf.alignment.indent_level = 0
    xf.alignment.shrink_to_fit = 0
    xf.alignment.text_direction = 0
    xf.border = XFBorder()
    xf.border.diag_up = 0
    xf.border.diag_down = 0
    xf.border.diag_colour_index = 0
    xf.border.diag_line_style = 0 # no line
    xf.background = XFBackground()
    xf.protection = XFProtection()
    # fill in the known standard formats
    if bv >= 50 and not self.xfcount:
        # i.e. do this once before we process the first XF record
        for x in std_format_code_types.keys():
            if not self.format_map.has_key(x):
                ty = std_format_code_types[x]
                fmt_str = std_format_strings[x]
                fmtobj = Format(x, ty, fmt_str)
                self.format_map[x] = fmtobj
    if bv >= 80:
        unpack_fmt = '<HHHBBBBIiH'
        (xf.font_index, xf.format_key, pkd_type_par,
        pkd_align1, xf.alignment.rotation, pkd_align2,
        pkd_used, pkd_brdbkg1, pkd_brdbkg2, pkd_brdbkg3,
        ) = unpack(unpack_fmt, data[0:20])
        upkbits(xf.protection, pkd_type_par, (
            (0, 0x01, 'cell_locked'),
            (1, 0x02, 'formula_hidden'),
            ))
        upkbits(xf, pkd_type_par, (
            (2, 0x0004, 'is_style'),
            # Following is not in OOo docs, but is mentioned
            # in Gnumeric source and also in (deep breath)
            # org.apache.poi.hssf.record.ExtendedFormatRecord.java
            (3, 0x0008, 'lotus_123_prefix'), # Meaning is not known.
            (4, 0xFFF0, 'parent_style_index'),
            ))
        upkbits(xf.alignment, pkd_align1, (
            (0, 0x07, 'hor_align'),
            (3, 0x08, 'text_wrapped'),
            (4, 0x70, 'vert_align'),
            ))
        upkbits(xf.alignment, pkd_align2, (
            (0, 0x0f, 'indent_level'),
            (4, 0x10, 'shrink_to_fit'),
            (6, 0xC0, 'text_direction'),
            ))
        reg = pkd_used >> 2
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, reg & 1)
            reg >>= 1
        upkbitsL(xf.border, pkd_brdbkg1, (
            (0,  0x0000000f,  'left_line_style'),
            (4,  0x000000f0,  'right_line_style'),
            (8,  0x00000f00,  'top_line_style'),
            (12, 0x0000f000,  'bottom_line_style'),
            (16, 0x007f0000,  'left_colour_index'),
            (23, 0x3f800000,  'right_colour_index'),
            (30, 0x40000000,  'diag_down'),
            (31, 0x80000000L, 'diag_up'),
            ))
        upkbits(xf.border, pkd_brdbkg2, (
            (0,  0x0000007F, 'top_colour_index'),
            (7,  0x00003F80, 'bottom_colour_index'),
            (14, 0x001FC000, 'diag_colour_index'),
            (21, 0x01E00000, 'diag_line_style'),
            ))
        upkbitsL(xf.background, pkd_brdbkg2, (
            (26, 0xFC000000L, 'fill_pattern'),
            ))
        upkbits(xf.background, pkd_brdbkg3, (
            (0, 0x007F, 'pattern_colour_index'),
            (7, 0x3F80, 'background_colour_index'),
            ))
    elif bv >= 50:
        unpack_fmt = '<HHHBBIi'
        (xf.font_index, xf.format_key, pkd_type_par,
        pkd_align1, pkd_orient_used,
        pkd_brdbkg1, pkd_brdbkg2,
        ) = unpack(unpack_fmt, data[0:16])
        upkbits(xf.protection, pkd_type_par, (
            (0, 0x01, 'cell_locked'),
            (1, 0x02, 'formula_hidden'),
            ))
        upkbits(xf, pkd_type_par, (
            (2, 0x0004, 'is_style'),
            (3, 0x0008, 'lotus_123_prefix'), # Meaning is not known.
            (4, 0xFFF0, 'parent_style_index'),
            ))
        upkbits(xf.alignment, pkd_align1, (
            (0, 0x07, 'hor_align'),
            (3, 0x08, 'text_wrapped'),
            (4, 0x70, 'vert_align'),
            ))
        orientation = pkd_orient_used & 0x03
        xf.alignment.rotation = [0, 255, 90, 180][orientation]
        reg = pkd_orient_used >> 2
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, reg & 1)
            reg >>= 1
        upkbitsL(xf.background, pkd_brdbkg1, (
            ( 0, 0x0000007F, 'pattern_colour_index'),
            ( 7, 0x00003F80, 'background_colour_index'),
            (16, 0x003F0000, 'fill_pattern'),
            ))
        upkbitsL(xf.border, pkd_brdbkg1, (
            (22, 0x01C00000,  'bottom_line_style'),
            (25, 0xFE000000L, 'bottom_colour_index'),
            ))
        upkbits(xf.border, pkd_brdbkg2, (
            ( 0, 0x00000007, 'top_line_style'),
            ( 3, 0x00000038, 'left_line_style'),
            ( 6, 0x000001C0, 'right_line_style'),
            ( 9, 0x0000FE00, 'top_colour_index'),
            (16, 0x007F0000, 'left_colour_index'),
            (23, 0x3F800000, 'right_colour_index'),
            ))
    elif bv >= 40:
        unpack_fmt = '<BBHBBHI'
        (xf.font_index, xf.format_key, pkd_type_par,
        pkd_align_orient, pkd_used,
        pkd_bkg_34, pkd_brd_34,
        ) = unpack(unpack_fmt, data[0:12])
        upkbits(xf.protection, pkd_type_par, (
            (0, 0x01, 'cell_locked'),
            (1, 0x02, 'formula_hidden'),
            ))
        upkbits(xf, pkd_type_par, (
            (2, 0x0004, 'is_style'),
            (3, 0x0008, 'lotus_123_prefix'), # Meaning is not known.
            (4, 0xFFF0, 'parent_style_index'),
            ))
        upkbits(xf.alignment, pkd_align_orient, (
            (0, 0x07, 'hor_align'),
            (3, 0x08, 'text_wrapped'),
            (4, 0x30, 'vert_align'),
            ))
        orientation = (pkd_align_orient & 0xC0) >> 6
        xf.alignment.rotation = [0, 255, 90, 180][orientation]
        reg = pkd_used >> 2
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, reg & 1)
            reg >>= 1
        upkbits(xf.background, pkd_bkg_34, (
            ( 0, 0x003F, 'fill_pattern'),
            ( 6, 0x07C0, 'pattern_colour_index'),
            (11, 0xF800, 'background_colour_index'),
            ))
        upkbitsL(xf.border, pkd_brd_34, (
            ( 0, 0x00000007,  'top_line_style'),
            ( 3, 0x000000F8,  'top_colour_index'),
            ( 8, 0x00000700,  'left_line_style'),
            (11, 0x0000F800,  'left_colour_index'),
            (16, 0x00070000,  'bottom_line_style'),
            (19, 0x00F80000,  'bottom_colour_index'),
            (24, 0x07000000,  'right_line_style'),
            (27, 0xF8000000L, 'right_colour_index'),
            ))
    elif bv == 30:
        unpack_fmt = '<BBBBHHI'
        (xf.font_index, xf.format_key, pkd_type_prot,
        pkd_used, pkd_align_par,
        pkd_bkg_34, pkd_brd_34,
        ) = unpack(unpack_fmt, data[0:12])
        upkbits(xf.protection, pkd_type_prot, (
            (0, 0x01, 'cell_locked'),
            (1, 0x02, 'formula_hidden'),
            ))
        upkbits(xf, pkd_type_prot, (
            (2, 0x0004, 'is_style'),
            (3, 0x0008, 'lotus_123_prefix'), # Meaning is not known.
            ))
        upkbits(xf.alignment, pkd_align_par, (
            (0, 0x07, 'hor_align'),
            (3, 0x08, 'text_wrapped'),
            ))
        upkbits(xf, pkd_align_par, (
            (4, 0xFFF0, 'parent_style_index'),
            ))
        reg = pkd_used >> 2
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, reg & 1)
            reg >>= 1
        upkbits(xf.background, pkd_bkg_34, (
            ( 0, 0x003F, 'fill_pattern'),
            ( 6, 0x07C0, 'pattern_colour_index'),
            (11, 0xF800, 'background_colour_index'),
            ))
        upkbitsL(xf.border, pkd_brd_34, (
            ( 0, 0x00000007,  'top_line_style'),
            ( 3, 0x000000F8,  'top_colour_index'),
            ( 8, 0x00000700,  'left_line_style'),
            (11, 0x0000F800,  'left_colour_index'),
            (16, 0x00070000,  'bottom_line_style'),
            (19, 0x00F80000,  'bottom_colour_index'),
            (24, 0x07000000,  'right_line_style'),
            (27, 0xF8000000L, 'right_colour_index'),
            ))
        xf.alignment.vert_align = 2 # bottom
        xf.alignment.rotation = 0
    elif bv == 21:
        #### Warning: incomplete treatment; formatting_info not fully supported.
        #### Probably need to offset incoming BIFF2 XF[n] to BIFF8-like XF[n+16],
        #### and create XF[0:16] like the standard ones in BIFF8
        #### *AND* add 16 to all XF references in cell records :-(
        (xf.font_index, format_etc, halign_etc) = unpack('<BxBB', data)
        xf.format_key = format_etc & 0x3F
        upkbits(xf.protection, format_etc, (
            (6, 0x40, 'cell_locked'),
            (7, 0x80, 'formula_hidden'),
            ))
        upkbits(xf.alignment, halign_etc, (
            (0, 0x07, 'hor_align'),
            ))
        for mask, side in ((0x08, 'left'), (0x10, 'right'), (0x20, 'top'), (0x40, 'bottom')):
            if halign_etc & mask:
                colour_index, line_style = 8, 1 # black, thin
            else:
                colour_index, line_style = 0, 0 # none, none
            setattr(xf.border, side + '_colour_index', colour_index)
            setattr(xf.border, side + '_line_style', line_style)
        bg = xf.background
        if halign_etc & 0x80:
            bg.fill_pattern = 17
        else:
            bg.fill_pattern = 0
        bg.background_colour_index = 9 # white
        bg.pattern_colour_index = 8 # black
        xf.parent_style_index = 0 # ???????????
        xf.alignment.vert_align = 2 # bottom
        xf.alignment.rotation = 0
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, 1)
    else:
        raise XLRDError('programmer stuff-up: bv=%d' % bv)

    xf.xf_index = len(self.xf_list)
    self.xf_list.append(xf)
    self.xfcount += 1
    if blah:
        xf.dump(
            self.logfile,
            header="--- handle_xf: xf[%d] ---" % xf.xf_index,
            footer=" ",
        )
    # Now for some assertions ...
    if self.formatting_info:
        if xf.is_style and xf.parent_style_index != 0x0FFF:
            msg = "WARNING *** XF[%d] is a style XF but parent_style_index is 0x%04x, not 0x0fff\n"
            fprintf(self.logfile, msg, xf.xf_index, xf.parent_style_index)
        check_colour_indexes_in_obj(self, xf, xf.xf_index)
    if not self.format_map.has_key(xf.format_key):
        msg = "WARNING *** XF[%d] unknown (raw) format key (%d, 0x%04x)\n"
        fprintf(self.logfile, msg,
                xf.xf_index, xf.format_key, xf.format_key)
        xf.format_key = 0

def xf_epilogue(self):
    # self is a Book instance.
    self._xf_epilogue_done = 1
    num_xfs = len(self.xf_list)
    blah = DEBUG or self.verbosity >= 3
    blah1 = DEBUG or self.verbosity >= 1
    if blah:
        fprintf(self.logfile, "xf_epilogue called ...\n")

    def check_same(book_arg, xf_arg, parent_arg, attr):
        # the _arg caper is to avoid a Warning msg from Python 2.1 :-(
        if getattr(xf_arg, attr) != getattr(parent_arg, attr):
            fprintf(book_arg.logfile,
                "NOTE !!! XF[%d] parent[%d] %s different\n",
                xf_arg.xf_index, parent_arg.xf_index, attr)

    for xfx in xrange(num_xfs):
        xf = self.xf_list[xfx]
        if not self.format_map.has_key(xf.format_key):
            msg = "ERROR *** XF[%d] unknown format key (%d, 0x%04x)\n"
            fprintf(self.logfile, msg,
                    xf.xf_index, xf.format_key, xf.format_key)
            xf.format_key = 0
        cellty_from_fmtty = {
            FNU: XL_CELL_NUMBER,
            FUN: XL_CELL_NUMBER,
            FGE: XL_CELL_NUMBER,
            FDT: XL_CELL_DATE,
            FTX: XL_CELL_NUMBER, # Yes, a number can be formatted as text.
            }
        fmt = self.format_map[xf.format_key]
        cellty = cellty_from_fmtty[fmt.type]
        self._xf_index_to_xl_type_map[xf.xf_index] = cellty
        # Now for some assertions etc
        if not self.formatting_info:
            continue
        if xf.is_style:
            continue
        if not(0 <= xf.parent_style_index < num_xfs):
            fprintf(self.logfile,
                "WARNING *** XF[%d]: is_style=%d but parent_style_index=%d\n",
                xf.xf_index, xf.is_style, xf.parent_style_index)
            # make it conform
            xf.parent_style_index = 0
        if self.biff_version >= 30:
            assert xf.parent_style_index != xf.xf_index
            assert self.xf_list[xf.parent_style_index].is_style
            if blah1 and xf.parent_style_index > xf.xf_index:
                fprintf(self.logfile,
                    "NOTE !!! XF[%d]: parent_style_index is %d; out of order?\n",
                    xf.xf_index, xf.parent_style_index)
            parent = self.xf_list[xf.parent_style_index]
            if not xf._alignment_flag and not parent._alignment_flag:
                if blah1: check_same(self, xf, parent, 'alignment')
            if not xf._background_flag and not parent._background_flag:
                if blah1: check_same(self, xf, parent, 'background')
            if not xf._border_flag and not parent._border_flag:
                if blah1: check_same(self, xf, parent, 'border')
            if not xf._protection_flag and not parent._protection_flag:
                if blah1: check_same(self, xf, parent, 'protection')
            if not xf._format_flag and not parent._format_flag:
                if blah1 and xf.format_key != parent.format_key:
                    fprintf(self.logfile,
                        "NOTE !!! XF[%d] fmtk=%d, parent[%d] fmtk=%r\n%r / %r\n",
                        xf.xf_index, xf.format_key, parent.xf_index, parent.format_key,
                        self.format_map[xf.format_key].format_str,
                        self.format_map[parent.format_key].format_str)
            if not xf._font_flag and not parent._font_flag:
                if blah1 and xf.font_index != parent.font_index:
                    fprintf(self.logfile,
                        "NOTE !!! XF[%d] fontx=%d, parent[%d] fontx=%r\n",
                        xf.xf_index, xf.font_index, parent.xf_index, parent.font_index)

def initialise_book(book):
    initialise_colour_map(book)
    book._xf_epilogue_done = 0
    methods = (
        handle_font,
        handle_efont,
        handle_format,
        is_date_format_string,
        handle_palette,
        palette_epilogue,
        handle_style,
        handle_xf,
        xf_epilogue,
        )
    for method in methods:
        setattr(book.__class__, method.__name__, method)

##
# <p>A collection of the border-related attributes of an XF record.
# Items correspond to those in the Excel UI's Format/Cells/Border tab.</p>
# <p> An explanations of "colour index" is given in the Formatting
# section at the start of this document.
# There are five line style attributes; possible values and the
# associated meanings are:
# 0&nbsp;=&nbsp;No line,
# 1&nbsp;=&nbsp;Thin,
# 2&nbsp;=&nbsp;Medium,
# 3&nbsp;=&nbsp;Dashed,
# 4&nbsp;=&nbsp;Dotted,
# 5&nbsp;=&nbsp;Thick,
# 6&nbsp;=&nbsp;Double,
# 7&nbsp;=&nbsp;Hair,
# 8&nbsp;=&nbsp;Medium dashed,
# 9&nbsp;=&nbsp;Thin dash-dotted,
# 10&nbsp;=&nbsp;Medium dash-dotted,
# 11&nbsp;=&nbsp;Thin dash-dot-dotted,
# 12&nbsp;=&nbsp;Medium dash-dot-dotted,
# 13&nbsp;=&nbsp;Slanted medium dash-dotted.
# The line styles 8 to 13 appear in BIFF8 files (Excel 97 and later) only.
# For pictures of the line styles, refer to OOo docs s3.10 (p22)
# "Line Styles for Cell Borders (BIFF3-BIFF8)".</p>
# <br /> -- New in version 0.6.1
class XFBorder(BaseObject, EqNeAttrs):

    ##
    # The colour index for the cell's top line
    top_colour_index = 0
    ##
    # The colour index for the cell's bottom line
    bottom_colour_index = 0
    ##
    # The colour index for the cell's left line
    left_colour_index = 0
    ##
    # The colour index for the cell's right line
    right_colour_index = 0
    ##
    # The colour index for the cell's diagonal lines, if any
    diag_colour_index = 0
    ##
    # The line style for the cell's top line
    top_line_style = 0
    ##
    # The line style for the cell's bottom line
    bottom_line_style = 0
    ##
    # The line style for the cell's left line
    left_line_style = 0
    ##
    # The line style for the cell's right line
    right_line_style = 0
    ##
    # The line style for the cell's diagonal lines, if any
    diag_line_style = 0
    ##
    # 1 = draw a diagonal from top left to bottom right
    diag_down = 0
    ##
    # 1 = draw a diagonal from bottom left to top right
    diag_up = 0

##
# A collection of the background-related attributes of an XF record.
# Items correspond to those in the Excel UI's Format/Cells/Patterns tab.
# An explanation of "colour index" is given in the Formatting
# section at the start of this document.
# <br /> -- New in version 0.6.1
class XFBackground(BaseObject, EqNeAttrs):

    ##
    # See section 3.11 of the OOo docs.
    fill_pattern = 0
    ##
    # See section 3.11 of the OOo docs.
    background_colour_index = 0
    ##
    # See section 3.11 of the OOo docs.
    pattern_colour_index = 0

##
# A collection of the alignment and similar attributes of an XF record.
# Items correspond to those in the Excel UI's Format/Cells/Alignment tab.
# <br /> -- New in version 0.6.1

class XFAlignment(BaseObject, EqNeAttrs):

    ##
    # Values: section 6.115 (p 214) of OOo docs
    hor_align = 0
    ##
    # Values: section 6.115 (p 215) of OOo docs
    vert_align = 0
    ##
    # Values: section 6.115 (p 215) of OOo docs.<br />
    # Note: file versions BIFF7 and earlier use the documented
    # "orientation" attribute; this will be mapped (without loss)
    # into "rotation".
    rotation = 0
    ##
    # 1 = text is wrapped at right margin
    text_wrapped = 0
    ##
    # A number in range(15).
    indent_level = 0
    ##
    # 1 = shrink font size to fit text into cell.
    shrink_to_fit = 0
    ##
    # 0 = according to context; 1 = left-to-right; 2 = right-to-left
    text_direction = 0

##
# A collection of the protection-related attributes of an XF record.
# Items correspond to those in the Excel UI's Format/Cells/Protection tab.
# Note the OOo docs include the "cell or style" bit
# in this bundle of attributes.
# This is incorrect; the bit is used in determining which bundles to use.
# <br /> -- New in version 0.6.1

class XFProtection(BaseObject, EqNeAttrs):

    ##
    # 1 = Cell is prevented from being changed, moved, resized, or deleted
    # (only if the sheet is protected).
    cell_locked = 0
    ##
    # 1 = Hide formula so that it doesn't appear in the formula bar when
    # the cell is selected (only if the sheet is protected).
    formula_hidden = 0

##
# eXtended Formatting information for cells, rows, columns and styles.
# <br /> -- New in version 0.6.1
#
# <p>Each of the 6 flags below describes the validity of
# a specific group of attributes.
# <br />
# In cell XFs, flag==0 means the attributes of the parent style XF are used,
# (but only if the attributes are valid there); flag==1 means the attributes
# of this XF are used.<br />
# In style XFs, flag==0 means the attribute setting is valid; flag==1 means
# the attribute should be ignored.<br />
# Note that the API
# provides both "raw" XFs and "computed" XFs -- in the latter case, cell XFs
# have had the above inheritance mechanism applied.
# </p>

class XF(BaseObject):

    ##
    # 0 = cell XF, 1 = style XF
    is_style = 0
    ##
    # cell XF: Index into Book.xf_list
    # of this XF's style XF<br />
    # style XF: 0xFFF
    parent_style_index = 0
    ##
    #
    _format_flag = 0
    ##
    #
    _font_flag = 0
    ##
    #
    _alignment_flag = 0
    ##
    #
    _border_flag = 0
    ##
    #
    _background_flag = 0
    ##
    # &nbsp;
    _protection_flag = 0
    ##
    # Index into Book.xf_list
    xf_index = 0
    ##
    # Index into Book.font_list
    font_index = 0
    ##
    # Key into Book.format_map
    # <p>
    # Warning: OOo docs on the XF record call this "Index to FORMAT record".
    # It is not an index in the Python sense. It is a key to a map.
    # It is true <i>only</i> for Excel 4.0 and earlier files
    # that the key into format_map from an XF instance
    # is the same as the index into format_list, and <i>only</i>
    # if the index is less than 164.
    # </p>
    format_key = 0
    ##
    # An instance of an XFProtection object.
    protection = None
    ##
    # An instance of an XFBackground object.
    background = None
    ##
    # An instance of an XFAlignment object.
    alignment = None
    ##
    # An instance of an XFBorder object.
    border = None

########NEW FILE########
__FILENAME__ = formula
# -*- coding: cp1252 -*-

##
# Module for parsing/evaluating Microsoft Excel formulas.
#
# <p>Copyright  2005-2009 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under
# a BSD-style licence.</p>
##

# No part of the content of this file was derived from the works of David Giffin.

import copy
from struct import unpack
from timemachine import *
from biffh import unpack_unicode_update_pos, unpack_string_update_pos, \
    XLRDError, hex_char_dump, error_text_from_code, BaseObject

__all__ = [
    'oBOOL', 'oERR', 'oNUM', 'oREF', 'oREL', 'oSTRG', 'oUNK',
    'decompile_formula',
    'dump_formula',
    'evaluate_name_formula',
    'okind_dict',
    'rangename3d', 'rangename3drel', 'cellname', 'cellnameabs', 'colname',
    ]

# sztabN[opcode] -> the number of bytes to consume.
# -1 means variable
# -2 means this opcode not implemented in this version.
# Which N to use? Depends on biff_version; see szdict.
sztab0 = [-2, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -2, -1, 8, 4, 2, 2, 3, 9, 8, 2, 3, 8, 4, 7, 5, 5, 5, 2, 4, 7, 4, 7, 2, 2, -2, -2, -2, -2, -2, -2, -2, -2, 3, -2, -2, -2, -2, -2, -2, -2]
sztab1 = [-2, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -2, -1, 11, 5, 2, 2, 3, 9, 9, 2, 3, 11, 4, 7, 7, 7, 7, 3, 4, 7, 4, 7, 3, 3, -2, -2, -2, -2, -2, -2, -2, -2, 3, -2, -2, -2, -2, -2, -2, -2]
sztab2 = [-2, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -2, -1, 11, 5, 2, 2, 3, 9, 9, 3, 4, 11, 4, 7, 7, 7, 7, 3, 4, 7, 4, 7, 3, 3, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]
sztab3 = [-2, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -2, -1, -2, -2, 2, 2, 3, 9, 9, 3, 4, 15, 4, 7, 7, 7, 7, 3, 4, 7, 4, 7, 3, 3, -2, -2, -2, -2, -2, -2, -2, -2, -2, 25, 18, 21, 18, 21, -2, -2]
sztab4 = [-2, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -2, -2, 2, 2, 3, 9, 9, 3, 4, 5, 5, 9, 7, 7, 7, 3, 5, 9, 5, 9, 3, 3, -2, -2, -2, -2, -2, -2, -2, -2, -2, 7, 7, 11, 7, 11, -2, -2]

szdict = {
    20 : sztab0,
    30 : sztab1,
    40 : sztab2,
    45 : sztab2,
    50 : sztab3,
    70 : sztab3,
    80 : sztab4,
    }

# For debugging purposes ... the name for each opcode
# (without the prefix "t" used on OOo docs)
onames = ['Unk00', 'Exp', 'Tbl', 'Add', 'Sub', 'Mul', 'Div', 'Power', 'Concat', 'LT', 'LE', 'EQ', 'GE', 'GT', 'NE', 'Isect', 'List', 'Range', 'Uplus', 'Uminus', 'Percent', 'Paren', 'MissArg', 'Str', 'Extended', 'Attr', 'Sheet', 'EndSheet', 'Err', 'Bool', 'Int', 'Num', 'Array', 'Func', 'FuncVar', 'Name', 'Ref', 'Area', 'MemArea', 'MemErr', 'MemNoMem', 'MemFunc', 'RefErr', 'AreaErr', 'RefN', 'AreaN', 'MemAreaN', 'MemNoMemN', '', '', '', '', '', '', '', '', 'FuncCE', 'NameX', 'Ref3d', 'Area3d', 'RefErr3d', 'AreaErr3d', '', '']

func_defs = {
    # index: (name, min#args, max#args, flags, #known_args, return_type, kargs)
    0  : ('COUNT',            0, 30, 0x04,  1, 'V', 'R'),
    1  : ('IF',               2,  3, 0x04,  3, 'V', 'VRR'),
    2  : ('ISNA',             1,  1, 0x02,  1, 'V', 'V'),
    3  : ('ISERROR',          1,  1, 0x02,  1, 'V', 'V'),
    4  : ('SUM',              0, 30, 0x04,  1, 'V', 'R'),
    5  : ('AVERAGE',          1, 30, 0x04,  1, 'V', 'R'),
    6  : ('MIN',              1, 30, 0x04,  1, 'V', 'R'),
    7  : ('MAX',              1, 30, 0x04,  1, 'V', 'R'),
    8  : ('ROW',              0,  1, 0x04,  1, 'V', 'R'),
    9  : ('COLUMN',           0,  1, 0x04,  1, 'V', 'R'),
    10 : ('NA',               0,  0, 0x02,  0, 'V', ''),
    11 : ('NPV',              2, 30, 0x04,  2, 'V', 'VR'),
    12 : ('STDEV',            1, 30, 0x04,  1, 'V', 'R'),
    13 : ('DOLLAR',           1,  2, 0x04,  1, 'V', 'V'),
    14 : ('FIXED',            2,  3, 0x04,  3, 'V', 'VVV'),
    15 : ('SIN',              1,  1, 0x02,  1, 'V', 'V'),
    16 : ('COS',              1,  1, 0x02,  1, 'V', 'V'),
    17 : ('TAN',              1,  1, 0x02,  1, 'V', 'V'),
    18 : ('ATAN',             1,  1, 0x02,  1, 'V', 'V'),
    19 : ('PI',               0,  0, 0x02,  0, 'V', ''),
    20 : ('SQRT',             1,  1, 0x02,  1, 'V', 'V'),
    21 : ('EXP',              1,  1, 0x02,  1, 'V', 'V'),
    22 : ('LN',               1,  1, 0x02,  1, 'V', 'V'),
    23 : ('LOG10',            1,  1, 0x02,  1, 'V', 'V'),
    24 : ('ABS',              1,  1, 0x02,  1, 'V', 'V'),
    25 : ('INT',              1,  1, 0x02,  1, 'V', 'V'),
    26 : ('SIGN',             1,  1, 0x02,  1, 'V', 'V'),
    27 : ('ROUND',            2,  2, 0x02,  2, 'V', 'VV'),
    28 : ('LOOKUP',           2,  3, 0x04,  2, 'V', 'VR'),
    29 : ('INDEX',            2,  4, 0x0c,  4, 'R', 'RVVV'),
    30 : ('REPT',             2,  2, 0x02,  2, 'V', 'VV'),
    31 : ('MID',              3,  3, 0x02,  3, 'V', 'VVV'),
    32 : ('LEN',              1,  1, 0x02,  1, 'V', 'V'),
    33 : ('VALUE',            1,  1, 0x02,  1, 'V', 'V'),
    34 : ('TRUE',             0,  0, 0x02,  0, 'V', ''),
    35 : ('FALSE',            0,  0, 0x02,  0, 'V', ''),
    36 : ('AND',              1, 30, 0x04,  1, 'V', 'R'),
    37 : ('OR',               1, 30, 0x04,  1, 'V', 'R'),
    38 : ('NOT',              1,  1, 0x02,  1, 'V', 'V'),
    39 : ('MOD',              2,  2, 0x02,  2, 'V', 'VV'),
    40 : ('DCOUNT',           3,  3, 0x02,  3, 'V', 'RRR'),
    41 : ('DSUM',             3,  3, 0x02,  3, 'V', 'RRR'),
    42 : ('DAVERAGE',         3,  3, 0x02,  3, 'V', 'RRR'),
    43 : ('DMIN',             3,  3, 0x02,  3, 'V', 'RRR'),
    44 : ('DMAX',             3,  3, 0x02,  3, 'V', 'RRR'),
    45 : ('DSTDEV',           3,  3, 0x02,  3, 'V', 'RRR'),
    46 : ('VAR',              1, 30, 0x04,  1, 'V', 'R'),
    47 : ('DVAR',             3,  3, 0x02,  3, 'V', 'RRR'),
    48 : ('TEXT',             2,  2, 0x02,  2, 'V', 'VV'),
    49 : ('LINEST',           1,  4, 0x04,  4, 'A', 'RRVV'),
    50 : ('TREND',            1,  4, 0x04,  4, 'A', 'RRRV'),
    51 : ('LOGEST',           1,  4, 0x04,  4, 'A', 'RRVV'),
    52 : ('GROWTH',           1,  4, 0x04,  4, 'A', 'RRRV'),
    56 : ('PV',               3,  5, 0x04,  5, 'V', 'VVVVV'),
    57 : ('FV',               3,  5, 0x04,  5, 'V', 'VVVVV'),
    58 : ('NPER',             3,  5, 0x04,  5, 'V', 'VVVVV'),
    59 : ('PMT',              3,  5, 0x04,  5, 'V', 'VVVVV'),
    60 : ('RATE',             3,  6, 0x04,  6, 'V', 'VVVVVV'),
    61 : ('MIRR',             3,  3, 0x02,  3, 'V', 'RVV'),
    62 : ('IRR',              1,  2, 0x04,  2, 'V', 'RV'),
    63 : ('RAND',             0,  0, 0x0a,  0, 'V', ''),
    64 : ('MATCH',            2,  3, 0x04,  3, 'V', 'VRR'),
    65 : ('DATE',             3,  3, 0x02,  3, 'V', 'VVV'),
    66 : ('TIME',             3,  3, 0x02,  3, 'V', 'VVV'),
    67 : ('DAY',              1,  1, 0x02,  1, 'V', 'V'),
    68 : ('MONTH',            1,  1, 0x02,  1, 'V', 'V'),
    69 : ('YEAR',             1,  1, 0x02,  1, 'V', 'V'),
    70 : ('WEEKDAY',          1,  2, 0x04,  2, 'V', 'VV'),
    71 : ('HOUR',             1,  1, 0x02,  1, 'V', 'V'),
    72 : ('MINUTE',           1,  1, 0x02,  1, 'V', 'V'),
    73 : ('SECOND',           1,  1, 0x02,  1, 'V', 'V'),
    74 : ('NOW',              0,  0, 0x0a,  0, 'V', ''),
    75 : ('AREAS',            1,  1, 0x02,  1, 'V', 'R'),
    76 : ('ROWS',             1,  1, 0x02,  1, 'V', 'R'),
    77 : ('COLUMNS',          1,  1, 0x02,  1, 'V', 'R'),
    78 : ('OFFSET',           3,  5, 0x04,  5, 'R', 'RVVVV'),
    82 : ('SEARCH',           2,  3, 0x04,  3, 'V', 'VVV'),
    83 : ('TRANSPOSE',        1,  1, 0x02,  1, 'A', 'A'),
    86 : ('TYPE',             1,  1, 0x02,  1, 'V', 'V'),
    92 : ('SERIESSUM',        4,  4, 0x02,  4, 'V', 'VVVA'),
    97 : ('ATAN2',            2,  2, 0x02,  2, 'V', 'VV'),
    98 : ('ASIN',             1,  1, 0x02,  1, 'V', 'V'),
    99 : ('ACOS',             1,  1, 0x02,  1, 'V', 'V'),
    100: ('CHOOSE',           2, 30, 0x04,  2, 'V', 'VR'),
    101: ('HLOOKUP',          3,  4, 0x04,  4, 'V', 'VRRV'),
    102: ('VLOOKUP',          3,  4, 0x04,  4, 'V', 'VRRV'),
    105: ('ISREF',            1,  1, 0x02,  1, 'V', 'R'),
    109: ('LOG',              1,  2, 0x04,  2, 'V', 'VV'),
    111: ('CHAR',             1,  1, 0x02,  1, 'V', 'V'),
    112: ('LOWER',            1,  1, 0x02,  1, 'V', 'V'),
    113: ('UPPER',            1,  1, 0x02,  1, 'V', 'V'),
    114: ('PROPER',           1,  1, 0x02,  1, 'V', 'V'),
    115: ('LEFT',             1,  2, 0x04,  2, 'V', 'VV'),
    116: ('RIGHT',            1,  2, 0x04,  2, 'V', 'VV'),
    117: ('EXACT',            2,  2, 0x02,  2, 'V', 'VV'),
    118: ('TRIM',             1,  1, 0x02,  1, 'V', 'V'),
    119: ('REPLACE',          4,  4, 0x02,  4, 'V', 'VVVV'),
    120: ('SUBSTITUTE',       3,  4, 0x04,  4, 'V', 'VVVV'),
    121: ('CODE',             1,  1, 0x02,  1, 'V', 'V'),
    124: ('FIND',             2,  3, 0x04,  3, 'V', 'VVV'),
    125: ('CELL',             1,  2, 0x0c,  2, 'V', 'VR'),
    126: ('ISERR',            1,  1, 0x02,  1, 'V', 'V'),
    127: ('ISTEXT',           1,  1, 0x02,  1, 'V', 'V'),
    128: ('ISNUMBER',         1,  1, 0x02,  1, 'V', 'V'),
    129: ('ISBLANK',          1,  1, 0x02,  1, 'V', 'V'),
    130: ('T',                1,  1, 0x02,  1, 'V', 'R'),
    131: ('N',                1,  1, 0x02,  1, 'V', 'R'),
    140: ('DATEVALUE',        1,  1, 0x02,  1, 'V', 'V'),
    141: ('TIMEVALUE',        1,  1, 0x02,  1, 'V', 'V'),
    142: ('SLN',              3,  3, 0x02,  3, 'V', 'VVV'),
    143: ('SYD',              4,  4, 0x02,  4, 'V', 'VVVV'),
    144: ('DDB',              4,  5, 0x04,  5, 'V', 'VVVVV'),
    148: ('INDIRECT',         1,  2, 0x0c,  2, 'R', 'VV'),
    162: ('CLEAN',            1,  1, 0x02,  1, 'V', 'V'),
    163: ('MDETERM',          1,  1, 0x02,  1, 'V', 'A'),
    164: ('MINVERSE',         1,  1, 0x02,  1, 'A', 'A'),
    165: ('MMULT',            2,  2, 0x02,  2, 'A', 'AA'),
    167: ('IPMT',             4,  6, 0x04,  6, 'V', 'VVVVVV'),
    168: ('PPMT',             4,  6, 0x04,  6, 'V', 'VVVVVV'),
    169: ('COUNTA',           0, 30, 0x04,  1, 'V', 'R'),
    183: ('PRODUCT',          0, 30, 0x04,  1, 'V', 'R'),
    184: ('FACT',             1,  1, 0x02,  1, 'V', 'V'),
    189: ('DPRODUCT',         3,  3, 0x02,  3, 'V', 'RRR'),
    190: ('ISNONTEXT',        1,  1, 0x02,  1, 'V', 'V'),
    193: ('STDEVP',           1, 30, 0x04,  1, 'V', 'R'),
    194: ('VARP',             1, 30, 0x04,  1, 'V', 'R'),
    195: ('DSTDEVP',          3,  3, 0x02,  3, 'V', 'RRR'),
    196: ('DVARP',            3,  3, 0x02,  3, 'V', 'RRR'),
    197: ('TRUNC',            1,  2, 0x04,  2, 'V', 'VV'),
    198: ('ISLOGICAL',        1,  1, 0x02,  1, 'V', 'V'),
    199: ('DCOUNTA',          3,  3, 0x02,  3, 'V', 'RRR'),
    204: ('USDOLLAR',         1,  2, 0x04,  2, 'V', 'VV'),
    205: ('FINDB',            2,  3, 0x04,  3, 'V', 'VVV'),
    206: ('SEARCHB',          2,  3, 0x04,  3, 'V', 'VVV'),
    207: ('REPLACEB',         4,  4, 0x02,  4, 'V', 'VVVV'),
    208: ('LEFTB',            1,  2, 0x04,  2, 'V', 'VV'),
    209: ('RIGHTB',           1,  2, 0x04,  2, 'V', 'VV'),
    210: ('MIDB',             3,  3, 0x02,  3, 'V', 'VVV'),
    211: ('LENB',             1,  1, 0x02,  1, 'V', 'V'),
    212: ('ROUNDUP',          2,  2, 0x02,  2, 'V', 'VV'),
    213: ('ROUNDDOWN',        2,  2, 0x02,  2, 'V', 'VV'),
    214: ('ASC',              1,  1, 0x02,  1, 'V', 'V'),
    215: ('DBCS',             1,  1, 0x02,  1, 'V', 'V'),
    216: ('RANK',             2,  3, 0x04,  3, 'V', 'VRV'),
    219: ('ADDRESS',          2,  5, 0x04,  5, 'V', 'VVVVV'),
    220: ('DAYS360',          2,  3, 0x04,  3, 'V', 'VVV'),
    221: ('TODAY',            0,  0, 0x0a,  0, 'V', ''),
    222: ('VDB',              5,  7, 0x04,  7, 'V', 'VVVVVVV'),
    227: ('MEDIAN',           1, 30, 0x04,  1, 'V', 'R'),
    228: ('SUMPRODUCT',       1, 30, 0x04,  1, 'V', 'A'),
    229: ('SINH',             1,  1, 0x02,  1, 'V', 'V'),
    230: ('COSH',             1,  1, 0x02,  1, 'V', 'V'),
    231: ('TANH',             1,  1, 0x02,  1, 'V', 'V'),
    232: ('ASINH',            1,  1, 0x02,  1, 'V', 'V'),
    233: ('ACOSH',            1,  1, 0x02,  1, 'V', 'V'),
    234: ('ATANH',            1,  1, 0x02,  1, 'V', 'V'),
    235: ('DGET',             3,  3, 0x02,  3, 'V', 'RRR'),
    244: ('INFO',             1,  1, 0x02,  1, 'V', 'V'),
    247: ('DB',               4,  5, 0x04,  5, 'V', 'VVVVV'),
    252: ('FREQUENCY',        2,  2, 0x02,  2, 'A', 'RR'),
    261: ('ERROR.TYPE',       1,  1, 0x02,  1, 'V', 'V'),
    269: ('AVEDEV',           1, 30, 0x04,  1, 'V', 'R'),
    270: ('BETADIST',         3,  5, 0x04,  1, 'V', 'V'),
    271: ('GAMMALN',          1,  1, 0x02,  1, 'V', 'V'),
    272: ('BETAINV',          3,  5, 0x04,  1, 'V', 'V'),
    273: ('BINOMDIST',        4,  4, 0x02,  4, 'V', 'VVVV'),
    274: ('CHIDIST',          2,  2, 0x02,  2, 'V', 'VV'),
    275: ('CHIINV',           2,  2, 0x02,  2, 'V', 'VV'),
    276: ('COMBIN',           2,  2, 0x02,  2, 'V', 'VV'),
    277: ('CONFIDENCE',       3,  3, 0x02,  3, 'V', 'VVV'),
    278: ('CRITBINOM',        3,  3, 0x02,  3, 'V', 'VVV'),
    279: ('EVEN',             1,  1, 0x02,  1, 'V', 'V'),
    280: ('EXPONDIST',        3,  3, 0x02,  3, 'V', 'VVV'),
    281: ('FDIST',            3,  3, 0x02,  3, 'V', 'VVV'),
    282: ('FINV',             3,  3, 0x02,  3, 'V', 'VVV'),
    283: ('FISHER',           1,  1, 0x02,  1, 'V', 'V'),
    284: ('FISHERINV',        1,  1, 0x02,  1, 'V', 'V'),
    285: ('FLOOR',            2,  2, 0x02,  2, 'V', 'VV'),
    286: ('GAMMADIST',        4,  4, 0x02,  4, 'V', 'VVVV'),
    287: ('GAMMAINV',         3,  3, 0x02,  3, 'V', 'VVV'),
    288: ('CEILING',          2,  2, 0x02,  2, 'V', 'VV'),
    289: ('HYPGEOMDIST',      4,  4, 0x02,  4, 'V', 'VVVV'),
    290: ('LOGNORMDIST',      3,  3, 0x02,  3, 'V', 'VVV'),
    291: ('LOGINV',           3,  3, 0x02,  3, 'V', 'VVV'),
    292: ('NEGBINOMDIST',     3,  3, 0x02,  3, 'V', 'VVV'),
    293: ('NORMDIST',         4,  4, 0x02,  4, 'V', 'VVVV'),
    294: ('NORMSDIST',        1,  1, 0x02,  1, 'V', 'V'),
    295: ('NORMINV',          3,  3, 0x02,  3, 'V', 'VVV'),
    296: ('NORMSINV',         1,  1, 0x02,  1, 'V', 'V'),
    297: ('STANDARDIZE',      3,  3, 0x02,  3, 'V', 'VVV'),
    298: ('ODD',              1,  1, 0x02,  1, 'V', 'V'),
    299: ('PERMUT',           2,  2, 0x02,  2, 'V', 'VV'),
    300: ('POISSON',          3,  3, 0x02,  3, 'V', 'VVV'),
    301: ('TDIST',            3,  3, 0x02,  3, 'V', 'VVV'),
    302: ('WEIBULL',          4,  4, 0x02,  4, 'V', 'VVVV'),
    303: ('SUMXMY2',          2,  2, 0x02,  2, 'V', 'AA'),
    304: ('SUMX2MY2',         2,  2, 0x02,  2, 'V', 'AA'),
    305: ('SUMX2PY2',         2,  2, 0x02,  2, 'V', 'AA'),
    306: ('CHITEST',          2,  2, 0x02,  2, 'V', 'AA'),
    307: ('CORREL',           2,  2, 0x02,  2, 'V', 'AA'),
    308: ('COVAR',            2,  2, 0x02,  2, 'V', 'AA'),
    309: ('FORECAST',         3,  3, 0x02,  3, 'V', 'VAA'),
    310: ('FTEST',            2,  2, 0x02,  2, 'V', 'AA'),
    311: ('INTERCEPT',        2,  2, 0x02,  2, 'V', 'AA'),
    312: ('PEARSON',          2,  2, 0x02,  2, 'V', 'AA'),
    313: ('RSQ',              2,  2, 0x02,  2, 'V', 'AA'),
    314: ('STEYX',            2,  2, 0x02,  2, 'V', 'AA'),
    315: ('SLOPE',            2,  2, 0x02,  2, 'V', 'AA'),
    316: ('TTEST',            4,  4, 0x02,  4, 'V', 'AAVV'),
    317: ('PROB',             3,  4, 0x04,  3, 'V', 'AAV'),
    318: ('DEVSQ',            1, 30, 0x04,  1, 'V', 'R'),
    319: ('GEOMEAN',          1, 30, 0x04,  1, 'V', 'R'),
    320: ('HARMEAN',          1, 30, 0x04,  1, 'V', 'R'),
    321: ('SUMSQ',            0, 30, 0x04,  1, 'V', 'R'),
    322: ('KURT',             1, 30, 0x04,  1, 'V', 'R'),
    323: ('SKEW',             1, 30, 0x04,  1, 'V', 'R'),
    324: ('ZTEST',            2,  3, 0x04,  2, 'V', 'RV'),
    325: ('LARGE',            2,  2, 0x02,  2, 'V', 'RV'),
    326: ('SMALL',            2,  2, 0x02,  2, 'V', 'RV'),
    327: ('QUARTILE',         2,  2, 0x02,  2, 'V', 'RV'),
    328: ('PERCENTILE',       2,  2, 0x02,  2, 'V', 'RV'),
    329: ('PERCENTRANK',      2,  3, 0x04,  2, 'V', 'RV'),
    330: ('MODE',             1, 30, 0x04,  1, 'V', 'A'),
    331: ('TRIMMEAN',         2,  2, 0x02,  2, 'V', 'RV'),
    332: ('TINV',             2,  2, 0x02,  2, 'V', 'VV'),
    336: ('CONCATENATE',      0, 30, 0x04,  1, 'V', 'V'),
    337: ('POWER',            2,  2, 0x02,  2, 'V', 'VV'),
    342: ('RADIANS',          1,  1, 0x02,  1, 'V', 'V'),
    343: ('DEGREES',          1,  1, 0x02,  1, 'V', 'V'),
    344: ('SUBTOTAL',         2, 30, 0x04,  2, 'V', 'VR'),
    345: ('SUMIF',            2,  3, 0x04,  3, 'V', 'RVR'),
    346: ('COUNTIF',          2,  2, 0x02,  2, 'V', 'RV'),
    347: ('COUNTBLANK',       1,  1, 0x02,  1, 'V', 'R'),
    350: ('ISPMT',            4,  4, 0x02,  4, 'V', 'VVVV'),
    351: ('DATEDIF',          3,  3, 0x02,  3, 'V', 'VVV'),
    352: ('DATESTRING',       1,  1, 0x02,  1, 'V', 'V'),
    353: ('NUMBERSTRING',     2,  2, 0x02,  2, 'V', 'VV'),
    354: ('ROMAN',            1,  2, 0x04,  2, 'V', 'VV'),
    358: ('GETPIVOTDATA',     2,  2, 0x02,  2, 'V', 'RV'),
    359: ('HYPERLINK',        1,  2, 0x04,  2, 'V', 'VV'),
    360: ('PHONETIC',         1,  1, 0x02,  1, 'V', 'V'),
    361: ('AVERAGEA',         1, 30, 0x04,  1, 'V', 'R'),
    362: ('MAXA',             1, 30, 0x04,  1, 'V', 'R'),
    363: ('MINA',             1, 30, 0x04,  1, 'V', 'R'),
    364: ('STDEVPA',          1, 30, 0x04,  1, 'V', 'R'),
    365: ('VARPA',            1, 30, 0x04,  1, 'V', 'R'),
    366: ('STDEVA',           1, 30, 0x04,  1, 'V', 'R'),
    367: ('VARA',             1, 30, 0x04,  1, 'V', 'R'),
    368: ('BAHTTEXT',         1,  1, 0x02,  1, 'V', 'V'),
    369: ('THAIDAYOFWEEK',    1,  1, 0x02,  1, 'V', 'V'),
    370: ('THAIDIGIT',        1,  1, 0x02,  1, 'V', 'V'),
    371: ('THAIMONTHOFYEAR',  1,  1, 0x02,  1, 'V', 'V'),
    372: ('THAINUMSOUND',     1,  1, 0x02,  1, 'V', 'V'),
    373: ('THAINUMSTRING',    1,  1, 0x02,  1, 'V', 'V'),
    374: ('THAISTRINGLENGTH', 1,  1, 0x02,  1, 'V', 'V'),
    375: ('ISTHAIDIGIT',      1,  1, 0x02,  1, 'V', 'V'),
    376: ('ROUNDBAHTDOWN',    1,  1, 0x02,  1, 'V', 'V'),
    377: ('ROUNDBAHTUP',      1,  1, 0x02,  1, 'V', 'V'),
    378: ('THAIYEAR',         1,  1, 0x02,  1, 'V', 'V'),
    379: ('RTD',              2,  5, 0x04,  1, 'V', 'V'),
    }

tAttrNames = {
    0x00: "Skip??", # seen in SAMPLES.XLS which shipped with Excel 5.0
    0x01: "Volatile",
    0x02: "If",
    0x04: "Choose",
    0x08: "Skip",
    0x10: "Sum",
    0x20: "Assign",
    0x40: "Space",
    0x41: "SpaceVolatile",
    }

_error_opcodes = {}
for _x in [0x07, 0x08, 0x0A, 0x0B, 0x1C, 0x1D, 0x2F]:
    _error_opcodes[_x] = 1
is_error_opcode = _error_opcodes.has_key

tRangeFuncs = (min, max, min, max, min, max)
tIsectFuncs = (max, min, max, min, max, min)

def do_box_funcs(box_funcs, boxa, boxb):
    return tuple([
        func(numa, numb)
        for func, numa, numb in zip(box_funcs, boxa.coords, boxb.coords)
        ])

def adjust_cell_addr_biff8(rowval, colval, reldelta, browx=None, bcolx=None):
    row_rel = (colval >> 15) & 1
    col_rel = (colval >> 14) & 1
    rowx = rowval
    colx = colval & 0xff
    if reldelta:
        if row_rel and rowx >= 32768:
            rowx -= 65536
        if col_rel and colx >= 128:
            colx -= 256
    else:
        if row_rel:
            rowx -= browx
        if col_rel:
            colx -= bcolx
    return rowx, colx, row_rel, col_rel

def adjust_cell_addr_biff_le7(
        rowval, colval, reldelta, browx=None, bcolx=None):
    row_rel = (rowval >> 15) & 1
    col_rel = (rowval >> 14) & 1
    rowx = rowval & 0x3fff
    colx = colval
    if reldelta:
        if row_rel and rowx >= 8192:
            rowx -= 16384
        if col_rel and colx >= 128:
            colx -= 256
    else:
        if row_rel:
            rowx -= browx
        if col_rel:
            colx -= bcolx
    return rowx, colx, row_rel, col_rel

def get_cell_addr(data, pos, bv, reldelta, browx=None, bcolx=None):
    if bv >= 80:
        rowval, colval = unpack("<HH", data[pos:pos+4])
        # print "    rv=%04xh cv=%04xh" % (rowval, colval)
        return adjust_cell_addr_biff8(rowval, colval, reldelta, browx, bcolx)
    else:
        rowval, colval = unpack("<HB", data[pos:pos+3])
        # print "    rv=%04xh cv=%04xh" % (rowval, colval)
        return adjust_cell_addr_biff_le7(
                    rowval, colval, reldelta, browx, bcolx)

def get_cell_range_addr(data, pos, bv, reldelta, browx=None, bcolx=None):
    if bv >= 80:
        row1val, row2val, col1val, col2val = unpack("<HHHH", data[pos:pos+8])
        # print "    rv=%04xh cv=%04xh" % (row1val, col1val)
        # print "    rv=%04xh cv=%04xh" % (row2val, col2val)
        res1 = adjust_cell_addr_biff8(row1val, col1val, reldelta, browx, bcolx)
        res2 = adjust_cell_addr_biff8(row2val, col2val, reldelta, browx, bcolx)
        return res1, res2
    else:
        row1val, row2val, col1val, col2val = unpack("<HHBB", data[pos:pos+6])
        # print "    rv=%04xh cv=%04xh" % (row1val, col1val)
        # print "    rv=%04xh cv=%04xh" % (row2val, col2val)
        res1 = adjust_cell_addr_biff_le7(
                    row1val, col1val, reldelta, browx, bcolx)
        res2 = adjust_cell_addr_biff_le7(
                    row2val, col2val, reldelta, browx, bcolx)
        return res1, res2

def get_externsheet_local_range(bk, refx, blah=0):
    try:
        info = bk._externsheet_info[refx]
    except IndexError:
        print "!!! get_externsheet_local_range: refx=%d, not in range(%d)" \
            % (refx, len(bk._externsheet_info))
        return (-101, -101)
    ref_recordx, ref_first_sheetx, ref_last_sheetx = info
    if ref_recordx == bk._supbook_addins_inx:
        if blah:
            print "/// get_externsheet_local_range(refx=%d) -> addins %r" % (refx, info)
        assert ref_first_sheetx == 0xFFFE == ref_last_sheetx
        return (-5, -5)
    if ref_recordx != bk._supbook_locals_inx:
        if blah:
            print "/// get_externsheet_local_range(refx=%d) -> external %r" % (refx, info)
        return (-4, -4) # external reference
    if ref_first_sheetx == 0xFFFE == ref_last_sheetx:
        if blah:
            print "/// get_externsheet_local_range(refx=%d) -> unspecified sheet %r" % (refx, info)
        return (-1, -1) # internal reference, any sheet
    if ref_first_sheetx == 0xFFFF == ref_last_sheetx:
        if blah:
            print "/// get_externsheet_local_range(refx=%d) -> deleted sheet(s)" % (refx, )
        return (-2, -2) # internal reference, deleted sheet(s)
    nsheets = len(bk._all_sheets_map)
    if not(0 <= ref_first_sheetx <= ref_last_sheetx < nsheets):
        if blah:
            print "/// get_externsheet_local_range(refx=%d) -> %r" % (refx, info)
            print "--- first/last sheet not in range(%d)" % nsheets
        return (-102, -102) # stuffed up somewhere :-(
    xlrd_sheetx1 = bk._all_sheets_map[ref_first_sheetx]
    xlrd_sheetx2 = bk._all_sheets_map[ref_last_sheetx]
    if not(0 <= xlrd_sheetx1 <= xlrd_sheetx2):
        return (-3, -3) # internal reference, but to a macro sheet
    return xlrd_sheetx1, xlrd_sheetx2

def get_externsheet_local_range_b57(
        bk, raw_extshtx, ref_first_sheetx, ref_last_sheetx, blah=0):
    if raw_extshtx > 0:
        if blah:
            print "/// get_externsheet_local_range_b57(raw_extshtx=%d) -> external" % raw_extshtx
        return (-4, -4) # external reference
    if ref_first_sheetx == -1 and ref_last_sheetx == -1:
        return (-2, -2) # internal reference, deleted sheet(s)
    nsheets = len(bk._all_sheets_map)
    if not(0 <= ref_first_sheetx <= ref_last_sheetx < nsheets):
        if blah:
            print "/// get_externsheet_local_range_b57(%d, %d, %d) -> ???" \
                % (raw_extshtx, ref_first_sheetx, ref_last_sheetx)
            print "--- first/last sheet not in range(%d)" % nsheets
        return (-103, -103) # stuffed up somewhere :-(
    xlrd_sheetx1 = bk._all_sheets_map[ref_first_sheetx]
    xlrd_sheetx2 = bk._all_sheets_map[ref_last_sheetx]
    if not(0 <= xlrd_sheetx1 <= xlrd_sheetx2):
        return (-3, -3) # internal reference, but to a macro sheet
    return xlrd_sheetx1, xlrd_sheetx2

class FormulaError(Exception):
    pass

oBOOL = 3
oERR =  4
oMSNG = 5 # tMissArg
oNUM =  2
oREF = -1
oREL = -2
oSTRG = 1
oUNK =  0

okind_dict = {
    -2: "oREL",
    -1: "oREF",
    0 : "oUNK",
    1 : "oSTRG",
    2 : "oNUM",
    3 : "oBOOL",
    4 : "oERR",
    5 : "oMSNG",
    }

listsep = ',' #### probably should depend on locale

##
# Used in evaluating formulas.
# The following table describes the kinds and how their values
# are represented.</p>
#
# <table border="1" cellpadding="7">
# <tr>
# <th>Kind symbol</th>
# <th>Kind number</th>
# <th>Value representation</th>
# </tr>
# <tr>
# <td>oBOOL</td>
# <td align="center">3</td>
# <td>integer: 0 => False; 1 => True</td>
# </tr>
# <tr>
# <td>oERR</td>
# <td align="center">4</td>
# <td>None, or an int error code (same as XL_CELL_ERROR in the Cell class).
# </td>
# </tr>
# <tr>
# <td>oMSNG</td>
# <td align="center">5</td>
# <td>Used by Excel as a placeholder for a missing (not supplied) function
# argument. Should *not* appear as a final formula result. Value is None.</td>
# </tr>
# <tr>
# <td>oNUM</td>
# <td align="center">2</td>
# <td>A float. Note that there is no way of distinguishing dates.</td>
# </tr>
# <tr>
# <td>oREF</td>
# <td align="center">-1</td>
# <td>The value is either None or a non-empty list of
# absolute Ref3D instances.<br>
# </td>
# </tr>
# <tr>
# <td>oREL</td>
# <td align="center">-2</td>
# <td>The value is None or a non-empty list of
# fully or partially relative Ref3D instances.
# </td>
# </tr>
# <tr>
# <td>oSTRG</td>
# <td align="center">1</td>
# <td>A Unicode string.</td>
# </tr>
# <tr>
# <td>oUNK</td>
# <td align="center">0</td>
# <td>The kind is unknown or ambiguous. The value is None</td>
# </tr>
# </table>
#<p></p>

class Operand(object):

    ##
    # None means that the actual value of the operand is a variable
    # (depends on cell data), not a constant.
    value = None
    ##
    # oUNK means that the kind of operand is not known unambiguously.
    kind = oUNK
    ##
    # The reconstituted text of the original formula. Function names will be
    # in English irrespective of the original language, which doesn't seem
    # to be recorded anywhere. The separator is ",", not ";" or whatever else
    # might be more appropriate for the end-user's locale; patches welcome.
    text = '?'

    def __init__(self, akind=None, avalue=None, arank=0, atext='?'):
        if akind is not None:
            self.kind = akind
        if avalue is not None:
            self.value = avalue
        self.rank = arank
        # rank is an internal gizmo (operator precedence);
        # it's used in reconstructing formula text.
        self.text = atext

    def __repr__(self):
        kind_text = okind_dict.get(self.kind, "?Unknown kind?")
        return "Operand(kind=%s, value=%r, text=%r)" \
            % (kind_text, self.value, self.text)

if CAN_SUBCLASS_BUILTIN:
    _ref3d_base = tuple
else:
    _ref3d_base = object

##
# <p>Represents an absolute or relative 3-dimensional reference to a box
# of one or more cells.<br />
# -- New in version 0.6.0
# </p>
#
# <p>The <i>coords</i> attribute is a tuple of the form:<br />
# (shtxlo, shtxhi, rowxlo, rowxhi, colxlo, colxhi)<br />
# where 0 <= thingxlo <= thingx < thingxhi.<br />
# Note that it is quite possible to have thingx > nthings; for example
# Print_Titles could have colxhi == 256 and/or rowxhi == 65536
# irrespective of how many columns/rows are actually used in the worksheet.
# The caller will need to decide how to handle this situation.
# Keyword: IndexError :-)
# </p>
#
# <p>The components of the coords attribute are also available as individual
# attributes: shtxlo, shtxhi, rowxlo, rowxhi, colxlo, and colxhi.</p>
#
# <p>The <i>relflags</i> attribute is a 6-tuple of flags which indicate whether
# the corresponding (sheet|row|col)(lo|hi) is relative (1) or absolute (0).<br>
# Note that there is necessarily no information available as to what cell(s)
# the reference could possibly be relative to. The caller must decide what if
# any use to make of oREL operands. Note also that a partially relative
# reference may well be a typo.
# For example, define name A1Z10 as $a$1:$z10 (missing $ after z)
# while the cursor is on cell Sheet3!A27.<br>
# The resulting Ref3D instance will have coords = (2, 3, 0, -16, 0, 26)
# and relflags = (0, 0, 0, 1, 0, 0).<br>
# So far, only one possibility of a sheet-relative component in
# a reference has been noticed: a 2D reference located in the "current sheet".
# <br /> This will appear as coords = (0, 1, ...) and relflags = (1, 1, ...).

class Ref3D(_ref3d_base):

    def __init__(self, atuple):
        self.coords = atuple[0:6]
        self.relflags = atuple[6:12]
        if not self.relflags:
            self.relflags = (0, 0, 0, 0, 0, 0)
        (self.shtxlo, self.shtxhi,
        self.rowxlo, self.rowxhi,
        self.colxlo, self.colxhi) = self.coords

    def __repr__(self):
        if not self.relflags or self.relflags == (0, 0, 0, 0, 0, 0):
            return "Ref3D(coords=%r)" % (self.coords, )
        else:
            return "Ref3D(coords=%r, relflags=%r)" \
                % (self.coords, self.relflags)

tAdd = 0x03
tSub = 0x04
tMul = 0x05
tDiv = 0x06
tPower = 0x07
tConcat = 0x08
tLT, tLE, tEQ, tGE, tGT, tNE = range(0x09, 0x0F)

import operator as opr

def nop(x):
    return x

def _opr_pow(x, y): return x ** y

def _opr_lt(x, y): return x <  y
def _opr_le(x, y): return x <= y
def _opr_eq(x, y): return x == y
def _opr_ge(x, y): return x >= y
def _opr_gt(x, y): return x >  y
def _opr_ne(x, y): return x != y

def num2strg(num):
    """Attempt to emulate Excel's default conversion
       from number to string.
    """
    s = str(num)
    if s.endswith(".0"):
        s = s[:-2]
    return s

_arith_argdict = {oNUM: nop,     oSTRG: float}
_cmp_argdict =   {oNUM: nop,     oSTRG: nop}
# Seems no conversions done on relops; in Excel, "1" > 9 produces TRUE.
_strg_argdict =  {oNUM:num2strg, oSTRG:nop}
binop_rules = {
    tAdd:   (_arith_argdict, oNUM, opr.add,  30, '+'),
    tSub:   (_arith_argdict, oNUM, opr.sub,  30, '-'),
    tMul:   (_arith_argdict, oNUM, opr.mul,  40, '*'),
    tDiv:   (_arith_argdict, oNUM, opr.div,  40, '/'),
    tPower: (_arith_argdict, oNUM, _opr_pow, 50, '^',),
    tConcat:(_strg_argdict, oSTRG, opr.add,  20, '&'),
    tLT:    (_cmp_argdict, oBOOL, _opr_lt,   10, '<'),
    tLE:    (_cmp_argdict, oBOOL, _opr_le,   10, '<='),
    tEQ:    (_cmp_argdict, oBOOL, _opr_eq,   10, '='),
    tGE:    (_cmp_argdict, oBOOL, _opr_ge,   10, '>='),
    tGT:    (_cmp_argdict, oBOOL, _opr_gt,   10, '>'),
    tNE:    (_cmp_argdict, oBOOL, _opr_ne,   10, '<>'),
    }

unop_rules = {
    0x13: (lambda x: -x,        70, '-', ''), # unary minus
    0x12: (lambda x: x,         70, '+', ''), # unary plus
    0x14: (lambda x: x / 100.0, 60, '',  '%'),# percent
    }

LEAF_RANK = 90
FUNC_RANK = 90

STACK_ALARM_LEVEL = 5
STACK_PANIC_LEVEL = 10

def evaluate_name_formula(bk, nobj, namex, blah=0, level=0):
    if level > STACK_ALARM_LEVEL:
        blah = 1
    data = nobj.raw_formula
    fmlalen = nobj.basic_formula_len
    bv = bk.biff_version
    reldelta = 1 # All defined name formulas use "Method B" [OOo docs]
    if blah:
        print "::: evaluate_name_formula %r %r %d %d %r level=%d" \
            % (namex, nobj.name, fmlalen, bv, data, level)
        hex_char_dump(data, 0, fmlalen)
    if level > STACK_PANIC_LEVEL:
        raise XLRDError("Excessive indirect references in NAME formula")
    sztab = szdict[bv]
    pos = 0
    stack = []
    any_rel = 0
    any_err = 0
    any_external = 0
    unk_opnd = Operand(oUNK, None)
    error_opnd = Operand(oERR, None)
    spush = stack.append

    def do_binop(opcd, stk):
        assert len(stk) >= 2
        bop = stk.pop()
        aop = stk.pop()
        argdict, result_kind, func, rank, sym = binop_rules[opcd]
        otext = ''.join([
            '('[:aop.rank < rank],
            aop.text,
            ')'[:aop.rank < rank],
            sym,
            '('[:bop.rank < rank],
            bop.text,
            ')'[:bop.rank < rank],
            ])
        resop = Operand(result_kind, None, rank, otext)
        try:
            bconv = argdict[bop.kind]
            aconv = argdict[aop.kind]
        except KeyError:
            stk.append(resop)
            return
        if bop.value is None or aop.value is None:
            stk.append(resop)
            return
        bval = bconv(bop.value)
        aval = aconv(aop.value)
        result = func(aval, bval)
        if result_kind == oBOOL:
            result = intbool(result) # -> 1 or 0
        resop.value = result
        stk.append(resop)

    def do_unaryop(opcode, arglist, result_kind, stk):
        assert len(stk) >= 1
        aop = stk.pop()
        assert aop.kind in arglist
        val = aop.value
        func, rank, sym1, sym2 = unop_rules[opcode]
        otext = ''.join([
            sym1,
            '('[:aop.rank < rank],
            aop.text,
            ')'[:aop.rank < rank],
            sym2,
            ])
        if val is not None:
            val = func(val)
        stk.append(Operand(result_kind, val, rank, otext))

    def not_in_name_formula(op_arg, oname_arg):
        msg = "ERROR *** Token 0x%02x (%s) found in NAME formula" \
              % (op_arg, oname_arg)
        raise FormulaError(msg)

    if fmlalen == 0:
        stack = [unk_opnd]

    while 0 <= pos < fmlalen:
        op = ord(data[pos])
        opcode = op & 0x1f
        optype = (op & 0x60) >> 5
        if optype:
            opx = opcode + 32
        else:
            opx = opcode
        oname = onames[opx] # + [" RVA"][optype]
        sz = sztab[opx]
        if blah:
            print "Pos:%d Op:0x%02x Name:t%s Sz:%d opcode:%02xh optype:%02xh" \
                % (pos, op, oname, sz, opcode, optype)
            print "Stack =", stack
        if sz == -2:
            msg = 'ERROR *** Unexpected token 0x%02x ("%s"); biff_version=%d' \
                % (op, oname, bv)
            raise FormulaError(msg)
        if not optype:
            if 0x00 <= opcode <= 0x02: # unk_opnd, tExp, tTbl
                not_in_name_formula(op, oname)
            elif 0x03 <= opcode <= 0x0E:
                # Add, Sub, Mul, Div, Power
                # tConcat
                # tLT, ..., tNE
                do_binop(opcode, stack)
            elif opcode == 0x0F: # tIsect
                if blah: print >> bk.logfile, "tIsect pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ' '
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF)
                res.text = otext
                if bop.kind == oERR or aop.kind == oERR:
                    res.kind = oERR
                elif bop.kind == oUNK or aop.kind == oUNK:
                    # This can happen with undefined
                    # (go search in the current sheet) labels.
                    # For example =Bob Sales
                    # Each label gets a NAME record with an empty formula (!)
                    # Evaluation of the tName token classifies it as oUNK
                    # res.kind = oREF
                    pass
                elif bop.kind == oREF == aop.kind:
                    if aop.value is not None and bop.value is not None:
                        assert len(aop.value) == 1
                        assert len(bop.value) == 1
                        coords = do_box_funcs(
                            tIsectFuncs, aop.value[0], bop.value[0])
                        res.value = [Ref3D(coords)]
                elif bop.kind == oREL == aop.kind:
                    res.kind = oREL
                    if aop.value is not None and bop.value is not None:
                        assert len(aop.value) == 1
                        assert len(bop.value) == 1
                        coords = do_box_funcs(
                            tIsectFuncs, aop.value[0], bop.value[0])
                        relfa = aop.value[0].relflags
                        relfb = bop.value[0].relflags
                        if relfa == relfb:
                            res.value = [Ref3D(coords + relfa)]
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tIsect post", stack
            elif opcode == 0x10: # tList
                if blah: print >> bk.logfile, "tList pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ','
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF, None, rank, otext)
                if bop.kind == oERR or aop.kind == oERR:
                    res.kind = oERR
                elif bop.kind in (oREF, oREL) and aop.kind in (oREF, oREL):
                    res.kind = oREF
                    if aop.kind == oREL or bop.kind == oREL:
                        res.kind = oREL
                    if aop.value is not None and bop.value is not None:
                        assert len(aop.value) >= 1
                        assert len(bop.value) == 1
                        res.value = aop.value + bop.value
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tList post", stack
            elif opcode == 0x11: # tRange
                if blah: print >> bk.logfile, "tRange pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ':'
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF, None, rank, otext)
                if bop.kind == oERR or aop.kind == oERR:
                    res = oERR
                elif bop.kind == oREF == aop.kind:
                    if aop.value is not None and bop.value is not None:
                        assert len(aop.value) == 1
                        assert len(bop.value) == 1
                        coords = do_box_funcs(
                            tRangeFuncs, aop.value[0], bop.value[0])
                        res.value = [Ref3D(coords)]
                elif bop.kind == oREL == aop.kind:
                    res.kind = oREL
                    if aop.value is not None and bop.value is not None:
                        assert len(aop.value) == 1
                        assert len(bop.value) == 1
                        coords = do_box_funcs(
                            tRangeFuncs, aop.value[0], bop.value[0])
                        relfa = aop.value[0].relflags
                        relfb = bop.value[0].relflags
                        if relfa == relfb:
                            res.value = [Ref3D(coords + relfa)]
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tRange post", stack
            elif 0x12 <= opcode <= 0x14: # tUplus, tUminus, tPercent
                do_unaryop(opcode, (oUNK, oNUM,), oNUM, stack)
            elif opcode == 0x15: # tParen
                # source cosmetics
                pass
            elif opcode == 0x16: # tMissArg
                spush(Operand(oMSNG, None, LEAF_RANK, ''))
            elif opcode == 0x17: # tStr
                if bv <= 70:
                    strg, newpos = unpack_string_update_pos(
                                        data, pos+1, bk.encoding, lenlen=1)
                else:
                    strg, newpos = unpack_unicode_update_pos(
                                        data, pos+1, lenlen=1)
                sz = newpos - pos
                if blah: print >> bk.logfile, "   sz=%d strg=%r" % (sz, strg)
                text = '"' + strg.replace('"', '""') + '"'
                spush(Operand(oSTRG, strg, LEAF_RANK, text))
            elif opcode == 0x18: # tExtended
                # new with BIFF 8
                assert bv >= 80
                # not in OOo docs
                raise FormulaError("tExtended token not implemented")
            elif opcode == 0x19: # tAttr
                subop, nc = unpack("<BH", data[pos+1:pos+4])
                subname = tAttrNames.get(subop, "??Unknown??")
                if subop == 0x04: # Choose
                    sz = nc * 2 + 6
                elif subop == 0x10: # Sum (single arg)
                    sz = 4
                    if blah: print >> bk.logfile, "tAttrSum", stack
                    assert len(stack) >= 1
                    aop = stack[-1]
                    otext = 'SUM(%s)' % aop.text
                    stack[-1] = Operand(oNUM, None, FUNC_RANK, otext)
                else:
                    sz = 4
                if blah:
                    print "   subop=%02xh subname=t%s sz=%d nc=%02xh" \
                        % (subop, subname, sz, nc)
            elif 0x1A <= opcode <= 0x1B: # tSheet, tEndSheet
                assert bv < 50
                raise FormulaError("tSheet & tEndsheet tokens not implemented")
            elif 0x1C <= opcode <= 0x1F: # tErr, tBool, tInt, tNum
                inx = opcode - 0x1C
                nb = [1, 1, 2, 8][inx]
                kind = [oERR, oBOOL, oNUM, oNUM][inx]
                value, = unpack("<" + "BBHd"[inx], data[pos+1:pos+1+nb])
                if inx == 2: # tInt
                    value = float(value)
                    text = str(value)
                elif inx == 3: # tNum
                    text = str(value)
                elif inx == 1: # tBool
                    text = ('FALSE', 'TRUE')[value]
                else:
                    text = '"' +error_text_from_code[value] + '"'
                spush(Operand(kind, value, LEAF_RANK, text))
            else:
                raise FormulaError("Unhandled opcode: 0x%02x" % opcode)
            if sz <= 0:
                raise FormulaError("Size not set for opcode 0x%02x" % opcode)
            pos += sz
            continue
        if opcode == 0x00: # tArray
            spush(unk_opnd)
        elif opcode == 0x01: # tFunc
            nb = 1 + int(bv >= 40)
            funcx = unpack("<" + " BH"[nb], data[pos+1:pos+1+nb])[0]
            func_attrs = func_defs.get(funcx, None)
            if not func_attrs:
                print >> bk.logfile, "*** formula/tFunc unknown FuncID:%d" \
                      % funcx
                spush(unk_opnd)
            else:
                func_name, nargs = func_attrs[:2]
                if blah:
                    print "    FuncID=%d name=%s nargs=%d" \
                          % (funcx, func_name, nargs)
                assert len(stack) >= nargs
                argtext = listsep.join([arg.text for arg in stack[-nargs:]])
                otext = "%s(%s)" % (func_name, argtext)
                del stack[-nargs:]
                res = Operand(oUNK, None, FUNC_RANK, otext)
                spush(res)
        elif opcode == 0x02: #tFuncVar
            nb = 1 + int(bv >= 40)
            nargs, funcx = unpack("<B" + " BH"[nb], data[pos+1:pos+2+nb])
            prompt, nargs = divmod(nargs, 128)
            macro, funcx = divmod(funcx, 32768)
            if blah:
                print "   FuncID=%d nargs=%d macro=%d prompt=%d" \
                      % (funcx, nargs, macro, prompt)
            func_attrs = func_defs.get(funcx, None)
            if not func_attrs:
                print >> bk.logfile, "*** formula/tFuncVar unknown FuncID:%d" \
                      % funcx
                spush(unk_opnd)
            else:
                func_name, minargs, maxargs = func_attrs[:3]
                if blah:
                    print "    name: %r, min~max args: %d~%d" \
                        % (func_name, minargs, maxargs)
                assert minargs <= nargs <= maxargs
                assert len(stack) >= nargs
                assert len(stack) >= nargs
                argtext = listsep.join([arg.text for arg in stack[-nargs:]])
                otext = "%s(%s)" % (func_name, argtext)
                res = Operand(oUNK, None, FUNC_RANK, otext)
                if funcx == 1: # IF
                    testarg = stack[-nargs]
                    if testarg.kind not in (oNUM, oBOOL):
                        if blah and testarg.kind != oUNK:
                            print "IF testarg kind?"
                    elif testarg.value not in (0, 1):
                        if blah and testarg.value is not None:
                            print "IF testarg value?"
                    else:
                        if nargs == 2 and not testarg.value:
                            # IF(FALSE, tv) => FALSE
                            res.kind, res.value = oBOOL, 0
                        else:
                            respos = -nargs + 2 - int(testarg.value)
                            chosen = stack[respos]
                            if chosen.kind == oMSNG:
                                res.kind, res.value = oNUM, 0
                            else:
                                res.kind, res.value = chosen.kind, chosen.value
                        if blah:
                            print "$$$$$$ IF => constant"
                elif funcx == 100: # CHOOSE
                    testarg = stack[-nargs]
                    if testarg.kind == oNUM:
                        if 1 <= testarg.value < nargs:
                            chosen = stack[-nargs + int(testarg.value)]
                            if chosen.kind == oMSNG:
                                res.kind, res.value = oNUM, 0
                            else:
                                res.kind, res.value = chosen.kind, chosen.value
                del stack[-nargs:]
                spush(res)
        elif opcode == 0x03: #tName
            tgtnamex = unpack("<H", data[pos+1:pos+3])[0] - 1
            # Only change with BIFF version is number of trailing UNUSED bytes!
            if blah: print >> bk.logfile, "   tgtnamex=%d" % tgtnamex
            tgtobj = bk.name_obj_list[tgtnamex]
            if not tgtobj.evaluated:
                ### recursive ###
                evaluate_name_formula(bk, tgtobj, tgtnamex, blah, level+1)
            if tgtobj.macro or tgtobj.binary \
            or tgtobj.any_err:
                if blah:
                    tgtobj.dump(
                        bk.logfile,
                        header="!!! tgtobj has problems!!!",
                        footer="-----------       --------",
                        )
                res = Operand(oUNK, None)
                any_err = any_err or tgtobj.macro or tgtobj.binary or tgtobj.any_err
                any_rel = any_rel or tgtobj.any_rel
            else:
                assert len(tgtobj.stack) == 1
                res = copy.deepcopy(tgtobj.stack[0])
            res.rank = LEAF_RANK
            if tgtobj.scope == -1:
                res.text = tgtobj.name
            else:
                res.text = "%s!%s" \
                           % (bk._sheet_names[tgtobj.scope], tgtobj.name)
            if blah:
                print >> bk.logfile, "    tName: setting text to", repr(res.text)
            spush(res)
        elif opcode == 0x04: # tRef
            # not_in_name_formula(op, oname)
            res = get_cell_addr(data, pos+1, bv, reldelta)
            if blah: print >> bk.logfile, "  ", res
            rowx, colx, row_rel, col_rel = res
            shx1 = shx2 = 0 ####### N.B. relative to the CURRENT SHEET
            any_rel = 1
            coords = (shx1, shx2+1, rowx, rowx+1, colx, colx+1)
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if optype == 1:
                relflags = (1, 1, row_rel, row_rel, col_rel, col_rel)
                res = Operand(oREL, [Ref3D(coords + relflags)])
            spush(res)
        elif opcode == 0x05: # tArea
            # not_in_name_formula(op, oname)
            res1, res2 = get_cell_range_addr(data, pos+1, bv, reldelta)
            if blah: print >> bk.logfile, "  ", res1, res2
            rowx1, colx1, row_rel1, col_rel1 = res1
            rowx2, colx2, row_rel2, col_rel2 = res2
            shx1 = shx2 = 0 ####### N.B. relative to the CURRENT SHEET
            any_rel = 1
            coords = (shx1, shx2+1, rowx1, rowx2+1, colx1, colx2+1)
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if optype == 1:
                relflags = (1, 1, row_rel1, row_rel2, col_rel1, col_rel2)
                res = Operand(oREL, [Ref3D(coords + relflags)])
            spush(res)
        elif opcode == 0x06: # tMemArea
            not_in_name_formula(op, oname)
        elif opcode == 0x09: # tMemFunc
            nb = unpack("<H", data[pos+1:pos+3])[0]
            if blah: print >> bk.logfile, "  %d bytes of cell ref formula" % nb
            # no effect on stack
        elif opcode == 0x0C: #tRefN
            not_in_name_formula(op, oname)
            # res = get_cell_addr(data, pos+1, bv, reldelta=1)
            # # note *ALL* tRefN usage has signed offset for relative addresses
            # any_rel = 1
            # if blah: print >> bk.logfile, "   ", res
            # spush(res)
        elif opcode == 0x0D: #tAreaN
            not_in_name_formula(op, oname)
            # res = get_cell_range_addr(data, pos+1, bv, reldelta=1)
            # # note *ALL* tAreaN usage has signed offset for relative addresses
            # any_rel = 1
            # if blah: print >> bk.logfile, "   ", res
        elif opcode == 0x1A: # tRef3d
            if bv >= 80:
                res = get_cell_addr(data, pos+3, bv, reldelta)
                refx = unpack("<H", data[pos+1:pos+3])[0]
                shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            else:
                res = get_cell_addr(data, pos+15, bv, reldelta)
                raw_extshtx, raw_shx1, raw_shx2 = \
                             unpack("<hxxxxxxxxhh", data[pos+1:pos+15])
                if blah:
                    print >> bk.logfile, "tRef3d", raw_extshtx, raw_shx1, raw_shx2
                shx1, shx2 = get_externsheet_local_range_b57(
                                bk, raw_extshtx, raw_shx1, raw_shx2, blah)
            rowx, colx, row_rel, col_rel = res
            is_rel = row_rel or col_rel
            any_rel = any_rel or is_rel
            coords = (shx1, shx2+1, rowx, rowx+1, colx, colx+1)
            any_err |= shx1 < -1
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if is_rel:
                relflags = (0, 0, row_rel, row_rel, col_rel, col_rel)
                ref3d = Ref3D(coords + relflags)
                res.kind = oREL
                res.text = rangename3drel(bk, ref3d)
            else:
                ref3d = Ref3D(coords)
                res.kind = oREF
                res.text = rangename3d(bk, ref3d)
            res.rank = LEAF_RANK
            if optype == 1:
                res.value = [ref3d]
            spush(res)
        elif opcode == 0x1B: # tArea3d
            if bv >= 80:
                res1, res2 = get_cell_range_addr(data, pos+3, bv, reldelta)
                refx = unpack("<H", data[pos+1:pos+3])[0]
                shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            else:
                res1, res2 = get_cell_range_addr(data, pos+15, bv, reldelta)
                raw_extshtx, raw_shx1, raw_shx2 = \
                             unpack("<hxxxxxxxxhh", data[pos+1:pos+15])
                if blah:
                    print >> bk.logfile, "tArea3d", raw_extshtx, raw_shx1, raw_shx2
                shx1, shx2 = get_externsheet_local_range_b57(
                                bk, raw_extshtx, raw_shx1, raw_shx2, blah)
            any_err |= shx1 < -1
            rowx1, colx1, row_rel1, col_rel1 = res1
            rowx2, colx2, row_rel2, col_rel2 = res2
            is_rel = row_rel1 or col_rel1 or row_rel2 or col_rel2
            any_rel = any_rel or is_rel
            coords = (shx1, shx2+1, rowx1, rowx2+1, colx1, colx2+1)
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if is_rel:
                relflags = (0, 0, row_rel1, row_rel2, col_rel1, col_rel2)
                ref3d = Ref3D(coords + relflags)
                res.kind = oREL
                res.text = rangename3drel(bk, ref3d)
            else:
                ref3d = Ref3D(coords)
                res.kind = oREF
                res.text = rangename3d(bk, ref3d)
            res.rank = LEAF_RANK
            if optype == 1:
                res.value = [ref3d]

            spush(res)
        elif opcode == 0x19: # tNameX
            dodgy = 0
            res = Operand(oUNK, None)
            if bv >= 80:
                refx, tgtnamex = unpack("<HH", data[pos+1:pos+5])
                tgtnamex -= 1
                origrefx = refx
            else:
                refx, tgtnamex = unpack("<hxxxxxxxxH", data[pos+1:pos+13])
                tgtnamex -= 1
                origrefx = refx
                if refx > 0:
                    refx -= 1
                elif refx < 0:
                    refx = -refx - 1
                else:
                    dodgy = 1
            if blah:
                print >> bk.logfile, \
                    "   origrefx=%d refx=%d tgtnamex=%d dodgy=%d" \
                    % (origrefx, refx, tgtnamex, dodgy)
            if tgtnamex == namex:
                if blah: print >> bk.logfile, "!!!! Self-referential !!!!"
                dodgy = any_err = 1
            if not dodgy:
                if bv >= 80:
                    shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
                elif origrefx > 0:
                    shx1, shx2 = (-4, -4) # external ref
                else:
                    exty = bk._externsheet_type_b57[refx]
                    if exty == 4: # non-specific sheet in own doc't
                        shx1, shx2 = (-1, -1) # internal, any sheet
                    else:
                        shx1, shx2 = (-666, -666)
            if dodgy or shx1 < -1:
                otext = "<<Name #%d in external(?) file #%d>>" \
                        % (tgtnamex, origrefx)
                res = Operand(oUNK, None, LEAF_RANK, otext)
            else:
                tgtobj = bk.name_obj_list[tgtnamex]
                if not tgtobj.evaluated:
                    ### recursive ###
                    evaluate_name_formula(bk, tgtobj, tgtnamex, blah, level+1)
                if tgtobj.macro or tgtobj.binary \
                or tgtobj.any_err:
                    if blah:
                        tgtobj.dump(
                            bk.logfile,
                            header="!!! bad tgtobj !!!",
                            footer="------------------",
                            )
                    res = Operand(oUNK, None)
                    any_err = any_err or tgtobj.macro or tgtobj.binary or tgtobj.any_err
                    any_rel = any_rel or tgtobj.any_rel
                else:
                    assert len(tgtobj.stack) == 1
                    res = copy.deepcopy(tgtobj.stack[0])
                res.rank = LEAF_RANK
                if tgtobj.scope == -1:
                    res.text = tgtobj.name
                else:
                    res.text = "%s!%s" \
                               % (bk._sheet_names[tgtobj.scope], tgtobj.name)
                if blah:
                    print >> bk.logfile, "    tNameX: setting text to", repr(res.text)
            spush(res)
        elif is_error_opcode(opcode):
            any_err = 1
            spush(error_opnd)
        else:
            if blah:
                print >> bk.logfile, "FORMULA: /// Not handled yet: t" + oname
            any_err = 1
        if sz <= 0:
            raise FormulaError("Fatal: token size is not positive")
        pos += sz
    any_rel = not not any_rel
    if blah:
        print "End of formula. level=%d any_rel=%d any_err=%d stack=%r" % \
            (level, not not any_rel, any_err, stack)
        if len(stack) >= 2:
            print "*** Stack has unprocessed args"
        print
    nobj.stack = stack
    if len(stack) != 1:
        nobj.result = None
    else:
        nobj.result = stack[0]
    nobj.any_rel = any_rel
    nobj.any_err = any_err
    nobj.any_external = any_external
    nobj.evaluated = 1

#### under construction ####
def decompile_formula(bk, fmla, fmlalen,
    reldelta, browx=None, bcolx=None,
    # browx & bcolx are required when reldelta == 0
    blah=0, level=0):
    if level > STACK_ALARM_LEVEL:
        blah = 1
    data = fmla
    bv = bk.biff_version
    if blah:
        print "::: decompile_formula len=%d reldelta=%d %r level=%d" \
            % (fmlalen, reldelta, data, level)
        hex_char_dump(data, 0, fmlalen)
    if level > STACK_PANIC_LEVEL:
        raise XLRDError("Excessive indirect references in formula")
    sztab = szdict[bv]
    pos = 0
    stack = []
    any_rel = 0
    any_err = 0
    any_external = 0
    unk_opnd = Operand(oUNK, None)
    error_opnd = Operand(oERR, None)
    spush = stack.append

    def do_binop(opcd, stk):
        assert len(stk) >= 2
        bop = stk.pop()
        aop = stk.pop()
        argdict, result_kind, func, rank, sym = binop_rules[opcd]
        otext = ''.join([
            '('[:aop.rank < rank],
            aop.text,
            ')'[:aop.rank < rank],
            sym,
            '('[:bop.rank < rank],
            bop.text,
            ')'[:bop.rank < rank],
            ])
        resop = Operand(result_kind, None, rank, otext)
        stk.append(resop)

    def do_unaryop(opcode, arglist, result_kind, stk):
        assert len(stk) >= 1
        aop = stk.pop()
        assert aop.kind in arglist
        func, rank, sym1, sym2 = unop_rules[opcode]
        otext = ''.join([
            sym1,
            '('[:aop.rank < rank],
            aop.text,
            ')'[:aop.rank < rank],
            sym2,
            ])
        stk.append(Operand(result_kind, None, rank, otext))

    def not_in_name_formula(op_arg, oname_arg):
        msg = "ERROR *** Unexpected token 0x%02x (%s) found in formula" \
              % (op_arg, oname_arg)
        # print msg
        raise FormulaError(msg)

    if fmlalen == 0:
        stack = [unk_opnd]

    while 0 <= pos < fmlalen:
        op = ord(data[pos])
        opcode = op & 0x1f
        optype = (op & 0x60) >> 5
        if optype:
            opx = opcode + 32
        else:
            opx = opcode
        oname = onames[opx] # + [" RVA"][optype]
        sz = sztab[opx]
        if blah:
            print "Pos:%d Op:0x%02x opname:t%s Sz:%d opcode:%02xh optype:%02xh" \
                % (pos, op, oname, sz, opcode, optype)
            print "Stack =", stack
        if sz == -2:
            msg = 'ERROR *** Unexpected token 0x%02x ("%s"); biff_version=%d' \
                % (op, oname, bv)
            raise FormulaError(msg)
        if not optype:
            if 0x00 <= opcode <= 0x02: # unk_opnd, tExp, tTbl
                not_in_name_formula(op, oname)
            elif 0x03 <= opcode <= 0x0E:
                # Add, Sub, Mul, Div, Power
                # tConcat
                # tLT, ..., tNE
                do_binop(opcode, stack)
            elif opcode == 0x0F: # tIsect
                if blah: print >> bk.logfile, "tIsect pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ' '
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF)
                res.text = otext
                if bop.kind == oERR or aop.kind == oERR:
                    res.kind = oERR
                elif bop.kind == oUNK or aop.kind == oUNK:
                    # This can happen with undefined
                    # (go search in the current sheet) labels.
                    # For example =Bob Sales
                    # Each label gets a NAME record with an empty formula (!)
                    # Evaluation of the tName token classifies it as oUNK
                    # res.kind = oREF
                    pass
                elif bop.kind == oREF == aop.kind:
                    pass
                elif bop.kind == oREL == aop.kind:
                    res.kind = oREL
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tIsect post", stack
            elif opcode == 0x10: # tList
                if blah: print >> bk.logfile, "tList pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ','
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF, None, rank, otext)
                if bop.kind == oERR or aop.kind == oERR:
                    res.kind = oERR
                elif bop.kind in (oREF, oREL) and aop.kind in (oREF, oREL):
                    res.kind = oREF
                    if aop.kind == oREL or bop.kind == oREL:
                        res.kind = oREL
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tList post", stack
            elif opcode == 0x11: # tRange
                if blah: print >> bk.logfile, "tRange pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                sym = ':'
                rank = 80 ########## check #######
                otext = ''.join([
                    '('[:aop.rank < rank],
                    aop.text,
                    ')'[:aop.rank < rank],
                    sym,
                    '('[:bop.rank < rank],
                    bop.text,
                    ')'[:bop.rank < rank],
                    ])
                res = Operand(oREF, None, rank, otext)
                if bop.kind == oERR or aop.kind == oERR:
                    res = oERR
                elif bop.kind == oREF == aop.kind:
                    pass
                else:
                    pass
                spush(res)
                if blah: print >> bk.logfile, "tRange post", stack
            elif 0x12 <= opcode <= 0x14: # tUplus, tUminus, tPercent
                do_unaryop(opcode, (oUNK, oNUM,), oNUM, stack)
            elif opcode == 0x15: # tParen
                # source cosmetics
                pass
            elif opcode == 0x16: # tMissArg
                spush(Operand(oMSNG, None, LEAF_RANK, ''))
            elif opcode == 0x17: # tStr
                if bv <= 70:
                    strg, newpos = unpack_string_update_pos(
                                        data, pos+1, bk.encoding, lenlen=1)
                else:
                    strg, newpos = unpack_unicode_update_pos(
                                        data, pos+1, lenlen=1)
                sz = newpos - pos
                if blah: print >> bk.logfile, "   sz=%d strg=%r" % (sz, strg)
                text = '"' + strg.replace('"', '""') + '"'
                spush(Operand(oSTRG, None, LEAF_RANK, text))
            elif opcode == 0x18: # tExtended
                # new with BIFF 8
                assert bv >= 80
                # not in OOo docs
                raise FormulaError("tExtended token not implemented")
            elif opcode == 0x19: # tAttr
                subop, nc = unpack("<BH", data[pos+1:pos+4])
                subname = tAttrNames.get(subop, "??Unknown??")
                if subop == 0x04: # Choose
                    sz = nc * 2 + 6
                elif subop == 0x10: # Sum (single arg)
                    sz = 4
                    if blah: print >> bk.logfile, "tAttrSum", stack
                    assert len(stack) >= 1
                    aop = stack[-1]
                    otext = 'SUM(%s)' % aop.text
                    stack[-1] = Operand(oNUM, None, FUNC_RANK, otext)
                else:
                    sz = 4
                if blah:
                    print "   subop=%02xh subname=t%s sz=%d nc=%02xh" \
                        % (subop, subname, sz, nc)
            elif 0x1A <= opcode <= 0x1B: # tSheet, tEndSheet
                assert bv < 50
                raise FormulaError("tSheet & tEndsheet tokens not implemented")
            elif 0x1C <= opcode <= 0x1F: # tErr, tBool, tInt, tNum
                inx = opcode - 0x1C
                nb = [1, 1, 2, 8][inx]
                kind = [oERR, oBOOL, oNUM, oNUM][inx]
                value, = unpack("<" + "BBHd"[inx], data[pos+1:pos+1+nb])
                if inx == 2: # tInt
                    value = float(value)
                    text = str(value)
                elif inx == 3: # tNum
                    text = str(value)
                elif inx == 1: # tBool
                    text = ('FALSE', 'TRUE')[value]
                else:
                    text = '"' +error_text_from_code[value] + '"'
                spush(Operand(kind, None, LEAF_RANK, text))
            else:
                raise FormulaError("Unhandled opcode: 0x%02x" % opcode)
            if sz <= 0:
                raise FormulaError("Size not set for opcode 0x%02x" % opcode)
            pos += sz
            continue
        if opcode == 0x00: # tArray
            spush(unk_opnd)
        elif opcode == 0x01: # tFunc
            nb = 1 + int(bv >= 40)
            funcx = unpack("<" + " BH"[nb], data[pos+1:pos+1+nb])[0]
            func_attrs = func_defs.get(funcx, None)
            if not func_attrs:
                print >> bk.logfile, "*** formula/tFunc unknown FuncID:%d" % funcx
                spush(unk_opnd)
            else:
                func_name, nargs = func_attrs[:2]
                if blah:
                    print "    FuncID=%d name=%s nargs=%d" \
                          % (funcx, func_name, nargs)
                assert len(stack) >= nargs
                argtext = listsep.join([arg.text for arg in stack[-nargs:]])
                otext = "%s(%s)" % (func_name, argtext)
                del stack[-nargs:]
                res = Operand(oUNK, None, FUNC_RANK, otext)
                spush(res)
        elif opcode == 0x02: #tFuncVar
            nb = 1 + int(bv >= 40)
            nargs, funcx = unpack("<B" + " BH"[nb], data[pos+1:pos+2+nb])
            prompt, nargs = divmod(nargs, 128)
            macro, funcx = divmod(funcx, 32768)
            if blah:
                print "   FuncID=%d nargs=%d macro=%d prompt=%d" \
                      % (funcx, nargs, macro, prompt)
            #### TODO #### if funcx == 255: # call add-in function
            if funcx == 255:
                func_attrs = ("CALL_ADDIN", 1, 30)
            else:
                func_attrs = func_defs.get(funcx, None)
            if not func_attrs:
                print >> bk.logfile, "*** formula/tFuncVar unknown FuncID:%d" \
                      % funcx
                spush(unk_opnd)
            else:
                func_name, minargs, maxargs = func_attrs[:3]
                if blah:
                    print "    name: %r, min~max args: %d~%d" \
                        % (func_name, minargs, maxargs)
                assert minargs <= nargs <= maxargs
                assert len(stack) >= nargs
                assert len(stack) >= nargs
                argtext = listsep.join([arg.text for arg in stack[-nargs:]])
                otext = "%s(%s)" % (func_name, argtext)
                res = Operand(oUNK, None, FUNC_RANK, otext)
                del stack[-nargs:]
                spush(res)
        elif opcode == 0x03: #tName
            tgtnamex = unpack("<H", data[pos+1:pos+3])[0] - 1
            # Only change with BIFF version is number of trailing UNUSED bytes!
            if blah: print >> bk.logfile, "   tgtnamex=%d" % tgtnamex
            tgtobj = bk.name_obj_list[tgtnamex]
            if tgtobj.scope == -1:
                otext = tgtobj.name
            else:
                otext = "%s!%s" % (bk._sheet_names[tgtobj.scope], tgtobj.name)
            if blah:
                print >> bk.logfile, "    tName: setting text to", repr(otext)
            res = Operand(oUNK, None, LEAF_RANK, otext)
            spush(res)
        elif opcode == 0x04: # tRef
            res = get_cell_addr(data, pos+1, bv, reldelta, browx, bcolx)
            if blah: print >> bk.logfile, "  ", res
            rowx, colx, row_rel, col_rel = res
            is_rel = row_rel or col_rel
            if is_rel:
                okind = oREL
            else:
                okind = oREF
            otext = cellnamerel(rowx, colx, row_rel, col_rel)
            res = Operand(okind, None, LEAF_RANK, otext)
            spush(res)
        elif opcode == 0x05: # tArea
            res1, res2 = get_cell_range_addr(
                            data, pos+1, bv, reldelta, browx, bcolx)
            if blah: print >> bk.logfile, "  ", res1, res2
            rowx1, colx1, row_rel1, col_rel1 = res1
            rowx2, colx2, row_rel2, col_rel2 = res2
            coords = (rowx1, rowx2+1, colx1, colx2+1)
            relflags = (row_rel1, row_rel2, col_rel1, col_rel2)
            is_rel = intbool(sum(relflags))
            if is_rel:
                okind = oREL
            else:
                okind = oREF
            if blah: print >> bk.logfile, "   ", coords, relflags
            otext = rangename2drel(coords, relflags)
            res = Operand(okind, None, LEAF_RANK, otext)
            spush(res)
        elif opcode == 0x06: # tMemArea
            not_in_name_formula(op, oname)
        elif opcode == 0x09: # tMemFunc
            nb = unpack("<H", data[pos+1:pos+3])[0]
            if blah: print >> bk.logfile, "  %d bytes of cell ref formula" % nb
            # no effect on stack
        elif opcode == 0x0C: #tRefN
            not_in_name_formula(op, oname)
            # res = get_cell_addr(data, pos+1, bv, reldelta=1)
            # # note *ALL* tRefN usage has signed offset for relative addresses
            # any_rel = 1
            # if blah: print >> bk.logfile, "   ", res
            # spush(res)
        elif opcode == 0x0D: #tAreaN
            not_in_name_formula(op, oname)
            # res = get_cell_range_addr(data, pos+1, bv, reldelta=1)
            # # note *ALL* tAreaN usage has signed offset for relative addresses
            # any_rel = 1
            # if blah: print >> bk.logfile, "   ", res
        elif opcode == 0x1A: # tRef3d
            if bv >= 80:
                res = get_cell_addr(data, pos+3, bv, reldelta, browx, bcolx)
                refx = unpack("<H", data[pos+1:pos+3])[0]
                shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            else:
                res = get_cell_addr(data, pos+15, bv, reldelta, browx, bcolx)
                raw_extshtx, raw_shx1, raw_shx2 = \
                             unpack("<hxxxxxxxxhh", data[pos+1:pos+15])
                if blah:
                    print >> bk.logfile, "tRef3d", raw_extshtx, raw_shx1, raw_shx2
                shx1, shx2 = get_externsheet_local_range_b57(
                                bk, raw_extshtx, raw_shx1, raw_shx2, blah)
            rowx, colx, row_rel, col_rel = res
            is_rel = row_rel or col_rel
            any_rel = any_rel or is_rel
            coords = (shx1, shx2+1, rowx, rowx+1, colx, colx+1)
            any_err |= shx1 < -1
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if is_rel:
                relflags = (0, 0, row_rel, row_rel, col_rel, col_rel)
                ref3d = Ref3D(coords + relflags)
                res.kind = oREL
                res.text = rangename3drel(bk, ref3d)
            else:
                ref3d = Ref3D(coords)
                res.kind = oREF
                res.text = rangename3d(bk, ref3d)
            res.rank = LEAF_RANK
            res.value = None
            spush(res)
        elif opcode == 0x1B: # tArea3d
            if bv >= 80:
                res1, res2 = get_cell_range_addr(data, pos+3, bv, reldelta)
                refx = unpack("<H", data[pos+1:pos+3])[0]
                shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            else:
                res1, res2 = get_cell_range_addr(data, pos+15, bv, reldelta)
                raw_extshtx, raw_shx1, raw_shx2 = \
                             unpack("<hxxxxxxxxhh", data[pos+1:pos+15])
                if blah:
                    print >> bk.logfile, "tArea3d", raw_extshtx, raw_shx1, raw_shx2
                shx1, shx2 = get_externsheet_local_range_b57(
                                bk, raw_extshtx, raw_shx1, raw_shx2, blah)
            any_err |= shx1 < -1
            rowx1, colx1, row_rel1, col_rel1 = res1
            rowx2, colx2, row_rel2, col_rel2 = res2
            is_rel = row_rel1 or col_rel1 or row_rel2 or col_rel2
            any_rel = any_rel or is_rel
            coords = (shx1, shx2+1, rowx1, rowx2+1, colx1, colx2+1)
            if blah: print >> bk.logfile, "   ", coords
            res = Operand(oUNK, None)
            if is_rel:
                relflags = (0, 0, row_rel1, row_rel2, col_rel1, col_rel2)
                ref3d = Ref3D(coords + relflags)
                res.kind = oREL
                res.text = rangename3drel(bk, ref3d)
            else:
                ref3d = Ref3D(coords)
                res.kind = oREF
                res.text = rangename3d(bk, ref3d)
            res.rank = LEAF_RANK
            spush(res)
        elif opcode == 0x19: # tNameX
            dodgy = 0
            res = Operand(oUNK, None)
            if bv >= 80:
                refx, tgtnamex = unpack("<HH", data[pos+1:pos+5])
                tgtnamex -= 1
                origrefx = refx
            else:
                refx, tgtnamex = unpack("<hxxxxxxxxH", data[pos+1:pos+13])
                tgtnamex -= 1
                origrefx = refx
                if refx > 0:
                    refx -= 1
                elif refx < 0:
                    refx = -refx - 1
                else:
                    dodgy = 1
            if blah:
                print >> bk.logfile, \
                    "   origrefx=%d refx=%d tgtnamex=%d dodgy=%d" \
                    % (origrefx, refx, tgtnamex, dodgy)
            # if tgtnamex == namex:
            #     if blah: print >> bk.logfile, "!!!! Self-referential !!!!"
            #     dodgy = any_err = 1
            if not dodgy:
                if bv >= 80:
                    shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
                elif origrefx > 0:
                    shx1, shx2 = (-4, -4) # external ref
                else:
                    exty = bk._externsheet_type_b57[refx]
                    if exty == 4: # non-specific sheet in own doc't
                        shx1, shx2 = (-1, -1) # internal, any sheet
                    else:
                        shx1, shx2 = (-666, -666)
            okind = oUNK
            ovalue = None
            if shx1 == -5: # addin func name
                okind = oSTRG
                ovalue = bk.addin_func_names[tgtnamex]
                otext = '"' + ovalue.replace('"', '""') + '"'
            elif dodgy or shx1 < -1:
                otext = "<<Name #%d in external(?) file #%d>>" \
                        % (tgtnamex, origrefx)
            else:
                tgtobj = bk.name_obj_list[tgtnamex]
                if tgtobj.scope == -1:
                    otext = tgtobj.name
                else:
                    otext = "%s!%s" \
                            % (bk._sheet_names[tgtobj.scope], tgtobj.name)
                if blah:
                    print >> bk.logfile, "    tNameX: setting text to", repr(res.text)
            res = Operand(okind, ovalue, LEAF_RANK, otext)
            spush(res)
        elif is_error_opcode(opcode):
            any_err = 1
            spush(error_opnd)
        else:
            if blah:
                print >> bk.logfile, "FORMULA: /// Not handled yet: t" + oname
            any_err = 1
        if sz <= 0:
            raise FormulaError("Fatal: token size is not positive")
        pos += sz
    any_rel = not not any_rel
    if blah:
        print "End of formula. level=%d any_rel=%d any_err=%d stack=%r" % \
            (level, not not any_rel, any_err, stack)
        if len(stack) >= 2:
            print "*** Stack has unprocessed args"
        print

    if len(stack) != 1:
        result = None
    else:
        result = stack[0].text
    return result

#### under deconstruction ###
def dump_formula(bk, data, fmlalen, bv, reldelta, blah=0, isname=0):
    if blah:
        print "dump_formula", fmlalen, bv, len(data)
        hex_char_dump(data, 0, fmlalen)
    assert bv >= 80 #### this function needs updating ####
    sztab = szdict[bv]
    pos = 0
    stack = []
    any_rel = 0
    any_err = 0
    spush = stack.append
    while 0 <= pos < fmlalen:
        op = ord(data[pos])
        opcode = op & 0x1f
        optype = (op & 0x60) >> 5
        if optype:
            opx = opcode + 32
        else:
            opx = opcode
        oname = onames[opx] # + [" RVA"][optype]

        sz = sztab[opx]
        if blah:
            print "Pos:%d Op:0x%02x Name:t%s Sz:%d opcode:%02xh optype:%02xh" \
                % (pos, op, oname, sz, opcode, optype)
        if not optype:
            if 0x01 <= opcode <= 0x02: # tExp, tTbl
                # reference to a shared formula or table record
                rowx, colx = unpack("<HH", data[pos+1:pos+5])
                if blah: print >> bk.logfile, "  ", (rowx, colx)
            elif opcode == 0x10: # tList
                if blah: print >> bk.logfile, "tList pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                spush(aop + bop)
                if blah: print >> bk.logfile, "tlist post", stack
            elif opcode == 0x11: # tRange
                if blah: print >> bk.logfile, "tRange pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                assert len(aop) == 1
                assert len(bop) == 1
                result = do_box_funcs(tRangeFuncs, aop[0], bop[0])
                spush(result)
                if blah: print >> bk.logfile, "tRange post", stack
            elif opcode == 0x0F: # tIsect
                if blah: print >> bk.logfile, "tIsect pre", stack
                assert len(stack) >= 2
                bop = stack.pop()
                aop = stack.pop()
                assert len(aop) == 1
                assert len(bop) == 1
                result = do_box_funcs(tIsectFuncs, aop[0], bop[0])
                spush(result)
                if blah: print >> bk.logfile, "tIsect post", stack
            elif opcode == 0x19: # tAttr
                subop, nc = unpack("<BH", data[pos+1:pos+4])
                subname = tAttrNames.get(subop, "??Unknown??")
                if subop == 0x04: # Choose
                    sz = nc * 2 + 6
                else:
                    sz = 4
                if blah: print >> bk.logfile, "   subop=%02xh subname=t%s sz=%d nc=%02xh" % (subop, subname, sz, nc)
            elif opcode == 0x17: # tStr
                if bv <= 70:
                    nc = ord(data[pos+1])
                    strg = data[pos+2:pos+2+nc] # left in 8-bit encoding
                    sz = nc + 2
                else:
                    strg, newpos = unpack_unicode_update_pos(data, pos+1, lenlen=1)
                    sz = newpos - pos
                if blah: print >> bk.logfile, "   sz=%d strg=%r" % (sz, strg)
            else:
                if sz <= 0:
                    print "**** Dud size; exiting ****"
                    return
            pos += sz
            continue
        if opcode == 0x00: # tArray
            pass
        elif opcode == 0x01: # tFunc
            nb = 1 + int(bv >= 40)
            funcx = unpack("<" + " BH"[nb], data[pos+1:pos+1+nb])
            if blah: print >> bk.logfile, "   FuncID=%d" % funcx
        elif opcode == 0x02: #tFuncVar
            nb = 1 + int(bv >= 40)
            nargs, funcx = unpack("<B" + " BH"[nb], data[pos+1:pos+2+nb])
            prompt, nargs = divmod(nargs, 128)
            macro, funcx = divmod(funcx, 32768)
            if blah: print >> bk.logfile, "   FuncID=%d nargs=%d macro=%d prompt=%d" % (funcx, nargs, macro, prompt)
        elif opcode == 0x03: #tName
            namex = unpack("<H", data[pos+1:pos+3])
            # Only change with BIFF version is the number of trailing UNUSED bytes!!!
            if blah: print >> bk.logfile, "   namex=%d" % namex
        elif opcode == 0x04: # tRef
            res = get_cell_addr(data, pos+1, bv, reldelta)
            if blah: print >> bk.logfile, "  ", res
        elif opcode == 0x05: # tArea
            res = get_cell_range_addr(data, pos+1, bv, reldelta)
            if blah: print >> bk.logfile, "  ", res
        elif opcode == 0x09: # tMemFunc
            nb = unpack("<H", data[pos+1:pos+3])[0]
            if blah: print >> bk.logfile, "  %d bytes of cell ref formula" % nb
        elif opcode == 0x0C: #tRefN
            res = get_cell_addr(data, pos+1, bv, reldelta=1)
            # note *ALL* tRefN usage has signed offset for relative addresses
            any_rel = 1
            if blah: print >> bk.logfile, "   ", res
        elif opcode == 0x0D: #tAreaN
            res = get_cell_range_addr(data, pos+1, bv, reldelta=1)
            # note *ALL* tAreaN usage has signed offset for relative addresses
            any_rel = 1
            if blah: print >> bk.logfile, "   ", res
        elif opcode == 0x1A: # tRef3d
            refx = unpack("<H", data[pos+1:pos+3])[0]
            res = get_cell_addr(data, pos+3, bv, reldelta)
            if blah: print >> bk.logfile, "  ", refx, res
            rowx, colx, row_rel, col_rel = res
            any_rel = any_rel or row_rel or col_rel
            shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            any_err |= shx1 < -1
            coords = (shx1, shx2+1, rowx, rowx+1, colx, colx+1)
            if blah: print >> bk.logfile, "   ", coords
            if optype == 1: spush([coords])
        elif opcode == 0x1B: # tArea3d
            refx = unpack("<H", data[pos+1:pos+3])[0]
            res1, res2 = get_cell_range_addr(data, pos+3, bv, reldelta)
            if blah: print >> bk.logfile, "  ", refx, res1, res2
            rowx1, colx1, row_rel1, col_rel1 = res1
            rowx2, colx2, row_rel2, col_rel2 = res2
            any_rel = any_rel or row_rel1 or col_rel1 or row_rel2 or col_rel2
            shx1, shx2 = get_externsheet_local_range(bk, refx, blah)
            any_err |= shx1 < -1
            coords = (shx1, shx2+1, rowx1, rowx2+1, colx1, colx2+1)
            if blah: print >> bk.logfile, "   ", coords
            if optype == 1: spush([coords])
        elif opcode == 0x19: # tNameX
            refx, namex = unpack("<HH", data[pos+1:pos+5])
            if blah: print >> bk.logfile, "   refx=%d namex=%d" % (refx, namex)
        elif is_error_opcode(opcode):
            any_err = 1
        else:
            if blah: print >> bk.logfile, "FORMULA: /// Not handled yet: t" + oname
            any_err = 1
        if sz <= 0:
            print "**** Dud size; exiting ****"
            return
        pos += sz
    if blah:
        print >> bk.logfile, "End of formula. any_rel=%d any_err=%d stack=%r" % \
            (not not any_rel, any_err, stack)
        if len(stack) >= 2:
            print >> bk.logfile, "*** Stack has unprocessed args"

# === Some helper functions for displaying cell references ===

# Note that a "non-standard" syntax is used in row and column
# components in relative references.
# For example, consider a relative reference: up two rows, right 3 columns.
# On screen, with cursor in cell D10, this would appear as G8.
# On screen, with cursor in cell Z100, this would appear as AC98.
# On screen, with cursor in cell A1, this would appear as D65535.
# These functions will display such a reference as [@+3,#-2].
# "@" refers to the unknown base column.
# "#" refers to the unknown base row.
#
# I'm aware of only one possibility of a sheet-relative component in
# a reference: a 2D reference located in the "current sheet".
# xlrd stores this internally with bounds of (0, 1, ...) and
# relative flags of (1, 1, ...). These functions display the
# sheet component as empty, just like Excel etc.

def rownamerel(rowx, rowxrel):
    if not rowxrel:
        return "$%d" % rowx
    if rowx > 0:
        return "#+%d" % rowx
    if rowx < 0:
        return "#-%d" % (-rowx)
    return "#"

def colnamerel(colx, colxrel):
    if not colxrel:
        return "$" + colname(colx)
    if colx > 0:
        return "@+%d" % colx
    if colx < 0:
        return "@-%d" % (-colx)
    return "@"
##
# Utility function: (5, 7) => 'H6'
def cellname(rowx, colx):
    """ (5, 7) => 'H6' """
    return "%s%d" % (colname(colx), rowx+1)

##
# Utility function: (5, 7) => '$H$6'
def cellnameabs(rowx, colx):
    """ (5, 7) => '$H$6' """
    return "$%s$%d" % (colname(colx), rowx+1)

def cellnamerel(rowx, colx, rowxrel, colxrel):
    if not rowxrel and not colxrel:
        return cellnameabs(rowx, colx)
    return "[%s,%s]" % (
        colnamerel(colx, colxrel),
        rownamerel(rowx, rowxrel))
##
# Utility function: 7 => 'H', 27 => 'AB'
def colname(colx):
    """ 7 => 'H', 27 => 'AB' """
    alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    if colx <= 25:
        return alphabet[colx]
    else:
        xdiv26, xmod26 = divmod(colx, 26)
        return alphabet[xdiv26 - 1] + alphabet[xmod26]

def rangename2d(rlo, rhi, clo, chi):
    """ (5, 20, 7, 10) => '$H$6:$J$20' """
    if rhi == rlo+1 and chi == clo+1:
        return cellnameabs(rlo, clo)
    return "%s:%s" % (cellnameabs(rlo, clo), cellnameabs(rhi-1, chi-1))

def rangename2drel((rlo, rhi, clo, chi), (rlorel, rhirel, clorel, chirel)):
    return "%s:%s" % (
        cellnamerel(rlo, clo, rlorel, clorel),
        cellnamerel(rhi-1, chi-1, rhirel, chirel)
        )
##
# Utility function:
# <br /> Ref3D((1, 4, 5, 20, 7, 10)) => 'Sheet2:Sheet3!$H$6:$J$20'
def rangename3d(book, ref3d):
    """ Ref3D(1, 4, 5, 20, 7, 10) => 'Sheet2:Sheet3!$H$6:$J$20'
        (assuming Excel's default sheetnames) """
    coords = ref3d.coords
    return "%s!%s" % (
        sheetrange(book, *coords[:2]),
        rangename2d(*coords[2:6]))

##
# Utility function:
# <br /> Ref3D(coords=(0, 1, -32, -22, -13, 13), relflags=(0, 0, 1, 1, 1, 1))
# => 'Sheet1![@-13,#-32]:[@+12,#-23]'
# where '@' refers to the current or base column and '#'
# refers to the current or base row.
def rangename3drel(book, ref3d):
    coords = ref3d.coords
    relflags = ref3d.relflags
    shdesc = sheetrangerel(book, coords[:2], relflags[:2])
    rngdesc = rangename2drel(coords[2:6], relflags[2:6])
    if not shdesc:
        return rngdesc
    return "%s!%s" % (shdesc, rngdesc)

def quotedsheetname(shnames, shx):
    if shx >= 0:
        shname = shnames[shx]
    else:
        shname = {
            -1: "?internal; any sheet?",
            -2: "internal; deleted sheet",
            -3: "internal; macro sheet",
            -4: "<<external>>",
            }.get(shx, "?error %d?" % shx)
    if "'" in shname:
        return "'" + shname.replace("'", "''") + "'"
    if " " in shname:
        return "'" + shname + "'"
    return shname

def sheetrange(book, slo, shi):
    shnames = book.sheet_names()
    shdesc = quotedsheetname(shnames, slo)
    if slo != shi-1:
        shdesc += ":" + quotedsheetname(shnames, shi-1)
    return shdesc

def sheetrangerel(book, (slo, shi), (slorel, shirel)):
    if not slorel and not shirel:
        return sheetrange(book, slo, shi)
    assert (slo == 0 == shi-1) and slorel and shirel
    return ""

# ==============================================================

########NEW FILE########
__FILENAME__ = licences
# -*- coding: cp1252 -*-

"""
Portions copyright  2005-2009, Stephen John Machin, Lingfo Pty Ltd
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

3. None of the names of Stephen John Machin, Lingfo Pty Ltd and any
contributors may be used to endorse or promote products derived from this
software without specific prior written permission.

This software is  PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
THE POSSIBILITY OF SUCH DAMAGE.
"""

"""
/*-
 * Copyright (c) 2001 David Giffin.
 * All rights reserved.
 *
 * Based on the the Java version: Andrew Khan Copyright (c) 2000.
 *
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in
 *    the documentation and/or other materials provided with the
 *    distribution.
 *
 * 3. All advertising materials mentioning features or use of this
 *    software must display the following acknowledgment:
 *    "This product includes software developed by
 *     David Giffin <david@giffin.org>."
 *
 * 4. Redistributions of any form whatsoever must retain the following
 *    acknowledgment:
 *    "This product includes software developed by
 *     David Giffin <david@giffin.org>."
 *
 * This software is  PROVIDED BY DAVID GIFFIN ``AS IS'' AND ANY
 * EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL DAVID GIFFIN OR
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 */
"""

########NEW FILE########
__FILENAME__ = sheet
# -*- coding: cp1252 -*-

##
# <p> Portions copyright  2005-2009 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under a BSD-style licence.</p>
##

# 2009-05-31 SJM Fixed problem with no CODEPAGE record on extremely minimal BIFF2.x 3rd-party file
# 2009-04-27 SJM Integrated on_demand patch by Armando Serrano Lombillo
# 2008-02-09 SJM Excel 2.0: build XFs on the fly from cell attributes
# 2007-12-04 SJM Added support for Excel 2.x (BIFF2) files.
# 2007-10-11 SJM Added missing entry for blank cell type to ctype_text
# 2007-07-11 SJM Allow for BIFF2/3-style FORMAT record in BIFF4/8 file
# 2007-04-22 SJM Remove experimental "trimming" facility.

from biffh import *
from timemachine import *
from struct import unpack
from formula import dump_formula, decompile_formula, rangename2d
from formatting import nearest_colour_index, Format
import time

DEBUG = 0
OBJ_MSO_DEBUG = 0

_WINDOW2_options = (
    # Attribute names and initial values to use in case
    # a WINDOW2 record is not written.
    ("show_formulas", 0),
    ("show_grid_lines", 1),
    ("show_sheet_headers", 1),
    ("panes_are_frozen", 0),
    ("show_zero_values", 1),
    ("automatic_grid_line_colour", 1),
    ("columns_from_right_to_left", 0),
    ("show_outline_symbols", 1),
    ("remove_splits_if_pane_freeze_is_removed", 0),
    ("sheet_selected", 0),
    # "sheet_visible" appears to be merely a clone of "sheet_selected".
    # The real thing is the visibility attribute from the BOUNDSHEET record.
    ("sheet_visible", 0),
    ("show_in_page_break_preview", 0),
    )

##
# <p>Contains the data for one worksheet.</p>
#
# <p>In the cell access functions, "rowx" is a row index, counting from zero, and "colx" is a
# column index, counting from zero.
# Negative values for row/column indexes and slice positions are supported in the expected fashion.</p>
#
# <p>For information about cell types and cell values, refer to the documentation of the Cell class.</p>
#
# <p>WARNING: You don't call this class yourself. You access Sheet objects via the Book object that
# was returned when you called xlrd.open_workbook("myfile.xls").</p>


class Sheet(BaseObject):
    ##
    # Name of sheet.
    name = ''

    ##
    # Number of rows in sheet. A row index is in range(thesheet.nrows).
    nrows = 0

    ##
    # Number of columns in sheet. A column index is in range(thesheet.ncols).
    ncols = 0

    ##
    # The map from a column index to a Colinfo object. Often there is an entry
    # in COLINFO records for all column indexes in range(257).
    # Note that xlrd ignores the entry for the non-existent
    # 257th column. On the other hand, there may be no entry for unused columns.
    # <br /> -- New in version 0.6.1
    colinfo_map = {}

    ##
    # The map from a row index to a Rowinfo object. Note that it is possible
    # to have missing entries -- at least one source of XLS files doesn't
    # bother writing ROW records.
    # <br /> -- New in version 0.6.1
    rowinfo_map = {}

    ##
    # List of address ranges of cells containing column labels.
    # These are set up in Excel by Insert > Name > Labels > Columns.
    # <br> -- New in version 0.6.0
    # <br>How to deconstruct the list:
    # <pre>
    # for crange in thesheet.col_label_ranges:
    #     rlo, rhi, clo, chi = crange
    #     for rx in xrange(rlo, rhi):
    #         for cx in xrange(clo, chi):
    #             print "Column label at (rowx=%d, colx=%d) is %r" \
    #                 (rx, cx, thesheet.cell_value(rx, cx))
    # </pre>
    col_label_ranges = []

    ##
    # List of address ranges of cells containing row labels.
    # For more details, see <i>col_label_ranges</i> above.
    # <br> -- New in version 0.6.0
    row_label_ranges = []

    ##
    # List of address ranges of cells which have been merged.
    # These are set up in Excel by Format > Cells > Alignment, then ticking
    # the "Merge cells" box.
    # <br> -- New in version 0.6.1. Extracted only if open_workbook(..., formatting_info=True)
    # <br>How to deconstruct the list:
    # <pre>
    # for crange in thesheet.merged_cells:
    #     rlo, rhi, clo, chi = crange
    #     for rowx in xrange(rlo, rhi):
    #         for colx in xrange(clo, chi):
    #             # cell (rlo, clo) (the top left one) will carry the data
    #             # and formatting info; the remainder will be recorded as
    #             # blank cells, but a renderer will apply the formatting info
    #             # for the top left cell (e.g. border, pattern) to all cells in
    #             # the range.
    # </pre>
    merged_cells = []

    ##
    # Default column width from DEFCOLWIDTH record, else None.
    # From the OOo docs:<br />
    # """Column width in characters, using the width of the zero character
    # from default font (first FONT record in the file). Excel adds some
    # extra space to the default width, depending on the default font and
    # default font size. The algorithm how to exactly calculate the resulting
    # column width is not known.<br />
    # Example: The default width of 8 set in this record results in a column
    # width of 8.43 using Arial font with a size of 10 points."""<br />
    # For the default hierarchy, refer to the Colinfo class above.
    # <br /> -- New in version 0.6.1
    defcolwidth = None

    ##
    # Default column width from STANDARDWIDTH record, else None.
    # From the OOo docs:<br />
    # """Default width of the columns in 1/256 of the width of the zero
    # character, using default font (first FONT record in the file)."""<br />
    # For the default hierarchy, refer to the Colinfo class above.
    # <br /> -- New in version 0.6.1
    standardwidth = None

    ##
    # Default value to be used for a row if there is
    # no ROW record for that row.
    # From the <i>optional</i> DEFAULTROWHEIGHT record.
    default_row_height = None

    ##
    # Default value to be used for a row if there is
    # no ROW record for that row.
    # From the <i>optional</i> DEFAULTROWHEIGHT record.
    default_row_height_mismatch = None

    ##
    # Default value to be used for a row if there is
    # no ROW record for that row.
    # From the <i>optional</i> DEFAULTROWHEIGHT record.
    default_row_hidden = None

    ##
    # Default value to be used for a row if there is
    # no ROW record for that row.
    # From the <i>optional</i> DEFAULTROWHEIGHT record.
    default_additional_space_above = None

    ##
    # Default value to be used for a row if there is
    # no ROW record for that row.
    # From the <i>optional</i> DEFAULTROWHEIGHT record.
    default_additional_space_below = None

    ##
    # Visibility of the sheet. 0 = visible, 1 = hidden (can be unhidden
    # by user -- Format/Sheet/Unhide), 2 = "very hidden" (can be unhidden
    # only by VBA macro).
    visibility = 0

    ##
    # A 256-element tuple corresponding to the contents of the GCW record for this sheet.
    # If no such record, treat as all bits zero.
    # Applies to BIFF4-7 only. See docs of Colinfo class for discussion.
    gcw = (0, ) * 256

    def __init__(self, book, position, name, number):
        self.book = book
        self.biff_version = book.biff_version
        self._position = position
        self.logfile = book.logfile
        self.pickleable = book.pickleable
        self.dont_use_array = not(array_array and (CAN_PICKLE_ARRAY or not book.pickleable))
        self.name = name
        self.number = number
        self.verbosity = book.verbosity
        self.formatting_info = book.formatting_info
        self._xf_index_to_xl_type_map = book._xf_index_to_xl_type_map
        self.nrows = 0 # actual, including possibly empty cells
        self.ncols = 0
        self._maxdatarowx = -1 # highest rowx containing a non-empty cell
        self._maxdatacolx = -1 # highest colx containing a non-empty cell
        self._dimnrows = 0 # as per DIMENSIONS record
        self._dimncols = 0
        self._cell_values = []
        self._cell_types = []
        self._cell_xf_indexes = []
        self._need_fix_ragged_rows = 0
        self.defcolwidth = None
        self.standardwidth = None
        self.default_row_height = None
        self.default_row_height_mismatch = 0
        self.default_row_hidden = 0
        self.default_additional_space_above = 0
        self.default_additional_space_below = 0
        self.colinfo_map = {}
        self.rowinfo_map = {}
        self.col_label_ranges = []
        self.row_label_ranges = []
        self.merged_cells = []
        self._xf_index_stats = [0, 0, 0, 0]
        self.visibility = book._sheet_visibility[number] # from BOUNDSHEET record
        for attr, defval in _WINDOW2_options:
            setattr(self, attr, defval)
        self.first_visible_rowx = 0
        self.first_visible_colx = 0
        self.gridline_colour_index = 0x40
        self.gridline_colour_rgb = None # pre-BIFF8
        self.cached_page_break_preview_mag_factor = 0
        self.cached_normal_view_mag_factor = 0
        self._ixfe = None # BIFF2 only
        self._cell_attr_to_xfx = {} # BIFF2.0 only

        #### Don't initialise this here, use class attribute initialisation.
        #### self.gcw = (0, ) * 256 ####

        if self.biff_version >= 80:
            self.utter_max_rows = 65536
        else:
            self.utter_max_rows = 16384
        self.utter_max_cols = 256

    ##
    # Cell object in the given row and column.
    def cell(self, rowx, colx):
        if self.formatting_info:
            xfx = self.cell_xf_index(rowx, colx)
        else:
            xfx = None
        return Cell(
            self._cell_types[rowx][colx],
            self._cell_values[rowx][colx],
            xfx,
            )

    ##
    # Value of the cell in the given row and column.
    def cell_value(self, rowx, colx):
        return self._cell_values[rowx][colx]

    ##
    # Type of the cell in the given row and column.
    # Refer to the documentation of the Cell class.
    def cell_type(self, rowx, colx):
        return self._cell_types[rowx][colx]

    ##
    # XF index of the cell in the given row and column.
    # This is an index into Book.xf_list.
    # <br /> -- New in version 0.6.1
    def cell_xf_index(self, rowx, colx):
        self.req_fmt_info()
        xfx = self._cell_xf_indexes[rowx][colx]
        if xfx > -1:
            self._xf_index_stats[0] += 1
            return xfx
        # Check for a row xf_index
        try:
            xfx = self.rowinfo_map[rowx].xf_index
            if xfx > -1:
                self._xf_index_stats[1] += 1
                return xfx
        except KeyError:
            pass
        # Check for a column xf_index
        try:
            xfx = self.colinfo_map[colx].xf_index
            assert xfx > -1
            self._xf_index_stats[2] += 1
            return xfx
        except KeyError:
            # If all else fails, 15 is used as hardwired global default xf_index.
            self._xf_index_stats[3] += 1
            return 15

    ##
    # Returns a sequence of the Cell objects in the given row.
    def row(self, rowx):
        return [
            self.cell(rowx, colx)
            for colx in xrange(self.ncols)
            ]

    ##
    # Returns a slice of the types
    # of the cells in the given row.
    def row_types(self, rowx, start_colx=0, end_colx=None):
        if end_colx is None:
            return self._cell_types[rowx][start_colx:]
        return self._cell_types[rowx][start_colx:end_colx]

    ##
    # Returns a slice of the values
    # of the cells in the given row.
    def row_values(self, rowx, start_colx=0, end_colx=None):
        if end_colx is None:
            return self._cell_values[rowx][start_colx:]
        return self._cell_values[rowx][start_colx:end_colx]

    ##
    # Returns a slice of the Cell objects in the given row.
    def row_slice(self, rowx, start_colx=0, end_colx=None):
        nc = self.ncols
        if start_colx < 0:
            start_colx += nc
            if start_colx < 0:
                start_colx = 0
        if end_colx is None or end_colx > nc:
            end_colx = nc
        elif end_colx < 0:
            end_colx += nc
        return [
            self.cell(rowx, colx)
            for colx in xrange(start_colx, end_colx)
            ]

    ##
    # Returns a slice of the Cell objects in the given column.
    def col_slice(self, colx, start_rowx=0, end_rowx=None):
        nr = self.nrows
        if start_rowx < 0:
            start_rowx += nr
            if start_rowx < 0:
                start_rowx = 0
        if end_rowx is None or end_rowx > nr:
            end_rowx = nr
        elif end_rowx < 0:
            end_rowx += nr
        return [
            self.cell(rowx, colx)
            for rowx in xrange(start_rowx, end_rowx)
            ]

    ##
    # Returns a slice of the values of the cells in the given column.
    def col_values(self, colx, start_rowx=0, end_rowx=None):
        nr = self.nrows
        if start_rowx < 0:
            start_rowx += nr
            if start_rowx < 0:
                start_rowx = 0
        if end_rowx is None or end_rowx > nr:
            end_rowx = nr
        elif end_rowx < 0:
            end_rowx += nr
        return [
            self._cell_values[rowx][colx]
            for rowx in xrange(start_rowx, end_rowx)
            ]

    ##
    # Returns a slice of the types of the cells in the given column.
    def col_types(self, colx, start_rowx=0, end_rowx=None):
        nr = self.nrows
        if start_rowx < 0:
            start_rowx += nr
            if start_rowx < 0:
                start_rowx = 0
        if end_rowx is None or end_rowx > nr:
            end_rowx = nr
        elif end_rowx < 0:
            end_rowx += nr
        return [
            self._cell_types[rowx][colx]
            for rowx in xrange(start_rowx, end_rowx)
            ]

    ##
    # Returns a sequence of the Cell objects in the given column.
    def col(self, colx):
        return self.col_slice(colx)
    # Above two lines just for the docs. Here's the real McCoy:
    col = col_slice

    # === Following methods are used in building the worksheet.
    # === They are not part of the API.

    def extend_cells(self, nr, nc):
        # print "extend_cells_2", self.nrows, self.ncols, nr, nc
        assert 1 <= nc <= self.utter_max_cols
        assert 1 <= nr <= self.utter_max_rows
        if nr <= self.nrows:
            # New cell is in an existing row, so extend that row (if necessary).
            # Note that nr < self.nrows means that the cell data
            # is not in ascending row order!!
            self._need_fix_ragged_rows = 1
            nrx = nr - 1
            trow = self._cell_types[nrx]
            tlen = len(trow)
            nextra = max(nc, self.ncols) - tlen
            if nextra > 0:
                xce = XL_CELL_EMPTY
                if self.dont_use_array:
                    trow.extend([xce] * nextra)
                    if self.formatting_info:
                        self._cell_xf_indexes[nrx].extend([-1] * nextra)
                else:
                    aa = array_array
                    trow.extend(aa('B', [xce]) * nextra)
                    if self.formatting_info:
                        self._cell_xf_indexes[nrx].extend(aa('h', [-1]) * nextra)
                self._cell_values[nrx].extend([''] * nextra)
        if nc > self.ncols:
            self.ncols = nc
            self._need_fix_ragged_rows = 1
        if nr > self.nrows:
            scta = self._cell_types.append
            scva = self._cell_values.append
            scxa = self._cell_xf_indexes.append
            fmt_info = self.formatting_info
            xce = XL_CELL_EMPTY
            nc = self.ncols
            if self.dont_use_array:
                for _unused in xrange(self.nrows, nr):
                    scta([xce] * nc)
                    scva([''] * nc)
                    if fmt_info:
                        scxa([-1] * nc)
            else:
                aa = array_array
                for _unused in xrange(self.nrows, nr):
                    scta(aa('B', [xce]) * nc)
                    scva([''] * nc)
                    if fmt_info:
                        scxa(aa('h', [-1]) * nc)
            self.nrows = nr

    def fix_ragged_rows(self):
        t0 = time.time()
        ncols = self.ncols
        xce = XL_CELL_EMPTY
        aa = array_array
        s_cell_types = self._cell_types
        s_cell_values = self._cell_values
        s_cell_xf_indexes = self._cell_xf_indexes
        s_dont_use_array = self.dont_use_array
        s_fmt_info = self.formatting_info
        totrowlen = 0
        for rowx in xrange(self.nrows):
            trow = s_cell_types[rowx]
            rlen = len(trow)
            totrowlen += rlen
            nextra = ncols - rlen
            if nextra > 0:
                s_cell_values[rowx][rlen:] = [''] * nextra
                if s_dont_use_array:
                    trow[rlen:] = [xce] * nextra
                    if s_fmt_info:
                        s_cell_xf_indexes[rowx][rlen:] = [-1] * nextra
                else:
                    trow.extend(aa('B', [xce]) * nextra)
                    if s_fmt_info:
                        s_cell_xf_indexes[rowx][rlen:] = aa('h', [-1]) * nextra
        self._fix_ragged_rows_time = time.time() - t0
        if 0 and self.nrows:
            avgrowlen = float(totrowlen) / self.nrows
            print >> self.logfile, \
                "sheet %d: avg row len %.1f; max row len %d" \
                % (self.number, avgrowlen, self.ncols)

    def tidy_dimensions(self):
        if self.verbosity >= 3:
            fprintf(self.logfile,
                "tidy_dimensions: nrows=%d ncols=%d _need_fix_ragged_rows=%d\n",
                self.nrows, self.ncols, self._need_fix_ragged_rows,
                )
        if 1 and self.merged_cells:
            nr = nc = 0
            umaxrows = self.utter_max_rows
            umaxcols = self.utter_max_cols
            for crange in self.merged_cells:
                rlo, rhi, clo, chi = crange
                if not (0 <= rlo < rhi <= umaxrows) \
                or not (0 <= clo < chi <= umaxcols):
                    fprintf(self.logfile,
                        "*** WARNING: sheet #%d (%r), MERGEDCELLS bad range %r\n",
                        self.number, self.name, crange)
                if rhi > nr: nr = rhi
                if chi > nc: nc = chi
            self.extend_cells(nr, nc)
        if self.verbosity >= 1 \
        and (self.nrows != self._dimnrows or self.ncols != self._dimncols):
            fprintf(self.logfile,
                "NOTE *** sheet %d (%r): DIMENSIONS R,C = %d,%d should be %d,%d\n",
                self.number,
                self.name,
                self._dimnrows,
                self._dimncols,
                self.nrows,
                self.ncols,
                )
        if self._need_fix_ragged_rows:
            self.fix_ragged_rows()

    def put_cell(self, rowx, colx, ctype, value, xf_index):
        try:
            self._cell_types[rowx][colx] = ctype
            self._cell_values[rowx][colx] = value
            if self.formatting_info:
                self._cell_xf_indexes[rowx][colx] = xf_index
        except IndexError:
            # print >> self.logfile, "put_cell extending", rowx, colx
            self.extend_cells(rowx+1, colx+1)
            try:
                self._cell_types[rowx][colx] = ctype
                self._cell_values[rowx][colx] = value
                if self.formatting_info:
                    self._cell_xf_indexes[rowx][colx] = xf_index
            except:
                print >> self.logfile, "put_cell", rowx, colx
                raise
        except:
            print >> self.logfile, "put_cell", rowx, colx
            raise

    def put_blank_cell(self, rowx, colx, xf_index):
        # This is used for cells from BLANK and MULBLANK records
        ctype = XL_CELL_BLANK
        value = ''
        try:
            self._cell_types[rowx][colx] = ctype
            self._cell_values[rowx][colx] = value
            self._cell_xf_indexes[rowx][colx] = xf_index
        except IndexError:
            # print >> self.logfile, "put_cell extending", rowx, colx
            self.extend_cells(rowx+1, colx+1)
            try:
                self._cell_types[rowx][colx] = ctype
                self._cell_values[rowx][colx] = value
                self._cell_xf_indexes[rowx][colx] = xf_index
            except:
                print >> self.logfile, "put_cell", rowx, colx
                raise
        except:
            print >> self.logfile, "put_cell", rowx, colx
            raise

    def put_number_cell(self, rowx, colx, value, xf_index):
        ctype = self._xf_index_to_xl_type_map[xf_index]
        try:
            self._cell_types[rowx][colx] = ctype
            self._cell_values[rowx][colx] = value
            if self.formatting_info:
                self._cell_xf_indexes[rowx][colx] = xf_index
        except IndexError:
            # print >> self.logfile, "put_number_cell extending", rowx, colx
            self.extend_cells(rowx+1, colx+1)
            try:
                self._cell_types[rowx][colx] = ctype
                self._cell_values[rowx][colx] = value
                if self.formatting_info:
                    self._cell_xf_indexes[rowx][colx] = xf_index
            except:
                print >> self.logfile, "put_number_cell", rowx, colx
                raise
        except:
            print >> self.logfile, "put_number_cell", rowx, colx
            raise

    # === Methods after this line neither know nor care about how cells are stored.

    def read(self, bk):
        global rc_stats
        DEBUG = 0
        blah = DEBUG or self.verbosity >= 2
        blah_rows = DEBUG or self.verbosity >= 4
        blah_formulas = 1 and blah
        oldpos = bk._position
        bk._position = self._position
        XL_SHRFMLA_ETC_ETC = (
            XL_SHRFMLA, XL_ARRAY, XL_TABLEOP, XL_TABLEOP2,
            XL_ARRAY2, XL_TABLEOP_B2,
            )
        self_put_number_cell = self.put_number_cell
        self_put_cell = self.put_cell
        self_put_blank_cell = self.put_blank_cell
        local_unpack = unpack
        bk_get_record_parts = bk.get_record_parts
        bv = self.biff_version
        fmt_info = self.formatting_info
        eof_found = 0
        while 1:
            # if DEBUG: print "SHEET.READ: about to read from position %d" % bk._position
            rc, data_len, data = bk_get_record_parts()
            # if rc in rc_stats:
            #     rc_stats[rc] += 1
            # else:
            #     rc_stats[rc] = 1
            # if DEBUG: print "SHEET.READ: op 0x%04x, %d bytes %r" % (rc, data_len, data)
            if rc == XL_NUMBER:
                rowx, colx, xf_index, d = local_unpack('<HHHd', data)
                # if xf_index == 0:
                #     fprintf(self.logfile,
                #         "NUMBER: r=%d c=%d xfx=%d %f\n", rowx, colx, xf_index, d)
                self_put_number_cell(rowx, colx, d, xf_index)
            elif rc == XL_LABELSST:
                rowx, colx, xf_index, sstindex = local_unpack('<HHHi', data)
                # print "LABELSST", rowx, colx, sstindex, bk._sharedstrings[sstindex]
                self_put_cell(rowx, colx, XL_CELL_TEXT, bk._sharedstrings[sstindex], xf_index)
            elif rc == XL_LABEL or rc == XL_RSTRING:
                # RSTRING has extra richtext info at the end, but we ignore it.
                rowx, colx, xf_index = local_unpack('<HHH', data[0:6])
                if bv < BIFF_FIRST_UNICODE:
                    strg = unpack_string(data, 6, bk.encoding or bk.derive_encoding, lenlen=2)
                else:
                    strg = unpack_unicode(data, 6, lenlen=2)
                self_put_cell(rowx, colx, XL_CELL_TEXT, strg, xf_index)
            elif rc == XL_RK:
                rowx, colx, xf_index = local_unpack('<HHH', data[:6])
                d = unpack_RK(data[6:10])
                self_put_number_cell(rowx, colx, d, xf_index)
            elif rc == XL_MULRK:
                mulrk_row, mulrk_first = local_unpack('<HH', data[0:4])
                mulrk_last, = local_unpack('<H', data[-2:])
                pos = 4
                for colx in xrange(mulrk_first, mulrk_last+1):
                    xf_index, = local_unpack('<H', data[pos:pos+2])
                    d = unpack_RK(data[pos+2:pos+6])
                    pos += 6
                    self_put_number_cell(mulrk_row, colx, d, xf_index)
            elif rc == XL_ROW:
                # Version 0.6.0a3: ROW records are just not worth using (for memory allocation).
                # Version 0.6.1: now used for formatting info.
                if not fmt_info: continue
                rowx, bits1, bits2 = local_unpack('<H4xH4xi', data[0:16])
                if not(0 <= rowx < self.utter_max_rows):
                    print >> self.logfile, \
                        "*** NOTE: ROW record has row index %d; " \
                        "should have 0 <= rowx < %d -- record ignored!" \
                        % (rowx, self.utter_max_rows)
                    continue
                r = Rowinfo()
                # Using upkbits() is far too slow on a file
                # with 30 sheets each with 10K rows :-(
                #    upkbits(r, bits1, (
                #        ( 0, 0x7FFF, 'height'),
                #        (15, 0x8000, 'has_default_height'),
                #        ))
                #    upkbits(r, bits2, (
                #        ( 0, 0x00000007, 'outline_level'),
                #        ( 4, 0x00000010, 'outline_group_starts_ends'),
                #        ( 5, 0x00000020, 'hidden'),
                #        ( 6, 0x00000040, 'height_mismatch'),
                #        ( 7, 0x00000080, 'has_default_xf_index'),
                #        (16, 0x0FFF0000, 'xf_index'),
                #        (28, 0x10000000, 'additional_space_above'),
                #        (29, 0x20000000, 'additional_space_below'),
                #        ))
                # So:
                r.height = bits1 & 0x7fff
                r.has_default_height = (bits1 >> 15) & 1
                r.outline_level = bits2 & 7
                r.outline_group_starts_ends = (bits2 >> 4) & 1
                r.hidden = (bits2 >> 5) & 1
                r.height_mismatch = (bits2 >> 6) & 1
                r.has_default_xf_index = (bits2 >> 7) & 1
                r.xf_index = (bits2 >> 16) & 0xfff
                r.additional_space_above = (bits2 >> 28) & 1
                r.additional_space_below = (bits2 >> 29) & 1
                if not r.has_default_xf_index:
                    r.xf_index = -1
                self.rowinfo_map[rowx] = r
                if 0 and r.xf_index > -1:
                    fprintf(self.logfile,
                        "**ROW %d %d %d\n",
                        self.number, rowx, r.xf_index)
                if blah_rows:
                    print >> self.logfile, 'ROW', rowx, bits1, bits2
                    r.dump(self.logfile,
                        header="--- sh #%d, rowx=%d ---" % (self.number, rowx))
            elif rc in XL_FORMULA_OPCODES: # 06, 0206, 0406
                # DEBUG = 1
                # if DEBUG: print "FORMULA: rc: 0x%04x data: %r" % (rc, data)
                if bv >= 50:
                    rowx, colx, xf_index, result_str, flags = local_unpack('<HHH8sH', data[0:16])
                    lenlen = 2
                    tkarr_offset = 20
                elif bv >= 30:
                    rowx, colx, xf_index, result_str, flags = local_unpack('<HHH8sH', data[0:16])
                    lenlen = 2
                    tkarr_offset = 16
                else: # BIFF2
                    rowx, colx, cell_attr,  result_str, flags = local_unpack('<HH3s8sB', data[0:16])
                    xf_index =  self.fixed_BIFF2_xfindex(cell_attr, rowx, colx)
                    lenlen = 1
                    tkarr_offset = 16
                if blah_formulas: # testing formula dumper
                    #### XXXX FIXME
                    fprintf(self.logfile, "FORMULA: rowx=%d colx=%d\n", rowx, colx)
                    fmlalen = local_unpack("<H", data[20:22])[0]
                    decompile_formula(bk, data[22:], fmlalen,
                        reldelta=0, browx=rowx, bcolx=colx, blah=1)
                if result_str[6:8] == "\xFF\xFF":
                    if result_str[0]  == '\x00':
                        # need to read next record (STRING)
                        gotstring = 0
                        # if flags & 8:
                        if 1: # "flags & 8" applies only to SHRFMLA
                            # actually there's an optional SHRFMLA or ARRAY etc record to skip over
                            rc2, data2_len, data2 = bk.get_record_parts()
                            if rc2 == XL_STRING or rc2 == XL_STRING_B2:
                                gotstring = 1
                            elif rc2 == XL_ARRAY:
                                row1x, rownx, col1x, colnx, array_flags, tokslen = \
                                    local_unpack("<HHBBBxxxxxH", data2[:14])
                                if blah_formulas:
                                    fprintf(self.logfile, "ARRAY: %d %d %d %d %d\n",
                                        row1x, rownx, col1x, colnx, array_flags)
                                    dump_formula(bk, data2[14:], tokslen, bv, reldelta=0, blah=1)
                            elif rc2 == XL_SHRFMLA:
                                row1x, rownx, col1x, colnx, nfmlas, tokslen = \
                                    local_unpack("<HHBBxBH", data2[:10])
                                if blah_formulas:
                                    fprintf(self.logfile, "SHRFMLA (sub): %d %d %d %d %d\n",
                                        row1x, rownx, col1x, colnx, nfmlas)
                                    decompile_formula(bk, data2[10:], tokslen, reldelta=1, blah=1)
                            elif rc2 not in XL_SHRFMLA_ETC_ETC:
                                raise XLRDError(
                                    "Expected SHRFMLA, ARRAY, TABLEOP* or STRING record; found 0x%04x" % rc2)
                            # if DEBUG: print "gotstring:", gotstring
                        # now for the STRING record
                        if not gotstring:
                            rc2, _unused_len, data2 = bk.get_record_parts()
                            if rc2 not in (XL_STRING, XL_STRING_B2):
                                raise XLRDError("Expected STRING record; found 0x%04x" % rc2)
                        # if DEBUG: print "STRING: data=%r BIFF=%d cp=%d" % (data2, self.biff_version, bk.encoding)
                        if self.biff_version < BIFF_FIRST_UNICODE:
                            strg = unpack_string(data2, 0, bk.encoding or bk.derive_encoding, lenlen=1 + int(bv > 20))
                        else:
                            strg = unpack_unicode(data2, 0, lenlen=2)
                        self.put_cell(rowx, colx, XL_CELL_TEXT, strg, xf_index)
                        # if DEBUG: print "FORMULA strg %r" % strg
                    elif result_str[0] == '\x01':
                        # boolean formula result
                        value = ord(result_str[2])
                        self.put_cell(rowx, colx, XL_CELL_BOOLEAN, value, xf_index)
                    elif result_str[0] == '\x02':
                        # Error in cell
                        value = ord(result_str[2])
                        self.put_cell(rowx, colx, XL_CELL_ERROR, value, xf_index)
                    elif result_str[0] == '\x03':
                        # empty ... i.e. empty (zero-length) string, NOT an empty cell.
                        self.put_cell(rowx, colx, XL_CELL_TEXT, u"", xf_index)
                    else:
                        raise XLRDError("unexpected special case (0x%02x) in FORMULA" % ord(result_str[0]))
                else:
                    # it is a number
                    d = local_unpack('<d', result_str)[0]
                    self_put_number_cell(rowx, colx, d, xf_index)
            elif rc == XL_BOOLERR:
                rowx, colx, xf_index, value, is_err = local_unpack('<HHHBB', data[:8])
                # Note OOo Calc 2.0 writes 9-byte BOOLERR records.
                # OOo docs say 8. Excel writes 8.
                cellty = (XL_CELL_BOOLEAN, XL_CELL_ERROR)[is_err]
                # if DEBUG: print "XL_BOOLERR", rowx, colx, xf_index, value, is_err
                self.put_cell(rowx, colx, cellty, value, xf_index)
            elif rc == XL_COLINFO:
                if not fmt_info: continue
                c = Colinfo()
                first_colx, last_colx, c.width, c.xf_index, flags \
                    = local_unpack("<HHHHH", data[:10])
                #### Colinfo.width is denominated in 256ths of a character,
                #### *not* in characters.
                if not(0 <= first_colx <= last_colx <= 256):
                    # Note: 256 instead of 255 is a common mistake.
                    # We silently ignore the non-existing 257th column in that case.
                    print >> self.logfile, \
                        "*** NOTE: COLINFO record has first col index %d, last %d; " \
                        "should have 0 <= first <= last <= 255 -- record ignored!" \
                        % (first_colx, last_colx)
                    del c
                    continue
                upkbits(c, flags, (
                    ( 0, 0x0001, 'hidden'),
                    ( 1, 0x0002, 'bit1_flag'),
                    # *ALL* colinfos created by Excel in "default" cases are 0x0002!!
                    # Maybe it's "locked" by analogy with XFProtection data.
                    ( 8, 0x0700, 'outline_level'),
                    (12, 0x1000, 'collapsed'),
                    ))
                for colx in xrange(first_colx, last_colx+1):
                    if colx > 255: break # Excel does 0 to 256 inclusive
                    self.colinfo_map[colx] = c
                    if 0:
                        fprintf(self.logfile,
                            "**COL %d %d %d\n",
                            self.number, colx, c.xf_index)
                if blah:
                    fprintf(
                        self.logfile,
                        "COLINFO sheet #%d cols %d-%d: wid=%d xf_index=%d flags=0x%04x\n",
                        self.number, first_colx, last_colx, c.width, c.xf_index, flags,
                        )
                    c.dump(self.logfile, header='===')
            elif rc == XL_DEFCOLWIDTH:
                self.defcolwidth, = local_unpack("<H", data[:2])
                if 0: print >> self.logfile, 'DEFCOLWIDTH', self.defcolwidth
            elif rc == XL_STANDARDWIDTH:
                if data_len != 2:
                    print >> self.logfile, '*** ERROR *** STANDARDWIDTH', data_len, repr(data)
                self.standardwidth, = local_unpack("<H", data[:2])
                if 0: print >> self.logfile, 'STANDARDWIDTH', self.standardwidth
            elif rc == XL_GCW:
                if not fmt_info: continue # useless w/o COLINFO
                assert data_len == 34
                assert data[0:2] == "\x20\x00"
                iguff = unpack("<8i", data[2:34])
                gcw = []
                for bits in iguff:
                    for j in xrange(32):
                        gcw.append(bits & 1)
                        bits >>= 1
                self.gcw = tuple(gcw)
                if 0:
                    showgcw = "".join(map(lambda x: "F "[x], gcw)).rstrip().replace(' ', '.')
                    print "GCW:", showgcw
            elif rc == XL_BLANK:
                if not fmt_info: continue
                rowx, colx, xf_index = local_unpack('<HHH', data[:6])
                if 0: print >> self.logfile, "BLANK", rowx, colx, xf_index
                self_put_blank_cell(rowx, colx, xf_index)
            elif rc == XL_MULBLANK: # 00BE
                if not fmt_info: continue
                mul_row, mul_first = local_unpack('<HH', data[0:4])
                mul_last, = local_unpack('<H', data[-2:])
                if 0:
                    print >> self.logfile, "MULBLANK", mul_row, mul_first, mul_last
                pos = 4
                for colx in xrange(mul_first, mul_last+1):
                    xf_index, = local_unpack('<H', data[pos:pos+2])
                    pos += 2
                    self_put_blank_cell(mul_row, colx, xf_index)
            elif rc == XL_DIMENSION or rc == XL_DIMENSION2:
                # if data_len == 10:
                # Was crashing on BIFF 4.0 file w/o the two trailing unused bytes.
                # Reported by Ralph Heimburger.
                if bv < 80:
                    dim_tuple = local_unpack('<HxxH', data[2:8])
                else:
                    dim_tuple = local_unpack('<ixxH', data[4:12])
                self.nrows, self.ncols = 0, 0
                self._dimnrows, self._dimncols = dim_tuple
                if not self.book._xf_epilogue_done:
                    # Needed for bv <= 40
                    self.book.xf_epilogue()
                if blah:
                    fprintf(self.logfile,
                        "sheet %d(%r) DIMENSIONS: ncols=%d nrows=%d\n",
                        self.number, self.name, self._dimncols, self._dimnrows
                        )
            elif rc == XL_EOF:
                DEBUG = 0
                if DEBUG: print >> self.logfile, "SHEET.READ: EOF"
                eof_found = 1
                break
            elif rc == XL_OBJ:
                # handle SHEET-level objects; note there's a separate Book.handle_obj
                self.handle_obj(data)
            elif rc == XL_MSO_DRAWING:
                self.handle_msodrawingetc(rc, data_len, data)
            elif rc == XL_TXO:
                self.handle_txo(data)
            elif rc == XL_NOTE:
                self.handle_note(data)
            elif rc == XL_FEAT11:
                self.handle_feat11(data)
            elif rc in bofcodes: ##### EMBEDDED BOF #####
                version, boftype = local_unpack('<HH', data[0:4])
                if boftype != 0x20: # embedded chart
                    print >> self.logfile, \
                        "*** Unexpected embedded BOF (0x%04x) at offset %d: version=0x%04x type=0x%04x" \
                        % (rc, bk._position - data_len - 4, version, boftype)
                while 1:
                    code, data_len, data = bk.get_record_parts()
                    if code == XL_EOF:
                        break
                if DEBUG: print >> self.logfile, "---> found EOF"
            elif rc == XL_COUNTRY:
                bk.handle_country(data)
            elif rc == XL_LABELRANGES:
                pos = 0
                pos = unpack_cell_range_address_list_update_pos(
                        self.row_label_ranges, data, pos, bv, addr_size=8,
                        )
                pos = unpack_cell_range_address_list_update_pos(
                        self.col_label_ranges, data, pos, bv, addr_size=8,
                        )
                assert pos == data_len
            elif rc == XL_ARRAY:
                row1x, rownx, col1x, colnx, array_flags, tokslen = \
                    local_unpack("<HHBBBxxxxxH", data[:14])
                if blah_formulas:
                    print "ARRAY:", row1x, rownx, col1x, colnx, array_flags
                    dump_formula(bk, data[14:], tokslen, bv, reldelta=0, blah=1)
            elif rc == XL_SHRFMLA:
                row1x, rownx, col1x, colnx, nfmlas, tokslen = \
                    local_unpack("<HHBBxBH", data[:10])
                if blah_formulas:
                    print "SHRFMLA (main):", row1x, rownx, col1x, colnx, nfmlas
                    decompile_formula(bk, data[10:], tokslen, reldelta=0, blah=1)
            elif rc == XL_CONDFMT:
                if not fmt_info: continue
                assert bv >= 80
                num_CFs, needs_recalc, browx1, browx2, bcolx1, bcolx2 = \
                    unpack("<6H", data[0:12])
                if self.verbosity >= 1:
                    fprintf(self.logfile,
                        "\n*** WARNING: Ignoring CONDFMT (conditional formatting) record\n" \
                        "*** in Sheet %d (%r).\n" \
                        "*** %d CF record(s); needs_recalc_or_redraw = %d\n" \
                        "*** Bounding box is %s\n",
                        self.number, self.name, num_CFs, needs_recalc,
                        rangename2d(browx1, browx2+1, bcolx1, bcolx2+1),
                        )
                olist = [] # updated by the function
                pos = unpack_cell_range_address_list_update_pos(
                    olist, data, 12, bv, addr_size=8)
                # print >> self.logfile, repr(result), len(result)
                if self.verbosity >= 1:
                    fprintf(self.logfile,
                        "*** %d individual range(s):\n" \
                        "*** %s\n",
                        len(olist),
                        ", ".join([rangename2d(*coords) for coords in olist]),
                        )
            elif rc == XL_CF:
                if not fmt_info: continue
                cf_type, cmp_op, sz1, sz2, flags = unpack("<BBHHi", data[0:10])
                font_block = (flags >> 26) & 1
                bord_block = (flags >> 28) & 1
                patt_block = (flags >> 29) & 1
                if self.verbosity >= 1:
                    fprintf(self.logfile,
                        "\n*** WARNING: Ignoring CF (conditional formatting) sub-record.\n" \
                        "*** cf_type=%d, cmp_op=%d, sz1=%d, sz2=%d, flags=0x%08x\n" \
                        "*** optional data blocks: font=%d, border=%d, pattern=%d\n",
                        cf_type, cmp_op, sz1, sz2, flags,
                        font_block, bord_block, patt_block,
                        )
                # hex_char_dump(data, 0, data_len)
                pos = 12
                if font_block:
                    (font_height, font_options, weight, escapement, underline,
                    font_colour_index, two_bits, font_esc, font_underl) = \
                    unpack("<64x i i H H B 3x i 4x i i i 18x", data[pos:pos+118])
                    font_style = (two_bits > 1) & 1
                    posture = (font_options > 1) & 1
                    font_canc = (two_bits > 7) & 1
                    cancellation = (font_options > 7) & 1
                    if self.verbosity >= 1:
                        fprintf(self.logfile,
                            "*** Font info: height=%d, weight=%d, escapement=%d,\n" \
                            "*** underline=%d, colour_index=%d, esc=%d, underl=%d,\n" \
                            "*** style=%d, posture=%d, canc=%d, cancellation=%d\n",
                            font_height, weight, escapement, underline,
                            font_colour_index, font_esc, font_underl,
                            font_style, posture, font_canc, cancellation,
                            )
                    pos += 118
                if bord_block:
                    pos += 8
                if patt_block:
                    pos += 4
                fmla1 = data[pos:pos+sz1]
                pos += sz1
                if blah and sz1:
                    fprintf(self.logfile,
                        "*** formula 1:\n",
                        )
                    dump_formula(bk, fmla1, sz1, bv, reldelta=0, blah=1)
                fmla2 = data[pos:pos+sz2]
                pos += sz2
                assert pos == data_len
                if blah and sz2:
                    fprintf(self.logfile,
                        "*** formula 2:\n",
                        )
                    dump_formula(bk, fmla2, sz2, bv, reldelta=0, blah=1)
            elif rc == XL_DEFAULTROWHEIGHT:
                if data_len == 4:
                    bits, self.default_row_height = unpack("<HH", data[:4])
                elif data_len == 2:
                    self.default_row_height, = unpack("<H", data)
                    bits = 0
                    fprintf(self.logfile,
                        "*** WARNING: DEFAULTROWHEIGHT record len is 2, " \
                        "should be 4; assuming BIFF2 format\n")
                else:
                    bits = 0
                    fprintf(self.logfile,
                        "*** WARNING: DEFAULTROWHEIGHT record len is %d, " \
                        "should be 4; ignoring this record\n",
                        data_len)
                self.default_row_height_mismatch = bits & 1
                self.default_row_hidden = (bits >> 1) & 1
                self.default_additional_space_above = (bits >> 2) & 1
                self.default_additional_space_below = (bits >> 3) & 1
            elif rc == XL_MERGEDCELLS:
                if not fmt_info: continue
                pos = unpack_cell_range_address_list_update_pos(
                    self.merged_cells, data, 0, bv, addr_size=8)
                if blah:
                    fprintf(self.logfile,
                        "MERGEDCELLS: %d ranges\n", int_floor_div(pos - 2, 8))
                assert pos == data_len, \
                    "MERGEDCELLS: pos=%d data_len=%d" % (pos, data_len)
            elif rc == XL_WINDOW2:
                if bv >= 80:
                    (options,
                    self.first_visible_rowx, self.first_visible_colx,
                    self.gridline_colour_index,
                    self.cached_page_break_preview_mag_factor,
                    self.cached_normal_view_mag_factor
                    ) = unpack("<HHHHxxHH", data[:14])
                else: # BIFF3-7
                    (options,
                    self.first_visible_rowx, self.first_visible_colx,
                    ) = unpack("<HHH", data[:6])
                    self.gridline_colour_rgb = unpack("<BBB", data[6:9])
                    self.gridline_colour_index = \
                        nearest_colour_index(
                            self.book.colour_map,
                            self.gridline_colour_rgb,
                            debug=0)
                    self.cached_page_break_preview_mag_factor = 0 # default (60%)
                    self.cached_normal_view_mag_factor = 0 # default (100%)
                # options -- Bit, Mask, Contents:
                # 0 0001H 0 = Show formula results 1 = Show formulas
                # 1 0002H 0 = Do not show grid lines 1 = Show grid lines
                # 2 0004H 0 = Do not show sheet headers 1 = Show sheet headers
                # 3 0008H 0 = Panes are not frozen 1 = Panes are frozen (freeze)
                # 4 0010H 0 = Show zero values as empty cells 1 = Show zero values
                # 5 0020H 0 = Manual grid line colour 1 = Automatic grid line colour
                # 6 0040H 0 = Columns from left to right 1 = Columns from right to left
                # 7 0080H 0 = Do not show outline symbols 1 = Show outline symbols
                # 8 0100H 0 = Keep splits if pane freeze is removed 1 = Remove splits if pane freeze is removed
                # 9 0200H 0 = Sheet not selected 1 = Sheet selected (BIFF5-BIFF8)
                # 10 0400H 0 = Sheet not visible 1 = Sheet visible (BIFF5-BIFF8)
                # 11 0800H 0 = Show in normal view 1 = Show in page break preview (BIFF8)
                # The freeze flag specifies, if a following PANE record (6.71) describes unfrozen or frozen panes.
                for attr, _unused_defval in _WINDOW2_options:
                    setattr(self, attr, options & 1)
                    options >>= 1
                # print "WINDOW2: visible=%d selected=%d" \
                #     % (self.sheet_visible, self.sheet_selected)
            #### all of the following are for BIFF <= 4W
            elif bv <= 45:
                if rc == XL_FORMAT or rc == XL_FORMAT2:
                    bk.handle_format(data, rc)
                elif rc == XL_FONT or rc == XL_FONT_B3B4:
                    bk.handle_font(data)
                elif rc == XL_STYLE:
                    if not self.book._xf_epilogue_done:
                        self.book.xf_epilogue()
                    bk.handle_style(data)
                elif rc == XL_PALETTE:
                    bk.handle_palette(data)
                elif rc == XL_BUILTINFMTCOUNT:
                    bk.handle_builtinfmtcount(data)
                elif rc == XL_XF4 or rc == XL_XF3 or rc == XL_XF2: #### N.B. not XL_XF
                    bk.handle_xf(data)
                elif rc == XL_DATEMODE:
                    bk.handle_datemode(data)
                elif rc == XL_CODEPAGE:
                    bk.handle_codepage(data)
                elif rc == XL_FILEPASS:
                    bk.handle_filepass(data)
                elif rc == XL_WRITEACCESS:
                    bk.handle_writeaccess(data)
                elif rc == XL_IXFE:
                    self._ixfe = local_unpack('<H', data)[0]
                elif rc == XL_NUMBER_B2:
                    rowx, colx, cell_attr, d = local_unpack('<HH3sd', data)
                    self_put_number_cell(rowx, colx, d, self.fixed_BIFF2_xfindex(cell_attr, rowx, colx))
                elif rc == XL_INTEGER:
                    rowx, colx, cell_attr, d = local_unpack('<HH3sH', data)
                    self_put_number_cell(rowx, colx, float(d), self.fixed_BIFF2_xfindex(cell_attr, rowx, colx))
                elif rc == XL_LABEL_B2:
                    rowx, colx, cell_attr = local_unpack('<HH3s', data[0:7])
                    strg = unpack_string(data, 7, bk.encoding or bk.derive_encoding(), lenlen=1)
                    self_put_cell(rowx, colx, XL_CELL_TEXT, strg, self.fixed_BIFF2_xfindex(cell_attr, rowx, colx))
                elif rc == XL_BOOLERR_B2:
                    rowx, colx, cell_attr, value, is_err = local_unpack('<HH3sBB', data)
                    cellty = (XL_CELL_BOOLEAN, XL_CELL_ERROR)[is_err]
                    # if DEBUG: print "XL_BOOLERR_B2", rowx, colx, cell_attr, value, is_err
                    self.put_cell(rowx, colx, cellty, value, self.fixed_BIFF2_xfindex(cell_attr, rowx, colx))
                elif rc == XL_BLANK_B2:
                    if not fmt_info: continue
                    rowx, colx, cell_attr = local_unpack('<HH3s', data[:7])
                    self_put_blank_cell(rowx, colx, self.fixed_BIFF2_xfindex(cell_attr, rowx, colx))
                elif rc == XL_EFONT:
                    bk.handle_efont(data)
                elif rc == XL_ROW_B2:
                    if not fmt_info: continue
                    rowx, bits1, has_defaults = local_unpack('<H4xH2xB', data[0:11])
                    if not(0 <= rowx < self.utter_max_rows):
                        print >> self.logfile, \
                            "*** NOTE: ROW_B2 record has row index %d; " \
                            "should have 0 <= rowx < %d -- record ignored!" \
                            % (rowx, self.utter_max_rows)
                        continue
                    r = Rowinfo()
                    r.height = bits1 & 0x7fff
                    r.has_default_height = (bits1 >> 15) & 1
                    r.outline_level = 0
                    r.outline_group_starts_ends = 0
                    r.hidden = 0
                    r.height_mismatch = 0
                    r.has_default_xf_index = has_defaults & 1
                    r.additional_space_above = 0
                    r.additional_space_below = 0
                    if not r.has_default_xf_index:
                        r.xf_index = -1
                    elif data_len == 18:
                        # Seems the XF index in the cell_attr is dodgy
                        xfx = local_unpack('<H', data[16:18])[0]
                        r.xf_index = self.fixed_BIFF2_xfindex(cell_attr=None, rowx=rowx, colx=-1, true_xfx=xfx)
                    else:
                        cell_attr = data[13:16]
                        r.xf_index = self.fixed_BIFF2_xfindex(cell_attr, rowx, colx=-1)
                    self.rowinfo_map[rowx] = r
                    if 0 and r.xf_index > -1:
                        fprintf(self.logfile,
                            "**ROW %d %d %d\n",
                            self.number, rowx, r.xf_index)
                    if blah_rows:
                        print >> self.logfile, 'ROW_B2', rowx, bits1, has_defaults
                        r.dump(self.logfile,
                            header="--- sh #%d, rowx=%d ---" % (self.number, rowx))
                elif rc == XL_COLWIDTH: # BIFF2 only
                    if not fmt_info: continue
                    first_colx, last_colx, width\
                        = local_unpack("<BBH", data[:4])
                    if not(first_colx <= last_colx):
                        print >> self.logfile, \
                            "*** NOTE: COLWIDTH record has first col index %d, last %d; " \
                            "should have first <= last -- record ignored!" \
                            % (first_colx, last_colx)
                        continue
                    for colx in xrange(first_colx, last_colx+1):
                        if self.colinfo_map.has_key(colx):
                            c = self.colinfo_map[colx]
                        else:
                            c = Colinfo()
                            self.colinfo_map[colx] = c
                        c.width = width
                    if blah:
                        fprintf(
                            self.logfile,
                            "COLWIDTH sheet #%d cols %d-%d: wid=%d\n",
                            self.number, first_colx, last_colx, width
                            )
                elif rc == XL_COLUMNDEFAULT: # BIFF2 only
                    if not fmt_info: continue
                    first_colx, last_colx = local_unpack("<HH", data[:4])
                    #### Warning OOo docs wrong; first_colx <= colx < last_colx
                    if blah:
                        fprintf(
                            self.logfile,
                            "COLUMNDEFAULT sheet #%d cols in range(%d, %d)\n",
                            self.number, first_colx, last_colx
                            )
                    if not(0 <= first_colx < last_colx <= 256):
                        print >> self.logfile, \
                            "*** NOTE: COLUMNDEFAULT record has first col index %d, last %d; " \
                            "should have 0 <= first < last <= 256" \
                            % (first_colx, last_colx)
                        last_colx = min(last_colx, 256)
                    for colx in xrange(first_colx, last_colx):
                        offset = 4 + 3 * (colx - first_colx)
                        cell_attr = data[offset:offset+3]
                        xf_index = self.fixed_BIFF2_xfindex(cell_attr, rowx=-1, colx=colx)
                        if self.colinfo_map.has_key(colx):
                            c = self.colinfo_map[colx]
                        else:
                            c = Colinfo()
                            self.colinfo_map[colx] = c
                        c.xf_index = xf_index
            else:
                # if DEBUG: print "SHEET.READ: Unhandled record type %02x %d bytes %r" % (rc, data_len, data)
                pass
        if not eof_found:
            raise XLRDError("Sheet %d (%r) missing EOF record" \
                % (self.number, self.name))
        self.tidy_dimensions()
        bk._position = oldpos
        return 1

    def fixed_BIFF2_xfindex(self, cell_attr, rowx, colx, true_xfx=None):
        DEBUG = 0
        blah = DEBUG or self.verbosity >= 2
        if self.biff_version == 21:
            if self._xf_index_to_xl_type_map:
                if true_xfx is not None:
                    xfx = true_xfx
                else:
                    xfx = ord(cell_attr[0]) & 0x3F
                if xfx == 0x3F:
                    if self._ixfe is None:
                        raise XLRDError("BIFF2 cell record has XF index 63 but no preceding IXFE record.")
                    xfx = self._ixfe
                    # OOo docs are capable of interpretation that each
                    # cell record is preceded immediately by its own IXFE record.
                    # Empirical evidence is that (sensibly) an IXFE record applies to all
                    # following cell records until another IXFE comes along.
                return xfx
            # Have either Excel 2.0, or broken 2.1 w/o XF records -- same effect.
            self.biff_version = self.book.biff_version = 20
        #### check that XF slot in cell_attr is zero
        xfx_slot = ord(cell_attr[0]) & 0x3F
        assert xfx_slot == 0
        xfx = self._cell_attr_to_xfx.get(cell_attr)
        if xfx is not None:
            return xfx
        if blah:
            fprintf(self.logfile, "New cell_attr %r at (%r, %r)\n", cell_attr, rowx, colx)
        book = self.book
        xf = self.fake_XF_from_BIFF20_cell_attr(cell_attr)
        xfx = len(book.xf_list)
        xf.xf_index = xfx
        book.xf_list.append(xf)
        if blah:
            xf.dump(self.logfile, header="=== Faked XF %d ===" % xfx, footer="======")
        if not book.format_map.has_key(xf.format_key):
            msg = "ERROR *** XF[%d] unknown format key (%d, 0x%04x)\n"
            fprintf(self.logfile, msg,
                    xf.xf_index, xf.format_key, xf.format_key)
            fmt = Format(xf.format_key, FUN, u"General")
            book.format_map[xf.format_key] = fmt
            while len(book.format_list) <= xf.format_key:
                book.format_list.append(fmt)
        cellty_from_fmtty = {
            FNU: XL_CELL_NUMBER,
            FUN: XL_CELL_NUMBER,
            FGE: XL_CELL_NUMBER,
            FDT: XL_CELL_DATE,
            FTX: XL_CELL_NUMBER, # Yes, a number can be formatted as text.
            }
        fmt = book.format_map[xf.format_key]
        cellty = cellty_from_fmtty[fmt.type]
        self._xf_index_to_xl_type_map[xf.xf_index] = cellty
        self._cell_attr_to_xfx[cell_attr] = xfx
        return xfx

    def fake_XF_from_BIFF20_cell_attr(self, cell_attr):
        from formatting import XF, XFAlignment, XFBorder, XFBackground, XFProtection
        xf = XF()
        xf.alignment = XFAlignment()
        xf.alignment.indent_level = 0
        xf.alignment.shrink_to_fit = 0
        xf.alignment.text_direction = 0
        xf.border = XFBorder()
        xf.border.diag_up = 0
        xf.border.diag_down = 0
        xf.border.diag_colour_index = 0
        xf.border.diag_line_style = 0 # no line
        xf.background = XFBackground()
        xf.protection = XFProtection()
        (prot_bits, font_and_format, halign_etc) = unpack('<BBB', cell_attr)
        xf.format_key = font_and_format & 0x3F
        xf.font_index = (font_and_format & 0xC0) >> 6
        upkbits(xf.protection, prot_bits, (
            (6, 0x40, 'cell_locked'),
            (7, 0x80, 'formula_hidden'),
            ))
        xf.alignment.hor_align = halign_etc & 0x07
        for mask, side in ((0x08, 'left'), (0x10, 'right'), (0x20, 'top'), (0x40, 'bottom')):
            if halign_etc & mask:
                colour_index, line_style = 8, 1 # black, thin
            else:
                colour_index, line_style = 0, 0 # none, none
            setattr(xf.border, side + '_colour_index', colour_index)
            setattr(xf.border, side + '_line_style', line_style)
        bg = xf.background
        if halign_etc & 0x80:
            bg.fill_pattern = 17
        else:
            bg.fill_pattern = 0
        bg.background_colour_index = 9 # white
        bg.pattern_colour_index = 8 # black
        xf.parent_style_index = 0 # ???????????
        xf.alignment.vert_align = 2 # bottom
        xf.alignment.rotation = 0
        for attr_stem in \
            "format font alignment border background protection".split():
            attr = "_" + attr_stem + "_flag"
            setattr(xf, attr, 1)
        return xf

    def req_fmt_info(self):
        if not self.formatting_info:
            raise XLRDError("Feature requires open_workbook(..., formatting_info=True)")

    ##
    # Determine column display width.
    # <br /> -- New in version 0.6.1
    # <br />
    # @param colx Index of the queried column, range 0 to 255.
    # Note that it is possible to find out the width that will be used to display
    # columns with no cell information e.g. column IV (colx=255).
    # @return The column width that will be used for displaying
    # the given column by Excel, in units of 1/256th of the width of a
    # standard character (the digit zero in the first font).

    def computed_column_width(self, colx):
        self.req_fmt_info()
        if self.biff_version >= 80:
            colinfo = self.colinfo_map.get(colx, None)
            if colinfo is not None:
                return colinfo.width
            if self.standardwidth is not None:
                return self.standardwidth
        elif self.biff_version >= 40:
            if self.gcw[colx]:
                if self.standardwidth is not None:
                    return self.standardwidth
            else:
                colinfo = self.colinfo_map.get(colx, None)
                if colinfo is not None:
                    return colinfo.width
        elif self.biff_version == 30:
            colinfo = self.colinfo_map.get(colx, None)
            if colinfo is not None:
                return colinfo.width
        # All roads lead to Rome and the DEFCOLWIDTH ...
        if self.defcolwidth is not None:
            return self.defcolwidth * 256
        return 8 * 256 # 8 is what Excel puts in a DEFCOLWIDTH record

    def handle_msodrawingetc(self, recid, data_len, data):
        if not OBJ_MSO_DEBUG:
            return
        DEBUG = 1
        if self.biff_version < 80:
            return
        o = MSODrawing()
        pos = 0
        while pos < data_len:
            tmp, fbt, cb = unpack('<HHI', data[pos:pos+8])
            ver = tmp & 0xF
            inst = (tmp >> 4) & 0xFFF
            if ver == 0xF:
                ndb = 0 # container
            else:
                ndb = cb
            if DEBUG:
                hex_char_dump(data, pos, ndb + 8, base=0, fout=self.logfile)
                fprintf(self.logfile,
                    "fbt:0x%04X  inst:%d  ver:0x%X  cb:%d (0x%04X)\n",
                    fbt, inst, ver, cb, cb)
            if fbt == 0xF010: # Client Anchor
                assert ndb == 18
                (o.anchor_unk,
                o.anchor_colx_lo, o.anchor_rowx_lo,
                o.anchor_colx_hi, o.anchor_rowx_hi) = unpack('<Hiiii', data[pos+8:pos+8+ndb])
            elif fbt == 0xF011: # Client Data
                # must be followed by an OBJ record
                assert cb == 0
                assert pos + 8 == data_len
            else:
                pass
            pos += ndb + 8
        else:
            # didn't break out of while loop
            assert pos == data_len
        if DEBUG:
            o.dump(self.logfile, header="=== MSODrawing ===", footer= " ")


    def handle_obj(self, data):
        if not OBJ_MSO_DEBUG:
            return
        DEBUG = 1
        if self.biff_version < 80:
            return
        o = MSObj()
        data_len = len(data)
        pos = 0
        if DEBUG:
            fprintf(self.logfile, "... OBJ record ...\n")
        while pos < data_len:
            ft, cb = unpack('<HH', data[pos:pos+4])
            if DEBUG:
                hex_char_dump(data, pos, cb, base=0, fout=self.logfile)
            if ft == 0x15: # ftCmo ... s/b first
                assert pos == 0
                o.type, o.id, option_flags = unpack('<HHH', data[pos+4:pos+10])
                upkbits(o, option_flags, (
                    ( 0, 0x0001, 'locked'),
                    ( 4, 0x0010, 'printable'),
                    ( 8, 0x0100, 'autofilter'), # not documented in Excel 97 dev kit
                    ( 9, 0x0200, 'scrollbar_flag'), # not documented in Excel 97 dev kit
                    (13, 0x2000, 'autofill'),
                    (14, 0x4000, 'autoline'),
                    ))
            elif ft == 0x00:
                assert cb == 0
                assert pos + 4 == data_len
            elif ft == 0x0C: # Scrollbar
                values = unpack('<5H', data[pos+8:pos+18])
                for value, tag in zip(values, ('value', 'min', 'max', 'inc', 'page')):
                    setattr(o, 'scrollbar_' + tag, value)
            elif ft == 0x0D: # "Notes structure" [used for cell comments]
                pass ############## not documented in Excel 97 dev kit
            elif ft == 0x13: # list box data
                if o.autofilter: # non standard exit. NOT documented
                    break
            else:
                pass
            pos += cb + 4
        else:
            # didn't break out of while loop
            assert pos == data_len
        if DEBUG:
            o.dump(self.logfile, header="=== MSOBj ===", footer= " ")

    def handle_note(self, data):
        if not OBJ_MSO_DEBUG:
            return
        DEBUG = 1
        if self.biff_version < 80:
            return
        if DEBUG:
            fprintf(self.logfile, '... NOTE record ...\n')
            hex_char_dump(data, 0, len(data), base=0, fout=self.logfile)
        o = MSNote()
        data_len = len(data)
        o.rowx, o.colx, option_flags, o.object_id = unpack('<4H', data[:8])
        o.show = (option_flags >> 1) & 1
        # Docs say NULL [sic] bytes padding between string count and string data
        # to ensure that string is word-aligned. Appears to be nonsense.
        # There also seems to be a random(?) byte after the string (not counted in the
        # string length.
        o.original_author, endpos = unpack_unicode_update_pos(data, 8, lenlen=2)
        assert endpos == data_len - 1
        o.last_byte = data[-1]
        if DEBUG:
            o.dump(self.logfile, header="=== MSNote ===", footer= " ")

    def handle_txo(self, data):
        if not OBJ_MSO_DEBUG:
            return
        DEBUG = 1
        if self.biff_version < 80:
            return
        o = MSTxo()
        data_len = len(data)
        option_flags, o.rot, cchText, cbRuns = unpack('<HH6xHH4x', data)
        upkbits(o, option_flags, (
            (3, 0x000E, 'horz_align'),
            (6, 0x0070, 'vert_align'),
            (9, 0x0200, 'lock_text'),
            ))
        rc2, data2_len, data2 = self.book.get_record_parts()
        assert rc2 == XL_CONTINUE
        o.text, endpos = unpack_unicode_update_pos(data2, 0, known_len=cchText)
        assert endpos == data2_len
        rc3, data3_len, data3 = self.book.get_record_parts()
        assert rc3 == XL_CONTINUE
        # ignore the formatting runs for the moment
        if DEBUG:
            o.dump(self.logfile, header="=== MSTxo ===", footer= " ")

    def handle_feat11(self, data):
        if not OBJ_MSO_DEBUG:
            return
        # rt: Record type; this matches the BIFF rt in the first two bytes of the record; =0872h
        # grbitFrt: FRT cell reference flag (see table below for details)
        # Ref0: Range reference to a worksheet cell region if grbitFrt=1 (bitFrtRef). Otherwise blank.
        # isf: Shared feature type index =5 for Table
        # fHdr: =0 since this is for feat not feat header
        # reserved0: Reserved for future use =0 for Table
        # cref: Count of ref ranges this feature is on
        # cbFeatData: Count of byte for the current feature data.
        # reserved1: =0 currently not used
        # Ref1: Repeat of Ref0. UNDOCUMENTED
        rt, grbitFrt, Ref0, isf, fHdr, reserved0, cref, cbFeatData, reserved1, Ref1 = unpack('<HH8sHBiHiH8s', data[0:35])
        assert reserved0 == 0
        assert reserved1 == 0
        assert isf == 5
        assert rt == 0x872
        assert fHdr == 0
        assert Ref1 == Ref0
        print "FEAT11: grbitFrt=%d  Ref0=%r cref=%d cbFeatData=%d" % (grbitFrt, Ref0, cref, cbFeatData)
        # lt: Table data source type:
        #   =0 for Excel Worksheet Table =1 for read-write SharePoint linked List
        #   =2 for XML mapper Table =3 for Query Table
        # idList: The ID of the Table (unique per worksheet)
        # crwHeader: How many header/title rows the Table has at the top
        # crwTotals: How many total rows the Table has at the bottom
        # idFieldNext: Next id to try when assigning a unique id to a new field
        # cbFSData: The size of the Fixed Data portion of the Table data structure.
        # rupBuild: the rupBuild that generated the record
        # unusedShort: UNUSED short that can be used later. The value is reserved during round-tripping.
        # listFlags: Collection of bit flags: (see listFlags' bit setting table below for detail.)
        # lPosStmCache: Table data stream position of cached data
        # cbStmCache: Count of bytes of cached data
        # cchStmCache: Count of characters of uncompressed cached data in the stream
        # lem: Table edit mode (see List (Table) Editing Mode (lem) setting table below for details.)
        # rgbHashParam: Hash value for SharePoint Table
        # cchName: Count of characters in the Table name string rgbName
        (lt, idList, crwHeader, crwTotals, idFieldNext, cbFSData,
        rupBuild, unusedShort, listFlags, lPosStmCache, cbStmCache,
        cchStmCache, lem, rgbHashParam, cchName) = unpack('<iiiiiiHHiiiii16sH', data[35:35+66])
        print "lt=%d  idList=%d crwHeader=%d  crwTotals=%d  idFieldNext=%d cbFSData=%d\n"\
            "rupBuild=%d  unusedShort=%d listFlags=%04X  lPosStmCache=%d  cbStmCache=%d\n"\
            "cchStmCache=%d  lem=%d  rgbHashParam=%r  cchName=%d" % (
            lt, idList, crwHeader, crwTotals, idFieldNext, cbFSData,
            rupBuild, unusedShort,listFlags, lPosStmCache, cbStmCache,
            cchStmCache, lem, rgbHashParam, cchName)

class MSODrawing(BaseObject):
    pass

class MSObj(BaseObject):
    pass

class MSTxo(BaseObject):
    pass

class MSNote(BaseObject):
    pass

# === helpers ===

def unpack_RK(rk_str):
    flags = ord(rk_str[0])
    if flags & 2:
        # There's a SIGNED 30-bit integer in there!
        i,  = unpack('<i', rk_str)
        i >>= 2 # div by 4 to drop the 2 flag bits
        if flags & 1:
            return i / 100.0
        return float(i)
    else:
        # It's the most significant 30 bits of an IEEE 754 64-bit FP number
        d, = unpack('<d', '\0\0\0\0' + chr(flags & 252) + rk_str[1:4])
        if flags & 1:
            return d / 100.0
        return d

##### =============== Cell ======================================== #####

cellty_from_fmtty = {
    FNU: XL_CELL_NUMBER,
    FUN: XL_CELL_NUMBER,
    FGE: XL_CELL_NUMBER,
    FDT: XL_CELL_DATE,
    FTX: XL_CELL_NUMBER, # Yes, a number can be formatted as text.
    }

ctype_text = {
    XL_CELL_EMPTY: 'empty',
    XL_CELL_TEXT: 'text',
    XL_CELL_NUMBER: 'number',
    XL_CELL_DATE: 'xldate',
    XL_CELL_BOOLEAN: 'bool',
    XL_CELL_ERROR: 'error',
    XL_CELL_BLANK: 'blank',
    }

##
# <p>Contains the data for one cell.</p>
#
# <p>WARNING: You don't call this class yourself. You access Cell objects
# via methods of the Sheet object(s) that you found in the Book object that
# was returned when you called xlrd.open_workbook("myfile.xls").</p>
# <p> Cell objects have three attributes: <i>ctype</i> is an int, <i>value</i>
# (which depends on <i>ctype</i>) and <i>xf_index</i>.
# If "formatting_info" is not enabled when the workbook is opened, xf_index will be None.
# The following table describes the types of cells and how their values
# are represented in Python.</p>
#
# <table border="1" cellpadding="7">
# <tr>
# <th>Type symbol</th>
# <th>Type number</th>
# <th>Python value</th>
# </tr>
# <tr>
# <td>XL_CELL_EMPTY</td>
# <td align="center">0</td>
# <td>empty string u''</td>
# </tr>
# <tr>
# <td>XL_CELL_TEXT</td>
# <td align="center">1</td>
# <td>a Unicode string</td>
# </tr>
# <tr>
# <td>XL_CELL_NUMBER</td>
# <td align="center">2</td>
# <td>float</td>
# </tr>
# <tr>
# <td>XL_CELL_DATE</td>
# <td align="center">3</td>
# <td>float</td>
# </tr>
# <tr>
# <td>XL_CELL_BOOLEAN</td>
# <td align="center">4</td>
# <td>int; 1 means TRUE, 0 means FALSE</td>
# </tr>
# <tr>
# <td>XL_CELL_ERROR</td>
# <td align="center">5</td>
# <td>int representing internal Excel codes; for a text representation,
# refer to the supplied dictionary error_text_from_code</td>
# </tr>
# <tr>
# <td>XL_CELL_BLANK</td>
# <td align="center">6</td>
# <td>empty string u''. Note: this type will appear only when
# open_workbook(..., formatting_info=True) is used.</td>
# </tr>
# </table>
#<p></p>

class Cell(BaseObject):

    __slots__ = ['ctype', 'value', 'xf_index']

    def __init__(self, ctype, value, xf_index=None):
        self.ctype = ctype
        self.value = value
        self.xf_index = xf_index

    def __repr__(self):
        if self.xf_index is None:
            return "%s:%r" % (ctype_text[self.ctype], self.value)
        else:
            return "%s:%r (XF:%r)" % (ctype_text[self.ctype], self.value, self.xf_index)

##
# There is one and only one instance of an empty cell -- it's a singleton. This is it.
# You may use a test like "acell is empty_cell".
empty_cell = Cell(XL_CELL_EMPTY, '')

##### =============== Colinfo and Rowinfo ============================== #####

##
# Width and default formatting information that applies to one or
# more columns in a sheet. Derived from COLINFO records.
#
# <p> Here is the default hierarchy for width, according to the OOo docs:
#
# <br />"""In BIFF3, if a COLINFO record is missing for a column,
# the width specified in the record DEFCOLWIDTH is used instead.
#
# <br />In BIFF4-BIFF7, the width set in this [COLINFO] record is only used,
# if the corresponding bit for this column is cleared in the GCW
# record, otherwise the column width set in the DEFCOLWIDTH record
# is used (the STANDARDWIDTH record is always ignored in this case [see footnote!]).
#
# <br />In BIFF8, if a COLINFO record is missing for a column,
# the width specified in the record STANDARDWIDTH is used.
# If this [STANDARDWIDTH] record is also missing,
# the column width of the record DEFCOLWIDTH is used instead."""
# <br />
#
# Footnote:  The docs on the GCW record say this:
# """<br />
# If a bit is set, the corresponding column uses the width set in the STANDARDWIDTH
# record. If a bit is cleared, the corresponding column uses the width set in the
# COLINFO record for this column.
# <br />If a bit is set, and the worksheet does not contain the STANDARDWIDTH record, or if
# the bit is cleared, and the worksheet does not contain the COLINFO record, the DEFCOLWIDTH
# record of the worksheet will be used instead.
# <br />"""<br />
# At the moment (2007-01-17) xlrd is going with the GCW version of the story.
# Reference to the source may be useful: see the computed_column_width(colx) method
# of the Sheet class.
# <br />-- New in version 0.6.1
# </p>

class Colinfo(BaseObject):
    ##
    # Width of the column in 1/256 of the width of the zero character,
    # using default font (first FONT record in the file).
    width = 0
    ##
    # XF index to be used for formatting empty cells.
    xf_index = -1
    ##
    # 1 = column is hidden
    hidden = 0
    ##
    # Value of a 1-bit flag whose purpose is unknown
    # but is often seen set to 1
    bit1_flag = 0
    ##
    # Outline level of the column, in range(7).
    # (0 = no outline)
    outline_level = 0
    ##
    # 1 = column is collapsed
    collapsed = 0

##
# Height and default formatting information that applies to a row in a sheet.
# Derived from ROW records.
# <br /> -- New in version 0.6.1

class Rowinfo(BaseObject):
    ##
    # Height of the row, in twips. One twip == 1/20 of a point
    height = 0
    ##
    # 0 = Row has custom height; 1 = Row has default height
    has_default_height = 0
    ##
    # Outline level of the row
    outline_level = 0
    ##
    # 1 = Outline group starts or ends here (depending on where the
    # outline buttons are located, see WSBOOL record [TODO ??]),
    # <i>and</i> is collapsed
    outline_group_starts_ends = 0
    ##
    # 1 = Row is hidden (manually, or by a filter or outline group)
    hidden = 0
    ##
    # 1 = Row height and default font height do not match
    height_mismatch = 0
    ##
    # 1 = the xf_index attribute is usable; 0 = ignore it
    has_default_xf_index = 0
    ##
    # Index to default XF record for empty cells in this row.
    # Don't use this if has_default_xf_index == 0.
    xf_index = -9999
    ##
    # This flag is set, if the upper border of at least one cell in this row
    # or if the lower border of at least one cell in the row above is
    # formatted with a thick line style. Thin and medium line styles are not
    # taken into account.
    additional_space_above = 0
    ##
    # This flag is set, if the lower border of at least one cell in this row
    # or if the upper border of at least one cell in the row below is
    # formatted with a medium or thick line style. Thin line styles are not
    # taken into account.
    additional_space_below = 0

########NEW FILE########
__FILENAME__ = timemachine
# -*- coding: cp1252 -*-

##
# <p>Copyright  2006-2008 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under a BSD-style licence.</p>
##

# timemachine.py -- adaptation for earlier Pythons e.g. 2.1
# usage: from timemachine import *

# 2008-02-08 SJM Generalised method of detecting IronPython

import sys

python_version = sys.version_info[:2] # e.g. version 2.4 -> (2, 4)

CAN_PICKLE_ARRAY = python_version >= (2, 5)
CAN_SUBCLASS_BUILTIN = python_version >= (2, 2)

if sys.version.find("IronPython") >= 0:
    array_array = None
else:
    from array import array as array_array

if python_version < (2, 2):
    class object:
        pass
    False = 0
    True = 1

def int_floor_div(x, y):
    return divmod(x, y)[0]

def intbool(x):
    if x:
        return 1
    return 0

if python_version < (2, 3):
    def sum(sequence, start=0):
        tot = start
        for item in aseq:
            tot += item
        return tot

########NEW FILE########
__FILENAME__ = xldate
# -*- coding: cp1252 -*-

# No part of the content of this file was derived from the works of David Giffin.

##
# <p>Copyright  2005-2008 Stephen John Machin, Lingfo Pty Ltd</p>
# <p>This module is part of the xlrd package, which is released under a BSD-style licence.</p>
#
# <p>Provides function(s) for dealing with Microsoft Excel  dates.</p>
##

# 2008-10-18 SJM Fix bug in xldate_from_date_tuple (affected some years after 2099)

# The conversion from days to (year, month, day) starts with
# an integral "julian day number" aka JDN.
# FWIW, JDN 0 corresponds to noon on Monday November 24 in Gregorian year -4713.
# More importantly:
#    Noon on Gregorian 1900-03-01 (day 61 in the 1900-based system) is JDN 2415080.0
#    Noon on Gregorian 1904-01-02 (day  1 in the 1904-based system) is JDN 2416482.0

from timemachine import int_floor_div as ifd

_JDN_delta = (2415080 - 61, 2416482 - 1)
assert _JDN_delta[1] - _JDN_delta[0] == 1462

class XLDateError(ValueError): pass

class XLDateNegative(XLDateError): pass
class XLDateAmbiguous(XLDateError): pass
class XLDateTooLarge(XLDateError): pass
class XLDateBadDatemode(XLDateError): pass
class XLDateBadTuple(XLDateError): pass

_XLDAYS_TOO_LARGE = (2958466, 2958466 - 1462) # This is equivalent to 10000-01-01

##
# Convert an Excel number (presumed to represent a date, a datetime or a time) into
# a tuple suitable for feeding to datetime or mx.DateTime constructors.
# @param xldate The Excel number
# @param datemode 0: 1900-based, 1: 1904-based.
# <br>WARNING: when using this function to
# interpret the contents of a workbook, you should pass in the Book.datemode
# attribute of that workbook. Whether
# the workbook has ever been anywhere near a Macintosh is irrelevant.
# @return Gregorian (year, month, day, hour, minute, nearest_second).
# <br>Special case: if 0.0 <= xldate < 1.0, it is assumed to represent a time;
# (0, 0, 0, hour, minute, second) will be returned.
# <br>Note: 1904-01-01 is not regarded as a valid date in the datemode 1 system; its "serial number"
# is zero.
# @throws XLDateNegative xldate < 0.00
# @throws XLDateAmbiguous The 1900 leap-year problem (datemode == 0 and 1.0 <= xldate < 61.0)
# @throws XLDateTooLarge Gregorian year 10000 or later
# @throws XLDateBadDatemode datemode arg is neither 0 nor 1
# @throws XLDateError Covers the 4 specific errors

def xldate_as_tuple(xldate, datemode):
    if datemode not in (0, 1):
        raise XLDateBadDatemode(datemode)
    if xldate == 0.00:
        return (0, 0, 0, 0, 0, 0)
    if xldate < 0.00:
        raise XLDateNegative(xldate)
    xldays = int(xldate)
    frac = xldate - xldays
    seconds = int(round(frac * 86400.0))
    assert 0 <= seconds <= 86400
    if seconds == 86400:
        hour = minute = second = 0
        xldays += 1
    else:
        # second = seconds % 60; minutes = seconds // 60
        minutes, second = divmod(seconds, 60)
        # minute = minutes % 60; hour    = minutes // 60
        hour, minute = divmod(minutes, 60)
    if xldays >= _XLDAYS_TOO_LARGE[datemode]:
        raise XLDateTooLarge(xldate)

    if xldays == 0:
        return (0, 0, 0, hour, minute, second)

    if xldays < 61 and datemode == 0:
        raise XLDateAmbiguous(xldate)

    jdn = xldays + _JDN_delta[datemode]
    yreg = (ifd(ifd(jdn * 4 + 274277, 146097) * 3, 4) + jdn + 1363) * 4 + 3
    mp = ifd(yreg % 1461, 4) * 535 + 333
    d = ifd(mp % 16384, 535) + 1
    # mp /= 16384
    mp >>= 14
    if mp >= 10:
        return (ifd(yreg, 1461) - 4715, mp - 9, d, hour, minute, second)
    else:
        return (ifd(yreg, 1461) - 4716, mp + 3, d, hour, minute, second)

# === conversions from date/time to xl numbers

def _leap(y):
    if y % 4: return 0
    if y % 100: return 1
    if y % 400: return 0
    return 1

_days_in_month = (None, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)

##
# Convert a date tuple (year, month, day) to an Excel date.
# @param year Gregorian year.
# @param month 1 <= month <= 12
# @param day 1 <= day <= last day of that (year, month)
# @param datemode 0: 1900-based, 1: 1904-based.
# @throws XLDateAmbiguous The 1900 leap-year problem (datemode == 0 and 1.0 <= xldate < 61.0)
# @throws XLDateBadDatemode datemode arg is neither 0 nor 1
# @throws XLDateBadTuple (year, month, day) is too early/late or has invalid component(s)
# @throws XLDateError Covers the specific errors

def xldate_from_date_tuple((year, month, day), datemode):

    if datemode not in (0, 1):
        raise XLDateBadDatemode(datemode)

    if year == 0 and month == 0 and day == 0:
        return 0.00

    if not (1900 <= year <= 9999):
        raise XLDateBadTuple("Invalid year: %r" % ((year, month, day),))
    if not (1 <= month <= 12):
        raise XLDateBadTuple("Invalid month: %r" % ((year, month, day),))
    if  day < 1 \
    or (day > _days_in_month[month] and not(day == 29 and month == 2 and _leap(year))):
        raise XLDateBadTuple("Invalid day: %r" % ((year, month, day),))

    Yp = year + 4716
    M = month
    if M <= 2:
        Yp = Yp - 1
        Mp = M + 9
    else:
        Mp = M - 3
    jdn = ifd(1461 * Yp, 4) + ifd(979 * Mp + 16, 32) + \
        day - 1364 - ifd(ifd(Yp + 184, 100) * 3, 4)
    xldays = jdn - _JDN_delta[datemode]
    if xldays <= 0:
        raise XLDateBadTuple("Invalid (year, month, day): %r" % ((year, month, day),))
    if xldays < 61 and datemode == 0:
        raise XLDateAmbiguous("Before 1900-03-01: %r" % ((year, month, day),))
    return float(xldays)

##
# Convert a time tuple (hour, minute, second) to an Excel "date" value (fraction of a day).
# @param hour 0 <= hour < 24
# @param minute 0 <= minute < 60
# @param second 0 <= second < 60
# @throws XLDateBadTuple Out-of-range hour, minute, or second

def xldate_from_time_tuple((hour, minute, second)):
    if 0 <= hour < 24 and 0 <= minute < 60 and 0 <= second < 60:
        return ((second / 60.0 + minute) / 60.0 + hour) / 24.0
    raise XLDateBadTuple("Invalid (hour, minute, second): %r" % ((hour, minute, second),))

##
# Convert a datetime tuple (year, month, day, hour, minute, second) to an Excel date value.
# For more details, refer to other xldate_from_*_tuple functions.
# @param datetime_tuple (year, month, day, hour, minute, second)
# @param datemode 0: 1900-based, 1: 1904-based.

def xldate_from_datetime_tuple(datetime_tuple, datemode):
    return (
        xldate_from_date_tuple(datetime_tuple[:3], datemode)
        +
        xldate_from_time_tuple(datetime_tuple[3:])
        )

########NEW FILE########
__FILENAME__ = antlr
## This file is part of PyANTLR. See LICENSE.txt for license
## details..........Copyright (C) Wolfgang Haefelinger, 2004.

## This file was copied for use with xlwt from the 2.7.7 ANTLR distribution. Yes, it
## says 2.7.5 below. The 2.7.5 distribution version didn't have a
## version in it.

## Here is the contents of the ANTLR 2.7.7 LICENSE.txt referred to above.

# SOFTWARE RIGHTS
#
# ANTLR 1989-2006 Developed by Terence Parr
# Partially supported by University of San Francisco & jGuru.com
#
# We reserve no legal rights to the ANTLR--it is fully in the
# public domain. An individual or company may do whatever
# they wish with source code distributed with ANTLR or the
# code generated by ANTLR, including the incorporation of
# ANTLR, or its output, into commerical software.
#
# We encourage users to develop software with ANTLR. However,
# we do ask that credit is given to us for developing
# ANTLR. By "credit", we mean that if you use ANTLR or
# incorporate any source code into one of your programs
# (commercial product, research project, or otherwise) that
# you acknowledge this fact somewhere in the documentation,
# research report, etc... If you like ANTLR and have
# developed a nice tool with the output, please mention that
# you developed it using ANTLR. In addition, we ask that the
# headers remain intact in our source code. As long as these
# guidelines are kept, we expect to continue enhancing this
# system and expect to make other tools available as they are
# completed.
#
# The primary ANTLR guy:
#
# Terence Parr
# parrt@cs.usfca.edu
# parrt@antlr.org

## End of contents of the ANTLR 2.7.7 LICENSE.txt ########################

## get sys module
import sys

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     global symbols                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### ANTLR Standard Tokens
SKIP                = -1
INVALID_TYPE        = 0
EOF_TYPE            = 1
EOF                 = 1
NULL_TREE_LOOKAHEAD = 3
MIN_USER_TYPE       = 4

### ANTLR's EOF Symbol
EOF_CHAR            = ''

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    general functions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## Version should be automatically derived from configure.in. For now,
## we need to bump it ourselfs. Don't remove the <version> tags.
## <version>
def version():
    r = {
        'major'  : '2',
        'minor'  : '7',
        'micro'  : '5',
        'patch'  : '' ,
        'version': '2.7.5'
        }
    return r
## </version>

def error(fmt,*args):
    if fmt:
        print "error: ", fmt % tuple(args)

def ifelse(cond,_then,_else):
    if cond :
        r = _then
    else:
        r = _else
    return r

def is_string_type(x):
    # return  (isinstance(x,str) or isinstance(x,unicode))
    # Simplify; xlwt doesn't support Python < 2.3
    return isinstance(basestring)

def assert_string_type(x):
    assert is_string_type(x)
    pass

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     ANTLR Exceptions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ANTLRException(Exception):

    def __init__(self, *args):
        Exception.__init__(self, *args)


class RecognitionException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)
        self.fileName = None
        self.line = -1
        self.column = -1
        if len(args) >= 2:
            self.fileName = args[1]
        if len(args) >= 3:
            self.line = args[2]
        if len(args) >= 4:
            self.column = args[3]

    def __str__(self):
        buf = ['']
        if self.fileName:
            buf.append(self.fileName + ":")
        if self.line != -1:
            if not self.fileName:
                buf.append("line ")
            buf.append(str(self.line))
            if self.column != -1:
                buf.append(":" + str(self.column))
            buf.append(":")
        buf.append(" ")
        return str('').join(buf)

    __repr__ = __str__


class NoViableAltException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)
        self.token = None
        self.node  = None
        if isinstance(args[0],AST):
            self.node = args[0]
        elif isinstance(args[0],Token):
            self.token = args[0]
        else:
            raise TypeError("NoViableAltException requires Token or AST argument")

    def __str__(self):
        if self.token:
            line = self.token.getLine()
            col  = self.token.getColumn()
            text = self.token.getText()
            return "unexpected symbol at line %s (column %s): \"%s\"" % (line,col,text)
        if self.node == ASTNULL:
            return "unexpected end of subtree"
        assert self.node
        ### hackish, we assume that an AST contains method getText
        return "unexpected node: %s" % (self.node.getText())

    __repr__ = __str__


class NoViableAltForCharException(RecognitionException):

    def __init__(self, *args):
        self.foundChar = None
        if len(args) == 2:
            self.foundChar = args[0]
            scanner = args[1]
            RecognitionException.__init__(self, "NoViableAlt",
                                          scanner.getFilename(),
                                          scanner.getLine(),
                                          scanner.getColumn())
        elif len(args) == 4:
            self.foundChar = args[0]
            fileName = args[1]
            line = args[2]
            column = args[3]
            RecognitionException.__init__(self, "NoViableAlt",
                                          fileName, line, column)
        else:
            RecognitionException.__init__(self, "NoViableAlt",
                                          '', -1, -1)

    def __str__(self):
        mesg = "unexpected char: "
        if self.foundChar >= ' ' and self.foundChar <= '~':
            mesg += "'" + self.foundChar + "'"
        elif self.foundChar:
            mesg += "0x" + hex(ord(self.foundChar)).upper()[2:]
        else:
            mesg += "<None>"
        return mesg

    __repr__ = __str__


class SemanticException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)


class MismatchedCharException(RecognitionException):

    NONE = 0
    CHAR = 1
    NOT_CHAR = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args = args
        if len(args) == 5:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedCharException.NOT_RANGE
            else:
                self.mismatchType = MismatchedCharException.RANGE
            self.foundChar = args[0]
            self.expecting = args[1]
            self.upper = args[2]
            self.scanner = args[4]
            RecognitionException.__init__(self, "Mismatched char range",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and is_string_type(args[1]):
            # Expected char / not char
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_CHAR
            else:
                self.mismatchType = MismatchedCharException.CHAR
            self.foundChar = args[0]
            self.expecting = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and isinstance(args[1], BitSet):
            # Expected BitSet / not BitSet
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_SET
            else:
                self.mismatchType = MismatchedCharException.SET
            self.foundChar = args[0]
            self.set = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char set",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        else:
            self.mismatchType = MismatchedCharException.NONE
            RecognitionException.__init__(self, "Mismatched char")

    ## Append a char to the msg buffer.  If special,
    #  then show escaped version
    #
    def appendCharName(self, sb, c):
        if not c or c == 65535:
            # 65535 = (char) -1 = EOF
            sb.append("'<EOF>'")
        elif c == '\n':
            sb.append("'\\n'")
        elif c == '\r':
            sb.append("'\\r'");
        elif c == '\t':
            sb.append("'\\t'")
        else:
            sb.append('\'' + c + '\'')

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedCharException.CHAR:
            sb.append("expecting ")
            self.appendCharName(sb, self.expecting)
            sb.append(", found ")
            self.appendCharName(sb, self.foundChar)
        elif self.mismatchType == MismatchedCharException.NOT_CHAR:
            sb.append("expecting anything but '")
            self.appendCharName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedCharException.RANGE, MismatchedCharException.NOT_RANGE]:
            sb.append("expecting char ")
            if self.mismatchType == MismatchedCharException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendCharName(sb, self.expecting)
            sb.append("..")
            appendCharName(sb, self.upper)
            sb.append(", found ")
            appendCharName(sb, self.foundChar)
        elif self.mismatchType in [MismatchedCharException.SET, MismatchedCharException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedCharException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendCharName(sb, self.set[i])
            sb.append("), found ")
            self.appendCharName(sb, self.foundChar)

        return str().join(sb).strip()

    __repr__ = __str__


class MismatchedTokenException(RecognitionException):

    NONE = 0
    TOKEN = 1
    NOT_TOKEN = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args =  args
        self.tokenNames = []
        self.token = None
        self.tokenText = ''
        self.node =  None
        if len(args) == 6:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_RANGE
            else:
                self.mismatchType = MismatchedTokenException.RANGE
            self.tokenNames = args[0]
            self.expecting = args[2]
            self.upper = args[3]
            self.fileName = args[5]

        elif len(args) == 4 and isinstance(args[2], int):
            # Expected token / not token
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_TOKEN
            else:
                self.mismatchType = MismatchedTokenException.TOKEN
            self.tokenNames = args[0]
            self.expecting = args[2]

        elif len(args) == 4 and isinstance(args[2], BitSet):
            # Expected BitSet / not BitSet
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_SET
            else:
                self.mismatchType = MismatchedTokenException.SET
            self.tokenNames = args[0]
            self.set = args[2]

        else:
            self.mismatchType = MismatchedTokenException.NONE
            RecognitionException.__init__(self, "Mismatched Token: expecting any AST node", "<AST>", -1, -1)

        if len(args) >= 2:
            if isinstance(args[1],Token):
                self.token = args[1]
                self.tokenText = self.token.getText()
                RecognitionException.__init__(self, "Mismatched Token",
                                              self.fileName,
                                              self.token.getLine(),
                                              self.token.getColumn())
            elif isinstance(args[1],AST):
                self.node = args[1]
                self.tokenText = str(self.node)
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>",
                                              self.node.getLine(),
                                              self.node.getColumn())
            else:
                self.tokenText = "<empty tree>"
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>", -1, -1)

    def appendTokenName(self, sb, tokenType):
        if tokenType == INVALID_TYPE:
            sb.append("<Set of tokens>")
        elif tokenType < 0 or tokenType >= len(self.tokenNames):
            sb.append("<" + str(tokenType) + ">")
        else:
            sb.append(self.tokenNames[tokenType])

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedTokenException.TOKEN:
            sb.append("expecting ")
            self.appendTokenName(sb, self.expecting)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType == MismatchedTokenException.NOT_TOKEN:
            sb.append("expecting anything but '")
            self.appendTokenName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedTokenException.RANGE, MismatchedTokenException.NOT_RANGE]:
            sb.append("expecting token ")
            if self.mismatchType == MismatchedTokenException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendTokenName(sb, self.expecting)
            sb.append("..")
            appendTokenName(sb, self.upper)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType in [MismatchedTokenException.SET, MismatchedTokenException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedTokenException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendTokenName(sb, self.set[i])
            sb.append("), found " + self.tokenText)

        return str().join(sb).strip()

    __repr__ = __str__


class TokenStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a TokenStreamException
class TokenStreamIOException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            TokenStreamException.__init__(self, str(io))
            self.io = io
        else:
            TokenStreamException.__init__(self, *args)
            self.io = self


# Wraps a RecognitionException in a TokenStreamException
class TokenStreamRecognitionException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], RecognitionException):
            recog = args[0]
            TokenStreamException.__init__(self, str(recog))
            self.recog = recog
        else:
            raise TypeError("TokenStreamRecognitionException requires RecognitionException argument")

    def __str__(self):
        return str(self.recog)

    __repr__ = __str__


class TokenStreamRetryException(TokenStreamException):

    def __init__(self, *args):
        TokenStreamException.__init__(self, *args)


class CharStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a CharStreamException
class CharStreamIOException(CharStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            CharStreamException.__init__(self, str(io))
            self.io = io
        else:
            CharStreamException.__init__(self, *args)
            self.io = self


class TryAgain(Exception):
    pass


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Token                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Token(object):
    SKIP                = -1
    INVALID_TYPE        = 0
    EOF_TYPE            = 1
    EOF                 = 1
    NULL_TREE_LOOKAHEAD = 3
    MIN_USER_TYPE       = 4

    def __init__(self,**argv):
        try:
            self.type = argv['type']
        except:
            self.type = INVALID_TYPE
        try:
            self.text = argv['text']
        except:
            self.text = "<no text>"

    def isEOF(self):
        return (self.type == EOF_TYPE)

    def getColumn(self):
        return 0

    def getLine(self):
        return 0

    def getFilename(self):
        return None

    def setFilename(self,name):
        return self

    def getText(self):
        return "<no text>"

    def setText(self,text):
        if is_string_type(text):
            pass
        else:
            raise TypeError("Token.setText requires string argument")
        return self

    def setColumn(self,column):
        return self

    def setLine(self,line):
        return self

    def getType(self):
        return self.type

    def setType(self,type):
        if isinstance(type,int):
            self.type = type
        else:
            raise TypeError("Token.setType requires integer argument")
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        return '["%s",<%s>]' % (self.getText(),tval)

    __str__ = toString
    __repr__ = toString

### static attribute ..
Token.badToken = Token( type=INVALID_TYPE, text="<no text>")

if __name__ == "__main__":
    print "testing .."
    T = Token.badToken
    print T

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonToken                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonToken(Token):

    def __init__(self,**argv):
        Token.__init__(self,**argv)
        self.line = 0
        self.col  = 0
        try:
            self.line = argv['line']
        except:
            pass
        try:
            self.col = argv['col']
        except:
            pass

    def getLine(self):
        return self.line

    def getText(self):
        return self.text

    def getColumn(self):
        return self.col

    def setLine(self,line):
        self.line = line
        return self

    def setText(self,text):
        self.text = text
        return self

    def setColumn(self,col):
        self.col = col
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        d = {
           'text' : self.text,
           'type' : tval,
           'line' : self.line,
           'colm' : self.col
           }

        fmt = '["%(text)s",<%(type)s>,line=%(line)s,col=%(colm)s]'
        return fmt % d

    __str__ = toString
    __repr__ = toString


if __name__ == '__main__' :
    T = CommonToken()
    print T
    T = CommonToken(col=15,line=1,text="some text", type=5)
    print T
    T = CommonToken()
    T.setLine(1).setColumn(15).setText("some text").setType(5)
    print T
    print T.getLine()
    print T.getColumn()
    print T.getText()
    print T.getType()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    CommonHiddenStreamToken                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonHiddenStreamToken(CommonToken):
    def __init__(self,*args):
        CommonToken.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def setHiddenAfter(self,t):
        self.hiddenAfter = t

    def setHiddenBefore(self, t):
        self.hiddenBefore = t

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Queue                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## Shall be a circular buffer on tokens ..
class Queue(object):

    def __init__(self):
        self.buffer = [] # empty list

    def append(self,item):
        self.buffer.append(item)

    def elementAt(self,index):
        return self.buffer[index]

    def reset(self):
        self.buffer = []

    def removeFirst(self):
        self.buffer.pop(0)

    def length(self):
        return len(self.buffer)

    def __str__(self):
        return str(self.buffer)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       InputBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class InputBuffer(object):
    def __init__(self):
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def __str__(self):
        return "(%s,%s,%s,%s)" % (
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

    def commit(self):
        self.nMarkers -= 1

    def consume(self) :
        self.numToConsume += 1

    ## probably better to return a list of items
    ## because of unicode. Or return a unicode
    ## string ..
    def getLAChars(self) :
        i = self.markerOffset
        n = self.queue.length()
        s = ''
        while i<n:
            s += self.queue.elementAt(i)
        return s

    ## probably better to return a list of items
    ## because of unicode chars
    def getMarkedChars(self) :
        s = ''
        i = 0
        n = self.markerOffset
        while i<n:
            s += self.queue.elementAt(i)
        return s

    def isMarked(self) :
        return self.nMarkers != 0

    def fill(self,k):
        ### abstract method
        raise NotImplementedError()

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharBuffer                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharBuffer(InputBuffer):
    def __init__(self,reader):
        ##assert isinstance(reader,file)
        super(CharBuffer,self).__init__()
        ## a reader is supposed to be anything that has
        ## a method 'read(int)'.
        self.input = reader

    def __str__(self):
        base = super(CharBuffer,self).__str__()
        return "CharBuffer{%s,%s" % (base,str(input))

    def fill(self,amount):
        try:
            self.syncConsume()
            while self.queue.length() < (amount + self.markerOffset) :
                ## retrieve just one char - what happend at end
                ## of input?
                c = self.input.read(1)
                ### python's behaviour is to return the empty string  on
                ### EOF, ie. no exception whatsoever is thrown. An empty
                ### python  string  has  the  nice feature that it is of
                ### type 'str' and  "not ''" would return true. Contrary,
                ### one can't  do  this: '' in 'abc'. This should return
                ### false,  but all we  get  is  then  a TypeError as an
                ### empty string is not a character.

                ### Let's assure then that we have either seen a
                ### character or an empty string (EOF).
                assert len(c) == 0 or len(c) == 1

                ### And it shall be of type string (ASCII or UNICODE).
                assert is_string_type(c)

                ### Just append EOF char to buffer. Note that buffer may
                ### contain then just more than one EOF char ..

                ### use unicode chars instead of ASCII ..
                self.queue.append(c)
        except Exception,e:
            raise CharStreamIOException(e)
        ##except: # (mk) Cannot happen ...
            ##error ("unexpected exception caught ..")
            ##assert 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LexerSharedInputState                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LexerSharedInputState(object):
    def __init__(self,ibuf):
        assert isinstance(ibuf,InputBuffer)
        self.input = ibuf
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None

    def reset(self):
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None
        self.input.reset()

    def LA(self,k):
        return self.input.LA(k)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStream                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStream(object):
    def nextToken(self):
        pass

    def __iter__(self):
        return TokenStreamIterator(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamIterator                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamIterator(object):
    def __init__(self,inst):
        if isinstance(inst,TokenStream):
            self.inst = inst
            return
        raise TypeError("TokenStreamIterator requires TokenStream object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamSelector                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamSelector(TokenStream):

    def __init__(self):
        self._input = None
        self._stmap = {}
        self._stack = []

    def addInputStream(self,stream,key):
        self._stmap[key] = stream

    def getCurrentStream(self):
        return self._input

    def getStream(self,sname):
        try:
            stream = self._stmap[sname]
        except:
            raise ValueError("TokenStream " + sname + " not found");
        return stream;

    def nextToken(self):
        while 1:
            try:
                return self._input.nextToken()
            except TokenStreamRetryException,r:
                ### just retry "forever"
                pass

    def pop(self):
        stream = self._stack.pop();
        self.select(stream);
        return stream;

    def push(self,arg):
        self._stack.append(self._input);
        self.select(arg)

    def retry(self):
        raise TokenStreamRetryException()

    def select(self,arg):
        if isinstance(arg,TokenStream):
            self._input = arg
            return
        if is_string_type(arg):
            self._input = self.getStream(arg)
            return
        raise TypeError("TokenStreamSelector.select requires " +
                        "TokenStream or string argument")

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamBasicFilter                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamBasicFilter(TokenStream):

    def __init__(self,input):

        self.input = input;
        self.discardMask = BitSet()

    def discard(self,arg):
        if isinstance(arg,int):
            self.discardMask.add(arg)
            return
        if isinstance(arg,BitSet):
            self.discardMark = arg
            return
        raise TypeError("TokenStreamBasicFilter.discard requires" +
                        "integer or BitSet argument")

    def nextToken(self):
        tok = self.input.nextToken()
        while tok and self.discardMask.member(tok.getType()):
            tok = self.input.nextToken()
        return tok

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamHiddenTokenFilter              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamHiddenTokenFilter(TokenStreamBasicFilter):

    def __init__(self,input):
        TokenStreamBasicFilter.__init__(self,input)
        self.hideMask = BitSet()
        self.nextMonitoredToken = None
        self.lastHiddenToken = None
        self.firstHidden = None

    def consume(self):
        self.nextMonitoredToken = self.input.nextToken()

    def consumeFirst(self):
        self.consume()

        p = None;
        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                if not p:
                    p = self.LA(1)
                else:
                    p.setHiddenAfter(self.LA(1))
                    self.LA(1).setHiddenBefore(p)
                    p = self.LA(1)
                self.lastHiddenToken = p
                if not self.firstHidden:
                    self.firstHidden = p
            self.consume()

    def getDiscardMask(self):
        return self.discardMask

    def getHiddenAfter(self,t):
        return t.getHiddenAfter()

    def getHiddenBefore(self,t):
        return t.getHiddenBefore()

    def getHideMask(self):
        return self.hideMask

    def getInitialHiddenToken(self):
        return self.firstHidden

    def hide(self,m):
        if isinstance(m,int):
            self.hideMask.add(m)
            return
        if isinstance(m.BitMask):
            self.hideMask = m
            return

    def LA(self,i):
        return self.nextMonitoredToken

    def nextToken(self):
        if not self.LA(1):
            self.consumeFirst()

        monitored = self.LA(1)

        monitored.setHiddenBefore(self.lastHiddenToken)
        self.lastHiddenToken = None

        self.consume()
        p = monitored

        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                p.setHiddenAfter(self.LA(1))
                if p != monitored:
                    self.LA(1).setHiddenBefore(p)
                p = self.lastHiddenToken = self.LA(1)
            self.consume()
        return monitored

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       StringBuffer                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class StringBuffer:
    def __init__(self,string=None):
        if string:
            self.text = list(string)
        else:
            self.text = []

    def setLength(self,sz):
        if not sz :
            self.text = []
            return
        assert sz>0
        if sz >= self.length():
            return
        ### just reset to empty buffer
        self.text = self.text[0:sz]

    def length(self):
        return len(self.text)

    def append(self,c):
        self.text.append(c)

    ### return buffer as string. Arg 'a' is  used  as index
    ## into the buffer and 2nd argument shall be the length.
    ## If 2nd args is absent, we return chars till end of
    ## buffer starting with 'a'.
    def getString(self,a=None,length=None):
        if not a :
            a = 0
        assert a>=0
        if a>= len(self.text) :
            return ""

        if not length:
            ## no second argument
            L = self.text[a:]
        else:
            assert (a+length) <= len(self.text)
            b = a + length
            L = self.text[a:b]
        s = ""
        for x in L : s += x
        return s

    toString = getString ## alias

    def __str__(self):
        return str(self.text)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Reader                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## When reading Japanese chars, it happens that a stream returns a
## 'char' of length 2. This looks like  a  bug  in the appropriate
## codecs - but I'm  rather  unsure about this. Anyway, if this is
## the case, I'm going to  split  this string into a list of chars
## and put them  on  hold, ie. on a  buffer. Next time when called
## we read from buffer until buffer is empty.
## wh: nov, 25th -> problem does not appear in Python 2.4.0.c1.

class Reader(object):
    def __init__(self,stream):
        self.cin = stream
        self.buf = []

    def read(self,num):
        assert num==1

        if len(self.buf):
            return self.buf.pop()

        ## Read a char - this may return a string.
        ## Is this a bug in codecs/Python?
        c = self.cin.read(1)

        if not c or len(c)==1:
            return c

        L = list(c)
        L.reverse()
        for x in L:
            self.buf.append(x)

        ## read one char ..
        return self.read(1)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharScanner                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScanner(TokenStream):
    ## class members
    NO_CHAR = 0
    EOF_CHAR = ''  ### EOF shall be the empty string.

    def __init__(self, *argv, **kwargs):
        super(CharScanner, self).__init__()
        self.saveConsumedInput = True
        self.tokenClass = None
        self.caseSensitive = True
        self.caseSensitiveLiterals = True
        self.literals = None
        self.tabsize = 8
        self._returnToken = None
        self.commitToPath = False
        self.traceDepth = 0
        self.text = StringBuffer()
        self.hashString = hash(self)
        self.setTokenObjectClass(CommonToken)
        self.setInput(*argv)

    def __iter__(self):
        return CharScannerIterator(self)

    def setInput(self,*argv):
        ## case 1:
        ## if there's no arg we default to read from
        ## standard input
        if not argv:
            import sys
            self.setInput(sys.stdin)
            return

        ## get 1st argument
        arg1 = argv[0]

        ## case 2:
        ## if arg1 is a string,  we assume it's a file name
        ## and  open  a  stream  using 2nd argument as open
        ## mode. If there's no 2nd argument we fall back to
        ## mode '+rb'.
        if is_string_type(arg1):
            f = open(arg1,"rb")
            self.setInput(f)
            self.setFilename(arg1)
            return

        ## case 3:
        ## if arg1 is a file we wrap it by a char buffer (
        ## some additional checks?? No, can't do this in
        ## general).
        if isinstance(arg1,file):
            self.setInput(CharBuffer(arg1))
            return

        ## case 4:
        ## if arg1 is of type SharedLexerInputState we use
        ## argument as is.
        if isinstance(arg1,LexerSharedInputState):
            self.inputState = arg1
            return

        ## case 5:
        ## check whether argument type is of type input
        ## buffer. If so create a SharedLexerInputState and
        ## go ahead.
        if isinstance(arg1,InputBuffer):
            self.setInput(LexerSharedInputState(arg1))
            return

        ## case 6:
        ## check whether argument type has a method read(int)
        ## If so create CharBuffer ...
        try:
            if arg1.read:
                rd = Reader(arg1)
                cb = CharBuffer(rd)
                ss = LexerSharedInputState(cb)
                self.inputState = ss
            return
        except:
            pass

        ## case 7:
        ## raise wrong argument exception
        raise TypeError(argv)

    def setTabSize(self,size) :
        self.tabsize = size

    def getTabSize(self) :
        return self.tabsize

    def setCaseSensitive(self,t) :
        self.caseSensitive = t

    def setCommitToPath(self,commit) :
        self.commitToPath = commit

    def setFilename(self,f) :
        self.inputState.filename = f

    def setLine(self,line) :
        self.inputState.line = line

    def setText(self,s) :
        self.resetText()
        self.text.append(s)

    def getCaseSensitive(self) :
        return self.caseSensitive

    def getCaseSensitiveLiterals(self) :
        return self.caseSensitiveLiterals

    def getColumn(self) :
        return self.inputState.column

    def setColumn(self,c) :
        self.inputState.column = c

    def getCommitToPath(self) :
        return self.commitToPath

    def getFilename(self) :
        return self.inputState.filename

    def getInputBuffer(self) :
        return self.inputState.input

    def getInputState(self) :
        return self.inputState

    def setInputState(self,state) :
        assert isinstance(state,LexerSharedInputState)
        self.inputState = state

    def getLine(self) :
        return self.inputState.line

    def getText(self) :
        return str(self.text)

    def getTokenObject(self) :
        return self._returnToken

    def LA(self,i) :
        c = self.inputState.input.LA(i)
        if not self.caseSensitive:
            ### E0006
            c = c.__class__.lower(c)
        return c

    def makeToken(self,type) :
        try:
            ## dynamically load a class
            assert self.tokenClass
            tok = self.tokenClass()
            tok.setType(type)
            tok.setColumn(self.inputState.tokenStartColumn)
            tok.setLine(self.inputState.tokenStartLine)
            return tok
        except:
            self.panic("unable to create new token")
        return Token.badToken

    def mark(self) :
        return self.inputState.input.mark()

    def _match_bitset(self,b) :
        if b.member(self.LA(1)):
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), b, False, self)

    def _match_string(self,s) :
        for c in s:
            if self.LA(1) == c:
                self.consume()
            else:
                raise MismatchedCharException(self.LA(1), c, False, self)

    def match(self,item):
        if is_string_type(item):
            return self._match_string(item)
        else:
            return self._match_bitset(item)

    def matchNot(self,c) :
        if self.LA(1) != c:
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), c, True, self)

    def matchRange(self,c1,c2) :
        if self.LA(1) < c1 or self.LA(1) > c2 :
            raise MismatchedCharException(self.LA(1), c1, c2, False, self)
        else:
            self.consume()

    def newline(self) :
        self.inputState.line += 1
        self.inputState.column = 1

    def tab(self) :
        c = self.getColumn()
        nc = ( ((c-1)/self.tabsize) + 1) * self.tabsize + 1
        self.setColumn(nc)

    def panic(self,s='') :
        print "CharScanner: panic: " + s
        sys.exit(1)

    def reportError(self,ex) :
        print ex

    def reportError(self,s) :
        if not self.getFilename():
            print "error: " + str(s)
        else:
            print self.getFilename() + ": error: " + str(s)

    def reportWarning(self,s) :
        if not self.getFilename():
            print "warning: " + str(s)
        else:
            print self.getFilename() + ": warning: " + str(s)

    def resetText(self) :
        self.text.setLength(0)
        self.inputState.tokenStartColumn = self.inputState.column
        self.inputState.tokenStartLine = self.inputState.line

    def rewind(self,pos) :
        self.inputState.input.rewind(pos)

    def setTokenObjectClass(self,cl):
        self.tokenClass = cl

    def testForLiteral(self,token):
        if not token:
            return
        assert isinstance(token,Token)

        _type = token.getType()

        ## special tokens can't be literals
        if _type in [SKIP,INVALID_TYPE,EOF_TYPE,NULL_TREE_LOOKAHEAD] :
            return

        _text = token.getText()
        if not _text:
            return

        assert is_string_type(_text)
        _type = self.testLiteralsTable(_text,_type)
        token.setType(_type)
        return _type

    def testLiteralsTable(self,*args):
        if is_string_type(args[0]):
            s = args[0]
            i = args[1]
        else:
            s = self.text.getString()
            i = args[0]

        ## check whether integer has been given
        if not isinstance(i,int):
            assert isinstance(i,int)

        ## check whether we have a dict
        assert isinstance(self.literals,dict)
        try:
            ## E0010
            if not self.caseSensitiveLiterals:
                s = s.__class__.lower(s)
            i = self.literals[s]
        except:
            pass
        return i

    def toLower(self,c):
        return c.__class__.lower()

    def traceIndent(self):
        print ' ' * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.traceIndent()
        print "> lexer %s c== %s" % (rname,self.LA(1))

    def traceOut(self,rname):
        self.traceIndent()
        print "< lexer %s c== %s" % (rname,self.LA(1))
        self.traceDepth -= 1

    def uponEOF(self):
        pass

    def append(self,c):
        if self.saveConsumedInput :
            self.text.append(c)

    def commit(self):
        self.inputState.input.commit()

    def consume(self):
        if not self.inputState.guessing:
            c = self.LA(1)
            if self.caseSensitive:
                self.append(c)
            else:
                # use input.LA(), not LA(), to get original case
                # CharScanner.LA() would toLower it.
                c =  self.inputState.input.LA(1)
                self.append(c)

            if c and c in "\t":
                self.tab()
            else:
                self.inputState.column += 1
        self.inputState.input.consume()

    ## Consume chars until one matches the given char
    def consumeUntil_char(self,c):
        while self.LA(1) != EOF_CHAR and self.LA(1) != c:
            self.consume()

    ## Consume chars until one matches the given set
    def consumeUntil_bitset(self,bitset):
        while self.LA(1) != EOF_CHAR and not self.set.member(self.LA(1)):
            self.consume()

    ### If symbol seen is EOF then generate and set token, otherwise
    ### throw exception.
    def default(self,la1):
        if not la1 :
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
        else:
            self.raise_NoViableAlt(la1)

    def filterdefault(self,la1,*args):
        if not la1:
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
            return

        if not args:
            self.consume()
            raise TryAgain()
        else:
            ### apply filter object
            self.commit();
            try:
                func=args[0]
                args=args[1:]
                apply(func,args)
            except RecognitionException, e:
                ## catastrophic failure
                self.reportError(e);
                self.consume();
            raise TryAgain()

    def raise_NoViableAlt(self,la1=None):
        if not la1: la1 = self.LA(1)
        fname = self.getFilename()
        line  = self.getLine()
        col   = self.getColumn()
        raise NoViableAltForCharException(la1,fname,line,col)

    def set_return_token(self,_create,_token,_ttype,_offset):
        if _create and not _token and (not _ttype == SKIP):
            string = self.text.getString(_offset)
            _token = self.makeToken(_ttype)
            _token.setText(string)
        self._returnToken = _token
        return _token

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                   CharScannerIterator                          ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScannerIterator:

    def __init__(self,inst):
        if isinstance(inst,CharScanner):
            self.inst = inst
            return
        raise TypeError("CharScannerIterator requires CharScanner object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BitSet                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### I'm assuming here that a long is 64bits. It appears however, that
### a long is of any size. That means we can use a single long as the
### bitset (!), ie. Python would do almost all the work (TBD).

class BitSet(object):
    BITS     = 64
    NIBBLE   = 4
    LOG_BITS = 6
    MOD_MASK = BITS -1

    def __init__(self,data=None):
        if not data:
            BitSet.__init__(self,[long(0)])
            return
        if isinstance(data,int):
            BitSet.__init__(self,[long(data)])
            return
        if isinstance(data,long):
            BitSet.__init__(self,[data])
            return
        if not isinstance(data,list):
            raise TypeError("BitSet requires integer, long, or " +
                            "list argument")
        for x in data:
            if not isinstance(x,long):
                raise TypeError(self,"List argument item is " +
                                "not a long: %s" % (x))
        self.data = data

    def __str__(self):
        bits = len(self.data) * BitSet.BITS
        s = ""
        for i in xrange(0,bits):
            if self.at(i):
                s += "1"
            else:
                s += "o"
            if not ((i+1) % 10):
                s += '|%s|' % (i+1)
        return s

    def __repr__(self):
        return str(self)

    def member(self,item):
        if not item:
            return False

        if isinstance(item,int):
            return self.at(item)

        if not is_string_type(item):
            raise TypeError(self,"char or unichar expected: %s" % (item))

        ## char is a (unicode) string with at most lenght 1, ie.
        ## a char.

        if len(item) != 1:
            raise TypeError(self,"char expected: %s" % (item))

        ### handle ASCII/UNICODE char
        num = ord(item)

        ### check whether position num is in bitset
        return self.at(num)

    def wordNumber(self,bit):
        return bit >> BitSet.LOG_BITS

    def bitMask(self,bit):
        pos = bit & BitSet.MOD_MASK  ## bit mod BITS
        return (1L << pos)

    def set(self,bit,on=True):
        # grow bitset as required (use with care!)
        i = self.wordNumber(bit)
        mask = self.bitMask(bit)
        if i>=len(self.data):
            d = i - len(self.data) + 1
            for x in xrange(0,d):
                self.data.append(0L)
            assert len(self.data) == i+1
        if on:
            self.data[i] |=  mask
        else:
            self.data[i] &= (~mask)

    ### make add an alias for set
    add = set

    def off(self,bit,off=True):
        self.set(bit,not off)

    def at(self,bit):
        i = self.wordNumber(bit)
        v = self.data[i]
        m = self.bitMask(bit)
        return v & m


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      some further funcs                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def illegalarg_ex(func):
    raise ValueError(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

def runtime_ex(func):
    raise RuntimeException(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TokenBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenBuffer(object):
    def __init__(self,stream):
        self.input = stream
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def consume(self) :
        self.numToConsume += 1

    def fill(self, amount):
        self.syncConsume()
        while self.queue.length() < (amount + self.markerOffset):
            self.queue.append(self.input.nextToken())

    def getInput(self):
        return self.input

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1).type

    def LT(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

    def __str__(self):
        return "(%s,%s,%s,%s,%s)" % (
           self.input,
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ParserSharedInputState                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ParserSharedInputState(object):

    def __init__(self):
        self.input = None
        self.reset()

    def reset(self):
        self.guessing = 0
        self.filename = None
        if self.input:
            self.input.reset()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Parser                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Parser(object):

    def __init__(self, *args, **kwargs):
        self.tokenNames = None
        self.returnAST  = None
        self.astFactory = None
        self.tokenTypeToASTClassMap = {}
        self.ignoreInvalidDebugCalls = False
        self.traceDepth = 0
        if not args:
            self.inputState = ParserSharedInputState()
            return
        arg0 = args[0]
        assert isinstance(arg0,ParserSharedInputState)
        self.inputState = arg0
        return

    def getTokenTypeToASTClassMap(self):
        return self.tokenTypeToASTClassMap


    def addMessageListener(self, l):
        if not self.ignoreInvalidDebugCalls:
            illegalarg_ex(addMessageListener)

    def addParserListener(self,l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserListener)

    def addParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserMatchListener)

    def addParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addParserTokenListener)

    def addSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSemanticPredicateListener)

    def addSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSyntacticPredicateListener)

    def addTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addTraceListener)

    def consume(self):
        raise NotImplementedError()

    def _consumeUntil_type(self,tokenType):
        while self.LA(1) != EOF_TYPE and self.LA(1) != tokenType:
            self.consume()

    def _consumeUntil_bitset(self, set):
        while self.LA(1) != EOF_TYPE and not set.member(self.LA(1)):
            self.consume()

    def consumeUntil(self,arg):
        if isinstance(arg,int):
            self._consumeUntil_type(arg)
        else:
            self._consumeUntil_bitset(arg)

    def defaultDebuggingSetup(self):
        pass

    def getAST(self) :
        return self.returnAST

    def getASTFactory(self) :
        return self.astFactory

    def getFilename(self) :
        return self.inputState.filename

    def getInputState(self) :
        return self.inputState

    def setInputState(self, state) :
        self.inputState = state

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self) :
        return self.tokenNames

    def isDebugMode(self) :
        return self.false

    def LA(self, i):
        raise NotImplementedError()

    def LT(self, i):
        raise NotImplementedError()

    def mark(self):
        return self.inputState.input.mark()

    def _match_int(self,t):
        if (self.LA(1) != t):
            raise MismatchedTokenException(
               self.tokenNames, self.LT(1), t, False, self.getFilename())
        else:
            self.consume()

    def _match_set(self, b):
        if (not b.member(self.LA(1))):
            raise MismatchedTokenException(
               self.tokenNames,self.LT(1), b, False, self.getFilename())
        else:
            self.consume()

    def match(self,set) :
        if isinstance(set,int):
            self._match_int(set)
            return
        if isinstance(set,BitSet):
            self._match_set(set)
            return
        raise TypeError("Parser.match requires integer ot BitSet argument")

    def matchNot(self,t):
        if self.LA(1) == t:
            raise MismatchedTokenException(
               tokenNames, self.LT(1), t, True, self.getFilename())
        else:
            self.consume()

    def removeMessageListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeMessageListener)

    def removeParserListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserListener)

    def removeParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserMatchListener)

    def removeParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserTokenListener)

    def removeSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSemanticPredicateListener)

    def removeSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSyntacticPredicateListener)

    def removeTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeTraceListener)

    def reportError(self,x) :
        fmt = "syntax error:"
        f = self.getFilename()
        if f:
            fmt = ("%s:" % f) + fmt
        if isinstance(x,Token):
            line = x.getColumn()
            col  = x.getLine()
            text = x.getText()
            fmt  = fmt + 'unexpected symbol at line %s (column %s) : "%s"'
            print >>sys.stderr, fmt % (line,col,text)
        else:
            print >>sys.stderr, fmt,str(x)

    def reportWarning(self,s):
        f = self.getFilename()
        if f:
            print "%s:warning: %s" % (f,str(x))
        else:
            print "warning: %s" % (str(x))

    def rewind(self, pos) :
        self.inputState.input.rewind(pos)

    def setASTFactory(self, f) :
        self.astFactory = f

    def setASTNodeClass(self, cl) :
        self.astFactory.setASTNodeType(cl)

    def setASTNodeType(self, nodeType) :
        self.setASTNodeClass(nodeType)

    def setDebugMode(self, debugMode) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(setDebugMode)

    def setFilename(self, f) :
        self.inputState.filename = f

    def setIgnoreInvalidDebugCalls(self, value) :
        self.ignoreInvalidDebugCalls = value

    def setTokenBuffer(self, t) :
        self.inputState.input = t

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.trace("> ", rname)

    def traceOut(self,rname):
        self.trace("< ", rname)
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to Parser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to Parser
    def makeASTRoot(self,currentAST,root) :
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LLkParser                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LLkParser(Parser):

    def __init__(self, *args, **kwargs):
        try:
            arg1 = args[0]
        except:
            arg1 = 1

        if isinstance(arg1,int):
            super(LLkParser,self).__init__()
            self.k = arg1
            return

        if isinstance(arg1,ParserSharedInputState):
            super(LLkParser,self).__init__(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenBuffer):
            super(LLkParser,self).__init__()
            self.setTokenBuffer(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenStream):
            super(LLkParser,self).__init__()
            tokenBuf = TokenBuffer(arg1)
            self.setTokenBuffer(tokenBuf)
            self.set_k(1,*args)
            return

        ### unknown argument
        raise TypeError("LLkParser requires integer, " +
                        "ParserSharedInputStream or TokenStream argument")

    def consume(self):
        self.inputState.input.consume()

    def LA(self,i):
        return self.inputState.input.LA(i)

    def LT(self,i):
        return self.inputState.input.LT(i)

    def set_k(self,index,*args):
        try:
            self.k = args[index]
        except:
            self.k = 1

    def trace(self,ee,rname):
        print type(self)
        self.traceIndent()
        guess = ""
        if self.inputState.guessing > 0:
            guess = " [guessing]"
        print(ee + rname + guess)
        for i in xrange(1,self.k+1):
            if i != 1:
                print(", ")
            if self.LT(i) :
                v = self.LT(i).getText()
            else:
                v = "null"
            print "LA(%s) == %s" % (i,v)
        print("\n")

    def traceIn(self,rname):
        self.traceDepth += 1;
        self.trace("> ", rname);

    def traceOut(self,rname):
        self.trace("< ", rname);
        self.traceDepth -= 1;

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TreeParserSharedInputState                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParserSharedInputState(object):
    def __init__(self):
        self.guessing = 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TreeParser                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParser(object):

    def __init__(self, *args, **kwargs):
        self.inputState = TreeParserSharedInputState()
        self._retTree   = None
        self.tokenNames = []
        self.returnAST  = None
        self.astFactory = ASTFactory()
        self.traceDepth = 0

    def getAST(self):
        return self.returnAST

    def getASTFactory(self):
        return self.astFactory

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self):
        return self.tokenNames

    def match(self,t,set) :
        assert isinstance(set,int) or isinstance(set,BitSet)
        if not t or t == ASTNULL:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,int) and t.getType() != set:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,BitSet) and not set.member(t.getType):
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

    def matchNot(self,t, ttype) :
        if not t or (t == ASTNULL) or (t.getType() == ttype):
            raise MismatchedTokenException(getTokenNames(), t, ttype, True)

    def reportError(self,ex):
        print >>sys.stderr,"error:",ex

    def  reportWarning(self, s):
        print "warning:",s

    def setASTFactory(self,f):
        self.astFactory = f

    def setASTNodeType(self,nodeType):
        self.setASTNodeClass(nodeType)

    def setASTNodeClass(self,nodeType):
        self.astFactory.setASTNodeType(nodeType)

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname,t):
        self.traceDepth += 1
        self.traceIndent()
        print("> " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))

    def traceOut(self,rname,t):
        self.traceIndent()
        print("< " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to TreeParser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to TreeParser
    def makeASTRoot(self,currentAST,root):
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               funcs to work on trees                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def rightmost(ast):
    if ast:
        while(ast.right):
            ast = ast.right
    return ast

def cmptree(s,t,partial):
    while(s and t):
        ### as a quick optimization, check roots first.
        if not s.equals(t):
            return False

        ### if roots match, do full list match test on children.
        if not cmptree(s.getFirstChild(),t.getFirstChild(),partial):
            return False

        s = s.getNextSibling()
        t = t.getNextSibling()

    r = ifelse(partial,not t,not s and not t)
    return r

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                          AST                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class AST(object):
    def __init__(self):
        pass

    def addChild(self, c):
        pass

    def equals(self, t):
        return False

    def equalsList(self, t):
        return False

    def equalsListPartial(self, t):
        return False

    def equalsTree(self, t):
        return False

    def equalsTreePartial(self, t):
        return False

    def findAll(self, tree):
        return None

    def findAllPartial(self, subtree):
        return None

    def getFirstChild(self):
        return self

    def getNextSibling(self):
        return self

    def getText(self):
        return ""

    def getType(self):
        return INVALID_TYPE

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    def getNumberOfChildren(self):
        return 0

    def initialize(self, t, txt):
        pass

    def initialize(self, t):
        pass

    def setFirstChild(self, c):
        pass

    def setNextSibling(self, n):
        pass

    def setText(self, text):
        pass

    def setType(self, ttype):
        pass

    def toString(self):
        self.getText()

    __str__ = toString

    def toStringList(self):
        return self.getText()

    def toStringTree(self):
        return self.getText()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTNULLType                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### There is only one instance of this class **/
class ASTNULLType(AST):
    def __init__(self):
        AST.__init__(self)
        pass

    def getText(self):
        return "<ASTNULL>"

    def getType(self):
        return NULL_TREE_LOOKAHEAD


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BaseAST                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class BaseAST(AST):

    verboseStringConversion = False
    tokenNames = None

    def __init__(self):
        self.down  = None ## kid
        self.right = None ## sibling

    def addChild(self,node):
        if node:
            t = rightmost(self.down)
            if t:
                t.right = node
            else:
                assert not self.down
                self.down = node

    def getNumberOfChildren(self):
        t = self.down
        n = 0
        while t:
            n += 1
            t = t.right
        return n

    def doWorkForFindAll(self,v,target,partialMatch):
        sibling = self

        while sibling:
            c1 = partialMatch and sibling.equalsTreePartial(target)
            if c1:
                v.append(sibling)
            else:
                c2 = not partialMatch and sibling.equalsTree(target)
                if c2:
                    v.append(sibling)

            ### regardless of match or not, check any children for matches
            if sibling.getFirstChild():
                sibling.getFirstChild().doWorkForFindAll(v,target,partialMatch)

            sibling = sibling.getNextSibling()

    ### Is node t equal to 'self' in terms of token type and text?
    def equals(self,t):
        if not t:
            return False
        return self.getText() == t.getText() and self.getType() == t.getType()

    ### Is t an exact structural and equals() match of this tree.  The
    ### 'self' reference is considered the start of a sibling list.
    ###
    def equalsList(self, t):
        return cmptree(self, t, partial=False)

    ### Is 't' a subtree of this list?
    ### The siblings of the root are NOT ignored.
    ###
    def equalsListPartial(self,t):
        return cmptree(self,t,partial=True)

    ### Is tree rooted at 'self' equal to 't'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTree(self, t):
        return self.equals(t) and \
               cmptree(self.getFirstChild(), t.getFirstChild(), partial=False)

    ### Is 't' a subtree of the tree rooted at 'self'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTreePartial(self, t):
        if not t:
            return True
        return self.equals(t) and cmptree(
           self.getFirstChild(), t.getFirstChild(), partial=True)

    ### Walk the tree looking for all exact subtree matches.  Return
    ### an ASTEnumerator that lets the caller walk the list
    ### of subtree roots found herein.
    def findAll(self,target):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not target:
            return None
        # find all matches recursively
        self.doWorkForFindAll(roots, target, False)
        return roots

    ### Walk the tree looking for all subtrees.  Return
    ###  an ASTEnumerator that lets the caller walk the list
    ###  of subtree roots found herein.
    def findAllPartial(self,sub):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not sub:
            return None

        self.doWorkForFindAll(roots, sub, True)  ### find all matches recursively
        return roots

    ### Get the first child of this node None if not children
    def getFirstChild(self):
        return self.down

    ### Get the next sibling in line after this one
    def getNextSibling(self):
        return self.right

    ### Get the token text for this node
    def getText(self):
        return ""

    ### Get the token type for this node
    def getType(self):
        return 0

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    ### Remove all children */
    def removeChildren(self):
        self.down = None

    def setFirstChild(self,c):
        self.down = c

    def setNextSibling(self, n):
        self.right = n

    ### Set the token text for this node
    def setText(self, text):
        pass

    ### Set the token type for this node
    def setType(self, ttype):
        pass

    ### static
    def setVerboseStringConversion(verbose,names):
        verboseStringConversion = verbose
        tokenNames = names
    setVerboseStringConversion = staticmethod(setVerboseStringConversion)

    ### Return an array of strings that maps token ID to it's text.
    ##  @since 2.7.3
    def getTokenNames():
        return tokenNames

    def toString(self):
        return self.getText()

    ### return tree as lisp string - sibling included
    def toStringList(self):
        ts = self.toStringTree()
        sib = self.getNextSibling()
        if sib:
            ts += sib.toStringList()
        return ts

    __str__ = toStringList

    ### return tree as string - siblings ignored
    def toStringTree(self):
        ts = ""
        kid = self.getFirstChild()
        if kid:
            ts += " ("
        ts += " " + self.toString()
        if kid:
            ts += kid.toStringList()
            ts += " )"
        return ts

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonAST                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### Common AST node implementation
class CommonAST(BaseAST):
    def __init__(self,token=None):
        super(CommonAST,self).__init__()
        self.ttype = INVALID_TYPE
        self.text  = "<no text>"
        self.line  = 0
        self.column= 0
        self.initialize(token)
        #assert self.text

    ### Get the token text for this node
    def getText(self):
        return self.text

    ### Get the token type for this node
    def getType(self):
        return self.ttype

    ### Get the line for this node
    def getLine(self):
        return self.line

    ### Get the column for this node
    def getColumn(self):
        return self.column

    def initialize(self,*args):
        if not args:
            return

        arg0 = args[0]

        if isinstance(arg0,int):
            arg1 = args[1]
            self.setType(arg0)
            self.setText(arg1)
            return

        if isinstance(arg0,AST) or isinstance(arg0,Token):
            self.setText(arg0.getText())
            self.setType(arg0.getType())
            self.line = arg0.getLine()
            self.column = arg0.getColumn()
            return

    ### Set the token text for this node
    def setText(self,text_):
        assert is_string_type(text_)
        self.text = text_

    ### Set the token type for this node
    def setType(self,ttype_):
        assert isinstance(ttype_,int)
        self.ttype = ttype_

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     CommonASTWithHiddenTokens                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonASTWithHiddenTokens(CommonAST):

    def __init__(self,*args):
        CommonAST.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def initialize(self,*args):
        CommonAST.initialize(self,*args)
        if args and isinstance(args[0],Token):
            assert isinstance(args[0],CommonHiddenStreamToken)
            self.hiddenBefore = args[0].getHiddenBefore()
            self.hiddenAfter  = args[0].getHiddenAfter()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTPair                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTPair(object):
    def __init__(self):
        self.root = None          ### current root of tree
        self.child = None         ### current child to which siblings are added

    ### Make sure that child is the last sibling */
    def advanceChildToEnd(self):
        if self.child:
            while self.child.getNextSibling():
                self.child = self.child.getNextSibling()

    ### Copy an ASTPair.  Don't call it clone() because we want type-safety */
    def copy(self):
        tmp = ASTPair()
        tmp.root = self.root
        tmp.child = self.child
        return tmp

    def toString(self):
        r = ifelse(not root,"null",self.root.getText())
        c = ifelse(not child,"null",self.child.getText())
        return "[%s,%s]" % (r,c)

    __str__ = toString
    __repr__ = toString


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTFactory                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTFactory(object):
    def __init__(self,table=None):
        self._class = None
        self._classmap = ifelse(table,table,None)

    def create(self,*args):
        if not args:
            return self.create(INVALID_TYPE)

        arg0 = args[0]
        arg1 = None
        arg2 = None

        try:
            arg1 = args[1]
            arg2 = args[2]
        except:
            pass

        # ctor(int)
        if isinstance(arg0,int) and not arg2:
            ### get class for 'self' type
            c = self.getASTNodeType(arg0)
            t = self.create(c)
            if t:
                t.initialize(arg0, ifelse(arg1,arg1,""))
            return t

        # ctor(int,something)
        if isinstance(arg0,int) and arg2:
            t = self.create(arg2)
            if t:
                t.initialize(arg0,arg1)
            return t

        # ctor(AST)
        if isinstance(arg0,AST):
            t = self.create(arg0.getType())
            if t:
                t.initialize(arg0)
            return t

        # ctor(token)
        if isinstance(arg0,Token) and not arg1:
            ttype = arg0.getType()
            assert isinstance(ttype,int)
            t = self.create(ttype)
            if t:
                t.initialize(arg0)
            return t

        # ctor(token,class)
        if isinstance(arg0,Token) and arg1:
            assert isinstance(arg1,type)
            assert issubclass(arg1,AST)
            # this creates instance of 'arg1' using 'arg0' as
            # argument. Wow, that's magic!
            t = arg1(arg0)
            assert t and isinstance(t,AST)
            return t

        # ctor(class)
        if isinstance(arg0,type):
            ### next statement creates instance of type (!)
            t = arg0()
            assert isinstance(t,AST)
            return t


    def setASTNodeClass(self,className=None):
        if not className:
            return
        assert isinstance(className,type)
        assert issubclass(className,AST)
        self._class = className

    ### kind of misnomer - use setASTNodeClass instead.
    setASTNodeType = setASTNodeClass

    def getASTNodeClass(self):
        return self._class



    def getTokenTypeToASTClassMap(self):
        return self._classmap

    def setTokenTypeToASTClassMap(self,amap):
        self._classmap = amap

    def error(self, e):
        import sys
        print >> sys.stderr, e

    def setTokenTypeASTNodeType(self, tokenType, className):
        """
        Specify a mapping between a token type and a (AST) class.
        """
        if not self._classmap:
            self._classmap = {}

        if not className:
            try:
                del self._classmap[tokenType]
            except:
                pass
        else:
            ### here we should also perform actions to ensure that
            ### a. class can be loaded
            ### b. class is a subclass of AST
            ###
            assert isinstance(className,type)
            assert issubclass(className,AST)  ## a & b
            ### enter the class
            self._classmap[tokenType] = className

    def getASTNodeType(self,tokenType):
        """
        For a given token type return the AST node type. First we
        lookup a mapping table, second we try _class
        and finally we resolve to "antlr.CommonAST".
        """

        # first
        if self._classmap:
            try:
                c = self._classmap[tokenType]
                if c:
                    return c
            except:
                pass
        # second
        if self._class:
            return self._class

        # default
        return CommonAST

    ### methods that have been moved to file scope - just listed
    ### here to be somewhat consistent with original API
    def dup(self,t):
        return antlr.dup(t,self)

    def dupList(self,t):
        return antlr.dupList(t,self)

    def dupTree(self,t):
        return antlr.dupTree(t,self)

    ### methods moved to other classes
    ### 1. makeASTRoot  -> Parser
    ### 2. addASTChild  -> Parser

    ### non-standard: create alias for longish method name
    maptype = setTokenTypeASTNodeType

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTVisitor                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTVisitor(object):
    def __init__(self,*args):
        pass

    def visit(self,ast):
        pass

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               static methods and variables                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

ASTNULL = ASTNULLType()

### wh: moved from ASTFactory as there's nothing ASTFactory-specific
### in this method.
def make(*nodes):
    if not nodes:
        return None

    for i in xrange(0,len(nodes)):
        node = nodes[i]
        if node:
            assert isinstance(node,AST)

    root = nodes[0]
    tail = None
    if root:
        root.setFirstChild(None)

    for i in xrange(1,len(nodes)):
        if not nodes[i]:
            continue
        if not root:
            root = tail = nodes[i]
        elif not tail:
            root.setFirstChild(nodes[i])
            tail = root.getFirstChild()
        else:
            tail.setNextSibling(nodes[i])
            tail = tail.getNextSibling()

        ### Chase tail to last sibling
        while tail.getNextSibling():
            tail = tail.getNextSibling()
    return root

def dup(t,factory):
    if not t:
        return None

    if factory:
        dup_t = factory.create(t.__class__)
    else:
        raise TypeError("dup function requires ASTFactory argument")
    dup_t.initialize(t)
    return dup_t

def dupList(t,factory):
    result = dupTree(t,factory)
    nt = result
    while t:
        ## for each sibling of the root
        t = t.getNextSibling()
        nt.setNextSibling(dupTree(t,factory))
        nt = nt.getNextSibling()
    return result

def dupTree(t,factory):
    result = dup(t,factory)
    if t:
        result.setFirstChild(dupList(t.getFirstChild(),factory))
    return result

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
### $Id: antlr.py 3750 2009-02-13 00:13:04Z sjmachin $

# Local Variables:    ***
# mode: python        ***
# py-indent-offset: 4 ***
# End:                ***

########NEW FILE########
__FILENAME__ = BIFFRecords
# -*- coding: cp1252 -*-
from struct import pack
from UnicodeUtils import upack1, upack2
import sys

class SharedStringTable(object):
    _SST_ID = 0x00FC
    _CONTINUE_ID = 0x003C

    def __init__(self, encoding):
        self.encoding = encoding
        self._str_indexes = {}
        self._tally = []
        self._add_calls = 0
        # Following 3 attrs are used for temporary storage in the
        # get_biff_record() method and methods called by it. The pseudo-
        # initialisation here is for documentation purposes only.
        self._sst_record = None
        self._continues = None
        self._current_piece = None

    def add_str(self, s):
        if self.encoding != 'ascii' and not isinstance(s, unicode):
            s = unicode(s, self.encoding)
        self._add_calls += 1
        if s not in self._str_indexes:
            idx = len(self._str_indexes)
            self._str_indexes[s] = idx
            self._tally.append(1)
        else:
            idx = self._str_indexes[s]
            self._tally[idx] += 1
        return idx

    def del_str(self, idx):
        # This is called when we are replacing the contents of a string cell.
        assert self._tally[idx] > 0
        self._tally[idx] -= 1
        self._add_calls -= 1

    def str_index(self, s):
        return self._str_indexes[s]

    def get_biff_record(self):
        self._sst_record = ''
        self._continues = [None, None]
        self._current_piece = pack('<II', 0, 0)
        data = [(idx, s) for s, idx in self._str_indexes.iteritems()]
        data.sort() # in index order
        for idx, s in data:
            if self._tally[idx] == 0:
                s = u''
            self._add_to_sst(s)
        del data
        self._new_piece()
        self._continues[0] = pack('<2HII', self._SST_ID, len(self._sst_record), self._add_calls, len(self._str_indexes))
        self._continues[1] = self._sst_record[8:]
        self._sst_record = None
        self._current_piece = None
        result = ''.join(self._continues)
        self._continues = None
        return result


    def _add_to_sst(self, s):
        u_str = upack2(s, self.encoding)

        is_unicode_str = u_str[2] == '\x01'
        if is_unicode_str:
            atom_len = 5 # 2 byte -- len,
                         # 1 byte -- options,
                         # 2 byte -- 1st sym
        else:
            atom_len = 4 # 2 byte -- len,
                         # 1 byte -- options,
                         # 1 byte -- 1st sym

        self._save_atom(u_str[0:atom_len])
        self._save_splitted(u_str[atom_len:], is_unicode_str)

    def _new_piece(self):
        if self._sst_record == '':
            self._sst_record = self._current_piece
        else:
            curr_piece_len = len(self._current_piece)
            self._continues.append(pack('<2H%ds'%curr_piece_len, self._CONTINUE_ID, curr_piece_len, self._current_piece))
        self._current_piece = ''

    def _save_atom(self, s):
        atom_len = len(s)
        free_space = 0x2020 - len(self._current_piece)
        if free_space < atom_len:
            self._new_piece()
        self._current_piece += s

    def _save_splitted(self, s, is_unicode_str):
        i = 0
        str_len = len(s)
        while i < str_len:
            piece_len = len(self._current_piece)
            free_space = 0x2020 - piece_len
            tail_len = str_len - i
            need_more_space = free_space < tail_len

            if not need_more_space:
                atom_len = tail_len
            else:
                if is_unicode_str:
                    atom_len = free_space & 0xFFFE
                else:
                    atom_len = free_space

            self._current_piece += s[i:i+atom_len]

            if need_more_space:
                self._new_piece()
                if is_unicode_str:
                    self._current_piece += '\x01'
                else:
                    self._current_piece += '\x00'

            i += atom_len


class BiffRecord(object):

    _rec_data = '' # class attribute; child classes need to set this.

    # Sheer waste.
    # def __init__(self):
    #     self._rec_data = ''

    def get_rec_id(self):
        return _REC_ID

    def get_rec_header(self):
        return pack('<2H', self._REC_ID, len(self._rec_data))

    # Not over-ridden by any child classes, never called (except by "get"; see below).
    # def get_rec_data(self):
    #     return self._rec_data

    def get(self):
        # data = self.get_rec_data()
        data = self._rec_data

        if len(data) > 0x2020: # limit for BIFF7/8
            chunks = []
            pos = 0
            while pos < len(data):
                chunk_pos = pos + 0x2020
                chunk = data[pos:chunk_pos]
                chunks.append(chunk)
                pos = chunk_pos
            continues = pack('<2H', self._REC_ID, len(chunks[0])) + chunks[0]
            for chunk in chunks[1:]:
                continues += pack('<2H%ds'%len(chunk), 0x003C, len(chunk), chunk)
                # 0x003C -- CONTINUE record id
            return continues
        else:
            return self.get_rec_header() + data


class Biff8BOFRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Version, contains 0600H for BIFF8 and BIFF8X
    2      2    Type of the following data:
                  0005H = Workbook globals
                  0006H = Visual Basic module
                  0010H = Worksheet
                  0020H = Chart
                  0040H = Macro sheet
                  0100H = Workspace file
    4      2    Build identifier
    6      2    Build year
    8      4    File history flags
    12     4    Lowest Excel version that can read all records in this file
    """
    _REC_ID      = 0x0809
    # stream types
    BOOK_GLOBAL = 0x0005
    VB_MODULE   = 0x0006
    WORKSHEET   = 0x0010
    CHART       = 0x0020
    MACROSHEET  = 0x0040
    WORKSPACE   = 0x0100

    def __init__(self, rec_type):
        version  = 0x0600
        build    = 0x0DBB
        year     = 0x07CC
        file_hist_flags = 0x00L
        ver_can_read    = 0x06L

        self._rec_data = pack('<4H2I', version, rec_type, build, year, file_hist_flags, ver_can_read)


class InteraceHdrRecord(BiffRecord):
    _REC_ID = 0x00E1

    def __init__(self):
        self._rec_data = pack('BB', 0xB0, 0x04)


class InteraceEndRecord(BiffRecord):
    _REC_ID = 0x00E2

    def __init__(self):
        self._rec_data = ''


class MMSRecord(BiffRecord):
    _REC_ID = 0x00C1

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class WriteAccessRecord(BiffRecord):
    """
    This record is part of the file protection. It contains the name of the
    user  that  has  saved  the  file. The user name is always stored as an
    equal-sized  string.  All  unused  characters after the name are filled
    with space characters. It is not required to write the mentioned string
    length. Every other length will be accepted too.
    """
    _REC_ID = 0x005C

    def __init__(self, owner):
        uowner = owner[0:0x30]
        uowner_len = len(uowner)
        self._rec_data = pack('%ds%ds' % (uowner_len, 0x70 - uowner_len), uowner, ' '*(0x70 - uowner_len))


class DSFRecord(BiffRecord):
    """
    This  record  specifies  if the file contains an additional BIFF5/BIFF7
    workbook stream.
    Record DSF, BIFF8:
    Offset Size Contents
    0        2     0 = Only the BIFF8 Workbook stream is present
                   1 = Additional BIFF5/BIFF7 Book stream is in the file
    A  double  stream file can be read by Excel 5.0 and Excel 95, and still
    contains  all  new  features  added to BIFF8 (which are left out in the
    BIFF5/BIFF7 Book stream).
    """
    _REC_ID = 0x0161

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class TabIDRecord(BiffRecord):
    _REC_ID = 0x013D

    def __init__(self, sheetcount):
        for i in range(sheetcount):
            self._rec_data += pack('<H', i+1)


class FnGroupCountRecord(BiffRecord):
    _REC_ID = 0x009C

    def __init__(self):
        self._rec_data = pack('BB', 0x0E, 0x00)


class WindowProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It determines
    whether  the window configuration of this document is protected. Window
    protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0019

    def __init__(self, wndprotect):
        self._rec_data = pack('<H', wndprotect)


class ObjectProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection.
    It determines whether the objects of the current sheet are protected.
    Object protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0063


    def __init__(self, objprotect):
        self._rec_data = pack('<H', objprotect)


class ScenProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    determines whether the scenarios of the current sheet are protected.
    Scenario protection is not active, if this record is omitted.
    """
    _REC_ID = 0x00DD


    def __init__(self, scenprotect):
        self._rec_data = pack('<H', scenprotect)


class ProtectRecord(BiffRecord):
    """
    This  record is part of the worksheet/workbook protection. It specifies
    whether  a  worksheet  or a workbook is protected against modification.
    Protection is not active, if this record is omitted.
    """

    _REC_ID = 0x0012

    def __init__(self, protect):
        self._rec_data = pack('<H', protect)


class PasswordRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    stores a 16-bit hash value, calculated from the worksheet or workbook
    protection password.
    """
    _REC_ID = 0x0013
    def passwd_hash(self, plaintext):
        """
        Based on the algorithm provided by Daniel Rentz of OpenOffice.
        """
        if plaintext == "":
            return 0

        passwd_hash = 0x0000
        for i, char in enumerate(plaintext):
            c = ord(char) << (i + 1)
            low_15 = c & 0x7fff
            high_15 = c & 0x7fff << 15
            high_15 = high_15 >> 15
            c = low_15 | high_15
            passwd_hash ^= c
        passwd_hash ^= len(plaintext)
        passwd_hash ^= 0xCE4B
        return passwd_hash

    def __init__(self, passwd = ""):
        self._rec_data = pack('<H', self.passwd_hash(passwd))


class Prot4RevRecord(BiffRecord):
    _REC_ID = 0x01AF

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class Prot4RevPassRecord(BiffRecord):
    _REC_ID = 0x01BC

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class BackupRecord(BiffRecord):
    """
    This  record  contains  a Boolean value determining whether Excel makes
    a backup of the file while saving.
    """
    _REC_ID = 0x0040

    def __init__(self, backup):
        self._rec_data = pack('<H', backup)

class HideObjRecord(BiffRecord):
    """
    This record specifies whether and how to show objects in the workbook.

    Record HIDEOBJ, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Viewing mode for objects:
                        0 = Show all objects
                        1 = Show placeholders
                        2 = Do not show objects
    """
    _REC_ID = 0x008D

    def __init__(self):
        self._rec_data = pack('<H', 0x00)



class RefreshAllRecord(BiffRecord):
    """
    """

    _REC_ID = 0x01B7

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class BookBoolRecord(BiffRecord):
    """
    This record contains a Boolean value determining whether to save values
    linked  from external workbooks (CRN records and XCT records). In BIFF3
    and BIFF4 this option is stored in the WSBOOL record.

    Record BOOKBOOL, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       0 = Save external linked values;
                    1 = Do not save external linked values
    """

    _REC_ID = 0x00DA

    def __init__(self):
        self._rec_data = pack('<H', 0x00)


class CountryRecord(BiffRecord):
    """
    This   record   stores  two  Windows  country  identifiers.  The  first
    represents  the  user  interface language of the Excel version that has
    saved  the file, and the second represents the system regional settings
    at the time the file was saved.

    Record COUNTRY, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Windows country identifier of the user interface language of Excel
    2       2       Windows country identifier of the system regional settings

    The  following  table  shows most of the used country identifiers. Most
    of  these  identifiers  are  equal to the international country calling
    codes.

    1   USA
    2   Canada
    7   Russia
    """

    _REC_ID = 0x008C

    def __init__(self, ui_id, sys_settings_id):
        self._rec_data = pack('<2H', ui_id, sys_settings_id)


class UseSelfsRecord(BiffRecord):
    """
    This  record  specifies if the formulas in the workbook can use natural
    language  formulas.  This  type  of  formula can refer to cells by its
    content or the content of the column or row header cell.

    Record USESELFS, BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not use natural language formulas
                    1 = Use natural language formulas

    """

    _REC_ID = 0x0160

    def __init__(self):
        self._rec_data = pack('<H', 0x01)


class EOFRecord(BiffRecord):
    _REC_ID = 0x000A

    def __init__(self):
        self._rec_data = ''


class DateModeRecord(BiffRecord):
    """
    This  record  specifies  the  base date for displaying date values. All
    dates  are  stored as count of days past this base date. In BIFF2-BIFF4
    this   record  is  part  of  the  Calculation  Settings  Block.
    In BIFF5-BIFF8 it is stored in the Workbook Globals Substream.

    Record DATEMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Base is 1899-Dec-31 (the cell = 1 represents 1900-Jan-01)
                    1 = Base is 1904-Jan-01 (the cell = 1 represents 1904-Jan-02)
    """
    _REC_ID = 0x0022

    def __init__(self, from1904):
        if from1904:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class PrecisionRecord(BiffRecord):
    """
    This record stores if formulas use the real cell values for calculation
    or  the  values  displayed  on  the screen. In BIFF2- BIFF4 this record
    is  part of the Calculation Settings Block. In BIFF5-BIFF8 it is stored
    in the Workbook Globals Substream.

    Record PRECISION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Use displayed values;
                    1 = Use real cell values
    """
    _REC_ID = 0x000E

    def __init__(self, use_real_values):
        if use_real_values:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class CodepageBiff8Record(BiffRecord):
    """
    This record stores the text encoding used to write byte strings, stored
    as MS Windows code page identifier. The CODEPAGE record in BIFF8 always
    contains  the  code  page  1200  (UTF-16).  Therefore  it is not
    possible  to  obtain the encoding used for a protection password (it is
    not UTF-16).

    Record CODEPAGE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Code page identifier used for byte string text encoding:
                      016FH = 367 = ASCII
                      01B5H = 437 = IBM PC CP-437 (US)
                      02D0H = 720 = IBM PC CP-720 (OEM Arabic)
                      02E1H = 737 = IBM PC CP-737 (Greek)
                      0307H = 775 = IBM PC CP-775 (Baltic)
                      0352H = 850 = IBM PC CP-850 (Latin I)
                      0354H = 852 = IBM PC CP-852 (Latin II (Central European))
                      0357H = 855 = IBM PC CP-855 (Cyrillic)
                      0359H = 857 = IBM PC CP-857 (Turkish)
                      035AH = 858 = IBM PC CP-858 (Multilingual Latin I with Euro)
                      035CH = 860 = IBM PC CP-860 (Portuguese)
                      035DH = 861 = IBM PC CP-861 (Icelandic)
                      035EH = 862 = IBM PC CP-862 (Hebrew)
                      035FH = 863 = IBM PC CP-863 (Canadian (French))
                      0360H = 864 = IBM PC CP-864 (Arabic)
                      0361H = 865 = IBM PC CP-865 (Nordic)
                      0362H = 866 = IBM PC CP-866 (Cyrillic (Russian))
                      0365H = 869 = IBM PC CP-869 (Greek (Modern))
                      036AH = 874 = Windows CP-874 (Thai)
                      03A4H = 932 = Windows CP-932 (Japanese Shift-JIS)
                      03A8H = 936 = Windows CP-936 (Chinese Simplified GBK)
                      03B5H = 949 = Windows CP-949 (Korean (Wansung))
                      03B6H = 950 = Windows CP-950 (Chinese Traditional BIG5)
                      04B0H = 1200 = UTF-16 (BIFF8)
                      04E2H = 1250 = Windows CP-1250 (Latin II) (Central European)
                      04E3H = 1251 = Windows CP-1251 (Cyrillic)
                      04E4H = 1252 = Windows CP-1252 (Latin I) (BIFF4-BIFF7)
                      04E5H = 1253 = Windows CP-1253 (Greek)
                      04E6H = 1254 = Windows CP-1254 (Turkish)
                      04E7H = 1255 = Windows CP-1255 (Hebrew)
                      04E8H = 1256 = Windows CP-1256 (Arabic)
                      04E9H = 1257 = Windows CP-1257 (Baltic)
                      04EAH = 1258 = Windows CP-1258 (Vietnamese)
                      0551H = 1361 = Windows CP-1361 (Korean (Johab))
                      2710H = 10000 = Apple Roman
                      8000H = 32768 = Apple Roman
                      8001H = 32769 = Windows CP-1252 (Latin I) (BIFF2-BIFF3)
    """
    _REC_ID = 0x0042
    UTF_16 = 0x04B0

    def __init__(self):
        self._rec_data = pack('<H', self.UTF_16)

class Window1Record(BiffRecord):
    """
    Offset Size Contents
    0      2    Horizontal position of the document window (in twips = 1/20 of a point)
    2      2    Vertical position of the document window (in twips = 1/20 of a point)
    4      2    Width of the document window (in twips = 1/20 of a point)
    6      2    Height of the document window (in twips = 1/20 of a point)
    8      2    Option flags:
                  Bits  Mask  Contents
                  0     0001H 0 = Window is visible 1 = Window is hidden
                  1     0002H 0 = Window is open 1 = Window is minimised
                  3     0008H 0 = Horizontal scroll bar hidden 1 = Horizontal scroll bar visible
                  4     0010H 0 = Vertical scroll bar hidden 1 = Vertical scroll bar visible
                  5     0020H 0 = Worksheet tab bar hidden 1 = Worksheet tab bar visible
    10     2    Index to active (displayed) worksheet
    12     2    Index of first visible tab in the worksheet tab bar
    14     2    Number of selected worksheets (highlighted in the worksheet tab bar)
    16     2    Width of worksheet tab bar (in 1/1000 of window width). The remaining space is used by the
                horizontal scrollbar.
    """
    _REC_ID = 0x003D
    # flags

    def __init__(self,
                 hpos_twips, vpos_twips,
                 width_twips, height_twips,
                 flags,
                 active_sheet,
                 first_tab_index, selected_tabs, tab_width):
        self._rec_data = pack('<9H', hpos_twips, vpos_twips,
                                      width_twips, height_twips,
                                      flags,
                                      active_sheet,
                                      first_tab_index, selected_tabs, tab_width)

class FontRecord(BiffRecord):
    """
    WARNING
        The font with index 4 is omitted in all BIFF versions.
        This means the first four fonts have zero-based indexes, and
        the fifth font and all following fonts are referenced with one-based
        indexes.

    Offset Size Contents
    0      2    Height of the font (in twips = 1/20 of a point)
    2      2    Option flags:
                  Bit Mask    Contents
                  0   0001H   1 = Characters are bold (redundant, see below)
                  1   0002H   1 = Characters are italic
                  2   0004H   1 = Characters are underlined (redundant, see below)
                  3   0008H   1 = Characters are struck out
                        0010H 1 = Outline
                        0020H  1 = Shadow
    4     2     Colour index
    6     2     Font weight (100-1000).
                Standard values are 0190H (400) for normal text and 02BCH
                (700) for bold text.
    8     2     Escapement type:
                  0000H = None
                  0001H = Superscript
                  0002H = Subscript
    10    1     Underline type:
                  00H = None
                  01H = Single
                  21H = Single accounting
                  02H = Double
                  22H = Double accounting
    11    1     Font family:
                  00H = None (unknown or don't care)
                  01H = Roman (variable width, serifed)
                  02H = Swiss (variable width, sans-serifed)
                  03H = Modern (fixed width, serifed or sans-serifed)
                  04H = Script (cursive)
                  05H = Decorative (specialised, i.e. Old English, Fraktur)
    12    1     Character set:
                  00H = 0 = ANSI Latin
                  01H = 1 = System default
                  02H = 2 = Symbol
                  4DH = 77 = Apple Roman
                  80H = 128 = ANSI Japanese Shift-JIS
                  81H = 129 = ANSI Korean (Hangul)
                  82H = 130 = ANSI Korean (Johab)
                  86H = 134 = ANSI Chinese Simplified GBK
                  88H = 136 = ANSI Chinese Traditional BIG5
                  A1H = 161 = ANSI Greek
                  A2H = 162 = ANSI Turkish
                  A3H = 163 = ANSI Vietnamese
                  B1H = 177 = ANSI Hebrew
                  B2H = 178 = ANSI Arabic
                  BAH = 186 = ANSI Baltic
                  CCH = 204 = ANSI Cyrillic
                  DEH = 222 = ANSI Thai
                  EEH = 238 = ANSI Latin II (Central European)
                  FFH = 255 = OEM Latin I
    13    1     Not used
    14    var.  Font name:
                  BIFF5/BIFF7: Byte string, 8-bit string length
                  BIFF8: Unicode string, 8-bit string length
    The boldness and underline flags are still set in the options field,
    but not used on reading the font. Font weight and underline type
    are specified in separate fields instead.
    """
    _REC_ID = 0x0031

    def __init__(self,
                    height, options, colour_index, weight, escapement,
                    underline, family, charset,
                    name):
        uname = upack1(name)
        uname_len = len(uname)

        self._rec_data = pack('<5H4B%ds' % uname_len, height, options, colour_index, weight, escapement,
                                                underline, family, charset, 0x00,
                                                uname)

class NumberFormatRecord(BiffRecord):
    """
    Record FORMAT, BIFF8:
    Offset  Size    Contents
    0       2       Format index used in other records
    2       var.    Number format string (Unicode string, 16-bit string length)

    From  BIFF5  on,  the built-in number formats will be omitted. The built-in
    formats  are  dependent  on  the current regional settings of the operating
    system.  The following table shows which number formats are used by default
    in  a  US-English  environment.  All indexes from 0 to 163 are reserved for
    built-in formats. The first user-defined format starts at 164.

    The built-in number formats, BIFF5-BIFF8

    Index   Type        Format string
    0       General     General
    1       Decimal     0
    2       Decimal     0.00
    3       Decimal     #,##0
    4       Decimal     #,##0.00
    5       Currency    "$"#,##0_);("$"#,##
    6       Currency    "$"#,##0_);[Red]("$"#,##
    7       Currency    "$"#,##0.00_);("$"#,##
    8       Currency    "$"#,##0.00_);[Red]("$"#,##
    9       Percent     0%
    10      Percent     0.00%
    11      Scientific  0.00E+00
    12      Fraction    # ?/?
    13      Fraction    # ??/??
    14      Date        M/D/YY
    15      Date        D-MMM-YY
    16      Date        D-MMM
    17      Date        MMM-YY
    18      Time        h:mm AM/PM
    19      Time        h:mm:ss AM/PM
    20      Time        h:mm
    21      Time        h:mm:ss
    22      Date/Time   M/D/YY h:mm
    37      Account     _(#,##0_);(#,##0)
    38      Account     _(#,##0_);[Red](#,##0)
    39      Account     _(#,##0.00_);(#,##0.00)
    40      Account     _(#,##0.00_);[Red](#,##0.00)
    41      Currency    _("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)
    42      Currency    _(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)
    43      Currency    _("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)
    44      Currency    _(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)
    45      Time        mm:ss
    46      Time        [h]:mm:ss
    47      Time        mm:ss.0
    48      Scientific  ##0.0E+0
    49      Text        @
    """
    _REC_ID = 0x041E

    def __init__(self, idx, fmtstr):
        ufmtstr = upack2(fmtstr)
        ufmtstr_len = len(ufmtstr)

        self._rec_data = pack('<H%ds' % ufmtstr_len, idx, ufmtstr)


class XFRecord(BiffRecord):
    """
    XF Substructures
    -------------------------------------------------------------------------
    XF_TYPE_PROT  XF Type and Cell Protection (3 Bits), BIFF3-BIFF8
    These 3 bits are part of a specific data byte.
    Bit Mask    Contents
    0   01H     1 = Cell is locked
    1   02H     1 = Formula is hidden
    2   04H     0 = Cell XF; 1 = Style XF

    XF_USED_ATTRIB   Attributes   Used  from  Parent  Style  XF  (6  Bits),
    BIFF3-BIFF8  Each  bit  describes  the  validity  of  a  specific group
    of  attributes.  In  cell XFs a cleared bit means the attributes of the
    parent  style XF are used (but only if the attributes are valid there),
    a  set  bit  means  the  attributes  of  this XF are used. In style XFs
    a cleared bit means the attribute setting is valid, a set bit means the
    attribute should be ignored.
    Bit Mask    Contents
    0   01H     Flag for number format
    1   02H     Flag for font
    2   04H     Flag for horizontal and vertical alignment, text wrap, indentation, orientation, rotation, and
                text direction
    3   08H     Flag for border lines
    4   10H     Flag for background area style
    5   20H     Flag for cell protection (cell locked and formula hidden)

    XF_HOR_ALIGN  Horizontal Alignment (3 Bits), BIFF2-BIFF8 The horizontal
    alignment consists of 3 bits and is part of a specific data byte.
    Value   Horizontal alignment
    00H     General
    01H     Left
    02H     Centred
    03H     Right
    04H     Filled
    05H     Justified (BIFF4-BIFF8X)
    06H     Centred across selection (BIFF4-BIFF8X)
    07H     Distributed (BIFF8X)

    XF_VERT_ALIGN Vertical Alignment (2 or 3 Bits), BIFF4-BIFF8
    The vertical alignment consists of 2 bits (BIFF4) or 3 bits (BIFF5-BIFF8)
    and is part of a specific data byte. Vertical alignment is not available
    in BIFF2 and BIFF3.
    Value   Vertical alignment
    00H     Top
    01H     Centred
    02H     Bottom
    03H     Justified (BIFF5-BIFF8X)
    04H     Distributed (BIFF8X)

    XF_ORIENTATION  Text  Orientation  (2  Bits),  BIFF4-BIFF7  In the BIFF
    versions  BIFF4-BIFF7,  text  can  be  rotated  in  steps of 90 degrees
    or  stacked.  The  orientation  mode  consists of 2 bits and is part of
    a specific data byte. In BIFF8 a rotation angle occurs instead of these
    flags.
    Value   Text orientation
    00H     Not rotated
    01H     Letters are stacked top-to-bottom, but not rotated
    02H     Text is rotated 90 degrees counterclockwise
    03H     Text is rotated 90 degrees clockwise

    XF_ROTATION Text Rotation Angle (1 Byte), BIFF8
    Value   Text rotation
    0       Not rotated
    1-90    1 to 90 degrees counterclockwise
    91-180  1 to 90 degrees clockwise
    255     Letters are stacked top-to-bottom, but not rotated

    XF_BORDER_34  Cell  Border  Style  (4  Bytes), BIFF3-BIFF4 Cell borders
    contain a line style and a line colour for each line of the border.
    Bit     Mask        Contents
    2-0     00000007H   Top line style
    7-3     000000F8H   Colour index for top line colour
    10-8    00000700H   Left line style
    15-11   0000F800H   Colour index for left line colour
    18-16   00070000H   Bottom line style
    23-19   00F80000H   Colour index for bottom line colour
    26-24   07000000H   Right line style
    31-27   F8000000H   Colour index for right line colour

    XF_AREA_34  Cell  Background  Area  Style (2 Bytes), BIFF3-BIFF4 A cell
    background  area  style  contains  an area pattern and a foreground and
    background colour.
    Bit     Mask    Contents
    5-0     003FH   Fill pattern
    10-6    07C0H   Colour index for pattern colour
    15-11   F800H   Colour index for pattern background
 ---------------------------------------------------------------------------------------------
    Record XF, BIFF8:
    Offset      Size    Contents
    0           2       Index to FONT record
    2           2       Index to FORMAT record
    4           2       Bit     Mask    Contents
                        2-0     0007H   XF_TYPE_PROT . XF type, cell protection (see above)
                        15-4    FFF0H   Index to parent style XF (always FFFH in style XFs)
    6           1       Bit     Mask    Contents
                        2-0     07H     XF_HOR_ALIGN . Horizontal alignment (see above)
                        3       08H     1 = Text is wrapped at right border
                        6-4     70H     XF_VERT_ALIGN . Vertical alignment (see above)
    7           1       XF_ROTATION: Text rotation angle (see above)
    8           1       Bit     Mask    Contents
                        3-0     0FH     Indent level
                        4       10H     1 = Shrink content to fit into cell
                        5               merge
                        7-6     C0H     Text direction (BIFF8X only)
                                        00b = According to context
                                        01b = Left-to-right
                                        10b = Right-to-left
    9           1       Bit     Mask    Contents
                        7-2     FCH     XF_USED_ATTRIB . Used attributes (see above)
    10          4       Cell border lines and background area:
                        Bit     Mask      Contents
                        3-0     0000000FH Left line style
                        7-4     000000F0H Right line style
                        11-8    00000F00H Top line style
                        15-12   0000F000H Bottom line style
                        22-16   007F0000H Colour index for left line colour
                        29-23   3F800000H Colour index for right line colour
                        30      40000000H 1 = Diagonal line from top left to right bottom
                        31      80000000H 1 = Diagonal line from bottom left to right top
    14          4       Bit     Mask      Contents
                        6-0     0000007FH Colour index for top line colour
                        13-7    00003F80H Colour index for bottom line colour
                        20-14   001FC000H Colour index for diagonal line colour
                        24-21   01E00000H Diagonal line style
                        31-26   FC000000H Fill pattern
    18          2       Bit     Mask    Contents
                        6-0     007FH   Colour index for pattern colour
                        13-7    3F80H   Colour index for pattern background

    """
    _REC_ID = 0x00E0

    def __init__(self, xf, xftype='cell'):
        font_xf_idx, fmt_str_xf_idx, alignment, borders, pattern, protection = xf
        fnt = pack('<H', font_xf_idx)
        fmt = pack('<H', fmt_str_xf_idx)
        if xftype == 'cell':
            prt = pack('<H',
                ((protection.cell_locked    & 0x01) << 0) |
                ((protection.formula_hidden & 0x01) << 1)
            )
        else:
            prt = pack('<H', 0xFFF5)
        aln = pack('B',
            ((alignment.horz & 0x07) << 0) |
            ((alignment.wrap & 0x01) << 3) |
            ((alignment.vert & 0x07) << 4)
        )
        rot = pack('B', alignment.rota)
        txt = pack('B',
            ((alignment.inde & 0x0F) << 0) |
            ((alignment.shri & 0x01) << 4) |
            ((alignment.merg & 0x01) << 5) |
            ((alignment.dire & 0x03) << 6)
        )
        if xftype == 'cell':
            used_attr = pack('B', 0xF8)
        else:
            used_attr = pack('B', 0xF4)

        if borders.left == borders.NO_LINE:
            borders.left_colour = 0x00
        if borders.right == borders.NO_LINE:
            borders.right_colour = 0x00
        if borders.top == borders.NO_LINE:
            borders.top_colour = 0x00
        if borders.bottom == borders.NO_LINE:
            borders.bottom_colour = 0x00
        if borders.diag == borders.NO_LINE:
            borders.diag_colour = 0x00
        brd1 = pack('<L',
            ((borders.left          & 0x0F) << 0 ) |
            ((borders.right         & 0x0F) << 4 ) |
            ((borders.top           & 0x0F) << 8 ) |
            ((borders.bottom        & 0x0F) << 12) |
            ((borders.left_colour   & 0x7F) << 16) |
            ((borders.right_colour  & 0x7F) << 23) |
            ((borders.need_diag1    & 0x01) << 30) |
            ((borders.need_diag2    & 0x01) << 31)
        )
        brd2 = pack('<L',
            ((borders.top_colour    & 0x7F) << 0 ) |
            ((borders.bottom_colour & 0x7F) << 7 ) |
            ((borders.diag_colour   & 0x7F) << 14) |
            ((borders.diag          & 0x0F) << 21) |
            ((pattern.pattern       & 0x3F) << 26)
        )
        pat = pack('<H',
            ((pattern.pattern_fore_colour & 0x7F) << 0 ) |
            ((pattern.pattern_back_colour & 0x7F) << 7 )
        )
        self._rec_data = fnt + fmt + prt + \
                        aln + rot + txt + used_attr + \
                        brd1 + brd2 + \
                        pat

class StyleRecord(BiffRecord):
    """
    STYLE record for user-defined cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 0 for user-defined styles
    2       var.    BIFF2-BIFF7: Non-empty byte string, 8-bit string length
                    BIFF8: Non-empty Unicode string, 16-bit string length
    STYLE record for built-in cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 1 for built-in styles
    2       1       Identifier of the built-in cell style:
                        00H = Normal
                        01H = RowLevel_lv (see next field)
                        02H = ColLevel_lv (see next field)
                        03H = Comma
                        04H = Currency
                        05H = Percent
                        06H = Comma [0] (BIFF4-BIFF8)
                        07H = Currency [0] (BIFF4-BIFF8)
                        08H = Hyperlink (BIFF8)
                        09H = Followed Hyperlink (BIFF8)
    3       1       Level for RowLevel or ColLevel style
                    (zero-based, lv), FFH otherwise
    The  RowLevel  and  ColLevel  styles specify the formatting of subtotal
    cells  in  a specific outline level. The level is specified by the last
    field  in the STYLE record. Valid values are 0-6 for the outline levels
    1-7.
    """
    _REC_ID = 0x0293

    def __init__(self):
        self._rec_data = pack('<HBB', 0x8000, 0x00, 0xFF)
        # TODO: implement user-defined styles???


class PaletteRecord(BiffRecord):
    """
    This  record  contains  the  definition  of  all  user-defined  colours
    available for cell and object formatting.

    Record PALETTE, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Number of following colours (nm). Contains 16 in BIFF3-BIFF4 and 56 in BIFF5-BIFF8.
    2       4*nm    List of nm RGB colours

    The following table shows how colour indexes are used in other records:

    Colour index    Resulting colour or internal list index
    00H             Built-in Black (R = 00H, G = 00H, B = 00H)
    01H             Built-in White (R = FFH, G = FFH, B = FFH)
    02H             Built-in Red (R = FFH, G = 00H, B = 00H)
    03H             Built-in Green (R = 00H, G = FFH, B = 00H)
    04H             Built-in Blue (R = 00H, G = 00H, B = FFH)
    05H             Built-in Yellow (R = FFH, G = FFH, B = 00H)
    06H             Built-in Magenta (R = FFH, G = 00H, B = FFH)
    07H             Built-in Cyan (R = 00H, G = FFH, B = FFH)
    08H             First user-defined colour from the PALETTE record (entry 0 from record colour list)
    .........................

    17H (BIFF3-BIFF4) Last user-defined colour from the PALETTE record (entry 15 or 55 from record colour list)
    3FH (BIFF5-BIFF8)

    18H (BIFF3-BIFF4) System window text colour for border lines (used in records XF, CF, and
    40H (BIFF5-BIFF8) WINDOW2 (BIFF8 only))

    19H (BIFF3-BIFF4) System window background colour for pattern background (used in records XF, and CF)
    41H (BIFF5-BIFF8)

    43H             System face colour (dialogue background colour)
    4DH             System window text colour for chart border lines
    4EH             System window background colour for chart areas
    4FH             Automatic colour for chart border lines (seems to be always Black)
    50H             System ToolTip background colour (used in note objects)
    51H             System ToolTip text colour (used in note objects)
    7FFFH           System window text colour for fonts (used in records FONT, EFONT, and CF)

    """
    _REC_ID = 0x0092


class BoundSheetRecord(BiffRecord):
    """
    This  record  is  located  in  the workbook globals area and represents
    a  sheet  inside  of  the  workbook. For each sheet a BOUNDSHEET record
    is  written.  It  stores  the sheet name and a stream offset to the BOF
    record    within   the   workbook   stream.  The  record  is also known
    as BUNDLESHEET.

    Record BOUNDSHEET, BIFF5-BIFF8:
    Offset  Size    Contents
    0       4       Absolute stream position of the BOF record of the sheet represented by this record. This
                    field is never encrypted in protected files.
    4       1       Visibility:
                        00H = Visible
                        01H = Hidden
                        02H = Strong hidden
    5       1       Sheet type:
                        00H = Worksheet
                        02H = Chart
                        06H = Visual Basic module
    6       var.    Sheet name:
                        BIFF5/BIFF7: Byte string, 8-bit string length
                        BIFF8: Unicode string, 8-bit string length
    """
    _REC_ID = 0x0085

    def __init__(self, stream_pos, visibility, sheetname, encoding='ascii'):
        usheetname = upack1(sheetname, encoding)
        uusheetname_len = len(usheetname)

        self._rec_data = pack('<LBB%ds' % uusheetname_len, stream_pos, visibility, 0x00, usheetname)


class ContinueRecord(BiffRecord):
    """
    Whenever  the content of a record exceeds the given limits (see table),
    the  record  must  be  split.  Several  CONTINUE records containing the
    additional data are added after the parent record.

    BIFF version    Maximum data size of a record
    BIFF2-BIFF7     2080 bytes (2084 bytes including record header)
    BIFF8           8224 bytes (8228 bytes including record header) (0x2020)

    Record CONTINUE, BIFF2-BIFF8:
    Offset  Size    Contents
    0       var.    Data continuation of the previous record

    Unicode  strings  are  split in a special way. At the beginning of each
    CONTINUE  record  the option flags byte is repeated. Only the character
    size  flag  will  be set in this flags byte, the Rich-Text flag and the
    Far-East  flag  are set to zero. In each CONTINUE record it is possible
    that  the  character  size  changes  from  8-bit  characters  to 16-bit
    characters and vice versa.

    Never  a  Unicode  string  is  split  until  and  including  the  first
    character.  That means, all header fields (string length, option flags,
    optional Rich-Text size, and optional Far-East data size) and the first
    character  of  the string have to occur together in the leading record,
    or  have  to  be  moved completely into the CONTINUE record. Formatting
    runs cannot be split between their components (character index and FONT
    record  index).  If  a string is split between two formatting runs, the
    option flags field will not be repeated in the CONTINUE record.
    """
    _REC_ID = 0x003C


class SSTRecord(BiffRecord):
    """
    This  record  contains  a  list  of  all  strings  used anywhere in the
    workbook.  Each string occurs only once. The workbook uses indexes into
    the list to reference the strings.

    Record SST, BIFF8:
    Offset  Size    Contents
    0       4       Total number of strings in the workbook (see below)
    4       4       Number of following strings (nm)
    8       var.    List of nm Unicode strings, 16-bit string length

    The  first  field  of  the  SST  record  counts  the  total  occurrence
    of  strings  in  the  workbook.  For  instance,  the string AAA is used
    3  times  and  the string BBB is used 2 times. The first field contains
    5 and the second field contains 2, followed by the two strings.
    """
    _REC_ID = 0x00FC


class ExtSSTRecord(BiffRecord):
    """
    This  record  occurs  in  conjunction  with  the SST record. It is used
    by  Excel  to create a hash table with stream offsets to the SST record
    to optimise string search operations. Excel may not shorten this record
    if  strings  are deleted from the shared string table, so the last part
    might  contain  invalid  data. The stream indexes in this record divide
    the SST into portions containing a constant number of strings.

    Record EXTSST, BIFF8:

    Offset  Size    Contents
    0       2       Number of strings in a portion, this number is >=8
    2       var.    List of OFFSET structures for all portions. Each OFFSET contains the following data:
                        Offset Size Contents
                        0       4   Absolute stream position of first string of the portion
                        4       2   Position of first string of the portion inside of current record,
                                    including record header. This counter restarts at zero, if the SST
                                    record is continued with a CONTINUE record.
                        6       2   Not used
    """
    _REC_ID = 0x00FF

    def __init__(self, sst_stream_pos, str_placement, portions_len):
        extsst = {}
        abs_stream_pos = sst_stream_pos
        str_counter = 0
        portion_counter = 0
        while str_counter < len(str_placement):
            str_chunk_num, pos_in_chunk = str_placement[str_counter]
            if str_chunk_num <> portion_counter:
                portion_counter = str_chunk_num
                abs_stream_pos += portions_len[portion_counter-1]
                #print hex(abs_stream_pos)
            str_stream_pos = abs_stream_pos + pos_in_chunk + 4 # header
            extsst[str_counter] = (pos_in_chunk, str_stream_pos)
            str_counter += 1

        exsst_str_count_delta = max(8, len(str_placement)*8/0x2000) # maybe smth else?
        self._rec_data = pack('<H', exsst_str_count_delta)
        str_counter = 0
        while str_counter < len(str_placement):
            self._rec_data += pack('<IHH', extsst[str_counter][1], extsst[str_counter][0], 0)
            str_counter += exsst_str_count_delta

class DimensionsRecord(BiffRecord):
    """
    Record DIMENSIONS, BIFF8:

    Offset  Size    Contents
    0       4       Index to first used row
    4       4       Index to last used row, increased by 1
    8       2       Index to first used column
    10      2       Index to last used column, increased by 1
    12      2       Not used
    """
    _REC_ID = 0x0200
    def __init__(self, first_used_row, last_used_row, first_used_col, last_used_col):
        if first_used_row > last_used_row or first_used_col > last_used_col:
            # Special case: empty worksheet
            first_used_row = first_used_col = 0
            last_used_row = last_used_col = -1
        self._rec_data = pack('<2L3H',
            first_used_row, last_used_row + 1,
            first_used_col, last_used_col + 1,
            0x00)


class Window2Record(BiffRecord):
    """
    Record WINDOW2, BIFF8:

    Offset  Size Contents
    0       2 Option flags (see below)
    2       2 Index to first visible row
    4       2 Index to first visible column
    6       2 Colour index of grid line colour. Note that in BIFF2-BIFF7 an RGB colour is
                written instead.
    8       2 Not used
    10      2 Cached magnification factor in page break preview (in percent); 0 = Default (60%)
    12      2 Cached magnification factor in normal view (in percent); 0 = Default (100%)
    14      4 Not used

    In  BIFF8  this record stores used magnification factors for page break
    preview  and  normal  view.  These  values  are  used  to  restore  the
    magnification,  when the view is changed. The real magnification of the
    currently  active  view  is  stored  in the SCL record. The type of the
    active view is stored in the option flags field (see below).

     0 0001H 0 = Show formula results 1 = Show formulas
     1 0002H 0 = Do not show grid lines 1 = Show grid lines
     2 0004H 0 = Do not show sheet headers 1 = Show sheet headers
     3 0008H 0 = Panes are not frozen 1 = Panes are frozen (freeze)
     4 0010H 0 = Show zero values as empty cells 1 = Show zero values
     5 0020H 0 = Manual grid line colour 1 = Automatic grid line colour
     6 0040H 0 = Columns from left to right 1 = Columns from right to left
     7 0080H 0 = Do not show outline symbols 1 = Show outline symbols
     8 0100H 0 = Keep splits if pane freeze is removed 1 = Remove splits if pane freeze is removed
     9 0200H 0 = Sheet not selected 1 = Sheet selected (BIFF5-BIFF8)
    10 0400H 0 = Sheet not visible 1 = Sheet visible (BIFF5-BIFF8)
    11 0800H 0 = Show in normal view 1 = Show in page break preview (BIFF8)

    The freeze flag specifies, if a following PANE record describes unfrozen or frozen panes.

    *** This class appends the optional SCL record ***

    Record SCL, BIFF4-BIFF8:

    This record stores the magnification of the active view of the current worksheet.
    In BIFF8 this can be either the normal view or the page break preview.
    This is determined in the WINDOW2 record. The SCL record is part of the
    Sheet View Settings Block.

    Offset  Size    Contents
    0       2       Numerator of the view magnification fraction (num)
    2       2       Denumerator [denominator] of the view magnification fraction (den)
    The magnification is stored as reduced fraction. The magnification results from num/den.

    SJM note: Excel expresses (e.g.) 25% in reduced form i.e. 1/4. Reason unknown. This code
    writes 25/100, and Excel is happy with that.

    """
    _REC_ID = 0x023E

    def __init__(self, options, first_visible_row, first_visible_col,
        grid_colour, preview_magn, normal_magn, scl_magn):
        self._rec_data = pack('<7HL', options,
                                    first_visible_row, first_visible_col,
                                    grid_colour,
                                    0x00,
                                    preview_magn, normal_magn,
                                    0x00L)
        if scl_magn:
            self._scl_rec = pack('<4H', 0x00A0, 4, scl_magn, 100)
        else:
            self._scl_rec = ''

    def get(self):
        return self.get_rec_header() + self._rec_data + self._scl_rec


class PanesRecord(BiffRecord):
    """
    This record stores the position of window panes. It is part of the Sheet
    View Settings Block. If the sheet does not contain any splits, this
    record will not occur.
    A sheet can be split in two different ways, with unfrozen panes or with
    frozen panes. A flag in the WINDOW2 record specifies, if the panes are
    frozen, which affects the contents of this record.

    Record PANE, BIFF2-BIFF8:
    Offset      Size        Contents
    0           2           Position of the vertical split
                            (px, 0 = No vertical split):
                            Unfrozen pane: Width of the left pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            columns in left pane(s)
    2           2           Position of the horizontal split
                            (py, 0 = No horizontal split):
                            Unfrozen pane: Height of the top pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            rows in top pane(s)
    4           2           Index to first visible row
                            in bottom pane(s)
    6           2           Index to first visible column
                            in right pane(s)
    8           1           Identifier of pane with active
                            cell cursor
    [9]         1           Not used (BIFF5-BIFF8 only, not written
                            in BIFF2-BIFF4)

    If the panes are frozen, pane0 is always active, regardless
    of the cursor position. The correct identifiers for all possible
    combinations of visible panes are shown in the following pictures.

    px = 0, py = 0                  px = 0, py > 0
    --------------------------      ------------|-------------
    |                        |      |                        |
    |                        |      |           3            |
    |                        |      |                        |
    -           3            -      --------------------------
    |                        |      |                        |
    |                        |      |           2            |
    |                        |      |                        |
    --------------------------      ------------|-------------

    px > 0, py = 0                  px > 0, py > 0
    ------------|-------------      ------------|-------------
    |           |            |      |           |            |
    |           |            |      |     3     |      2     |
    |           |            |      |           |            |
    -     3     |      1     -      --------------------------
    |           |            |      |           |            |
    |           |            |      |     1     |      0     |
    |           |            |      |           |            |
    ------------|-------------      ------------|-------------
    """
    _REC_ID = 0x0041
    def __init__(self, px, py, first_row_bottom, first_col_right, active_pane):
        self._rec_data = pack('<5H',
                                            px, py,
                                            first_row_bottom, first_col_right,
                                            active_pane)


class RowRecord(BiffRecord):
    """
    This  record  contains  the properties of a single row in a sheet. Rows
    and cells in a sheet are divided into blocks of 32 rows.

    Record ROW, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index of this row
    2       2       Index to column of the first cell which is described by a cell record
    4       2       Index to column of the last cell which is described by a cell record,
                    increased by 1
    6       2       Bit     Mask    Contents
                    14-0    7FFFH   Height of the row, in twips = 1/20 of a point
                    15      8000H   0 = Row has custom height; 1 = Row has default height
    8       2       Not used
    10      2       In BIFF3-BIFF4 this field contains a relative offset
                    to calculate stream position of the first cell record
                    for this row. In BIFF5-BIFF8 this field is not used
                    anymore, but the DBCELL record instead.
    12      4       Option flags and default row formatting:
                    Bit     Mask        Contents
                    2-0     00000007H   Outline level of the row
                    4       00000010H   1 = Outline group starts or ends here (depending
                                        on where the outline buttons are located,
                                        see WSBOOL record), and is collapsed
                    5       00000020H   1 = Row is hidden (manually, or by a filter or outline group)
                    6       00000040H   1 = Row height and default font height do not match
                    7       00000080H   1 = Row has explicit default format (fl)
                    8       00000100H   Always 1
                    27-16   0FFF0000H   If fl=1: Index to default XF record
                    28      10000000H   1 = Additional space above the row. This flag is set,
                                        if the upper border of at least one cell in this row
                                        or if the lower border of at least one cell in the row
                                        above is formatted with a thick line style.
                                        Thin and medium line styles are not taken into account.
                    29      20000000H   1 = Additional space below the row. This flag is set,
                                        if the lower border of at least one cell in this row
                                        or if the upper border of at least one cell in the row
                                        below is formatted with a medium or thick line style.
                                        Thin line styles are not taken into account.
    """

    _REC_ID = 0x0208

    def __init__(self, index, first_col, last_col, height_options, options):
        self._rec_data = pack('<6HL', index, first_col, last_col + 1,
                                        height_options,
                                        0x00, 0x00,
                                        options)

class LabelSSTRecord(BiffRecord):
    """
    This record represents a cell that contains a string. It replaces the
    LABEL record and RSTRING record used in BIFF2-BIFF7.
    """
    _REC_ID = 0x00FD

    def __init__(self, row, col, xf_idx, sst_idx):
        self._rec_data = pack('<3HL', row, col, xf_idx, sst_idx)


class MergedCellsRecord(BiffRecord):
    """
    This record contains all merged cell ranges of the current sheet.

    Record MERGEDCELLS, BIFF8:

    Offset  Size    Contents
    0       var.    Cell range address list with all merged ranges

    ------------------------------------------------------------------

    A cell range address list consists of a field with the number of ranges
    and the list of the range addresses.

    Cell range address list, BIFF8:

    Offset  Size            Contents
    0       2               Number of following cell range addresses (nm)
    2       8*nm            List of nm cell range addresses

    ---------------------------------------------------------------------
    Cell range address, BIFF8:

    Offset  Size    Contents
    0       2       Index to first row
    2       2       Index to last row
    4       2       Index to first column
    6       2       Index to last column

    """
    _REC_ID = 0x00E5

    def __init__(self, merged_list):
        i = len(merged_list) - 1
        while i >= 0:
            j = 0
            merged = ''
            while (i >= 0) and (j < 0x403):
                r1, r2, c1, c2 = merged_list[i]
                merged += pack('<4H', r1, r2, c1, c2)
                i -= 1
                j += 1
            self._rec_data += pack('<3H', self._REC_ID, len(merged) + 2, j) + \
                                    merged

    # for some reason Excel doesn't use CONTINUE
    def get(self):
        return self._rec_data

class MulBlankRecord(BiffRecord):
    """
    This  record  represents  a  cell  range  of empty cells. All cells are
    located in the same row.

    Record MULBLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2*nc    List of nc=lc-fc+1 16-bit indexes to XF records
    4+2*nc  2       Index to last column (lc)
    """
    _REC_ID = 0x00BE

    def __init__(self, row, first_col, last_col, xf_index):
        blanks_count = last_col-first_col+1
        self._rec_data = pack('%dH' % blanks_count, *([xf_index]*blanks_count))
        self._rec_data = pack('<2H', row, first_col) +  self._rec_data + pack('<H',  last_col)


class BlankRecord(BiffRecord):
    """
    This  record  represents  an empty cell.

    Record BLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2       indexes to XF record
    """
    _REC_ID = 0x0201

    def __init__(self, row, col, xf_index):
        self._rec_data = pack('<3H', row, col, xf_index)


class RKRecord(BiffRecord):
    """
    This record represents a cell that contains an RK value (encoded integer or
    floating-point value). If a floating-point value cannot be encoded to an RK value,
    a NUMBER record will be written.
    """
    _REC_ID = 0x027E

    def __init__(self, row, col, xf_index, rk_encoded):
        self._rec_data = pack('<3Hi', row, col, xf_index, rk_encoded)


class NumberRecord(BiffRecord):
    """
    This record represents a cell that contains an IEEE-754 floating-point value.
    """
    _REC_ID = 0x0203

    def __init__(self, row, col, xf_index, number):
        self._rec_data = pack('<3Hd', row, col, xf_index, number)

class BoolErrRecord(BiffRecord):
    """
    This record represents a cell that contains a boolean or error value.
    """
    _REC_ID = 0x0205

    def __init__(self, row, col, xf_index, number, is_error):
        self._rec_data = pack('<3HBB', row, col, xf_index, number, is_error)


class FormulaRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Index to row
    2      2    Index to column
    4      2    Index to XF record
    6      8    Result of the formula
    14     2    Option flags:
                Bit Mask    Contents
                0   0001H   1 = Recalculate always
                1   0002H   1 = Calculate on open
                3   0008H   1 = Part of a shared formula
    16     4    Not used
    20     var. Formula data (RPN token array)

    """
    _REC_ID = 0x0006

    def __init__(self, row, col, xf_index, rpn, calc_flags=0):
        self._rec_data = pack('<3HQHL', row, col, xf_index, 0xFFFF000000000003, calc_flags & 3, 0) + rpn


class GutsRecord(BiffRecord):
    """
    This record contains information about the layout of outline symbols.

    Record GUTS, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Width of the area to display row outlines (left of the sheet), in pixel
    2       2       Height of the area to display column outlines (above the sheet), in pixel
    4       2       Number of visible row outline levels (used row levels + 1; or 0, if not used)
    6       2       Number of visible column outline levels (used column levels + 1; or 0, if not used)

    """

    _REC_ID = 0x0080

    def __init__(self, row_gut_width, col_gut_height, row_visible_levels, col_visible_levels):
        self._rec_data = pack('<4H', row_gut_width, col_gut_height, row_visible_levels, col_visible_levels)

class WSBoolRecord(BiffRecord):
    """
    This  record stores a 16 bit value with Boolean options for the current
    sheet.  From BIFF5 on the "Save external linked values" option is moved
    to the record BOOKBOOL.

    Option flags of record WSBOOL, BIFF3-BIFF8:

    Bit     Mask    Contents
    0       0001H   0 = Do not show automatic page breaks
                    1 = Show automatic page breaks
    4       0010H   0 = Standard sheet
                    1 = Dialogue sheet (BIFF5-BIFF8)
    5       0020H   0 = No automatic styles in outlines
                    1 = Apply automatic styles to outlines
    6       0040H   0 = Outline buttons above outline group
                    1 = Outline buttons below outline group
    7       0080H   0 = Outline buttons left of outline group
                    1 = Outline buttons right of outline group
    8       0100H   0 = Scale printout in percent
                    1 = Fit printout to number of pages
    9       0200H   0 = Save external linked values (BIFF3?BIFF4 only)
                    1 = Do not save external linked values (BIFF3?BIFF4 only)
    10      0400H   0 = Do not show row outline symbols
                    1 = Show row outline symbols
    11      0800H   0 = Do not show column outline symbols
                    1 = Show column outline symbols
    13-12   3000H   These flags specify the arrangement of windows.
                    They are stored in BIFF4 only.
                    00 = Arrange windows tiled
                    01 = Arrange windows horizontal
                    10 = Arrange windows vertical112 = Arrange windows cascaded
    The following flags are valid for BIFF4-BIFF8 only:
    14      4000H   0 = Standard expression evaluation
                    1 = Alternative expression evaluation
    15      8000H   0 = Standard formula entries
                    1 = Alternative formula entries

    """
    _REC_ID = 0x0081

    def __init__(self, options):
        self._rec_data = pack('<H', options)

class ColInfoRecord(BiffRecord):
    """
    This record specifies the width for a given range of columns.
    If a column does not have a corresponding COLINFO record,
    the width specified in the record STANDARDWIDTH is used. If
    this record is also not present, the contents of the record
    DEFCOLWIDTH is used instead.
    This record also specifies a default XF record to use for
    cells in the columns that are not described by any cell record
    (which contain the XF index for that cell). Additionally,
    the option flags field contains hidden, outline, and collapsed
    options applied at the columns.

    Record COLINFO, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index to first column in the range
    2       2       Index to last column in the range
    4       2       Width of the columns in 1/256 of the width of the zero character, using default font
                    (first FONT record in the file)
    6       2       Index to XF record for default column formatting
    8       2       Option flags:
                    Bits    Mask    Contents
                    0       0001H   1 = Columns are hidden
                    10-8    0700H   Outline level of the columns (0 = no outline)
                    12      1000H   1 = Columns are collapsed
    10      2       Not used

    """
    _REC_ID = 0x007D

    def __init__(self, first_col, last_col, width, xf_index, options):
        self._rec_data = pack('<6H', first_col, last_col, width, xf_index, options, 0)

class CalcModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It specifies whether to calculate formulas manually,
    automatically or automatically except for multiple table operations.

    Record CALCMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       FFFFH = automatic except for multiple table operations
                    0000H = manually
                    0001H = automatically (default)
    """
    _REC_ID = 0x000D

    def __init__(self, calc_mode):
        self._rec_data = pack('<h', calc_mode)


class CalcCountRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block. It specifies the maximum
    number of times the formulas should be iteratively calculated. This is a fail-safe
    against mutually recursive formulas locking up a spreadsheet application.

    Record CALCCOUNT, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Maximum number of iterations allowed in circular references
    """

    _REC_ID = 0x000C

    def __init__(self, calc_count):
        self._rec_data = pack('<H', calc_count)

class RefModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores which method is used to show cell addresses in formulas.
    The RC mode uses numeric indexes for rows and columns,
    i.e. R(1)C(-1), or R1C1:R2C2.
    The A1 mode uses characters for columns and numbers for rows,
    i.e. B1, or $A$1:$B$2.

    Record REFMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = RC mode; 1 = A1 mode

    """
    _REC_ID = 0x00F

    def __init__(self, ref_mode):
        self._rec_data = pack('<H', ref_mode)

class IterationRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores if iterations are allowed while calculating recursive formulas.

    Record ITERATION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Iterations off; 1 = Iterations on
    """
    _REC_ID = 0x011

    def __init__(self, iterations_on):
        self._rec_data = pack('<H', iterations_on)

class DeltaRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores the maximum change of the result to exit an iteration.

    Record DELTA, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Maximum change in iteration
                    (IEEE 754 floating-point value,
                     64bit double precision)
    """
    _REC_ID = 0x010

    def __init__(self, delta):
        self._rec_data = pack('<d', delta)

class SaveRecalcRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It contains the Recalculate before save option in
    Excel's calculation settings dialogue.

    Record SAVERECALC, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not recalculate;
                    1 = Recalculate before saving the document

    """
    _REC_ID = 0x05F

    def __init__(self, recalc):
        self._rec_data = pack('<H', recalc)

class PrintHeadersRecord(BiffRecord):
    """
    This record stores if the row and column headers
    (the areas with row numbers and column letters) will be printed.

    Record PRINTHEADERS, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print row/column headers;
                    1 = Print row/column headers
    """
    _REC_ID = 0x02A

    def __init__(self, print_headers):
        self._rec_data = pack('<H', print_headers)


class PrintGridLinesRecord(BiffRecord):
    """
    This record stores if sheet grid lines will be printed.

    Record PRINTGRIDLINES, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print sheet grid lines;
                    1 = Print sheet grid lines

    """
    _REC_ID = 0x02B

    def __init__(self, print_grid):
        self._rec_data = pack('<H', print_grid)


class GridSetRecord(BiffRecord):
    """
    This record specifies if the option to print sheet grid lines
    (record PRINTGRIDLINES) has ever been changed.

    Record GRIDSET, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print grid lines option never changed
                    1 = Print grid lines option changed
    """
    _REC_ID = 0x082

    def __init__(self, print_grid_changed):
        self._rec_data = pack('<H', print_grid_changed)


class DefaultRowHeightRecord(BiffRecord):
    """
    This record specifies the default height and default flags
    for rows that do not have a corresponding ROW record.

    Record DEFAULTROWHEIGHT, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Option flags:
                    Bit Mask    Contents
                    0   0001H   1 = Row height and default font height do not match
                    1   0002H   1 = Row is hidden
                    2   0004H   1 = Additional space above the row
                    3   0008H   1 = Additional space below the row
    2       2       Default height for unused rows, in twips = 1/20 of a point

    """
    _REC_ID = 0x0225

    def __init__(self, options, def_height):
        self._rec_data = pack('<2H', options, def_height)


class DefColWidthRecord(BiffRecord):
    """
    This record specifies the default column width for columns that
    do not have a specific width set using the record COLINFO or COLWIDTH.
    This record has no effect, if a STANDARDWIDTH record is present in the file.

    Record DEFCOLWIDTH, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Column width in characters, using the width of the zero
                    character from default font (first FONT record in the file)
    """
    _REC_ID = 0x0055

    def __init__(self, def_width):
        self._rec_data = pack('<H', options, def_width)

class HorizontalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    horizontal manual page breaks.

    Record HORIZONTALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following row index structures (nm)
    2       6nm   List of nm row index structures. Each row index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first row below the page break
                    2       2       Index to first column of this page break
                    4       2       Index to last column of this page break

    The row indexes in the lists must be ordered ascending.
    If in BIFF8 a row contains several page breaks, they must be ordered
    ascending by start column index.
    """
    _REC_ID = 0x001B

    def __init__(self, breaks_list):
        self._rec_data = pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += pack('<3H', r, c1, c2)

class VerticalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    vertical manual page breaks.

    Record VERTICALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following column index structures (nm)
    2       6nm   List of nm column index structures. Each column index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first column following the page
                                    break
                    2       2       Index to first row of this page break
                    4       2       Index to last row of this page break

    The column indexes in the lists must be ordered ascending.
    If in BIFF8 a column contains several page breaks, they must be ordered
    ascending by start row index.
    """
    _REC_ID = 0x001A

    def __init__(self, breaks_list):
        self._rec_data = pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += pack('<3H', r, c1, c2)

class HeaderRecord(BiffRecord):
    """
    This record is part of the Page Settings Block. It specifies the
    page  header  string  for  the current worksheet. If this record is not
    present  or  completely  empty  (record  size is 0), the sheet does not
    contain a page header.

    Record HEADER for non-empty page header, BIFF2-BIFF8:
    Offset      Size    Contents
    0           var.    Page header string
                        BIFF2-BIFF7:    Non-empty byte string, 8bit string
                        length
                        BIFF8: Non-empty Unicode string, 16bit string length
    The  header  string may contain special commands, i.e. placeholders for
    the  page  number,  current  date, or text formatting attributes. These
    fields  are  represented  by  single  letters (exception: font name and
    size,  see  below)  with  a  leading  ampersand ("&"). If the ampersand
    is  part  of the regular header text, it will be duplicated ("&&"). The
    page  header is divided into 3 sections: the left, the centred, and the
    right  section.  Each  section  is introduced by a special command. All
    text  and all commands following are part of the selected section. Each
    section  starts  with the text formatting specified in the default font
    (first  FONT  record  in  the  file). Active formatting attributes from
    a previous section do not go into the next section.

    The following table shows all available commands:

    Command         Contents
    &&              The "&" character itself
    &L              Start of the left section
    &C              Start of the centred section
    &R              Start of the right section
    &P              Current page number
    &N              Page count
    &D              Current date
    &T              Current time
    &A              Sheet name (BIFF5-BIFF8)
    &F              File name without path
    &Z              File path without file name (BIFF8X)
    &G              Picture (BIFF8X)
    &B              Bold on/off (BIFF2-BIFF4)
    &I              Italic on/off (BIFF2-BIFF4)
    &U              Underlining on/off
    &E              Double underlining on/off (BIFF5-BIFF8)
    &S              Strikeout on/off
    &X              Superscript on/off (BIFF5-BIFF8)
    &Y              Subscript on/off (BIFF5-BIFF8)
    &"<fontname>"   Set new font <fontname>
    &"<fontname>,<fontstyle>"
                    Set new font with specified style <fontstyle>.
                    The style <fontstyle> is in most cases one of
                    "Regular", "Bold", "Italic", or "Bold Italic".
                    But this setting is dependent on the used font,
                    it may differ (localised style names, or "Standard",
                    "Oblique", ...). (BIFF5-BIFF8)
    &<fontheight>   Set font height in points (<fontheight> is a decimal value).
                    If this command is followed by a plain number to be printed
                    in the header, it will be separated from the font height
                    with a space character.

    """
    _REC_ID = 0x0014

    def __init__(self, header_str):
        self._rec_data = upack2(header_str)

class FooterRecord(BiffRecord):
    """
    Semantic is equal to HEADER record
    """
    _REC_ID = 0x0015

    def __init__(self, footer_str):
        self._rec_data = upack2(footer_str)


class HCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred horizontally when printed.

    Record HCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet left aligned
                    1 = Print sheet centred horizontally

    """
    _REC_ID = 0x0083

    def __init__(self, is_horz_center):
        self._rec_data = pack('<H', is_horz_center)


class VCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred vertically when printed.

    Record VCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet aligned at top page border
                    1 = Print sheet vertically centred

    """
    _REC_ID = 0x0084

    def __init__(self, is_vert_center):
        self._rec_data = pack('<H', is_vert_center)


class LeftMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the left
    page margin of the current worksheet.

    Record LEFTMARGIN, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Left page margin in inches
                    (IEEE 754 floating-point value, 64bit double precision)

    """
    _REC_ID = 0x0026

    def __init__(self, margin):
        self._rec_data = pack('<d', margin)


class RightMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the right
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Right page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0027

    def __init__(self, margin):
        self._rec_data = pack('<d', margin)

class TopMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the top
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Top page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0028

    def __init__(self, margin):
        self._rec_data = pack('<d', margin)


class BottomMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the bottom
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Bottom page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0029

    def __init__(self, margin):
        self._rec_data = pack('<d', margin)

class SetupPageRecord(BiffRecord):
    """
    This   record   is  part of the Page Settings Block. It stores the page
    format   settings   of   the  current sheet. The pages may be scaled in
    percent   or  by  using  an  absolute  number of pages. This setting is
    located   in  the  WSBOOL  record.  If  pages  are  scaled in  percent,
    the   scaling  factor  in  this  record is used, otherwise the "Fit  to
    pages"  values. One of the "Fit to pages" values may be 0. In this case
    the sheet is scaled to fit only to the other value.

    Record SETUP, BIFF5-BIFF8:

    Offset      Size    Contents
    0           2       Paper size (see below)
    2           2       Scaling factor in percent
    4           2       Start page number
    6           2       Fit worksheet width to this number of pages
                        (0 = use as many as needed)
    8           2       Fit worksheet height to this number of pages
                        (0 = use as many as needed)
    10          2       Option flags:
                        Bit     Mask        Contents
                        0       0001H       0 = Print pages in columns
                                            1 = Print pages in rows
                        1       0002H       0 = Landscape
                                            1 = Portrait
                        2       0004H       1 = Paper size, scaling factor,
                                            paper orientation (portrait/landscape),
                                            print resolution and number of copies
                                            are not initialised
                        3       0008H       0 = Print coloured
                                            1 = Print black and white
                        4       0010H       0 = Default print quality
                                            1 = Draft quality
                        5       0020H       0 = Do not print cell notes
                                            1 = Print cell notes
                        6       0040H       0 = Paper orientation setting is valid
                                            1 = Paper orientation setting not
                                            initialised
                        7       0080H       0 = Automatic page numbers
                                            1 = Use start page number
                        The following flags are valid for BIFF8 only:
                        9       0200H       0 = Print notes as displayed
                                            1 = Print notes at end of sheet
                        11-10   0C00H       00 = Print errors as displayed
                                            01 = Do not print errors
                                            10 = Print errors as "--"
                                            11 = Print errors as "#N/A!"
    12          2       Print resolution in dpi
    14          2       Vertical print resolution in dpi
    16          8       Header margin (IEEE 754 floating-point value,
                        64bit double precision)
    24          8       Footer margin (IEEE 754 floating-point value,
                        64bit double precision)
    32          2       Number of copies to print


    PAPER TYPES:

    Index   Paper type              Paper size
    0       Undefined
    1       Letter                  8 1/2" x 11"
    2       Letter small            8 1/2" x 11"
    3       Tabloid                 11" x 17"
    4       Ledger                  17" x 11"
    5       Legal                   8 1/2" x 14"
    6       Statement               5 1/2" x 8 1/2"
    7       Executive               7 1/4" x 10 1/2"
    8       A3                      297mm x 420mm
    9       A4                      210mm x 297mm
    10      A4 small                210mm x 297mm
    11      A5                      148mm x 210mm
    12      B4 (JIS)                257mm x 364mm
    13      B5 (JIS)                182mm x 257mm
    14      Folio                   8 1/2" x 13"
    15      Quarto                  215mm x 275mm
    16      10x14                   10" x 14"
    17      11x17                   11" x 17"
    18      Note                    8 1/2" x 11"
    19      Envelope #9             3 7/8" x 8 7/8"
    20      Envelope #10            4 1/8" x 9 1/2"
    21      Envelope #11            4 1/2" x 10 3/8"
    22      Envelope #12            4 3/4" x 11"
    23      Envelope #14            5" x 11 1/2"
    24      C                       17" x 22"
    25      D                       22" x 34"
    26      E                       34" x 44"
    27      Envelope DL             110mm x 220mm
    28      Envelope C5             162mm x 229mm
    29      Envelope C3             324mm x 458mm
    30      Envelope C4             229mm x 324mm
    31      Envelope C6             114mm x 162mm
    32      Envelope C6/C5          114mm x 229mm
    33      B4 (ISO)                250mm x 353mm
    34      B5 (ISO)                176mm x 250mm
    35      B6 (ISO)                125mm x 176mm
    36      Envelope Italy          110mm x 230mm
    37      Envelope Monarch        3 7/8" x 7 1/2"
    38      63/4 Envelope           3 5/8" x 6 1/2"
    39      US Standard Fanfold     14 7/8" x 11"
    40      German Std. Fanfold     8 1/2" x 12"
    41      German Legal Fanfold    8 1/2" x 13"
    42      B4 (ISO)                250mm x 353mm
    43      Japanese Postcard       100mm x 148mm
    44      9x11                    9" x 11"
    45      10x11                   10" x 11"
    46      15x11                   15" x 11"
    47      Envelope Invite         220mm x 220mm
    48      Undefined
    49      Undefined
    50      Letter Extra            9 1/2" x 12"
    51      Legal Extra             9 1/2" x 15"
    52      Tabloid Extra           11 11/16" x 18"
    53      A4 Extra                235mm x 322mm
    54      Letter Transverse       8 1/2" x 11"
    55      A4 Transverse           210mm x 297mm
    56      Letter Extra Transv.    9 1/2" x 12"
    57      Super A/A4              227mm x 356mm
    58      Super B/A3              305mm x 487mm
    59      Letter Plus             8 1/2" x 12 11/16"
    60      A4 Plus                 210mm x 330mm
    61      A5 Transverse           148mm x 210mm
    62      B5 (JIS) Transverse     182mm x 257mm
    63      A3 Extra                322mm x 445mm
    64      A5 Extra                174mm x 235mm
    65      B5 (ISO) Extra          201mm x 276mm
    66      A2                      420mm x 594mm
    67      A3 Transverse           297mm x 420mm
    68      A3 Extra Transverse     322mm x 445mm
    69      Dbl. Japanese Postcard  200mm x 148mm
    70      A6                      105mm x 148mm
    71
    72
    73
    74
    75      Letter Rotated          11" x 8 1/2"
    76      A3 Rotated              420mm x 297mm
    77      A4 Rotated              297mm x 210mm
    78      A5 Rotated              210mm x 148mm
    79      B4 (JIS) Rotated        364mm x 257mm
    80      B5 (JIS) Rotated        257mm x 182mm
    81      Japanese Postcard Rot.  148mm x 100mm
    82      Dbl. Jap. Postcard Rot. 148mm x 200mm
    83      A6 Rotated              148mm x 105mm
    84
    85
    86
    87
    88      B6 (JIS)                128mm x 182mm
    89      B6 (JIS) Rotated        182mm x 128mm
    90      12x11                   12" x 11"

    """
    _REC_ID = 0x00A1
    def __init__(self, paper, scaling, start_num, fit_width_to, fit_height_to,
                    options,
                    hres, vres,
                    header_margin, footer_margin,
                    num_copies):
        self._rec_data = pack('<8H2dH', paper, scaling, start_num,
                                        fit_width_to, fit_height_to, \
                                        options,
                                        hres, vres,
                                        header_margin, footer_margin,
                                        num_copies)

class NameRecord(BiffRecord):
    """
    This record is part of a Link Table. It contains the name and the token
    array of an internal defined name. Token arrays of defined names
    contain tokens with aberrant token classes.

    Record NAME, BIFF5/BIFF7:
    Offset      Size    Contents
       0          2     Option flags, see below
       2          1     Keyboard shortcut (only for command macro names, see below)
       3          1     Length of the name (character count, ln)
       4          2     Size of the formula data (sz)
       6          2     0 = Global name, otherwise index to EXTERNSHEET record (one-based)
       8          2     0 = Global name, otherwise index to sheet (one-based)
      10          1     Length of menu text (character count, lm)
      11          1     Length of description text (character count, ld)
      12          1     Length of help topic text (character count, lh)
      13          1     Length of status bar text (character count, ls)
      14         ln     Character array of the name
    14+ln        sz     Formula data (RPN token array without size field, 4)
  14+ln+sz       lm     Character array of menu text
     var.        ld     Character array of description text
     var.        lh     Character array of help topic text
     var.        ls     Character array of status bar text

    Record NAME, BIFF8:
    Offset      Size Contents
       0          2  Option flags, see below
       2          1  Keyboard shortcut (only for command macro names, see below)
       3          1  Length of the name (character count, ln)
       4          2  Size of the formula data (sz)
       6          2  Not used
       8          2  0 = Global name, otherwise index to sheet (one-based)
      10          1  Length of menu text (character count, lm)
      11          1  Length of description text (character count, ld)
      12          1  Length of help topic text (character count, lh)
      13          1  Length of status bar text (character count, ls)
      14        var. Name (Unicode string without length field, 3.4)
     var.        sz  Formula data (RPN token array without size field, 4)
    [var.]      var. (optional, only if lm > 0) Menu text (Unicode string without length field, 3.4)
    [var.]      var. (optional, only if ld > 0) Description text (Unicode string without length field, 3.4)
    [var.]      var. (optional, only if lh > 0) Help topic text (Unicode string without length field, 3.4)
    [var.]      var. (optional, only if ls > 0) Status bar text (Unicode string without length field, 3.4)
    """
    _REC_ID = 0x0018

    def __init__(self, options, keyboard_shortcut, name, sheet_index, rpn, menu_text='', desc_text='', help_text='', status_text=''):
        if type(name) == int:
            uname = chr(name)
        else:
            uname = upack1(name)[1:]
        uname_len = len(uname)

        #~ self._rec_data = pack('<HBBHHHBBBB%ds%ds' % (uname_len, len(rpn)), options, keyboard_shortcut, uname_len, len(rpn), 0x0000, sheet_index, len(menu_text), len(desc_text), len(help_text), len(status_text), uname, rpn) + menu_text + desc_text + help_text + status_text
        self._rec_data = pack('<HBBHHHBBBBB%ds%ds' % (uname_len, len(rpn)), options, keyboard_shortcut, uname_len, len(rpn), 0x0000, sheet_index, 0x00, len(menu_text), len(desc_text), len(help_text), len(status_text), uname, rpn) + menu_text + desc_text + help_text + status_text

# Excel (both 2003 and 2007) don't like refs
# split over a record boundary, which is what the
# standard BiffRecord.get method does.

# 8224 max data bytes in a BIFF record
# 6 bytes per ref
# 1370 = floor((8224 - 2) / 6.0) max refs in a record

_maxRefPerRecord = 1370

class ExternSheetRecord(BiffRecord):
    """
    In BIFF8 the record stores a list with indexes to SUPBOOK
    records (list of REF structures, 6.100). See 5.10.3 for
    details about external references in BIFF8.

    Record EXTERNSHEET, BIFF8:
    Offset          Size      Contents
       0             2        Number of following REF structures (nm)
       2           6nm        List of nm REF structures. Each REF contains the following data:
                              Offset     Size     Contents
                                 0         2      Index to SUPBOOK record
                                 2         2      Index to first SUPBOOK sheet
                                 4         2      Index to last SUPBOOK sheet
    """
    _REC_ID = 0x0017

    def __init__(self, refs):

        # do we always need this ref? or only if there are no refs?
        # (I believe that if there are no refs then we should not generate the link table - Ruben)
        #refs.insert(0, (0,0,0))

        self.refs = refs

    def get(self):
        res = []
        nrefs = len(self.refs)
        for idx in xrange(0, nrefs, _maxRefPerRecord):
            chunk = self.refs[idx:idx+_maxRefPerRecord]
            krefs = len(chunk)
            if idx: # CONTINUE record
                header = pack("<HH", 0x003C, 6 * krefs)
            else: # ExternSheetRecord
                header = pack("<HHH", self._REC_ID, 6 * krefs + 2, nrefs)
            res.append(header)
            res.extend([pack("<HHH", *r) for r in chunk])
        return ''.join(res)

class SupBookRecord(BiffRecord):
    """
    This record mainly stores the URL of an external document
    and a list of sheet names inside this document. Furthermore
    it is used to store DDE and OLE object links, or to indicate
    an internal 3D reference or an add-in function. See 5.10.3
    for details about external references in BIFF8.

    """
    _REC_ID = 0x01AE

class InternalReferenceSupBookRecord(SupBookRecord):
    """
    In each file occurs a SUPBOOK that is used for internal 3D
    references. It stores the number of sheets of the own document.

    Record SUPBOOK for 3D references, BIFF8:
    Offset         Size   Contents
      0             2     Number of sheets in this document
      2             2     01H 04H (relict of BIFF5/BIFF7, the byte string "<04H>", see 3.9.1)

    """

    def __init__(self, num_sheets):
        self._rec_data = pack('<HBB', num_sheets, 0x01, 0x04)

class XcallSupBookRecord(SupBookRecord):
    """
    Add-in function names are stored in EXTERNNAME records following this record.

    Offset  Size    Contents
    0       2       0001H
    2       2       01H 3AH (relict of BIFF5, the byte string ':', see EXTERNSHEET record, 5.41)

    """

    def __init__(self):
        self._rec_data = pack('<HBB', 1, 0x01, 0x3A)


class ExternnameRecord(BiffRecord):
    """
    Record EXTERNNAME for external names and Analysis add-in functions, BIFF5-BIFF8:
    Offset  Size    Contents
    0       2       Option flags (see below)
    2       2       0 for global names, or:
                    BIFF5: One-based index to EXTERNSHEET record containing the sheet name,
                    BIFF8: One-based index to sheet list in preceding EXTERNALBOOK record.
    4       2       Not used
    6       var.    BIFF5: Name (byte string, 8-bit string length, ?2.5.2).
                    BIFF8: Name (Unicode string, 8-bit string length, ?2.5.3).
                    See DEFINEDNAME record (?5.33) for a list of built-in names, if the built-in flag is set
                    in the option flags above.
    var.    var.    Formula data (RPN token array, ?3)

    Option flags for external names (BIFF5-BIFF8)
    Bit     Mask    Contents
    0       0001H   0 = Standard name; 1 = Built-in name
    1       0002H   0 = Manual link; 1 = Automatic link (DDE links and OLE links only)
    2       0004H   1 = Picture link (DDE links and OLE links only)
    3       0008H   1 = This is the StdDocumentName identifier (DDE links only)
    4       0010H   1 = OLE link
    14-5    7FE0H   Clipboard format of last successful update (DDE links and OLE links only)
    15      8000H   1 = Iconified picture link (BIFF8 OLE links only)
    """
    _REC_ID = 0x0023

    def __init__(self, options=0, index=0, name=None, fmla=None):
        self._rec_data = pack('<HHH', options, index, 0) + upack1(name) + fmla

########NEW FILE########
__FILENAME__ = Bitmap
# -*- coding: windows-1251 -*-

#  Portions are Copyright (C) 2005 Roman V. Kiseliov
#  Portions are Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>
#  Portions are Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)

from BIFFRecords import BiffRecord
from struct import *


def _size_col(sheet, col):
    return sheet.col_width(col)


def _size_row(sheet, row):
    return sheet.row_height(row)


def _position_image(sheet, row_start, col_start, x1, y1, width, height):
    """Calculate the vertices that define the position of the image as required by
    the OBJ record.

             +------------+------------+
             |     A      |      B     |
       +-----+------------+------------+
       |     |(x1,y1)     |            |
       |  1  |(A1)._______|______      |
       |     |    |              |     |
       |     |    |              |     |
       +-----+----|    BITMAP    |-----+
       |     |    |              |     |
       |  2  |    |______________.     |
       |     |            |        (B2)|
       |     |            |     (x2,y2)|
       +---- +------------+------------+

    Example of a bitmap that covers some of the area from cell A1 to cell B2.

    Based on the width and height of the bitmap we need to calculate 8 vars:
        col_start, row_start, col_end, row_end, x1, y1, x2, y2.
    The width and height of the cells are also variable and have to be taken into
    account.
    The values of col_start and row_start are passed in from the calling
    function. The values of col_end and row_end are calculated by subtracting
    the width and height of the bitmap from the width and height of the
    underlying cells.
    The vertices are expressed as a percentage of the underlying cell width as
    follows (rhs values are in pixels):

           x1 = X / W *1024
           y1 = Y / H *256
           x2 = (X-1) / W *1024
           y2 = (Y-1) / H *256

           Where:  X is distance from the left side of the underlying cell
                   Y is distance from the top of the underlying cell
                   W is the width of the cell
                   H is the height of the cell

    Note: the SDK incorrectly states that the height should be expressed as a
    percentage of 1024.

    col_start  - Col containing upper left corner of object
    row_start  - Row containing top left corner of object
    x1  - Distance to left side of object
    y1  - Distance to top of object
    width  - Width of image frame
    height  - Height of image frame

    """
    # Adjust start column for offsets that are greater than the col width
    while x1 >= _size_col(sheet, col_start):
        x1 -= _size_col(sheet, col_start)
        col_start += 1
    # Adjust start row for offsets that are greater than the row height
    while y1 >= _size_row(sheet, row_start):
        y1 -= _size_row(sheet, row_start)
        row_start += 1
    # Initialise end cell to the same as the start cell
    row_end = row_start   # Row containing bottom right corner of object
    col_end = col_start   # Col containing lower right corner of object
    width = width + x1 - 1
    height = height + y1 - 1
    # Subtract the underlying cell widths to find the end cell of the image
    while (width >= _size_col(sheet, col_end)):
        width -= _size_col(sheet, col_end)
        col_end += 1
    # Subtract the underlying cell heights to find the end cell of the image
    while (height >= _size_row(sheet, row_end)):
        height -= _size_row(sheet, row_end)
        row_end += 1
    # Bitmap isn't allowed to start or finish in a hidden cell, i.e. a cell
    # with zero height or width.
    if ((_size_col(sheet, col_start) == 0) or (_size_col(sheet, col_end) == 0)
            or (_size_row(sheet, row_start) == 0) or (_size_row(sheet, row_end) == 0)):
        return
    # Convert the pixel values to the percentage value expected by Excel
    x1 = int(float(x1) / _size_col(sheet, col_start) * 1024)
    y1 = int(float(y1) / _size_row(sheet, row_start) * 256)
    # Distance to right side of object
    x2 = int(float(width) / _size_col(sheet, col_end) * 1024)
    # Distance to bottom of object
    y2 = int(float(height) / _size_row(sheet, row_end) * 256)
    return (col_start, x1, row_start, y1, col_end, x2, row_end, y2)


class ObjBmpRecord(BiffRecord):
    _REC_ID = 0x005D    # Record identifier

    def __init__(self, row, col, sheet, im_data_bmp, x, y, scale_x, scale_y):
        # Scale the frame of the image.
        width = im_data_bmp.width * scale_x
        height = im_data_bmp.height * scale_y

        # Calculate the vertices of the image and write the OBJ record
        coordinates = _position_image(sheet, row, col, x, y, width, height)
        # print coordinates
        col_start, x1, row_start, y1, col_end, x2, row_end, y2 = coordinates

        """Store the OBJ record that precedes an IMDATA record. This could be generalise
        to support other Excel objects.

        """
        cObj = 0x0001      # Count of objects in file (set to 1)
        OT = 0x0008        # Object type. 8 = Picture
        id = 0x0001        # Object ID
        grbit = 0x0614     # Option flags
        colL = col_start    # Col containing upper left corner of object
        dxL = x1            # Distance from left side of cell
        rwT = row_start     # Row containing top left corner of object
        dyT = y1            # Distance from top of cell
        colR = col_end      # Col containing lower right corner of object
        dxR = x2            # Distance from right of cell
        rwB = row_end       # Row containing bottom right corner of object
        dyB = y2            # Distance from bottom of cell
        cbMacro = 0x0000    # Length of FMLA structure
        Reserved1 = 0x0000  # Reserved
        Reserved2 = 0x0000  # Reserved
        icvBack = 0x09      # Background colour
        icvFore = 0x09      # Foreground colour
        fls = 0x00          # Fill pattern
        fAuto = 0x00        # Automatic fill
        icv = 0x08          # Line colour
        lns = 0xff          # Line style
        lnw = 0x01          # Line weight
        fAutoB = 0x00       # Automatic border
        frs = 0x0000        # Frame style
        cf = 0x0009         # Image format, 9 = bitmap
        Reserved3 = 0x0000  # Reserved
        cbPictFmla = 0x0000 # Length of FMLA structure
        Reserved4 = 0x0000  # Reserved
        grbit2 = 0x0001     # Option flags
        Reserved5 = 0x0000  # Reserved

        data = pack("<L", cObj)
        data += pack("<H", OT)
        data += pack("<H", id)
        data += pack("<H", grbit)
        data += pack("<H", colL)
        data += pack("<H", dxL)
        data += pack("<H", rwT)
        data += pack("<H", dyT)
        data += pack("<H", colR)
        data += pack("<H", dxR)
        data += pack("<H", rwB)
        data += pack("<H", dyB)
        data += pack("<H", cbMacro)
        data += pack("<L", Reserved1)
        data += pack("<H", Reserved2)
        data += pack("<B", icvBack)
        data += pack("<B", icvFore)
        data += pack("<B", fls)
        data += pack("<B", fAuto)
        data += pack("<B", icv)
        data += pack("<B", lns)
        data += pack("<B", lnw)
        data += pack("<B", fAutoB)
        data += pack("<H", frs)
        data += pack("<L", cf)
        data += pack("<H", Reserved3)
        data += pack("<H", cbPictFmla)
        data += pack("<H", Reserved4)
        data += pack("<H", grbit2)
        data += pack("<L", Reserved5)

        self._rec_data = data

def _process_bitmap(bitmap):
    """Convert a 24 bit bitmap into the modified internal format used by Windows.
    This is described in BITMAPCOREHEADER and BITMAPCOREINFO structures in the
    MSDN library.

    """
    # Open file and binmode the data in case the platform needs it.
    fh = file(bitmap, "rb")
    try:
        # Slurp the file into a string.
        data = fh.read()
    finally:
        fh.close()
    # Check that the file is big enough to be a bitmap.
    if len(data) <= 0x36:
        raise Exception("bitmap doesn't contain enough data.")
    # The first 2 bytes are used to identify the bitmap.
    if (data[:2] != "BM"):
        raise Exception("bitmap doesn't appear to to be a valid bitmap image.")
    # Remove bitmap data: ID.
    data = data[2:]
    # Read and remove the bitmap size. This is more reliable than reading
    # the data size at offset 0x22.
    #
    size = unpack("<L", data[:4])[0]
    size -=  0x36   # Subtract size of bitmap header.
    size +=  0x0C   # Add size of BIFF header.
    data = data[4:]
    # Remove bitmap data: reserved, offset, header length.
    data = data[12:]
    # Read and remove the bitmap width and height. Verify the sizes.
    width, height = unpack("<LL", data[:8])
    data = data[8:]
    if (width > 0xFFFF):
        raise Exception("bitmap: largest image width supported is 65k.")
    if (height > 0xFFFF):
        raise Exception("bitmap: largest image height supported is 65k.")
    # Read and remove the bitmap planes and bpp data. Verify them.
    planes, bitcount = unpack("<HH", data[:4])
    data = data[4:]
    if (bitcount != 24):
        raise Exception("bitmap isn't a 24bit true color bitmap.")
    if (planes != 1):
        raise Exception("bitmap: only 1 plane supported in bitmap image.")
    # Read and remove the bitmap compression. Verify compression.
    compression = unpack("<L", data[:4])[0]
    data = data[4:]
    if (compression != 0):
        raise Exception("bitmap: compression not supported in bitmap image.")
    # Remove bitmap data: data size, hres, vres, colours, imp. colours.
    data = data[20:]
    # Add the BITMAPCOREHEADER data
    header = pack("<LHHHH", 0x000c, width, height, 0x01, 0x18)
    data = header + data
    return (width, height, size, data)


class ImDataBmpRecord(BiffRecord):
    _REC_ID = 0x007F

    def __init__(self, filename):
        """Insert a 24bit bitmap image in a worksheet. The main record required is
        IMDATA but it must be proceeded by a OBJ record to define its position.

        """
        BiffRecord.__init__(self)

        self.width, self.height, self.size, data = _process_bitmap(filename)
        # Write the IMDATA record to store the bitmap data
        cf = 0x09
        env = 0x01
        lcb = self.size
        self._rec_data = pack("<HHL", cf, env, lcb) + data

########NEW FILE########
__FILENAME__ = Cell
# -*- coding: windows-1252 -*-

from struct import unpack, pack
import BIFFRecords

class StrCell(object):
    __slots__ = ["rowx", "colx", "xf_idx", "sst_idx"]

    def __init__(self, rowx, colx, xf_idx, sst_idx):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx
        self.sst_idx = sst_idx

    def get_biff_data(self):
        # return BIFFRecords.LabelSSTRecord(self.rowx, self.colx, self.xf_idx, self.sst_idx).get()
        return pack('<5HL', 0x00FD, 10, self.rowx, self.colx, self.xf_idx, self.sst_idx)

class BlankCell(object):
    __slots__ = ["rowx", "colx", "xf_idx"]

    def __init__(self, rowx, colx, xf_idx):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx

    def get_biff_data(self):
        # return BIFFRecords.BlankRecord(self.rowx, self.colx, self.xf_idx).get()
        return pack('<5H', 0x0201, 6, self.rowx, self.colx, self.xf_idx)

class MulBlankCell(object):
    __slots__ = ["rowx", "colx1", "colx2", "xf_idx"]

    def __init__(self, rowx, colx1, colx2, xf_idx):
        self.rowx = rowx
        self.colx1 = colx1
        self.colx2 = colx2
        self.xf_idx = xf_idx

    def get_biff_data(self):
        return BIFFRecords.MulBlankRecord(self.rowx,
            self.colx1, self.colx2, self.xf_idx).get()

class NumberCell(object):
    __slots__ = ["rowx", "colx", "xf_idx", "number"]

    def __init__(self, rowx, colx, xf_idx, number):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx
        self.number = float(number)

    def get_encoded_data(self):
        rk_encoded = 0
        num = self.number

        # The four possible kinds of RK encoding are *not* mutually exclusive.
        # The 30-bit integer variety picks up the most.
        # In the code below, the four varieties are checked in descending order
        # of bangs per buck, or not at all.
        # SJM 2007-10-01

        if -0x20000000 <= num < 0x20000000: # fits in 30-bit *signed* int
            inum = int(num)
            if inum == num: # survives round-trip
                # print "30-bit integer RK", inum, hex(inum)
                rk_encoded = 2 | (inum << 2)
                return 1, rk_encoded

        temp = num * 100

        if -0x20000000 <= temp < 0x20000000:
            # That was step 1: the coded value will fit in
            # a 30-bit signed integer.
            itemp = int(round(temp, 0))
            # That was step 2: "itemp" is the best candidate coded value.
            # Now for step 3: simulate the decoding,
            # to check for round-trip correctness.
            if itemp / 100.0 == num:
                # print "30-bit integer RK*100", itemp, hex(itemp)
                rk_encoded = 3 | (itemp << 2)
                return 1, rk_encoded

        if 0: # Cost of extra pack+unpack not justified by tiny yield.
            packed = pack('<d', num)
            w01, w23 = unpack('<2i', packed)
            if not w01 and not(w23 & 3):
                # 34 lsb are 0
                # print "float RK", w23, hex(w23)
                return 1, w23

            packed100 = pack('<d', temp)
            w01, w23 = unpack('<2i', packed100)
            if not w01 and not(w23 & 3):
                # 34 lsb are 0
                # print "float RK*100", w23, hex(w23)
                return 1, w23 | 1

        #print "Number"
        #print
        return 0, pack('<5Hd', 0x0203, 14, self.rowx, self.colx, self.xf_idx, num)

    def get_biff_data(self):
        isRK, value = self.get_encoded_data()
        if isRK:
            return pack('<5Hi', 0x27E, 10, self.rowx, self.colx, self.xf_idx, value)
        return value # NUMBER record already packed

class BooleanCell(object):
    __slots__ = ["rowx", "colx", "xf_idx", "number"]

    def __init__(self, rowx, colx, xf_idx, number):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx
        self.number = number

    def get_biff_data(self):
        return BIFFRecords.BoolErrRecord(self.rowx,
            self.colx, self.xf_idx, self.number, 0).get()

error_code_map = {
    0x00:  0, # Intersection of two cell ranges is empty
    0x07:  7, # Division by zero
    0x0F: 15, # Wrong type of operand
    0x17: 23, # Illegal or deleted cell reference
    0x1D: 29, # Wrong function or range name
    0x24: 36, # Value range overflow
    0x2A: 42, # Argument or function not available
    '#NULL!' :  0, # Intersection of two cell ranges is empty
    '#DIV/0!':  7, # Division by zero
    '#VALUE!': 36, # Wrong type of operand
    '#REF!'  : 23, # Illegal or deleted cell reference
    '#NAME?' : 29, # Wrong function or range name
    '#NUM!'  : 36, # Value range overflow
    '#N/A!'  : 42, # Argument or function not available
}

class ErrorCell(object):
    __slots__ = ["rowx", "colx", "xf_idx", "number"]

    def __init__(self, rowx, colx, xf_idx, error_string_or_code):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx
        try:
            self.number = error_code_map[error_string_or_code]
        except KeyError:
            raise Exception('Illegal error value (%r)' % error_string_or_code)

    def get_biff_data(self):
        return BIFFRecords.BoolErrRecord(self.rowx,
            self.colx, self.xf_idx, self.number, 1).get()

class FormulaCell(object):
    __slots__ = ["rowx", "colx", "xf_idx", "frmla", "calc_flags"]

    def __init__(self, rowx, colx, xf_idx, frmla, calc_flags=0):
        self.rowx = rowx
        self.colx = colx
        self.xf_idx = xf_idx
        self.frmla = frmla
        self.calc_flags = calc_flags

    def get_biff_data(self):
        return BIFFRecords.FormulaRecord(self.rowx,
            self.colx, self.xf_idx, self.frmla.rpn(), self.calc_flags).get()

# module-level function for *internal* use by the Row module

def _get_cells_biff_data_mul(rowx, cell_items):
    # Return the BIFF data for all cell records in the row.
    # Adjacent BLANK|RK records are combined into MUL(BLANK|RK) records.
    pieces = []
    nitems = len(cell_items)
    i = 0
    while i < nitems:
        icolx, icell = cell_items[i]
        if isinstance(icell, NumberCell):
            isRK, value = icell.get_encoded_data()
            if not isRK:
                pieces.append(value) # pre-packed NUMBER record
                i += 1
                continue
            muldata = [(value, icell.xf_idx)]
            target = NumberCell
        elif isinstance(icell, BlankCell):
            muldata = [icell.xf_idx]
            target = BlankCell
        else:
            pieces.append(icell.get_biff_data())
            i += 1
            continue
        lastcolx = icolx
        j = i
        packed_record = ''
        for j in xrange(i+1, nitems):
            jcolx, jcell = cell_items[j]
            if jcolx != lastcolx + 1:
                nexti = j
                break
            if not isinstance(jcell, target):
                nexti = j
                break
            if target == NumberCell:
                isRK, value = jcell.get_encoded_data()
                if not isRK:
                    packed_record = value
                    nexti = j + 1
                    break
                muldata.append((value, jcell.xf_idx))
            else:
                muldata.append(jcell.xf_idx)
            lastcolx = jcolx
        else:
            nexti = j + 1
        if target == NumberCell:
            if lastcolx == icolx:
                # RK record
                value, xf_idx = muldata[0]
                pieces.append(pack('<5Hi', 0x027E, 10, rowx, icolx, xf_idx, value))
            else:
                # MULRK record
                nc = lastcolx - icolx + 1
                pieces.append(pack('<4H', 0x00BD, 6 * nc + 6, rowx, icolx))
                pieces.append(''.join([pack('<Hi', xf_idx, value) for value, xf_idx in muldata]))
                pieces.append(pack('<H', lastcolx))
        else:
            if lastcolx == icolx:
                # BLANK record
                xf_idx = muldata[0]
                pieces.append(pack('<5H', 0x0201, 6, rowx, icolx, xf_idx))
            else:
                # MULBLANK record
                nc = lastcolx - icolx + 1
                pieces.append(pack('<4H', 0x00BE, 2 * nc + 6, rowx, icolx))
                pieces.append(''.join([pack('<H', xf_idx) for xf_idx in muldata]))
                pieces.append(pack('<H', lastcolx))
        if packed_record:
            pieces.append(packed_record)
        i = nexti
    return ''.join(pieces)

########NEW FILE########
__FILENAME__ = Column
# -*- coding: windows-1252 -*-

from BIFFRecords import ColInfoRecord

class Column(object):
    def __init__(self, colx, parent_sheet):
        if not(isinstance(colx, int) and 0 <= colx <= 255):
            raise ValueError("column index (%r) not an int in range(256)" % colx)
        self._index = colx
        self._parent = parent_sheet
        self._parent_wb = parent_sheet.get_parent()
        self._xf_index = 0x0F

        self.width = 0x0B92
        self.hidden = 0
        self.level = 0
        self.collapse = 0

    def set_style(self, style):
        self._xf_index = self._parent_wb.add_style(style)

    def width_in_pixels(self):
        # *** Approximation ****
        return int(round(self.width * 0.0272 + 0.446, 0))

    def get_biff_record(self):
        options =  (self.hidden & 0x01) << 0
        options |= (self.level & 0x07) << 8
        options |= (self.collapse & 0x01) << 12

        return ColInfoRecord(self._index, self._index, self.width, self._xf_index, options).get()

########NEW FILE########
__FILENAME__ = CompoundDoc
# -*- coding: windows-1252 -*-

import sys
import struct

class Reader:
    def __init__(self, filename, dump = False):
        self.dump = dump
        self.STREAMS = {}

        doc = file(filename, 'rb').read()
        self.header, self.data = doc[0:512], doc[512:]
        del doc

        self.__build_header()
        self.__build_MSAT()
        self.__build_SAT()
        self.__build_directory()
        self.__build_short_sectors_data()

        if len(self.short_sectors_data) > 0:
            self.__build_SSAT()
        else:
            if self.dump and (self.total_ssat_sectors != 0 or self.ssat_start_sid != -2):
                print 'NOTE: header says that must be', self.total_ssat_sectors, 'short sectors'
                print 'NOTE: starting at', self.ssat_start_sid, 'sector'
                print 'NOTE: but file does not contains data in short sectors'
            self.ssat_start_sid = -2
            self.total_ssat_sectors = 0
            self.SSAT = [-2]

        for dentry in self.dir_entry_list[1:]:
            (did,
             sz, name,
             t, c,
             did_left, did_right, did_root,
             dentry_start_sid,
             stream_size
            ) = dentry
            stream_data = ''
            if stream_size > 0:
                if stream_size >= self.min_stream_size:
                    args = (self.data, self.SAT, dentry_start_sid, self.sect_size)
                else:
                    args = (self.short_sectors_data, self.SSAT, dentry_start_sid, self.short_sect_size)
                stream_data = self.get_stream_data(*args)

            if name != '':
                # BAD IDEA: names may be equal. NEED use full paths...
                self.STREAMS[name] = stream_data


    def __build_header(self):
        self.doc_magic             = self.header[0:8]

        if self.doc_magic != '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1':
            raise Exception, 'Not an OLE file.'

        self.file_uid              = self.header[8:24]
        self.rev_num               = self.header[24:26]
        self.ver_num               = self.header[26:28]
        self.byte_order            = self.header[28:30]
        self.log2_sect_size,       = struct.unpack('<H', self.header[30:32])
        self.log2_short_sect_size, = struct.unpack('<H', self.header[32:34])
        self.total_sat_sectors,    = struct.unpack('<L', self.header[44:48])
        self.dir_start_sid,        = struct.unpack('<l', self.header[48:52])
        self.min_stream_size,      = struct.unpack('<L', self.header[56:60])
        self.ssat_start_sid,       = struct.unpack('<l', self.header[60:64])
        self.total_ssat_sectors,   = struct.unpack('<L', self.header[64:68])
        self.msat_start_sid,       = struct.unpack('<l', self.header[68:72])
        self.total_msat_sectors,   = struct.unpack('<L', self.header[72:76])

        self.sect_size        = 1 << self.log2_sect_size
        self.short_sect_size  = 1 << self.log2_short_sect_size

        if self.dump:
            print 'file magic: '
            print_bin_data(self.doc_magic)

            print 'file uid: '
            print_bin_data(self.file_uid)

            print 'revision number: '
            print_bin_data(self.rev_num)

            print 'version number: '
            print_bin_data(self.ver_num)

            print 'byte order: '
            print_bin_data(self.byte_order)

            print 'sector size                                :', hex(self.sect_size), self.sect_size
            #print 'total sectors in file                      :', hex(self.total_sectors), self.total_sectors
            print 'short sector size                          :', hex(self.short_sect_size), self.short_sect_size
            print 'Total number of sectors used for the SAT   :', hex(self.total_sat_sectors), self.total_sat_sectors
            print 'SID of first sector of the directory stream:', hex(self.dir_start_sid), self.dir_start_sid
            print 'Minimum size of a standard stream          :', hex(self.min_stream_size), self.min_stream_size
            print 'SID of first sector of the SSAT            :', hex(self.ssat_start_sid), self.ssat_start_sid
            print 'Total number of sectors used for the SSAT  :', hex(self.total_ssat_sectors), self.total_ssat_sectors
            print 'SID of first additional sector of the MSAT :', hex(self.msat_start_sid), self.msat_start_sid
            print 'Total number of sectors used for the MSAT  :', hex(self.total_msat_sectors), self.total_msat_sectors


    def __build_MSAT(self):
        self.MSAT = list(struct.unpack('<109l', self.header[76:]))

        next = self.msat_start_sid
        while next > 0:
            msat_sector = struct.unpack('<128l', self.data[next*self.sect_size:(next+1)*self.sect_size])
            self.MSAT.extend(msat_sector[:127])
            next = msat_sector[-1]

        if self.dump:
            print 'MSAT (header part): \n', self.MSAT[:109]
            print 'additional MSAT sectors: \n', self.MSAT[109:]


    def __build_SAT(self):
        sat_stream = ''.join([self.data[i*self.sect_size:(i+1)*self.sect_size] for i in self.MSAT if i >= 0])

        sat_sids_count = len(sat_stream) >> 2
        self.SAT = struct.unpack('<%dl' % sat_sids_count, sat_stream) # SIDs tuple

        if self.dump:
            print 'SAT sid count:\n', sat_sids_count
            print 'SAT content:\n', self.SAT


    def __build_SSAT(self):
        ssat_stream = self.get_stream_data(self.data, self.SAT, self.ssat_start_sid, self.sect_size)

        ssids_count = len(ssat_stream) >> 2
        self.SSAT = struct.unpack('<%dl' % ssids_count, ssat_stream)

        if self.dump:
            print 'SSID count:', ssids_count
            print 'SSAT content:\n', self.SSAT


    def __build_directory(self):
        dir_stream = self.get_stream_data(self.data, self.SAT, self.dir_start_sid, self.sect_size)

        self.dir_entry_list = []

        i = 0
        while i < len(dir_stream):
            dentry = dir_stream[i:i+128] # 128 -- dir entry size
            i += 128

            did = len(self.dir_entry_list)
            sz, = struct.unpack('<H', dentry[64:66])
            if sz > 0 :
                name = dentry[0:sz-2].decode('utf_16_le', 'replace')
            else:
                name = u''
            t,  = struct.unpack('B', dentry[66])
            c,  = struct.unpack('B', dentry[67])
            did_left ,  = struct.unpack('<l', dentry[68:72])
            did_right ,  = struct.unpack('<l', dentry[72:76])
            did_root ,  = struct.unpack('<l', dentry[76:80])
            dentry_start_sid ,  = struct.unpack('<l', dentry[116:120])
            stream_size ,  = struct.unpack('<L', dentry[120:124])

            self.dir_entry_list.extend([(did, sz, name, t, c,
                                            did_left, did_right, did_root,
                                            dentry_start_sid, stream_size)])

        if self.dump:
            dentry_types = {
                0x00: 'Empty',
                0x01: 'User storage',
                0x02: 'User stream',
                0x03: 'LockBytes',
                0x04: 'Property',
                0x05: 'Root storage'
            }
            node_colours = {
                0x00: 'Red',
                0x01: 'Black'
            }
            print 'total directory entries:', len(self.dir_entry_list)

            for dentry in self.dir_entry_list:
                (did, sz, name, t, c,
                 did_left, did_right, did_root,
                 dentry_start_sid, stream_size) = dentry
                print 'DID', did
                print 'Size of the used area of the character buffer of the name:', sz
                print 'dir entry name:', repr(name)
                print 'type of entry:', t, dentry_types[t]
                print 'entry colour:', c, node_colours[c]
                print 'left child DID :', did_left
                print 'right child DID:', did_right
                print 'root DID       :', did_root
                print 'start SID       :', dentry_start_sid
                print 'stream size     :', stream_size
                if stream_size == 0:
                    print 'stream is empty'
                elif stream_size >= self.min_stream_size:
                    print 'stream stored as normal stream'
                else:
                    print 'stream stored as short-stream'


    def __build_short_sectors_data(self):
        (did, sz, name, t, c,
         did_left, did_right, did_root,
         dentry_start_sid, stream_size) = self.dir_entry_list[0]
        assert t == 0x05 # Short-Stream Container Stream (SSCS) resides in Root Storage
        if stream_size == 0:
            self.short_sectors_data = ''
        else:
            self.short_sectors_data = self.get_stream_data(self.data, self.SAT, dentry_start_sid, self.sect_size)


    def get_stream_data(self, data, SAT, start_sid, sect_size):
        sid = start_sid
        chunks = [(sid, sid)]
        stream_data = ''

        while SAT[sid] >= 0:
            next_in_chain = SAT[sid]
            last_chunk_start, last_chunk_finish = chunks[-1]
            if next_in_chain == last_chunk_finish + 1:
                chunks[-1] = last_chunk_start, next_in_chain
            else:
                chunks.extend([(next_in_chain, next_in_chain)])
            sid = next_in_chain
        for s, f in chunks:
            stream_data += data[s*sect_size:(f+1)*sect_size]
        #print chunks
        return stream_data


def print_bin_data(data):
    i = 0
    while i < len(data):
        j = 0
        while (i < len(data)) and (j < 16):
            c = '0x%02X' % ord(data[i])
            sys.stdout.write(c)
            sys.stdout.write(' ')
            i += 1
            j += 1
        print
    if i == 0:
        print '<NO DATA>'



# This implementation writes only 'Root Entry', 'Workbook' streams
# and 2 empty streams for aligning directory stream on sector boundary
#
# LAYOUT:
# 0         header
# 76                MSAT (1st part: 109 SID)
# 512       workbook stream
# ...       additional MSAT sectors if streams' size > about 7 Mb == (109*512 * 128)
# ...       SAT
# ...       directory stream
#
# NOTE: this layout is "ad hoc". It can be more general. RTFM

class XlsDoc:
    SECTOR_SIZE = 0x0200
    MIN_LIMIT   = 0x1000

    SID_FREE_SECTOR  = -1
    SID_END_OF_CHAIN = -2
    SID_USED_BY_SAT  = -3
    SID_USED_BY_MSAT = -4

    def __init__(self):
        #self.book_stream = ''                # padded
        self.book_stream_sect = []

        self.dir_stream = ''
        self.dir_stream_sect = []

        self.packed_SAT = ''
        self.SAT_sect = []

        self.packed_MSAT_1st = ''
        self.packed_MSAT_2nd = ''
        self.MSAT_sect_2nd = []

        self.header = ''

    def __build_directory(self): # align on sector boundary
        self.dir_stream = ''

        dentry_name      = '\x00'.join('Root Entry\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x05 # root storage
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = 1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left,
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        )

        dentry_name      = '\x00'.join('Workbook\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x02 # user stream
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = 0
        dentry_stream_sz = self.book_stream_len

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left,
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        )

        # padding
        dentry_name      = ''
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x00 # empty
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left,
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        ) * 2

    def __build_sat(self):
        # Build SAT
        book_sect_count = self.book_stream_len >> 9
        dir_sect_count  = len(self.dir_stream) >> 9

        total_sect_count     = book_sect_count + dir_sect_count
        SAT_sect_count       = 0
        MSAT_sect_count      = 0
        SAT_sect_count_limit = 109
        while total_sect_count > 128*SAT_sect_count or SAT_sect_count > SAT_sect_count_limit:
            SAT_sect_count   += 1
            total_sect_count += 1
            if SAT_sect_count > SAT_sect_count_limit:
                MSAT_sect_count      += 1
                total_sect_count     += 1
                SAT_sect_count_limit += 127


        SAT = [self.SID_FREE_SECTOR]*128*SAT_sect_count

        sect = 0
        while sect < book_sect_count - 1:
            self.book_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.book_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        while sect < book_sect_count + MSAT_sect_count:
            self.MSAT_sect_2nd.append(sect)
            SAT[sect] = self.SID_USED_BY_MSAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count:
            self.SAT_sect.append(sect)
            SAT[sect] = self.SID_USED_BY_SAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count + dir_sect_count - 1:
            self.dir_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.dir_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        self.packed_SAT = struct.pack('<%dl' % (SAT_sect_count*128), *SAT)

        MSAT_1st = [self.SID_FREE_SECTOR]*109
        for i, SAT_sect_num in zip(range(0, 109), self.SAT_sect):
            MSAT_1st[i] = SAT_sect_num
        self.packed_MSAT_1st = struct.pack('<109l', *MSAT_1st)

        MSAT_2nd = [self.SID_FREE_SECTOR]*128*MSAT_sect_count
        if MSAT_sect_count > 0:
            MSAT_2nd[- 1] = self.SID_END_OF_CHAIN

        i = 109
        msat_sect = 0
        sid_num = 0
        while i < SAT_sect_count:
            if (sid_num + 1) % 128 == 0:
                #print 'link: ',
                msat_sect += 1
                if msat_sect < len(self.MSAT_sect_2nd):
                    MSAT_2nd[sid_num] = self.MSAT_sect_2nd[msat_sect]
            else:
                #print 'sid: ',
                MSAT_2nd[sid_num] = self.SAT_sect[i]
                i += 1
            #print sid_num, MSAT_2nd[sid_num]
            sid_num += 1

        self.packed_MSAT_2nd = struct.pack('<%dl' % (MSAT_sect_count*128), *MSAT_2nd)

        #print vars()
        #print zip(range(0, sect), SAT)
        #print self.book_stream_sect
        #print self.MSAT_sect_2nd
        #print MSAT_2nd
        #print self.SAT_sect
        #print self.dir_stream_sect


    def __build_header(self):
        doc_magic             = '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1'
        file_uid              = '\x00'*16
        rev_num               = '\x3E\x00'
        ver_num               = '\x03\x00'
        byte_order            = '\xFE\xFF'
        log_sect_size         = struct.pack('<H', 9)
        log_short_sect_size   = struct.pack('<H', 6)
        not_used0             = '\x00'*10
        total_sat_sectors     = struct.pack('<L', len(self.SAT_sect))
        dir_start_sid         = struct.pack('<l', self.dir_stream_sect[0])
        not_used1             = '\x00'*4
        min_stream_size       = struct.pack('<L', 0x1000)
        ssat_start_sid        = struct.pack('<l', -2)
        total_ssat_sectors    = struct.pack('<L', 0)

        if len(self.MSAT_sect_2nd) == 0:
            msat_start_sid        = struct.pack('<l', -2)
        else:
            msat_start_sid        = struct.pack('<l', self.MSAT_sect_2nd[0])

        total_msat_sectors    = struct.pack('<L', len(self.MSAT_sect_2nd))

        self.header =       ''.join([  doc_magic,
                                        file_uid,
                                        rev_num,
                                        ver_num,
                                        byte_order,
                                        log_sect_size,
                                        log_short_sect_size,
                                        not_used0,
                                        total_sat_sectors,
                                        dir_start_sid,
                                        not_used1,
                                        min_stream_size,
                                        ssat_start_sid,
                                        total_ssat_sectors,
                                        msat_start_sid,
                                        total_msat_sectors
                                    ])


    def save(self, file_name_or_filelike_obj, stream):
        # 1. Align stream on 0x1000 boundary (and therefore on sector boundary)
        padding = '\x00' * (0x1000 - (len(stream) % 0x1000))
        self.book_stream_len = len(stream) + len(padding)

        self.__build_directory()
        self.__build_sat()
        self.__build_header()

        f = file_name_or_filelike_obj
        we_own_it = not hasattr(f, 'write')
        if we_own_it:
            f = open(file_name_or_filelike_obj, 'wb')
        f.write(self.header)
        f.write(self.packed_MSAT_1st)
        f.write(stream)
        f.write(padding)
        f.write(self.packed_MSAT_2nd)
        f.write(self.packed_SAT)
        f.write(self.dir_stream)
        if we_own_it:
            f.close()

########NEW FILE########
__FILENAME__ = ExcelFormula
# -*- coding: windows-1252 -*-

import ExcelFormulaParser, ExcelFormulaLexer
import struct
from antlr import ANTLRException


class Formula(object):
    __slots__ = ["__init__",  "__s", "__parser", "__sheet_refs", "__xcall_refs"]


    def __init__(self, s):
        try:
            self.__s = s
            lexer = ExcelFormulaLexer.Lexer(s)
            self.__parser = ExcelFormulaParser.Parser(lexer)
            self.__parser.formula()
            self.__sheet_refs = self.__parser.sheet_references
            self.__xcall_refs = self.__parser.xcall_references
        except ANTLRException, e:
            # print e
            raise ExcelFormulaParser.FormulaParseException, "can't parse formula " + s

    def get_references(self):
        return self.__sheet_refs, self.__xcall_refs

    def patch_references(self, patches):
        for offset, idx in patches:
            self.__parser.rpn = self.__parser.rpn[:offset] + struct.pack('<H', idx) + self.__parser.rpn[offset+2:]

    def text(self):
        return self.__s

    def rpn(self):
        '''
        Offset    Size    Contents
        0         2       Size of the following formula data (sz)
        2         sz      Formula data (RPN token array)
        [2+sz]    var.    (optional) Additional data for specific tokens

        '''
        return struct.pack("<H", len(self.__parser.rpn)) + self.__parser.rpn

########NEW FILE########
__FILENAME__ = ExcelFormulaLexer
# -*- coding: windows-1252 -*-

import sys
from antlr import EOF, CommonToken as Tok, TokenStream, TokenStreamException
import struct
import ExcelFormulaParser
from re import compile as recompile, match, LOCALE, UNICODE, IGNORECASE, VERBOSE


int_const_pattern = r"\d+\b"
flt_const_pattern = r"""
    (?:
        (?: \d* \. \d+ ) # .1 .12 .123 etc 9.1 etc 98.1 etc
        |
        (?: \d+ \. ) # 1. 12. 123. etc
    )
    # followed by optional exponent part
    (?: [Ee] [+-]? \d+ ) ?
    """
str_const_pattern = r'"(?:[^"]|"")*"'
#range2d_pattern   = recompile(r"\$?[A-I]?[A-Z]\$?\d+:\$?[A-I]?[A-Z]\$?\d+"
ref2d_r1c1_pattern = r"[Rr]0*[1-9][0-9]*[Cc]0*[1-9][0-9]*"
ref2d_pattern     = r"\$?[A-I]?[A-Z]\$?0*[1-9][0-9]*"
true_pattern      = r"TRUE\b"
false_pattern     = r"FALSE\b"
if_pattern        = r"IF\b"
choose_pattern    = r"CHOOSE\b"
name_pattern      = r"\w[\.\w]*"
quotename_pattern = r"'(?:[^']|'')*'" #### It's essential that this bracket be non-grouping.
ne_pattern        = r"<>"
ge_pattern        = r">="
le_pattern        = r"<="

pattern_type_tuples = (
    (flt_const_pattern, ExcelFormulaParser.NUM_CONST),
    (int_const_pattern, ExcelFormulaParser.INT_CONST),
    (str_const_pattern, ExcelFormulaParser.STR_CONST),
#    (range2d_pattern  , ExcelFormulaParser.RANGE2D),
    (ref2d_r1c1_pattern, ExcelFormulaParser.REF2D_R1C1),
    (ref2d_pattern    , ExcelFormulaParser.REF2D),
    (true_pattern     , ExcelFormulaParser.TRUE_CONST),
    (false_pattern    , ExcelFormulaParser.FALSE_CONST),
    (if_pattern       , ExcelFormulaParser.FUNC_IF),
    (choose_pattern   , ExcelFormulaParser.FUNC_CHOOSE),
    (name_pattern     , ExcelFormulaParser.NAME),
    (quotename_pattern, ExcelFormulaParser.QUOTENAME),
    (ne_pattern,        ExcelFormulaParser.NE),
    (ge_pattern,        ExcelFormulaParser.GE),
    (le_pattern,        ExcelFormulaParser.LE),
)

_re = recompile(
    '(' + ')|('.join([i[0] for i in pattern_type_tuples]) + ')',
    VERBOSE+LOCALE+IGNORECASE)

_toktype = [None] + [i[1] for i in pattern_type_tuples]
# need dummy at start because re.MatchObject.lastindex counts from 1

single_char_lookup = {
    '=': ExcelFormulaParser.EQ,
    '<': ExcelFormulaParser.LT,
    '>': ExcelFormulaParser.GT,
    '+': ExcelFormulaParser.ADD,
    '-': ExcelFormulaParser.SUB,
    '*': ExcelFormulaParser.MUL,
    '/': ExcelFormulaParser.DIV,
    ':': ExcelFormulaParser.COLON,
    ';': ExcelFormulaParser.SEMICOLON,
    ',': ExcelFormulaParser.COMMA,
    '(': ExcelFormulaParser.LP,
    ')': ExcelFormulaParser.RP,
    '&': ExcelFormulaParser.CONCAT,
    '%': ExcelFormulaParser.PERCENT,
    '^': ExcelFormulaParser.POWER,
    '!': ExcelFormulaParser.BANG,
    }

class Lexer(TokenStream):
    def __init__(self, text):
        self._text = text[:]
        self._pos = 0
        self._line = 0

    def isEOF(self):
        return len(self._text) <= self._pos

    def curr_ch(self):
        return self._text[self._pos]

    def next_ch(self, n = 1):
        self._pos += n

    def is_whitespace(self):
        return self.curr_ch() in " \t\n\r\f\v"

    def match_pattern(self):
        m = _re.match(self._text, self._pos)
        if not m:
            return None
        self._pos = m.end(0)
        return Tok(type = _toktype[m.lastindex], text = m.group(0), col = m.start(0) + 1)

    def nextToken(self):
        # skip whitespace
        while not self.isEOF() and self.is_whitespace():
            self.next_ch()
        if self.isEOF():
            return Tok(type = EOF)
        # first, try to match token with 2 or more chars
        t = self.match_pattern()
        if t:
            return t
        # second, we want 1-char tokens
        te = self.curr_ch()
        try:
            ty = single_char_lookup[te]
        except KeyError:
            raise TokenStreamException(
                "Unexpected char %r in column %u." % (self.curr_ch(), self._pos))
        self.next_ch()
        return Tok(type=ty, text=te, col=self._pos)

if __name__ == '__main__':
    try:
        for t in Lexer(""" 1.23 456 "abcd" R2C2 a1 iv65536 true false if choose a_name 'qname' <> >= <= """):
            print t
    except TokenStreamException, e:
        print "error:", e

########NEW FILE########
__FILENAME__ = ExcelFormulaParser
### $ANTLR 2.7.7 (20060930): "xlwt/excel-formula.g" -> "ExcelFormulaParser.py"$
### import antlr and other modules ..
import sys
import antlr

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False
### header action >>>
import struct
import Utils
from UnicodeUtils import upack1
from ExcelMagic import *

_RVAdelta =     {"R": 0, "V": 0x20, "A": 0x40}
_RVAdeltaRef =  {"R": 0, "V": 0x20, "A": 0x40, "D": 0x20}
_RVAdeltaArea = {"R": 0, "V": 0x20, "A": 0x40, "D": 0}


class FormulaParseException(Exception):
    """
    An exception indicating that a Formula could not be successfully parsed.
    """
### header action <<<
### preamble action>>>

### preamble action <<<

### import antlr.Token
from antlr import Token
### >>>The Known Token Types <<<
SKIP                = antlr.SKIP
INVALID_TYPE        = antlr.INVALID_TYPE
EOF_TYPE            = antlr.EOF_TYPE
EOF                 = antlr.EOF
NULL_TREE_LOOKAHEAD = antlr.NULL_TREE_LOOKAHEAD
MIN_USER_TYPE       = antlr.MIN_USER_TYPE
TRUE_CONST = 4
FALSE_CONST = 5
STR_CONST = 6
NUM_CONST = 7
INT_CONST = 8
FUNC_IF = 9
FUNC_CHOOSE = 10
NAME = 11
QUOTENAME = 12
EQ = 13
NE = 14
GT = 15
LT = 16
GE = 17
LE = 18
ADD = 19
SUB = 20
MUL = 21
DIV = 22
POWER = 23
PERCENT = 24
LP = 25
RP = 26
LB = 27
RB = 28
COLON = 29
COMMA = 30
SEMICOLON = 31
REF2D = 32
REF2D_R1C1 = 33
BANG = 34
CONCAT = 35

class Parser(antlr.LLkParser):
    ### user action >>>
    ### user action <<<

    def __init__(self, *args, **kwargs):
        antlr.LLkParser.__init__(self, *args, **kwargs)
        self.tokenNames = _tokenNames
        ### __init__ header action >>>
        self.rpn = ""
        self.sheet_references = []
        self.xcall_references = []
        ### __init__ header action <<<

    def formula(self):

        pass
        self.expr("V")

    def expr(self,
        arg_type
    ):

        pass
        self.prec0_expr(arg_type)
        while True:
            if ((self.LA(1) >= EQ and self.LA(1) <= LE)):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [EQ]:
                    pass
                    self.match(EQ)
                    op = struct.pack('B', ptgEQ)
                elif la1 and la1 in [NE]:
                    pass
                    self.match(NE)
                    op = struct.pack('B', ptgNE)
                elif la1 and la1 in [GT]:
                    pass
                    self.match(GT)
                    op = struct.pack('B', ptgGT)
                elif la1 and la1 in [LT]:
                    pass
                    self.match(LT)
                    op = struct.pack('B', ptgLT)
                elif la1 and la1 in [GE]:
                    pass
                    self.match(GE)
                    op = struct.pack('B', ptgGE)
                elif la1 and la1 in [LE]:
                    pass
                    self.match(LE)
                    op = struct.pack('B', ptgLE)
                else:
                    raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                self.prec0_expr(arg_type)
                self.rpn += op
            else:
                break


    def prec0_expr(self,
        arg_type
    ):

        pass
        self.prec1_expr(arg_type)
        while True:
            if (self.LA(1)==CONCAT):
                pass
                pass
                self.match(CONCAT)
                op = struct.pack('B', ptgConcat)
                self.prec1_expr(arg_type)
                self.rpn += op
            else:
                break


    def prec1_expr(self,
        arg_type
    ):

        pass
        self.prec2_expr(arg_type)
        while True:
            if (self.LA(1)==ADD or self.LA(1)==SUB):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [ADD]:
                    pass
                    self.match(ADD)
                    op = struct.pack('B', ptgAdd)
                elif la1 and la1 in [SUB]:
                    pass
                    self.match(SUB)
                    op = struct.pack('B', ptgSub)
                else:
                    raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                self.prec2_expr(arg_type)
                self.rpn += op;
                          # print "**prec1_expr4 %s" % arg_type
            else:
                break


    def prec2_expr(self,
        arg_type
    ):

        pass
        self.prec3_expr(arg_type)
        while True:
            if (self.LA(1)==MUL or self.LA(1)==DIV):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [MUL]:
                    pass
                    self.match(MUL)
                    op = struct.pack('B', ptgMul)
                elif la1 and la1 in [DIV]:
                    pass
                    self.match(DIV)
                    op = struct.pack('B', ptgDiv)
                else:
                    raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                self.prec3_expr(arg_type)
                self.rpn += op
            else:
                break


    def prec3_expr(self,
        arg_type
    ):

        pass
        self.prec4_expr(arg_type)
        while True:
            if (self.LA(1)==POWER):
                pass
                pass
                self.match(POWER)
                op = struct.pack('B', ptgPower)
                self.prec4_expr(arg_type)
                self.rpn += op
            else:
                break


    def prec4_expr(self,
        arg_type
    ):

        pass
        self.prec5_expr(arg_type)
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [PERCENT]:
            pass
            self.match(PERCENT)
            self.rpn += struct.pack('B', ptgPercent)
        elif la1 and la1 in [EOF,EQ,NE,GT,LT,GE,LE,ADD,SUB,MUL,DIV,POWER,RP,COMMA,SEMICOLON,CONCAT]:
            pass
        else:
            raise antlr.NoViableAltException(self.LT(1), self.getFilename())


    def prec5_expr(self,
        arg_type
    ):

        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,FUNC_IF,FUNC_CHOOSE,NAME,QUOTENAME,LP,REF2D]:
            pass
            self.primary(arg_type)
        elif la1 and la1 in [SUB]:
            pass
            self.match(SUB)
            self.primary(arg_type)
            self.rpn += struct.pack('B', ptgUminus)
        else:
            raise antlr.NoViableAltException(self.LT(1), self.getFilename())


    def primary(self,
        arg_type
    ):

        str_tok = None
        int_tok = None
        num_tok = None
        ref2d_tok = None
        ref2d1_tok = None
        ref2d2_tok = None
        ref3d_ref2d = None
        ref3d_ref2d2 = None
        name_tok = None
        func_tok = None
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST]:
            pass
            self.match(TRUE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 1)
        elif la1 and la1 in [FALSE_CONST]:
            pass
            self.match(FALSE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 0)
        elif la1 and la1 in [STR_CONST]:
            pass
            str_tok = self.LT(1)
            self.match(STR_CONST)
            self.rpn += struct.pack("B", ptgStr) + upack1(str_tok.text[1:-1].replace("\"\"", "\""))
        elif la1 and la1 in [NUM_CONST]:
            pass
            num_tok = self.LT(1)
            self.match(NUM_CONST)
            self.rpn += struct.pack("<Bd", ptgNum, float(num_tok.text))
        elif la1 and la1 in [FUNC_IF]:
            pass
            self.match(FUNC_IF)
            self.match(LP)
            self.expr("V")
            la1 = self.LA(1)
            if False:
                pass
            elif la1 and la1 in [SEMICOLON]:
                pass
                self.match(SEMICOLON)
            elif la1 and la1 in [COMMA]:
                pass
                self.match(COMMA)
            else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())

            self.rpn += struct.pack("<BBH", ptgAttr, 0x02, 0) # tAttrIf
            pos0 = len(self.rpn) - 2
            self.expr(arg_type)
            la1 = self.LA(1)
            if False:
                pass
            elif la1 and la1 in [SEMICOLON]:
                pass
                self.match(SEMICOLON)
            elif la1 and la1 in [COMMA]:
                pass
                self.match(COMMA)
            else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())

            self.rpn += struct.pack("<BBH", ptgAttr, 0x08, 0) # tAttrSkip
            pos1 = len(self.rpn) - 2
            self.rpn = self.rpn[:pos0] + struct.pack("<H", pos1-pos0) + self.rpn[pos0+2:]
            self.expr(arg_type)
            self.match(RP)
            self.rpn += struct.pack("<BBH", ptgAttr, 0x08, 3) # tAttrSkip
            self.rpn += struct.pack("<BBH", ptgFuncVarR, 3, 1) # 3 = nargs, 1 = IF func
            pos2 = len(self.rpn)
            self.rpn = self.rpn[:pos1] + struct.pack("<H", pos2-(pos1+2)-1) + self.rpn[pos1+2:]
        elif la1 and la1 in [FUNC_CHOOSE]:
            pass
            self.match(FUNC_CHOOSE)
            arg_type = "R"
            rpn_chunks = []
            self.match(LP)
            self.expr("V")
            rpn_start = len(self.rpn)
            ref_markers = [len(self.sheet_references)]
            while True:
                if (self.LA(1)==COMMA or self.LA(1)==SEMICOLON):
                    pass
                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [SEMICOLON]:
                        pass
                        self.match(SEMICOLON)
                    elif la1 and la1 in [COMMA]:
                        pass
                        self.match(COMMA)
                    else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                    mark = len(self.rpn)
                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,FUNC_IF,FUNC_CHOOSE,NAME,QUOTENAME,SUB,LP,REF2D]:
                        pass
                        self.expr(arg_type)
                    elif la1 and la1 in [RP,COMMA,SEMICOLON]:
                        pass
                        self.rpn += struct.pack("B", ptgMissArg)
                    else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                    rpn_chunks.append(self.rpn[mark:])
                    ref_markers.append(len(self.sheet_references))
                else:
                    break

            self.match(RP)
            self.rpn = self.rpn[:rpn_start]
            nc = len(rpn_chunks)
            chunklens = [len(chunk) for chunk in rpn_chunks]
            skiplens = [0] * nc
            skiplens[-1] = 3
            for ic in xrange(nc-1, 0, -1):
                skiplens[ic-1] = skiplens[ic] + chunklens[ic] + 4
            jump_pos = [2 * nc + 2]
            for ic in xrange(nc):
                jump_pos.append(jump_pos[-1] + chunklens[ic] + 4)
            chunk_shift = 2 * nc + 6 # size of tAttrChoose
            for ic in xrange(nc):
                for refx in xrange(ref_markers[ic], ref_markers[ic+1]):
                    ref = self.sheet_references[refx]
                    self.sheet_references[refx] = (ref[0], ref[1], ref[2] + chunk_shift)
                chunk_shift += 4 # size of tAttrSkip
            choose_rpn = []
            choose_rpn.append(struct.pack("<BBH", ptgAttr, 0x04, nc)) # 0x04 is tAttrChoose
            choose_rpn.append(struct.pack("<%dH" % (nc+1), *jump_pos))
            for ic in xrange(nc):
                choose_rpn.append(rpn_chunks[ic])
                choose_rpn.append(struct.pack("<BBH", ptgAttr, 0x08, skiplens[ic])) # 0x08 is tAttrSkip
            choose_rpn.append(struct.pack("<BBH", ptgFuncVarV, nc+1, 100)) # 100 is CHOOSE fn
            self.rpn += "".join(choose_rpn)
        elif la1 and la1 in [LP]:
            pass
            self.match(LP)
            self.expr(arg_type)
            self.match(RP)
            self.rpn += struct.pack("B", ptgParen)
        else:
            if (self.LA(1)==INT_CONST) and (_tokenSet_0.member(self.LA(2))):
                pass
                int_tok = self.LT(1)
                self.match(INT_CONST)
                # print "**int_const", int_tok.text
                int_value = int(int_tok.text)
                if int_value <= 65535:
                    self.rpn += struct.pack("<BH", ptgInt, int_value)
                else:
                    self.rpn += struct.pack("<Bd", ptgNum, float(int_value))
            elif (self.LA(1)==REF2D) and (_tokenSet_0.member(self.LA(2))):
                pass
                ref2d_tok = self.LT(1)
                self.match(REF2D)
                # print "**ref2d %s %s" % (ref2d_tok.text, arg_type)
                r, c = Utils.cell_to_packed_rowcol(ref2d_tok.text)
                ptg = ptgRefR + _RVAdeltaRef[arg_type]
                self.rpn += struct.pack("<B2H", ptg, r, c)
            elif (self.LA(1)==REF2D) and (self.LA(2)==COLON):
                pass
                ref2d1_tok = self.LT(1)
                self.match(REF2D)
                self.match(COLON)
                ref2d2_tok = self.LT(1)
                self.match(REF2D)
                r1, c1 = Utils.cell_to_packed_rowcol(ref2d1_tok.text)
                r2, c2 = Utils.cell_to_packed_rowcol(ref2d2_tok.text)
                ptg = ptgAreaR + _RVAdeltaArea[arg_type]
                self.rpn += struct.pack("<B4H", ptg, r1, r2, c1, c2)
            elif (self.LA(1)==INT_CONST or self.LA(1)==NAME or self.LA(1)==QUOTENAME) and (self.LA(2)==COLON or self.LA(2)==BANG):
                pass
                sheet1=self.sheet()
                sheet2 = sheet1
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [COLON]:
                    pass
                    self.match(COLON)
                    sheet2=self.sheet()
                elif la1 and la1 in [BANG]:
                    pass
                else:
                    raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                self.match(BANG)
                ref3d_ref2d = self.LT(1)
                self.match(REF2D)
                ptg = ptgRef3dR + _RVAdeltaRef[arg_type]
                rpn_ref2d = ""
                r1, c1 = Utils.cell_to_packed_rowcol(ref3d_ref2d.text)
                rpn_ref2d = struct.pack("<3H", 0x0000, r1, c1)
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [COLON]:
                    pass
                    self.match(COLON)
                    ref3d_ref2d2 = self.LT(1)
                    self.match(REF2D)
                    ptg = ptgArea3dR + _RVAdeltaArea[arg_type]
                    r2, c2 = Utils.cell_to_packed_rowcol(ref3d_ref2d2.text)
                    rpn_ref2d = struct.pack("<5H", 0x0000, r1, r2, c1, c2)
                elif la1 and la1 in [EOF,EQ,NE,GT,LT,GE,LE,ADD,SUB,MUL,DIV,POWER,PERCENT,RP,COMMA,SEMICOLON,CONCAT]:
                    pass
                else:
                    raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                self.rpn += struct.pack("<B", ptg)
                self.sheet_references.append((sheet1, sheet2, len(self.rpn)))
                self.rpn += rpn_ref2d
            elif (self.LA(1)==NAME) and (_tokenSet_0.member(self.LA(2))):
                pass
                name_tok = self.LT(1)
                self.match(NAME)
                raise Exception("[formula] found unexpected NAME token (%r)" % name_tok.txt)
                # #### TODO: handle references to defined names here
            elif (self.LA(1)==NAME) and (self.LA(2)==LP):
                pass
                func_tok = self.LT(1)
                self.match(NAME)
                func_toku = func_tok.text.upper()
                if func_toku in all_funcs_by_name:
                    (opcode,
                    min_argc,
                    max_argc,
                    func_type,
                    arg_type_str) = all_funcs_by_name[func_toku]
                    arg_type_list = list(arg_type_str)
                else:
                    raise Exception("[formula] unknown function (%s)" % func_tok.text)
                # print "**func_tok1 %s %s" % (func_toku, func_type)
                xcall = opcode < 0
                if xcall:
                    # The name of the add-in function is passed as the 1st arg
                    # of the hidden XCALL function
                    self.xcall_references.append((func_toku, len(self.rpn) + 1))
                    self.rpn += struct.pack("<BHHH",
                        ptgNameXR,
                        0xadde, # ##PATCHME## index to REF entry in EXTERNSHEET record
                        0xefbe, # ##PATCHME## one-based index to EXTERNNAME record
                        0x0000) # unused
                self.match(LP)
                arg_count=self.expr_list(arg_type_list, min_argc, max_argc)
                self.match(RP)
                if arg_count > max_argc or arg_count < min_argc:
                    raise Exception, "%d parameters for function: %s" % (arg_count, func_tok.text)
                if xcall:
                    func_ptg = ptgFuncVarR + _RVAdelta[func_type]
                    self.rpn += struct.pack("<2BH", func_ptg, arg_count + 1, 255) # 255 is magic XCALL function
                elif min_argc == max_argc:
                    func_ptg = ptgFuncR + _RVAdelta[func_type]
                    self.rpn += struct.pack("<BH", func_ptg, opcode)
                elif arg_count == 1 and func_tok.text.upper() == "SUM":
                    self.rpn += struct.pack("<BBH", ptgAttr, 0x10, 0) # tAttrSum
                else:
                    func_ptg = ptgFuncVarR + _RVAdelta[func_type]
                    self.rpn += struct.pack("<2BH", func_ptg, arg_count, opcode)
            else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())


    def sheet(self):
        ref = None

        sheet_ref_name = None
        sheet_ref_int = None
        sheet_ref_quote = None
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [NAME]:
            pass
            sheet_ref_name = self.LT(1)
            self.match(NAME)
            ref = sheet_ref_name.text
        elif la1 and la1 in [INT_CONST]:
            pass
            sheet_ref_int = self.LT(1)
            self.match(INT_CONST)
            ref = sheet_ref_int.text
        elif la1 and la1 in [QUOTENAME]:
            pass
            sheet_ref_quote = self.LT(1)
            self.match(QUOTENAME)
            ref = sheet_ref_quote.text[1:-1].replace("''", "'")
        else:
            raise antlr.NoViableAltException(self.LT(1), self.getFilename())

        return ref

    def expr_list(self,
        arg_type_list, min_argc, max_argc
    ):
        arg_cnt = None

        arg_cnt = 0
        arg_type = arg_type_list[arg_cnt]
        # print "**expr_list1[%d] req=%s" % (arg_cnt, arg_type)
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,FUNC_IF,FUNC_CHOOSE,NAME,QUOTENAME,SUB,LP,REF2D]:
            pass
            self.expr(arg_type)
            arg_cnt += 1
            while True:
                if (self.LA(1)==COMMA or self.LA(1)==SEMICOLON):
                    pass
                    if arg_cnt < len(arg_type_list):
                        arg_type = arg_type_list[arg_cnt]
                    else:
                        arg_type = arg_type_list[-1]
                    if arg_type == "+":
                        arg_type = arg_type_list[-2]
                    # print "**expr_list2[%d] req=%s" % (arg_cnt, arg_type)
                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [SEMICOLON]:
                        pass
                        self.match(SEMICOLON)
                    elif la1 and la1 in [COMMA]:
                        pass
                        self.match(COMMA)
                    else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,FUNC_IF,FUNC_CHOOSE,NAME,QUOTENAME,SUB,LP,REF2D]:
                        pass
                        self.expr(arg_type)
                    elif la1 and la1 in [RP,COMMA,SEMICOLON]:
                        pass
                        self.rpn += struct.pack("B", ptgMissArg)
                    else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())

                    arg_cnt += 1
                else:
                    break

        elif la1 and la1 in [RP]:
            pass
        else:
            raise antlr.NoViableAltException(self.LT(1), self.getFilename())

        return arg_cnt


_tokenNames = [
    "<0>",
    "EOF",
    "<2>",
    "NULL_TREE_LOOKAHEAD",
    "TRUE_CONST",
    "FALSE_CONST",
    "STR_CONST",
    "NUM_CONST",
    "INT_CONST",
    "FUNC_IF",
    "FUNC_CHOOSE",
    "NAME",
    "QUOTENAME",
    "EQ",
    "NE",
    "GT",
    "LT",
    "GE",
    "LE",
    "ADD",
    "SUB",
    "MUL",
    "DIV",
    "POWER",
    "PERCENT",
    "LP",
    "RP",
    "LB",
    "RB",
    "COLON",
    "COMMA",
    "SEMICOLON",
    "REF2D",
    "REF2D_R1C1",
    "BANG",
    "CONCAT"
]


### generate bit set
def mk_tokenSet_0():
    ### var1
    data = [ 37681618946L, 0L]
    return data
_tokenSet_0 = antlr.BitSet(mk_tokenSet_0())

########NEW FILE########
__FILENAME__ = ExcelMagic
# -*- coding: ascii -*-
"""
lots of Excel Magic Numbers
"""

# Boundaries BIFF8+

MAX_ROW = 65536
MAX_COL = 256


biff_records = {
    0x0000: "DIMENSIONS",
    0x0001: "BLANK",
    0x0002: "INTEGER",
    0x0003: "NUMBER",
    0x0004: "LABEL",
    0x0005: "BOOLERR",
    0x0006: "FORMULA",
    0x0007: "STRING",
    0x0008: "ROW",
    0x0009: "BOF",
    0x000A: "EOF",
    0x000B: "INDEX",
    0x000C: "CALCCOUNT",
    0x000D: "CALCMODE",
    0x000E: "PRECISION",
    0x000F: "REFMODE",
    0x0010: "DELTA",
    0x0011: "ITERATION",
    0x0012: "PROTECT",
    0x0013: "PASSWORD",
    0x0014: "HEADER",
    0x0015: "FOOTER",
    0x0016: "EXTERNCOUNT",
    0x0017: "EXTERNSHEET",
    0x0018: "NAME",
    0x0019: "WINDOWPROTECT",
    0x001A: "VERTICALPAGEBREAKS",
    0x001B: "HORIZONTALPAGEBREAKS",
    0x001C: "NOTE",
    0x001D: "SELECTION",
    0x001E: "FORMAT",
    0x001F: "FORMATCOUNT",
    0x0020: "COLUMNDEFAULT",
    0x0021: "ARRAY",
    0x0022: "1904",
    0x0023: "EXTERNNAME",
    0x0024: "COLWIDTH",
    0x0025: "DEFAULTROWHEIGHT",
    0x0026: "LEFTMARGIN",
    0x0027: "RIGHTMARGIN",
    0x0028: "TOPMARGIN",
    0x0029: "BOTTOMMARGIN",
    0x002A: "PRINTHEADERS",
    0x002B: "PRINTGRIDLINES",
    0x002F: "FILEPASS",
    0x0031: "FONT",
    0x0036: "TABLE",
    0x003C: "CONTINUE",
    0x003D: "WINDOW1",
    0x003E: "WINDOW2",
    0x0040: "BACKUP",
    0x0041: "PANE",
    0x0042: "CODEPAGE",
    0x0043: "XF",
    0x0044: "IXFE",
    0x0045: "EFONT",
    0x004D: "PLS",
    0x0050: "DCON",
    0x0051: "DCONREF",
    0x0053: "DCONNAME",
    0x0055: "DEFCOLWIDTH",
    0x0056: "BUILTINFMTCNT",
    0x0059: "XCT",
    0x005A: "CRN",
    0x005B: "FILESHARING",
    0x005C: "WRITEACCESS",
    0x005D: "OBJ",
    0x005E: "UNCALCED",
    0x005F: "SAFERECALC",
    0x0060: "TEMPLATE",
    0x0063: "OBJPROTECT",
    0x007D: "COLINFO",
    0x007E: "RK",
    0x007F: "IMDATA",
    0x0080: "GUTS",
    0x0081: "WSBOOL",
    0x0082: "GRIDSET",
    0x0083: "HCENTER",
    0x0084: "VCENTER",
    0x0085: "BOUNDSHEET",
    0x0086: "WRITEPROT",
    0x0087: "ADDIN",
    0x0088: "EDG",
    0x0089: "PUB",
    0x008C: "COUNTRY",
    0x008D: "HIDEOBJ",
    0x008E: "BUNDLESOFFSET",
    0x008F: "BUNDLEHEADER",
    0x0090: "SORT",
    0x0091: "SUB",
    0x0092: "PALETTE",
    0x0093: "STYLE",
    0x0094: "LHRECORD",
    0x0095: "LHNGRAPH",
    0x0096: "SOUND",
    0x0098: "LPR",
    0x0099: "STANDARDWIDTH",
    0x009A: "FNGROUPNAME",
    0x009B: "FILTERMODE",
    0x009C: "FNGROUPCOUNT",
    0x009D: "AUTOFILTERINFO",
    0x009E: "AUTOFILTER",
    0x00A0: "SCL",
    0x00A1: "SETUP",
    0x00A9: "COORDLIST",
    0x00AB: "GCW",
    0x00AE: "SCENMAN",
    0x00AF: "SCENARIO",
    0x00B0: "SXVIEW",
    0x00B1: "SXVD",
    0x00B2: "SXVI",
    0x00B4: "SXIVD",
    0x00B5: "SXLI",
    0x00B6: "SXPI",
    0x00B8: "DOCROUTE",
    0x00B9: "RECIPNAME",
    0x00BC: "SHRFMLA",
    0x00BD: "MULRK",
    0x00BE: "MULBLANK",
    0x00C1: "MMS",
    0x00C2: "ADDMENU",
    0x00C3: "DELMENU",
    0x00C5: "SXDI",
    0x00C6: "SXDB",
    0x00C7: "SXFIELD",
    0x00C8: "SXINDEXLIST",
    0x00C9: "SXDOUBLE",
    0x00CD: "SXSTRING",
    0x00CE: "SXDATETIME",
    0x00D0: "SXTBL",
    0x00D1: "SXTBRGITEM",
    0x00D2: "SXTBPG",
    0x00D3: "OBPROJ",
    0x00D5: "SXIDSTM",
    0x00D6: "RSTRING",
    0x00D7: "DBCELL",
    0x00DA: "BOOKBOOL",
    0x00DC: "SXEXT|PARAMQRY",
    0x00DD: "SCENPROTECT",
    0x00DE: "OLESIZE",
    0x00DF: "UDDESC",
    0x00E0: "XF",
    0x00E1: "INTERFACEHDR",
    0x00E2: "INTERFACEEND",
    0x00E3: "SXVS",
    0x00E5: "MERGEDCELLS",
    0x00E9: "BITMAP",
    0x00EB: "MSODRAWINGGROUP",
    0x00EC: "MSODRAWING",
    0x00ED: "MSODRAWINGSELECTION",
    0x00F0: "SXRULE",
    0x00F1: "SXEX",
    0x00F2: "SXFILT",
    0x00F6: "SXNAME",
    0x00F7: "SXSELECT",
    0x00F8: "SXPAIR",
    0x00F9: "SXFMLA",
    0x00FB: "SXFORMAT",
    0x00FC: "SST",
    0x00FD: "LABELSST",
    0x00FF: "EXTSST",
    0x0100: "SXVDEX",
    0x0103: "SXFORMULA",
    0x0122: "SXDBEX",
    0x0137: "CHTRINSERT",
    0x0138: "CHTRINFO",
    0x013B: "CHTRCELLCONTENT",
    0x013D: "TABID",
    0x0140: "CHTRMOVERANGE",
    0x014D: "CHTRINSERTTAB",
    0x015F: "LABELRANGES",
    0x0160: "USESELFS",
    0x0161: "DSF",
    0x0162: "XL5MODIFY",
    0x0196: "CHTRHEADER",
    0x01A9: "USERBVIEW",
    0x01AA: "USERSVIEWBEGIN",
    0x01AB: "USERSVIEWEND",
    0x01AD: "QSI",
    0x01AE: "SUPBOOK",
    0x01AF: "PROT4REV",
    0x01B0: "CONDFMT",
    0x01B1: "CF",
    0x01B2: "DVAL",
    0x01B5: "DCONBIN",
    0x01B6: "TXO",
    0x01B7: "REFRESHALL",
    0x01B8: "HLINK",
    0x01BA: "CODENAME",
    0x01BB: "SXFDBTYPE",
    0x01BC: "PROT4REVPASS",
    0x01BE: "DV",
    0x01C0: "XL9FILE",
    0x01C1: "RECALCID",
    0x0200: "DIMENSIONS",
    0x0201: "BLANK",
    0x0203: "NUMBER",
    0x0204: "LABEL",
    0x0205: "BOOLERR",
    0x0206: "FORMULA",
    0x0207: "STRING",
    0x0208: "ROW",
    0x0209: "BOF",
    0x020B: "INDEX",
    0x0218: "NAME",
    0x0221: "ARRAY",
    0x0223: "EXTERNNAME",
    0x0225: "DEFAULTROWHEIGHT",
    0x0231: "FONT",
    0x0236: "TABLE",
    0x023E: "WINDOW2",
    0x0243: "XF",
    0x027E: "RK",
    0x0293: "STYLE",
    0x0406: "FORMULA",
    0x0409: "BOF",
    0x041E: "FORMAT",
    0x0443: "XF",
    0x04BC: "SHRFMLA",
    0x0800: "SCREENTIP",
    0x0803: "WEBQRYSETTINGS",
    0x0804: "WEBQRYTABLES",
    0x0809: "BOF",
    0x0862: "SHEETLAYOUT",
    0x0867: "SHEETPROTECTION",
    0x1001: "UNITS",
    0x1002: "ChartChart",
    0x1003: "ChartSeries",
    0x1006: "ChartDataformat",
    0x1007: "ChartLineformat",
    0x1009: "ChartMarkerformat",
    0x100A: "ChartAreaformat",
    0x100B: "ChartPieformat",
    0x100C: "ChartAttachedlabel",
    0x100D: "ChartSeriestext",
    0x1014: "ChartChartformat",
    0x1015: "ChartLegend",
    0x1016: "ChartSerieslist",
    0x1017: "ChartBar",
    0x1018: "ChartLine",
    0x1019: "ChartPie",
    0x101A: "ChartArea",
    0x101B: "ChartScatter",
    0x101C: "ChartChartline",
    0x101D: "ChartAxis",
    0x101E: "ChartTick",
    0x101F: "ChartValuerange",
    0x1020: "ChartCatserrange",
    0x1021: "ChartAxislineformat",
    0x1022: "ChartFormatlink",
    0x1024: "ChartDefaulttext",
    0x1025: "ChartText",
    0x1026: "ChartFontx",
    0x1027: "ChartObjectLink",
    0x1032: "ChartFrame",
    0x1033: "BEGIN",
    0x1034: "END",
    0x1035: "ChartPlotarea",
    0x103A: "Chart3D",
    0x103C: "ChartPicf",
    0x103D: "ChartDropbar",
    0x103E: "ChartRadar",
    0x103F: "ChartSurface",
    0x1040: "ChartRadararea",
    0x1041: "ChartAxisparent",
    0x1043: "ChartLegendxn",
    0x1044: "ChartShtprops",
    0x1045: "ChartSertocrt",
    0x1046: "ChartAxesused",
    0x1048: "ChartSbaseref",
    0x104A: "ChartSerparent",
    0x104B: "ChartSerauxtrend",
    0x104E: "ChartIfmt",
    0x104F: "ChartPos",
    0x1050: "ChartAlruns",
    0x1051: "ChartAI",
    0x105B: "ChartSerauxerrbar",
    0x105D: "ChartSerfmt",
    0x105F: "Chart3DDataFormat",
    0x1060: "ChartFbi",
    0x1061: "ChartBoppop",
    0x1062: "ChartAxcext",
    0x1063: "ChartDat",
    0x1064: "ChartPlotgrowth",
    0x1065: "ChartSiindex",
    0x1066: "ChartGelframe",
    0x1067: "ChartBoppcustom",
    0xFFFF: ""
}


all_funcs_by_name = {
    # Includes Analysis ToolPak aka ATP aka add-in aka xcall functions,
    # distinguished by -ve opcode.
    # name: (opcode, min # args, max # args, func return type, func arg types)
    # + in func arg types means more of the same.
    'ABS'         : ( 24, 1,  1, 'V', 'V'),
    'ACCRINT'     : ( -1, 6,  7, 'V', 'VVVVVVV'),
    'ACCRINTM'    : ( -1, 3,  5, 'V', 'VVVVV'),
    'ACOS'        : ( 99, 1,  1, 'V', 'V'),
    'ACOSH'       : (233, 1,  1, 'V', 'V'),
    'ADDRESS'     : (219, 2,  5, 'V', 'VVVVV'),
    'AMORDEGRC'   : ( -1, 7,  7, 'V', 'VVVVVVV'),
    'AMORLINC'    : ( -1, 7,  7, 'V', 'VVVVVVV'),
    'AND'         : ( 36, 1, 30, 'V', 'D+'),
    'AREAS'       : ( 75, 1,  1, 'V', 'R'),
    'ASC'         : (214, 1,  1, 'V', 'V'),
    'ASIN'        : ( 98, 1,  1, 'V', 'V'),
    'ASINH'       : (232, 1,  1, 'V', 'V'),
    'ATAN'        : ( 18, 1,  1, 'V', 'V'),
    'ATAN2'       : ( 97, 2,  2, 'V', 'VV'),
    'ATANH'       : (234, 1,  1, 'V', 'V'),
    'AVEDEV'      : (269, 1, 30, 'V', 'D+'),
    'AVERAGE'     : (  5, 1, 30, 'V', 'D+'),
    'AVERAGEA'    : (361, 1, 30, 'V', 'D+'),
    'BAHTTEXT'    : (368, 1,  1, 'V', 'V'),
    'BESSELI'     : ( -1, 2,  2, 'V', 'VV'),
    'BESSELJ'     : ( -1, 2,  2, 'V', 'VV'),
    'BESSELK'     : ( -1, 2,  2, 'V', 'VV'),
    'BESSELY'     : ( -1, 2,  2, 'V', 'VV'),
    'BETADIST'    : (270, 3,  5, 'V', 'VVVVV'),
    'BETAINV'     : (272, 3,  5, 'V', 'VVVVV'),
    'BIN2DEC'     : ( -1, 1,  1, 'V', 'V'),
    'BIN2HEX'     : ( -1, 1,  2, 'V', 'VV'),
    'BIN2OCT'     : ( -1, 1,  2, 'V', 'VV'),
    'BINOMDIST'   : (273, 4,  4, 'V', 'VVVV'),
    'CEILING'     : (288, 2,  2, 'V', 'VV'),
    'CELL'        : (125, 1,  2, 'V', 'VR'),
    'CHAR'        : (111, 1,  1, 'V', 'V'),
    'CHIDIST'     : (274, 2,  2, 'V', 'VV'),
    'CHIINV'      : (275, 2,  2, 'V', 'VV'),
    'CHITEST'     : (306, 2,  2, 'V', 'AA'),
    'CHOOSE'      : (100, 2, 30, 'R', 'VR+'),
    'CLEAN'       : (162, 1,  1, 'V', 'V'),
    'CODE'        : (121, 1,  1, 'V', 'V'),
    'COLUMN'      : (  9, 0,  1, 'V', 'R'),
    'COLUMNS'     : ( 77, 1,  1, 'V', 'R'),
    'COMBIN'      : (276, 2,  2, 'V', 'VV'),
    'COMPLEX'     : ( -1, 2,  3, 'V', 'VVV'),
    'CONCATENATE' : (336, 1, 30, 'V', 'V+'),
    'CONFIDENCE'  : (277, 3,  3, 'V', 'VVV'),
    'CONVERT'     : ( -1, 3,  3, 'V', 'VVV'),
    'CORREL'      : (307, 2,  2, 'V', 'AA'),
    'COS'         : ( 16, 1,  1, 'V', 'V'),
    'COSH'        : (230, 1,  1, 'V', 'V'),
    'COUNT'       : (  0, 1, 30, 'V', 'D+'),
    'COUNTA'      : (169, 1, 30, 'V', 'D+'),
    'COUNTBLANK'  : (347, 1,  1, 'V', 'R'),
    'COUNTIF'     : (346, 2,  2, 'V', 'RV'),
    'COUPDAYBS'   : ( -1, 3,  5, 'V', 'VVVVV'),
    'COUPDAYS'    : ( -1, 3,  5, 'V', 'VVVVV'),
    'COUPDAYSNC'  : ( -1, 3,  5, 'V', 'VVVVV'),
    'COUPNCD'     : ( -1, 3,  5, 'V', 'VVVVV'),
    'COUPNUM'     : ( -1, 3,  5, 'V', 'VVVVV'),
    'COUPPCD'     : ( -1, 3,  5, 'V', 'VVVVV'),
    'COVAR'       : (308, 2,  2, 'V', 'AA'),
    'CRITBINOM'   : (278, 3,  3, 'V', 'VVV'),
    'CUMIPMT'     : ( -1, 6,  6, 'V', 'VVVVVV'),
    'CUMPRINC'    : ( -1, 6,  6, 'V', 'VVVVVV'),
    'DATE'        : ( 65, 3,  3, 'V', 'VVV'),
    'DATEDIF'     : (351, 3,  3, 'V', 'VVV'),
    'DATEVALUE'   : (140, 1,  1, 'V', 'V'),
    'DAVERAGE'    : ( 42, 3,  3, 'V', 'RRR'),
    'DAY'         : ( 67, 1,  1, 'V', 'V'),
    'DAYS360'     : (220, 2,  3, 'V', 'VVV'),
    'DB'          : (247, 4,  5, 'V', 'VVVVV'),
    'DBCS'        : (215, 1,  1, 'V', 'V'),
    'DCOUNT'      : ( 40, 3,  3, 'V', 'RRR'),
    'DCOUNTA'     : (199, 3,  3, 'V', 'RRR'),
    'DDB'         : (144, 4,  5, 'V', 'VVVVV'),
    'DEC2BIN'     : ( -1, 1,  2, 'V', 'VV'),
    'DEC2HEX'     : ( -1, 1,  2, 'V', 'VV'),
    'DEC2OCT'     : ( -1, 1,  2, 'V', 'VV'),
    'DEGREES'     : (343, 1,  1, 'V', 'V'),
    'DELTA'       : ( -1, 1,  2, 'V', 'VV'),
    'DEVSQ'       : (318, 1, 30, 'V', 'D+'),
    'DGET'        : (235, 3,  3, 'V', 'RRR'),
    'DISC'        : ( -1, 4,  5, 'V', 'VVVVV'),
    'DMAX'        : ( 44, 3,  3, 'V', 'RRR'),
    'DMIN'        : ( 43, 3,  3, 'V', 'RRR'),
    'DOLLAR'      : ( 13, 1,  2, 'V', 'VV'),
    'DOLLARDE'    : ( -1, 2,  2, 'V', 'VV'),
    'DOLLARFR'    : ( -1, 2,  2, 'V', 'VV'),
    'DPRODUCT'    : (189, 3,  3, 'V', 'RRR'),
    'DSTDEV'      : ( 45, 3,  3, 'V', 'RRR'),
    'DSTDEVP'     : (195, 3,  3, 'V', 'RRR'),
    'DSUM'        : ( 41, 3,  3, 'V', 'RRR'),
    'DURATION'    : ( -1, 5,  6, 'V', 'VVVVVV'),
    'DVAR'        : ( 47, 3,  3, 'V', 'RRR'),
    'DVARP'       : (196, 3,  3, 'V', 'RRR'),
    'EDATE'       : ( -1, 2,  2, 'V', 'VV'),
    'EFFECT'      : ( -1, 2,  2, 'V', 'VV'),
    'EOMONTH'     : ( -1, 1,  2, 'V', 'VV'),
    'ERF'         : ( -1, 1,  2, 'V', 'VV'),
    'ERFC'        : ( -1, 1,  1, 'V', 'V'),
    'ERROR.TYPE'  : (261, 1,  1, 'V', 'V'),
    'EVEN'        : (279, 1,  1, 'V', 'V'),
    'EXACT'       : (117, 2,  2, 'V', 'VV'),
    'EXP'         : ( 21, 1,  1, 'V', 'V'),
    'EXPONDIST'   : (280, 3,  3, 'V', 'VVV'),
    'FACT'        : (184, 1,  1, 'V', 'V'),
    'FACTDOUBLE'  : ( -1, 1,  1, 'V', 'V'),
    'FALSE'       : ( 35, 0,  0, 'V', '-'),
    'FDIST'       : (281, 3,  3, 'V', 'VVV'),
    'FIND'        : (124, 2,  3, 'V', 'VVV'),
    'FINDB'       : (205, 2,  3, 'V', 'VVV'),
    'FINV'        : (282, 3,  3, 'V', 'VVV'),
    'FISHER'      : (283, 1,  1, 'V', 'V'),
    'FISHERINV'   : (284, 1,  1, 'V', 'V'),
    'FIXED'       : ( 14, 2,  3, 'V', 'VVV'),
    'FLOOR'       : (285, 2,  2, 'V', 'VV'),
    'FORECAST'    : (309, 3,  3, 'V', 'VAA'),
    'FREQUENCY'   : (252, 2,  2, 'A', 'RR'),
    'FTEST'       : (310, 2,  2, 'V', 'AA'),
    'FV'          : ( 57, 3,  5, 'V', 'VVVVV'),
    'FVSCHEDULE'  : ( -1, 2,  2, 'V', 'VA'),
    'GAMMADIST'   : (286, 4,  4, 'V', 'VVVV'),
    'GAMMAINV'    : (287, 3,  3, 'V', 'VVV'),
    'GAMMALN'     : (271, 1,  1, 'V', 'V'),
    'GCD'         : ( -1, 1, 29, 'V', 'V+'),
    'GEOMEAN'     : (319, 1, 30, 'V', 'D+'),
    'GESTEP'      : ( -1, 1,  2, 'V', 'VV'),
    'GETPIVOTDATA': (358, 2, 30, 'A', 'VAV+'),
    'GROWTH'      : ( 52, 1,  4, 'A', 'RRRV'),
    'HARMEAN'     : (320, 1, 30, 'V', 'D+'),
    'HEX2BIN'     : ( -1, 1,  2, 'V', 'VV'),
    'HEX2DEC'     : ( -1, 1,  1, 'V', 'V'),
    'HEX2OCT'     : ( -1, 1,  2, 'V', 'VV'),
    'HLOOKUP'     : (101, 3,  4, 'V', 'VRRV'),
    'HOUR'        : ( 71, 1,  1, 'V', 'V'),
    'HYPERLINK'   : (359, 1,  2, 'V', 'VV'),
    'HYPGEOMDIST' : (289, 4,  4, 'V', 'VVVV'),
    'IF'          : (  1, 2,  3, 'R', 'VRR'),
    'IMABS'       : ( -1, 1,  1, 'V', 'V'),
    'IMAGINARY'   : ( -1, 1,  1, 'V', 'V'),
    'IMARGUMENT'  : ( -1, 1,  1, 'V', 'V'),
    'IMCONJUGATE' : ( -1, 1,  1, 'V', 'V'),
    'IMCOS'       : ( -1, 1,  1, 'V', 'V'),
    'IMDIV'       : ( -1, 2,  2, 'V', 'VV'),
    'IMEXP'       : ( -1, 1,  1, 'V', 'V'),
    'IMLN'        : ( -1, 1,  1, 'V', 'V'),
    'IMLOG10'     : ( -1, 1,  1, 'V', 'V'),
    'IMLOG2'      : ( -1, 1,  1, 'V', 'V'),
    'IMPOWER'     : ( -1, 2,  2, 'V', 'VV'),
    'IMPRODUCT'   : ( -1, 2,  2, 'V', 'VV'),
    'IMREAL'      : ( -1, 1,  1, 'V', 'V'),
    'IMSIN'       : ( -1, 1,  1, 'V', 'V'),
    'IMSQRT'      : ( -1, 1,  1, 'V', 'V'),
    'IMSUB'       : ( -1, 2,  2, 'V', 'VV'),
    'IMSUM'       : ( -1, 1, 29, 'V', 'V+'),
    'INDEX'       : ( 29, 2,  4, 'R', 'RVVV'),
    'INDIRECT'    : (148, 1,  2, 'R', 'VV'),
    'INFO'        : (244, 1,  1, 'V', 'V'),
    'INT'         : ( 25, 1,  1, 'V', 'V'),
    'INTERCEPT'   : (311, 2,  2, 'V', 'AA'),
    'INTRATE'     : ( -1, 4,  5, 'V', 'VVVVV'),
    'IPMT'        : (167, 4,  6, 'V', 'VVVVVV'),
    'IRR'         : ( 62, 1,  2, 'V', 'RV'),
    'ISBLANK'     : (129, 1,  1, 'V', 'V'),
    'ISERR'       : (126, 1,  1, 'V', 'V'),
    'ISERROR'     : (  3, 1,  1, 'V', 'V'),
    'ISEVEN'      : ( -1, 1,  1, 'V', 'V'),
    'ISLOGICAL'   : (198, 1,  1, 'V', 'V'),
    'ISNA'        : (  2, 1,  1, 'V', 'V'),
    'ISNONTEXT'   : (190, 1,  1, 'V', 'V'),
    'ISNUMBER'    : (128, 1,  1, 'V', 'V'),
    'ISODD'       : ( -1, 1,  1, 'V', 'V'),
    'ISPMT'       : (350, 4,  4, 'V', 'VVVV'),
    'ISREF'       : (105, 1,  1, 'V', 'R'),
    'ISTEXT'      : (127, 1,  1, 'V', 'V'),
    'KURT'        : (322, 1, 30, 'V', 'D+'),
    'LARGE'       : (325, 2,  2, 'V', 'RV'),
    'LCM'         : ( -1, 1, 29, 'V', 'V+'),
    'LEFT'        : (115, 1,  2, 'V', 'VV'),
    'LEFTB'       : (208, 1,  2, 'V', 'VV'),
    'LEN'         : ( 32, 1,  1, 'V', 'V'),
    'LENB'        : (211, 1,  1, 'V', 'V'),
    'LINEST'      : ( 49, 1,  4, 'A', 'RRVV'),
    'LN'          : ( 22, 1,  1, 'V', 'V'),
    'LOG'         : (109, 1,  2, 'V', 'VV'),
    'LOG10'       : ( 23, 1,  1, 'V', 'V'),
    'LOGEST'      : ( 51, 1,  4, 'A', 'RRVV'),
    'LOGINV'      : (291, 3,  3, 'V', 'VVV'),
    'LOGNORMDIST' : (290, 3,  3, 'V', 'VVV'),
    'LOOKUP'      : ( 28, 2,  3, 'V', 'VRR'),
    'LOWER'       : (112, 1,  1, 'V', 'V'),
    'MATCH'       : ( 64, 2,  3, 'V', 'VRR'),
    'MAX'         : (  7, 1, 30, 'V', 'D+'),
    'MAXA'        : (362, 1, 30, 'V', 'D+'),
    'MDETERM'     : (163, 1,  1, 'V', 'A'),
    'MDURATION'   : ( -1, 5,  6, 'V', 'VVVVVV'),
    'MEDIAN'      : (227, 1, 30, 'V', 'D+'),
    'MID'         : ( 31, 3,  3, 'V', 'VVV'),
    'MIDB'        : (210, 3,  3, 'V', 'VVV'),
    'MIN'         : (  6, 1, 30, 'V', 'D+'),
    'MINA'        : (363, 1, 30, 'V', 'D+'),
    'MINUTE'      : ( 72, 1,  1, 'V', 'V'),
    'MINVERSE'    : (164, 1,  1, 'A', 'A'),
    'MIRR'        : ( 61, 3,  3, 'V', 'RVV'),
    'MMULT'       : (165, 2,  2, 'A', 'AA'),
    'MOD'         : ( 39, 2,  2, 'V', 'VV'),
    'MODE'        : (330, 1, 30, 'V', 'A+'), ################ weird #################
    'MONTH'       : ( 68, 1,  1, 'V', 'V'),
    'MROUND'      : ( -1, 2,  2, 'V', 'VV'),
    'MULTINOMIAL' : ( -1, 1, 29, 'V', 'V+'),
    'N'           : (131, 1,  1, 'V', 'R'),
    'NA'          : ( 10, 0,  0, 'V', '-'),
    'NEGBINOMDIST': (292, 3,  3, 'V', 'VVV'),
    'NETWORKDAYS' : ( -1, 2,  3, 'V', 'VVR'),
    'NOMINAL'     : ( -1, 2,  2, 'V', 'VV'),
    'NORMDIST'    : (293, 4,  4, 'V', 'VVVV'),
    'NORMINV'     : (295, 3,  3, 'V', 'VVV'),
    'NORMSDIST'   : (294, 1,  1, 'V', 'V'),
    'NORMSINV'    : (296, 1,  1, 'V', 'V'),
    'NOT'         : ( 38, 1,  1, 'V', 'V'),
    'NOW'         : ( 74, 0,  0, 'V', '-'),
    'NPER'        : ( 58, 3,  5, 'V', 'VVVVV'),
    'NPV'         : ( 11, 2, 30, 'V', 'VD+'),
    'OCT2BIN'     : ( -1, 1,  2, 'V', 'VV'),
    'OCT2DEC'     : ( -1, 1,  1, 'V', 'V'),
    'OCT2HEX'     : ( -1, 1,  2, 'V', 'VV'),
    'ODD'         : (298, 1,  1, 'V', 'V'),
    'ODDFPRICE'   : ( -1, 9,  9, 'V', 'VVVVVVVVV'),
    'ODDFYIELD'   : ( -1, 9,  9, 'V', 'VVVVVVVVV'),
    'ODDLPRICE'   : ( -1, 8,  8, 'V', 'VVVVVVVV'),
    'ODDLYIELD'   : ( -1, 8,  8, 'V', 'VVVVVVVV'),
    'OFFSET'      : ( 78, 3,  5, 'R', 'RVVVV'),
    'OR'          : ( 37, 1, 30, 'V', 'D+'),
    'PEARSON'     : (312, 2,  2, 'V', 'AA'),
    'PERCENTILE'  : (328, 2,  2, 'V', 'RV'),
    'PERCENTRANK' : (329, 2,  3, 'V', 'RVV'),
    'PERMUT'      : (299, 2,  2, 'V', 'VV'),
    'PHONETIC'    : (360, 1,  1, 'V', 'R'),
    'PI'          : ( 19, 0,  0, 'V', '-'),
    'PMT'         : ( 59, 3,  5, 'V', 'VVVVV'),
    'POISSON'     : (300, 3,  3, 'V', 'VVV'),
    'POWER'       : (337, 2,  2, 'V', 'VV'),
    'PPMT'        : (168, 4,  6, 'V', 'VVVVVV'),
    'PRICE'       : ( -1, 6,  7, 'V', 'VVVVVVV'),
    'PRICEDISC'   : ( -1, 4,  5, 'V', 'VVVVV'),
    'PRICEMAT'    : ( -1, 5,  6, 'V', 'VVVVVV'),
    'PROB'        : (317, 3,  4, 'V', 'AAVV'),
    'PRODUCT'     : (183, 1, 30, 'V', 'D+'),
    'PROPER'      : (114, 1,  1, 'V', 'V'),
    'PV'          : ( 56, 3,  5, 'V', 'VVVVV'),
    'QUARTILE'    : (327, 2,  2, 'V', 'RV'),
    'QUOTIENT'    : ( -1, 2,  2, 'V', 'VV'),
    'RADIANS'     : (342, 1,  1, 'V', 'V'),
    'RAND'        : ( 63, 0,  0, 'V', '-'),
    'RANDBETWEEN' : ( -1, 2,  2, 'V', 'VV'),
    'RANK'        : (216, 2,  3, 'V', 'VRV'),
    'RATE'        : ( 60, 3,  6, 'V', 'VVVVVV'),
    'RECEIVED'    : ( -1, 4,  5, 'V', 'VVVVV'),
    'REPLACE'     : (119, 4,  4, 'V', 'VVVV'),
    'REPLACEB'    : (207, 4,  4, 'V', 'VVVV'),
    'REPT'        : ( 30, 2,  2, 'V', 'VV'),
    'RIGHT'       : (116, 1,  2, 'V', 'VV'),
    'RIGHTB'      : (209, 1,  2, 'V', 'VV'),
    'ROMAN'       : (354, 1,  2, 'V', 'VV'),
    'ROUND'       : ( 27, 2,  2, 'V', 'VV'),
    'ROUNDDOWN'   : (213, 2,  2, 'V', 'VV'),
    'ROUNDUP'     : (212, 2,  2, 'V', 'VV'),
    'ROW'         : (  8, 0,  1, 'V', 'R'),
    'ROWS'        : ( 76, 1,  1, 'V', 'R'),
    'RSQ'         : (313, 2,  2, 'V', 'AA'),
    'RTD'         : (379, 3, 30, 'A', 'VVV+'),
    'SEARCH'      : ( 82, 2,  3, 'V', 'VVV'),
    'SEARCHB'     : (206, 2,  3, 'V', 'VVV'),
    'SECOND'      : ( 73, 1,  1, 'V', 'V'),
    'SERIESSUM'   : ( -1, 4,  4, 'V', 'VVVA'),
    'SIGN'        : ( 26, 1,  1, 'V', 'V'),
    'SIN'         : ( 15, 1,  1, 'V', 'V'),
    'SINH'        : (229, 1,  1, 'V', 'V'),
    'SKEW'        : (323, 1, 30, 'V', 'D+'),
    'SLN'         : (142, 3,  3, 'V', 'VVV'),
    'SLOPE'       : (315, 2,  2, 'V', 'AA'),
    'SMALL'       : (326, 2,  2, 'V', 'RV'),
    'SQRT'        : ( 20, 1,  1, 'V', 'V'),
    'SQRTPI'      : ( -1, 1,  1, 'V', 'V'),
    'STANDARDIZE' : (297, 3,  3, 'V', 'VVV'),
    'STDEV'       : ( 12, 1, 30, 'V', 'D+'),
    'STDEVA'      : (366, 1, 30, 'V', 'D+'),
    'STDEVP'      : (193, 1, 30, 'V', 'D+'),
    'STDEVPA'     : (364, 1, 30, 'V', 'D+'),
    'STEYX'       : (314, 2,  2, 'V', 'AA'),
    'SUBSTITUTE'  : (120, 3,  4, 'V', 'VVVV'),
    'SUBTOTAL'    : (344, 2, 30, 'V', 'VR+'),
    'SUM'         : (  4, 1, 30, 'V', 'D+'),
    'SUMIF'       : (345, 2,  3, 'V', 'RVR'),
    'SUMPRODUCT'  : (228, 1, 30, 'V', 'A+'),
    'SUMSQ'       : (321, 1, 30, 'V', 'D+'),
    'SUMX2MY2'    : (304, 2,  2, 'V', 'AA'),
    'SUMX2PY2'    : (305, 2,  2, 'V', 'AA'),
    'SUMXMY2'     : (303, 2,  2, 'V', 'AA'),
    'SYD'         : (143, 4,  4, 'V', 'VVVV'),
    'T'           : (130, 1,  1, 'V', 'R'),
    'TAN'         : ( 17, 1,  1, 'V', 'V'),
    'TANH'        : (231, 1,  1, 'V', 'V'),
    'TBILLEQ'     : ( -1, 3,  3, 'V', 'VVV'),
    'TBILLPRICE'  : ( -1, 3,  3, 'V', 'VVV'),
    'TBILLYIELD'  : ( -1, 3,  3, 'V', 'VVV'),
    'TDIST'       : (301, 3,  3, 'V', 'VVV'),
    'TEXT'        : ( 48, 2,  2, 'V', 'VV'),
    'TIME'        : ( 66, 3,  3, 'V', 'VVV'),
    'TIMEVALUE'   : (141, 1,  1, 'V', 'V'),
    'TINV'        : (332, 2,  2, 'V', 'VV'),
    'TODAY'       : (221, 0,  0, 'V', '-'),
    'TRANSPOSE'   : ( 83, 1,  1, 'A', 'A'),
    'TREND'       : ( 50, 1,  4, 'A', 'RRRV'),
    'TRIM'        : (118, 1,  1, 'V', 'V'),
    'TRIMMEAN'    : (331, 2,  2, 'V', 'RV'),
    'TRUE'        : ( 34, 0,  0, 'V', '-'),
    'TRUNC'       : (197, 1,  2, 'V', 'VV'),
    'TTEST'       : (316, 4,  4, 'V', 'AAVV'),
    'TYPE'        : ( 86, 1,  1, 'V', 'V'),
    'UPPER'       : (113, 1,  1, 'V', 'V'),
    'USDOLLAR'    : (204, 1,  2, 'V', 'VV'),
    'VALUE'       : ( 33, 1,  1, 'V', 'V'),
    'VAR'         : ( 46, 1, 30, 'V', 'D+'),
    'VARA'        : (367, 1, 30, 'V', 'D+'),
    'VARP'        : (194, 1, 30, 'V', 'D+'),
    'VARPA'       : (365, 1, 30, 'V', 'D+'),
    'VDB'         : (222, 5,  7, 'V', 'VVVVVVV'),
    'VLOOKUP'     : (102, 3,  4, 'V', 'VRRV'),
    'WEEKDAY'     : ( 70, 1,  2, 'V', 'VV'),
    'WEEKNUM'     : ( -1, 1,  2, 'V', 'VV'),
    'WEIBULL'     : (302, 4,  4, 'V', 'VVVV'),
    'WORKDAY'     : ( -1, 2,  3, 'V', 'VVR'),
    'XIRR'        : ( -1, 2,  3, 'V', 'AAV'),
    'XNPV'        : ( -1, 3,  3, 'V', 'VAA'),
    'YEAR'        : ( 69, 1,  1, 'V', 'V'),
    'YEARFRAC'    : ( -1, 2,  3, 'V', 'VVV'),
    'YIELD'       : ( -1, 6,  7, 'V', 'VVVVVVV'),
    'YIELDDISC'   : ( -1, 4,  5, 'V', 'VVVVV'),
    'YIELDMAT'    : ( -1, 5,  6, 'V', 'VVVVVV'),
    'ZTEST'       : (324, 2,  3, 'V', 'RVV'),
    }

# Formulas Parse things

ptgExp          = 0x01
ptgTbl          = 0x02
ptgAdd          = 0x03
ptgSub          = 0x04
ptgMul          = 0x05
ptgDiv          = 0x06
ptgPower        = 0x07
ptgConcat       = 0x08
ptgLT           = 0x09
ptgLE           = 0x0a
ptgEQ           = 0x0b
ptgGE           = 0x0c
ptgGT           = 0x0d
ptgNE           = 0x0e
ptgIsect        = 0x0f
ptgUnion        = 0x10
ptgRange        = 0x11
ptgUplus        = 0x12
ptgUminus       = 0x13
ptgPercent      = 0x14
ptgParen        = 0x15
ptgMissArg      = 0x16
ptgStr          = 0x17
ptgExtend       = 0x18
ptgAttr         = 0x19
ptgSheet        = 0x1a
ptgEndSheet     = 0x1b
ptgErr          = 0x1c
ptgBool         = 0x1d
ptgInt          = 0x1e
ptgNum          = 0x1f

ptgArrayR       = 0x20
ptgFuncR        = 0x21
ptgFuncVarR     = 0x22
ptgNameR        = 0x23
ptgRefR         = 0x24
ptgAreaR        = 0x25
ptgMemAreaR     = 0x26
ptgMemErrR      = 0x27
ptgMemNoMemR    = 0x28
ptgMemFuncR     = 0x29
ptgRefErrR      = 0x2a
ptgAreaErrR     = 0x2b
ptgRefNR        = 0x2c
ptgAreaNR       = 0x2d
ptgMemAreaNR    = 0x2e
ptgMemNoMemNR   = 0x2f
ptgNameXR       = 0x39
ptgRef3dR       = 0x3a
ptgArea3dR      = 0x3b
ptgRefErr3dR    = 0x3c
ptgAreaErr3dR   = 0x3d

ptgArrayV       = 0x40
ptgFuncV        = 0x41
ptgFuncVarV     = 0x42
ptgNameV        = 0x43
ptgRefV         = 0x44
ptgAreaV        = 0x45
ptgMemAreaV     = 0x46
ptgMemErrV      = 0x47
ptgMemNoMemV    = 0x48
ptgMemFuncV     = 0x49
ptgRefErrV      = 0x4a
ptgAreaErrV     = 0x4b
ptgRefNV        = 0x4c
ptgAreaNV       = 0x4d
ptgMemAreaNV    = 0x4e
ptgMemNoMemNV   = 0x4f
ptgFuncCEV      = 0x58
ptgNameXV       = 0x59
ptgRef3dV       = 0x5a
ptgArea3dV      = 0x5b
ptgRefErr3dV    = 0x5c
ptgAreaErr3dV   = 0x5d

ptgArrayA       = 0x60
ptgFuncA        = 0x61
ptgFuncVarA     = 0x62
ptgNameA        = 0x63
ptgRefA         = 0x64
ptgAreaA        = 0x65
ptgMemAreaA     = 0x66
ptgMemErrA      = 0x67
ptgMemNoMemA    = 0x68
ptgMemFuncA     = 0x69
ptgRefErrA      = 0x6a
ptgAreaErrA     = 0x6b
ptgRefNA        = 0x6c
ptgAreaNA       = 0x6d
ptgMemAreaNA    = 0x6e
ptgMemNoMemNA   = 0x6f
ptgFuncCEA      = 0x78
ptgNameXA       = 0x79
ptgRef3dA       = 0x7a
ptgArea3dA      = 0x7b
ptgRefErr3dA    = 0x7c
ptgAreaErr3dA   = 0x7d


PtgNames = {
    ptgExp         : "ptgExp",
    ptgTbl         : "ptgTbl",
    ptgAdd         : "ptgAdd",
    ptgSub         : "ptgSub",
    ptgMul         : "ptgMul",
    ptgDiv         : "ptgDiv",
    ptgPower       : "ptgPower",
    ptgConcat      : "ptgConcat",
    ptgLT          : "ptgLT",
    ptgLE          : "ptgLE",
    ptgEQ          : "ptgEQ",
    ptgGE          : "ptgGE",
    ptgGT          : "ptgGT",
    ptgNE          : "ptgNE",
    ptgIsect       : "ptgIsect",
    ptgUnion       : "ptgUnion",
    ptgRange       : "ptgRange",
    ptgUplus       : "ptgUplus",
    ptgUminus      : "ptgUminus",
    ptgPercent     : "ptgPercent",
    ptgParen       : "ptgParen",
    ptgMissArg     : "ptgMissArg",
    ptgStr         : "ptgStr",
    ptgExtend      : "ptgExtend",
    ptgAttr        : "ptgAttr",
    ptgSheet       : "ptgSheet",
    ptgEndSheet    : "ptgEndSheet",
    ptgErr         : "ptgErr",
    ptgBool        : "ptgBool",
    ptgInt         : "ptgInt",
    ptgNum         : "ptgNum",
    ptgArrayR      : "ptgArrayR",
    ptgFuncR       : "ptgFuncR",
    ptgFuncVarR    : "ptgFuncVarR",
    ptgNameR       : "ptgNameR",
    ptgRefR        : "ptgRefR",
    ptgAreaR       : "ptgAreaR",
    ptgMemAreaR    : "ptgMemAreaR",
    ptgMemErrR     : "ptgMemErrR",
    ptgMemNoMemR   : "ptgMemNoMemR",
    ptgMemFuncR    : "ptgMemFuncR",
    ptgRefErrR     : "ptgRefErrR",
    ptgAreaErrR    : "ptgAreaErrR",
    ptgRefNR       : "ptgRefNR",
    ptgAreaNR      : "ptgAreaNR",
    ptgMemAreaNR   : "ptgMemAreaNR",
    ptgMemNoMemNR  : "ptgMemNoMemNR",
    ptgNameXR      : "ptgNameXR",
    ptgRef3dR      : "ptgRef3dR",
    ptgArea3dR     : "ptgArea3dR",
    ptgRefErr3dR   : "ptgRefErr3dR",
    ptgAreaErr3dR  : "ptgAreaErr3dR",
    ptgArrayV      : "ptgArrayV",
    ptgFuncV       : "ptgFuncV",
    ptgFuncVarV    : "ptgFuncVarV",
    ptgNameV       : "ptgNameV",
    ptgRefV        : "ptgRefV",
    ptgAreaV       : "ptgAreaV",
    ptgMemAreaV    : "ptgMemAreaV",
    ptgMemErrV     : "ptgMemErrV",
    ptgMemNoMemV   : "ptgMemNoMemV",
    ptgMemFuncV    : "ptgMemFuncV",
    ptgRefErrV     : "ptgRefErrV",
    ptgAreaErrV    : "ptgAreaErrV",
    ptgRefNV       : "ptgRefNV",
    ptgAreaNV      : "ptgAreaNV",
    ptgMemAreaNV   : "ptgMemAreaNV",
    ptgMemNoMemNV  : "ptgMemNoMemNV",
    ptgFuncCEV     : "ptgFuncCEV",
    ptgNameXV      : "ptgNameXV",
    ptgRef3dV      : "ptgRef3dV",
    ptgArea3dV     : "ptgArea3dV",
    ptgRefErr3dV   : "ptgRefErr3dV",
    ptgAreaErr3dV  : "ptgAreaErr3dV",
    ptgArrayA      : "ptgArrayA",
    ptgFuncA       : "ptgFuncA",
    ptgFuncVarA    : "ptgFuncVarA",
    ptgNameA       : "ptgNameA",
    ptgRefA        : "ptgRefA",
    ptgAreaA       : "ptgAreaA",
    ptgMemAreaA    : "ptgMemAreaA",
    ptgMemErrA     : "ptgMemErrA",
    ptgMemNoMemA   : "ptgMemNoMemA",
    ptgMemFuncA    : "ptgMemFuncA",
    ptgRefErrA     : "ptgRefErrA",
    ptgAreaErrA    : "ptgAreaErrA",
    ptgRefNA       : "ptgRefNA",
    ptgAreaNA      : "ptgAreaNA",
    ptgMemAreaNA   : "ptgMemAreaNA",
    ptgMemNoMemNA  : "ptgMemNoMemNA",
    ptgFuncCEA     : "ptgFuncCEA",
    ptgNameXA      : "ptgNameXA",
    ptgRef3dA      : "ptgRef3dA",
    ptgArea3dA     : "ptgArea3dA",
    ptgRefErr3dA   : "ptgRefErr3dA",
    ptgAreaErr3dA  : "ptgAreaErr3dA"
}


error_msg_by_code = {
    0x00: u"#NULL!",  # intersection of two cell ranges is empty
    0x07: u"#DIV/0!", # division by zero
    0x0F: u"#VALUE!", # wrong type of operand
    0x17: u"#REF!",   # illegal or deleted cell reference
    0x1D: u"#NAME?",  # wrong function or range name
    0x24: u"#NUM!",   # value range overflow
    0x2A: u"#N/A!"    # argument or function not available
}

########NEW FILE########
__FILENAME__ = Formatting
#!/usr/bin/env python
'''
The  XF  record is able to store explicit cell formatting attributes or the
attributes  of  a cell style. Explicit formatting includes the reference to
a  cell  style  XF  record. This allows to extend a defined cell style with
some  explicit  attributes.  The  formatting  attributes  are  divided into
6 groups:

Group           Attributes
-------------------------------------
Number format   Number format index (index to FORMAT record)
Font            Font index (index to FONT record)
Alignment       Horizontal and vertical alignment, text wrap, indentation,
                orientation/rotation, text direction
Border          Border line styles and colours
Background      Background area style and colours
Protection      Cell locked, formula hidden

For  each  group  a flag in the cell XF record specifies whether to use the
attributes  contained  in  that  XF  record  or  in  the  referenced  style
XF  record. In style XF records, these flags specify whether the attributes
will  overwrite  explicit  cell  formatting  when  the  style is applied to
a  cell. Changing a cell style (without applying this style to a cell) will
change  all  cells which already use that style and do not contain explicit
cell  attributes for the changed style attributes. If a cell XF record does
not  contain  explicit  attributes  in a group (if the attribute group flag
is not set), it repeats the attributes of its style XF record.

'''

import BIFFRecords

class Font(object):

    ESCAPEMENT_NONE         = 0x00
    ESCAPEMENT_SUPERSCRIPT  = 0x01
    ESCAPEMENT_SUBSCRIPT    = 0x02

    UNDERLINE_NONE          = 0x00
    UNDERLINE_SINGLE        = 0x01
    UNDERLINE_SINGLE_ACC    = 0x21
    UNDERLINE_DOUBLE        = 0x02
    UNDERLINE_DOUBLE_ACC    = 0x22

    FAMILY_NONE         = 0x00
    FAMILY_ROMAN        = 0x01
    FAMILY_SWISS        = 0x02
    FAMILY_MODERN       = 0x03
    FAMILY_SCRIPT       = 0x04
    FAMILY_DECORATIVE   = 0x05

    CHARSET_ANSI_LATIN          = 0x00
    CHARSET_SYS_DEFAULT         = 0x01
    CHARSET_SYMBOL              = 0x02
    CHARSET_APPLE_ROMAN         = 0x4D
    CHARSET_ANSI_JAP_SHIFT_JIS  = 0x80
    CHARSET_ANSI_KOR_HANGUL     = 0x81
    CHARSET_ANSI_KOR_JOHAB      = 0x82
    CHARSET_ANSI_CHINESE_GBK    = 0x86
    CHARSET_ANSI_CHINESE_BIG5   = 0x88
    CHARSET_ANSI_GREEK          = 0xA1
    CHARSET_ANSI_TURKISH        = 0xA2
    CHARSET_ANSI_VIETNAMESE     = 0xA3
    CHARSET_ANSI_HEBREW         = 0xB1
    CHARSET_ANSI_ARABIC         = 0xB2
    CHARSET_ANSI_BALTIC         = 0xBA
    CHARSET_ANSI_CYRILLIC       = 0xCC
    CHARSET_ANSI_THAI           = 0xDE
    CHARSET_ANSI_LATIN_II       = 0xEE
    CHARSET_OEM_LATIN_I         = 0xFF

    def __init__(self):
        # twip = 1/20 of a point = 1/1440 of a inch
        # usually resolution == 96 pixels per 1 inch
        # (rarely 120 pixels per 1 inch or another one)

        self.height = 0x00C8 # 200: this is font with height 10 points
        self.italic = False
        self.struck_out = False
        self.outline = False
        self.shadow = False
        self.colour_index = 0x7FFF
        self.bold = False
        self._weight = 0x0190 # 0x02BC gives bold font
        self.escapement = self.ESCAPEMENT_NONE
        self.underline = self.UNDERLINE_NONE
        self.family = self.FAMILY_NONE
        self.charset = self.CHARSET_SYS_DEFAULT
        self.name = 'Arial'

    def get_biff_record(self):
        height = self.height

        options = 0x00
        if self.bold:
            options |= 0x01
            self._weight = 0x02BC
        if self.italic:
            options |= 0x02
        if self.underline != self.UNDERLINE_NONE:
            options |= 0x04
        if self.struck_out:
            options |= 0x08
        if self.outline:
            options |= 0x010
        if self.shadow:
            options |= 0x020

        colour_index = self.colour_index
        weight = self._weight
        escapement = self.escapement
        underline = self.underline
        family = self.family
        charset = self.charset
        name = self.name

        return BIFFRecords.FontRecord(height, options, colour_index, weight, escapement,
                    underline, family, charset,
                    name)

    def _search_key(self):
        return (
            self.height,
            self.italic,
            self.struck_out,
            self.outline,
            self.shadow,
            self.colour_index,
            self.bold,
            self._weight,
            self.escapement,
            self.underline,
            self.family,
            self.charset,
            self.name,
            )

class Alignment(object):
    HORZ_GENERAL                = 0x00
    HORZ_LEFT                   = 0x01
    HORZ_CENTER                 = 0x02
    HORZ_RIGHT                  = 0x03
    HORZ_FILLED                 = 0x04
    HORZ_JUSTIFIED              = 0x05 # BIFF4-BIFF8X
    HORZ_CENTER_ACROSS_SEL      = 0x06 # Centred across selection (BIFF4-BIFF8X)
    HORZ_DISTRIBUTED            = 0x07 # Distributed (BIFF8X)

    VERT_TOP                    = 0x00
    VERT_CENTER                 = 0x01
    VERT_BOTTOM                 = 0x02
    VERT_JUSTIFIED              = 0x03 # Justified (BIFF5-BIFF8X)
    VERT_DISTRIBUTED            = 0x04 # Distributed (BIFF8X)

    DIRECTION_GENERAL           = 0x00 # BIFF8X
    DIRECTION_LR                = 0x01
    DIRECTION_RL                = 0x02

    ORIENTATION_NOT_ROTATED     = 0x00
    ORIENTATION_STACKED         = 0x01
    ORIENTATION_90_CC           = 0x02
    ORIENTATION_90_CW           = 0x03

    ROTATION_0_ANGLE            = 0x00
    ROTATION_STACKED            = 0xFF

    WRAP_AT_RIGHT               = 0x01
    NOT_WRAP_AT_RIGHT           = 0x00

    SHRINK_TO_FIT               = 0x01
    NOT_SHRINK_TO_FIT           = 0x00

    def __init__(self):
        self.horz = self.HORZ_GENERAL
        self.vert = self.VERT_BOTTOM
        self.dire = self.DIRECTION_GENERAL
        self.orie = self.ORIENTATION_NOT_ROTATED
        self.rota = self.ROTATION_0_ANGLE
        self.wrap = self.NOT_WRAP_AT_RIGHT
        self.shri = self.NOT_SHRINK_TO_FIT
        self.inde = 0
        self.merg = 0

    def _search_key(self):
        return (
            self.horz, self.vert, self.dire, self.orie, self.rota,
            self.wrap, self.shri, self.inde, self.merg,
            )

class Borders(object):
    NO_LINE = 0x00
    THIN    = 0x01
    MEDIUM  = 0x02
    DASHED  = 0x03
    DOTTED  = 0x04
    THICK   = 0x05
    DOUBLE  = 0x06
    HAIR    = 0x07
    #The following for BIFF8
    MEDIUM_DASHED               = 0x08
    THIN_DASH_DOTTED            = 0x09
    MEDIUM_DASH_DOTTED          = 0x0A
    THIN_DASH_DOT_DOTTED        = 0x0B
    MEDIUM_DASH_DOT_DOTTED      = 0x0C
    SLANTED_MEDIUM_DASH_DOTTED  = 0x0D

    NEED_DIAG1      = 0x01
    NEED_DIAG2      = 0x01
    NO_NEED_DIAG1   = 0x00
    NO_NEED_DIAG2   = 0x00

    def __init__(self):
        self.left   = self.NO_LINE
        self.right  = self.NO_LINE
        self.top    = self.NO_LINE
        self.bottom = self.NO_LINE
        self.diag   = self.NO_LINE

        self.left_colour   = 0x40
        self.right_colour  = 0x40
        self.top_colour    = 0x40
        self.bottom_colour = 0x40
        self.diag_colour   = 0x40

        self.need_diag1 = self.NO_NEED_DIAG1
        self.need_diag2 = self.NO_NEED_DIAG2

    def _search_key(self):
        return (
             self.left, self.right, self.top, self.bottom, self.diag,
             self.left_colour, self.right_colour, self.top_colour,
             self.bottom_colour, self.diag_colour,
             self.need_diag1, self.need_diag2,
            )

class Pattern(object):
    # patterns 0x00 - 0x12
    NO_PATTERN      = 0x00
    SOLID_PATTERN   = 0x01

    def __init__(self):
        self.pattern = self.NO_PATTERN
        self.pattern_fore_colour = 0x40
        self.pattern_back_colour = 0x41

    def _search_key(self):
        return (
            self.pattern,
            self.pattern_fore_colour,
            self.pattern_back_colour,
            )

class Protection(object):
    def __init__(self):
        self.cell_locked = 1
        self.formula_hidden = 0

    def _search_key(self):
        return (
            self.cell_locked,
            self.formula_hidden,
            )

########NEW FILE########
__FILENAME__ = Row
# -*- coding: windows-1252 -*-

import BIFFRecords
import Style
from Cell import StrCell, BlankCell, NumberCell, FormulaCell, MulBlankCell, BooleanCell, ErrorCell, \
    _get_cells_biff_data_mul
import ExcelFormula
import datetime as dt
try:
    from decimal import Decimal
except ImportError:
    # Python 2.3: decimal not supported; create dummy Decimal class
    class Decimal(object):
        pass


class Row(object):
    __slots__ = [# private variables
                 "__idx",
                 "__parent",
                 "__parent_wb",
                 "__cells",
                 "__min_col_idx",
                 "__max_col_idx",
                 "__xf_index",
                 "__has_default_xf_index",
                 "__height_in_pixels",
                 # public variables
                 "height",
                 "has_default_height",
                 "height_mismatch",
                 "level",
                 "collapse",
                 "hidden",
                 "space_above",
                 "space_below"]

    def __init__(self, rowx, parent_sheet):
        if not (isinstance(rowx, int) and 0 <= rowx <= 65535):
            raise ValueError("row index (%r) not an int in range(65536)" % rowx)
        self.__idx = rowx
        self.__parent = parent_sheet
        self.__parent_wb = parent_sheet.get_parent()
        self.__cells = {}
        self.__min_col_idx = 0
        self.__max_col_idx = 0
        self.__xf_index = 0x0F
        self.__has_default_xf_index = 0
        self.__height_in_pixels = 0x11

        self.height = 0x00FF
        self.has_default_height = 0x00
        self.height_mismatch = 0
        self.level = 0
        self.collapse = 0
        self.hidden = 0
        self.space_above = 0
        self.space_below = 0


    def __adjust_height(self, style):
        twips = style.font.height
        points = float(twips)/20.0
        # Cell height in pixels can be calcuted by following approx. formula:
        # cell height in pixels = font height in points * 83/50 + 2/5
        # It works when screen resolution is 96 dpi
        pix = int(round(points*83.0/50.0 + 2.0/5.0))
        if pix > self.__height_in_pixels:
            self.__height_in_pixels = pix


    def __adjust_bound_col_idx(self, *args):
        for arg in args:
            iarg = int(arg)
            if not ((0 <= iarg <= 255) and arg == iarg):
                raise ValueError("column index (%r) not an int in range(256)" % arg)
            sheet = self.__parent
            if iarg < self.__min_col_idx:
                self.__min_col_idx = iarg
            if iarg > self.__max_col_idx:
                self.__max_col_idx = iarg
            if iarg < sheet.first_used_col:
                sheet.first_used_col = iarg
            if iarg > sheet.last_used_col:
                sheet.last_used_col = iarg

    def __excel_date_dt(self, date):
        if isinstance(date, dt.date) and (not isinstance(date, dt.datetime)):
            epoch = dt.date(1899, 12, 31)
        elif isinstance(date, dt.time):
            date = dt.datetime.combine(dt.datetime(1900, 1, 1), date)
            epoch = dt.datetime(1900, 1, 1, 0, 0, 0)
        else:
            epoch = dt.datetime(1899, 12, 31, 0, 0, 0)
        delta = date - epoch
        xldate = delta.days + float(delta.seconds) / (24*60*60)
        # Add a day for Excel's missing leap day in 1900
        if xldate > 59:
            xldate += 1
        return xldate

    def get_height_in_pixels(self):
        return self.__height_in_pixels


    def set_style(self, style):
        self.__adjust_height(style)
        self.__xf_index = self.__parent_wb.add_style(style)
        self.__has_default_xf_index = 1


    def get_xf_index(self):
        return self.__xf_index


    def get_cells_count(self):
        return len(self.__cells)


    def get_min_col(self):
        return self.__min_col_idx


    def get_max_col(self):
        return self.__max_col_idx


    def get_row_biff_data(self):
        height_options = (self.height & 0x07FFF)
        height_options |= (self.has_default_height & 0x01) << 15

        options =  (self.level & 0x07) << 0
        options |= (self.collapse & 0x01) << 4
        options |= (self.hidden & 0x01) << 5
        options |= (self.height_mismatch & 0x01) << 6
        options |= (self.__has_default_xf_index & 0x01) << 7
        options |= (0x01 & 0x01) << 8
        options |= (self.__xf_index & 0x0FFF) << 16
        options |= (self.space_above & 1) << 28
        options |= (self.space_below & 1) << 29

        return BIFFRecords.RowRecord(self.__idx, self.__min_col_idx,
            self.__max_col_idx, height_options, options).get()

    def insert_cell(self, col_index, cell_obj):
        if col_index in self.__cells:
            if not self.__parent._cell_overwrite_ok:
                msg = "Attempt to overwrite cell: sheetname=%r rowx=%d colx=%d" \
                    % (self.__parent.name, self.__idx, col_index)
                raise Exception(msg)
            prev_cell_obj = self.__cells[col_index]
            sst_idx = getattr(prev_cell_obj, 'sst_idx', None)
            if sst_idx is not None:
                self.__parent_wb.del_str(sst_idx)
        self.__cells[col_index] = cell_obj

    def insert_mulcells(self, colx1, colx2, cell_obj):
        self.insert_cell(colx1, cell_obj)
        for col_index in xrange(colx1+1, colx2+1):
            self.insert_cell(col_index, None)

    def get_cells_biff_data(self):
        cell_items = [item for item in self.__cells.iteritems() if item[1] is not None]
        cell_items.sort() # in column order
        return _get_cells_biff_data_mul(self.__idx, cell_items)
        # previously:
        # return ''.join([cell.get_biff_data() for colx, cell in cell_items])

    def get_index(self):
        return self.__idx

    def set_cell_text(self, colx, value, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx, StrCell(self.__idx, colx, xf_index, self.__parent_wb.add_str(value)))

    def set_cell_blank(self, colx, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx, BlankCell(self.__idx, colx, xf_index))

    def set_cell_mulblanks(self, first_colx, last_colx, style=Style.default_style):
        assert 0 <= first_colx <= last_colx <= 255
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(first_colx, last_colx)
        xf_index = self.__parent_wb.add_style(style)
        # ncols = last_colx - first_colx + 1
        self.insert_mulcells(first_colx, last_colx, MulBlankCell(self.__idx, first_colx, last_colx, xf_index))

    def set_cell_number(self, colx, number, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx, NumberCell(self.__idx, colx, xf_index, number))

    def set_cell_date(self, colx, datetime_obj, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx,
            NumberCell(self.__idx, colx, xf_index, self.__excel_date_dt(datetime_obj)))

    def set_cell_formula(self, colx, formula, style=Style.default_style, calc_flags=0):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.__parent_wb.add_sheet_reference(formula)
        self.insert_cell(colx, FormulaCell(self.__idx, colx, xf_index, formula, calc_flags=0))

    def set_cell_boolean(self, colx, value, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx, BooleanCell(self.__idx, colx, xf_index, bool(value)))

    def set_cell_error(self, colx, error_string_or_code, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(colx)
        xf_index = self.__parent_wb.add_style(style)
        self.insert_cell(colx, ErrorCell(self.__idx, colx, xf_index, error_string_or_code))

    def write(self, col, label, style=Style.default_style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(col)
        style_index = self.__parent_wb.add_style(style)
        if isinstance(label, basestring):
            if len(label) > 0:
                self.insert_cell(col,
                    StrCell(self.__idx, col, style_index, self.__parent_wb.add_str(label))
                    )
            else:
                self.insert_cell(col, BlankCell(self.__idx, col, style_index))
        elif isinstance(label, bool): # bool is subclass of int; test bool first
            self.insert_cell(col, BooleanCell(self.__idx, col, style_index, label))
        elif isinstance(label, (float, int, long, Decimal)):
            self.insert_cell(col, NumberCell(self.__idx, col, style_index, label))
        elif isinstance(label, (dt.datetime, dt.date, dt.time)):
            date_number = self.__excel_date_dt(label)
            self.insert_cell(col, NumberCell(self.__idx, col, style_index, date_number))
        elif label is None:
            self.insert_cell(col, BlankCell(self.__idx, col, style_index))
        elif isinstance(label, ExcelFormula.Formula):
            self.__parent_wb.add_sheet_reference(label)
            self.insert_cell(col, FormulaCell(self.__idx, col, style_index, label))
        else:
            raise Exception("Unexpected data type %r" % type(label))

    write_blanks = set_cell_mulblanks

########NEW FILE########
__FILENAME__ = Style
# -*- coding: windows-1252 -*-

import Formatting
from BIFFRecords import *

FIRST_USER_DEFINED_NUM_FORMAT_IDX = 164

class XFStyle(object):

    def __init__(self):
        self.num_format_str  = 'General'
        self.font            = Formatting.Font()
        self.alignment       = Formatting.Alignment()
        self.borders         = Formatting.Borders()
        self.pattern         = Formatting.Pattern()
        self.protection      = Formatting.Protection()

default_style = XFStyle()

class StyleCollection(object):
    _std_num_fmt_list = [
            'general',
            '0',
            '0.00',
            '#,##0',
            '#,##0.00',
            '"$"#,##0_);("$"#,##',
            '"$"#,##0_);[Red]("$"#,##',
            '"$"#,##0.00_);("$"#,##',
            '"$"#,##0.00_);[Red]("$"#,##',
            '0%',
            '0.00%',
            '0.00E+00',
            '# ?/?',
            '# ??/??',
            'M/D/YY',
            'D-MMM-YY',
            'D-MMM',
            'MMM-YY',
            'h:mm AM/PM',
            'h:mm:ss AM/PM',
            'h:mm',
            'h:mm:ss',
            'M/D/YY h:mm',
            '_(#,##0_);(#,##0)',
            '_(#,##0_);[Red](#,##0)',
            '_(#,##0.00_);(#,##0.00)',
            '_(#,##0.00_);[Red](#,##0.00)',
            '_("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)',
            '_(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)',
            '_("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)',
            '_(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)',
            'mm:ss',
            '[h]:mm:ss',
            'mm:ss.0',
            '##0.0E+0',
            '@'
    ]

    def __init__(self, style_compression=0):
        self.style_compression = style_compression
        self.stats = [0, 0, 0, 0, 0, 0]
        self._font_id2x = {}
        self._font_x2id = {}
        self._font_val2x = {}

        for x in (0, 1, 2, 3, 5): # The font with index 4 is omitted in all BIFF versions
            font = Formatting.Font()
            search_key = font._search_key()
            self._font_id2x[font] = x
            self._font_x2id[x] = font
            self._font_val2x[search_key] = x

        self._xf_id2x = {}
        self._xf_x2id = {}
        self._xf_val2x = {}

        self._num_formats = {}
        for fmtidx, fmtstr in zip(range(0, 23), StyleCollection._std_num_fmt_list[0:23]):
            self._num_formats[fmtstr] = fmtidx
        for fmtidx, fmtstr in zip(range(37, 50), StyleCollection._std_num_fmt_list[23:]):
            self._num_formats[fmtstr] = fmtidx

        self.default_style = XFStyle()
        self._default_xf = self._add_style(self.default_style)[0]

    def add(self, style):
        if style == None:
            return 0x10
        return self._add_style(style)[1]

    def _add_style(self, style):
        num_format_str = style.num_format_str
        if num_format_str in self._num_formats:
            num_format_idx = self._num_formats[num_format_str]
        else:
            num_format_idx = (
                FIRST_USER_DEFINED_NUM_FORMAT_IDX
                + len(self._num_formats)
                - len(StyleCollection._std_num_fmt_list)
                )
            self._num_formats[num_format_str] = num_format_idx

        font = style.font
        if font in self._font_id2x:
            font_idx = self._font_id2x[font]
            self.stats[0] += 1
        elif self.style_compression:
            search_key = font._search_key()
            font_idx = self._font_val2x.get(search_key)
            if font_idx is not None:
                self._font_id2x[font] = font_idx
                self.stats[1] += 1
            else:
                font_idx = len(self._font_x2id) + 1 # Why plus 1? Font 4 is missing
                self._font_id2x[font] = font_idx
                self._font_val2x[search_key] = font_idx
                self._font_x2id[font_idx] = font
                self.stats[2] += 1
        else:
            font_idx = len(self._font_id2x) + 1
            self._font_id2x[font] = font_idx
            self.stats[2] += 1

        gof = (style.alignment, style.borders, style.pattern, style.protection)
        xf = (font_idx, num_format_idx) + gof
        if xf in self._xf_id2x:
            xf_index = self._xf_id2x[xf]
            self.stats[3] += 1
        elif self.style_compression == 2:
            xf_key = (font_idx, num_format_idx) + tuple([obj._search_key() for obj in gof])
            xf_index = self._xf_val2x.get(xf_key)
            if xf_index is not None:
                self._xf_id2x[xf] = xf_index
                self.stats[4] += 1
            else:
                xf_index = 0x10 + len(self._xf_x2id)
                self._xf_id2x[xf] = xf_index
                self._xf_val2x[xf_key] = xf_index
                self._xf_x2id[xf_index] = xf
                self.stats[5] += 1
        else:
            xf_index = 0x10 + len(self._xf_id2x)
            self._xf_id2x[xf] = xf_index
            self.stats[5] += 1

        if xf_index >= 0xFFF:
            # 12 bits allowed, 0xFFF is a sentinel value
            raise ValueError("More than 4094 XFs (styles)")

        return xf, xf_index

    def get_biff_data(self):
        result = ''
        result += self._all_fonts()
        result += self._all_num_formats()
        result += self._all_cell_styles()
        result += self._all_styles()
        return result

    def _all_fonts(self):
        result = ''
        if self.style_compression:
            alist = self._font_x2id.items()
        else:
            alist = [(x, o) for o, x in self._font_id2x.items()]
        alist.sort()
        for font_idx, font in alist:
            result += font.get_biff_record().get()
        return result

    def _all_num_formats(self):
        result = ''
        alist = [
            (v, k)
            for k, v in self._num_formats.items()
            if v >= FIRST_USER_DEFINED_NUM_FORMAT_IDX
            ]
        alist.sort()
        for fmtidx, fmtstr in alist:
            result += NumberFormatRecord(fmtidx, fmtstr).get()
        return result

    def _all_cell_styles(self):
        result = ''
        for i in range(0, 16):
            result += XFRecord(self._default_xf, 'style').get()
        if self.style_compression == 2:
            alist = self._xf_x2id.items()
        else:
            alist = [(x, o) for o, x in self._xf_id2x.items()]
        alist.sort()
        for xf_idx, xf in alist:
            result += XFRecord(xf).get()
        return result

    def _all_styles(self):
        return StyleRecord().get()

# easyxf and its supporting objects ###################################

class EasyXFException(Exception):
    pass

class EasyXFCallerError(EasyXFException):
    pass

class EasyXFAuthorError(EasyXFException):
    pass

class IntULim(object):
    # If astring represents a valid unsigned integer ('123', '0xabcd', etc)
    # and it is <= limit, return the int value; otherwise return None.

    def __init__(self, limit):
        self.limit = limit

    def __call__(self, astring):
        try:
            value = int(astring, 0)
        except ValueError:
            return None
        if not 0 <= value <= self.limit:
            return None
        return value

bool_map = {
    # Text values for all Boolean attributes
    '1': 1, 'yes': 1, 'true':  1, 'on':  1,
    '0': 0, 'no':  0, 'false': 0, 'off': 0,
    }

border_line_map = {
    # Text values for these borders attributes:
    # left, right, top, bottom and diag
    'no_line':  0x00,
    'thin':     0x01,
    'medium':   0x02,
    'dashed':   0x03,
    'dotted':   0x04,
    'thick':    0x05,
    'double':   0x06,
    'hair':     0x07,
    'medium_dashed':                0x08,
    'thin_dash_dotted':             0x09,
    'medium_dash_dotted':           0x0a,
    'thin_dash_dot_dotted':         0x0b,
    'medium_dash_dot_dotted':       0x0c,
    'slanted_medium_dash_dotted':   0x0d,
    }

charset_map = {
    # Text values for font.charset
    'ansi_latin':           0x00,
    'sys_default':          0x01,
    'symbol':               0x02,
    'apple_roman':          0x4d,
    'ansi_jap_shift_jis':   0x80,
    'ansi_kor_hangul':      0x81,
    'ansi_kor_johab':       0x82,
    'ansi_chinese_gbk':     0x86,
    'ansi_chinese_big5':    0x88,
    'ansi_greek':           0xa1,
    'ansi_turkish':         0xa2,
    'ansi_vietnamese':      0xa3,
    'ansi_hebrew':          0xb1,
    'ansi_arabic':          0xb2,
    'ansi_baltic':          0xba,
    'ansi_cyrillic':        0xcc,
    'ansi_thai':            0xde,
    'ansi_latin_ii':        0xee,
    'oem_latin_i':          0xff,
    }


# Text values for colour indices. "grey" is a synonym of "gray".
# The names are those given by Microsoft Excel 2003 to the colours
# in the default palette. There is no great correspondence with
# any W3C name-to-RGB mapping.
_colour_map_text = """\
aqua 0x31
black 0x08
blue 0x0C
blue_gray 0x36
bright_green 0x0B
brown 0x3C
coral 0x1D
cyan_ega 0x0F
dark_blue 0x12
dark_blue_ega 0x12
dark_green 0x3A
dark_green_ega 0x11
dark_purple 0x1C
dark_red 0x10
dark_red_ega 0x10
dark_teal 0x38
dark_yellow 0x13
gold 0x33
gray_ega 0x17
gray25 0x16
gray40 0x37
gray50 0x17
gray80 0x3F
green 0x11
ice_blue 0x1F
indigo 0x3E
ivory 0x1A
lavender 0x2E
light_blue 0x30
light_green 0x2A
light_orange 0x34
light_turquoise 0x29
light_yellow 0x2B
lime 0x32
magenta_ega 0x0E
ocean_blue 0x1E
olive_ega 0x13
olive_green 0x3B
orange 0x35
pale_blue 0x2C
periwinkle 0x18
pink 0x0E
plum 0x3D
purple_ega 0x14
red 0x0A
rose 0x2D
sea_green 0x39
silver_ega 0x16
sky_blue 0x28
tan 0x2F
teal 0x15
teal_ega 0x15
turquoise 0x0F
violet 0x14
white 0x09
yellow 0x0D"""

colour_map = {}
for _line in _colour_map_text.splitlines():
    _name, _num = _line.split()
    _num = int(_num, 0)
    colour_map[_name] = _num
    if 'gray' in _name:
        colour_map[_name.replace('gray', 'grey')] = _num
del _colour_map_text, _line, _name, _num


pattern_map = {
    # Text values for pattern.pattern
    # xlwt/doc/pattern_examples.xls showcases all of these patterns.
    'no_fill':              0,
    'none':                 0,
    'solid':                1,
    'solid_fill':           1,
    'solid_pattern':        1,
    'fine_dots':            2,
    'alt_bars':             3,
    'sparse_dots':          4,
    'thick_horz_bands':     5,
    'thick_vert_bands':     6,
    'thick_backward_diag':  7,
    'thick_forward_diag':   8,
    'big_spots':            9,
    'bricks':               10,
    'thin_horz_bands':      11,
    'thin_vert_bands':      12,
    'thin_backward_diag':   13,
    'thin_forward_diag':    14,
    'squares':              15,
    'diamonds':             16,
    }

def any_str_func(s):
    return s.strip()

def colour_index_func(s, maxval=0x7F):
    try:
        value = int(s, 0)
    except ValueError:
        return None
    if not (0 <= value <= maxval):
        return None
    return value

colour_index_func_7 = colour_index_func

def colour_index_func_15(s):
    return colour_index_func(s, maxval=0x7FFF)

def rotation_func(s):
    try:
        value = int(s, 0)
    except ValueError:
        return None
    if not (-90 <= value <= 90):
        raise EasyXFCallerError("rotation %d: should be -90 to +90 degrees" % value)
    if value < 0:
        value = 90 - value # encode as 91 to 180 (clockwise)
    return value

xf_dict = {
    'align': 'alignment', # synonym
    'alignment': {
        'dire': {
            'general': 0,
            'lr': 1,
            'rl': 2,
            },
        'direction': 'dire',
        'horiz': 'horz',
        'horizontal': 'horz',
        'horz': {
            'general': 0,
            'left': 1,
            'center': 2,
            'centre': 2, # "align: horiz centre" means xf.alignment.horz is set to 2
            'right': 3,
            'filled': 4,
            'justified': 5,
            'center_across_selection': 6,
            'centre_across_selection': 6,
            'distributed': 7,
            },
        'inde': IntULim(15), # restriction: 0 <= value <= 15
        'indent': 'inde',
        'rota': [{'stacked': 255, 'none': 0, }, rotation_func],
        'rotation': 'rota',
        'shri': bool_map,
        'shrink': 'shri',
        'shrink_to_fit': 'shri',
        'vert': {
            'top': 0,
            'center': 1,
            'centre': 1,
            'bottom': 2,
            'justified': 3,
            'distributed': 4,
            },
         'vertical': 'vert',
         'wrap': bool_map,
         },
    'border': 'borders',
    'borders': {
        'left':     [border_line_map, IntULim(0x0d)],
        'right':    [border_line_map, IntULim(0x0d)],
        'top':      [border_line_map, IntULim(0x0d)],
        'bottom':   [border_line_map, IntULim(0x0d)],
        'diag':     [border_line_map, IntULim(0x0d)],
        'top_colour':       [colour_map, colour_index_func_7],
        'bottom_colour':    [colour_map, colour_index_func_7],
        'left_colour':      [colour_map, colour_index_func_7],
        'right_colour':     [colour_map, colour_index_func_7],
        'diag_colour':      [colour_map, colour_index_func_7],
        'top_color':        'top_colour',
        'bottom_color':     'bottom_colour',
        'left_color':       'left_colour',
        'right_color':      'right_colour',
        'diag_color':       'diag-colour',
        'need_diag_1':  bool_map,
        'need_diag_2':  bool_map,
        },
    'font': {
        'bold': bool_map,
        'charset': charset_map,
        'color':  'colour_index',
        'color_index':  'colour_index',
        'colour':  'colour_index',
        'colour_index': [colour_map, colour_index_func_15],
        'escapement': {'none': 0, 'superscript': 1, 'subscript': 2},
        'family': {'none': 0, 'roman': 1, 'swiss': 2, 'modern': 3, 'script': 4, 'decorative': 5, },
        'height': IntULim(0xFFFF), # practical limits are much narrower e.g. 160 to 1440 (8pt to 72pt)
        'italic': bool_map,
        'name': any_str_func,
        'outline': bool_map,
        'shadow': bool_map,
        'struck_out': bool_map,
        'underline': [bool_map, {'none': 0, 'single': 1, 'single_acc': 0x21, 'double': 2, 'double_acc': 0x22, }],
        },
    'pattern': {
        'back_color':   'pattern_back_colour',
        'back_colour':  'pattern_back_colour',
        'fore_color':   'pattern_fore_colour',
        'fore_colour':  'pattern_fore_colour',
        'pattern': [pattern_map, IntULim(16)],
        'pattern_back_color':   'pattern_back_colour',
        'pattern_back_colour':  [colour_map, colour_index_func_7],
        'pattern_fore_color':   'pattern_fore_colour',
        'pattern_fore_colour':  [colour_map, colour_index_func_7],
        },
    'protection': {
        'cell_locked' :   bool_map,
        'formula_hidden': bool_map,
        },
    }

def _esplit(s, split_char, esc_char="\\"):
    escaped = False
    olist = ['']
    for c in s:
        if escaped:
            olist[-1] += c
            escaped = False
        elif c == esc_char:
            escaped = True
        elif c == split_char:
            olist.append('')
        else:
            olist[-1] += c
    return olist

def _parse_strg_to_obj(strg, obj, parse_dict,
    field_sep=",", line_sep=";", intro_sep=":", esc_char="\\", debug=False):
    for line in _esplit(strg, line_sep, esc_char):
        line = line.strip()
        if not line:
            break
        split_line = _esplit(line, intro_sep, esc_char)
        if len(split_line) != 2:
            raise EasyXFCallerError('line %r should have exactly 1 "%c"' % (line, intro_sep))
        section, item_str = split_line
        section = section.strip().lower()
        for counter in range(2):
            result = parse_dict.get(section)
            if result is None:
                raise EasyXFCallerError('section %r is unknown' % section)
            if isinstance(result, dict):
                break
            if not isinstance(result, str):
                raise EasyXFAuthorError(
                    'section %r should map to dict or str object; found %r' % (section, type(result)))
            # synonym
            old_section = section
            section = result
        else:
            raise EasyXFAuthorError('Attempt to define synonym of synonym (%r: %r)' % (old_section, result))
        section_dict = result
        section_obj = getattr(obj, section, None)
        if section_obj is None:
            raise EasyXFAuthorError('instance of %s class has no attribute named %s' % (obj.__class__.__name__, section))
        for kv_str in _esplit(item_str, field_sep, esc_char):
            guff = kv_str.split()
            if not guff:
                continue
            k = guff[0].lower().replace('-', '_')
            v = ' '.join(guff[1:])
            if not v:
                raise EasyXFCallerError("no value supplied for %s.%s" % (section, k))
            for counter in xrange(2):
                result = section_dict.get(k)
                if result is None:
                    raise EasyXFCallerError('%s.%s is not a known attribute' % (section, k))
                if not isinstance(result, basestring):
                    break
                # synonym
                old_k = k
                k = result
            else:
                raise EasyXFAuthorError('Attempt to define synonym of synonym (%r: %r)' % (old_k, result))
            value_info = result
            if not isinstance(value_info, list):
                value_info = [value_info]
            for value_rule in value_info:
                if isinstance(value_rule, dict):
                    # dict maps strings to integer field values
                    vl = v.lower().replace('-', '_')
                    if vl in value_rule:
                        value = value_rule[vl]
                        break
                elif callable(value_rule):
                    value = value_rule(v)
                    if value is not None:
                        break
                else:
                    raise EasyXFAuthorError("unknown value rule for attribute %r: %r" % (k, value_rule))
            else:
                raise EasyXFCallerError("unexpected value %r for %s.%s" % (v, section, k))
            try:
                orig = getattr(section_obj, k)
            except AttributeError:
                raise EasyXFAuthorError('%s.%s in dictionary but not in supplied object' % (section, k))
            if debug: print "+++ %s.%s = %r # %s; was %r" % (section, k, value, v, orig)
            setattr(section_obj, k, value)

def easyxf(strg_to_parse="", num_format_str=None,
    field_sep=",", line_sep=";", intro_sep=":", esc_char="\\", debug=False):
    xfobj = XFStyle()
    if num_format_str is not None:
        xfobj.num_format_str = num_format_str
    if strg_to_parse:
        _parse_strg_to_obj(strg_to_parse, xfobj, xf_dict,
            field_sep=field_sep, line_sep=line_sep, intro_sep=intro_sep, esc_char=esc_char, debug=debug)
    return xfobj

########NEW FILE########
__FILENAME__ = UnicodeUtils
# -*- coding: windows-1252 -*-

'''
From BIFF8 on, strings are always stored using UTF-16LE  text encoding. The
character  array  is  a  sequence  of  16-bit  values4.  Additionally it is
possible  to  use  a  compressed  format, which omits the high bytes of all
characters, if they are all zero.

The following tables describe the standard format of the entire string, but
in many records the strings differ from this format. This will be mentioned
separately. It is possible (but not required) to store Rich-Text formatting
information  and  Asian  phonetic information inside a Unicode string. This
results  in  four  different  ways  to  store a string. The character array
is not zero-terminated.

The  string  consists  of  the  character count (as usual an 8-bit value or
a  16-bit value), option flags, the character array and optional formatting
information.  If the string is empty, sometimes the option flags field will
not occur. This is mentioned at the respective place.

Offset  Size    Contents
0       1 or 2  Length of the string (character count, ln)
1 or 2  1       Option flags:
                  Bit   Mask Contents
                  0     01H  Character compression (ccompr):
                               0 = Compressed (8-bit characters)
                               1 = Uncompressed (16-bit characters)
                  2     04H  Asian phonetic settings (phonetic):
                               0 = Does not contain Asian phonetic settings
                               1 = Contains Asian phonetic settings
                  3     08H  Rich-Text settings (richtext):
                               0 = Does not contain Rich-Text settings
                               1 = Contains Rich-Text settings
[2 or 3] 2      (optional, only if richtext=1) Number of Rich-Text formatting runs (rt)
[var.]   4      (optional, only if phonetic=1) Size of Asian phonetic settings block (in bytes, sz)
var.     ln or
         2ln   Character array (8-bit characters or 16-bit characters, dependent on ccompr)
[var.]   4rt   (optional, only if richtext=1) List of rt formatting runs
[var.]   sz     (optional, only if phonetic=1) Asian Phonetic Settings Block
'''


from struct import pack

def upack2(s, encoding='ascii'):
    # If not unicode, make it so.
    if isinstance(s, unicode):
        us = s
    else:
        us = unicode(s, encoding)
    # Limit is based on number of content characters
    # (not on number of bytes in packed result)
    len_us = len(us)
    if len_us > 65535:
        raise Exception('String longer than 65535 characters')
    try:
        encs = us.encode('latin1')
        # Success here means all chars are in U+0000 to U+00FF
        # inclusive, meaning that we can use "compressed format".
        flag = 0
    except UnicodeEncodeError:
        encs = us.encode('utf_16_le')
        flag = 1
    return pack('<HB', len_us, flag) + encs

def upack1(s, encoding='ascii'):
    # Same as upack2(), but with a one-byte length field.
    if isinstance(s, unicode):
        us = s
    else:
        us = unicode(s, encoding)
    len_us = len(us)
    if len_us > 255:
        raise Exception('String longer than 255 characters')
    try:
        encs = us.encode('latin1')
        flag = 0
    except UnicodeEncodeError:
        encs = us.encode('utf_16_le')
        flag = 1
    return pack('<BB', len_us, flag) + encs

########NEW FILE########
__FILENAME__ = Utils
# pyXLWriter: A library for generating Excel Spreadsheets
# Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>
# Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)
#
# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# This library is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser
# General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
#----------------------------------------------------------------------------
# This module was written/ported from PERL Spreadsheet::WriteExcel module
# The author of the PERL Spreadsheet::WriteExcel module is John McNamara
# <jmcnamara@cpan.org>
#----------------------------------------------------------------------------
# See the README.txt distributed with pyXLWriter for more details.

# Portions are (C) Roman V. Kiseliov, 2005


# Utilities for work with reference to cells and with sheetnames


__rev_id__ = """$Id: Utils.py 3844 2009-05-20 01:02:54Z sjmachin $"""

import re
from struct import pack
from ExcelMagic import MAX_ROW, MAX_COL


_re_cell_ex = re.compile(r"(\$?)([A-I]?[A-Z])(\$?)(\d+)", re.IGNORECASE)
_re_row_range = re.compile(r"\$?(\d+):\$?(\d+)")
_re_col_range = re.compile(r"\$?([A-I]?[A-Z]):\$?([A-I]?[A-Z])", re.IGNORECASE)
_re_cell_range = re.compile(r"\$?([A-I]?[A-Z]\$?\d+):\$?([A-I]?[A-Z]\$?\d+)", re.IGNORECASE)
_re_cell_ref = re.compile(r"\$?([A-I]?[A-Z]\$?\d+)", re.IGNORECASE)


def col_by_name(colname):
    """
    """
    col = 0
    pow = 1
    for i in xrange(len(colname)-1, -1, -1):
        ch = colname[i]
        col += (ord(ch) - ord('A') + 1) * pow
        pow *= 26
    return col - 1


def cell_to_rowcol(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.

    Returns: row, col, row_abs, col_abs

    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Ill-formed single_cell reference: %s" % cell)
    col_abs, col, row_abs, row = m.groups()
    row_abs = bool(row_abs)
    col_abs = bool(col_abs)
    row = int(row) - 1
    col = col_by_name(col.upper())
    return row, col, row_abs, col_abs


def cell_to_rowcol2(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.

    Returns: row, col

    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Error in cell format")
    col_abs, col, row_abs, row = m.groups()
    # Convert base26 column string to number
    # All your Base are belong to us.
    row = int(row) - 1
    col = col_by_name(col.upper())
    return row, col


def rowcol_to_cell(row, col, row_abs=False, col_abs=False):
    """Convert numeric row/col notation to an Excel cell reference string in
    A1 notation.

    """
    assert 0 <= row < MAX_ROW # MAX_ROW counts from 1
    assert 0 <= col < MAX_COL # MAX_COL counts from 1
    d = col // 26
    m = col % 26
    chr1 = ""    # Most significant character in AA1
    if row_abs:
        row_abs = '$'
    else:
        row_abs = ''
    if col_abs:
        col_abs = '$'
    else:
        col_abs = ''
    if d > 0:
        chr1 = chr(ord('A') + d  - 1)
    chr2 = chr(ord('A') + m)
    # Zero index to 1-index
    return col_abs + chr1 + chr2 + row_abs + str(row + 1)

def rowcol_pair_to_cellrange(row1, col1, row2, col2,
    row1_abs=False, col1_abs=False, row2_abs=False, col2_abs=False):
    """Convert two (row,column) pairs
    into a cell range string in A1:B2 notation.

    Returns: cell range string
    """
    assert row1 <= row2
    assert col1 <= col2
    return (
        rowcol_to_cell(row1, col1, row1_abs, col1_abs)
        + ":"
        + rowcol_to_cell(row2, col2, row2_abs, col2_abs)
        )

def cellrange_to_rowcol_pair(cellrange):
    """Convert cell range string in A1 notation to numeric row/col
    pair.

    Returns: row1, col1, row2, col2

    """
    cellrange = cellrange.upper()
    # Convert a row range: '1:3'
    res = _re_row_range.match(cellrange)
    if res:
        row1 = int(res.group(1)) - 1
        col1 = 0
        row2 = int(res.group(2)) - 1
        col2 = -1
        return row1, col1, row2, col2
    # Convert a column range: 'A:A' or 'B:G'.
    # A range such as A:A is equivalent to A1:A16384, so add rows as required
    res = _re_col_range.match(cellrange)
    if res:
        col1 = col_by_name(res.group(1).upper())
        row1 = 0
        col2 = col_by_name(res.group(2).upper())
        row2 = -1
        return row1, col1, row2, col2
    # Convert a cell range: 'A1:B7'
    res = _re_cell_range.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        row2, col2 = cell_to_rowcol2(res.group(2))
        return row1, col1, row2, col2
    # Convert a cell reference: 'A1' or 'AD2000'
    res = _re_cell_ref.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        return row1, col1, row1, col1
    raise Exception("Unknown cell reference %s" % (cell))


def cell_to_packed_rowcol(cell):
    """ pack row and column into the required 4 byte format """
    row, col, row_abs, col_abs = cell_to_rowcol(cell)
    if col >= MAX_COL:
        raise Exception("Column %s greater than IV in formula" % cell)
    if row >= MAX_ROW: # this for BIFF8. for BIFF7 available 2^14
        raise Exception("Row %s greater than %d in formula" % (cell, MAX_ROW))
    col |= int(not row_abs) << 15
    col |= int(not col_abs) << 14
    return row, col

# === sheetname functions ===

def valid_sheet_name(sheet_name):
    if sheet_name == u"" or sheet_name[0] == u"'" or len(sheet_name) > 31:
        return False
    for c in sheet_name:
        if c in u"[]:\\?/*\x00":
            return False
    return True

def quote_sheet_name(unquoted_sheet_name):
    if not valid_sheet_name(unquoted_sheet_name):
        raise Exception(
            'attempt to quote an invalid worksheet name %r' % unquoted_sheet_name)
    return u"'" + unquoted_sheet_name.replace(u"'", u"''") + u"'"

########NEW FILE########
__FILENAME__ = Workbook
# -*- coding: windows-1252 -*-
'''
Record Order in BIFF8
  Workbook Globals Substream
      BOF Type = workbook globals
      Interface Header
      MMS
      Interface End
      WRITEACCESS
      CODEPAGE
      DSF
      TABID
      FNGROUPCOUNT
      Workbook Protection Block
            WINDOWPROTECT
            PROTECT
            PASSWORD
            PROT4REV
            PROT4REVPASS
      BACKUP
      HIDEOBJ
      WINDOW1
      DATEMODE
      PRECISION
      REFRESHALL
      BOOKBOOL
      FONT +
      FORMAT *
      XF +
      STYLE +
    ? PALETTE
      USESELFS

      BOUNDSHEET +

      COUNTRY
    ? Link Table
      SST
      ExtSST
      EOF
'''

import BIFFRecords
import Style

class Workbook(object):

    #################################################################
    ## Constructor
    #################################################################
    def __init__(self, encoding='ascii', style_compression=0):
        self.encoding = encoding
        self.__owner = 'None'
        self.__country_code = None # 0x07 is Russia :-)
        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0
        self.__backup_on_save = 0
        # for WINDOW1 record
        self.__hpos_twips = 0x01E0
        self.__vpos_twips = 0x005A
        self.__width_twips = 0x3FCF
        self.__height_twips = 0x2A4E

        self.__active_sheet = 0
        self.__first_tab_index = 0
        self.__selected_tabs = 0x01
        self.__tab_width_twips = 0x0258

        self.__wnd_hidden = 0
        self.__wnd_mini = 0
        self.__hscroll_visible = 1
        self.__vscroll_visible = 1
        self.__tabs_visible = 1

        self.__styles = Style.StyleCollection(style_compression)

        self.__dates_1904 = 0
        self.__use_cell_values = 1

        self.__sst = BIFFRecords.SharedStringTable(self.encoding)

        self.__worksheets = []
        self.__worksheet_idx_from_name = {}
        self.__sheet_refs = {}
        self._supbook_xref = {}
        self._xcall_xref = {}
        self._ownbook_supbookx = None
        self._ownbook_supbook_ref = None
        self._xcall_supbookx = None
        self._xcall_supbook_ref = None



    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    def get_style_stats(self):
        return self.__styles.stats[:]

    def set_owner(self, value):
        self.__owner = value

    def get_owner(self):
        return self.__owner

    owner = property(get_owner, set_owner)

    #################################################################

    def set_country_code(self, value):
        self.__country_code = value

    def get_country_code(self):
        return self.__country_code

    country_code = property(get_country_code, set_country_code)

    #################################################################

    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)

    #################################################################

    def set_backup_on_save(self, value):
        self.__backup_on_save = int(value)

    def get_backup_on_save(self):
        return bool(self.__backup_on_save)

    backup_on_save = property(get_backup_on_save, set_backup_on_save)

    #################################################################

    def set_hpos(self, value):
        self.__hpos_twips = value & 0xFFFF

    def get_hpos(self):
        return self.__hpos_twips

    hpos = property(get_hpos, set_hpos)

    #################################################################

    def set_vpos(self, value):
        self.__vpos_twips = value & 0xFFFF

    def get_vpos(self):
        return self.__vpos_twips

    vpos = property(get_vpos, set_vpos)

    #################################################################

    def set_width(self, value):
        self.__width_twips = value & 0xFFFF

    def get_width(self):
        return self.__width_twips

    width = property(get_width, set_width)

    #################################################################

    def set_height(self, value):
        self.__height_twips = value & 0xFFFF

    def get_height(self):
        return self.__height_twips

    height = property(get_height, set_height)

    #################################################################

    def set_active_sheet(self, value):
        self.__active_sheet = value & 0xFFFF
        self.__first_tab_index = self.__active_sheet

    def get_active_sheet(self):
        return self.__active_sheet

    active_sheet = property(get_active_sheet, set_active_sheet)

    #################################################################

    def set_tab_width(self, value):
        self.__tab_width_twips = value & 0xFFFF

    def get_tab_width(self):
        return self.__tab_width_twips

    tab_width = property(get_tab_width, set_tab_width)

    #################################################################

    def set_wnd_visible(self, value):
        self.__wnd_hidden = int(not value)

    def get_wnd_visible(self):
        return not bool(self.__wnd_hidden)

    wnd_visible = property(get_wnd_visible, set_wnd_visible)

    #################################################################

    def set_wnd_mini(self, value):
        self.__wnd_mini = int(value)

    def get_wnd_mini(self):
        return bool(self.__wnd_mini)

    wnd_mini = property(get_wnd_mini, set_wnd_mini)

    #################################################################

    def set_hscroll_visible(self, value):
        self.__hscroll_visible = int(value)

    def get_hscroll_visible(self):
        return bool(self.__hscroll_visible)

    hscroll_visible = property(get_hscroll_visible, set_hscroll_visible)

    #################################################################

    def set_vscroll_visible(self, value):
        self.__vscroll_visible = int(value)

    def get_vscroll_visible(self):
        return bool(self.__vscroll_visible)

    vscroll_visible = property(get_vscroll_visible, set_vscroll_visible)

    #################################################################

    def set_tabs_visible(self, value):
        self.__tabs_visible = int(value)

    def get_tabs_visible(self):
        return bool(self.__tabs_visible)

    tabs_visible = property(get_tabs_visible, set_tabs_visible)

    #################################################################

    def set_dates_1904(self, value):
        self.__dates_1904 = int(value)

    def get_dates_1904(self):
        return bool(self.__dates_1904)

    dates_1904 = property(get_dates_1904, set_dates_1904)

    #################################################################

    def set_use_cell_values(self, value):
        self.__use_cell_values = int(value)

    def get_use_cell_values(self):
        return bool(self.__use_cell_values)

    use_cell_values = property(get_use_cell_values, set_use_cell_values)

    #################################################################

    def get_default_style(self):
        return self.__styles.default_style

    default_style = property(get_default_style)

    ##################################################################
    ## Methods
    ##################################################################

    def add_style(self, style):
        return self.__styles.add(style)

    def add_str(self, s):
        return self.__sst.add_str(s)

    def del_str(self, sst_idx):
        self.__sst.del_str(sst_idx)

    def str_index(self, s):
        return self.__sst.str_index(s)

    def add_sheet(self, sheetname, cell_overwrite_ok=False):
        import Worksheet, Utils
        if not isinstance(sheetname, unicode):
            sheetname = sheetname.decode(self.encoding)
        if not Utils.valid_sheet_name(sheetname):
            raise Exception("invalid worksheet name %r" % sheetname)
        lower_name = sheetname.lower()
        if lower_name in self.__worksheet_idx_from_name:
            raise Exception("duplicate worksheet name %r" % sheetname)
        self.__worksheet_idx_from_name[lower_name] = len(self.__worksheets)
        self.__worksheets.append(Worksheet.Worksheet(sheetname, self, cell_overwrite_ok))
        return self.__worksheets[-1]

    def get_sheet(self, sheetnum):
        return self.__worksheets[sheetnum]

    def raise_bad_sheetname(self, sheetname):
        raise Exception("Formula: unknown sheet name %s" % sheetname)

    def convert_sheetindex(self, strg_ref, n_sheets):
        idx = int(strg_ref)
        if 0 <= idx < n_sheets:
            return idx
        msg = "Formula: sheet index (%s) >= number of sheets (%d)" % (strg_ref, n_sheets)
        raise Exception(msg)

    def _get_supbook_index(self, tag):
        if tag in self._supbook_xref:
            return self._supbook_xref[tag]
        self._supbook_xref[tag] = idx = len(self._supbook_xref)
        return idx

    def setup_ownbook(self):
        self._ownbook_supbookx = self._get_supbook_index(('ownbook', 0))
        self._ownbook_supbook_ref = None
        reference = (self._ownbook_supbookx, 0xFFFE, 0xFFFE)
        if reference in self.__sheet_refs:
            raise Exception("can't happen")
        self.__sheet_refs[reference] = self._ownbook_supbook_ref = len(self.__sheet_refs)

    def setup_xcall(self):
        self._xcall_supbookx = self._get_supbook_index(('xcall', 0))
        self._xcall_supbook_ref = None
        reference = (self._xcall_supbookx, 0xFFFE, 0xFFFE)
        if reference in self.__sheet_refs:
            raise Exception("can't happen")
        self.__sheet_refs[reference] = self._xcall_supbook_ref = len(self.__sheet_refs)

    def add_sheet_reference(self, formula):
        patches = []
        n_sheets = len(self.__worksheets)
        sheet_refs, xcall_refs = formula.get_references()

        for ref0, ref1, offset in sheet_refs:
            if not ref0.isdigit():
                try:
                    ref0n = self.__worksheet_idx_from_name[ref0.lower()]
                except KeyError:
                    self.raise_bad_sheetname(ref0)
            else:
                ref0n = self.convert_sheetindex(ref0, n_sheets)
            if ref1 == ref0:
                ref1n = ref0n
            elif not ref1.isdigit():
                try:
                    ref1n = self.__worksheet_idx_from_name[ref1.lower()]
                except KeyError:
                    self.raise_bad_sheetname(ref1)
            else:
                ref1n = self.convert_sheetindex(ref1, n_sheets)
            if ref1n < ref0n:
                msg = "Formula: sheets out of order; %r:%r -> (%d, %d)" \
                    % (ref0, ref1, ref0n, ref1n)
                raise Exception(msg)
            if self._ownbook_supbookx is None:
                self.setup_ownbook()
            reference = (self._ownbook_supbookx, ref0n, ref1n)
            if reference in self.__sheet_refs:
                patches.append((offset, self.__sheet_refs[reference]))
            else:
                nrefs = len(self.__sheet_refs)
                if nrefs > 65535:
                    raise Exception('More than 65536 inter-sheet references')
                self.__sheet_refs[reference] = nrefs
                patches.append((offset, nrefs))

        for funcname, offset in xcall_refs:
            if self._ownbook_supbookx is None:
                self.setup_ownbook()
            if self._xcall_supbookx is None:
                self.setup_xcall()
            # print funcname, self._supbook_xref
            patches.append((offset, self._xcall_supbook_ref))
            if not isinstance(funcname, unicode):
                funcname = funcname.decode(self.encoding)
            if funcname in self._xcall_xref:
                idx = self._xcall_xref[funcname]
            else:
                self._xcall_xref[funcname] = idx = len(self._xcall_xref)
            patches.append((offset + 2, idx + 1))

        formula.patch_references(patches)

    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.BOOK_GLOBAL).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()

    def __intf_hdr_rec(self):
        return BIFFRecords.InteraceHdrRecord().get()

    def __intf_end_rec(self):
        return BIFFRecords.InteraceEndRecord().get()

    def __intf_mms_rec(self):
        return BIFFRecords.MMSRecord().get()

    def __write_access_rec(self):
        return BIFFRecords.WriteAccessRecord(self.__owner).get()

    def __wnd_protect_rec(self):
        return BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()

    def __obj_protect_rec(self):
        return BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()

    def __protect_rec(self):
        return BIFFRecords.ProtectRecord(self.__protect).get()

    def __password_rec(self):
        return BIFFRecords.PasswordRecord().get()

    def __prot4rev_rec(self):
        return BIFFRecords.Prot4RevRecord().get()

    def __prot4rev_pass_rec(self):
        return BIFFRecords.Prot4RevPassRecord().get()

    def __backup_rec(self):
        return BIFFRecords.BackupRecord(self.__backup_on_save).get()

    def __hide_obj_rec(self):
        return BIFFRecords.HideObjRecord().get()

    def __window1_rec(self):
        flags = 0
        flags |= (self.__wnd_hidden) << 0
        flags |= (self.__wnd_mini) << 1
        flags |= (self.__hscroll_visible) << 3
        flags |= (self.__vscroll_visible) << 4
        flags |= (self.__tabs_visible) << 5

        return BIFFRecords.Window1Record(self.__hpos_twips, self.__vpos_twips,
                                self.__width_twips, self.__height_twips,
                                flags,
                                self.__active_sheet, self.__first_tab_index,
                                self.__selected_tabs, self.__tab_width_twips).get()

    def __codepage_rec(self):
        return BIFFRecords.CodepageBiff8Record().get()

    def __country_rec(self):
        if not self.__country_code:
            return ''
        return BIFFRecords.CountryRecord(self.__country_code, self.__country_code).get()

    def __dsf_rec(self):
        return BIFFRecords.DSFRecord().get()

    def __tabid_rec(self):
        return BIFFRecords.TabIDRecord(len(self.__worksheets)).get()

    def __fngroupcount_rec(self):
        return BIFFRecords.FnGroupCountRecord().get()

    def __datemode_rec(self):
        return BIFFRecords.DateModeRecord(self.__dates_1904).get()

    def __precision_rec(self):
        return BIFFRecords.PrecisionRecord(self.__use_cell_values).get()

    def __refresh_all_rec(self):
        return BIFFRecords.RefreshAllRecord().get()

    def __bookbool_rec(self):
        return BIFFRecords.BookBoolRecord().get()

    def __all_fonts_num_formats_xf_styles_rec(self):
        return self.__styles.get_biff_data()

    def __palette_rec(self):
        result = ''
        return result

    def __useselfs_rec(self):
        return BIFFRecords.UseSelfsRecord().get()

    def __boundsheets_rec(self, data_len_before, data_len_after, sheet_biff_lens):
        #  .................................
        # BOUNDSEHEET0
        # BOUNDSEHEET1
        # BOUNDSEHEET2
        # ..................................
        # WORKSHEET0
        # WORKSHEET1
        # WORKSHEET2
        boundsheets_len = 0
        for sheet in self.__worksheets:
            boundsheets_len += len(BIFFRecords.BoundSheetRecord(
                0x00L, sheet.visibility, sheet.name, self.encoding
                ).get())

        start = data_len_before + boundsheets_len + data_len_after

        result = ''
        for sheet_biff_len,  sheet in zip(sheet_biff_lens, self.__worksheets):
            result += BIFFRecords.BoundSheetRecord(
                start, sheet.visibility, sheet.name, self.encoding
                ).get()
            start += sheet_biff_len
        return result

    def __all_links_rec(self):
        pieces = []
        temp = [(idx, tag) for tag, idx in self._supbook_xref.items()]
        temp.sort()
        for idx, tag in temp:
            stype, snum = tag
            if stype == 'ownbook':
                rec = BIFFRecords.InternalReferenceSupBookRecord(len(self.__worksheets)).get()
                pieces.append(rec)
            elif stype == 'xcall':
                rec = BIFFRecords.XcallSupBookRecord().get()
                pieces.append(rec)
                temp = [(idx, name) for name, idx in self._xcall_xref.items()]
                temp.sort()
                for idx, name in temp:
                    rec = BIFFRecords.ExternnameRecord(
                        options=0, index=0, name=name, fmla='\x02\x00\x1c\x17').get()
                    pieces.append(rec)
            else:
                raise Exception('unknown supbook stype %r' % stype)
        if len(self.__sheet_refs) > 0:
            # get references in index order
            temp = [(idx, ref) for ref, idx in self.__sheet_refs.items()]
            temp.sort()
            temp = [ref for idx, ref in temp]
            externsheet_record = BIFFRecords.ExternSheetRecord(temp).get()
            pieces.append(externsheet_record)
        return ''.join(pieces)

    def __sst_rec(self):
        return self.__sst.get_biff_record()

    def __ext_sst_rec(self, abs_stream_pos):
        return ''
        #return BIFFRecords.ExtSSTRecord(abs_stream_pos, self.sst_record.str_placement,
        #self.sst_record.portions_len).get()

    def get_biff_data(self):
        before = ''
        before += self.__bof_rec()
        before += self.__intf_hdr_rec()
        before += self.__intf_mms_rec()
        before += self.__intf_end_rec()
        before += self.__write_access_rec()
        before += self.__codepage_rec()
        before += self.__dsf_rec()
        before += self.__tabid_rec()
        before += self.__fngroupcount_rec()
        before += self.__wnd_protect_rec()
        before += self.__protect_rec()
        before += self.__obj_protect_rec()
        before += self.__password_rec()
        before += self.__prot4rev_rec()
        before += self.__prot4rev_pass_rec()
        before += self.__backup_rec()
        before += self.__hide_obj_rec()
        before += self.__window1_rec()
        before += self.__datemode_rec()
        before += self.__precision_rec()
        before += self.__refresh_all_rec()
        before += self.__bookbool_rec()
        before += self.__all_fonts_num_formats_xf_styles_rec()
        before += self.__palette_rec()
        before += self.__useselfs_rec()

        country            = self.__country_rec()
        all_links          = self.__all_links_rec()

        shared_str_table   = self.__sst_rec()
        after = country + all_links + shared_str_table

        ext_sst = self.__ext_sst_rec(0) # need fake cause we need calc stream pos
        eof = self.__eof_rec()

        self.__worksheets[self.__active_sheet].selected = True
        sheets = ''
        sheet_biff_lens = []
        for sheet in self.__worksheets:
            data = sheet.get_biff_data()
            sheets += data
            sheet_biff_lens.append(len(data))

        bundlesheets = self.__boundsheets_rec(len(before), len(after)+len(ext_sst)+len(eof), sheet_biff_lens)

        sst_stream_pos = len(before) + len(bundlesheets) + len(country)  + len(all_links)
        ext_sst = self.__ext_sst_rec(sst_stream_pos)

        return before + bundlesheets + after + ext_sst + eof + sheets

    def save(self, filename):
        import CompoundDoc

        doc = CompoundDoc.XlsDoc()
        doc.save(filename, self.get_biff_data())

########NEW FILE########
__FILENAME__ = Worksheet
# -*- coding: windows-1252 -*-
'''
            BOF
            UNCALCED
            INDEX
            Calculation Settings Block
            PRINTHEADERS
            PRINTGRIDLINES
            GRIDSET
            GUTS
            DEFAULTROWHEIGHT
            WSBOOL
            Page Settings Block
            Worksheet Protection Block
            DEFCOLWIDTH
            COLINFO
            SORT
            DIMENSIONS
            Row Blocks
            WINDOW2
            SCL
            PANE
            SELECTION
            STANDARDWIDTH
            MERGEDCELLS
            LABELRANGES
            PHONETIC
            Conditional Formatting Table
            Hyperlink Table
            Data Validity Table
            SHEETLAYOUT (BIFF8X only)
            SHEETPROTECTION (BIFF8X only)
            RANGEPROTECTION (BIFF8X only)
            EOF
'''

import BIFFRecords
import Bitmap
import Formatting
import Style
import tempfile


class Worksheet(object):
    from Workbook import Workbook

    #################################################################
    ## Constructor
    #################################################################
    def __init__(self, sheetname, parent_book, cell_overwrite_ok=False):
        import Row
        self.Row = Row.Row

        import Column
        self.Column = Column.Column

        self.__name = sheetname
        self.__parent = parent_book
        self._cell_overwrite_ok = cell_overwrite_ok

        self.__rows = {}
        self.__cols = {}
        self.__merged_ranges = []
        self.__bmp_rec = ''

        self.__show_formulas = 0
        self.__show_grid = 1
        self.__show_headers = 1
        self.__panes_frozen = 0
        ### self.__show_empty_as_zero = 1 ### deprecated with extreme prejudice 2009-05-19
        self.show_zero_values = 1
        self.__auto_colour_grid = 1
        self.__cols_right_to_left = 0
        self.__show_outline = 1
        self.__remove_splits = 0
        self.__selected = 0
        # RED HERRING ALERT: "sheet_visible" is a clone of the "selected" attribute.
        # Typically a workbook created by the Excel UI will have one sheet
        # (the sheet that was selected when the user saved it)
        # with both bits set to 1, and all other sheets will have both
        # bits set to 0. The true visibility of the sheet is found in the "visibility"
        # attribute obtained from the BOUNDSHEET record.
        self.__sheet_visible = 0
        self.__page_preview = 0

        self.__first_visible_row = 0
        self.__first_visible_col = 0
        self.__grid_colour = 0x40
        self.__preview_magn = 60 # percent
        self.__normal_magn = 100 # percent

        self.visibility = 0 # from/to BOUNDSHEET record.

        self.__vert_split_pos = None
        self.__horz_split_pos = None
        self.__vert_split_first_visible = None
        self.__horz_split_first_visible = None
        self.__split_active_pane = None

        self.__row_gut_width = 0
        self.__col_gut_height = 0

        self.__show_auto_page_breaks = 1
        self.__dialogue_sheet = 0
        self.__auto_style_outline = 0
        self.__outline_below = 0
        self.__outline_right = 0
        self.__fit_num_pages = 0
        self.__show_row_outline = 1
        self.__show_col_outline = 1
        self.__alt_expr_eval = 0
        self.__alt_formula_entries = 0

        self.__row_default_height = 0x00FF
        self.row_default_height_mismatch = 0
        self.row_default_hidden = 0
        self.row_default_space_above = 0
        self.row_default_space_below = 0

        self.__col_default_width = 0x0008

        self.__calc_mode = 1
        self.__calc_count = 0x0064
        self.__RC_ref_mode = 1
        self.__iterations_on = 0
        self.__delta = 0.001
        self.__save_recalc = 0

        self.__print_headers = 0
        self.__print_grid = 0
        self.__grid_set = 1
        self.__vert_page_breaks = []
        self.__horz_page_breaks = []
        self.__header_str = '&P'
        self.__footer_str = '&F'
        self.__print_centered_vert = 0
        self.__print_centered_horz = 1
        self.__left_margin = 0.3 #0.5
        self.__right_margin = 0.3 #0.5
        self.__top_margin = 0.61 #1.0
        self.__bottom_margin = 0.37 #1.0
        self.__paper_size_code = 9 # A4
        self.__print_scaling = 100
        self.__start_page_number = 1
        self.__fit_width_to_pages = 1
        self.__fit_height_to_pages = 1
        self.__print_in_rows = 1
        self.__portrait = 1
        self.__print_not_colour = 0
        self.__print_draft = 0
        self.__print_notes = 0
        self.__print_notes_at_end = 0
        self.__print_omit_errors = 0
        self.__print_hres = 0x012C # 300 dpi
        self.__print_vres = 0x012C # 300 dpi
        self.__header_margin = 0.1
        self.__footer_margin = 0.1
        self.__copies_num = 1

        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0
        self.__scen_protect = 0
        self.__password = ''

        self.last_used_row = 0
        self.first_used_row = 65535
        self.last_used_col = 0
        self.first_used_col = 255
        self.row_tempfile = None
        self.__flushed_rows = {}
        self.__row_visible_levels = 0

    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    def set_name(self, value):
        self.__name = value

    def get_name(self):
        return self.__name

    name = property(get_name, set_name)

    #################################################################

    def get_parent(self):
        return self.__parent

    parent = property(get_parent)

    #################################################################

    def get_rows(self):
        return self.__rows

    rows = property(get_rows)

    #################################################################

    def get_cols(self):
        return self.__cols

    cols = property(get_cols)

    #################################################################

    def get_merged_ranges(self):
        return self.__merged_ranges

    merged_ranges = property(get_merged_ranges)

    #################################################################

    def get_bmp_rec(self):
        return self.__bmp_rec

    bmp_rec = property(get_bmp_rec)

    #################################################################

    def set_show_formulas(self, value):
        self.__show_formulas = int(value)

    def get_show_formulas(self):
        return bool(self.__show_formulas)

    show_formulas = property(get_show_formulas, set_show_formulas)

    #################################################################

    def set_show_grid(self, value):
        self.__show_grid = int(value)

    def get_show_grid(self):
        return bool(self.__show_grid)

    show_grid = property(get_show_grid, set_show_grid)

    #################################################################

    def set_show_headers(self, value):
        self.__show_headers = int(value)

    def get_show_headers(self):
        return bool(self.__show_headers)

    show_headers = property(get_show_headers, set_show_headers)

    #################################################################

    def set_panes_frozen(self, value):
        self.__panes_frozen = int(value)

    def get_panes_frozen(self):
        return bool(self.__panes_frozen)

    panes_frozen = property(get_panes_frozen, set_panes_frozen)

    #################################################################

    ### def set_show_empty_as_zero(self, value):
    ###     self.__show_empty_as_zero = int(value)

    ### def get_show_empty_as_zero(self):
    ###     return bool(self.__show_empty_as_zero)

    ### show_empty_as_zero = property(get_show_empty_as_zero, set_show_empty_as_zero)

    #################################################################

    def set_auto_colour_grid(self, value):
        self.__auto_colour_grid = int(value)

    def get_auto_colour_grid(self):
        return bool(self.__auto_colour_grid)

    auto_colour_grid = property(get_auto_colour_grid, set_auto_colour_grid)

    #################################################################

    def set_cols_right_to_left(self, value):
        self.__cols_right_to_left = int(value)

    def get_cols_right_to_left(self):
        return bool(self.__cols_right_to_left)

    cols_right_to_left = property(get_cols_right_to_left, set_cols_right_to_left)

    #################################################################

    def set_show_outline(self, value):
        self.__show_outline = int(value)

    def get_show_outline(self):
        return bool(self.__show_outline)

    show_outline = property(get_show_outline, set_show_outline)

    #################################################################

    def set_remove_splits(self, value):
        self.__remove_splits = int(value)

    def get_remove_splits(self):
        return bool(self.__remove_splits)

    remove_splits = property(get_remove_splits, set_remove_splits)

    #################################################################

    def set_selected(self, value):
        self.__selected = int(value)

    def get_selected(self):
        return bool(self.__selected)

    selected = property(get_selected, set_selected)

    #################################################################

    def set_sheet_visible(self, value):
        self.__sheet_visible = int(value)

    def get_sheet_visible(self):
        return bool(self.__sheet_visible)

    sheet_visible = property(get_sheet_visible, set_sheet_visible)

    #################################################################

    def set_page_preview(self, value):
        self.__page_preview = int(value)

    def get_page_preview(self):
        return bool(self.__page_preview)

    page_preview = property(get_page_preview, set_page_preview)

    #################################################################

    def set_first_visible_row(self, value):
        self.__first_visible_row = value

    def get_first_visible_row(self):
        return self.__first_visible_row

    first_visible_row = property(get_first_visible_row, set_first_visible_row)

    #################################################################

    def set_first_visible_col(self, value):
        self.__first_visible_col = value

    def get_first_visible_col(self):
        return self.__first_visible_col

    first_visible_col = property(get_first_visible_col, set_first_visible_col)

    #################################################################

    def set_grid_colour(self, value):
        self.__grid_colour = value

    def get_grid_colour(self):
        return self.__grid_colour

    grid_colour = property(get_grid_colour, set_grid_colour)

    #################################################################

    def set_preview_magn(self, value):
        self.__preview_magn = value

    def get_preview_magn(self):
        return self.__preview_magn

    preview_magn = property(get_preview_magn, set_preview_magn)

    #################################################################

    def set_normal_magn(self, value):
        self.__normal_magn = value

    def get_normal_magn(self):
        return self.__normal_magn

    normal_magn = property(get_normal_magn, set_normal_magn)

    #################################################################

    def set_vert_split_pos(self, value):
        self.__vert_split_pos = abs(value)

    def get_vert_split_pos(self):
        return self.__vert_split_pos

    vert_split_pos = property(get_vert_split_pos, set_vert_split_pos)

    #################################################################

    def set_horz_split_pos(self, value):
        self.__horz_split_pos = abs(value)

    def get_horz_split_pos(self):
        return self.__horz_split_pos

    horz_split_pos = property(get_horz_split_pos, set_horz_split_pos)

    #################################################################

    def set_vert_split_first_visible(self, value):
        self.__vert_split_first_visible = abs(value)

    def get_vert_split_first_visible(self):
        return self.__vert_split_first_visible

    vert_split_first_visible = property(get_vert_split_first_visible, set_vert_split_first_visible)

    #################################################################

    def set_horz_split_first_visible(self, value):
        self.__horz_split_first_visible = abs(value)

    def get_horz_split_first_visible(self):
        return self.__horz_split_first_visible

    horz_split_first_visible = property(get_horz_split_first_visible, set_horz_split_first_visible)

    #################################################################

    #def set_split_active_pane(self, value):
    #    self.__split_active_pane = abs(value) & 0x03
    #
    #def get_split_active_pane(self):
    #    return self.__split_active_pane
    #
    #split_active_pane = property(get_split_active_pane, set_split_active_pane)

    #################################################################

    #def set_row_gut_width(self, value):
    #    self.__row_gut_width = value
    #
    #def get_row_gut_width(self):
    #    return self.__row_gut_width
    #
    #row_gut_width = property(get_row_gut_width, set_row_gut_width)
    #
    #################################################################
    #
    #def set_col_gut_height(self, value):
    #    self.__col_gut_height = value
    #
    #def get_col_gut_height(self):
    #    return self.__col_gut_height
    #
    #col_gut_height = property(get_col_gut_height, set_col_gut_height)
    #
    #################################################################

    def set_show_auto_page_breaks(self, value):
        self.__show_auto_page_breaks = int(value)

    def get_show_auto_page_breaks(self):
        return bool(self.__show_auto_page_breaks)

    show_auto_page_breaks = property(get_show_auto_page_breaks, set_show_auto_page_breaks)

    #################################################################

    def set_dialogue_sheet(self, value):
        self.__dialogue_sheet = int(value)

    def get_dialogue_sheet(self):
        return bool(self.__dialogue_sheet)

    dialogue_sheet = property(get_dialogue_sheet, set_dialogue_sheet)

    #################################################################

    def set_auto_style_outline(self, value):
        self.__auto_style_outline = int(value)

    def get_auto_style_outline(self):
        return bool(self.__auto_style_outline)

    auto_style_outline = property(get_auto_style_outline, set_auto_style_outline)

    #################################################################

    def set_outline_below(self, value):
        self.__outline_below = int(value)

    def get_outline_below(self):
        return bool(self.__outline_below)

    outline_below = property(get_outline_below, set_outline_below)

    #################################################################

    def set_outline_right(self, value):
        self.__outline_right = int(value)

    def get_outline_right(self):
        return bool(self.__outline_right)

    outline_right = property(get_outline_right, set_outline_right)

    #################################################################

    def set_fit_num_pages(self, value):
        self.__fit_num_pages = value

    def get_fit_num_pages(self):
        return self.__fit_num_pages

    fit_num_pages = property(get_fit_num_pages, set_fit_num_pages)

    #################################################################

    def set_show_row_outline(self, value):
        self.__show_row_outline = int(value)

    def get_show_row_outline(self):
        return bool(self.__show_row_outline)

    show_row_outline = property(get_show_row_outline, set_show_row_outline)

    #################################################################

    def set_show_col_outline(self, value):
        self.__show_col_outline = int(value)

    def get_show_col_outline(self):
        return bool(self.__show_col_outline)

    show_col_outline = property(get_show_col_outline, set_show_col_outline)

    #################################################################

    def set_alt_expr_eval(self, value):
        self.__alt_expr_eval = int(value)

    def get_alt_expr_eval(self):
        return bool(self.__alt_expr_eval)

    alt_expr_eval = property(get_alt_expr_eval, set_alt_expr_eval)

    #################################################################

    def set_alt_formula_entries(self, value):
        self.__alt_formula_entries = int(value)

    def get_alt_formula_entries(self):
        return bool(self.__alt_formula_entries)

    alt_formula_entries = property(get_alt_formula_entries, set_alt_formula_entries)

    #################################################################

    def set_row_default_height(self, value):
        self.__row_default_height = value

    def get_row_default_height(self):
        return self.__row_default_height

    row_default_height = property(get_row_default_height, set_row_default_height)

    #################################################################

    def set_col_default_width(self, value):
        self.__col_default_width = value

    def get_col_default_width(self):
        return self.__col_default_width

    col_default_width = property(get_col_default_width, set_col_default_width)

    #################################################################

    def set_calc_mode(self, value):
        self.__calc_mode = value & 0x03

    def get_calc_mode(self):
        return self.__calc_mode

    calc_mode = property(get_calc_mode, set_calc_mode)

    #################################################################

    def set_calc_count(self, value):
        self.__calc_count = value

    def get_calc_count(self):
        return self.__calc_count

    calc_count = property(get_calc_count, set_calc_count)

    #################################################################

    def set_RC_ref_mode(self, value):
        self.__RC_ref_mode = int(value)

    def get_RC_ref_mode(self):
        return bool(self.__RC_ref_mode)

    RC_ref_mode = property(get_RC_ref_mode, set_RC_ref_mode)

    #################################################################

    def set_iterations_on(self, value):
        self.__iterations_on = int(value)

    def get_iterations_on(self):
        return bool(self.__iterations_on)

    iterations_on = property(get_iterations_on, set_iterations_on)

    #################################################################

    def set_delta(self, value):
        self.__delta = value

    def get_delta(self):
        return self.__delta

    delta = property(get_delta, set_delta)

    #################################################################

    def set_save_recalc(self, value):
        self.__save_recalc = int(value)

    def get_save_recalc(self):
        return bool(self.__save_recalc)

    save_recalc = property(get_save_recalc, set_save_recalc)

    #################################################################

    def set_print_headers(self, value):
        self.__print_headers = int(value)

    def get_print_headers(self):
        return bool(self.__print_headers)

    print_headers = property(get_print_headers, set_print_headers)

    #################################################################

    def set_print_grid(self, value):
        self.__print_grid = int(value)

    def get_print_grid(self):
        return bool(self.__print_grid)

    print_grid = property(get_print_grid, set_print_grid)

    #################################################################
    #
    #def set_grid_set(self, value):
    #    self.__grid_set = int(value)
    #
    #def get_grid_set(self):
    #    return bool(self.__grid_set)
    #
    #grid_set = property(get_grid_set, set_grid_set)
    #
    #################################################################

    def set_vert_page_breaks(self, value):
        self.__vert_page_breaks = value

    def get_vert_page_breaks(self):
        return self.__vert_page_breaks

    vert_page_breaks = property(get_vert_page_breaks, set_vert_page_breaks)

    #################################################################

    def set_horz_page_breaks(self, value):
        self.__horz_page_breaks = value

    def get_horz_page_breaks(self):
        return self.__horz_page_breaks

    horz_page_breaks = property(get_horz_page_breaks, set_horz_page_breaks)

    #################################################################

    def set_header_str(self, value):
        if isinstance(value, str):
            value = unicode(value, self.__parent.encoding)
        self.__header_str = value

    def get_header_str(self):
        return self.__header_str

    header_str = property(get_header_str, set_header_str)

    #################################################################

    def set_footer_str(self, value):
        if isinstance(value, str):
            value = unicode(value, self.__parent.encoding)
        self.__footer_str = value

    def get_footer_str(self):
        return self.__footer_str

    footer_str = property(get_footer_str, set_footer_str)

    #################################################################

    def set_print_centered_vert(self, value):
        self.__print_centered_vert = int(value)

    def get_print_centered_vert(self):
        return bool(self.__print_centered_vert)

    print_centered_vert = property(get_print_centered_vert, set_print_centered_vert)

    #################################################################

    def set_print_centered_horz(self, value):
        self.__print_centered_horz = int(value)

    def get_print_centered_horz(self):
        return bool(self.__print_centered_horz)

    print_centered_horz = property(get_print_centered_horz, set_print_centered_horz)

    #################################################################

    def set_left_margin(self, value):
        self.__left_margin = value

    def get_left_margin(self):
        return self.__left_margin

    left_margin = property(get_left_margin, set_left_margin)

    #################################################################

    def set_right_margin(self, value):
        self.__right_margin = value

    def get_right_margin(self):
        return self.__right_margin

    right_margin = property(get_right_margin, set_right_margin)

    #################################################################

    def set_top_margin(self, value):
        self.__top_margin = value

    def get_top_margin(self):
        return self.__top_margin

    top_margin = property(get_top_margin, set_top_margin)

    #################################################################

    def set_bottom_margin(self, value):
        self.__bottom_margin = value

    def get_bottom_margin(self):
        return self.__bottom_margin

    bottom_margin = property(get_bottom_margin, set_bottom_margin)

    #################################################################

    def set_paper_size_code(self, value):
        self.__paper_size_code = value

    def get_paper_size_code(self):
        return self.__paper_size_code

    paper_size_code = property(get_paper_size_code, set_paper_size_code)

    #################################################################

    def set_print_scaling(self, value):
        self.__print_scaling = value

    def get_print_scaling(self):
        return self.__print_scaling

    print_scaling = property(get_print_scaling, set_print_scaling)

    #################################################################

    def set_start_page_number(self, value):
        self.__start_page_number = value

    def get_start_page_number(self):
        return self.__start_page_number

    start_page_number = property(get_start_page_number, set_start_page_number)

    #################################################################

    def set_fit_width_to_pages(self, value):
        self.__fit_width_to_pages = value

    def get_fit_width_to_pages(self):
        return self.__fit_width_to_pages

    fit_width_to_pages = property(get_fit_width_to_pages, set_fit_width_to_pages)

    #################################################################

    def set_fit_height_to_pages(self, value):
        self.__fit_height_to_pages = value

    def get_fit_height_to_pages(self):
        return self.__fit_height_to_pages

    fit_height_to_pages = property(get_fit_height_to_pages, set_fit_height_to_pages)

    #################################################################

    def set_print_in_rows(self, value):
        self.__print_in_rows = int(value)

    def get_print_in_rows(self):
        return bool(self.__print_in_rows)

    print_in_rows = property(get_print_in_rows, set_print_in_rows)

    #################################################################

    def set_portrait(self, value):
        self.__portrait = int(value)

    def get_portrait(self):
        return bool(self.__portrait)

    portrait = property(get_portrait, set_portrait)

    #################################################################

    def set_print_colour(self, value):
        self.__print_not_colour = int(not value)

    def get_print_colour(self):
        return not bool(self.__print_not_colour)

    print_colour = property(get_print_colour, set_print_colour)

    #################################################################

    def set_print_draft(self, value):
        self.__print_draft = int(value)

    def get_print_draft(self):
        return bool(self.__print_draft)

    print_draft = property(get_print_draft, set_print_draft)

    #################################################################

    def set_print_notes(self, value):
        self.__print_notes = int(value)

    def get_print_notes(self):
        return bool(self.__print_notes)

    print_notes = property(get_print_notes, set_print_notes)

    #################################################################

    def set_print_notes_at_end(self, value):
        self.__print_notes_at_end = int(value)

    def get_print_notes_at_end(self):
        return bool(self.__print_notes_at_end)

    print_notes_at_end = property(get_print_notes_at_end, set_print_notes_at_end)

    #################################################################

    def set_print_omit_errors(self, value):
        self.__print_omit_errors = int(value)

    def get_print_omit_errors(self):
        return bool(self.__print_omit_errors)

    print_omit_errors = property(get_print_omit_errors, set_print_omit_errors)

    #################################################################

    def set_print_hres(self, value):
        self.__print_hres = value

    def get_print_hres(self):
        return self.__print_hres

    print_hres = property(get_print_hres, set_print_hres)

    #################################################################

    def set_print_vres(self, value):
        self.__print_vres = value

    def get_print_vres(self):
        return self.__print_vres

    print_vres = property(get_print_vres, set_print_vres)

    #################################################################

    def set_header_margin(self, value):
        self.__header_margin = value

    def get_header_margin(self):
        return self.__header_margin

    header_margin = property(get_header_margin, set_header_margin)

    #################################################################

    def set_footer_margin(self, value):
        self.__footer_margin = value

    def get_footer_margin(self):
        return self.__footer_margin

    footer_margin = property(get_footer_margin, set_footer_margin)

    #################################################################

    def set_copies_num(self, value):
        self.__copies_num = value

    def get_copies_num(self):
        return self.__copies_num

    copies_num = property(get_copies_num, set_copies_num)

    ##################################################################

    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)

    #################################################################

    def set_scen_protect(self, value):
        self.__scen_protect = int(value)

    def get_scen_protect(self):
        return bool(self.__scen_protect)

    scen_protect = property(get_scen_protect, set_scen_protect)

    #################################################################

    def set_password(self, value):
        self.__password = value

    def get_password(self):
        return self.__password

    password = property(get_password, set_password)

    ##################################################################
    ## Methods
    ##################################################################

    def get_parent(self):
        return self.__parent

    def write(self, r, c, label="", style=Style.default_style):
        self.row(r).write(c, label, style)

    def merge(self, r1, r2, c1, c2, style=Style.default_style):
        # Stand-alone merge of previously written cells.
        # Problems: (1) style to be used should be existing style of
        # the top-left cell, not an arg.
        # (2) should ensure that any previous data value in
        # non-top-left cells is nobbled.
        # Note: if a cell is set by a data record then later
        # is referenced by a [MUL]BLANK record, Excel will blank
        # out the cell on the screen, but OOo & Gnu will not
        # blank it out. Need to do something better than writing
        # multiple records. In the meantime, avoid this method and use
        # write_merge() instead.
        if c2 > c1:
            self.row(r1).write_blanks(c1 + 1, c2,  style)
        for r in range(r1+1, r2+1):
            self.row(r).write_blanks(c1, c2,  style)
        self.__merged_ranges.append((r1, r2, c1, c2))

    def write_merge(self, r1, r2, c1, c2, label="", style=Style.default_style):
        assert 0 <= c1 <= c2 <= 255
        assert 0 <= r1 <= r2 <= 65535
        self.write(r1, c1, label, style)
        if c2 > c1:
            self.row(r1).write_blanks(c1 + 1, c2,  style) # skip (r1, c1)
        for r in range(r1+1, r2+1):
            self.row(r).write_blanks(c1, c2,  style)
        self.__merged_ranges.append((r1, r2, c1, c2))

    def insert_bitmap(self, filename, row, col, x = 0, y = 0, scale_x = 1, scale_y = 1):
        bmp = Bitmap.ImDataBmpRecord(filename)
        obj = Bitmap.ObjBmpRecord(row, col, self, bmp, x, y, scale_x, scale_y)

        self.__bmp_rec += obj.get() + bmp.get()

    def col(self, indx):
        if indx not in self.__cols:
            self.__cols[indx] = self.Column(indx, self)
        return self.__cols[indx]

    def row(self, indx):
        if indx not in self.__rows:
            if indx in self.__flushed_rows:
                raise Exception("Attempt to reuse row index %d of sheet %r after flushing" % (indx, self.__name))
            self.__rows[indx] = self.Row(indx, self)
            if indx > self.last_used_row:
                self.last_used_row = indx
            if indx < self.first_used_row:
                self.first_used_row = indx
        return self.__rows[indx]

    def row_height(self, row): # in pixels
        if row in self.__rows:
            return self.__rows[row].get_height_in_pixels()
        else:
            return 17

    def col_width(self, col): # in pixels
        if col in self.__cols:
            return self.__cols[col].width_in_pixels()
        else:
            return 64


    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.WORKSHEET).get()

    def __update_row_visible_levels(self):
        if self.__rows:
            temp = max([self.__rows[r].level for r in self.__rows]) + 1
            self.__row_visible_levels = max(temp, self.__row_visible_levels)

    def __guts_rec(self):
        self.__update_row_visible_levels()
        col_visible_levels = 0
        if len(self.__cols) != 0:
            col_visible_levels = max([self.__cols[c].level for c in self.__cols]) + 1
        return BIFFRecords.GutsRecord(
            self.__row_gut_width, self.__col_gut_height, self.__row_visible_levels, col_visible_levels).get()

    def __defaultrowheight_rec(self):
        options = 0x0000
        options |= (self.row_default_height_mismatch & 1) << 0
        options |= (self.row_default_hidden & 1) << 1
        options |= (self.row_default_space_above & 1) << 2
        options |= (self.row_default_space_below & 1) << 3
        defht = self.__row_default_height
        return BIFFRecords.DefaultRowHeightRecord(options, defht).get()

    def __wsbool_rec(self):
        options = 0x00
        options |= (self.__show_auto_page_breaks & 0x01) << 0
        options |= (self.__dialogue_sheet & 0x01) << 4
        options |= (self.__auto_style_outline & 0x01) << 5
        options |= (self.__outline_below & 0x01) << 6
        options |= (self.__outline_right & 0x01) << 7
        options |= (self.__fit_num_pages & 0x01) << 8
        options |= (self.__show_row_outline & 0x01) << 10
        options |= (self.__show_col_outline & 0x01) << 11
        options |= (self.__alt_expr_eval & 0x01) << 14
        options |= (self.__alt_formula_entries & 0x01) << 15

        return BIFFRecords.WSBoolRecord(options).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()

    def __colinfo_rec(self):
        result = ''
        for col in self.__cols:
            result += self.__cols[col].get_biff_record()
        return result

    def __dimensions_rec(self):
        return BIFFRecords.DimensionsRecord(
            self.first_used_row, self.last_used_row,
            self.first_used_col, self.last_used_col
            ).get()

    def __window2_rec(self):
        # Appends SCL record.
        options = 0
        options |= (self.__show_formulas        & 0x01) << 0
        options |= (self.__show_grid            & 0x01) << 1
        options |= (self.__show_headers         & 0x01) << 2
        options |= (self.__panes_frozen         & 0x01) << 3
        options |= (self.show_zero_values       & 0x01) << 4
        options |= (self.__auto_colour_grid     & 0x01) << 5
        options |= (self.__cols_right_to_left   & 0x01) << 6
        options |= (self.__show_outline         & 0x01) << 7
        options |= (self.__remove_splits        & 0x01) << 8
        options |= (self.__selected             & 0x01) << 9
        options |= (self.__sheet_visible        & 0x01) << 10
        options |= (self.__page_preview         & 0x01) << 11
        if self.__page_preview:
            scl_magn = self.__preview_magn
        else:
            scl_magn = self.__normal_magn
        return BIFFRecords.Window2Record(
            options, self.__first_visible_row, self.__first_visible_col,
            self.__grid_colour,
            self.__preview_magn, self.__normal_magn, scl_magn).get()

    def __panes_rec(self):
        if self.__vert_split_pos is None and self.__horz_split_pos is None:
            return ""

        if self.__vert_split_pos is None:
            self.__vert_split_pos = 0
        if self.__horz_split_pos is None:
            self.__horz_split_pos = 0

        if self.__panes_frozen:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = self.__vert_split_pos
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = self.__horz_split_pos
        else:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = 0
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = 0
            # inspired by pyXLWriter
            self.__horz_split_pos = 20*self.__horz_split_pos + 255
            self.__vert_split_pos = 113.879*self.__vert_split_pos + 390

        if self.__vert_split_pos > 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 0
        elif self.__vert_split_pos > 0 and self.__horz_split_pos == 0:
            self.__split_active_pane = 1
        elif self.__vert_split_pos == 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 2
        else:
            self.__split_active_pane = 3

        result = BIFFRecords.PanesRecord(self.__vert_split_pos,
                                         self.__horz_split_pos,
                                         self.__horz_split_first_visible,
                                         self.__vert_split_first_visible,
                                         self.__split_active_pane).get()
        return result

    def __row_blocks_rec(self):
        result = []
        for row in self.__rows.itervalues():
            result.append(row.get_row_biff_data())
            result.append(row.get_cells_biff_data())
        return ''.join(result)

    def __merged_rec(self):
        return BIFFRecords.MergedCellsRecord(self.__merged_ranges).get()

    def __bitmaps_rec(self):
        return self.__bmp_rec

    def __calc_settings_rec(self):
        result = ''
        result += BIFFRecords.CalcModeRecord(self.__calc_mode & 0x01).get()
        result += BIFFRecords.CalcCountRecord(self.__calc_count & 0xFFFF).get()
        result += BIFFRecords.RefModeRecord(self.__RC_ref_mode & 0x01).get()
        result += BIFFRecords.IterationRecord(self.__iterations_on & 0x01).get()
        result += BIFFRecords.DeltaRecord(self.__delta).get()
        result += BIFFRecords.SaveRecalcRecord(self.__save_recalc & 0x01).get()
        return result

    def __print_settings_rec(self):
        result = ''
        result += BIFFRecords.PrintHeadersRecord(self.__print_headers).get()
        result += BIFFRecords.PrintGridLinesRecord(self.__print_grid).get()
        result += BIFFRecords.GridSetRecord(self.__grid_set).get()
        result += BIFFRecords.HorizontalPageBreaksRecord(self.__horz_page_breaks).get()
        result += BIFFRecords.VerticalPageBreaksRecord(self.__vert_page_breaks).get()
        result += BIFFRecords.HeaderRecord(self.__header_str).get()
        result += BIFFRecords.FooterRecord(self.__footer_str).get()
        result += BIFFRecords.HCenterRecord(self.__print_centered_horz).get()
        result += BIFFRecords.VCenterRecord(self.__print_centered_vert).get()
        result += BIFFRecords.LeftMarginRecord(self.__left_margin).get()
        result += BIFFRecords.RightMarginRecord(self.__right_margin).get()
        result += BIFFRecords.TopMarginRecord(self.__top_margin).get()
        result += BIFFRecords.BottomMarginRecord(self.__bottom_margin).get()

        setup_page_options =  (self.__print_in_rows & 0x01) << 0
        setup_page_options |=  (self.__portrait & 0x01) << 1
        setup_page_options |=  (0x00 & 0x01) << 2
        setup_page_options |=  (self.__print_not_colour & 0x01) << 3
        setup_page_options |=  (self.__print_draft & 0x01) << 4
        setup_page_options |=  (self.__print_notes & 0x01) << 5
        setup_page_options |=  (0x00 & 0x01) << 6
        setup_page_options |=  (0x01 & 0x01) << 7
        setup_page_options |=  (self.__print_notes_at_end & 0x01) << 9
        setup_page_options |=  (self.__print_omit_errors & 0x03) << 10

        result += BIFFRecords.SetupPageRecord(self.__paper_size_code,
                                self.__print_scaling,
                                self.__start_page_number,
                                self.__fit_width_to_pages,
                                self.__fit_height_to_pages,
                                setup_page_options,
                                self.__print_hres,
                                self.__print_vres,
                                self.__header_margin,
                                self.__footer_margin,
                                self.__copies_num).get()
        return result

    def __protection_rec(self):
        result = ''
        result += BIFFRecords.ProtectRecord(self.__protect).get()
        result += BIFFRecords.ScenProtectRecord(self.__scen_protect).get()
        result += BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()
        result += BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()
        result += BIFFRecords.PasswordRecord(self.__password).get()
        return result

    def get_biff_data(self):
        result = [
            self.__bof_rec(),
            self.__calc_settings_rec(),
            self.__guts_rec(),
            self.__defaultrowheight_rec(),
            self.__wsbool_rec(),
            self.__colinfo_rec(),
            self.__dimensions_rec(),
            self.__print_settings_rec(),
            self.__protection_rec(),
            ]
        if self.row_tempfile:
            self.row_tempfile.flush()
            self.row_tempfile.seek(0)
            result.append(self.row_tempfile.read())
        result.extend([
            self.__row_blocks_rec(),
            self.__merged_rec(),
            self.__bitmaps_rec(),
            self.__window2_rec(),
            self.__panes_rec(),
            self.__eof_rec(),
            ])
        return ''.join(result)

    def flush_row_data(self):
        if self.row_tempfile is None:
            self.row_tempfile = tempfile.TemporaryFile()
        self.row_tempfile.write(self.__row_blocks_rec())
        for rowx in self.__rows:
            self.__flushed_rows[rowx] = 1
        self.__update_row_visible_levels()
        self.__rows = {}

########NEW FILE########
__FILENAME__ = forms
from django import forms
from django.conf import settings
from django.contrib.auth.models import User
from django.utils.translation import ugettext_lazy as _

from scrum_log.models import ScrumLog
from django.forms.extras.widgets import SelectDateWidget


class ScrumLogForm(forms.ModelForm):
    def __init__(self,  *args, **kwargs):
        super(ScrumLogForm, self).__init__(*args, **kwargs)
        # self.fields['include_in_velocity'].label = "Include In Velocity Calculations"
    class Meta:
        model = ScrumLog
        fields = ('message', 'message_type', 'flagged' )

########NEW FILE########
__FILENAME__ = models
from datetime import datetime, date
import time
import re

from django.conf import settings
from django.core.urlresolvers import reverse
from django.db import models
from django.contrib.auth.models import User
from organizations.models import Organization
# from projects.models import Project, Iteration, Story
from django.template.loader import render_to_string

import logging

logger = logging.getLogger(__name__)

from avatar.templatetags.avatar_tags import avatar_url

class ScrumLog(models.Model):
    TYPE_NOTE=0
    TYPE_GROUP_NOTE=1
    TYPE_SOURCE_CONTROL=2
    GITHUB_ICON=2
    
    creator = models.ForeignKey(User, null=True)
    project = models.ForeignKey("projects.Project", related_name="log_items" )
    date = models.DateTimeField(auto_now_add=True )
    message = models.TextField()
    message_type = models.IntegerField()
    icon = models.IntegerField(default=1)
    flagged = models.BooleanField(default=False)
    def icon_url( self ):
        if self.icon == ScrumLog.GITHUB_ICON:
            return "%simages/octocat.png" % settings.SSL_STATIC_URL 
        if self.message_type == ScrumLog.TYPE_GROUP_NOTE:
            return "%spinax/images/silk/icons/group.png" % settings.SSL_STATIC_URL 
        return avatar_url(self.creator, 16)
        
    def render_text(self):
        return render_to_string("scrum_log/scrum_log_newsfeed.html",{"entry":self,"project":self.project})


# post_save.connect(add_attachment_extra, sender=Attachment)
# post_save.connect(add_subscription, sender=Organization)
# pre_delete.connect(organization_deleted, sender=Organization)

########NEW FILE########
__FILENAME__ = scrum_log_tags
from django import template
from django.core.urlresolvers import reverse
from django.utils.safestring import mark_safe
from django.utils.html import escape
from projects.models import Project
from scrum_log.forms import ScrumLogForm

register = template.Library()

@register.inclusion_tag('scrum_log/scrum_log.html')
def scrum_log(project):    
    return {"project":project}

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *
from django.views.generic.simple import direct_to_template


urlpatterns = patterns('scrum_log.views',
    url(r'^(?P<project_slug>[-\w]+)/add_scrum_log_ajax$', "add_scrum_log_ajax", name="add_scrum_log_ajax"),
    url(r'^(?P<project_slug>[-\w]+)/entries/(?P<page>[0-9]+)$', "entries", name="entries"),
    url(r'^(?P<id>[0-9]+)/delete$', "delete_log_entry", name="delete_log_entry"),
)

########NEW FILE########
__FILENAME__ = views
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponseRedirect, HttpResponse
from django.core.urlresolvers import reverse
from django.core.paginator import Paginator, InvalidPage
from django.contrib.auth.decorators import login_required
from django.contrib.auth.models import User
from django.core import serializers

from projects.models import Project
from projects.access import *
from models import ScrumLog

import json
import datetime
import logging

logger = logging.getLogger(__name__)

@login_required
def delete_log_entry(request, id):
    entry = get_object_or_404( ScrumLog, id=id )
    write_access_or_403(entry.project, request.user )
    if (entry.creator != request.user) and not has_admin_access(entry.project, request.user):
        return HttpResponse("FAILED")
    entry.delete()
    return HttpResponse("OK")
    
@login_required
def entries(request, project_slug, page):
    project = get_object_or_404( Project, slug=project_slug )
    read_access_or_403(project, request.user )
    items = project.log_items.all().order_by("-date")
    logger.debug(items.count())
    pages = Paginator(items, 20)
    
    if items.count() == 0:
        return render_to_response("scrum_log/empty_scrum_log.html", 
                                  {},
                                  context_instance=RequestContext(request))
        
    
    try:
        page_items = pages.page(page)
    except:
        return HttpResponse("")        
    
    return render_to_response("scrum_log/scrum_log_entries.html", 
                              {
                                "entries":page_items.object_list,
                                "project":project,
                                "page":page
                              },
                              context_instance=RequestContext(request))

@login_required
def add_scrum_log_ajax(request, project_slug):
    project = get_object_or_404( Project, slug=project_slug )
    write_access_or_403(project, request.user )
    message = request.POST.get("message")
    if message==None or len(message) == 0:
        return HttpResponse("FAILED")
    message_type = 1 if (request.POST.get("group") == "on" ) else 0
    flagged = request.POST.get("flagged") == "on"
    logger.debug(request.POST.get("flagged"))
    log = ScrumLog(creator=request.user, 
                   project=project,
                   message=message,
                   message_type=message_type,
                   flagged=flagged )
    log.save()
    return HttpResponse("OK")
########NEW FILE########
__FILENAME__ = models
from django.db import models

# Create your models here.

########NEW FILE########
__FILENAME__ = extra_tagging_tags
from django.template import Library
from django.conf import settings

register = Library()

@register.inclusion_tag("tag_app/tag_list.html")
def show_tags_for(obj, group=None):
    return {
        "obj": obj,
        "group": group,
        "MEDIA_URL": settings.MEDIA_URL,
        "STATIC_URL": settings.SSL_STATIC_URL,
    }

@register.inclusion_tag("tag_app/tag_count_list.html")
def show_tag_counts(tag_counts):
    return {"tag_counts": tag_counts}

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *

urlpatterns = patterns('',
    # all tags
    url(r'^(?P<tag>.+)/$', 'tag_app.views.tags', name='tag_results'),
)

########NEW FILE########
__FILENAME__ = views
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext

from blog.models import Post
from tagging.models import Tag, TaggedItem
from photos.models import Image
from projects.models import Project, Task
from projects.models import Topic as ProjectTopic

from tribes.models import Tribe
from tribes.models import Topic as TribeTopic

from wiki.models import Article as WikiArticle

def tags(request, tag, template_name='tags/index.html'):
    tag = get_object_or_404(Tag, name=tag)

    alltags = TaggedItem.objects.get_by_model(Post, tag).filter(status=2)

    phototags = TaggedItem.objects.get_by_model(Image, tag)


    project_tags = TaggedItem.objects.get_by_model(Project, tag).filter(deleted=False)
    project_topic_tags = TaggedItem.objects.get_by_model(ProjectTopic, tag).filter(project__deleted=False)
    project_task_tags = TaggedItem.objects.get_by_model(Task, tag).filter(project__deleted=False)

    tribe_tags = TaggedItem.objects.get_by_model(Tribe, tag).filter(deleted=False)
    tribe_topic_tags = TaggedItem.objects.get_by_model(TribeTopic, tag).filter(tribe__deleted=False)

    # @@@ TODO: tribe_wiki_article_tags and project_wiki_article_tags
    wiki_article_tags = TaggedItem.objects.get_by_model(WikiArticle, tag)

    return render_to_response(template_name, {
        'tag': tag,
        'alltags': alltags,
        'phototags': phototags,
        'project_tags': project_tags,
        'project_topic_tags': project_topic_tags,
        'project_task_tags': project_task_tags,
        'tribe_tags': tribe_tags,
        'tribe_topic_tags': tribe_topic_tags,
        'wiki_article_tags': wiki_article_tags,
    }, context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = models
from django.core.urlresolvers import reverse
from django.contrib.auth.models import  User
from django.utils.translation import ugettext_lazy as _
from django.db import models



class Tip(models.Model):
    approved = models.BooleanField(  )
    tip_text = models.CharField( max_length=200 )
    creator = models.ForeignKey( User )

########NEW FILE########
__FILENAME__ = urls

########NEW FILE########
__FILENAME__ = views

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import *

urlpatterns = patterns('tutorial.views',
    url(r'^(?P<name>\w+)/print', 'tutorial_print', name="tutorial_print"),
    url(r'^(?P<name>\w+)/(?P<page>\w+)', 'tutorial', name="tutorial_page"),
)

########NEW FILE########
__FILENAME__ = views
from django.shortcuts import render_to_response
from django.http import Http404
from django.template import RequestContext,TemplateDoesNotExist
from django.template.loader import get_template

from os.path import isfile, join

# syntax is name_of_tutorial: {url_of_page: name_of_page...
# this must match the templates found in tutorial/ . ie tutorial/name_of_tutorial/url_of_page.html
tutorials = { "scrumdo":[("start","What Is ScrumDo?"),
                         ("organizations", "Organizations"),
                         ("projects", "Projects"),
                         ("iterations", "Iteration Planning"),
                         ("basecamp", "Basecamp Integration"),
                         ],


              "scrum":[("start", "Start"),
                       ("roles", "Roles"),
                       ("backlog", "Backlog"),
                       ("planning", "Planning"),
                       ("stories", "Stories"),
                       ("releases", "Releases")],
              }

def list_has_key(lis,key):
    for k,v in lis:
        if k == key:
            return True
    return False

def list_get(lis, key):
    for k,v in lis:
        if k == key:
            return (k,v)
    return None


def tutorial(request, name, page="start"):
    def mk_path(page):
        return join("tutorial", name, page+".html")
    if not tutorials.has_key(name) or not list_has_key(tutorials[name], page) :
        raise Http404

    return render_to_response("tutorial/base.html", {
            "page_template": mk_path(page),
            "name": name,
            "this": list_get(tutorials[name], page),
            "pages": tutorials[name],
    }, context_instance=RequestContext(request))

def tutorial_print(request, name):
    if not tutorials.has_key(name):
        raise Http404

    return render_to_response("tutorial/print.html", {
            "page_templates" : map(lambda (x,y): join("tutorial", name, x + ".html"), tutorials[name]),
            "name":name,
            "this": ("print", "Print Version"),
            "pages": tutorials[name],
            },
                              context_instance=RequestContext(request))

########NEW FILE########
__FILENAME__ = account_tests
#!/usr/bin/python
# -*- coding: latin-1 -*-


from selenium import selenium
import unittest, time, re
import time

class AccountTests(unittest.TestCase):
    def setUp(self):
        self.verificationErrors = []
        self.selenium = selenium("localhost", 4444, "*chrome", "http://localhost:8000/")
        self.selenium.start()

    def test_sign_up(self):
        sel = self.selenium
        sel.open("/")
        sel.click("link=Sign Up")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user")
        sel.type("id_password1", "selenium_password")
        sel.type("id_password2", "selenium_password")
        sel.type("id_email", "selenium@scrumdo.com")
        sel.click(u"//input[@type='submit']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Successfully logged in as selenium_user"))
        except AssertionError, e: self.verificationErrors.append(str(e))

        sel.click("link=Logout")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Get Scrum Done with ScrumDo"))
        except AssertionError, e: self.verificationErrors.append(str(e))

    def test_organization_create(self):
        sel = self.selenium
        sel.open("/")
        sel.click("link=Login")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user")
        sel.type("id_password", "selenium_password")
        sel.click(u"//input[@type='submit']")
        sel.wait_for_page_to_load("30000")
        sel.click("css=#organizations-box .box-top .buttons a")
        sel.wait_for_page_to_load("30000")
        sel.type("id_name", "selenium test org")
        sel.type("id_slug", "selenium_test_organi")
        sel.type("id_description", "selenium_test_organization description.")
        sel.click("//button[@type='submit']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Organization Created."))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.click("link=New Project")
        sel.wait_for_page_to_load("30000")
        sel.type("id_slug", "selenium_test_proj")
        sel.type("id_name", "Test project from automated tests")
        sel.type("id_description", "Test project from automated tests")
        sel.click("//button[@type='submit']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Project Created"))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.click("link=Logout")
        sel.wait_for_page_to_load("30000")

    def test_login(self):
        sel = self.selenium
        sel.open("/")
        sel.click("link=Login")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user")
        sel.type("id_password", "selenium_password")
        sel.click("id_remember")
        sel.click(u"//input[@type='submit']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Successfully logged in"))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.click("link=Logout")
        sel.wait_for_page_to_load("30000")

    def tearDown(self):
        self.selenium.stop()
        self.assertEqual([], self.verificationErrors)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = homepage
from selenium import selenium
import unittest, time, re


class HomepageTest(unittest.TestCase):
    def setUp(self):
        self.verificationErrors = []
        self.selenium = selenium("localhost", 4444, "*chrome", "http://localhost:8000/")
        self.selenium.start()

    def test_homepage(self):
        sel = self.selenium
        sel.open("/")
        try: self.failUnless(sel.is_text_present("Get Scrum Done with ScrumDo"))
        except AssertionError, e: self.verificationErrors.append(str(e))

    def tearDown(self):
        self.selenium.stop()
        self.assertEqual([], self.verificationErrors)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = newsfeed_tests
#!/usr/bin/python
# -*- coding: latin-1 -*-


from selenium import selenium
import unittest, time, re
import time

class NewsfeedTests(unittest.TestCase):
    def setUp(self):
        self.verificationErrors = []
        self.selenium = selenium("localhost", 4444, "*chrome", "http://localhost:8000/")
        self.selenium.start()

    def test_create_story(self):
        sel = self.selenium
        sel.open("/")
        sel.click("link=Login")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user")
        sel.type("id_password", "selenium_password")
        sel.click(u"//input[@type='submit']")
        sel.wait_for_page_to_load("30000")
        sel.click("link=Test project from automated tests")
        sel.wait_for_page_to_load("30000")
        sel.type("id_summary", "Test story from automated tests")
        sel.click(u"//*[@id='add_button']")
        sel.do_command('waitForElementPresent', ("//*[@id='createdStories']/li", ))
        sel.click(u"link=selenium_user")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("Test story from automated tests"))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.click("link=Logout")
        sel.wait_for_page_to_load("30000")

    def test_newsfeed_activity_create(self):
        sel = self.selenium
        sel.open("/")
        sel.click("link=Sign Up")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user2")
        sel.type("id_password1", "selenium_password")
        sel.type("id_password2", "selenium_password")
        sel.type("id_email", "test@dbp.mm.st")
        sel.click(u"//input[@value='Sign Up ']")
        sel.wait_for_page_to_load("30000")
        sel.click("//div[@id='body']/div[2]/a[1]/h2")
        sel.wait_for_page_to_load("30000")
        sel.type("id_slug", "selenium_test_projec")
        sel.type("id_name", "Selenium Test Project")
        sel.type("id_description", "Selenium Test Project Description.")
        sel.click("//button[@type='submit']")
        sel.wait_for_page_to_load("30000")
        sel.type("id_recipient", "selenium_user")
        sel.click("//button[@type='submit']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("selenium_user"))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.type("id_summary", "Test story to see if newsfeed is working.")
        sel.click("add_button")
        sel.click("link=selenium_user2")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("created Test story to see if newsfeed is working"))
        except AssertionError, e: self.verificationErrors.append(str(e))
        sel.click("link=Logout")
        sel.wait_for_page_to_load("30000")
        sel.click("link=Login")
        sel.wait_for_page_to_load("30000")
        sel.type("id_username", "selenium_user")
        sel.type("id_password", "selenium_password")
        sel.click(u"//input[@value='Log in ']")
        sel.wait_for_page_to_load("30000")
        try: self.failUnless(sel.is_text_present("created Test story to see if newsfeed is working"))
        except AssertionError, e: self.verificationErrors.append(str(e))


    def tearDown(self):
        self.selenium.stop()
        self.assertEqual([], self.verificationErrors)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = selenium

"""
Copyright 2006 ThoughtWorks, Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
__docformat__ = "restructuredtext en"

# This file has been automatically generated via XSL

import httplib
import urllib
import re

class selenium:
    """
    Defines an object that runs Selenium commands.

    Element Locators
    ~~~~~~~~~~~~~~~~

    Element Locators tell Selenium which HTML element a command refers to.
    The format of a locator is:

    \ *locatorType*\ **=**\ \ *argument*


    We support the following strategies for locating elements:


    *   \ **identifier**\ =\ *id*:
        Select the element with the specified @id attribute. If no match is
        found, select the first element whose @name attribute is \ *id*.
        (This is normally the default; see below.)
    *   \ **id**\ =\ *id*:
        Select the element with the specified @id attribute.
    *   \ **name**\ =\ *name*:
        Select the first element with the specified @name attribute.

        *   username
        *   name=username


        The name may optionally be followed by one or more \ *element-filters*, separated from the name by whitespace.  If the \ *filterType* is not specified, \ **value**\  is assumed.

        *   name=flavour value=chocolate


    *   \ **dom**\ =\ *javascriptExpression*:

        Find an element by evaluating the specified string.  This allows you to traverse the HTML Document Object
        Model using JavaScript.  Note that you must not return a value in this string; simply make it the last expression in the block.

        *   dom=document.forms['myForm'].myDropdown
        *   dom=document.images[56]
        *   dom=function foo() { return document.links[1]; }; foo();


    *   \ **xpath**\ =\ *xpathExpression*:
        Locate an element using an XPath expression.

        *   xpath=//img[@alt='The image alt text']
        *   xpath=//table[@id='table1']//tr[4]/td[2]
        *   xpath=//a[contains(@href,'#id1')]
        *   xpath=//a[contains(@href,'#id1')]/@class
        *   xpath=(//table[@class='stylee'])//th[text()='theHeaderText']/../td
        *   xpath=//input[@name='name2' and @value='yes']
        *   xpath=//\*[text()="right"]


    *   \ **link**\ =\ *textPattern*:
        Select the link (anchor) element which contains text matching the
        specified \ *pattern*.

        *   link=The link text


    *   \ **css**\ =\ *cssSelectorSyntax*:
        Select the element using css selectors. Please refer to CSS2 selectors, CSS3 selectors for more information. You can also check the TestCssLocators test in the selenium test suite for an example of usage, which is included in the downloaded selenium core package.

        *   css=a[href="#id3"]
        *   css=span#firstChild + span


        Currently the css selector locator supports all css1, css2 and css3 selectors except namespace in css3, some pseudo classes(:nth-of-type, :nth-last-of-type, :first-of-type, :last-of-type, :only-of-type, :visited, :hover, :active, :focus, :indeterminate) and pseudo elements(::first-line, ::first-letter, ::selection, ::before, ::after).

    *   \ **ui**\ =\ *uiSpecifierString*:
        Locate an element by resolving the UI specifier string to another locator, and evaluating it. See the Selenium UI-Element Reference for more details.

        *   ui=loginPages::loginButton()
        *   ui=settingsPages::toggle(label=Hide Email)
        *   ui=forumPages::postBody(index=2)//a[2]





    Without an explicit locator prefix, Selenium uses the following default
    strategies:


    *   \ **dom**\ , for locators starting with "document."
    *   \ **xpath**\ , for locators starting with "//"
    *   \ **identifier**\ , otherwise

    Element Filters
    ~~~~~~~~~~~~~~~

    Element filters can be used with a locator to refine a list of candidate elements.  They are currently used only in the 'name' element-locator.

    Filters look much like locators, ie.

    \ *filterType*\ **=**\ \ *argument*

    Supported element-filters are:

    \ **value=**\ \ *valuePattern*


    Matches elements based on their values.  This is particularly useful for refining a list of similarly-named toggle-buttons.

    \ **index=**\ \ *index*


    Selects a single element based on its position in the list (offset from zero).

    String-match Patterns
    ~~~~~~~~~~~~~~~~~~~~~

    Various Pattern syntaxes are available for matching string values:


    *   \ **glob:**\ \ *pattern*:
        Match a string against a "glob" (aka "wildmat") pattern. "Glob" is a
        kind of limited regular-expression syntax typically used in command-line
        shells. In a glob pattern, "\*" represents any sequence of characters, and "?"
        represents any single character. Glob patterns match against the entire
        string.
    *   \ **regexp:**\ \ *regexp*:
        Match a string using a regular-expression. The full power of JavaScript
        regular-expressions is available.
    *   \ **regexpi:**\ \ *regexpi*:
        Match a string using a case-insensitive regular-expression.
    *   \ **exact:**\ \ *string*:

        Match a string exactly, verbatim, without any of that fancy wildcard
        stuff.



    If no pattern prefix is specified, Selenium assumes that it's a "glob"
    pattern.



    For commands that return multiple values (such as verifySelectOptions),
    the string being matched is a comma-separated list of the return values,
    where both commas and backslashes in the values are backslash-escaped.
    When providing a pattern, the optional matching syntax (i.e. glob,
    regexp, etc.) is specified once, as usual, at the beginning of the
    pattern.


    """

### This part is hard-coded in the XSL
    def __init__(self, host, port, browserStartCommand, browserURL):
        self.host = host
        self.port = port
        self.browserStartCommand = browserStartCommand
        self.browserURL = browserURL
        self.sessionId = None
        self.extensionJs = ""

    def setExtensionJs(self, extensionJs):
        self.extensionJs = extensionJs

    def start(self):
        result = self.get_string("getNewBrowserSession", [self.browserStartCommand, self.browserURL, self.extensionJs])
        try:
            self.sessionId = result
        except ValueError:
            raise Exception, result

    def stop(self):
        self.do_command("testComplete", [])
        self.sessionId = None

    def do_command(self, verb, args):
        conn = httplib.HTTPConnection(self.host, self.port)
        body = u'cmd=' + urllib.quote_plus(unicode(verb).encode('utf-8'))
        for i in range(len(args)):
            body += '&' + unicode(i+1) + '=' + urllib.quote_plus(unicode(args[i]).encode('utf-8'))
        if (None != self.sessionId):
            body += "&sessionId=" + unicode(self.sessionId)
        headers = {"Content-Type": "application/x-www-form-urlencoded; charset=utf-8"}
        conn.request("POST", "/selenium-server/driver/", body, headers)

        response = conn.getresponse()
        #print response.status, response.reason
        data = unicode(response.read(), "UTF-8")
        result = response.reason
        #print "Selenium Result: " + repr(data) + "\n\n"
        if (not data.startswith('OK')):
            raise Exception, data
        return data

    def get_string(self, verb, args):
        result = self.do_command(verb, args)
        return result[3:]

    def get_string_array(self, verb, args):
        csv = self.get_string(verb, args)
        token = ""
        tokens = []
        escape = False
        for i in range(len(csv)):
            letter = csv[i]
            if (escape):
                token = token + letter
                escape = False
                continue
            if (letter == '\\'):
                escape = True
            elif (letter == ','):
                tokens.append(token)
                token = ""
            else:
                token = token + letter
        tokens.append(token)
        return tokens

    def get_number(self, verb, args):
        # Is there something I need to do here?
        return self.get_string(verb, args)

    def get_number_array(self, verb, args):
        # Is there something I need to do here?
        return self.get_string_array(verb, args)

    def get_boolean(self, verb, args):
        boolstr = self.get_string(verb, args)
        if ("true" == boolstr):
            return True
        if ("false" == boolstr):
            return False
        raise ValueError, "result is neither 'true' nor 'false': " + boolstr

    def get_boolean_array(self, verb, args):
        boolarr = self.get_string_array(verb, args)
        for i in range(len(boolarr)):
            if ("true" == boolstr):
                boolarr[i] = True
                continue
            if ("false" == boolstr):
                boolarr[i] = False
                continue
            raise ValueError, "result is neither 'true' nor 'false': " + boolarr[i]
        return boolarr



### From here on, everything's auto-generated from XML


    def click(self,locator):
        """
        Clicks on a link, button, checkbox or radio button. If the click action
        causes a new page to load (like a link usually does), call
        waitForPageToLoad.

        'locator' is an element locator
        """
        self.do_command("click", [locator,])


    def double_click(self,locator):
        """
        Double clicks on a link, button, checkbox or radio button. If the double click action
        causes a new page to load (like a link usually does), call
        waitForPageToLoad.

        'locator' is an element locator
        """
        self.do_command("doubleClick", [locator,])


    def context_menu(self,locator):
        """
        Simulates opening the context menu for the specified element (as might happen if the user "right-clicked" on the element).

        'locator' is an element locator
        """
        self.do_command("contextMenu", [locator,])


    def click_at(self,locator,coordString):
        """
        Clicks on a link, button, checkbox or radio button. If the click action
        causes a new page to load (like a link usually does), call
        waitForPageToLoad.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("clickAt", [locator,coordString,])


    def double_click_at(self,locator,coordString):
        """
        Doubleclicks on a link, button, checkbox or radio button. If the action
        causes a new page to load (like a link usually does), call
        waitForPageToLoad.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("doubleClickAt", [locator,coordString,])


    def context_menu_at(self,locator,coordString):
        """
        Simulates opening the context menu for the specified element (as might happen if the user "right-clicked" on the element).

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("contextMenuAt", [locator,coordString,])


    def fire_event(self,locator,eventName):
        """
        Explicitly simulate an event, to trigger the corresponding "on\ *event*"
        handler.

        'locator' is an element locator
        'eventName' is the event name, e.g. "focus" or "blur"
        """
        self.do_command("fireEvent", [locator,eventName,])


    def focus(self,locator):
        """
        Move the focus to the specified element; for example, if the element is an input field, move the cursor to that field.

        'locator' is an element locator
        """
        self.do_command("focus", [locator,])


    def key_press(self,locator,keySequence):
        """
        Simulates a user pressing and releasing a key.

        'locator' is an element locator
        'keySequence' is Either be a string("\" followed by the numeric keycode  of the key to be pressed, normally the ASCII value of that key), or a single  character. For example: "w", "\119".
        """
        self.do_command("keyPress", [locator,keySequence,])


    def shift_key_down(self):
        """
        Press the shift key and hold it down until doShiftUp() is called or a new page is loaded.

        """
        self.do_command("shiftKeyDown", [])


    def shift_key_up(self):
        """
        Release the shift key.

        """
        self.do_command("shiftKeyUp", [])


    def meta_key_down(self):
        """
        Press the meta key and hold it down until doMetaUp() is called or a new page is loaded.

        """
        self.do_command("metaKeyDown", [])


    def meta_key_up(self):
        """
        Release the meta key.

        """
        self.do_command("metaKeyUp", [])


    def alt_key_down(self):
        """
        Press the alt key and hold it down until doAltUp() is called or a new page is loaded.

        """
        self.do_command("altKeyDown", [])


    def alt_key_up(self):
        """
        Release the alt key.

        """
        self.do_command("altKeyUp", [])


    def control_key_down(self):
        """
        Press the control key and hold it down until doControlUp() is called or a new page is loaded.

        """
        self.do_command("controlKeyDown", [])


    def control_key_up(self):
        """
        Release the control key.

        """
        self.do_command("controlKeyUp", [])


    def key_down(self,locator,keySequence):
        """
        Simulates a user pressing a key (without releasing it yet).

        'locator' is an element locator
        'keySequence' is Either be a string("\" followed by the numeric keycode  of the key to be pressed, normally the ASCII value of that key), or a single  character. For example: "w", "\119".
        """
        self.do_command("keyDown", [locator,keySequence,])


    def key_up(self,locator,keySequence):
        """
        Simulates a user releasing a key.

        'locator' is an element locator
        'keySequence' is Either be a string("\" followed by the numeric keycode  of the key to be pressed, normally the ASCII value of that key), or a single  character. For example: "w", "\119".
        """
        self.do_command("keyUp", [locator,keySequence,])


    def mouse_over(self,locator):
        """
        Simulates a user hovering a mouse over the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseOver", [locator,])


    def mouse_out(self,locator):
        """
        Simulates a user moving the mouse pointer away from the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseOut", [locator,])


    def mouse_down(self,locator):
        """
        Simulates a user pressing the left mouse button (without releasing it yet) on
        the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseDown", [locator,])


    def mouse_down_right(self,locator):
        """
        Simulates a user pressing the right mouse button (without releasing it yet) on
        the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseDownRight", [locator,])


    def mouse_down_at(self,locator,coordString):
        """
        Simulates a user pressing the left mouse button (without releasing it yet) at
        the specified location.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("mouseDownAt", [locator,coordString,])


    def mouse_down_right_at(self,locator,coordString):
        """
        Simulates a user pressing the right mouse button (without releasing it yet) at
        the specified location.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("mouseDownRightAt", [locator,coordString,])


    def mouse_up(self,locator):
        """
        Simulates the event that occurs when the user releases the mouse button (i.e., stops
        holding the button down) on the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseUp", [locator,])


    def mouse_up_right(self,locator):
        """
        Simulates the event that occurs when the user releases the right mouse button (i.e., stops
        holding the button down) on the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseUpRight", [locator,])


    def mouse_up_at(self,locator,coordString):
        """
        Simulates the event that occurs when the user releases the mouse button (i.e., stops
        holding the button down) at the specified location.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("mouseUpAt", [locator,coordString,])


    def mouse_up_right_at(self,locator,coordString):
        """
        Simulates the event that occurs when the user releases the right mouse button (i.e., stops
        holding the button down) at the specified location.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("mouseUpRightAt", [locator,coordString,])


    def mouse_move(self,locator):
        """
        Simulates a user pressing the mouse button (without releasing it yet) on
        the specified element.

        'locator' is an element locator
        """
        self.do_command("mouseMove", [locator,])


    def mouse_move_at(self,locator,coordString):
        """
        Simulates a user pressing the mouse button (without releasing it yet) on
        the specified element.

        'locator' is an element locator
        'coordString' is specifies the x,y position (i.e. - 10,20) of the mouse      event relative to the element returned by the locator.
        """
        self.do_command("mouseMoveAt", [locator,coordString,])


    def type(self,locator,value):
        """
        Sets the value of an input field, as though you typed it in.


        Can also be used to set the value of combo boxes, check boxes, etc. In these cases,
        value should be the value of the option selected, not the visible text.


        'locator' is an element locator
        'value' is the value to type
        """
        self.do_command("type", [locator,value,])


    def type_keys(self,locator,value):
        """
        Simulates keystroke events on the specified element, as though you typed the value key-by-key.


        This is a convenience method for calling keyDown, keyUp, keyPress for every character in the specified string;
        this is useful for dynamic UI widgets (like auto-completing combo boxes) that require explicit key events.

        Unlike the simple "type" command, which forces the specified value into the page directly, this command
        may or may not have any visible effect, even in cases where typing keys would normally have a visible effect.
        For example, if you use "typeKeys" on a form element, you may or may not see the results of what you typed in
        the field.

        In some cases, you may need to use the simple "type" command to set the value of the field and then the "typeKeys" command to
        send the keystroke events corresponding to what you just typed.


        'locator' is an element locator
        'value' is the value to type
        """
        self.do_command("typeKeys", [locator,value,])


    def set_speed(self,value):
        """
        Set execution speed (i.e., set the millisecond length of a delay which will follow each selenium operation).  By default, there is no such delay, i.e.,
        the delay is 0 milliseconds.

        'value' is the number of milliseconds to pause after operation
        """
        self.do_command("setSpeed", [value,])


    def get_speed(self):
        """
        Get execution speed (i.e., get the millisecond length of the delay following each selenium operation).  By default, there is no such delay, i.e.,
        the delay is 0 milliseconds.

        See also setSpeed.

        """
        return self.get_string("getSpeed", [])


    def check(self,locator):
        """
        Check a toggle-button (checkbox/radio)

        'locator' is an element locator
        """
        self.do_command("check", [locator,])


    def uncheck(self,locator):
        """
        Uncheck a toggle-button (checkbox/radio)

        'locator' is an element locator
        """
        self.do_command("uncheck", [locator,])


    def select(self,selectLocator,optionLocator):
        """
        Select an option from a drop-down using an option locator.



        Option locators provide different ways of specifying options of an HTML
        Select element (e.g. for selecting a specific option, or for asserting
        that the selected option satisfies a specification). There are several
        forms of Select Option Locator.


        *   \ **label**\ =\ *labelPattern*:
            matches options based on their labels, i.e. the visible text. (This
            is the default.)

            *   label=regexp:^[Oo]ther


        *   \ **value**\ =\ *valuePattern*:
            matches options based on their values.

            *   value=other


        *   \ **id**\ =\ *id*:

            matches options based on their ids.

            *   id=option1


        *   \ **index**\ =\ *index*:
            matches an option based on its index (offset from zero).

            *   index=2





        If no option locator prefix is provided, the default behaviour is to match on \ **label**\ .



        'selectLocator' is an element locator identifying a drop-down menu
        'optionLocator' is an option locator (a label by default)
        """
        self.do_command("select", [selectLocator,optionLocator,])


    def add_selection(self,locator,optionLocator):
        """
        Add a selection to the set of selected options in a multi-select element using an option locator.

        @see #doSelect for details of option locators

        'locator' is an element locator identifying a multi-select box
        'optionLocator' is an option locator (a label by default)
        """
        self.do_command("addSelection", [locator,optionLocator,])


    def remove_selection(self,locator,optionLocator):
        """
        Remove a selection from the set of selected options in a multi-select element using an option locator.

        @see #doSelect for details of option locators

        'locator' is an element locator identifying a multi-select box
        'optionLocator' is an option locator (a label by default)
        """
        self.do_command("removeSelection", [locator,optionLocator,])


    def remove_all_selections(self,locator):
        """
        Unselects all of the selected options in a multi-select element.

        'locator' is an element locator identifying a multi-select box
        """
        self.do_command("removeAllSelections", [locator,])


    def submit(self,formLocator):
        """
        Submit the specified form. This is particularly useful for forms without
        submit buttons, e.g. single-input "Search" forms.

        'formLocator' is an element locator for the form you want to submit
        """
        self.do_command("submit", [formLocator,])


    def open(self,url):
        """
        Opens an URL in the test frame. This accepts both relative and absolute
        URLs.

        The "open" command waits for the page to load before proceeding,
        ie. the "AndWait" suffix is implicit.

        \ *Note*: The URL must be on the same domain as the runner HTML
        due to security restrictions in the browser (Same Origin Policy). If you
        need to open an URL on another domain, use the Selenium Server to start a
        new browser session on that domain.

        'url' is the URL to open; may be relative or absolute
        """
        self.do_command("open", [url,])


    def open_window(self,url,windowID):
        """
        Opens a popup window (if a window with that ID isn't already open).
        After opening the window, you'll need to select it using the selectWindow
        command.


        This command can also be a useful workaround for bug SEL-339.  In some cases, Selenium will be unable to intercept a call to window.open (if the call occurs during or before the "onLoad" event, for example).
        In those cases, you can force Selenium to notice the open window's name by using the Selenium openWindow command, using
        an empty (blank) url, like this: openWindow("", "myFunnyWindow").


        'url' is the URL to open, which can be blank
        'windowID' is the JavaScript window ID of the window to select
        """
        self.do_command("openWindow", [url,windowID,])


    def select_window(self,windowID):
        """
        Selects a popup window using a window locator; once a popup window has been selected, all
        commands go to that window. To select the main window again, use null
        as the target.




        Window locators provide different ways of specifying the window object:
        by title, by internal JavaScript "name," or by JavaScript variable.


        *   \ **title**\ =\ *My Special Window*:
            Finds the window using the text that appears in the title bar.  Be careful;
            two windows can share the same title.  If that happens, this locator will
            just pick one.

        *   \ **name**\ =\ *myWindow*:
            Finds the window using its internal JavaScript "name" property.  This is the second
            parameter "windowName" passed to the JavaScript method window.open(url, windowName, windowFeatures, replaceFlag)
            (which Selenium intercepts).

        *   \ **var**\ =\ *variableName*:
            Some pop-up windows are unnamed (anonymous), but are associated with a JavaScript variable name in the current
            application window, e.g. "window.foo = window.open(url);".  In those cases, you can open the window using
            "var=foo".




        If no window locator prefix is provided, we'll try to guess what you mean like this:

        1.) if windowID is null, (or the string "null") then it is assumed the user is referring to the original window instantiated by the browser).

        2.) if the value of the "windowID" parameter is a JavaScript variable name in the current application window, then it is assumed
        that this variable contains the return value from a call to the JavaScript window.open() method.

        3.) Otherwise, selenium looks in a hash it maintains that maps string names to window "names".

        4.) If \ *that* fails, we'll try looping over all of the known windows to try to find the appropriate "title".
        Since "title" is not necessarily unique, this may have unexpected behavior.

        If you're having trouble figuring out the name of a window that you want to manipulate, look at the Selenium log messages
        which identify the names of windows created via window.open (and therefore intercepted by Selenium).  You will see messages
        like the following for each window as it is opened:

        ``debug: window.open call intercepted; window ID (which you can use with selectWindow()) is "myNewWindow"``

        In some cases, Selenium will be unable to intercept a call to window.open (if the call occurs during or before the "onLoad" event, for example).
        (This is bug SEL-339.)  In those cases, you can force Selenium to notice the open window's name by using the Selenium openWindow command, using
        an empty (blank) url, like this: openWindow("", "myFunnyWindow").


        'windowID' is the JavaScript window ID of the window to select
        """
        self.do_command("selectWindow", [windowID,])


    def select_pop_up(self,windowID):
        """
        Simplifies the process of selecting a popup window (and does not offer
        functionality beyond what ``selectWindow()`` already provides).

        *   If ``windowID`` is either not specified, or specified as
            "null", the first non-top window is selected. The top window is the one
            that would be selected by ``selectWindow()`` without providing a
            ``windowID`` . This should not be used when more than one popup
            window is in play.
        *   Otherwise, the window will be looked up considering
            ``windowID`` as the following in order: 1) the "name" of the
            window, as specified to ``window.open()``; 2) a javascript
            variable which is a reference to a window; and 3) the title of the
            window. This is the same ordered lookup performed by
            ``selectWindow`` .



        'windowID' is an identifier for the popup window, which can take on a                  number of different meanings
        """
        self.do_command("selectPopUp", [windowID,])


    def deselect_pop_up(self):
        """
        Selects the main window. Functionally equivalent to using
        ``selectWindow()`` and specifying no value for
        ``windowID``.

        """
        self.do_command("deselectPopUp", [])


    def select_frame(self,locator):
        """
        Selects a frame within the current window.  (You may invoke this command
        multiple times to select nested frames.)  To select the parent frame, use
        "relative=parent" as a locator; to select the top frame, use "relative=top".
        You can also select a frame by its 0-based index number; select the first frame with
        "index=0", or the third frame with "index=2".


        You may also use a DOM expression to identify the frame you want directly,
        like this: ``dom=frames["main"].frames["subframe"]``


        'locator' is an element locator identifying a frame or iframe
        """
        self.do_command("selectFrame", [locator,])


    def get_whether_this_frame_match_frame_expression(self,currentFrameString,target):
        """
        Determine whether current/locator identify the frame containing this running code.


        This is useful in proxy injection mode, where this code runs in every
        browser frame and window, and sometimes the selenium server needs to identify
        the "current" frame.  In this case, when the test calls selectFrame, this
        routine is called for each frame to figure out which one has been selected.
        The selected frame will return true, while all others will return false.


        'currentFrameString' is starting frame
        'target' is new frame (which might be relative to the current one)
        """
        return self.get_boolean("getWhetherThisFrameMatchFrameExpression", [currentFrameString,target,])


    def get_whether_this_window_match_window_expression(self,currentWindowString,target):
        """
        Determine whether currentWindowString plus target identify the window containing this running code.


        This is useful in proxy injection mode, where this code runs in every
        browser frame and window, and sometimes the selenium server needs to identify
        the "current" window.  In this case, when the test calls selectWindow, this
        routine is called for each window to figure out which one has been selected.
        The selected window will return true, while all others will return false.


        'currentWindowString' is starting window
        'target' is new window (which might be relative to the current one, e.g., "_parent")
        """
        return self.get_boolean("getWhetherThisWindowMatchWindowExpression", [currentWindowString,target,])


    def wait_for_pop_up(self,windowID,timeout):
        """
        Waits for a popup window to appear and load up.

        'windowID' is the JavaScript window "name" of the window that will appear (not the text of the title bar)                 If unspecified, or specified as "null", this command will                 wait for the first non-top window to appear (don't rely                 on this if you are working with multiple popups                 simultaneously).
        'timeout' is a timeout in milliseconds, after which the action will return with an error.                If this value is not specified, the default Selenium                timeout will be used. See the setTimeout() command.
        """
        self.do_command("waitForPopUp", [windowID,timeout,])


    def choose_cancel_on_next_confirmation(self):
        """


        By default, Selenium's overridden window.confirm() function will
        return true, as if the user had manually clicked OK; after running
        this command, the next call to confirm() will return false, as if
        the user had clicked Cancel.  Selenium will then resume using the
        default behavior for future confirmations, automatically returning
        true (OK) unless/until you explicitly call this command for each
        confirmation.



        Take note - every time a confirmation comes up, you must
        consume it with a corresponding getConfirmation, or else
        the next selenium operation will fail.



        """
        self.do_command("chooseCancelOnNextConfirmation", [])


    def choose_ok_on_next_confirmation(self):
        """


        Undo the effect of calling chooseCancelOnNextConfirmation.  Note
        that Selenium's overridden window.confirm() function will normally automatically
        return true, as if the user had manually clicked OK, so you shouldn't
        need to use this command unless for some reason you need to change
        your mind prior to the next confirmation.  After any confirmation, Selenium will resume using the
        default behavior for future confirmations, automatically returning
        true (OK) unless/until you explicitly call chooseCancelOnNextConfirmation for each
        confirmation.



        Take note - every time a confirmation comes up, you must
        consume it with a corresponding getConfirmation, or else
        the next selenium operation will fail.



        """
        self.do_command("chooseOkOnNextConfirmation", [])


    def answer_on_next_prompt(self,answer):
        """
        Instructs Selenium to return the specified answer string in response to
        the next JavaScript prompt [window.prompt()].

        'answer' is the answer to give in response to the prompt pop-up
        """
        self.do_command("answerOnNextPrompt", [answer,])


    def go_back(self):
        """
        Simulates the user clicking the "back" button on their browser.

        """
        self.do_command("goBack", [])


    def refresh(self):
        """
        Simulates the user clicking the "Refresh" button on their browser.

        """
        self.do_command("refresh", [])


    def close(self):
        """
        Simulates the user clicking the "close" button in the titlebar of a popup
        window or tab.

        """
        self.do_command("close", [])


    def is_alert_present(self):
        """
        Has an alert occurred?



        This function never throws an exception



        """
        return self.get_boolean("isAlertPresent", [])


    def is_prompt_present(self):
        """
        Has a prompt occurred?



        This function never throws an exception



        """
        return self.get_boolean("isPromptPresent", [])


    def is_confirmation_present(self):
        """
        Has confirm() been called?



        This function never throws an exception



        """
        return self.get_boolean("isConfirmationPresent", [])


    def get_alert(self):
        """
        Retrieves the message of a JavaScript alert generated during the previous action, or fail if there were no alerts.


        Getting an alert has the same effect as manually clicking OK. If an
        alert is generated but you do not consume it with getAlert, the next Selenium action
        will fail.

        Under Selenium, JavaScript alerts will NOT pop up a visible alert
        dialog.

        Selenium does NOT support JavaScript alerts that are generated in a
        page's onload() event handler. In this case a visible dialog WILL be
        generated and Selenium will hang until someone manually clicks OK.


        """
        return self.get_string("getAlert", [])


    def get_confirmation(self):
        """
        Retrieves the message of a JavaScript confirmation dialog generated during
        the previous action.



        By default, the confirm function will return true, having the same effect
        as manually clicking OK. This can be changed by prior execution of the
        chooseCancelOnNextConfirmation command.



        If an confirmation is generated but you do not consume it with getConfirmation,
        the next Selenium action will fail.



        NOTE: under Selenium, JavaScript confirmations will NOT pop up a visible
        dialog.



        NOTE: Selenium does NOT support JavaScript confirmations that are
        generated in a page's onload() event handler. In this case a visible
        dialog WILL be generated and Selenium will hang until you manually click
        OK.



        """
        return self.get_string("getConfirmation", [])


    def get_prompt(self):
        """
        Retrieves the message of a JavaScript question prompt dialog generated during
        the previous action.


        Successful handling of the prompt requires prior execution of the
        answerOnNextPrompt command. If a prompt is generated but you
        do not get/verify it, the next Selenium action will fail.

        NOTE: under Selenium, JavaScript prompts will NOT pop up a visible
        dialog.

        NOTE: Selenium does NOT support JavaScript prompts that are generated in a
        page's onload() event handler. In this case a visible dialog WILL be
        generated and Selenium will hang until someone manually clicks OK.


        """
        return self.get_string("getPrompt", [])


    def get_location(self):
        """
        Gets the absolute URL of the current page.

        """
        return self.get_string("getLocation", [])


    def get_title(self):
        """
        Gets the title of the current page.

        """
        return self.get_string("getTitle", [])


    def get_body_text(self):
        """
        Gets the entire text of the page.

        """
        return self.get_string("getBodyText", [])


    def get_value(self,locator):
        """
        Gets the (whitespace-trimmed) value of an input field (or anything else with a value parameter).
        For checkbox/radio elements, the value will be "on" or "off" depending on
        whether the element is checked or not.

        'locator' is an element locator
        """
        return self.get_string("getValue", [locator,])


    def get_text(self,locator):
        """
        Gets the text of an element. This works for any element that contains
        text. This command uses either the textContent (Mozilla-like browsers) or
        the innerText (IE-like browsers) of the element, which is the rendered
        text shown to the user.

        'locator' is an element locator
        """
        return self.get_string("getText", [locator,])


    def highlight(self,locator):
        """
        Briefly changes the backgroundColor of the specified element yellow.  Useful for debugging.

        'locator' is an element locator
        """
        self.do_command("highlight", [locator,])


    def get_eval(self,script):
        """
        Gets the result of evaluating the specified JavaScript snippet.  The snippet may
        have multiple lines, but only the result of the last line will be returned.


        Note that, by default, the snippet will run in the context of the "selenium"
        object itself, so ``this`` will refer to the Selenium object.  Use ``window`` to
        refer to the window of your application, e.g. ``window.document.getElementById('foo')``

        If you need to use
        a locator to refer to a single element in your application page, you can
        use ``this.browserbot.findElement("id=foo")`` where "id=foo" is your locator.


        'script' is the JavaScript snippet to run
        """
        return self.get_string("getEval", [script,])


    def is_checked(self,locator):
        """
        Gets whether a toggle-button (checkbox/radio) is checked.  Fails if the specified element doesn't exist or isn't a toggle-button.

        'locator' is an element locator pointing to a checkbox or radio button
        """
        return self.get_boolean("isChecked", [locator,])


    def get_table(self,tableCellAddress):
        """
        Gets the text from a cell of a table. The cellAddress syntax
        tableLocator.row.column, where row and column start at 0.

        'tableCellAddress' is a cell address, e.g. "foo.1.4"
        """
        return self.get_string("getTable", [tableCellAddress,])


    def get_selected_labels(self,selectLocator):
        """
        Gets all option labels (visible text) for selected options in the specified select or multi-select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string_array("getSelectedLabels", [selectLocator,])


    def get_selected_label(self,selectLocator):
        """
        Gets option label (visible text) for selected option in the specified select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string("getSelectedLabel", [selectLocator,])


    def get_selected_values(self,selectLocator):
        """
        Gets all option values (value attributes) for selected options in the specified select or multi-select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string_array("getSelectedValues", [selectLocator,])


    def get_selected_value(self,selectLocator):
        """
        Gets option value (value attribute) for selected option in the specified select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string("getSelectedValue", [selectLocator,])


    def get_selected_indexes(self,selectLocator):
        """
        Gets all option indexes (option number, starting at 0) for selected options in the specified select or multi-select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string_array("getSelectedIndexes", [selectLocator,])


    def get_selected_index(self,selectLocator):
        """
        Gets option index (option number, starting at 0) for selected option in the specified select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string("getSelectedIndex", [selectLocator,])


    def get_selected_ids(self,selectLocator):
        """
        Gets all option element IDs for selected options in the specified select or multi-select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string_array("getSelectedIds", [selectLocator,])


    def get_selected_id(self,selectLocator):
        """
        Gets option element ID for selected option in the specified select element.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string("getSelectedId", [selectLocator,])


    def is_something_selected(self,selectLocator):
        """
        Determines whether some option in a drop-down menu is selected.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_boolean("isSomethingSelected", [selectLocator,])


    def get_select_options(self,selectLocator):
        """
        Gets all option labels in the specified select drop-down.

        'selectLocator' is an element locator identifying a drop-down menu
        """
        return self.get_string_array("getSelectOptions", [selectLocator,])


    def get_attribute(self,attributeLocator):
        """
        Gets the value of an element attribute. The value of the attribute may
        differ across browsers (this is the case for the "style" attribute, for
        example).

        'attributeLocator' is an element locator followed by an @ sign and then the name of the attribute, e.g. "foo@bar"
        """
        return self.get_string("getAttribute", [attributeLocator,])


    def is_text_present(self,pattern):
        """
        Verifies that the specified text pattern appears somewhere on the rendered page shown to the user.

        'pattern' is a pattern to match with the text of the page
        """
        return self.get_boolean("isTextPresent", [pattern,])


    def is_element_present(self,locator):
        """
        Verifies that the specified element is somewhere on the page.

        'locator' is an element locator
        """
        return self.get_boolean("isElementPresent", [locator,])


    def is_visible(self,locator):
        """
        Determines if the specified element is visible. An
        element can be rendered invisible by setting the CSS "visibility"
        property to "hidden", or the "display" property to "none", either for the
        element itself or one if its ancestors.  This method will fail if
        the element is not present.

        'locator' is an element locator
        """
        return self.get_boolean("isVisible", [locator,])


    def is_editable(self,locator):
        """
        Determines whether the specified input element is editable, ie hasn't been disabled.
        This method will fail if the specified element isn't an input element.

        'locator' is an element locator
        """
        return self.get_boolean("isEditable", [locator,])


    def get_all_buttons(self):
        """
        Returns the IDs of all buttons on the page.


        If a given button has no ID, it will appear as "" in this array.


        """
        return self.get_string_array("getAllButtons", [])


    def get_all_links(self):
        """
        Returns the IDs of all links on the page.


        If a given link has no ID, it will appear as "" in this array.


        """
        return self.get_string_array("getAllLinks", [])


    def get_all_fields(self):
        """
        Returns the IDs of all input fields on the page.


        If a given field has no ID, it will appear as "" in this array.


        """
        return self.get_string_array("getAllFields", [])


    def get_attribute_from_all_windows(self,attributeName):
        """
        Returns every instance of some attribute from all known windows.

        'attributeName' is name of an attribute on the windows
        """
        return self.get_string_array("getAttributeFromAllWindows", [attributeName,])


    def dragdrop(self,locator,movementsString):
        """
        deprecated - use dragAndDrop instead

        'locator' is an element locator
        'movementsString' is offset in pixels from the current location to which the element should be moved, e.g., "+70,-300"
        """
        self.do_command("dragdrop", [locator,movementsString,])


    def set_mouse_speed(self,pixels):
        """
        Configure the number of pixels between "mousemove" events during dragAndDrop commands (default=10).

        Setting this value to 0 means that we'll send a "mousemove" event to every single pixel
        in between the start location and the end location; that can be very slow, and may
        cause some browsers to force the JavaScript to timeout.

        If the mouse speed is greater than the distance between the two dragged objects, we'll
        just send one "mousemove" at the start location and then one final one at the end location.


        'pixels' is the number of pixels between "mousemove" events
        """
        self.do_command("setMouseSpeed", [pixels,])


    def get_mouse_speed(self):
        """
        Returns the number of pixels between "mousemove" events during dragAndDrop commands (default=10).

        """
        return self.get_number("getMouseSpeed", [])


    def drag_and_drop(self,locator,movementsString):
        """
        Drags an element a certain distance and then drops it

        'locator' is an element locator
        'movementsString' is offset in pixels from the current location to which the element should be moved, e.g., "+70,-300"
        """
        self.do_command("dragAndDrop", [locator,movementsString,])


    def drag_and_drop_to_object(self,locatorOfObjectToBeDragged,locatorOfDragDestinationObject):
        """
        Drags an element and drops it on another element

        'locatorOfObjectToBeDragged' is an element to be dragged
        'locatorOfDragDestinationObject' is an element whose location (i.e., whose center-most pixel) will be the point where locatorOfObjectToBeDragged  is dropped
        """
        self.do_command("dragAndDropToObject", [locatorOfObjectToBeDragged,locatorOfDragDestinationObject,])


    def window_focus(self):
        """
        Gives focus to the currently selected window

        """
        self.do_command("windowFocus", [])


    def window_maximize(self):
        """
        Resize currently selected window to take up the entire screen

        """
        self.do_command("windowMaximize", [])


    def get_all_window_ids(self):
        """
        Returns the IDs of all windows that the browser knows about.

        """
        return self.get_string_array("getAllWindowIds", [])


    def get_all_window_names(self):
        """
        Returns the names of all windows that the browser knows about.

        """
        return self.get_string_array("getAllWindowNames", [])


    def get_all_window_titles(self):
        """
        Returns the titles of all windows that the browser knows about.

        """
        return self.get_string_array("getAllWindowTitles", [])


    def get_html_source(self):
        """
        Returns the entire HTML source between the opening and
        closing "html" tags.

        """
        return self.get_string("getHtmlSource", [])


    def set_cursor_position(self,locator,position):
        """
        Moves the text cursor to the specified position in the given input element or textarea.
        This method will fail if the specified element isn't an input element or textarea.

        'locator' is an element locator pointing to an input element or textarea
        'position' is the numerical position of the cursor in the field; position should be 0 to move the position to the beginning of the field.  You can also set the cursor to -1 to move it to the end of the field.
        """
        self.do_command("setCursorPosition", [locator,position,])


    def get_element_index(self,locator):
        """
        Get the relative index of an element to its parent (starting from 0). The comment node and empty text node
        will be ignored.

        'locator' is an element locator pointing to an element
        """
        return self.get_number("getElementIndex", [locator,])


    def is_ordered(self,locator1,locator2):
        """
        Check if these two elements have same parent and are ordered siblings in the DOM. Two same elements will
        not be considered ordered.

        'locator1' is an element locator pointing to the first element
        'locator2' is an element locator pointing to the second element
        """
        return self.get_boolean("isOrdered", [locator1,locator2,])


    def get_element_position_left(self,locator):
        """
        Retrieves the horizontal position of an element

        'locator' is an element locator pointing to an element OR an element itself
        """
        return self.get_number("getElementPositionLeft", [locator,])


    def get_element_position_top(self,locator):
        """
        Retrieves the vertical position of an element

        'locator' is an element locator pointing to an element OR an element itself
        """
        return self.get_number("getElementPositionTop", [locator,])


    def get_element_width(self,locator):
        """
        Retrieves the width of an element

        'locator' is an element locator pointing to an element
        """
        return self.get_number("getElementWidth", [locator,])


    def get_element_height(self,locator):
        """
        Retrieves the height of an element

        'locator' is an element locator pointing to an element
        """
        return self.get_number("getElementHeight", [locator,])


    def get_cursor_position(self,locator):
        """
        Retrieves the text cursor position in the given input element or textarea; beware, this may not work perfectly on all browsers.


        Specifically, if the cursor/selection has been cleared by JavaScript, this command will tend to
        return the position of the last location of the cursor, even though the cursor is now gone from the page.  This is filed as SEL-243.

        This method will fail if the specified element isn't an input element or textarea, or there is no cursor in the element.

        'locator' is an element locator pointing to an input element or textarea
        """
        return self.get_number("getCursorPosition", [locator,])


    def get_expression(self,expression):
        """
        Returns the specified expression.


        This is useful because of JavaScript preprocessing.
        It is used to generate commands like assertExpression and waitForExpression.


        'expression' is the value to return
        """
        return self.get_string("getExpression", [expression,])


    def get_xpath_count(self,xpath):
        """
        Returns the number of nodes that match the specified xpath, eg. "//table" would give
        the number of tables.

        'xpath' is the xpath expression to evaluate. do NOT wrap this expression in a 'count()' function; we will do that for you.
        """
        return self.get_number("getXpathCount", [xpath,])


    def assign_id(self,locator,identifier):
        """
        Temporarily sets the "id" attribute of the specified element, so you can locate it in the future
        using its ID rather than a slow/complicated XPath.  This ID will disappear once the page is
        reloaded.

        'locator' is an element locator pointing to an element
        'identifier' is a string to be used as the ID of the specified element
        """
        self.do_command("assignId", [locator,identifier,])


    def allow_native_xpath(self,allow):
        """
        Specifies whether Selenium should use the native in-browser implementation
        of XPath (if any native version is available); if you pass "false" to
        this function, we will always use our pure-JavaScript xpath library.
        Using the pure-JS xpath library can improve the consistency of xpath
        element locators between different browser vendors, but the pure-JS
        version is much slower than the native implementations.

        'allow' is boolean, true means we'll prefer to use native XPath; false means we'll only use JS XPath
        """
        self.do_command("allowNativeXpath", [allow,])


    def ignore_attributes_without_value(self,ignore):
        """
        Specifies whether Selenium will ignore xpath attributes that have no
        value, i.e. are the empty string, when using the non-native xpath
        evaluation engine. You'd want to do this for performance reasons in IE.
        However, this could break certain xpaths, for example an xpath that looks
        for an attribute whose value is NOT the empty string.

        The hope is that such xpaths are relatively rare, but the user should
        have the option of using them. Note that this only influences xpath
        evaluation when using the ajaxslt engine (i.e. not "javascript-xpath").

        'ignore' is boolean, true means we'll ignore attributes without value                        at the expense of xpath "correctness"; false means                        we'll sacrifice speed for correctness.
        """
        self.do_command("ignoreAttributesWithoutValue", [ignore,])


    def wait_for_condition(self,script,timeout):
        """
        Runs the specified JavaScript snippet repeatedly until it evaluates to "true".
        The snippet may have multiple lines, but only the result of the last line
        will be considered.


        Note that, by default, the snippet will be run in the runner's test window, not in the window
        of your application.  To get the window of your application, you can use
        the JavaScript snippet ``selenium.browserbot.getCurrentWindow()``, and then
        run your JavaScript in there


        'script' is the JavaScript snippet to run
        'timeout' is a timeout in milliseconds, after which this command will return with an error
        """
        self.do_command("waitForCondition", [script,timeout,])


    def set_timeout(self,timeout):
        """
        Specifies the amount of time that Selenium will wait for actions to complete.


        Actions that require waiting include "open" and the "waitFor\*" actions.

        The default timeout is 30 seconds.

        'timeout' is a timeout in milliseconds, after which the action will return with an error
        """
        self.do_command("setTimeout", [timeout,])


    def wait_for_page_to_load(self,timeout):
        """
        Waits for a new page to load.


        You can use this command instead of the "AndWait" suffixes, "clickAndWait", "selectAndWait", "typeAndWait" etc.
        (which are only available in the JS API).

        Selenium constantly keeps track of new pages loading, and sets a "newPageLoaded"
        flag when it first notices a page load.  Running any other Selenium command after
        turns the flag to false.  Hence, if you want to wait for a page to load, you must
        wait immediately after a Selenium command that caused a page-load.


        'timeout' is a timeout in milliseconds, after which this command will return with an error
        """
        self.do_command("waitForPageToLoad", [timeout,])


    def wait_for_frame_to_load(self,frameAddress,timeout):
        """
        Waits for a new frame to load.


        Selenium constantly keeps track of new pages and frames loading,
        and sets a "newPageLoaded" flag when it first notices a page load.


        See waitForPageToLoad for more information.

        'frameAddress' is FrameAddress from the server side
        'timeout' is a timeout in milliseconds, after which this command will return with an error
        """
        self.do_command("waitForFrameToLoad", [frameAddress,timeout,])


    def get_cookie(self):
        """
        Return all cookies of the current page under test.

        """
        return self.get_string("getCookie", [])


    def get_cookie_by_name(self,name):
        """
        Returns the value of the cookie with the specified name, or throws an error if the cookie is not present.

        'name' is the name of the cookie
        """
        return self.get_string("getCookieByName", [name,])


    def is_cookie_present(self,name):
        """
        Returns true if a cookie with the specified name is present, or false otherwise.

        'name' is the name of the cookie
        """
        return self.get_boolean("isCookiePresent", [name,])


    def create_cookie(self,nameValuePair,optionsString):
        """
        Create a new cookie whose path and domain are same with those of current page
        under test, unless you specified a path for this cookie explicitly.

        'nameValuePair' is name and value of the cookie in a format "name=value"
        'optionsString' is options for the cookie. Currently supported options include 'path', 'max_age' and 'domain'.      the optionsString's format is "path=/path/, max_age=60, domain=.foo.com". The order of options are irrelevant, the unit      of the value of 'max_age' is second.  Note that specifying a domain that isn't a subset of the current domain will      usually fail.
        """
        self.do_command("createCookie", [nameValuePair,optionsString,])


    def delete_cookie(self,name,optionsString):
        """
        Delete a named cookie with specified path and domain.  Be careful; to delete a cookie, you
        need to delete it using the exact same path and domain that were used to create the cookie.
        If the path is wrong, or the domain is wrong, the cookie simply won't be deleted.  Also
        note that specifying a domain that isn't a subset of the current domain will usually fail.

        Since there's no way to discover at runtime the original path and domain of a given cookie,
        we've added an option called 'recurse' to try all sub-domains of the current domain with
        all paths that are a subset of the current path.  Beware; this option can be slow.  In
        big-O notation, it operates in O(n\*m) time, where n is the number of dots in the domain
        name and m is the number of slashes in the path.

        'name' is the name of the cookie to be deleted
        'optionsString' is options for the cookie. Currently supported options include 'path', 'domain'      and 'recurse.' The optionsString's format is "path=/path/, domain=.foo.com, recurse=true".      The order of options are irrelevant. Note that specifying a domain that isn't a subset of      the current domain will usually fail.
        """
        self.do_command("deleteCookie", [name,optionsString,])


    def delete_all_visible_cookies(self):
        """
        Calls deleteCookie with recurse=true on all cookies visible to the current page.
        As noted on the documentation for deleteCookie, recurse=true can be much slower
        than simply deleting the cookies using a known domain/path.

        """
        self.do_command("deleteAllVisibleCookies", [])


    def set_browser_log_level(self,logLevel):
        """
        Sets the threshold for browser-side logging messages; log messages beneath this threshold will be discarded.
        Valid logLevel strings are: "debug", "info", "warn", "error" or "off".
        To see the browser logs, you need to
        either show the log window in GUI mode, or enable browser-side logging in Selenium RC.

        'logLevel' is one of the following: "debug", "info", "warn", "error" or "off"
        """
        self.do_command("setBrowserLogLevel", [logLevel,])


    def run_script(self,script):
        """
        Creates a new "script" tag in the body of the current test window, and
        adds the specified text into the body of the command.  Scripts run in
        this way can often be debugged more easily than scripts executed using
        Selenium's "getEval" command.  Beware that JS exceptions thrown in these script
        tags aren't managed by Selenium, so you should probably wrap your script
        in try/catch blocks if there is any chance that the script will throw
        an exception.

        'script' is the JavaScript snippet to run
        """
        self.do_command("runScript", [script,])


    def add_location_strategy(self,strategyName,functionDefinition):
        """
        Defines a new function for Selenium to locate elements on the page.
        For example,
        if you define the strategy "foo", and someone runs click("foo=blah"), we'll
        run your function, passing you the string "blah", and click on the element
        that your function
        returns, or throw an "Element not found" error if your function returns null.

        We'll pass three arguments to your function:

        *   locator: the string the user passed in
        *   inWindow: the currently selected window
        *   inDocument: the currently selected document


        The function must return null if the element can't be found.

        'strategyName' is the name of the strategy to define; this should use only   letters [a-zA-Z] with no spaces or other punctuation.
        'functionDefinition' is a string defining the body of a function in JavaScript.   For example: ``return inDocument.getElementById(locator);``
        """
        self.do_command("addLocationStrategy", [strategyName,functionDefinition,])


    def capture_entire_page_screenshot(self,filename,kwargs):
        """
        Saves the entire contents of the current window canvas to a PNG file.
        Contrast this with the captureScreenshot command, which captures the
        contents of the OS viewport (i.e. whatever is currently being displayed
        on the monitor), and is implemented in the RC only. Currently this only
        works in Firefox when running in chrome mode, and in IE non-HTA using
        the EXPERIMENTAL "Snapsie" utility. The Firefox implementation is mostly
        borrowed from the Screengrab! Firefox extension. Please see
        http://www.screengrab.org and http://snapsie.sourceforge.net/ for
        details.

        'filename' is the path to the file to persist the screenshot as. No                  filename extension will be appended by default.                  Directories will not be created if they do not exist,                    and an exception will be thrown, possibly by native                  code.
        'kwargs' is a kwargs string that modifies the way the screenshot                  is captured. Example: "background=#CCFFDD" .                  Currently valid options:
        *    background
            the background CSS for the HTML document. This                     may be useful to set for capturing screenshots of                     less-than-ideal layouts, for example where absolute                     positioning causes the calculation of the canvas                     dimension to fail and a black background is exposed                     (possibly obscuring black text).


        """
        self.do_command("captureEntirePageScreenshot", [filename,kwargs,])


    def rollup(self,rollupName,kwargs):
        """
        Executes a command rollup, which is a series of commands with a unique
        name, and optionally arguments that control the generation of the set of
        commands. If any one of the rolled-up commands fails, the rollup is
        considered to have failed. Rollups may also contain nested rollups.

        'rollupName' is the name of the rollup command
        'kwargs' is keyword arguments string that influences how the                    rollup expands into commands
        """
        self.do_command("rollup", [rollupName,kwargs,])


    def add_script(self,scriptContent,scriptTagId):
        """
        Loads script content into a new script tag in the Selenium document. This
        differs from the runScript command in that runScript adds the script tag
        to the document of the AUT, not the Selenium document. The following
        entities in the script content are replaced by the characters they
        represent:

            &lt;
            &gt;
            &amp;

        The corresponding remove command is removeScript.

        'scriptContent' is the Javascript content of the script to add
        'scriptTagId' is (optional) the id of the new script tag. If                       specified, and an element with this id already                       exists, this operation will fail.
        """
        self.do_command("addScript", [scriptContent,scriptTagId,])


    def remove_script(self,scriptTagId):
        """
        Removes a script tag from the Selenium document identified by the given
        id. Does nothing if the referenced tag doesn't exist.

        'scriptTagId' is the id of the script element to remove.
        """
        self.do_command("removeScript", [scriptTagId,])


    def use_xpath_library(self,libraryName):
        """
        Allows choice of one of the available libraries.

        'libraryName' is name of the desired library Only the following three can be chosen:
        *   "ajaxslt" - Google's library
        *   "javascript-xpath" - Cybozu Labs' faster library
        *   "default" - The default library.  Currently the default library is "ajaxslt" .

         If libraryName isn't one of these three, then  no change will be made.
        """
        self.do_command("useXpathLibrary", [libraryName,])


    def set_context(self,context):
        """
        Writes a message to the status bar and adds a note to the browser-side
        log.

        'context' is the message to be sent to the browser
        """
        self.do_command("setContext", [context,])


    def attach_file(self,fieldLocator,fileLocator):
        """
        Sets a file input (upload) field to the file listed in fileLocator

        'fieldLocator' is an element locator
        'fileLocator' is a URL pointing to the specified file. Before the file  can be set in the input field (fieldLocator), Selenium RC may need to transfer the file    to the local machine before attaching the file in a web page form. This is common in selenium  grid configurations where the RC server driving the browser is not the same  machine that started the test.   Supported Browsers: Firefox ("\*chrome") only.
        """
        self.do_command("attachFile", [fieldLocator,fileLocator,])


    def capture_screenshot(self,filename):
        """
        Captures a PNG screenshot to the specified file.

        'filename' is the absolute path to the file to be written, e.g. "c:\blah\screenshot.png"
        """
        self.do_command("captureScreenshot", [filename,])


    def capture_screenshot_to_string(self):
        """
        Capture a PNG screenshot.  It then returns the file as a base 64 encoded string.

        """
        return self.get_string("captureScreenshotToString", [])


    def captureNetworkTraffic(self, type):
        """
        Returns the network traffic seen by the browser, including headers, AJAX requests, status codes, and timings. When this function is called, the traffic log is cleared, so the returned content is only the traffic seen since the last call.

        'type' is The type of data to return the network traffic as. Valid values are: json, xml, or plain.
        """
        return self.get_string("captureNetworkTraffic", [type,])

    def addCustomRequestHeader(self, key, value):
        """
        Tells the Selenium server to add the specificed key and value as a custom outgoing request header. This only works if the browser is configured to use the built in Selenium proxy.

        'key' the header name.
        'value' the header value.
        """
        return self.do_command("addCustomRequestHeader", [key,value,])

    def capture_entire_page_screenshot_to_string(self,kwargs):
        """
        Downloads a screenshot of the browser current window canvas to a
        based 64 encoded PNG file. The \ *entire* windows canvas is captured,
        including parts rendered outside of the current view port.

        Currently this only works in Mozilla and when running in chrome mode.

        'kwargs' is A kwargs string that modifies the way the screenshot is captured. Example: "background=#CCFFDD". This may be useful to set for capturing screenshots of less-than-ideal layouts, for example where absolute positioning causes the calculation of the canvas dimension to fail and a black background is exposed  (possibly obscuring black text).
        """
        return self.get_string("captureEntirePageScreenshotToString", [kwargs,])


    def shut_down_selenium_server(self):
        """
        Kills the running Selenium Server and all browser sessions.  After you run this command, you will no longer be able to send
        commands to the server; you can't remotely start the server once it has been stopped.  Normally
        you should prefer to run the "stop" command, which terminates the current browser session, rather than
        shutting down the entire server.

        """
        self.do_command("shutDownSeleniumServer", [])


    def retrieve_last_remote_control_logs(self):
        """
        Retrieve the last messages logged on a specific remote control. Useful for error reports, especially
        when running multiple remote controls in a distributed environment. The maximum number of log messages
        that can be retrieve is configured on remote control startup.

        """
        return self.get_string("retrieveLastRemoteControlLogs", [])


    def key_down_native(self,keycode):
        """
        Simulates a user pressing a key (without releasing it yet) by sending a native operating system keystroke.
        This function uses the java.awt.Robot class to send a keystroke; this more accurately simulates typing
        a key on the keyboard.  It does not honor settings from the shiftKeyDown, controlKeyDown, altKeyDown and
        metaKeyDown commands, and does not target any particular HTML element.  To send a keystroke to a particular
        element, focus on the element first before running this command.

        'keycode' is an integer keycode number corresponding to a java.awt.event.KeyEvent; note that Java keycodes are NOT the same thing as JavaScript keycodes!
        """
        self.do_command("keyDownNative", [keycode,])


    def key_up_native(self,keycode):
        """
        Simulates a user releasing a key by sending a native operating system keystroke.
        This function uses the java.awt.Robot class to send a keystroke; this more accurately simulates typing
        a key on the keyboard.  It does not honor settings from the shiftKeyDown, controlKeyDown, altKeyDown and
        metaKeyDown commands, and does not target any particular HTML element.  To send a keystroke to a particular
        element, focus on the element first before running this command.

        'keycode' is an integer keycode number corresponding to a java.awt.event.KeyEvent; note that Java keycodes are NOT the same thing as JavaScript keycodes!
        """
        self.do_command("keyUpNative", [keycode,])


    def key_press_native(self,keycode):
        """
        Simulates a user pressing and releasing a key by sending a native operating system keystroke.
        This function uses the java.awt.Robot class to send a keystroke; this more accurately simulates typing
        a key on the keyboard.  It does not honor settings from the shiftKeyDown, controlKeyDown, altKeyDown and
        metaKeyDown commands, and does not target any particular HTML element.  To send a keystroke to a particular
        element, focus on the element first before running this command.

        'keycode' is an integer keycode number corresponding to a java.awt.event.KeyEvent; note that Java keycodes are NOT the same thing as JavaScript keycodes!
        """
        self.do_command("keyPressNative", [keycode,])

########NEW FILE########
__FILENAME__ = suite
import unittest

import homepage
import account_tests
import newsfeed_tests

suite = unittest.TestSuite()
suite.addTest(homepage.HomepageTest("test_homepage"))
suite.addTest(account_tests.AccountTests("test_sign_up"))
suite.addTest(account_tests.AccountTests("test_login"))
suite.addTest(account_tests.AccountTests("test_organization_create"))
suite.addTest(newsfeed_tests.NewsfeedTests("test_create_story"))
suite.addTest(newsfeed_tests.NewsfeedTests("test_newsfeed_activity_create"))

unittest.TextTestRunner(verbosity=2).run(suite)

########NEW FILE########
__FILENAME__ = modpython

import os
import sys

from os.path import abspath, dirname, join
from site import addsitedir

VIRTUALENV_BASE = "/Users/mhughes/projects/personal/backlog/pinax-env"
if not VIRTUALENV_BASE:
    raise Exception("VIRTUALENV_BASE is not set correctly.")

activate_this = join(VIRTUALENV_BASE, "bin/activate_this.py")
execfile(activate_this, dict(__file__=activate_this))

from django.core.handlers.modpython import ModPythonHandler


class PinaxModPythonHandler(ModPythonHandler):
    def __call__(self, req):
        # mod_python fakes the environ, and thus doesn't process SetEnv.
        # This fixes that. Django will call this again since there is no way
        # of overriding __call__ to just process the request.
        os.environ.update(req.subprocess_env)
        from django.conf import settings

        sys.path.insert(0, abspath(join(dirname(__file__), "../../")))

        sys.path.insert(0, join(settings.PINAX_ROOT, "apps"))
        sys.path.insert(0, join(settings.PROJECT_ROOT, "apps"))

        return super(PinaxModPythonHandler, self).__call__(req)


def handler(req):
    # mod_python hooks into this function.
    return PinaxModPythonHandler()(req)

########NEW FILE########
__FILENAME__ = feeds
from django.contrib.syndication.feeds import FeedDoesNotExist
from django.core.exceptions import ObjectDoesNotExist
from django.contrib.syndication.feeds import Feed
from django.shortcuts import get_object_or_404
from projects.models import Project, ProjectMember, Iteration, Story
from activities.models import NewsItem

import activities.feedgenerator as feedgenerator

import logging

logger = logging.getLogger(__name__)

# 
# class OrganizationStories(Feed):
# 
#     def __init__(self, slug, request):
#         self.feed_type = feedgenerator.DefaultFeed
#         self.slug = slug
#         self.request = request
#         self.feed_url = self.feed_url or request.path
#         self.title_template_name = self.title_template or ('feeds/%s_title.html' % slug)
#         self.description_template_name = self.description_template or ('feeds/%s_description.html' % slug)
# 
# 
#     def get_object(self,key_and_token):
#         if len(key_and_token) != 2:
#             raise FeedDoesNotExist
#         project = get_object_or_404(Project, pk=key_and_token[0])
#         if project.token == key_and_token[1]:
#             return project
#         else:
#             return None
# 
#     def item_pubdate(self, item):
#         # Returning the time the action was created, lets RSS readers sort them properly.
#         return item.created
# 
#     def title(self, obj):
#         return "Scrumdo - %s" % obj.name
# 
#     def link(self, obj):
#         if not obj:
#             raise FeedDoesNotExist
#         return obj.get_absolute_url()
# 
#     def item_enclosure_url(self, item):
#         try:
#             return obj.get_absolute_url()
#         except:
#             return ""
# 
#     def item_link(self, obj):
#         try:
#             return obj.get_absolute_url()
#         except:
#             return ""
# 
#     def item_guid(self, obj):
#         # We need to return unique GUIDs for each activity, or RSS readers will assume they're the same entry
#         return "GUID-%d" % obj.id
# 
#     def description(self, obj):
#         return "Recent work in all iterations of project."
# 
#     def items(self, obj):
#         if not obj.active:
#             return []
#         activities = NewsItem.objects.filter(project = obj)
#         return activities[:60]

class ProjectStories(Feed):

    def __init__(self, slug, request):
        self.feed_type = feedgenerator.DefaultFeed
        self.slug = slug
        self.request = request
        self.feed_url = self.feed_url or request.path
        self.title_template_name = self.title_template or ('feeds/%s_title.html' % slug)
        self.description_template_name = self.description_template or ('feeds/%s_description.html' % slug)


    def get_object(self,key_and_token):
        if len(key_and_token) != 2:
            raise FeedDoesNotExist
        project = get_object_or_404(Project, pk=key_and_token[0])
        if project.token == key_and_token[1]:
            return project
        else:
            return None

    def item_pubdate(self, item):
        # Returning the time the action was created, lets RSS readers sort them properly.
        return item.created

    def title(self, obj):
        return "Scrumdo - %s" % obj.name

    def link(self, obj):
        if not obj:
            raise FeedDoesNotExist
        return obj.get_absolute_url()

    def item_enclosure_url(self, item):
        try:
            return obj.get_absolute_url()
        except:
            return ""

    def item_link(self, obj):
        try:
            return obj.get_absolute_url()
        except:
            return ""

    def item_guid(self, obj):
        # We need to return unique GUIDs for each activity, or RSS readers will assume they're the same entry
        return "GUID-%d" % obj.id

    def description(self, obj):
        return "Recent work in all iterations of project."

    def items(self, obj):
        if not obj.active:
            return []
        activities = NewsItem.objects.filter(project = obj)
        return activities[:60]

def getIterationsStories(iterations):
    activities = []
    for iteration in iterations:
        activities.extend(StoryActivity.objects.filter(story__iteration=iteration))
        activities.extend(IterationActivity.objects.filter(iteration=iteration))
    print activities
    return sorted(activities[:30], lambda a,b: cmp(a.created,b.created))



class ProjectCurrentStories(ProjectStories):
    def description(self, obj):
        return "Recent work in current iteration of project."

    def title(self, obj):
        return "Scrumdo - %s - Current Iteration" % obj.name

    def items(self, obj):
        iterations = obj.get_current_iterations()
        return getIterationsStories(iterations)

class ProjectIterationStories(Feed):

    def __init__(self, slug, request):
        self.feed_type = feedgenerator.DefaultFeed
        self.slug = slug
        self.request = request
        self.feed_url = self.feed_url or request.path
        self.title_template_name = self.title_template or ('feeds/%s_title.html' % slug)
        self.description_template_name = self.description_template or ('feeds/%s_description.html' % slug)

    def get_object(self,project_and_iteration):
        logger.debug( project_and_iteration )
        if len(project_and_iteration) != 3:
            raise ObjectDoesNotExist
        project = get_object_or_404(Project, pk=project_and_iteration[0])
        if project.token == project_and_iteration[1]:
            iteration = get_object_or_404(Iteration, pk=project_and_iteration[2])
            return (project,iteration)
        else:
            return None

    def item_guid(self, obj):
        # We need to return unique GUIDs for each activity, or RSS readers will assume they're the same entry
        return "GUID-%d" % obj.id

    def title(self, obj):
        return "Scrumdo - %s / %s" % (obj[0].name, obj[1].name)

    def link(self, obj):
        if not obj[0]:
            raise FeedDoesNotExist
        return obj[0].get_absolute_url()

    def item_link(self, obj):
        try:
            return obj.story.iteration.get_absolute_url()
        except:
            return obj.iteration.get_absolute_url()

    def description(self, obj):
        return "Stories in Iteration %s of project." % obj[1].name

    def items(self, obj):
        return getIterationsStories([obj[1]])

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
import sys

from os.path import abspath, dirname, join

try:
    import pinax
except ImportError:
    sys.stderr.write("Error: Can't import Pinax. Make sure you are in a virtual environment that has Pinax installed or create one with pinax-boot.py.\n")
    sys.exit(1)

from django.conf import settings
from django.core.management import setup_environ, execute_from_command_line

try:
    import settings as settings_mod # Assumed to be in the same directory.
except ImportError:
    sys.stderr.write("Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\nYou'll have to run django-admin.py, passing it your settings module.\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\n" % __file__)
    sys.exit(1)

# setup the environment before we start accessing things in the settings.
setup_environ(settings_mod)

sys.path.insert(0, join(settings.PINAX_ROOT, "apps"))
sys.path.insert(0, join(settings.PROJECT_ROOT, "apps"))

if __name__ == "__main__":
    execute_from_command_line()

########NEW FILE########
__FILENAME__ = scrumdo_utils
from hashlib import sha1
from django.core.cache import cache as _djcache
import logging

logger = logging.getLogger(__name__)
def cache(seconds = 900):
    """
        Cache the result of a function call for the specified number of seconds,
        using Django's caching mechanism.
        Assumes that the function never returns None (as the cache returns None to indicate a miss), and that the function's result only depends on its parameters.
        Note that the ordering of parameters is important. e.g. myFunction(x = 1, y = 2), myFunction(y = 2, x = 1), and myFunction(1,2) will each be cached separately.

        Usage:

        @cache(600)
        def myExpensiveMethod(parm1, parm2, parm3):
            ....
            return expensiveResult
`
    """
    def doCache(f):
        def x(*args, **kwargs):
            key = sha1(str(f.__module__) + str(f.__name__) + str(args) + str(kwargs)).hexdigest()
            # logger.debug("Cache key: %s",str(args))
            result = _djcache.get(key)
            if result is None:
                result = f(*args, **kwargs)
                _djcache.set(key, result, seconds)
            return result
        return x
    return doCache

########NEW FILE########
__FILENAME__ = search_sites
import haystack
haystack.autodiscover()

########NEW FILE########
__FILENAME__ = settings
# -*- coding: utf-8 -*-

# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

# Django settings for code project.


# Import all
import os.path
import posixpath
import pinax
import logging



PINAX_ROOT = os.path.abspath(os.path.dirname(pinax.__file__))
PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))

# tells Pinax to use the default theme
PINAX_THEME = 'default'

# An extra path to look for scrumdo extras on.
EXTRA_PATH = False

DEBUG = True
TEMPLATE_DEBUG = DEBUG

# tells Pinax to serve media through django.views.static.serve.
SERVE_MEDIA = True

ADMINS = (
    # ('Your Name', 'your_email@domain.com'),
)

MANAGERS = ADMINS

DATABASE_ENGINE = 'mysql'    # 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'ado_mssql'.
DATABASE_NAME = 'scrumdo'       # Or path to database file if using sqlite3.
DATABASE_USER = 'scrumaster'             # Not used with sqlite3.
DATABASE_PASSWORD = 'dlfksj39028'         # Not used with sqlite3.
DATABASE_HOST = ''             # Set to empty string for localhost. Not used with sqlite3.
DATABASE_PORT = ''             # Set to empty string for default. Not used with sqlite3.

# Local time zone for this installation. Choices can be found here:
# http://www.postgresql.org/docs/8.1/static/datetime-keywords.html#DATETIME-TIMEZONE-SET-TABLE
# although not all variations may be possible on all operating systems.
# If running in a Windows environment this must be set to the same as your
# system time zone.
TIME_ZONE = 'US/Eastern'

# Language code for this installation. All choices can be found here:
# http://www.w3.org/TR/REC-html40/struct/dirlang.html#langcodes
# http://blogs.law.harvard.edu/tech/stories/storyReader$15
LANGUAGE_CODE = 'en'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# Absolute path to the directory that holds media.
# Example: "/home/media/media.lawrence.com/"
MEDIA_ROOT = os.path.join(PROJECT_ROOT, 'site_media', 'media')

# URL that handles the media served from MEDIA_ROOT.
# Example: "http://media.lawrence.com"
MEDIA_URL = '/site_media/media/'

# Absolute path to the directory that holds static files like app media.
# Example: "/home/media/media.lawrence.com/apps/"
STATIC_ROOT = os.path.join(PROJECT_ROOT, 'site_media', 'static')

# URL that handles the static files like app media.
# Example: "http://media.lawrence.com"
STATIC_URL = '/site_media/static/'
SSL_STATIC_URL = '/site_media/static/'

# Additional directories which hold static files
STATICFILES_DIRS = (
    ('scrumdo-web', os.path.join(PROJECT_ROOT, 'media')),
    ('pinax', os.path.join(PINAX_ROOT, 'media', PINAX_THEME)),
)

# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a
# trailing slash.
# Examples: "http://foo.com/media/", "/media/".
ADMIN_MEDIA_PREFIX = posixpath.join(STATIC_URL, "admin/")

# Make this unique, and don't share it with anybody.
SECRET_KEY = 'cl@#$@#!%$^!42164363246y@18*^@-!+$fu^q!sa6yh2^'

# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.load_template_source',
    'django.template.loaders.app_directories.load_template_source',
)

AVATAR_DEFAULT_URL =  STATIC_URL +'images/defaultAvatar.png'
AVATAR_GRAVATAR_BACKUP = True


MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django_openid.consumer.SessionConsumer',
    'account.middleware.LocaleMiddleware',
    'django.middleware.doc.XViewMiddleware',
    'pagination.middleware.PaginationMiddleware',
    'django_sorting.middleware.SortingMiddleware',
    'pinax.middleware.security.HideSensistiveFieldsMiddleware',
    'django.middleware.transaction.TransactionMiddleware',
    # 'debug_toolbar.middleware.DebugToolbarMiddleware'
)

ROOT_URLCONF = 'urls'

TEMPLATE_DIRS = (
    os.path.join(PROJECT_ROOT, "templates"),
    os.path.join(PINAX_ROOT, "templates", PINAX_THEME),
)

TEMPLATE_CONTEXT_PROCESSORS = (
    "django.core.context_processors.auth",
    "django.core.context_processors.debug",
    "django.core.context_processors.i18n",
    "django.core.context_processors.media",
    "django.core.context_processors.request",    
    "pinax.core.context_processors.pinax_settings",
    "projects.context_processors.projects_constants",
    "announcements.context_processors.site_wide_announcements",
    "account.context_processors.openid",
    "account.context_processors.account",
)

INSTALLED_APPS = (
    # included
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.humanize',
    'django.contrib.markup',
    'django.contrib.admin',
    'pinax.templatetags',

    # external
    'django_openid',
    'emailconfirmation',
    'mailer',
    'announcements',
    'pagination',
    'timezones',
    'avatar',
    'threadedcomments',
    'ajax_validation',
    'tagging',
    'uni_form',
    'wiki',
    'django_sorting',
    'attachments',
    'django_markup',
    'django_filters',
    'staticfiles',
    'notification',
#    'tastypie',

    # internal (for now)
    'basic_profiles',
    'account',
    'signup_codes',
    'about',
    'tag_app',
    'tagging_utils',
    'threadedcomments_extras',
    'groups',
    'projects',
    'organizations',
    'topics',
    'activities',
    'django_extensions',
    'django_evolution',
    'extras',
    'activities',
    'tutorial',
    'api',
    'developer',
    'haystack',
    'favorites',
    'scrum_log',
    # 'debug_toolbar',
)


HAYSTACK_SITECONF = "search_sites"
HAYSTACK_SEARCH_ENGINE = "solr"
HAYSTACK_SOLR_URL = 'http://localhost:8983/solr'


ABSOLUTE_URL_OVERRIDES = {
    "auth.user": lambda o: "/profiles/profile/%s/" % o.username,
}

MARKUP_FILTER_FALLBACK = 'none'
MARKUP_CHOICES = (
    ('restructuredtext', u'reStructuredText'),
    ('textile', u'Textile'),
    ('markdown', u'Markdown'),
    ('creole', u'Creole'),
)
WIKI_MARKUP_CHOICES = MARKUP_CHOICES

AUTH_PROFILE_MODULE = 'basic_profiles.Profile'
NOTIFICATION_LANGUAGE_MODULE = 'account.Account'

ACCOUNT_OPEN_SIGNUP = True
ACCOUNT_REQUIRED_EMAIL = True
ACCOUNT_EMAIL_VERIFICATION = False

EMAIL_CONFIRMATION_DAYS = 2
EMAIL_DEBUG = DEBUG

SITE_NAME = "ScrumDo Community Site"
LOGIN_URL = "/account/login/"
LOGIN_REDIRECT_URLNAME = "projects.views.home"


EMAIL_HOST='localhost'
EMAIL_HOST_USER=''
EMAIL_HOST_PASSWORD=''
EMAIL_PORT='25'

CONTACT_EMAIL = "help@example.com"
DEFAULT_FROM_EMAIL = 'noreply@example.com'
SERVER_EMAIL = 'noreply@example.com'
SUPPORT_URL = "http://support.example.com/"

GOOGLE_ANALYTICS = False
GOOGLE_ANALYTICS_ACCOUNT = ""

CACHE_BACKEND = 'locmem://'

INTERNAL_IPS = ('127.0.0.1',)

BASE_URL="http://localhost:8000"
SSL_BASE_URL="http://localhost:8000"

SCRUMDO_EXTRAS = ()
 #"extras.plugins.github_issues.GitHubIssuesExtra",
 #                 "extras.plugins.example.ExampleExtra",)


HOOKBOX_HOST = "http://192.168.1.125:8080"
HOOKBOX_SECRET = "juy789"

# local_settings.py can be used to override environment-specific settings
# like database and email that differ between development and production.
try:
    from local_settings import *
except ImportError as e:
    print "Could not import local_settings %s" % e

########NEW FILE########
__FILENAME__ = urls
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

from django.conf.urls.defaults import *
from django.conf import settings
import views
import feeds

from django.views.generic.simple import direct_to_template, redirect_to

from django.contrib import admin

admin.autodiscover()

from account.openid_consumer import PinaxConsumer


if settings.ACCOUNT_OPEN_SIGNUP:
    signup_view = "account.views.signup"
else:
    signup_view = "signup_codes.views.signup"

feeds = {    
    'project':feeds.ProjectStories
}


urlpatterns = patterns('',
    url(r'^$', "projects.views.home" , name="home"),
    url(r'^robots.txt$', direct_to_template, {'template': 'robots.txt'}),
    url(r'^status$', views.status),
    url(r'^crossdomain.xml$', direct_to_template, {'template': 'crossdomain.xml'}),
    url(r'^stats$', views.stats),
    url(r'^stats_data$', views.stats_data ),
    url(r'^account/signup/$', signup_view, name="acct_signup"),
    (r'^extras/', include('extras.urls')),
    (r'^forum/', 'django.views.generic.simple.redirect_to', {'url':settings.SUPPORT_URL}),
    (r'^feeds/(?P<url>.*)/$', 'django.contrib.syndication.views.feed', {'feed_dict': feeds}),
    (r'^feeds/(?P<url>.*)$', 'django.contrib.syndication.views.feed', {'feed_dict': feeds}),
    (r'^about/', include('about.urls')),
    (r'^account/', include('account.urls')),
    (r'^openid/(.*)', PinaxConsumer()),
    (r'^profiles/', include('basic_profiles.urls')),
    (r'^avatar/', include('avatar.urls')),
    (r'^comments/', include('threadedcomments.urls')),
    (r'^announcements/', include('announcements.urls')),
    (r'^tagging_utils/', include('tagging_utils.urls')),
    (r'^projects/', include('projects.urls')),
    (r'^activities/', include('activities.urls')),
    (r'^organization/', include('organizations.urls')),
    (r'^tutorial/', include('tutorial.urls')),
    (r'^admin/', include(admin.site.urls)),
    (r'^api/', include('api.urls')),
    (r'^developer/', include('developer.urls')),
    (r'^favorites/', include('favorites.urls')),
    # (r'^search/', include('haystack.urls')),
    url(r'^usage', "projects.views.usage", name="usage"),
)

if settings.SERVE_MEDIA:
    urlpatterns += patterns('',
        (r'^site_media/', include('staticfiles.urls')),
    )

########NEW FILE########
__FILENAME__ = views
# ScrumDo - Agile/Scrum story management web application
# Copyright (C) 2011 ScrumDo LLC
#
# This software is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy (See file COPYING) of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA

from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.http import HttpResponse
from django.core import serializers
from django.core.exceptions import PermissionDenied
from django.contrib.auth.decorators import login_required
from django.db import connection
from datetime import datetime, date

import time
import json

from projects.models import Project, SiteStats


@login_required
def stats(request):
    if not request.user.is_staff:
        raise PermissionDenied()
    cursor = connection.cursor()
    cursor.execute("select count(*) as story_count, p.*  from projects_project as p join projects_story as s on s.project_id=p.id group by p.id order by story_count DESC limit 50")

    topProjects = cursor.fetchall()
    print len(topProjects)



    return render_to_response( 'site_stats.html' , {'top_projects':topProjects}, context_instance=RequestContext(request))

def status(request):
    p = Project.objects.count()
    if p <= 0:
        raise Exception("Bad project count","Bad")
    return HttpResponse("ok")

def stats_data(request):
    stats = SiteStats.objects.all();
    user_data = []
    user_stats = { "label":"Users", "data":user_data}
    project_data = []
    project_stats = { "label":"Projects", "data":project_data}
    # story_data = []
    # story_stats = { "label":"Stories", "data":story_data, "yaxis": 2}
    for stat in stats:
        stat_time = int((time.mktime(stat.date.timetuple()) - time.timezone)*1000)
        user_data.append( [stat_time, stat.user_count] );
        project_data.append( [stat_time, stat.project_count] );
        # story_data.append( [stat_time, stat.story_count] );

    json_serializer = serializers.get_serializer("json")()
    result = json.dumps([user_stats,project_stats])
    return HttpResponse(result) #, mimetype='application/json'

########NEW FILE########

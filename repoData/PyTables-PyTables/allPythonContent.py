__FILENAME__ = blosc
from __future__ import print_function
import os
import sys
from time import time
import numpy as np
import tables as tb


niter = 3
dirname = "/scratch2/faltet/blosc-data/"
#expression = "a**2 + b**3 + 2*a*b + 3"
#expression = "a+b"
#expression = "a**2 + 2*a/b + 3"
#expression = "(a+b)**2 - (a**2 + b**2 + 2*a*b) + 1.1"
expression = "3*a-2*b+1.1"
shuffle = True


def create_file(kind, prec, synth):
    prefix_orig = 'cellzome/cellzome-'
    iname = dirname + prefix_orig + 'none-' + prec + '.h5'
    f = tb.open_file(iname, "r")

    if prec == "single":
        type_ = tb.Float32Atom()
    else:
        type_ = tb.Float64Atom()

    if synth:
        prefix = 'synth/synth-'
    else:
        prefix = 'cellzome/cellzome-'

    for clevel in range(10):
        oname = '%s/%s-%s%d-%s.h5' % (dirname, prefix, kind, clevel, prec)
        # print "creating...", iname
        f2 = tb.open_file(oname, "w")

        if kind in ["none", "numpy"]:
            filters = None
        else:
            filters = tb.Filters(
                complib=kind, complevel=clevel, shuffle=shuffle)

        for name in ['maxarea', 'mascotscore']:
            col = f.get_node('/', name)
            r = f2.create_carray('/', name, type_, col.shape, filters=filters)
            if synth:
                r[:] = np.arange(col.nrows, dtype=type_.dtype)
            else:
                r[:] = col[:]
        f2.close()
        if clevel == 0:
            size = 1.5 * float(os.stat(oname)[6])
    f.close()
    return size


def create_synth(kind, prec):

    prefix_orig = 'cellzome/cellzome-'
    iname = dirname + prefix_orig + 'none-' + prec + '.h5'
    f = tb.open_file(iname, "r")

    if prec == "single":
        type_ = tb.Float32Atom()
    else:
        type_ = tb.Float64Atom()

    prefix = 'synth/synth-'
    for clevel in range(10):
        oname = '%s/%s-%s%d-%s.h5' % (dirname, prefix, kind, clevel, prec)
        # print "creating...", iname
        f2 = tb.open_file(oname, "w")

        if kind in ["none", "numpy"]:
            filters = None
        else:
            filters = tb.Filters(
                complib=kind, complevel=clevel, shuffle=shuffle)

        for name in ['maxarea', 'mascotscore']:
            col = f.get_node('/', name)
            r = f2.create_carray('/', name, type_, col.shape, filters=filters)
            if name == 'maxarea':
                r[:] = np.arange(col.nrows, dtype=type_.dtype)
            else:
                r[:] = np.arange(col.nrows, 0, dtype=type_.dtype)

        f2.close()
        if clevel == 0:
            size = 1.5 * float(os.stat(oname)[6])
    f.close()
    return size


def process_file(kind, prec, clevel, synth):

    if kind == "numpy":
        lib = "none"
    else:
        lib = kind
    if synth:
        prefix = 'synth/synth-'
    else:
        prefix = 'cellzome/cellzome-'
    iname = '%s/%s-%s%d-%s.h5' % (dirname, prefix, kind, clevel, prec)
    f = tb.open_file(iname, "r")
    a_ = f.root.maxarea
    b_ = f.root.mascotscore

    oname = '%s/%s-%s%d-%s-r.h5' % (dirname, prefix, kind, clevel, prec)
    f2 = tb.open_file(oname, "w")
    if lib == "none":
        filters = None
    else:
        filters = tb.Filters(complib=lib, complevel=clevel, shuffle=shuffle)
    if prec == "single":
        type_ = tb.Float32Atom()
    else:
        type_ = tb.Float64Atom()
    r = f2.create_carray('/', 'r', type_, a_.shape, filters=filters)

    if kind == "numpy":
        a2, b2 = a_[:], b_[:]
        t0 = time()
        r = eval(expression, {'a': a2, 'b': b2})
        print("%5.2f" % round(time() - t0, 3))
    else:
        expr = tb.Expr(expression, {'a': a_, 'b': b_})
        expr.set_output(r)
        expr.eval()
    f.close()
    f2.close()
    size = float(os.stat(iname)[6]) + float(os.stat(oname)[6])
    return size


if __name__ == '__main__':
    if len(sys.argv) > 3:
        kind = sys.argv[1]
        prec = sys.argv[2]
        if sys.argv[3] == "synth":
            synth = True
        else:
            synth = False
    else:
        print("3 parameters required")
        sys.exit(1)

    # print "kind, precision, synth:", kind, prec, synth

    # print "Creating input files..."
    size_orig = create_file(kind, prec, synth)

    # print "Processing files for compression levels in range(10)..."
    for clevel in range(10):
        t0 = time()
        ts = []
        for i in range(niter):
            size = process_file(kind, prec, clevel, synth)
            ts.append(time() - t0)
            t0 = time()
        ratio = size_orig / size
        print("%5.2f, %5.2f" % (round(min(ts), 3), ratio))

########NEW FILE########
__FILENAME__ = bsddb-table-bench
#!/usr/bin/env python
###### WARNING #######
### This script is obsoleted ###
# If you get it working again, please drop me a line
# F. Alted 2004-01-27

from __future__ import print_function
import sys
import struct
import cPickle

from tables import *
import numpy as np

try:
    # For Python 2.3
    from bsddb import db
except ImportError:
    # For earlier Pythons w/distutils pybsddb
    from bsddb3 import db
import psyco


# This class is accessible only for the examples
class Small(IsDescription):
    """Record descriptor.

    A record has several columns. They are represented here as class
    attributes, whose names are the column names and their values will
    become their types. The IsColDescr class will take care the user
    will not add any new variables and that its type is correct.

    """

    var1 = StringCol(itemsize=16)
    var2 = Int32Col()
    var3 = Float64Col()

# Define a user record to characterize some kind of particles


class Medium(IsDescription):
    name = StringCol(itemsize=16, pos=0)  # 16-character String
    #float1      = Float64Col(shape=2, dflt=2.3)
    float1 = Float64Col(dflt=1.3, pos=1)
    float2 = Float64Col(dflt=2.3, pos=2)
    ADCcount = Int16Col(pos=3)     # signed short integer
    grid_i = Int32Col(pos=4)        # integer
    grid_j = Int32Col(pos=5)        # integer
    pressure = Float32Col(pos=6)    # float  (single-precision)
    energy = Float64Col(pos=7)      # double (double-precision)

# Define a user record to characterize some kind of particles


class Big(IsDescription):
    name = StringCol(itemsize=16)   # 16-character String
    #float1 = Float64Col(shape=32, dflt=np.arange(32))
    #float2 = Float64Col(shape=32, dflt=np.arange(32))
    float1 = Float64Col(shape=32, dflt=range(32))
    float2 = Float64Col(shape=32, dflt=[2.2] * 32)
    ADCcount = Int16Col()           # signed short integer
    grid_i = Int32Col()             # integer
    grid_j = Int32Col()             # integer
    pressure = Float32Col()         # float  (single-precision)
    energy = Float64Col()           # double (double-precision)


def createFile(filename, totalrows, recsize, verbose):

    # Open a 'n'ew file
    dd = db.DB()
    if recsize == "big":
        isrec = Description(Big)
    elif recsize == "medium":
        isrec = Medium()
    else:
        isrec = Description(Small)
    # dd.set_re_len(struct.calcsize(isrec._v_fmt))  # fixed length records
    dd.open(filename, db.DB_RECNO, db.DB_CREATE | db.DB_TRUNCATE)

    rowswritten = 0
    # Get the record object associated with the new table
    if recsize == "big":
        isrec = Big()
        arr = np.array(np.arange(32), type=np.Float64)
        arr2 = np.array(np.arange(32), type=np.Float64)
    elif recsize == "medium":
        isrec = Medium()
        arr = np.array(np.arange(2), type=np.Float64)
    else:
        isrec = Small()
    # print d
    # Fill the table
    if recsize == "big" or recsize == "medium":
        d = {"name": " ",
             "float1": 1.0,
             "float2": 2.0,
             "ADCcount": 12,
             "grid_i": 1,
             "grid_j": 1,
             "pressure": 1.9,
             "energy": 1.8,
             }
        for i in range(totalrows):
            #d['name']  = 'Particle: %6d' % (i)
            #d['TDCcount'] = i % 256
            d['ADCcount'] = (i * 256) % (1 << 16)
            if recsize == "big":
                #d.float1 = np.array([i]*32, np.Float64)
                #d.float2 = np.array([i**2]*32, np.Float64)
                arr[0] = 1.1
                d['float1'] = arr
                arr2[0] = 2.2
                d['float2'] = arr2
                pass
            else:
                d['float1'] = float(i)
                d['float2'] = float(i)
            d['grid_i'] = i
            d['grid_j'] = 10 - i
            d['pressure'] = float(i * i)
            d['energy'] = d['pressure']
            dd.append(cPickle.dumps(d))
#             dd.append(struct.pack(isrec._v_fmt,
#                                   d['name'], d['float1'], d['float2'],
#                                   d['ADCcount'],
#                                   d['grid_i'], d['grid_j'],
#                                   d['pressure'],  d['energy']))
    else:
        d = {"var1": " ", "var2": 1, "var3": 12.1e10}
        for i in range(totalrows):
            d['var1'] = str(i)
            d['var2'] = i
            d['var3'] = 12.1e10
            dd.append(cPickle.dumps(d))
            #dd.append(
            #    struct.pack(isrec._v_fmt, d['var1'], d['var2'], d['var3']))

    rowswritten += totalrows

    # Close the file
    dd.close()
    return (rowswritten, struct.calcsize(isrec._v_fmt))


def readFile(filename, recsize, verbose):
    # Open the HDF5 file in read-only mode
    #fileh = shelve.open(filename, "r")
    dd = db.DB()
    if recsize == "big":
        isrec = Big()
    elif recsize == "medium":
        isrec = Medium()
    else:
        isrec = Small()
    # dd.set_re_len(struct.calcsize(isrec._v_fmt))  # fixed length records
    # dd.set_re_pad('-') # sets the pad character...
    # dd.set_re_pad(45)  # ...test both int and char
    dd.open(filename, db.DB_RECNO)
    if recsize == "big" or recsize == "medium":
        print(isrec._v_fmt)
        c = dd.cursor()
        rec = c.first()
        e = []
        while rec:
            record = cPickle.loads(rec[1])
            #record = struct.unpack(isrec._v_fmt, rec[1])
            # if verbose:
            #    print record
            if record['grid_i'] < 20:
                e.append(record['grid_j'])
            # if record[4] < 20:
            #    e.append(record[5])
            rec = next(c)
    else:
        print(isrec._v_fmt)
        #e = [ t[1] for t in fileh[table] if t[1] < 20 ]
        c = dd.cursor()
        rec = c.first()
        e = []
        while rec:
            record = cPickle.loads(rec[1])
            #record = struct.unpack(isrec._v_fmt, rec[1])
            # if verbose:
            #    print record
            if record['var2'] < 20:
                e.append(record['var1'])
            # if record[1] < 20:
            #    e.append(record[2])
            rec = next(c)

    print("resulting selection list ==>", e)
    print("last record read ==>", record)
    print("Total selected records ==> ", len(e))

    # Close the file (eventually destroy the extended type)
    dd.close()


# Add code to test here
if __name__ == "__main__":
    import getopt
    import time

    usage = """usage: %s [-v] [-s recsize] [-i iterations] file
            -v verbose
            -s use [big] record, [medium] or [small]
            -i sets the number of rows in each table\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 's:vi:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    recsize = "medium"
    iterations = 100
    verbose = 0

    # Get the options
    for option in opts:
        if option[0] == '-s':
            recsize = option[1]
            if recsize not in ["big", "medium", "small"]:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-i':
            iterations = int(option[1])
        elif option[0] == '-v':
            verbose = 1

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    t1 = time.clock()
    psyco.bind(createFile)
    (rowsw, rowsz) = createFile(file, iterations, recsize, verbose)
    t2 = time.clock()
    tapprows = round(t2 - t1, 3)

    t1 = time.clock()
    psyco.bind(readFile)
    readFile(file, recsize, verbose)
    t2 = time.clock()
    treadrows = round(t2 - t1, 3)

    print("Rows written:", rowsw, " Row size:", rowsz)
    print("Time appending rows:", tapprows)
    if tapprows > 0.:
        print("Write rows/sec: ", int(iterations / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))
    print("Time reading rows:", treadrows)
    if treadrows > 0.:
        print("Read rows/sec: ", int(iterations / float(treadrows)))
        print("Read KB/s :", int(rowsw * rowsz / (treadrows * 1024)))

########NEW FILE########
__FILENAME__ = cacheout
# Program to clean out the filesystem cache
import numpy

a = numpy.arange(1000 * 100 * 125, dtype='f8')  # 100 MB of RAM
b = a * 3  # Another 100 MB
# delete the reference to the booked memory
del a
del b

# Do a loop to fully recharge the python interpreter
j = 2
for i in range(1000 * 1000):
    j += i * 2

########NEW FILE########
__FILENAME__ = chunkshape-bench
#!/usr/bin/env python
# Benchmark the effect of chunkshapes in reading large datasets.
# You need at least PyTables 2.1 to run this!
# F. Alted

from __future__ import print_function
import numpy
import tables
from time import time

dim1, dim2 = 360, 6109666
rows_to_read = range(0, 360, 36)

print("=" * 32)
# Create the EArray
f = tables.open_file("/tmp/test.h5", "w")
a = f.create_earray(f.root, "a", tables.Float64Atom(), shape=(dim1, 0),
                    expectedrows=dim2)
print("Chunkshape for original array:", a.chunkshape)

# Fill the EArray
t1 = time()
zeros = numpy.zeros((dim1, 1), dtype="float64")
for i in range(dim2):
    a.append(zeros)
tcre = round(time() - t1, 3)
thcre = round(dim1 * dim2 * 8 / (tcre * 1024 * 1024), 1)
print("Time to append %d rows: %s sec (%s MB/s)" % (a.nrows, tcre, thcre))

# Read some row vectors from the original array
t1 = time()
for i in rows_to_read:
    r1 = a[i, :]
tr1 = round(time() - t1, 3)
thr1 = round(dim2 * len(rows_to_read) * 8 / (tr1 * 1024 * 1024), 1)
print("Time to read ten rows in original array: %s sec (%s MB/s)" % (tr1,
                                                                     thr1))

print("=" * 32)
# Copy the array to another with a row-wise chunkshape
t1 = time()
#newchunkshape = (1, a.chunkshape[0]*a.chunkshape[1])
newchunkshape = (1, a.chunkshape[0] * a.chunkshape[1] * 10)  # ten times larger
b = a.copy(f.root, "b", chunkshape=newchunkshape)
tcpy = round(time() - t1, 3)
thcpy = round(dim1 * dim2 * 8 / (tcpy * 1024 * 1024), 1)
print("Chunkshape for row-wise chunkshape array:", b.chunkshape)
print("Time to copy the original array: %s sec (%s MB/s)" % (tcpy, thcpy))

# Read the same ten rows from the new copied array
t1 = time()
for i in rows_to_read:
    r2 = b[i, :]
tr2 = round(time() - t1, 3)
thr2 = round(dim2 * len(rows_to_read) * 8 / (tr2 * 1024 * 1024), 1)
print("Time to read with a row-wise chunkshape: %s sec (%s MB/s)" % (tr2,
                                                                     thr2))
print("=" * 32)
print("Speed-up with a row-wise chunkshape:", round(tr1 / tr2, 1))

f.close()

########NEW FILE########
__FILENAME__ = chunkshape-testing
#!/usr/bin/env python

"""Simple benchmark for testing chunkshapes and nrowsinbuf."""

from __future__ import print_function
import numpy
import tables
from time import time

L = 20
N = 2000
M = 30
complevel = 1

recarray = numpy.empty(shape=2, dtype='(2,2,2)i4,(2,3,3)f8,i4,i8')

f = tables.open_file("chunkshape.h5", mode="w")

# t = f.create_table(f.root, 'table', recarray, "mdim recarray")

# a0 = f.create_array(f.root, 'field0', recarray['f0'], "mdim int32 array")
# a1 = f.create_array(f.root, 'field1', recarray['f1'], "mdim float64 array")

# c0 = f.create_carray(f.root, 'cfield0',
#                     tables.Int32Atom(), (2,2,2),
#                     "mdim int32 carray")
# c1 = f.create_carray(f.root, 'cfield1',
#                     tables.Float64Atom(), (2,3,3),
#                     "mdim float64 carray")

f1 = tables.open_file("chunkshape1.h5", mode="w")
c1 = f.create_carray(f1.root, 'cfield1',
                     tables.Int32Atom(), (L, N, M),
                     "scalar int32 carray", tables.Filters(complevel=0))

t1 = time()
c1[:] = numpy.empty(shape=(L, 1, 1), dtype="int32")
print("carray1 populate time:", time() - t1)
f1.close()


f2 = tables.open_file("chunkshape2.h5", mode="w")
c2 = f.create_carray(f2.root, 'cfield2',
                     tables.Int32Atom(), (L, M, N),
                     "scalar int32 carray", tables.Filters(complevel))

t1 = time()
c2[:] = numpy.empty(shape=(L, 1, 1), dtype="int32")
print("carray2 populate time:", time() - t1)
f2.close()

f0 = tables.open_file("chunkshape0.h5", mode="w")
e0 = f.create_earray(f0.root, 'efield0',
                     tables.Int32Atom(), (0, L, M),
                     "scalar int32 carray", tables.Filters(complevel),
                     expectedrows=N)

t1 = time()
e0.append(numpy.empty(shape=(N, L, M), dtype="int32"))
print("earray0 populate time:", time() - t1)
f0.close()

f1 = tables.open_file("chunkshape1.h5", mode="w")
e1 = f.create_earray(f1.root, 'efield1',
                     tables.Int32Atom(), (L, 0, M),
                     "scalar int32 carray", tables.Filters(complevel),
                     expectedrows=N)

t1 = time()
e1.append(numpy.empty(shape=(L, N, M), dtype="int32"))
print("earray1 populate time:", time() - t1)
f1.close()


f2 = tables.open_file("chunkshape2.h5", mode="w")
e2 = f.create_earray(f2.root, 'efield2',
                     tables.Int32Atom(), (L, M, 0),
                     "scalar int32 carray", tables.Filters(complevel),
                     expectedrows=N)

t1 = time()
e2.append(numpy.empty(shape=(L, M, N), dtype="int32"))
print("earray2 populate time:", time() - t1)
f2.close()

# t1=time()
# c2[:] = numpy.empty(shape=(M, N), dtype="int32")
# print "carray populate time:", time()-t1

# f3 = f.create_carray(f.root, 'cfield3',
#                     tables.Float64Atom(), (3,),
#                     "scalar float64 carray", chunkshape=(32,))

# e2 = f.create_earray(f.root, 'efield2',
#                     tables.Int32Atom(), (0, M),
#                     "scalar int32 carray", expectedrows=N)
# t1=time()
# e2.append(numpy.empty(shape=(N, M), dtype="int32"))
# print "earray populate time:", time()-t1

# t1=time()
# c2._f_copy(newname='cfield2bis')
# print "carray copy time:", time()-t1
# t1=time()
# e2._f_copy(newname='efield2bis')
# print "earray copy time:", time()-t1

f.close()

########NEW FILE########
__FILENAME__ = collations
from __future__ import print_function
import numpy as np
import tables
from time import time

N = 1000 * 1000
NCOLL = 200  # 200 collections maximum

# In order to have reproducible results
np.random.seed(19)


class Energies(tables.IsDescription):
    collection = tables.UInt8Col()
    energy = tables.Float64Col()


def fill_bucket(lbucket):
    #c = np.random.normal(NCOLL/2, NCOLL/10, lbucket)
    c = np.random.normal(NCOLL / 2, NCOLL / 100, lbucket)
    e = np.arange(lbucket, dtype='f8')
    return c, e

# Fill the table
t1 = time()
f = tables.open_file("data.nobackup/collations.h5", "w")
table = f.create_table("/", "Energies", Energies, expectedrows=N)
# Fill the table with values
lbucket = 1000   # Fill in buckets of 1000 rows, for speed
for i in range(0, N, lbucket):
    bucket = fill_bucket(lbucket)
    table.append(bucket)
# Fill the remaining rows
bucket = fill_bucket(N % lbucket)
table.append(bucket)
f.close()
print("Time to create the table with %d entries: %.3f" % (N, time() - t1))

# Now, read the table and group it by collection
f = tables.open_file("data.nobackup/collations.h5", "a")
table = f.root.Energies

#########################################################
# First solution: load the table completely in memory
#########################################################
t1 = time()
t = table[:]  # convert to structured array
coll1 = []
collections = np.unique(t['collection'])
for c in collections:
    cond = t['collection'] == c
    energy_this_collection = t['energy'][cond]
    sener = energy_this_collection.sum()
    coll1.append(sener)
    print(c, ' : ', sener)
del collections, energy_this_collection
print("Time for first solution: %.3f" % (time() - t1))

#########################################################
# Second solution: load all the collections in memory
#########################################################
t1 = time()
collections = {}
for row in table:
    c = row['collection']
    e = row['energy']
    if c in collections:
        collections[c].append(e)
    else:
        collections[c] = [e]
# Convert the lists in numpy arrays
coll2 = []
for c in sorted(collections):
    energy_this_collection = np.array(collections[c])
    sener = energy_this_collection.sum()
    coll2.append(sener)
    print(c, ' : ', sener)
del collections, energy_this_collection
print("Time for second solution: %.3f" % (time() - t1))

t1 = time()
table.cols.collection.create_csindex()
# table.cols.collection.reindex()
print("Time for indexing: %.3f" % (time() - t1))

#########################################################
# Third solution: load each collection separately
#########################################################
t1 = time()
coll3 = []
for c in np.unique(table.col('collection')):
    energy_this_collection = table.read_where(
        'collection == c', field='energy')
    sener = energy_this_collection.sum()
    coll3.append(sener)
    print(c, ' : ', sener)
del energy_this_collection
print("Time for third solution: %.3f" % (time() - t1))


t1 = time()
table2 = table.copy('/', 'EnergySortedByCollation', overwrite=True,
                    sortby="collection", propindexes=True)
print("Time for sorting: %.3f" % (time() - t1))

#####################################################################
# Fourth solution: load each collection separately.  Sorted table.
#####################################################################
t1 = time()
coll4 = []
for c in np.unique(table2.col('collection')):
    energy_this_collection = table2.read_where(
        'collection == c', field='energy')
    sener = energy_this_collection.sum()
    coll4.append(sener)
    print(c, ' : ', sener)
    del energy_this_collection
print("Time for fourth solution: %.3f" % (time() - t1))


# Finally, check that all solutions do match
assert coll1 == coll2 == coll3 == coll4

f.close()

########NEW FILE########
__FILENAME__ = copy-bench
from __future__ import print_function
import tables
import sys
import time

if len(sys.argv) != 3:
    print("usage: %s source_file dest_file", sys.argv[0])
filesrc = sys.argv[1]
filedest = sys.argv[2]
filehsrc = tables.open_file(filesrc)
filehdest = tables.open_file(filedest, 'w')
ntables = 0
tsize = 0
t1 = time.time()
for group in filehsrc.walk_groups():
    if isinstance(group._v_parent, tables.File):
        groupdest = filehdest.root
    else:
        pathname = group._v_parent._v_pathname
        groupdest = filehdest.create_group(pathname, group._v_name,
                                           title=group._v_title)
    for table in filehsrc.list_nodes(group, classname='Table'):
        print("copying table -->", table)
        table.copy(groupdest, table.name)
        ntables += 1
        tsize += table.nrows * table.rowsize
tsizeMB = tsize / (1024 * 1024)
ttime = round(time.time() - t1, 3)
speed = round(tsizeMB / ttime, 2)
print("Copied %s tables for a total of %s MB in %s seconds (%s MB/s)" %
      (ntables, tsizeMB, ttime, speed))
filehsrc.close()
filehdest.close()

########NEW FILE########
__FILENAME__ = create-large-number-objects
"This creates an HDF5 file with a potentially large number of objects"

import sys
import numpy
import tables

filename = sys.argv[1]

# Open a new empty HDF5 file
fileh = tables.open_file(filename, mode="w")

# nlevels -- Number of levels in hierarchy
# ngroups -- Number of groups on each level
# ndatasets -- Number of arrays on each group
# LR: Low ratio groups/datasets
#nlevels, ngroups, ndatasets = (3, 1, 1000)
# MR: Medium ratio groups/datasets
nlevels, ngroups, ndatasets = (3, 10, 100)
#nlevels, ngroups, ndatasets = (3, 5, 10)
# HR: High ratio groups/datasets
#nlevels, ngroups, ndatasets = (30, 10, 10)

# Create an Array to save on disk
a = numpy.array([-1, 2, 4], numpy.int16)

group = fileh.root
group2 = fileh.root
for k in range(nlevels):
    for j in range(ngroups):
        for i in range(ndatasets):
            # Save the array on the HDF5 file
            fileh.create_array(group2, 'array' + str(i),
                               a, "Signed short array")
        # Create a new group
        group2 = fileh.create_group(group, 'group' + str(j))
    # Create a new group
    group3 = fileh.create_group(group, 'ngroup' + str(k))
    # Iterate over this new group (group3)
    group = group3
    group2 = group3

fileh.close()

########NEW FILE########
__FILENAME__ = deep-tree-h5py
from __future__ import print_function
import os
import subprocess
from time import time
import random
import numpy
import h5py

random.seed(2)


def show_stats(explain, tref):
    "Show the used memory (only works for Linux 2.6.x)."
    # Build the command to obtain memory info
    cmd = "cat /proc/%s/status" % os.getpid()
    sout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout
    for line in sout:
        if line.startswith("VmSize:"):
            vmsize = int(line.split()[1])
        elif line.startswith("VmRSS:"):
            vmrss = int(line.split()[1])
        elif line.startswith("VmData:"):
            vmdata = int(line.split()[1])
        elif line.startswith("VmStk:"):
            vmstk = int(line.split()[1])
        elif line.startswith("VmExe:"):
            vmexe = int(line.split()[1])
        elif line.startswith("VmLib:"):
            vmlib = int(line.split()[1])
    sout.close()
    print("Memory usage: ******* %s *******" % explain)
    print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
    print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
    print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))
    tnow = time()
    print("WallClock time:", round(tnow - tref, 3))
    return tnow


def populate(f, nlevels):
    g = f
    arr = numpy.zeros((10,), "f4")
    for i in range(nlevels):
        g["DS1"] = arr
        g["DS2"] = arr
        g.create_group('group2_')
        g = g.create_group('group')


def getnode(f, nlevels, niter, range_):
    for i in range(niter):
        nlevel = random.randrange(
            (nlevels - range_) / 2, (nlevels + range_) / 2)
        groupname = ""
        for i in range(nlevel):
            groupname += "/group"
        groupname += "/DS1"
        f[groupname]


if __name__ == '__main__':
    nlevels = 1024
    niter = 1000
    range_ = 256
    profile = True
    doprofile = True
    verbose = False

    if doprofile:
        import pstats
        import cProfile as prof

    if profile:
        tref = time()
    if profile:
        show_stats("Abans de crear...", tref)
    f = h5py.File("/tmp/deep-tree.h5", 'w')
    if doprofile:
        prof.run('populate(f, nlevels)', 'populate.prof')
        stats = pstats.Stats('populate.prof')
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        if verbose:
            stats.print_stats()
        else:
            stats.print_stats(20)
    else:
        populate(f, nlevels)
    f.close()
    if profile:
        show_stats("Despres de crear", tref)

#     if profile: tref = time()
#     if profile: show_stats("Abans d'obrir...", tref)
#     f = h5py.File("/tmp/deep-tree.h5", 'r')
#     if profile: show_stats("Abans d'accedir...", tref)
#     if doprofile:
#         prof.run('getnode(f, nlevels, niter, range_)', 'deep-tree.prof')
#         stats = pstats.Stats('deep-tree.prof')
#         stats.strip_dirs()
#         stats.sort_stats('time', 'calls')
#         if verbose:
#             stats.print_stats()
#         else:
#             stats.print_stats(20)
#     else:
#         getnode(f, nlevels, niter, range_)
#     if profile: show_stats("Despres d'accedir", tref)
#     f.close()
#     if profile: show_stats("Despres de tancar", tref)

#     f = h5py.File("/tmp/deep-tree.h5", 'r')
#     g = f
#     for i in range(nlevels):
#         dset = g["DS1"]
#         dset = g["DS2"]
#         group2 = g['group2_']
#         g = g['group']
#     f.close()

########NEW FILE########
__FILENAME__ = deep-tree
# Small benchmark for compare creation times with parameter
# PYTABLES_SYS_ATTRS active or not.

from __future__ import print_function
import os
import subprocess
from time import time
import random
#import numpy
import tables

random.seed(2)


def show_stats(explain, tref):
    "Show the used memory (only works for Linux 2.6.x)."
    # Build the command to obtain memory info
    cmd = "cat /proc/%s/status" % os.getpid()
    sout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout
    for line in sout:
        if line.startswith("VmSize:"):
            vmsize = int(line.split()[1])
        elif line.startswith("VmRSS:"):
            vmrss = int(line.split()[1])
        elif line.startswith("VmData:"):
            vmdata = int(line.split()[1])
        elif line.startswith("VmStk:"):
            vmstk = int(line.split()[1])
        elif line.startswith("VmExe:"):
            vmexe = int(line.split()[1])
        elif line.startswith("VmLib:"):
            vmlib = int(line.split()[1])
    sout.close()
    print("Memory usage: ******* %s *******" % explain)
    print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
    print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
    print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))
    tnow = time()
    print("WallClock time:", round(tnow - tref, 3))
    return tnow


def populate(f, nlevels):
    g = f.root
    #arr = numpy.zeros((10,), "f4")
    #descr = {'f0': tables.Int32Col(), 'f1': tables.Float32Col()}
    for i in range(nlevels):
        #dset = f.create_array(g, "DS1", arr)
        #dset = f.create_array(g, "DS2", arr)
        f.create_carray(g, "DS1", tables.IntAtom(), (10,))
        f.create_carray(g, "DS2", tables.IntAtom(), (10,))
        #dset = f.create_table(g, "DS1", descr)
        #dset = f.create_table(g, "DS2", descr)
        f.create_group(g, 'group2_')
        g = f.create_group(g, 'group')


def getnode(f, nlevels, niter, range_):
    for i in range(niter):
        nlevel = random.randrange(
            (nlevels - range_) / 2, (nlevels + range_) / 2)
        groupname = ""
        for i in range(nlevel):
            groupname += "/group"
        groupname += "/DS1"
        f.get_node(groupname)


if __name__ == '__main__':
    nlevels = 1024
    niter = 256
    range_ = 128
    nodeCacheSlots = 64
    pytables_sys_attrs = True
    profile = True
    doprofile = True
    verbose = False

    if doprofile:
        import pstats
        import cProfile as prof

    if profile:
        tref = time()
    if profile:
        show_stats("Abans de crear...", tref)
    f = tables.open_file("/tmp/PTdeep-tree.h5", 'w',
                         node_cache_slots=nodeCacheSlots,
                         pytables_sys_attrs=pytables_sys_attrs)
    if doprofile:
        prof.run('populate(f, nlevels)', 'populate.prof')
        stats = pstats.Stats('populate.prof')
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        if verbose:
            stats.print_stats()
        else:
            stats.print_stats(20)
    else:
        populate(f, nlevels)
    f.close()
    if profile:
        show_stats("Despres de crear", tref)

    if profile:
        tref = time()
    if profile:
        show_stats("Abans d'obrir...", tref)
    f = tables.open_file("/tmp/PTdeep-tree.h5", 'r',
                         node_cache_slots=nodeCacheSlots,
                         pytables_sys_attrs=pytables_sys_attrs)
    if profile:
        show_stats("Abans d'accedir...", tref)
    if doprofile:
        prof.run('getnode(f, nlevels, niter, range_)', 'getnode.prof')
        stats = pstats.Stats('getnode.prof')
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        if verbose:
            stats.print_stats()
        else:
            stats.print_stats(20)
    else:
        getnode(f, nlevels, niter, range_)
    if profile:
        show_stats("Despres d'accedir", tref)
    f.close()
    if profile:
        show_stats("Despres de tancar", tref)

########NEW FILE########
__FILENAME__ = evaluate
from __future__ import print_function
import sys
from time import time

import numpy as np
import tables as tb
from numexpr.necompiler import (
    getContext, getExprNames, getType, NumExpr)


shape = (1000, 160000)
#shape = (10,1600)
filters = tb.Filters(complevel=1, complib="blosc", shuffle=0)
ofilters = tb.Filters(complevel=1, complib="blosc", shuffle=0)
#filters = tb.Filters(complevel=1, complib="lzo", shuffle=0)
#ofilters = tb.Filters(complevel=1, complib="lzo", shuffle=0)

# TODO: Makes it sense to add a 's'tring typecode here?
typecode_to_dtype = {'b': 'bool', 'i': 'int32', 'l': 'int64', 'f': 'float32',
                     'd': 'float64', 'c': 'complex128'}


def _compute(result, function, arguments,
             start=None, stop=None, step=None):
    """Compute the `function` over the `arguments` and put the outcome in
    `result`"""
    arg0 = arguments[0]
    if hasattr(arg0, 'maindim'):
        maindim = arg0.maindim
        (start, stop, step) = arg0._process_range_read(start, stop, step)
        nrowsinbuf = arg0.nrowsinbuf
        print("nrowsinbuf-->", nrowsinbuf)
    else:
        maindim = 0
        (start, stop, step) = (0, len(arg0), 1)
        nrowsinbuf = len(arg0)
    shape = list(arg0.shape)
    shape[maindim] = len(range(start, stop, step))

    # The slices parameter for arg0.__getitem__
    slices = [slice(0, dim, 1) for dim in arg0.shape]

    # This is a hack to prevent doing unnecessary conversions
    # when copying buffers
    if hasattr(arg0, 'maindim'):
        for arg in arguments:
            arg._v_convert = False

    # Start the computation itself
    for start2 in range(start, stop, step * nrowsinbuf):
        # Save the records on disk
        stop2 = start2 + step * nrowsinbuf
        if stop2 > stop:
            stop2 = stop
        # Set the proper slice in the main dimension
        slices[maindim] = slice(start2, stop2, step)
        start3 = (start2 - start) / step
        stop3 = start3 + nrowsinbuf
        if stop3 > shape[maindim]:
            stop3 = shape[maindim]
        # Compute the slice to be filled in destination
        sl = []
        for i in range(maindim):
            sl.append(slice(None, None, None))
        sl.append(slice(start3, stop3, None))
        # Get the values for computing the buffer
        values = [arg.__getitem__(tuple(slices)) for arg in arguments]
        result[tuple(sl)] = function(*values)

    # Activate the conversion again (default)
    if hasattr(arg0, 'maindim'):
        for arg in arguments:
            arg._v_convert = True

    return result


def evaluate(ex, out=None, local_dict=None, global_dict=None, **kwargs):
    """Evaluate expression and return an array."""

    # First, get the signature for the arrays in expression
    context = getContext(kwargs)
    names, _ = getExprNames(ex, context)

    # Get the arguments based on the names.
    call_frame = sys._getframe(1)
    if local_dict is None:
        local_dict = call_frame.f_locals
    if global_dict is None:
        global_dict = call_frame.f_globals
    arguments = []
    types = []
    for name in names:
        try:
            a = local_dict[name]
        except KeyError:
            a = global_dict[name]
        arguments.append(a)
        if hasattr(a, 'atom'):
            types.append(a.atom)
        else:
            types.append(a)

    # Create a signature
    signature = [(name, getType(type_)) for (name, type_) in zip(names, types)]
    print("signature-->", signature)

    # Compile the expression
    compiled_ex = NumExpr(ex, signature, [], **kwargs)
    print("fullsig-->", compiled_ex.fullsig)

    _compute(out, compiled_ex, arguments)

    return


if __name__ == "__main__":
    iarrays = 0
    oarrays = 0
    doprofile = 1
    dokprofile = 0

    f = tb.open_file("/scratch2/faltet/evaluate.h5", "w")

    # Create some arrays
    if iarrays:
        a = np.ones(shape, dtype='float32')
        b = np.ones(shape, dtype='float32') * 2
        c = np.ones(shape, dtype='float32') * 3
    else:
        a = f.create_carray(f.root, 'a', tb.Float32Atom(dflt=1.),
                            shape=shape, filters=filters)
        a[:] = 1.
        b = f.create_carray(f.root, 'b', tb.Float32Atom(dflt=2.),
                            shape=shape, filters=filters)
        b[:] = 2.
        c = f.create_carray(f.root, 'c', tb.Float32Atom(dflt=3.),
                            shape=shape, filters=filters)
        c[:] = 3.
    if oarrays:
        out = np.empty(shape, dtype='float32')
    else:
        out = f.create_carray(f.root, 'out', tb.Float32Atom(),
                              shape=shape, filters=ofilters)

    t0 = time()
    if iarrays and oarrays:
        #out = ne.evaluate("a*b+c")
        out = a * b + c
    elif doprofile:
        import cProfile as prof
        import pstats
        prof.run('evaluate("a*b+c", out)', 'evaluate.prof')
        stats = pstats.Stats('evaluate.prof')
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        stats.print_stats(20)
    elif dokprofile:
        from cProfile import Profile
        import lsprofcalltree
        prof = Profile()
        prof.run('evaluate("a*b+c", out)')
        kcg = lsprofcalltree.KCacheGrind(prof)
        ofile = open('evaluate.kcg', 'w')
        kcg.output(ofile)
        ofile.close()
    else:
        evaluate("a*b+c", out)
    print("Time for evaluate-->", round(time() - t0, 3))

    # print "out-->", `out`
    # print `out[:]`

    f.close()

########NEW FILE########
__FILENAME__ = expression
from __future__ import print_function
from time import time
import os.path

import numpy as np
import tables as tb

OUT_DIR = "/scratch2/faltet/"   # the directory for data output

shape = (1000, 1000 * 1000)   # shape for input arrays
expr = "a*b+1"   # Expression to be computed

nrows, ncols = shape


def tables(docompute, dowrite, complib, verbose):

    # Filenames
    ifilename = os.path.join(OUT_DIR, "expression-inputs.h5")
    ofilename = os.path.join(OUT_DIR, "expression-outputs.h5")

    # Filters
    shuffle = True
    if complib == 'blosc':
        filters = tb.Filters(complevel=1, complib='blosc', shuffle=shuffle)
    elif complib == 'lzo':
        filters = tb.Filters(complevel=1, complib='lzo', shuffle=shuffle)
    elif complib == 'zlib':
        filters = tb.Filters(complevel=1, complib='zlib', shuffle=shuffle)
    else:
        filters = tb.Filters(complevel=0, shuffle=False)
    if verbose:
        print("Will use filters:", filters)

    if dowrite:
        f = tb.open_file(ifilename, 'w')

        # Build input arrays
        t0 = time()
        root = f.root
        a = f.create_carray(root, 'a', tb.Float32Atom(),
                            shape, filters=filters)
        b = f.create_carray(root, 'b', tb.Float32Atom(),
                            shape, filters=filters)
        if verbose:
            print("chunkshape:", a.chunkshape)
            print("chunksize:", np.prod(a.chunkshape) * a.dtype.itemsize)
        #row = np.linspace(0, 1, ncols)
        row = np.arange(0, ncols, dtype='float32')
        for i in range(nrows):
            a[i] = row * (i + 1)
            b[i] = row * (i + 1) * 2
        f.close()
        print("[tables.Expr] Time for creating inputs:", round(time() - t0, 3))

    if docompute:
        f = tb.open_file(ifilename, 'r')
        fr = tb.open_file(ofilename, 'w')
        a = f.root.a
        b = f.root.b
        r1 = f.create_carray(fr.root, 'r1', tb.Float32Atom(), shape,
                             filters=filters)
        # The expression
        e = tb.Expr(expr)
        e.set_output(r1)
        t0 = time()
        e.eval()
        if verbose:
            print("First ten values:", r1[0, :10])
        f.close()
        fr.close()
        print("[tables.Expr] Time for computing & save:",
              round(time() - t0, 3))


def memmap(docompute, dowrite, verbose):

    afilename = os.path.join(OUT_DIR, "memmap-a.bin")
    bfilename = os.path.join(OUT_DIR, "memmap-b.bin")
    rfilename = os.path.join(OUT_DIR, "memmap-output.bin")
    if dowrite:
        t0 = time()
        a = np.memmap(afilename, dtype='float32', mode='w+', shape=shape)
        b = np.memmap(bfilename, dtype='float32', mode='w+', shape=shape)

        # Fill arrays a and b
        #row = np.linspace(0, 1, ncols)
        row = np.arange(0, ncols, dtype='float32')
        for i in range(nrows):
            a[i] = row * (i + 1)
            b[i] = row * (i + 1) * 2
        del a, b  # flush data
        print("[numpy.memmap] Time for creating inputs:",
              round(time() - t0, 3))

    if docompute:
        t0 = time()
        # Reopen inputs in read-only mode
        a = np.memmap(afilename, dtype='float32', mode='r', shape=shape)
        b = np.memmap(bfilename, dtype='float32', mode='r', shape=shape)
        # Create the array output
        r = np.memmap(rfilename, dtype='float32', mode='w+', shape=shape)
        # Do the computation row by row
        for i in range(nrows):
            r[i] = eval(expr, {'a': a[i], 'b': b[i]})
        if verbose:
            print("First ten values:", r[0, :10])
        del a, b
        del r  # flush output data
        print("[numpy.memmap] Time for compute & save:", round(time() - t0, 3))


def do_bench(what, documpute, dowrite, complib, verbose):
    if what == "tables":
        tables(docompute, dowrite, complib, verbose)
    if what == "memmap":
        memmap(docompute, dowrite, verbose)


if __name__ == "__main__":
    import sys
    import os
    import getopt

    usage = """usage: %s [-T] [-M] [-c] [-w] [-v] [-z complib]
           -T use tables.Expr
           -M use numpy.memmap
           -c do the computation only
           -w write inputs only
           -v verbose mode
           -z select compression library ('zlib' or 'lzo').  Default is None.
""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'TMcwvz:')
    except:
        sys.stderr.write(usage)
        sys.exit(1)

    # default options
    usepytables = False
    usememmap = False
    docompute = False
    dowrite = False
    verbose = False
    complib = None

    # Get the options
    for option in opts:
        if option[0] == '-T':
            usepytables = True
        elif option[0] == '-M':
            usememmap = True
        elif option[0] == '-c':
            docompute = True
        elif option[0] == '-w':
            dowrite = True
        elif option[0] == '-v':
            verbose = True
        elif option[0] == '-z':
            complib = option[1]
            if complib not in ('blosc', 'lzo', 'zlib'):
                print(("complib must be 'lzo' or 'zlib' "
                       "and you passed: '%s'" % complib))
                sys.exit(1)

    # If not a backend selected, abort
    if not usepytables and not usememmap:
        print("Please select a backend:")
        print("PyTables.Expr: -T")
        print("NumPy.memmap: -M")
        sys.exit(1)

    # Select backend and do the benchmark
    if usepytables:
        what = "tables"
    if usememmap:
        what = "memmap"
    do_bench(what, docompute, dowrite, complib, verbose)

########NEW FILE########
__FILENAME__ = get-figures-ranges
from __future__ import print_function
from pylab import *

linewidth = 2
#markers= ['+', ',', 'o', '.', 's', 'v', 'x', '>', '<', '^']
#markers= [ 'x', '+', 'o', 's', 'v', '^', '>', '<', ]
markers = ['s', 'o', 'v', '^', '+', 'x', '>', '<', ]
markersize = 8


def get_values(filename):
    f = open(filename)
    sizes = []
    values = []
    isize = None
    for line in f:
        if line.startswith('range'):
            tmp = line.split(':')[1]
            tmp = tmp.strip()
            tmp = tmp[1:-1]
            lower, upper = int(tmp.split(',')[0]), int(tmp.split(',')[1])
            isize = upper - lower
            # print "isize-->", isize
        if isize is None or isize == 0:
            continue
        if insert and line.startswith('Insert time'):
            tmp = line.split(':')[1]
            #itime = float(tmp[:tmp.index(',')])
            itime = float(tmp)
            sizes.append(isize)
            values.append(itime)
        elif line.startswith('Index time'):
            tmp = line.split(':')[1]
            #xtime = float(tmp[:tmp.index(',')])
            xtime = float(tmp)
            txtime += xtime
            if create_index and create_index in line:
                sizes.append(isize)
                values.append(xtime)
            elif create_total and txtime > xtime:
                sizes.append(isize)
                values.append(txtime)
        elif table_size and line.startswith('Table size'):
            tsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(tsize)
        elif indexes_size and line.startswith('Indexes size'):
            xsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(xsize)
        elif total_size and line.startswith('Full size'):
            fsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(fsize)
        elif ((query or query_cold or query_warm) and
              line.startswith('[NOREP]')):
            tmp = line.split(':')[1]
            try:
                qtime = float(tmp[:tmp.index('+-')])
            except ValueError:
                qtime = float(tmp)
            if colname in line:
                if query and '1st' in line:
                    sizes.append(isize)
                    values.append(qtime)
                elif query_cold and 'cold' in line:
                    sizes.append(isize)
                    values.append(qtime)
                elif query_warm and 'warm' in line:
                    sizes.append(isize)
                    values.append(qtime)

    f.close()
    return sizes, values


def show_plot(plots, yaxis, legends, gtitle):
    xlabel('Number of hits')
    ylabel(yaxis)
    title(gtitle)
    #ylim(0, 100)
    grid(True)

#     legends = [f[f.find('-'):f.index('.out')] for f in filenames]
#     legends = [l.replace('-', ' ') for l in legends]
    #legend([p[0] for p in plots], legends, loc = "upper left")
    legend([p[0] for p in plots], legends, loc="best")

    #subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2)
    if outfile:
        savefig(outfile)
    else:
        show()

if __name__ == '__main__':

    import sys
    import getopt

    usage = """usage: %s [-o file] [-t title] [--insert] [--create-index] [--create-total] [--table-size] [--indexes-size] [--total-size] [--query=colname]  [--query-cold=colname] [--query-warm=colname] files
 -o filename for output (only .png and .jpg extensions supported)
 -t title of the plot
 --insert -- Insert time for table
 --create-index=colname -- Index time for column
 --create-total -- Total time for creation of table + indexes
 --table-size -- Size of table
 --indexes-size -- Size of all indexes
 --total-size -- Total size of table + indexes
 --query=colname -- Time for querying the specified column
 --query-cold=colname -- Time for querying the specified column (cold cache)
 --query-warm=colname -- Time for querying the specified column (warm cache)
 \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'o:t:',
                                    ['insert',
                                     'create-index=',
                                     'create-total',
                                     'table-size',
                                     'indexes-size',
                                     'total-size',
                                     'query=',
                                     'query-cold=',
                                     'query-warm=',
                                     ])
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    progname = sys.argv[0]
    args = sys.argv[1:]

    # if we pass too few parameters, abort
    if len(pargs) < 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    outfile = None
    insert = 0
    create_index = None
    create_total = 0
    table_size = 0
    indexes_size = 0
    total_size = 0
    query = 0
    query_cold = 0
    query_warm = 0
    colname = None
    yaxis = "No axis name"
    tit = None
    gtitle = "Please set a title!"

    # Get the options
    for option in opts:
        if option[0] == '-o':
            outfile = option[1]
        elif option[0] == '-t':
            tit = option[1]
        elif option[0] == '--insert':
            insert = 1
            yaxis = "Time (s)"
            gtitle = "Insert time for table"
        elif option[0] == '--create-index':
            create_index = option[1]
            yaxis = "Time (s)"
            gtitle = "Create index time for column " + create_index
        elif option[0] == '--create-total':
            create_total = 1
            yaxis = "Time (s)"
            gtitle = "Create time for table + indexes"
        elif option[0] == '--table-size':
            table_size = 1
            yaxis = "Size (MB)"
            gtitle = "Table size"
        elif option[0] == '--indexes-size':
            indexes_size = 1
            yaxis = "Size (MB)"
            gtitle = "Indexes size"
        elif option[0] == '--total-size':
            total_size = 1
            yaxis = "Size (MB)"
            gtitle = "Total size (table + indexes)"
        elif option[0] == '--query':
            query = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (first query)"
        elif option[0] == '--query-cold':
            query_cold = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (cold cache)"
        elif option[0] == '--query-warm':
            query_warm = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (warm cache)"

    filenames = pargs

    if tit:
        gtitle = tit

    plots = []
    legends = []
    for filename in filenames:
        plegend = filename[filename.find('-'):filename.index('.out')]
        plegend = plegend.replace('-', ' ')
        xval, yval = get_values(filename)
        print("Values for %s --> %s, %s" % (filename, xval, yval))
        if "PyTables" in filename or "pytables" in filename:
            plot = loglog(xval, yval, linewidth=2)
            #plot = semilogx(xval, yval, linewidth=2)
            plots.append(plot)
            setp(plot, marker=markers[0], markersize=markersize,
                 linewidth=linewidth)
        else:
            plots.append(loglog(xval, yval, linewidth=3, color='m'))
            #plots.append(semilogx(xval, yval, linewidth=3, color='m'))
        #plots.append(semilogx(xval, yval, linewidth=5))
        legends.append(plegend)
    if 0:  # Per a introduir dades simulades si es vol...
        xval = [1000, 10000, 100000, 1000000, 10000000,
                100000000, 1000000000]
#         yval = [0.003, 0.005, 0.02, 0.06, 1.2,
#                 40, 210]
        yval = [0.0009, 0.0011, 0.0022, 0.005, 0.02,
                0.2, 5.6]
        plots.append(loglog(xval, yval, linewidth=5))
        legends.append("PyTables Std")
    show_plot(plots, yaxis, legends, gtitle)

########NEW FILE########
__FILENAME__ = get-figures
from __future__ import print_function
from pylab import *

linewidth = 2
#markers= ['+', ',', 'o', '.', 's', 'v', 'x', '>', '<', '^']
#markers= [ 'x', '+', 'o', 's', 'v', '^', '>', '<', ]
markers = ['s', 'o', 'v', '^', '+', 'x', '>', '<', ]
markersize = 8


def get_values(filename):
    f = open(filename)
    sizes = []
    values = []
    for line in f:
        if line.startswith('Processing database:'):
            txtime = 0
            line = line.split(':')[1]
            # Check if entry is compressed and if has to be processed
            line = line[:line.rfind('.')]
            params = line.split('-')
            for param in params:
                if param[-1] in ('k', 'm', 'g'):
                    size = param
                    isize = int(size[:-1]) * 1000
                    if size[-1] == "m":
                        isize *= 1000
                    elif size[-1] == "g":
                        isize *= 1000 * 1000
        elif insert and line.startswith('Insert time'):
            tmp = line.split(':')[1]
            itime = float(tmp)
            sizes.append(isize)
            values.append(itime)
        elif (overlaps or entropy) and line.startswith('overlaps'):
            tmp = line.split(':')[1]
            e1, e2 = tmp.split()
            if isize in sizes:
                sizes.pop()
                values.pop()
            sizes.append(isize)
            if overlaps:
                values.append(int(e1) + 1)
            else:
                values.append(float(e2) + 1)
        elif (create_total or create_index) and line.startswith('Index time'):
            tmp = line.split(':')[1]
            xtime = float(tmp)
            txtime += xtime
            if create_index and create_index in line:
                sizes.append(isize)
                values.append(xtime)
            elif create_total and txtime > xtime:
                sizes.append(isize)
                values.append(txtime)
        elif table_size and line.startswith('Table size'):
            tsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(tsize)
        elif indexes_size and line.startswith('Indexes size'):
            xsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(xsize)
        elif total_size and line.startswith('Full size'):
            fsize = float(line.split(':')[1])
            sizes.append(isize)
            values.append(fsize)
        elif query and line.startswith('Query time'):
            tmp = line.split(':')[1]
            qtime = float(tmp)
            if colname in line:
                sizes.append(isize)
                values.append(qtime)
        elif ((query or query_cold or query_warm) and
              line.startswith('[NOREP]')):
            tmp = line.split(':')[1]
            try:
                qtime = float(tmp[:tmp.index('+-')])
            except ValueError:
                qtime = float(tmp)
            if colname in line:
                if query and '1st' in line:
                    sizes.append(isize)
                    values.append(qtime)
                elif query_cold and 'cold' in line:
                    sizes.append(isize)
                    values.append(qtime)
                elif query_warm and 'warm' in line:
                    sizes.append(isize)
                    values.append(qtime)
        elif query_repeated and line.startswith('[REP]'):
            if colname in line and 'warm' in line:
                tmp = line.split(':')[1]
                qtime = float(tmp[:tmp.index('+-')])
                sizes.append(isize)
                values.append(qtime)

    f.close()
    return sizes, values


def show_plot(plots, yaxis, legends, gtitle):
    xlabel('Number of rows')
    ylabel(yaxis)
    title(gtitle)
    #xlim(10**3, 10**9)
    xlim(10 ** 3, 10 ** 10)
    # ylim(1.0e-5)
    #ylim(-1e4, 1e5)
    #ylim(-1e3, 1e4)
    #ylim(-1e2, 1e3)
    grid(True)

#     legends = [f[f.find('-'):f.index('.out')] for f in filenames]
#     legends = [l.replace('-', ' ') for l in legends]
    legend([p[0] for p in plots], legends, loc="upper left")
    #legend([p[0] for p in plots], legends, loc = "center left")

    #subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2)
    if outfile:
        savefig(outfile)
    else:
        show()

if __name__ == '__main__':

    import sys
    import getopt

    usage = """usage: %s [-o file] [-t title] [--insert] [--create-index] [--create-total] [--overlaps] [--entropy] [--table-size] [--indexes-size] [--total-size] [--query=colname] [--query-cold=colname] [--query-warm=colname] [--query-repeated=colname] files
 -o filename for output (only .png and .jpg extensions supported)
 -t title of the plot
 --insert -- Insert time for table
 --create-index=colname -- Index time for column
 --create-total -- Total time for creation of table + indexes
 --overlaps -- The overlapping for the created index
 --entropy -- The entropy for the created index
 --table-size -- Size of table
 --indexes-size -- Size of all indexes
 --total-size -- Total size of table + indexes
 --query=colname -- Time for querying the specified column
 --query-cold=colname -- Time for querying the specified column (cold cache)
 --query-warm=colname -- Time for querying the specified column (warm cache)
 --query-repeated=colname -- Time for querying the specified column (rep query)
 \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'o:t:',
                                    ['insert',
                                     'create-index=',
                                     'create-total',
                                     'overlaps',
                                     'entropy',
                                     'table-size',
                                     'indexes-size',
                                     'total-size',
                                     'query=',
                                     'query-cold=',
                                     'query-warm=',
                                     'query-repeated=',
                                     ])
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    progname = sys.argv[0]
    args = sys.argv[1:]

    # if we pass too few parameters, abort
    if len(pargs) < 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    outfile = None
    insert = 0
    create_index = None
    create_total = 0
    overlaps = 0
    entropy = 0
    table_size = 0
    indexes_size = 0
    total_size = 0
    query = 0
    query_cold = 0
    query_warm = 0
    query_repeated = 0
    colname = None
    yaxis = "No axis name"
    tit = None
    gtitle = "Please set a title!"

    # Get the options
    for option in opts:
        if option[0] == '-o':
            outfile = option[1]
        elif option[0] == '-t':
            tit = option[1]
        elif option[0] == '--insert':
            insert = 1
            yaxis = "Time (s)"
            gtitle = "Insert time for table"
        elif option[0] == '--create-index':
            create_index = option[1]
            yaxis = "Time (s)"
            gtitle = "Create index time for " + create_index + " column"
        elif option[0] == '--create-total':
            create_total = 1
            yaxis = "Time (s)"
            gtitle = "Create time for table + indexes"
        elif option[0] == '--overlaps':
            overlaps = 1
            yaxis = "Overlapping index + 1"
            gtitle = "Overlapping for col4 column"
        elif option[0] == '--entropy':
            entropy = 1
            yaxis = "Entropy + 1"
            gtitle = "Entropy for col4 column"
        elif option[0] == '--table-size':
            table_size = 1
            yaxis = "Size (MB)"
            gtitle = "Table size"
        elif option[0] == '--indexes-size':
            indexes_size = 1
            yaxis = "Size (MB)"
            #gtitle = "Indexes size"
            gtitle = "Index size for col4 column"
        elif option[0] == '--total-size':
            total_size = 1
            yaxis = "Size (MB)"
            gtitle = "Total size (table + indexes)"
        elif option[0] == '--query':
            query = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (first query)"
        elif option[0] == '--query-cold':
            query_cold = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (cold cache)"
        elif option[0] == '--query-warm':
            query_warm = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (warm cache)"
        elif option[0] == '--query-repeated':
            query_repeated = 1
            colname = option[1]
            yaxis = "Time (s)"
            gtitle = "Query time for " + colname + " column (repeated query)"

    gtitle = gtitle.replace('col2', 'Int32')
    gtitle = gtitle.replace('col4', 'Float64')

    filenames = pargs

    if tit:
        gtitle = tit

    plots = []
    legends = []
    for i, filename in enumerate(filenames):
        plegend = filename[:filename.index('.out')]
        plegend = plegend.replace('-', ' ')
        #plegend = plegend.replace('zlib1', '')
        if filename.find('PyTables') != -1:
            xval, yval = get_values(filename)
            print("Values for %s --> %s, %s" % (filename, xval, yval))
            if xval != []:
                plot = loglog(xval, yval)
                #plot = semilogx(xval, yval)
                setp(plot, marker=markers[i], markersize=markersize,
                     linewidth=linewidth)
                plots.append(plot)
                legends.append(plegend)
        else:
            xval, yval = get_values(filename)
            print("Values for %s --> %s, %s" % (filename, xval, yval))
            plots.append(loglog(xval, yval, linewidth=3, color='m'))
            #plots.append(semilogx(xval, yval, linewidth=linewidth, color='m'))
            legends.append(plegend)
    if 0:  # Per a introduir dades simulades si es vol...
        xval = [1000, 10000, 100000, 1000000, 10000000,
                100000000, 1000000000]
#         yval = [0.003, 0.005, 0.02, 0.06, 1.2,
#                 40, 210]
        yval = [0.0009, 0.0011, 0.0022, 0.005, 0.02,
                0.2, 5.6]
        plots.append(loglog(xval, yval, linewidth=linewidth))
        legends.append("PyTables Std")
    show_plot(plots, yaxis, legends, gtitle)

########NEW FILE########
__FILENAME__ = indexed_search
from __future__ import print_function
from time import time
import subprocess
import random
import numpy

# Constants

STEP = 1000 * 100   # the size of the buffer to fill the table, in rows
SCALE = 0.1         # standard deviation of the noise compared with actual
                    # values
NI_NTIMES = 1       # The number of queries for doing a mean (non-idx cols)
# COLDCACHE = 10   # The number of reads where the cache is considered 'cold'
# WARMCACHE = 50   # The number of reads until the cache is considered 'warmed'
# READ_TIMES = WARMCACHE+50    # The number of complete calls to DB.query_db()
# COLDCACHE = 50   # The number of reads where the cache is considered 'cold'
# WARMCACHE = 50   # The number of reads until the cache is considered 'warmed'
# READ_TIMES = WARMCACHE+50    # The number of complete calls to DB.query_db()
MROW = 1000 * 1000.

# Test values
COLDCACHE = 5   # The number of reads where the cache is considered 'cold'
WARMCACHE = 5   # The number of reads until the cache is considered 'warmed'
READ_TIMES = 10    # The number of complete calls to DB.query_db()

# global variables
rdm_cod = ['lin', 'rnd']
prec = 6  # precision for printing floats purposes


def get_nrows(nrows_str):
    if nrows_str.endswith("k"):
        return int(float(nrows_str[:-1]) * 1000)
    elif nrows_str.endswith("m"):
        return int(float(nrows_str[:-1]) * 1000 * 1000)
    elif nrows_str.endswith("g"):
        return int(float(nrows_str[:-1]) * 1000 * 1000 * 1000)
    else:
        raise ValueError(
            "value of nrows must end with either 'k', 'm' or 'g' suffixes.")


class DB(object):

    def __init__(self, nrows, rng, userandom):
        global step, scale
        self.step = STEP
        self.scale = SCALE
        self.rng = rng
        self.userandom = userandom
        self.filename = '-'.join([rdm_cod[userandom], nrows])
        self.nrows = get_nrows(nrows)

    def get_db_size(self):
        sout = subprocess.Popen("sync;du -s %s" % self.filename, shell=True,
                                stdout=subprocess.PIPE).stdout
        line = [l for l in sout][0]
        return int(line.split()[0])

    def print_mtime(self, t1, explain):
        mtime = time() - t1
        print("%s:" % explain, round(mtime, 6))
        print("Krows/s:", round((self.nrows / 1000.) / mtime, 6))

    def print_qtime(self, colname, ltimes):
        qtime1 = ltimes[0]  # First measured time
        qtime2 = ltimes[-1]  # Last measured time
        print("Query time for %s:" % colname, round(qtime1, 6))
        print("Mrows/s:", round((self.nrows / (MROW)) / qtime1, 6))
        print("Query time for %s (cached):" % colname, round(qtime2, 6))
        print("Mrows/s (cached):", round((self.nrows / (MROW)) / qtime2, 6))

    def norm_times(self, ltimes):
        "Get the mean and stddev of ltimes, avoiding the extreme values."
        lmean = ltimes.mean()
        lstd = ltimes.std()
        ntimes = ltimes[ltimes < lmean + lstd]
        nmean = ntimes.mean()
        nstd = ntimes.std()
        return nmean, nstd

    def print_qtime_idx(self, colname, ltimes, repeated, verbose):
        if repeated:
            r = "[REP] "
        else:
            r = "[NOREP] "
        ltimes = numpy.array(ltimes)
        ntimes = len(ltimes)
        qtime1 = ltimes[0]  # First measured time
        ctimes = ltimes[1:COLDCACHE]
        cmean, cstd = self.norm_times(ctimes)
        wtimes = ltimes[WARMCACHE:]
        wmean, wstd = self.norm_times(wtimes)
        if verbose:
            print("Times for cold cache:\n", ctimes)
            # print "Times for warm cache:\n", wtimes
            print("Histogram for warm cache: %s\n%s" %
                  numpy.histogram(wtimes))
        print("%s1st query time for %s:" % (r, colname),
              round(qtime1, prec))
        print("%sQuery time for %s (cold cache):" % (r, colname),
              round(cmean, prec), "+-", round(cstd, prec))
        print("%sQuery time for %s (warm cache):" % (r, colname),
              round(wmean, prec), "+-", round(wstd, prec))

    def print_db_sizes(self, init, filled, indexed):
        table_size = (filled - init) / 1024.
        indexes_size = (indexed - filled) / 1024.
        print("Table size (MB):", round(table_size, 3))
        print("Indexes size (MB):", round(indexes_size, 3))
        print("Full size (MB):", round(table_size + indexes_size, 3))

    def fill_arrays(self, start, stop):
        arr_f8 = numpy.arange(start, stop, dtype='float64')
        arr_i4 = numpy.arange(start, stop, dtype='int32')
        if self.userandom:
            arr_f8 += numpy.random.normal(0, stop * self.scale,
                                          size=stop - start)
            arr_i4 = numpy.array(arr_f8, dtype='int32')
        return arr_i4, arr_f8

    def create_db(self, dtype, kind, optlevel, verbose):
        self.con = self.open_db(remove=1)
        self.create_table(self.con)
        init_size = self.get_db_size()
        t1 = time()
        self.fill_table(self.con)
        table_size = self.get_db_size()
        self.print_mtime(t1, 'Insert time')
        self.index_db(dtype, kind, optlevel, verbose)
        indexes_size = self.get_db_size()
        self.print_db_sizes(init_size, table_size, indexes_size)
        self.close_db(self.con)

    def index_db(self, dtype, kind, optlevel, verbose):
        if dtype == "int":
            idx_cols = ['col2']
        elif dtype == "float":
            idx_cols = ['col4']
        else:
            idx_cols = ['col2', 'col4']
        for colname in idx_cols:
            t1 = time()
            self.index_col(self.con, colname, kind, optlevel, verbose)
            self.print_mtime(t1, 'Index time (%s)' % colname)

    def query_db(self, niter, dtype, onlyidxquery, onlynonidxquery,
                 avoidfscache, verbose, inkernel):
        self.con = self.open_db()
        if dtype == "int":
            reg_cols = ['col1']
            idx_cols = ['col2']
        elif dtype == "float":
            reg_cols = ['col3']
            idx_cols = ['col4']
        else:
            reg_cols = ['col1', 'col3']
            idx_cols = ['col2', 'col4']
        if avoidfscache:
            rseed = int(numpy.random.randint(self.nrows))
        else:
            rseed = 19
        # Query for non-indexed columns
        numpy.random.seed(rseed)
        base = numpy.random.randint(self.nrows)
        if not onlyidxquery:
            for colname in reg_cols:
                ltimes = []
                random.seed(rseed)
                for i in range(NI_NTIMES):
                    t1 = time()
                    results = self.do_query(self.con, colname, base, inkernel)
                    ltimes.append(time() - t1)
                if verbose:
                    print("Results len:", results)
                self.print_qtime(colname, ltimes)
            # Always reopen the file after *every* query loop.
            # Necessary to make the benchmark to run correctly.
            self.close_db(self.con)
            self.con = self.open_db()
        # Query for indexed columns
        if not onlynonidxquery:
            for colname in idx_cols:
                ltimes = []
                numpy.random.seed(rseed)
                rndbase = numpy.random.randint(self.nrows, size=niter)
                # First, non-repeated queries
                for i in range(niter):
                    base = rndbase[i]
                    t1 = time()
                    results = self.do_query(self.con, colname, base, inkernel)
                    #results, tprof = self.do_query(
                    #    self.con, colname, base, inkernel)
                    ltimes.append(time() - t1)
                if verbose:
                    print("Results len:", results)
                self.print_qtime_idx(colname, ltimes, False, verbose)
                # Always reopen the file after *every* query loop.
                # Necessary to make the benchmark to run correctly.
                self.close_db(self.con)
                self.con = self.open_db()
                ltimes = []
# Second, repeated queries
#                 for i in range(niter):
#                     t1=time()
#                     results = self.do_query(
#                         self.con, colname, base, inkernel)
# results, tprof = self.do_query(self.con, colname, base, inkernel)
#                     ltimes.append(time()-t1)
#                 if verbose:
#                     print "Results len:", results
#                 self.print_qtime_idx(colname, ltimes, True, verbose)
                # Print internal PyTables index tprof statistics
                #tprof = numpy.array(tprof)
                #tmean, tstd = self.norm_times(tprof)
                # print "tprof-->", round(tmean, prec), "+-", round(tstd, prec)
                # print "tprof hist-->", \
                #    numpy.histogram(tprof)
                # print "tprof raw-->", tprof
                # Always reopen the file after *every* query loop.
                # Necessary to make the benchmark to run correctly.
                self.close_db(self.con)
                self.con = self.open_db()
        # Finally, close the file.
        self.close_db(self.con)

    def close_db(self, con):
        con.close()


if __name__ == "__main__":
    import sys
    import getopt

    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-T] [-P] [-v] [-f] [-k] [-p] [-m] [-c] [-q] [-i] [-I] [-S] [-x] [-z complevel] [-l complib] [-R range] [-N niter] [-n nrows] [-d datadir] [-O level] [-t kind] [-s] col -Q [suplim]
            -T use Pytables
            -P use Postgres
            -v verbose
            -f do a profile of the run (only query functionality & Python 2.5)
            -k do a profile for kcachegrind use (out file is 'indexed_search.kcg')
            -p use "psyco" if available
            -m use random values to fill the table
            -q do a query (both indexed and non-indexed versions)
            -i do a query (just indexed one)
            -I do a query (just in-kernel one)
            -S do a query (just standard one)
            -x choose a different seed for random numbers (i.e. avoid FS cache)
            -c create the database
            -z compress with zlib (no compression by default)
            -l use complib for compression (zlib used by default)
            -R select a range in a field in the form "start,stop" (def "0,10")
            -N number of iterations for reading
            -n sets the number of rows (in krows) in each table
            -d directory to save data (default: data.nobackup)
            -O set the optimization level for PyTables indexes
            -t select the index type: "medium" (default) or "full", "light", "ultralight"
            -s select a type column for operations ('int' or 'float'. def all)
            -Q do a repeteated query up to 10**value
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(
            sys.argv[1:], 'TPvfkpmcqiISxz:l:R:N:n:d:O:t:s:Q:')
    except:
        sys.stderr.write(usage)
        sys.exit(1)

    # default options
    usepytables = 0
    usepostgres = 0
    verbose = 0
    doprofile = 0
    dokprofile = 0
    usepsyco = 0
    userandom = 0
    docreate = 0
    optlevel = 0
    kind = "medium"
    docompress = 0
    complib = "zlib"
    doquery = False
    onlyidxquery = False
    onlynonidxquery = False
    inkernel = True
    avoidfscache = 0
    #rng = [-10, 10]
    rng = [-1000, -1000]
    repeatquery = 0
    repeatvalue = 0
    krows = '1k'
    niter = READ_TIMES
    dtype = "all"
    datadir = "data.nobackup"

    # Get the options
    for option in opts:
        if option[0] == '-T':
            usepytables = 1
        elif option[0] == '-P':
            usepostgres = 1
        elif option[0] == '-v':
            verbose = 1
        elif option[0] == '-f':
            doprofile = 1
        elif option[0] == '-k':
            dokprofile = 1
        elif option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-m':
            userandom = 1
        elif option[0] == '-c':
            docreate = 1
        elif option[0] == '-q':
            doquery = True
        elif option[0] == '-i':
            doquery = True
            onlyidxquery = True
        elif option[0] == '-I':
            doquery = True
            onlynonidxquery = True
        elif option[0] == '-S':
            doquery = True
            onlynonidxquery = True
            inkernel = False
        elif option[0] == '-x':
            avoidfscache = 1
        elif option[0] == '-z':
            docompress = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-N':
            niter = int(option[1])
        elif option[0] == '-n':
            krows = option[1]
        elif option[0] == '-d':
            datadir = option[1]
        elif option[0] == '-O':
            optlevel = int(option[1])
        elif option[0] == '-t':
            if option[1] in ('full', 'medium', 'light', 'ultralight'):
                kind = option[1]
            else:
                print("kind should be either 'full', 'medium', 'light' or "
                      "'ultralight'")
                sys.exit(1)
        elif option[0] == '-s':
            if option[1] in ('int', 'float'):
                dtype = option[1]
            else:
                print("column should be either 'int' or 'float'")
                sys.exit(1)
        elif option[0] == '-Q':
            repeatquery = 1
            repeatvalue = int(option[1])

    # If not database backend selected, abort
    if not usepytables and not usepostgres:
        print("Please select a backend:")
        print("PyTables: -T")
        print("Postgres: -P")
        sys.exit(1)

    # Create the class for the database
    if usepytables:
        from pytables_backend import PyTables_DB
        db = PyTables_DB(krows, rng, userandom, datadir,
                         docompress, complib, kind, optlevel)
    elif usepostgres:
        from postgres_backend import Postgres_DB
        db = Postgres_DB(krows, rng, userandom)

    if not avoidfscache:
        # in order to always generate the same random sequence
        numpy.random.seed(20)

    if verbose:
        if userandom:
            print("using random values")
        if onlyidxquery:
            print("doing indexed queries only")

    if psyco_imported and usepsyco:
        psyco.bind(db.create_db)
        psyco.bind(db.query_db)

    if docreate:
        if verbose:
            print("writing %s rows" % krows)
        db.create_db(dtype, kind, optlevel, verbose)

    if doquery:
        print("Calling query_db() %s times" % niter)
        if doprofile:
            import pstats
            import cProfile as prof
            prof.run(
                'db.query_db(niter, dtype, onlyidxquery, onlynonidxquery, '
                'avoidfscache, verbose, inkernel)',
                'indexed_search.prof')
            stats = pstats.Stats('indexed_search.prof')
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            if verbose:
                stats.print_stats()
            else:
                stats.print_stats(20)
        elif dokprofile:
            from cProfile import Profile
            import lsprofcalltree
            prof = Profile()
            prof.run(
                'db.query_db(niter, dtype, onlyidxquery, onlynonidxquery, '
                'avoidfscache, verbose, inkernel)')
            kcg = lsprofcalltree.KCacheGrind(prof)
            ofile = open('indexed_search.kcg', 'w')
            kcg.output(ofile)
            ofile.close()
        elif doprofile:
            import hotshot
            import hotshot.stats
            prof = hotshot.Profile("indexed_search.prof")
            benchtime, stones = prof.run(
                'db.query_db(niter, dtype, onlyidxquery, onlynonidxquery, '
                'avoidfscache, verbose, inkernel)')
            prof.close()
            stats = hotshot.stats.load("indexed_search.prof")
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            stats.print_stats(20)
        else:
            db.query_db(niter, dtype, onlyidxquery, onlynonidxquery,
                        avoidfscache, verbose, inkernel)

    if repeatquery:
        # Start by a range which is almost None
        db.rng = [1, 1]
        if verbose:
            print("range:", db.rng)
        db.query_db(niter, dtype, onlyidxquery, onlynonidxquery,
                    avoidfscache, verbose, inkernel)
        for i in range(repeatvalue):
            for j in (1, 2, 5):
                rng = j * 10 ** i
                db.rng = [-rng / 2, rng / 2]
                if verbose:
                    print("range:", db.rng)
#                 if usepostgres:
#                     os.system(
#                         "echo 1 > /proc/sys/vm/drop_caches;"
#                         " /etc/init.d/postgresql restart")
#                 else:
#                     os.system("echo 1 > /proc/sys/vm/drop_caches")
                db.query_db(niter, dtype, onlyidxquery, onlynonidxquery,
                            avoidfscache, verbose, inkernel)

########NEW FILE########
__FILENAME__ = keysort
from __future__ import print_function
from tables.indexesextension import keysort
import numpy
from time import time

N = 1000 * 1000
rnd = numpy.random.randint(N, size=N)

for dtype1 in ('S6', 'b1',
               'i1', 'i2', 'i4', 'i8',
               'u1', 'u2', 'u4', 'u8', 'f4', 'f8'):
    for dtype2 in ('u4', 'i8'):
        print("dtype array1, array2-->", dtype1, dtype2)
        a = numpy.array(rnd, dtype1)
        b = numpy.arange(N, dtype=dtype2)
        c = a.copy()

        t1 = time()
        d = c.argsort()
        # c.sort()
        # e=c
        e = c[d]
        f = b[d]
        tref = time() - t1
        print("normal sort time-->", tref)

        t1 = time()
        keysort(a, b)
        tks = time() - t1
        print("keysort time-->", tks, "    %.2fx" % (tref / tks,))
        assert numpy.alltrue(a == e)
        #assert numpy.alltrue(b == d)
        assert numpy.alltrue(f == d)

########NEW FILE########
__FILENAME__ = lookup_bench
"""Benchmark to help choosing the best chunksize so as to optimize the access
time in random lookups."""

from __future__ import print_function
from time import time
import os
import subprocess
import numpy
import tables

# Constants
NOISE = 1e-15    # standard deviation of the noise compared with actual values

rdm_cod = ['lin', 'rnd']


def get_nrows(nrows_str):
    if nrows_str.endswith("k"):
        return int(float(nrows_str[:-1]) * 1000)
    elif nrows_str.endswith("m"):
        return int(float(nrows_str[:-1]) * 1000 * 1000)
    elif nrows_str.endswith("g"):
        return int(float(nrows_str[:-1]) * 1000 * 1000 * 1000)
    else:
        raise ValueError(
            "value of nrows must end with either 'k', 'm' or 'g' suffixes.")


class DB(object):

    def __init__(self, nrows, dtype, chunksize, userandom, datadir,
                 docompress=0, complib='zlib'):
        self.dtype = dtype
        self.docompress = docompress
        self.complib = complib
        self.filename = '-'.join([rdm_cod[userandom],
                                  "n" + nrows, "s" + chunksize, dtype])
        # Complete the filename
        self.filename = "lookup-" + self.filename
        if docompress:
            self.filename += '-' + complib + str(docompress)
        self.filename = datadir + '/' + self.filename + '.h5'
        print("Processing database:", self.filename)
        self.userandom = userandom
        self.nrows = get_nrows(nrows)
        self.chunksize = get_nrows(chunksize)
        self.step = self.chunksize
        self.scale = NOISE

    def get_db_size(self):
        sout = subprocess.Popen("sync;du -s %s" % self.filename, shell=True,
                                stdout=subprocess.PIPE).stdout
        line = [l for l in sout][0]
        return int(line.split()[0])

    def print_mtime(self, t1, explain):
        mtime = time() - t1
        print("%s:" % explain, round(mtime, 6))
        print("Krows/s:", round((self.nrows / 1000.) / mtime, 6))

    def print_db_sizes(self, init, filled):
        array_size = (filled - init) / 1024.
        print("Array size (MB):", round(array_size, 3))

    def open_db(self, remove=0):
        if remove and os.path.exists(self.filename):
            os.remove(self.filename)
        con = tables.open_file(self.filename, 'a')
        return con

    def create_db(self, verbose):
        self.con = self.open_db(remove=1)
        self.create_array()
        init_size = self.get_db_size()
        t1 = time()
        self.fill_array()
        array_size = self.get_db_size()
        self.print_mtime(t1, 'Insert time')
        self.print_db_sizes(init_size, array_size)
        self.close_db()

    def create_array(self):
        # The filters chosen
        filters = tables.Filters(complevel=self.docompress,
                                 complib=self.complib)
        atom = tables.Atom.from_kind(self.dtype)
        self.con.create_earray(self.con.root, 'earray', atom, (0,),
                               filters=filters,
                               expectedrows=self.nrows,
                               chunkshape=(self.chunksize,))

    def fill_array(self):
        "Fills the array"
        earray = self.con.root.earray
        j = 0
        arr = self.get_array(0, self.step)
        for i in range(0, self.nrows, self.step):
            stop = (j + 1) * self.step
            if stop > self.nrows:
                stop = self.nrows
            ###arr = self.get_array(i, stop, dtype)
            earray.append(arr)
            j += 1
        earray.flush()

    def get_array(self, start, stop):
        arr = numpy.arange(start, stop, dtype='float')
        if self.userandom:
            arr += numpy.random.normal(0, stop * self.scale, size=stop - start)
        arr = arr.astype(self.dtype)
        return arr

    def print_qtime(self, ltimes):
        ltimes = numpy.array(ltimes)
        print("Raw query times:\n", ltimes)
        print("Histogram times:\n", numpy.histogram(ltimes[1:]))
        ntimes = len(ltimes)
        qtime1 = ltimes[0]  # First measured time
        if ntimes > 5:
            # Wait until the 5th iteration (in order to
            # ensure that the index is effectively cached) to take times
            qtime2 = sum(ltimes[5:]) / (ntimes - 5)
        else:
            qtime2 = ltimes[-1]  # Last measured time
        print("1st query time:", round(qtime1, 3))
        print("Mean (skipping the first 5 meas.):", round(qtime2, 3))

    def query_db(self, niter, avoidfscache, verbose):
        self.con = self.open_db()
        earray = self.con.root.earray
        if avoidfscache:
            rseed = int(numpy.random.randint(self.nrows))
        else:
            rseed = 19
        numpy.random.seed(rseed)
        numpy.random.randint(self.nrows)
        ltimes = []
        for i in range(niter):
            t1 = time()
            self.do_query(earray, numpy.random.randint(self.nrows))
            ltimes.append(time() - t1)
        self.print_qtime(ltimes)
        self.close_db()

    def do_query(self, earray, idx):
        return earray[idx]

    def close_db(self):
        self.con.close()


if __name__ == "__main__":
    import sys
    import getopt

    usage = """usage: %s [-v] [-m] [-c] [-q] [-x] [-z complevel] [-l complib] [-N niter] [-n nrows] [-d datadir] [-t] type [-s] chunksize
            -v verbose
            -m use random values to fill the array
            -q do a (random) lookup
            -x choose a different seed for random numbers (i.e. avoid FS cache)
            -c create the file
            -z compress with zlib (no compression by default)
            -l use complib for compression (zlib used by default)
            -N number of iterations for reading
            -n sets the number of rows in the array
            -d directory to save data (default: data.nobackup)
            -t select the type for array ('int' or 'float'. def 'float')
            -s select the chunksize for array
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vmcqxz:l:N:n:d:t:s:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    userandom = 0
    docreate = 0
    optlevel = 0
    docompress = 0
    complib = "zlib"
    doquery = False
    avoidfscache = 0
    krows = '1k'
    chunksize = '32k'
    niter = 50
    datadir = "data.nobackup"
    dtype = "float"

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-m':
            userandom = 1
        elif option[0] == '-c':
            docreate = 1
            createindex = 1
        elif option[0] == '-q':
            doquery = True
        elif option[0] == '-x':
            avoidfscache = 1
        elif option[0] == '-z':
            docompress = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-N':
            niter = int(option[1])
        elif option[0] == '-n':
            krows = option[1]
        elif option[0] == '-d':
            datadir = option[1]
        elif option[0] == '-t':
            if option[1] in ('int', 'float'):
                dtype = option[1]
            else:
                print("type should be either 'int' or 'float'")
                sys.exit(0)
        elif option[0] == '-s':
            chunksize = option[1]

    if not avoidfscache:
        # in order to always generate the same random sequence
        numpy.random.seed(20)

    if verbose:
        if userandom:
            print("using random values")

    db = DB(krows, dtype, chunksize, userandom, datadir, docompress, complib)

    if docreate:
        if verbose:
            print("writing %s rows" % krows)
        db.create_db(verbose)

    if doquery:
        print("Calling query_db() %s times" % niter)
        db.query_db(niter, avoidfscache, verbose)

########NEW FILE########
__FILENAME__ = LRU-experiments
# Testbed to perform experiments in order to determine best values for
# the node numbers in LRU cache. Tables version.

from __future__ import print_function
from time import time
from tables import *
import tables

print("PyTables version-->", tables.__version__)

filename = "/tmp/junk-tables-100.h5"
NLEAVES = 2000
NROWS = 1000


class Particle(IsDescription):
    name = StringCol(16, pos=1)         # 16-character String
    lati = Int32Col(pos=2)              # integer
    longi = Int32Col(pos=3)             # integer
    pressure = Float32Col(pos=4)        # float  (single-precision)
    temperature = Float64Col(pos=5)     # double (double-precision)


def create_junk():
    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w")
    # Create a new group
    group = fileh.create_group(fileh.root, "newgroup")

    for i in range(NLEAVES):
        # Create a new table in newgroup group
        table = fileh.create_table(group, 'table' + str(i), Particle,
                                   "A table", Filters(1))
        particle = table.row
        print("Creating table-->", table._v_name)

        # Fill the table with particles
        for i in range(NROWS):
            # This injects the row values.
            particle.append()
        table.flush()

    # Finally, close the file
    fileh.close()


def modify_junk_LRU():
    fileh = open_file(filename, 'a')
    group = fileh.root.newgroup
    for j in range(5):
        print("iter -->", j)
        for tt in fileh.walk_nodes(group):
            if isinstance(tt, Table):
                pass
#                 for row in tt:
#                     pass
    fileh.close()


def modify_junk_LRU2():
    fileh = open_file(filename, 'a')
    group = fileh.root.newgroup
    for j in range(20):
        t1 = time()
        for i in range(100):
            #print("table-->", tt._v_name)
            tt = getattr(group, "table" + str(i))
            #for row in tt:
            #    pass
        print("iter and time -->", j + 1, round(time() - t1, 3))
    fileh.close()


def modify_junk_LRU3():
    fileh = open_file(filename, 'a')
    group = fileh.root.newgroup
    for j in range(3):
        t1 = time()
        for tt in fileh.walk_nodes(group, "Table"):
            tt.attrs.TITLE
            for row in tt:
                pass
        print("iter and time -->", j + 1, round(time() - t1, 3))
    fileh.close()

if 1:
    # create_junk()
    # modify_junk_LRU()    # uses the iterator version (walk_nodes)
    # modify_junk_LRU2()   # uses a regular loop (getattr)
    modify_junk_LRU3()   # uses a regular loop (getattr)
else:
    import profile
    import pstats
    profile.run('modify_junk_LRU2()', 'modify.prof')
    stats = pstats.Stats('modify.prof')
    stats.strip_dirs()
    stats.sort_stats('time', 'calls')
    stats.print_stats()

########NEW FILE########
__FILENAME__ = LRU-experiments2
# Testbed to perform experiments in order to determine best values for
# the node numbers in LRU cache. Arrays version.

from __future__ import print_function
from time import time
import tables

print("PyTables version-->", tables.__version__)

filename = "/tmp/junk-array.h5"
NOBJS = 1000


def create_junk():
    fileh = tables.open_file(filename, mode="w")
    for i in range(NOBJS):
        fileh.create_array(fileh.root, 'array' + str(i), [1])
    fileh.close()


def modify_junk_LRU():
    fileh = tables.open_file(filename, 'a')
    group = fileh.root
    for j in range(5):
        print("iter -->", j)
        for tt in fileh.walk_nodes(group):
            if isinstance(tt, tables.Array):
#                 d = tt.read()
                pass

    fileh.close()


def modify_junk_LRU2():
    fileh = tables.open_file(filename, 'a')
    group = fileh.root
    for j in range(5):
        t1 = time()
        for i in range(100):  # The number
            #print("table-->", tt._v_name)
            tt = getattr(group, "array" + str(i))
            #d = tt.read()
        print("iter and time -->", j + 1, round(time() - t1, 3))
    fileh.close()

if 1:
    # create_junk()
    # modify_junk_LRU()    # uses the iterador version (walk_nodes)
    modify_junk_LRU2()   # uses a regular loop (getattr)
else:
    import profile
    import pstats
    profile.run('modify_junk_LRU2()', 'modify.prof')
    stats = pstats.Stats('modify.prof')
    stats.strip_dirs()
    stats.sort_stats('time', 'calls')
    stats.print_stats()

########NEW FILE########
__FILENAME__ = LRUcache-node-bench
from __future__ import print_function

import sys
import numpy
import tables
from time import time
#import psyco

filename = "/tmp/LRU-bench.h5"
nodespergroup = 250
niter = 100

print('nodespergroup:', nodespergroup)
print('niter:', niter)

if len(sys.argv) > 1:
    NODE_CACHE_SLOTS = int(sys.argv[1])
    print('NODE_CACHE_SLOTS:', NODE_CACHE_SLOTS)
else:
    NODE_CACHE_SLOTS = tables.parameters.NODE_CACHE_SLOTS
f = tables.open_file(filename, "w", node_cache_slots=NODE_CACHE_SLOTS)
g = f.create_group("/", "NodeContainer")
print("Creating nodes")
for i in range(nodespergroup):
    f.create_array(g, "arr%d" % i, [i])
f.close()

f = tables.open_file(filename)


def iternodes():
#     for a in f.root.NodeContainer:
#         pass
    indices = numpy.random.randn(nodespergroup * niter) * \
        30 + nodespergroup / 2.
    indices = indices.astype('i4').clip(0, nodespergroup - 1)
    g = f.get_node("/", "NodeContainer")
    for i in indices:
        a = f.get_node(g, "arr%d" % i)
        # print("a-->", a)

print("reading nodes...")
# First iteration (put in LRU cache)
t1 = time()
for a in f.root.NodeContainer:
    pass
print("time (init cache)-->", round(time() - t1, 3))


def timeLRU():
    # Next iterations
    t1 = time()
#     for i in range(niter):
#         iternodes()
    iternodes()
    print("time (from cache)-->", round((time() - t1) / niter, 3))


def profile(verbose=False):
    import pstats
    import cProfile as prof
    prof.run('timeLRU()', 'out.prof')
    stats = pstats.Stats('out.prof')
    stats.strip_dirs()
    stats.sort_stats('time', 'calls')
    if verbose:
        stats.print_stats()
    else:
        stats.print_stats(20)

# profile()
# psyco.bind(timeLRU)
timeLRU()

f.close()

# for N in 0 4 8 16 32 64 128 256 512 1024 2048 4096; do
#     env PYTHONPATH=../build/lib.linux-x86_64-2.7 \
#     python LRUcache-node-bench.py $N;
# done

########NEW FILE########
__FILENAME__ = open_close-bench
"""Testbed for open/close PyTables files.

This uses the HotShot profiler.

"""

from __future__ import print_function
import os
import sys
import getopt
import pstats
import cProfile as prof
import time
import subprocess  # From Python 2.4 on
import tables

filename = None
niter = 1


def show_stats(explain, tref):
    "Show the used memory"
    # Build the command to obtain memory info (only for Linux 2.6.x)
    cmd = "cat /proc/%s/status" % os.getpid()
    sout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout
    for line in sout:
        if line.startswith("VmSize:"):
            vmsize = int(line.split()[1])
        elif line.startswith("VmRSS:"):
            vmrss = int(line.split()[1])
        elif line.startswith("VmData:"):
            vmdata = int(line.split()[1])
        elif line.startswith("VmStk:"):
            vmstk = int(line.split()[1])
        elif line.startswith("VmExe:"):
            vmexe = int(line.split()[1])
        elif line.startswith("VmLib:"):
            vmlib = int(line.split()[1])
    sout.close()
    print("WallClock time:", time.time() - tref)
    print("Memory usage: ******* %s *******" % explain)
    print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
    print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
    print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))


def check_open_close():
    for i in range(niter):
        print(
            "------------------ open_close #%s -------------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        fileh.close()
        show_stats("After closing file", tref)


def check_only_open():
    for i in range(niter):
        print("------------------ only_open #%s -------------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        show_stats("Before closing file", tref)
        fileh.close()


def check_full_browse():
    for i in range(niter):
        print("------------------ full_browse #%s -----------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        for node in fileh:
            pass
        fileh.close()
        show_stats("After full browse", tref)


def check_partial_browse():
    for i in range(niter):
        print("------------------ partial_browse #%s --------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        for node in fileh.root.ngroup0.ngroup1:
            pass
        fileh.close()
        show_stats("After closing file", tref)


def check_full_browse_attrs():
    for i in range(niter):
        print("------------------ full_browse_attrs #%s -----------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        for node in fileh:
            # Access to an attribute
            klass = node._v_attrs.CLASS
        fileh.close()
        show_stats("After full browse", tref)


def check_partial_browse_attrs():
    for i in range(niter):
        print("------------------ partial_browse_attrs #%s --------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        for node in fileh.root.ngroup0.ngroup1:
            # Access to an attribute
            klass = node._v_attrs.CLASS
        fileh.close()
        show_stats("After closing file", tref)


def check_open_group():
    for i in range(niter):
        print("------------------ open_group #%s ------------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        group = fileh.root.ngroup0.ngroup1
        # Access to an attribute
        klass = group._v_attrs.CLASS
        fileh.close()
        show_stats("After closing file", tref)


def check_open_leaf():
    for i in range(niter):
        print("------------------ open_leaf #%s -----------------------" % i)
        tref = time.time()
        fileh = tables.open_file(filename)
        leaf = fileh.root.ngroup0.ngroup1.array9
        # Access to an attribute
        klass = leaf._v_attrs.CLASS
        fileh.close()
        show_stats("After closing file", tref)


if __name__ == '__main__':

    usage = """usage: %s [-v] [-p] [-n niter] [-O] [-o] [-B] [-b] [-g] [-l] [-A] [-a] [-E] [-S] datafile
              -v verbose  (total dump of profiling)
              -p do profiling
              -n number of iterations for reading
              -O Check open_close
              -o Check only_open
              -B Check full browse
              -b Check partial browse
              -A Check full browse and reading one attr each node
              -a Check partial browse and reading one attr each node
              -g Check open nested group
              -l Check open nested leaf
              -E Check everything
              -S Check everything as subprocess
              \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpn:OoBbAaglESs')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    progname = sys.argv[0]
    args = sys.argv[1:]

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    silent = 0  # if silent, does not print the final statistics
    profile = 0
    all_checks = 0
    all_system_checks = 0
    func = []

    # Checking options
    options = ['-O', '-o', '-B', '-b', '-A', '-a', '-g', '-l']

    # Dict to map options to checking functions
    option2func = {
        '-O': 'check_open_close',
        '-o': 'check_only_open',
        '-B': 'check_full_browse',
        '-b': 'check_partial_browse',
        '-A': 'check_full_browse_attrs',
        '-a': 'check_partial_browse_attrs',
        '-g': 'check_open_group',
        '-l': 'check_open_leaf',
    }

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-p':
            profile = 1
        elif option[0] in option2func:
            func.append(option2func[option[0]])
        elif option[0] == '-E':
            all_checks = 1
            for opt in options:
                func.append(option2func[opt])
        elif option[0] == '-S':
            all_system_checks = 1
        elif option[0] == '-s':
            silent = 1
        elif option[0] == '-n':
            niter = int(option[1])

    filename = pargs[0]

    tref = time.time()
    if all_system_checks:
        args.remove('-S')  # We don't want -S in the options list again
        for opt in options:
            opts = "%s \-s %s %s" % (progname, opt, " ".join(args))
            # print "opts-->", opts
            os.system("python2.4 %s" % opts)
    else:
        if profile:
            for ifunc in func:
                prof.run(ifunc + '()', ifunc + '.prof')
                stats = pstats.Stats(ifunc + '.prof')
                stats.strip_dirs()
                stats.sort_stats('time', 'calls')
                if verbose:
                    stats.print_stats()
                else:
                    stats.print_stats(20)
        else:
            for ifunc in func:
                eval(ifunc + '()')

    if not silent:
        print("------------------ End of run -------------------------")
        show_stats("Final statistics (after closing everything)", tref)

########NEW FILE########
__FILENAME__ = optimal-chunksize
"""Small benchmark on the effect of chunksizes and compression on HDF5 files.

Francesc Alted
2007-11-25

"""

from __future__ import print_function
import os
import math
import subprocess
import tempfile
from time import time
import numpy
import tables

# Size of dataset
# N, M = 512, 2**16     # 256 MB
# N, M = 512, 2**18     # 1 GB
# N, M = 512, 2**19     # 2 GB
N, M = 2000, 1000000  # 15 GB
# N, M = 4000, 1000000  # 30 GB
datom = tables.Float64Atom()   # elements are double precision


def quantize(data, least_significant_digit):
    """Quantize data to improve compression.

    data is quantized using around(scale*data)/scale, where scale is
    2**bits, and bits is determined from the least_significant_digit.
    For example, if least_significant_digit=1, bits will be 4.

    """

    precision = 10. ** -least_significant_digit
    exp = math.log(precision, 10)
    if exp < 0:
        exp = int(math.floor(exp))
    else:
        exp = int(math.ceil(exp))
    bits = math.ceil(math.log(10. ** -exp, 2))
    scale = 2. ** bits
    return numpy.around(scale * data) / scale


def get_db_size(filename):
    sout = subprocess.Popen("ls -sh %s" % filename, shell=True,
                            stdout=subprocess.PIPE).stdout
    line = [l for l in sout][0]
    return line.split()[0]


def bench(chunkshape, filters):
    numpy.random.seed(1)   # to have reproductible results
    filename = tempfile.mktemp(suffix='.h5')
    print("Doing test on the file system represented by:", filename)

    f = tables.open_file(filename, 'w')
    e = f.create_earray(f.root, 'earray', datom, shape=(0, M),
                        filters = filters,
                        chunkshape = chunkshape)
    # Fill the array
    t1 = time()
    for i in range(N):
        # e.append([numpy.random.rand(M)])  # use this for less compressibility
        e.append([quantize(numpy.random.rand(M), 6)])
    # os.system("sync")
    print("Creation time:", round(time() - t1, 3), end=' ')
    filesize = get_db_size(filename)
    filesize_bytes = os.stat(filename)[6]
    print("\t\tFile size: %d -- (%s)" % (filesize_bytes, filesize))

    # Read in sequential mode:
    e = f.root.earray
    t1 = time()
    # Flush everything to disk and flush caches
    #os.system("sync; echo 1 > /proc/sys/vm/drop_caches")
    for row in e:
        t = row
    print("Sequential read time:", round(time() - t1, 3), end=' ')

    # f.close()
    # return

    # Read in random mode:
    i_index = numpy.random.randint(0, N, 128)
    j_index = numpy.random.randint(0, M, 256)
    # Flush everything to disk and flush caches
    #os.system("sync; echo 1 > /proc/sys/vm/drop_caches")

    # Protection against too large chunksizes
    # 4 MB
    if 0 and filters.complevel and chunkshape[0] * chunkshape[1] * 8 > 2 ** 22:
        f.close()
        return

    t1 = time()
    for i in i_index:
        for j in j_index:
            t = e[i, j]
    print("\tRandom read time:", round(time() - t1, 3))

    f.close()

# Benchmark with different chunksizes and filters
# for complevel in (0, 1, 3, 6, 9):
for complib in (None, 'zlib', 'lzo', 'blosc'):
# for complib in ('blosc',):
    if complib:
        filters = tables.Filters(complevel=5, complib=complib)
    else:
        filters = tables.Filters(complevel=0)
    print("8<--" * 20, "\nFilters:", filters, "\n" + "-" * 80)
    # for ecs in (11, 14, 17, 20, 21, 22):
    for ecs in range(10, 24):
    # for ecs in (19,):
        chunksize = 2 ** ecs
        chunk1 = 1
        chunk2 = chunksize / datom.itemsize
        if chunk2 > M:
            chunk1 = chunk2 / M
            chunk2 = M
        chunkshape = (chunk1, chunk2)
        cs_str = str(chunksize / 1024) + " KB"
        print("***** Chunksize:", cs_str, "/ Chunkshape:", chunkshape, "*****")
        bench(chunkshape, filters)

########NEW FILE########
__FILENAME__ = plot-bar
#!/usr/bin/env python
# a stacked bar plot with errorbars

from __future__ import print_function
from pylab import *

checks = ['open_close', 'only_open',
          'full_browse', 'partial_browse',
          'full_browse_attrs', 'partial_browse_attrs',
          'open_group', 'open_leaf',
          'total']
width = 0.15       # the width of the bars: can also be len(x) sequence
colors = ['r', 'm', 'g', 'y', 'b']
ind = arange(len(checks))    # the x locations for the groups


def get_values(filename):
    values = []
    f = open(filename)
    for line in f:
        if show_memory:
            if line.startswith('VmData:'):
                values.append(float(line.split()[1]) / 1024.)
        else:
            if line.startswith('WallClock time:'):
                values.append(float(line.split(':')[1]))
    f.close()
    return values


def plot_bar(values, n):
    global ind
    if not gtotal:
        # Remove the grand totals
        values.pop()
        if n == 0:
            checks.pop()
            ind = arange(len(checks))
    p = bar(ind + width * n, values, width, color=colors[n])
    return p


def show_plot(bars, filenames, tit):
    if show_memory:
        ylabel('Memory (MB)')
    else:
        ylabel('Time (s)')
    title(tit)
    n = len(filenames)
    xticks(ind + width * n / 2., checks, rotation=45,
           horizontalalignment='right', fontsize=8)
    if not gtotal:
        #loc = 'center right'
        loc = 'upper left'
    else:
        loc = 'center left'

    legends = [f[:f.index('_')] for f in filenames]
    legends = [l.replace('-', ' ') for l in legends]
    legend([p[0] for p in bars], legends, loc=loc)

    subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2)
    if outfile:
        savefig(outfile)
    else:
        show()

if __name__ == '__main__':

    import sys
    import getopt

    usage = """usage: %s [-g] [-m] [-o file] [-t title] files
            -g grand total
            -m show memory instead of time
            -o filename for output (only .png and .jpg extensions supported)
            -t title of the plot
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'gmo:t:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    progname = sys.argv[0]
    args = sys.argv[1:]

    # if we pass too few parameters, abort
    if len(pargs) < 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    tit = "Comparison of differents PyTables versions"
    gtotal = 0
    show_memory = 0
    outfile = None

    # Get the options
    for option in opts:
        if option[0] == '-g':
            gtotal = 1
        elif option[0] == '-m':
            show_memory = 1
        elif option[0] == '-o':
            outfile = option[1]
        elif option[0] == '-t':
            tit = option[1]

    filenames = pargs
    bars = []
    n = 0
    for filename in filenames:
        values = get_values(filename)
        print("Values-->", values)
        bars.append(plot_bar(values, n))
        n += 1
    show_plot(bars, filenames, tit)

########NEW FILE########
__FILENAME__ = poly
#######################################################################
# This script compares the speed of the computation of a polynomial
# for different (numpy.memmap and tables.Expr) out-of-memory paradigms.
#
# Author: Francesc Alted
# Date: 2010-02-24
#######################################################################

from __future__ import print_function
import os
from time import time
import numpy as np
import tables as tb
import numexpr as ne

expr = ".25*x**3 + .75*x**2 - 1.5*x - 2"  # the polynomial to compute
N = 10 * 1000 * 1000    # the number of points to compute expression (80 MB)
step = 100 * 1000       # perform calculation in slices of `step` elements
dtype = np.dtype('f8')  # the datatype
#CHUNKSHAPE = (2**17,)
CHUNKSHAPE = None

# Global variable for the x values for pure numpy & numexpr
x = None

# *** The next variables do not need to be changed ***

# Filenames for numpy.memmap
fprefix = "numpy.memmap"             # the I/O file prefix
mpfnames = [fprefix + "-x.bin", fprefix + "-r.bin"]

# Filename for tables.Expr
h5fname = "tablesExpr.h5"     # the I/O file

MB = 1024 * 1024.               # a MegaByte


def print_filesize(filename, clib=None, clevel=0):
    """Print some statistics about file sizes."""

    # os.system("sync")    # make sure that all data has been flushed to disk
    if isinstance(filename, list):
        filesize_bytes = 0
        for fname in filename:
            filesize_bytes += os.stat(fname)[6]
    else:
        filesize_bytes = os.stat(filename)[6]
    filesize_MB = round(filesize_bytes / MB, 1)
    print("\t\tTotal file sizes: %d -- (%s MB)" % (
        filesize_bytes, filesize_MB), end=' ')
    if clevel > 0:
        print("(using %s lvl%s)" % (clib, clevel))
    else:
        print()


def populate_x_numpy():
    """Populate the values in x axis for numpy."""
    global x
    # Populate x in range [-1, 1]
    x = np.linspace(-1, 1, N)


def populate_x_memmap():
    """Populate the values in x axis for numpy.memmap."""
    # Create container for input
    x = np.memmap(mpfnames[0], dtype=dtype, mode="w+", shape=(N,))

    # Populate x in range [-1, 1]
    for i in range(0, N, step):
        chunk = np.linspace((2 * i - N) / float(N),
                            (2 * (i + step) - N) / float(N), step)
        x[i:i + step] = chunk
    del x        # close x memmap


def populate_x_tables(clib, clevel):
    """Populate the values in x axis for pytables."""
    f = tb.open_file(h5fname, "w")

    # Create container for input
    atom = tb.Atom.from_dtype(dtype)
    filters = tb.Filters(complib=clib, complevel=clevel)
    x = f.create_carray(f.root, "x", atom=atom, shape=(N,),
                        filters=filters,
                        chunkshape=CHUNKSHAPE,
                        )

    # Populate x in range [-1, 1]
    for i in range(0, N, step):
        chunk = np.linspace((2 * i - N) / float(N),
                            (2 * (i + step) - N) / float(N), step)
        x[i:i + step] = chunk
    f.close()


def compute_numpy():
    """Compute the polynomial with pure numpy."""
    y = eval(expr)


def compute_numexpr():
    """Compute the polynomial with pure numexpr."""
    y = ne.evaluate(expr)


def compute_memmap():
    """Compute the polynomial with numpy.memmap."""
    # Reopen inputs in read-only mode
    x = np.memmap(mpfnames[0], dtype=dtype, mode='r', shape=(N,))
    # Create the array output
    r = np.memmap(mpfnames[1], dtype=dtype, mode="w+", shape=(N,))

    # Do the computation by chunks and store in output
    r[:] = eval(expr)          # where is stored the result?
    # r = eval(expr)            # result is stored in-memory

    del x, r                   # close x and r memmap arrays
    print_filesize(mpfnames)


def compute_tables(clib, clevel):
    """Compute the polynomial with tables.Expr."""
    f = tb.open_file(h5fname, "a")
    x = f.root.x               # get the x input
    # Create container for output
    atom = tb.Atom.from_dtype(dtype)
    filters = tb.Filters(complib=clib, complevel=clevel)
    r = f.create_carray(f.root, "r", atom=atom, shape=(N,),
                        filters=filters,
                        chunkshape=CHUNKSHAPE,
                        )

    # Do the actual computation and store in output
    ex = tb.Expr(expr)         # parse the expression
    ex.set_output(r)            # where is stored the result?
                               # when commented out, the result goes in-memory
    ex.eval()                  # evaluate!

    f.close()
    print_filesize(h5fname, clib, clevel)


if __name__ == '__main__':

    tb.print_versions()

    print("Total size for datasets:",
          round(2 * N * dtype.itemsize / MB, 1), "MB")

    # Get the compression libraries supported
    # supported_clibs = [clib for clib in ("zlib", "lzo", "bzip2", "blosc")
    # supported_clibs = [clib for clib in ("zlib", "lzo", "blosc")
    supported_clibs = [clib for clib in ("blosc",)
                       if tb.which_lib_version(clib)]

    # Initialization code
    # for what in ["numpy", "numpy.memmap", "numexpr"]:
    for what in ["numpy", "numexpr"]:
        # break
        print("Populating x using %s with %d points..." % (what, N))
        t0 = time()
        if what == "numpy":
            populate_x_numpy()
            compute = compute_numpy
        elif what == "numexpr":
            populate_x_numpy()
            compute = compute_numexpr
        elif what == "numpy.memmap":
            populate_x_memmap()
            compute = compute_memmap
        print("*** Time elapsed populating:", round(time() - t0, 3))
        print("Computing: '%s' using %s" % (expr, what))
        t0 = time()
        compute()
        print("**************** Time elapsed computing:",
              round(time() - t0, 3))

    for what in ["tables.Expr"]:
        t0 = time()
        first = True    # Sentinel
        for clib in supported_clibs:
            # for clevel in (0, 1, 3, 6, 9):
            for clevel in range(10):
            # for clevel in (1,):
                if not first and clevel == 0:
                    continue
                print("Populating x using %s with %d points..." % (what, N))
                populate_x_tables(clib, clevel)
                print("*** Time elapsed populating:", round(time() - t0, 3))
                print("Computing: '%s' using %s" % (expr, what))
                t0 = time()
                compute_tables(clib, clevel)
                print("**************** Time elapsed computing:",
                      round(time() - t0, 3))
                first = False

########NEW FILE########
__FILENAME__ = postgres-search-bench
from __future__ import print_function
from time import time
import numpy
import random

DSN = "dbname=test port = 5435"

# in order to always generate the same random sequence
random.seed(19)


def flatten(l):
    """Flattens list of tuples l."""
    return [x[0] for x in l]


def fill_arrays(start, stop):
    col_i = numpy.arange(start, stop, type=numpy.Int32)
    if userandom:
        col_j = numpy.random.uniform(0, nrows, size=[stop - start])
    else:
        col_j = numpy.array(col_i, type=numpy.Float64)
    return col_i, col_j

# Generator for ensure pytables benchmark compatibility


def int_generator(nrows):
    step = 1000 * 100
    j = 0
    for i in range(nrows):
        if i >= step * j:
            stop = (j + 1) * step
            if stop > nrows:  # Seems unnecessary
                stop = nrows
            col_i, col_j = fill_arrays(i, stop)
            j += 1
            k = 0
        yield (col_i[k], col_j[k])
        k += 1


def int_generator_slow(nrows):
    for i in range(nrows):
        if userandom:
            yield (i, float(random.randint(0, nrows)))
        else:
            yield (i, float(i))


class Stream32(object):

    "Object simulating a file for reading"

    def __init__(self):
        self.n = None
        self.read_it = self.read_iter()

    # No va! Hi ha que convertir a un de normal!
    def readline(self, n=None):
        for tup in int_generator(nrows):
            sout = "%s\t%s\n" % tup
            if n is not None and len(sout) > n:
                for i in range(0, len(sout), n):
                    yield sout[i:i + n]
            else:
                yield sout

    def read_iter(self):
        sout = ""
        n = self.n
        for tup in int_generator(nrows):
            sout += "%s\t%s\n" % tup
            if n is not None and len(sout) > n:
                for i in range(n, len(sout), n):
                    rout = sout[:n]
                    sout = sout[n:]
                    yield rout
        yield sout

    def read(self, n=None):
        self.n = n
        try:
            str = next(self.read_it)
        except StopIteration:
            str = ""
        return str


def open_db(filename, remove=0):
    if not filename:
        con = sqlite.connect(DSN)
    else:
        con = sqlite.connect(filename)
    cur = con.cursor()
    return con, cur


def create_db(filename, nrows):
    con, cur = open_db(filename, remove=1)
    try:
        cur.execute("create table ints(i integer, j double precision)")
    except:
        con.rollback()
        cur.execute("DROP TABLE ints")
        cur.execute("create table ints(i integer, j double precision)")
    con.commit()
    con.set_isolation_level(2)
    t1 = time()
    st = Stream32()
    cur.copy_from(st, "ints")
    # In case of postgres, the speeds of generator and loop are similar
    #cur.executemany("insert into ints values (%s,%s)", int_generator(nrows))
#     for i in xrange(nrows):
#         cur.execute("insert into ints values (%s,%s)", (i, float(i)))
    con.commit()
    ctime = time() - t1
    if verbose:
        print("insert time:", round(ctime, 5))
        print("Krows/s:", round((nrows / 1000.) / ctime, 5))
    close_db(con, cur)


def index_db(filename):
    con, cur = open_db(filename)
    t1 = time()
    cur.execute("create index ij on ints(j)")
    con.commit()
    itime = time() - t1
    if verbose:
        print("index time:", round(itime, 5))
        print("Krows/s:", round(nrows / itime, 5))
    # Close the DB
    close_db(con, cur)


def query_db(filename, rng):
    con, cur = open_db(filename)
    t1 = time()
    ntimes = 10
    for i in range(ntimes):
        # between clause does not seem to take advantage of indexes
        # cur.execute("select j from ints where j between %s and %s" % \
        cur.execute("select i from ints where j >= %s and j <= %s" %
                    # cur.execute("select i from ints where i >= %s and i <=
                    # %s" %
                    (rng[0] + i, rng[1] + i))
        results = cur.fetchall()
    con.commit()
    qtime = (time() - t1) / ntimes
    if verbose:
        print("query time:", round(qtime, 5))
        print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
        results = sorted(flatten(results))
        print(results)
    close_db(con, cur)


def close_db(con, cur):
    cur.close()
    con.close()

if __name__ == "__main__":
    import sys
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-m] [-i] [-q] [-c] [-R range] [-n nrows] file
            -v verbose
            -p use "psyco" if available
            -m use random values to fill the table
            -q do query
            -c create the database
            -i index the table
            -2 use sqlite2 (default is use sqlite3)
            -R select a range in a field in the form "start,stop" (def "0,10")
            -n sets the number of rows (in krows) in each table
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpmiqc2R:n:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    usepsyco = 0
    userandom = 0
    docreate = 0
    createindex = 0
    doquery = 0
    sqlite_version = "3"
    rng = [0, 10]
    nrows = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-m':
            userandom = 1
        elif option[0] == '-i':
            createindex = 1
        elif option[0] == '-q':
            doquery = 1
        elif option[0] == '-c':
            docreate = 1
        elif option[0] == "-2":
            sqlite_version = "2"
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-n':
            nrows = int(option[1])

    # Catch the hdf5 file passed as the last argument
    filename = pargs[0]

#     if sqlite_version == "2":
#         import sqlite
#     else:
#         from pysqlite2 import dbapi2 as sqlite
    import psycopg2 as sqlite

    if verbose:
        # print "pysqlite version:", sqlite.version
        if userandom:
            print("using random values")

    if docreate:
        if verbose:
            print("writing %s krows" % nrows)
        if psyco_imported and usepsyco:
            psyco.bind(create_db)
        nrows *= 1000
        create_db(filename, nrows)

    if createindex:
        index_db(filename)

    if doquery:
        query_db(filename, rng)

########NEW FILE########
__FILENAME__ = postgres_backend
from __future__ import print_function
import subprocess  # Needs Python 2.4
from indexed_search import DB
import psycopg2 as db2

CLUSTER_NAME = "base"
DATA_DIR = "/scratch2/postgres/data/%s" % CLUSTER_NAME
#DATA_DIR = "/var/lib/pgsql/data/%s" % CLUSTER_NAME
DSN = "dbname=%s port=%s"
CREATE_DB = "createdb %s"
DROP_DB = "dropdb %s"
TABLE_NAME = "intsfloats"
PORT = 5432


class StreamChar(object):
    "Object simulating a file for reading"

    def __init__(self, db):
        self.db = db
        self.nrows = db.nrows
        self.step = db.step
        self.read_it = self.read_iter()

    def values_generator(self):
        j = 0
        for i in range(self.nrows):
            if i >= j * self.step:
                stop = (j + 1) * self.step
                if stop > self.nrows:
                    stop = self.nrows
                arr_i4, arr_f8 = self.db.fill_arrays(i, stop)
                j += 1
                k = 0
            yield (arr_i4[k], arr_i4[k], arr_f8[k], arr_f8[k])
            k += 1

    def read_iter(self):
        sout = ""
        n = self.nbytes
        for tup in self.values_generator():
            sout += "%s\t%s\t%s\t%s\n" % tup
            if n is not None and len(sout) > n:
                for i in range(n, len(sout), n):
                    rout = sout[:n]
                    sout = sout[n:]
                    yield rout
        yield sout

    def read(self, n=None):
        self.nbytes = n
        try:
            str = next(self.read_it)
        except StopIteration:
            str = ""
        return str

    # required by postgres2 driver, but not used
    def readline(self):
        pass


class Postgres_DB(DB):

    def __init__(self, nrows, rng, userandom):
        DB.__init__(self, nrows, rng, userandom)
        self.port = PORT

    def flatten(self, l):
        """Flattens list of tuples l."""
        return [x[0] for x in l]
        # return map(lambda x: x[col], l)

    # Overloads the method in DB class
    def get_db_size(self):
        sout = subprocess.Popen("sudo du -s %s" % DATA_DIR,
                                shell=True,
                                stdout=subprocess.PIPE).stdout
        line = [l for l in sout][0]
        return int(line.split()[0])

    def open_db(self, remove=0):
        if remove:
            sout = subprocess.Popen(DROP_DB % self.filename, shell=True,
                                    stdout=subprocess.PIPE).stdout
            for line in sout:
                print(line)
            sout = subprocess.Popen(CREATE_DB % self.filename, shell=True,
                                    stdout=subprocess.PIPE).stdout
            for line in sout:
                print(line)

        print("Processing database:", self.filename)
        con = db2.connect(DSN % (self.filename, self.port))
        self.cur = con.cursor()
        return con

    def create_table(self, con):
        self.cur.execute("""create table %s(
                          col1 integer,
                          col2 integer,
                          col3 double precision,
                          col4 double precision)""" % TABLE_NAME)
        con.commit()

    def fill_table(self, con):
        st = StreamChar(self)
        self.cur.copy_from(st, TABLE_NAME)
        con.commit()

    def index_col(self, con, colname, optlevel, idxtype, verbose):
        self.cur.execute("create index %s on %s(%s)" %
                         (colname + '_idx', TABLE_NAME, colname))
        con.commit()

    def do_query_simple(self, con, column, base):
        self.cur.execute(
            "select sum(%s) from %s where %s >= %s and %s <= %s" %
            (column, TABLE_NAME,
             column, base + self.rng[0],
             column, base + self.rng[1]))
#             "select * from %s where %s >= %s and %s <= %s" % \
#             (TABLE_NAME,
#              column, base+self.rng[0],
#              column, base+self.rng[1]))
        #results = self.flatten(self.cur.fetchall())
        results = self.cur.fetchall()
        return results

    def do_query(self, con, column, base, *unused):
        d = (self.rng[1] - self.rng[0]) / 2.
        inf1 = int(self.rng[0] + base)
        sup1 = int(self.rng[0] + d + base)
        inf2 = self.rng[0] + base * 2
        sup2 = self.rng[0] + d + base * 2
        # print "lims-->", inf1, inf2, sup1, sup2
        condition = "((%s>=%s) and (%s<%s)) or ((col2>%s) and (col2<%s))"
        #condition = "((col3>=%s) and (col3<%s)) or ((col1>%s) and (col1<%s))"
        condition += " and ((col1+3.1*col2+col3*col4) > 3)"
        #condition += " and (sqrt(col1^2+col2^2+col3^2+col4^2) > .1)"
        condition = condition % (column, inf2, column, sup2, inf1, sup1)
        # print "condition-->", condition
        self.cur.execute(
            #            "select sum(%s) from %s where %s" %
            "select %s from %s where %s" %
            (column, TABLE_NAME, condition))
        #results = self.flatten(self.cur.fetchall())
        results = self.cur.fetchall()
        #results = self.cur.fetchall()
        # print "results-->", results
        # return results
        return len(results)

    def close_db(self, con):
        self.cur.close()
        con.close()

########NEW FILE########
__FILENAME__ = pytables-search-bench
from __future__ import print_function
import os
from time import time
import random
import numpy as np
import tables

# in order to always generate the same random sequence
random.seed(19)
np.random.seed((19, 20))


def open_db(filename, remove=0):
    if remove and os.path.exists(filename):
        os.remove(filename)
    con = tables.open_file(filename, 'a')
    return con


def create_db(filename, nrows):

    class Record(tables.IsDescription):
        col1 = tables.Int32Col()
        col2 = tables.Int32Col()
        col3 = tables.Float64Col()
        col4 = tables.Float64Col()

    con = open_db(filename, remove=1)
    table = con.create_table(con.root, 'table', Record,
                             filters=filters, expectedrows=nrows)
    table.indexFilters = filters
    step = 1000 * 100
    scale = 0.1
    t1 = time()
    j = 0
    for i in range(0, nrows, step):
        stop = (j + 1) * step
        if stop > nrows:
            stop = nrows
        arr_f8 = np.arange(i, stop, type=np.Float64)
        arr_i4 = np.arange(i, stop, type=np.Int32)
        if userandom:
            arr_f8 += np.random.normal(0, stop * scale, shape=[stop - i])
            arr_i4 = np.array(arr_f8, type=np.Int32)
        recarr = np.rec.fromarrays([arr_i4, arr_i4, arr_f8, arr_f8])
        table.append(recarr)
        j += 1
    table.flush()
    ctime = time() - t1
    if verbose:
        print("insert time:", round(ctime, 5))
        print("Krows/s:", round((nrows / 1000.) / ctime, 5))
    index_db(table)
    close_db(con)


def index_db(table):
    t1 = time()
    table.cols.col2.create_index()
    itime = time() - t1
    if verbose:
        print("index time (int):", round(itime, 5))
        print("Krows/s:", round((nrows / 1000.) / itime, 5))
    t1 = time()
    table.cols.col4.create_index()
    itime = time() - t1
    if verbose:
        print("index time (float):", round(itime, 5))
        print("Krows/s:", round((nrows / 1000.) / itime, 5))


def query_db(filename, rng):
    con = open_db(filename)
    table = con.root.table
    # Query for integer columns
    # Query for non-indexed column
    if not doqueryidx:
        t1 = time()
        ntimes = 10
        for i in range(ntimes):
            results = [
                r['col1'] for r in table.where(
                    rng[0] + i <= table.cols.col1 <= rng[1] + i)
            ]
        qtime = (time() - t1) / ntimes
        if verbose:
            print("query time (int, not indexed):", round(qtime, 5))
            print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
            print(results)
    # Query for indexed column
    t1 = time()
    ntimes = 10
    for i in range(ntimes):
        results = [
            r['col1'] for r in table.where(
                rng[0] + i <= table.cols.col2 <= rng[1] + i)
        ]
    qtime = (time() - t1) / ntimes
    if verbose:
        print("query time (int, indexed):", round(qtime, 5))
        print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
        print(results)
    # Query for floating columns
    # Query for non-indexed column
    if not doqueryidx:
        t1 = time()
        ntimes = 10
        for i in range(ntimes):
            results = [
                r['col3'] for r in table.where(
                    rng[0] + i <= table.cols.col3 <= rng[1] + i)
            ]
        qtime = (time() - t1) / ntimes
        if verbose:
            print("query time (float, not indexed):", round(qtime, 5))
            print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
            print(results)
    # Query for indexed column
    t1 = time()
    ntimes = 10
    for i in range(ntimes):
        results = [r['col3'] for r in
                   table.where(rng[0] + i <= table.cols.col4 <= rng[1] + i)]
    qtime = (time() - t1) / ntimes
    if verbose:
        print("query time (float, indexed):", round(qtime, 5))
        print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
        print(results)
    close_db(con)


def close_db(con):
    con.close()

if __name__ == "__main__":
    import sys
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-m] [-c] [-q] [-i] [-z complevel] [-l complib] [-R range] [-n nrows] file
            -v verbose
            -p use "psyco" if available
            -m use random values to fill the table
            -q do a query (both indexed and non-indexed version)
            -i do a query (exclude non-indexed version)
            -c create the database
            -z compress with zlib (no compression by default)
            -l use complib for compression (zlib used by default)
            -R select a range in a field in the form "start,stop" (def "0,10")
            -n sets the number of rows (in krows) in each table
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpmcqiz:l:R:n:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    usepsyco = 0
    userandom = 0
    docreate = 0
    docompress = 0
    complib = "zlib"
    doquery = 0
    doqueryidx = 0
    rng = [0, 10]
    nrows = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-m':
            userandom = 1
        elif option[0] == '-c':
            docreate = 1
            createindex = 1
        elif option[0] == '-q':
            doquery = 1
        elif option[0] == '-i':
            doqueryidx = 1
        elif option[0] == '-z':
            docompress = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-n':
            nrows = int(option[1])

    # Catch the hdf5 file passed as the last argument
    filename = pargs[0]

    # The filters chosen
    filters = tables.Filters(complevel=docompress, complib=complib)

    if verbose:
        print("pytables version:", tables.__version__)
        if userandom:
            print("using random values")
        if doqueryidx:
            print("doing indexed queries only")

    if docreate:
        if verbose:
            print("writing %s krows" % nrows)
        if psyco_imported and usepsyco:
            psyco.bind(create_db)
        nrows *= 1000
        create_db(filename, nrows)

    if doquery:
        query_db(filename, rng)

########NEW FILE########
__FILENAME__ = pytables_backend
from __future__ import print_function
import os
import tables
from indexed_search import DB


class PyTables_DB(DB):

    def __init__(self, nrows, rng, userandom, datadir,
                 docompress=0, complib='zlib', kind="medium", optlevel=6):
        DB.__init__(self, nrows, rng, userandom)
        self.tprof = []
        # Specific part for pytables
        self.docompress = docompress
        self.complib = complib
        # Complete the filename
        self.filename = "pro-" + self.filename
        self.filename += '-' + 'O%s' % optlevel
        self.filename += '-' + kind
        if docompress:
            self.filename += '-' + complib + str(docompress)
        self.filename = datadir + '/' + self.filename + '.h5'
        # The chosen filters
        self.filters = tables.Filters(complevel=self.docompress,
                                      complib=self.complib,
                                      shuffle=1)
        print("Processing database:", self.filename)

    def open_db(self, remove=0):
        if remove and os.path.exists(self.filename):
            os.remove(self.filename)
        con = tables.open_file(self.filename, 'a')
        return con

    def close_db(self, con):
        # Remove first the table_cache attribute if it exists
        if hasattr(self, "table_cache"):
            del self.table_cache
        con.close()

    def create_table(self, con):
        class Record(tables.IsDescription):
            col1 = tables.Int32Col()
            col2 = tables.Int32Col()
            col3 = tables.Float64Col()
            col4 = tables.Float64Col()

        con.create_table(con.root, 'table', Record,
                         filters=self.filters, expectedrows=self.nrows)

    def fill_table(self, con):
        "Fills the table"
        table = con.root.table
        j = 0
        for i in range(0, self.nrows, self.step):
            stop = (j + 1) * self.step
            if stop > self.nrows:
                stop = self.nrows
            arr_i4, arr_f8 = self.fill_arrays(i, stop)
#             recarr = records.fromarrays([arr_i4, arr_i4, arr_f8, arr_f8])
#             table.append(recarr)
            table.append([arr_i4, arr_i4, arr_f8, arr_f8])
            j += 1
        table.flush()

    def index_col(self, con, column, kind, optlevel, verbose):
        col = getattr(con.root.table.cols, column)
        col.create_index(kind=kind, optlevel=optlevel, filters=self.filters,
                         tmp_dir="/scratch2/faltet",
                         _verbose=verbose, _blocksizes=None)
#                       _blocksizes=(2**27, 2**22, 2**15, 2**7))
#                       _blocksizes=(2**27, 2**22, 2**14, 2**6))
#                       _blocksizes=(2**27, 2**20, 2**13, 2**5),
#                        _testmode=True)

    def do_query(self, con, column, base, inkernel):
        if True:
            if not hasattr(self, "table_cache"):
                self.table_cache = table = con.root.table
                self.colobj = getattr(table.cols, column)
                #self.colobj = getattr(table.cols, 'col1')
                self.condvars = {"col": self.colobj,
                                 "col1": table.cols.col1,
                                 "col2": table.cols.col2,
                                 "col3": table.cols.col3,
                                 "col4": table.cols.col4,
                                 }
            table = self.table_cache
            colobj = self.colobj
        else:
            table = con.root.table
            colobj = getattr(table.cols, column)
            self.condvars = {"col": colobj,
                             "col1": table.cols.col1,
                             "col2": table.cols.col2,
                             "col3": table.cols.col3,
                             "col4": table.cols.col4,
                             }
        self.condvars['inf'] = self.rng[0] + base
        self.condvars['sup'] = self.rng[1] + base
        # For queries that can use two indexes instead of just one
        d = (self.rng[1] - self.rng[0]) / 2.
        inf1 = int(self.rng[0] + base)
        sup1 = int(self.rng[0] + d + base)
        inf2 = self.rng[0] + base * 2
        sup2 = self.rng[0] + d + base * 2
        self.condvars['inf1'] = inf1
        self.condvars['sup1'] = sup1
        self.condvars['inf2'] = inf2
        self.condvars['sup2'] = sup2
        #condition = "(inf == col2)"
        #condition = "(inf==col2) & (col4==sup)"
        #condition = "(inf==col2) | (col4==sup)"
        #condition = "(inf==col2) | (col2==sup)"
        #condition = "(inf==col2) & (col3==sup)"
        #condition = "((inf==col2) & (sup==col4)) & (col3==sup)"
        #condition = "((inf==col1) & (sup==col4)) & (col3==sup)"
        #condition = "(inf<=col1) & (col3<sup)"
        #condition = "(inf<=col2) & (col4<sup)"
        #condition = "(inf<=col4) & (col4<sup)"
        #condition = "((inf<=col4) & (col4<sup)) | (col2<3)"
        #condition = "((inf<=col4) | (col4<sup)) & (col2<3)"
        #condition = "((inf<=col4) | (col4<sup)) & ((inf<col2) & (col2<sup))"
        #condition = "((inf<=col4) & (col4<sup)) | ((inf<col2) & (col2<sup))"
        #condition = "((inf<=col4) & (col4<sup)) & ((inf<col2) & (col2<sup))"
        #condition = "((inf2<=col3) & (col3<sup2)) | ((inf1<col1) & (col1<sup1))"
        # print "lims-->", inf1, inf2, sup1, sup2
        condition = "((inf2<=col) & (col<sup2)) | ((inf1<col2) & (col2<sup1))"
        condition += " & ((col1+3.1*col2+col3*col4) > 3)"
        #condition += " & (col2*(col3+3.1)+col3*col4 > col1)"
#        condition = "(inf<=col) & (col<=sup) & (col3 >= 0)"
##        condition = "(inf<=col) & (col<=sup)"
#         condition = "(%s<=col) & (col<=%s)" % \
#                     (self.rng[0]+base, self.rng[1]+base)
        # condition = "(%s<=col1*col2) & (col3*col4<=%s)" % \
        #             (self.rng[0]+base, self.rng[1]+base)
        # condition = "(col**2.4==%s)" % (self.rng[0]+base)
        # condition = "(col==%s)" % (self.rng[0]+base)
        # condvars = {"col": colobj}
        #c = self.condvars
        # print "condvars-->", c['inf'], c['sup'], c['inf2'], c['sup2']
        ncoords = 0
        if colobj.is_indexed:
            results = [r[column]
                       for r in table.where(condition, self.condvars)]
#             coords = table.get_where_list(condition, self.condvars)
#             results = table.read_coordinates(coords, field=column)

#            results = table.read_where(condition, self.condvars, field=column)

        elif inkernel:
            print("Performing in-kernel query")
            results = [r[column]
                       for r in table.where(condition, self.condvars)]
            #coords = [r.nrow for r in table.where(condition, self.condvars)]
            #results = table.read_coordinates(coords)
#             for r in table.where(condition, self.condvars):
#                 var = r[column]
#                 ncoords += 1
        else:
#             coords = [r.nrow for r in table
#                       if (self.rng[0]+base <= r[column] <= self.rng[1]+base)]
#             results = table.read_coordinates(coords)
            print("Performing regular query")
            results = [
                r[column] for r in table if ((
                    (inf2 <= r['col4']) and (r['col4'] < sup2)) or
                    ((inf1 < r['col2']) and (r['col2'] < sup1)) and
                    ((r['col1'] + 3.1 * r['col2'] + r['col3'] * r['col4']) > 3)
                )]

        ncoords = len(results)

        # return coords
        # print "results-->", results
        # return results
        return ncoords
        #self.tprof.append( self.colobj.index.tprof )
        # return ncoords, self.tprof

########NEW FILE########
__FILENAME__ = recarray2-test
from __future__ import print_function
import os
import sys
import time
import numpy as np
import chararray
import recarray
import recarray2  # This is my modified version

usage = """usage: %s recordlength
     Set recordlength to 1000 at least to obtain decent figures!
""" % sys.argv[0]

try:
    reclen = int(sys.argv[1])
except:
    print(usage)
    sys.exit()

delta = 0.000001

# Creation of recarrays objects for test
x1 = np.array(np.arange(reclen))
x2 = chararray.array(None, itemsize=7, shape=reclen)
x3 = np.array(np.arange(reclen, reclen * 3, 2), np.Float64)
r1 = recarray.fromarrays([x1, x2, x3], names='a,b,c')
r2 = recarray2.fromarrays([x1, x2, x3], names='a,b,c')

print("recarray shape in test ==>", r2.shape)

print("Assignment in recarray original")
print("-------------------------------")
t1 = time.clock()
for row in range(reclen):
    #r1.field("b")[row] = "changed"
    r1.field("c")[row] = float(row ** 2)
t2 = time.clock()
origtime = round(t2 - t1, 3)
print("Assign time:", origtime, " Rows/s:", int(reclen / (origtime + delta)))
# print "Field b on row 2 after re-assign:", r1.field("c")[2]
print()

print("Assignment in recarray modified")
print("-------------------------------")
t1 = time.clock()
for row in range(reclen):
    rec = r2._row(row)  # select the row to be changed
    # rec.b = "changed"      # change the "b" field
    rec.c = float(row ** 2)  # Change the "c" field
t2 = time.clock()
ttime = round(t2 - t1, 3)
print("Assign time:", ttime, " Rows/s:", int(reclen / (ttime + delta)),
      end=' ')
print(" Speed-up:", round(origtime / ttime, 3))
# print "Field b on row 2 after re-assign:", r2.field("c")[2]
print()

print("Selection in recarray original")
print("------------------------------")
t1 = time.clock()
for row in range(reclen):
    rec = r1[row]
    if rec.field("a") < 3:
        print("This record pass the cut ==>", rec.field("c"), "(row", row, ")")
t2 = time.clock()
origtime = round(t2 - t1, 3)
print("Select time:", origtime, " Rows/s:", int(reclen / (origtime + delta)))
print()

print("Selection in recarray modified")
print("------------------------------")
t1 = time.clock()
for row in range(reclen):
    rec = r2._row(row)
    if rec.a < 3:
        print("This record pass the cut ==>", rec.c, "(row", row, ")")
t2 = time.clock()
ttime = round(t2 - t1, 3)
print("Select time:", ttime, " Rows/s:", int(reclen / (ttime + delta)),
      end=' ')
print(" Speed-up:", round(origtime / ttime, 3))
print()

print("Printing in recarray original")
print("------------------------------")
f = open("test.out", "w")
t1 = time.clock()
f.write(str(r1))
t2 = time.clock()
origtime = round(t2 - t1, 3)
f.close()
os.unlink("test.out")
print("Print time:", origtime, " Rows/s:", int(reclen / (origtime + delta)))
print()
print("Printing in recarray modified")
print("------------------------------")
f = open("test2.out", "w")
t1 = time.clock()
f.write(str(r2))
t2 = time.clock()
ttime = round(t2 - t1, 3)
f.close()
os.unlink("test2.out")
print("Print time:", ttime, " Rows/s:", int(reclen / (ttime + delta)), end=' ')
print(" Speed-up:", round(origtime / ttime, 3))
print()

########NEW FILE########
__FILENAME__ = search-bench-plot
from __future__ import print_function
import tables
from pylab import *


def get_values(filename, complib=''):
    f = tables.open_file(filename)
    nrows = f.root.small.create_best.cols.nrows[:]
    corrected_sizes = nrows / 10. ** 6
    if mb_units:
        corrected_sizes = 16 * nrows / 10. ** 6
    if insert:
        values = corrected_sizes / f.root.small.create_best.cols.tfill[:]
    if table_size:
        values = f.root.small.create_best.cols.fsize[:] / nrows
    if query:
        values = corrected_sizes / \
            f.root.small.search_best.inkernel.int.cols.time1[:]
    if query_cache:
        values = corrected_sizes / \
            f.root.small.search_best.inkernel.int.cols.time2[:]

    f.close()
    return nrows, values


def show_plot(plots, yaxis, legends, gtitle):
    xlabel('Number of rows')
    ylabel(yaxis)
    xlim(10 ** 3, 10 ** 8)
    title(gtitle)
    grid(True)

#     legends = [f[f.find('-'):f.index('.out')] for f in filenames]
#     legends = [l.replace('-', ' ') for l in legends]
    if table_size:
        legend([p[0] for p in plots], legends, loc="upper right")
    else:
        legend([p[0] for p in plots], legends, loc="upper left")

    #subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2)
    if outfile:
        savefig(outfile)
    else:
        show()

if __name__ == '__main__':

    import sys
    import getopt

    usage = """usage: %s [-o file] [-t title] [--insert] [--table-size] [--query] [--query-cache] [--MB-units] files
 -o filename for output (only .png and .jpg extensions supported)
 -t title of the plot
 --insert -- Insert time for table
 --table-size -- Size of table
 --query -- Time for querying the integer column
 --query-cache -- Time for querying the integer (cached)
 --MB-units -- Express speed in MB/s instead of MRows/s
 \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'o:t:',
                                    ['insert',
                                     'table-size',
                                     'query',
                                     'query-cache',
                                     'MB-units',
                                     ])
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    progname = sys.argv[0]
    args = sys.argv[1:]

    # if we pass too few parameters, abort
    if len(pargs) < 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    outfile = None
    insert = 0
    table_size = 0
    query = 0
    query_cache = 0
    mb_units = 0
    yaxis = "No axis name"
    tit = None
    gtitle = "Please set a title!"

    # Get the options
    for option in opts:
        if option[0] == '-o':
            outfile = option[1]
        elif option[0] == '-t':
            tit = option[1]
        elif option[0] == '--insert':
            insert = 1
            yaxis = "MRows/s"
            gtitle = "Writing with small (16 bytes) record size"
        elif option[0] == '--table-size':
            table_size = 1
            yaxis = "Bytes/row"
            gtitle = ("Disk space taken by a record (original record size: "
                      "16 bytes)")
        elif option[0] == '--query':
            query = 1
            yaxis = "MRows/s"
            gtitle = ("Selecting with small (16 bytes) record size (file not "
                      "in cache)")
        elif option[0] == '--query-cache':
            query_cache = 1
            yaxis = "MRows/s"
            gtitle = ("Selecting with small (16 bytes) record size (file in "
                      "cache)")
        elif option[0] == '--MB-units':
            mb_units = 1

    filenames = pargs

    if mb_units and yaxis == "MRows/s":
        yaxis = "MB/s"

    if tit:
        gtitle = tit

    plots = []
    legends = []
    for filename in filenames:
        plegend = filename[filename.find('cl-') + 3:filename.index('.h5')]
        plegend = plegend.replace('-', ' ')
        xval, yval = get_values(filename, '')
        print("Values for %s --> %s, %s" % (filename, xval, yval))
        #plots.append(loglog(xval, yval, linewidth=5))
        plots.append(semilogx(xval, yval, linewidth=4))
        legends.append(plegend)
    if 0:  # Per a introduir dades simulades si es vol...
        xval = [1000, 10000, 100000, 1000000, 10000000,
                100000000, 1000000000]
#         yval = [0.003, 0.005, 0.02, 0.06, 1.2,
#                 40, 210]
        yval = [0.0009, 0.0011, 0.0022, 0.005, 0.02,
                0.2, 5.6]
        plots.append(loglog(xval, yval, linewidth=5))
        legends.append("PyTables Std")
    show_plot(plots, yaxis, legends, gtitle)

########NEW FILE########
__FILENAME__ = search-bench
#!/usr/bin/env python

from __future__ import print_function
import sys
import math
import time
import random
import warnings

import numpy

from tables import *

# Initialize the random generator always with the same integer
# in order to have reproductible results
random.seed(19)
numpy.random.seed(19)

randomvalues = 0
worst = 0

Small = {
    "var1": StringCol(itemsize=4, dflt="Hi!", pos=2),
    "var2": Int32Col(pos=1),
    "var3": Float64Col(pos=0),
    #"var4" : BoolCol(),
}


def createNewBenchFile(bfile, verbose):

    class Create(IsDescription):
        nrows = Int32Col(pos=0)
        irows = Int32Col(pos=1)
        tfill = Float64Col(pos=2)
        tidx = Float64Col(pos=3)
        tcfill = Float64Col(pos=4)
        tcidx = Float64Col(pos=5)
        rowsecf = Float64Col(pos=6)
        rowseci = Float64Col(pos=7)
        fsize = Float64Col(pos=8)
        isize = Float64Col(pos=9)
        psyco = BoolCol(pos=10)

    class Search(IsDescription):
        nrows = Int32Col(pos=0)
        rowsel = Int32Col(pos=1)
        time1 = Float64Col(pos=2)
        time2 = Float64Col(pos=3)
        tcpu1 = Float64Col(pos=4)
        tcpu2 = Float64Col(pos=5)
        rowsec1 = Float64Col(pos=6)
        rowsec2 = Float64Col(pos=7)
        psyco = BoolCol(pos=8)

    if verbose:
        print("Creating a new benchfile:", bfile)
    # Open the benchmarking file
    bf = open_file(bfile, "w")
    # Create groups
    for recsize in ["small"]:
        group = bf.create_group("/", recsize, recsize + " Group")
        # Attach the row size of table as attribute
        if recsize == "small":
            group._v_attrs.rowsize = 16
        # Create a Table for writing bench
        bf.create_table(group, "create_best", Create, "best case")
        bf.create_table(group, "create_worst", Create, "worst case")
        for case in ["best", "worst"]:
            # create a group for searching bench (best case)
            groupS = bf.create_group(group, "search_" + case, "Search Group")
            # Create Tables for searching
            for mode in ["indexed", "inkernel", "standard"]:
                groupM = bf.create_group(groupS, mode, mode + " Group")
                # for searching bench
                # for atom in ["string", "int", "float", "bool"]:
                for atom in ["string", "int", "float"]:
                    bf.create_table(groupM, atom, Search, atom + " bench")
    bf.close()


def createFile(filename, nrows, filters, index, heavy, noise, verbose):

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="Searchsorted Benchmark",
                      filters=filters)
    rowswritten = 0

    # Create the test table
    table = fileh.create_table(fileh.root, 'table', Small, "test table",
                               None, nrows)

    t1 = time.time()
    cpu1 = time.clock()
    nrowsbuf = table.nrowsinbuf
    minimum = 0
    maximum = nrows
    for i in range(0, nrows, nrowsbuf):
        if i + nrowsbuf > nrows:
            j = nrows
        else:
            j = i + nrowsbuf
        if randomvalues:
            var3 = numpy.random.uniform(minimum, maximum, size=j - i)
        else:
            var3 = numpy.arange(i, j, dtype=numpy.float64)
            if noise > 0:
                var3 += numpy.random.uniform(-noise, noise, size=j - i)
        var2 = numpy.array(var3, dtype=numpy.int32)
        var1 = numpy.empty(shape=[j - i], dtype="S4")
        if not heavy:
            var1[:] = var2
        table.append([var3, var2, var1])
    table.flush()
    rowswritten += nrows
    time1 = time.time() - t1
    tcpu1 = time.clock() - cpu1
    print("Time for filling:", round(time1, 3),
          "Krows/s:", round(nrows / 1000. / time1, 3), end=' ')
    fileh.close()
    size1 = os.stat(filename)[6]
    print(", File size:", round(size1 / (1024. * 1024.), 3), "MB")
    fileh = open_file(filename, mode="a", title="Searchsorted Benchmark",
                      filters=filters)
    table = fileh.root.table
    rowsize = table.rowsize
    if index:
        t1 = time.time()
        cpu1 = time.clock()
        # Index all entries
        if not heavy:
            indexrows = table.cols.var1.create_index(filters=filters)
        for colname in ['var2', 'var3']:
            table.colinstances[colname].create_index(filters=filters)
        time2 = time.time() - t1
        tcpu2 = time.clock() - cpu1
        print("Time for indexing:", round(time2, 3),
              "iKrows/s:", round(indexrows / 1000. / time2, 3), end=' ')
    else:
        indexrows = 0
        time2 = 0.0000000001  # an ugly hack
        tcpu2 = 0.

    if verbose:
        if index:
            idx = table.cols.var1.index
            print("Index parameters:", repr(idx))
        else:
            print("NOT indexing rows")
    # Close the file
    fileh.close()

    size2 = os.stat(filename)[6] - size1
    if index:
        print(", Index size:", round(size2 / (1024. * 1024.), 3), "MB")
    return (rowswritten, indexrows, rowsize, time1, time2,
            tcpu1, tcpu2, size1, size2)


def benchCreate(file, nrows, filters, index, bfile, heavy,
                psyco, noise, verbose):

    # Open the benchfile in append mode
    bf = open_file(bfile, "a")
    recsize = "small"
    if worst:
        table = bf.get_node("/" + recsize + "/create_worst")
    else:
        table = bf.get_node("/" + recsize + "/create_best")

    (rowsw, irows, rowsz, time1, time2, tcpu1, tcpu2, size1, size2) = \
        createFile(file, nrows, filters, index, heavy, noise, verbose)
    # Collect data
    table.row["nrows"] = rowsw
    table.row["irows"] = irows
    table.row["tfill"] = time1
    table.row["tidx"] = time2
    table.row["tcfill"] = tcpu1
    table.row["tcidx"] = tcpu2
    table.row["fsize"] = size1
    table.row["isize"] = size2
    table.row["psyco"] = psyco
    tapprows = round(time1, 3)
    cpuapprows = round(tcpu1, 3)
    tpercent = int(round(cpuapprows / tapprows, 2) * 100)
    print("Rows written:", rowsw, " Row size:", rowsz)
    print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
          (tapprows, cpuapprows, tpercent))
    rowsecf = rowsw / tapprows
    table.row["rowsecf"] = rowsecf
    # print "Write rows/sec: ", rowsecf
    print("Total file size:",
          round((size1 + size2) / (1024. * 1024.), 3), "MB", end=' ')
    print(", Write KB/s (pure data):", int(rowsw * rowsz / (tapprows * 1024)))
    # print "Write KB/s :", int((size1+size2) / ((time1+time2) * 1024))
    tidxrows = time2
    cpuidxrows = round(tcpu2, 3)
    tpercent = int(round(cpuidxrows / tidxrows, 2) * 100)
    print("Rows indexed:", irows, " (IMRows):", irows / float(10 ** 6))
    print("Time indexing rows: %s s (real) %s s (cpu)  %s%%" %
          (round(tidxrows, 3), cpuidxrows, tpercent))
    rowseci = irows / tidxrows
    table.row["rowseci"] = rowseci
    table.row.append()
    bf.close()


def readFile(filename, atom, riter, indexmode, dselect, verbose):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    table = fileh.root.table
    var1 = table.cols.var1
    var2 = table.cols.var2
    var3 = table.cols.var3
    if indexmode == "indexed":
        if var2.index.nelements > 0:
            where = table._whereIndexed
        else:
            warnings.warn(
                "Not indexed table or empty index. Defaulting to in-kernel "
                "selection")
            indexmode = "inkernel"
            where = table._whereInRange
    elif indexmode == "inkernel":
        where = table.where
    if verbose:
        print("Max rows in buf:", table.nrowsinbuf)
        print("Rows in", table._v_pathname, ":", table.nrows)
        print("Buffersize:", table.rowsize * table.nrowsinbuf)
        print("MaxTuples:", table.nrowsinbuf)
        if indexmode == "indexed":
            print("Chunk size:", var2.index.sorted.chunksize)
            print("Number of elements per slice:", var2.index.nelemslice)
            print("Slice number in", table._v_pathname, ":", var2.index.nrows)

    #table.nrowsinbuf = 10
    # print "nrowsinbuf-->", table.nrowsinbuf
    rowselected = 0
    time2 = 0.
    tcpu2 = 0.
    results = []
    print("Select mode:", indexmode, ". Selecting for type:", atom)
    # Initialize the random generator always with the same integer
    # in order to have reproductible results on each read iteration
    random.seed(19)
    numpy.random.seed(19)
    for i in range(riter):
        # The interval for look values at. This is aproximately equivalent to
        # the number of elements to select
        rnd = numpy.random.randint(table.nrows)
        cpu1 = time.clock()
        t1 = time.time()
        if atom == "string":
            val = str(rnd)[-4:]
            if indexmode in ["indexed", "inkernel"]:
                results = [p.nrow
                           for p in where('var1 == val')]
            else:
                results = [p.nrow for p in table
                           if p["var1"] == val]
        elif atom == "int":
            val = rnd + dselect
            if indexmode in ["indexed", "inkernel"]:
                results = [p.nrow
                           for p in where('(rnd <= var3) & (var3 < val)')]
            else:
                results = [p.nrow for p in table
                           if rnd <= p["var2"] < val]
        elif atom == "float":
            val = rnd + dselect
            if indexmode in ["indexed", "inkernel"]:
                t1 = time.time()
                results = [p.nrow
                           for p in where('(rnd <= var3) & (var3 < val)')]
            else:
                results = [p.nrow for p in table
                           if float(rnd) <= p["var3"] < float(val)]
        else:
            raise ValueError("Value for atom '%s' not supported." % atom)
        rowselected += len(results)
        # print "selected values-->", results
        if i == 0:
            # First iteration
            time1 = time.time() - t1
            tcpu1 = time.clock() - cpu1
        else:
            if indexmode == "indexed":
                # if indexed, wait until the 5th iteration (in order to
                # insure that the index is effectively cached) to take times
                if i >= 5:
                    time2 += time.time() - t1
                    tcpu2 += time.clock() - cpu1
            else:
                time2 += time.time() - t1
                tcpu2 += time.clock() - cpu1

    if riter > 1:
        if indexmode == "indexed" and riter >= 5:
            correction = 5
        else:
            correction = 1
        time2 = time2 / (riter - correction)
        tcpu2 = tcpu2 / (riter - correction)
    if verbose and 1:
        print("Values that fullfill the conditions:")
        print(results)

    #rowsread = table.nrows * riter
    rowsread = table.nrows
    rowsize = table.rowsize

    # Close the file
    fileh.close()

    return (rowsread, rowselected, rowsize, time1, time2, tcpu1, tcpu2)


def benchSearch(file, riter, indexmode, bfile, heavy, psyco, dselect, verbose):

    # Open the benchfile in append mode
    bf = open_file(bfile, "a")
    recsize = "small"
    if worst:
        tableparent = "/" + recsize + "/search_worst/" + indexmode + "/"
    else:
        tableparent = "/" + recsize + "/search_best/" + indexmode + "/"

    # Do the benchmarks
    if not heavy:
        #atomlist = ["string", "int", "float", "bool"]
        atomlist = ["string", "int", "float"]
    else:
        #atomlist = ["int", "float", "bool"]
        atomlist = ["int", "float"]
    for atom in atomlist:
        tablepath = tableparent + atom
        table = bf.get_node(tablepath)
        (rowsr, rowsel, rowssz, time1, time2, tcpu1, tcpu2) = \
            readFile(file, atom, riter, indexmode, dselect, verbose)
        row = table.row
        row["nrows"] = rowsr
        row["rowsel"] = rowsel
        treadrows = round(time1, 6)
        row["time1"] = time1
        treadrows2 = round(time2, 6)
        row["time2"] = time2
        cpureadrows = round(tcpu1, 6)
        row["tcpu1"] = tcpu1
        cpureadrows2 = round(tcpu2, 6)
        row["tcpu2"] = tcpu2
        row["psyco"] = psyco
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        if riter > 1:
            tpercent2 = int(round(cpureadrows2 / treadrows2, 2) * 100)
        else:
            tpercent2 = 0.
        tMrows = rowsr / (1000 * 1000.)
        sKrows = rowsel / 1000.
        if atom == "string":  # just to print once
            print("Rows read:", rowsr, "Mread:", round(tMrows, 6), "Mrows")
        print("Rows selected:", rowsel, "Ksel:", round(sKrows, 6), "Krows")
        print("Time selecting (1st time): %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        if riter > 1:
            print("Time selecting (cached): %s s (real) %s s (cpu)  %s%%" %
                  (treadrows2, cpureadrows2, tpercent2))
        #rowsec1 = round(rowsr / float(treadrows), 6)/10**6
        rowsec1 = rowsr / treadrows
        row["rowsec1"] = rowsec1
        print("Read Mrows/sec: ", end=' ')
        print(round(rowsec1 / 10. ** 6, 6), "(first time)", end=' ')
        if riter > 1:
            rowsec2 = rowsr / treadrows2
            row["rowsec2"] = rowsec2
            print(round(rowsec2 / 10. ** 6, 6), "(cache time)")
        else:
            print()
        # Append the info to the table
        row.append()
        table.flush()
    # Close the benchmark file
    bf.close()


if __name__ == "__main__":
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-R] [-r] [-w] [-c level] [-l complib] [-S] [-F] [-n nrows] [-x] [-b file] [-t] [-h] [-k riter] [-m indexmode] [-N range] [-d range] datafile
            -v verbose
            -p use "psyco" if available
            -R use Random values for filling
            -r only read test
            -w only write test
            -c sets a compression level (do not set it or 0 for no compression)
            -l sets the compression library ("zlib", "lzo", "ucl", "bzip2" or "none")
            -S activate shuffling filter
            -F activate fletcher32 filter
            -n set the number of rows in tables (in krows)
            -x don't make indexes
            -b bench filename
            -t worsT searching case
            -h heavy benchmark (operations without strings)
            -m index mode for reading ("indexed" | "inkernel" | "standard")
            -N introduce (uniform) noise within range into the values
            -d the interval for look values (int, float) at. Default is 3.
            -k number of iterations for reading\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(
            sys.argv[1:], 'vpSFRrowxthk:b:c:l:n:m:N:d:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    dselect = 3.
    noise = 0.
    verbose = 0
    fieldName = None
    testread = 1
    testwrite = 1
    usepsyco = 0
    complevel = 0
    shuffle = 0
    fletcher32 = 0
    complib = "zlib"
    nrows = 1000
    index = 1
    heavy = 0
    bfile = "bench.h5"
    supported_imodes = ["indexed", "inkernel", "standard"]
    indexmode = "inkernel"
    riter = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-R':
            randomvalues = 1
        if option[0] == '-S':
            shuffle = 1
        if option[0] == '-F':
            fletcher32 = 1
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-x':
            index = 0
        elif option[0] == '-h':
            heavy = 1
        elif option[0] == '-t':
            worst = 1
        elif option[0] == '-b':
            bfile = option[1]
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-m':
            indexmode = option[1]
            if indexmode not in supported_imodes:
                raise ValueError(
                    "Indexmode should be any of '%s' and you passed '%s'" %
                    (supported_imodes, indexmode))
        elif option[0] == '-n':
            nrows = int(float(option[1]) * 1000)
        elif option[0] == '-N':
            noise = float(option[1])
        elif option[0] == '-d':
            dselect = float(option[1])
        elif option[0] == '-k':
            riter = int(option[1])

    if worst:
        nrows -= 1  # the worst case

    if complib == "none":
        # This means no compression at all
        complib = "zlib"  # just to make PyTables not complaining
        complevel = 0

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    # Build the Filters instance
    filters = Filters(complevel=complevel, complib=complib,
                      shuffle=shuffle, fletcher32=fletcher32)

    # Create the benchfile (if needed)
    if not os.path.exists(bfile):
        createNewBenchFile(bfile, verbose)

    if testwrite:
        if verbose:
            print("Compression level:", complevel)
            if complevel > 0:
                print("Compression library:", complib)
                if shuffle:
                    print("Suffling...")
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        benchCreate(file, nrows, filters, index, bfile, heavy,
                    usepsyco, noise, verbose)
    if testread:
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
        benchSearch(file, riter, indexmode, bfile, heavy, usepsyco,
                    dselect, verbose)

########NEW FILE########
__FILENAME__ = searchsorted-bench
#!/usr/bin/env python

from __future__ import print_function
import time
from tables import *


class Small(IsDescription):
    var1 = StringCol(itemsize=4)
    var2 = Int32Col()
    var3 = Float64Col()
    var4 = BoolCol()

# Define a user record to characterize some kind of particles


class Medium(IsDescription):
    var1 = StringCol(itemsize=16)   # 16-character String
    #float1 = Float64Col(dflt=2.3)
    #float2 = Float64Col(dflt=2.3)
    # zADCcount    = Int16Col()      # signed short integer
    var2 = Int32Col()               # signed short integer
    var3 = Float64Col()
    grid_i = Int32Col()             # integer
    grid_j = Int32Col()             # integer
    pressure = Float32Col()         # float  (single-precision)
    energy = Float64Col(shape=2)    # double (double-precision)


def createFile(filename, nrows, filters, atom, recsize, index, verbose):

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="Searchsorted Benchmark",
                      filters=filters)
    title = "This is the IndexArray title"
    # Create an IndexArray instance
    rowswritten = 0
    # Create an entry
    klass = {"small": Small, "medium": Medium}
    table = fileh.create_table(fileh.root, 'table', klass[recsize], title,
                               None, nrows)
    for i in range(nrows):
        #table.row['var1'] = str(i)
        #table.row['var2'] = random.randrange(nrows)
        table.row['var2'] = i
        table.row['var3'] = i
        #table.row['var4'] = i % 2
        #table.row['var4'] = i > 2
        table.row.append()
    rowswritten += nrows
    table.flush()
    rowsize = table.rowsize
    indexrows = 0

    # Index one entry:
    if index:
        if atom == "string":
            indexrows = table.cols.var1.create_index()
        elif atom == "bool":
            indexrows = table.cols.var4.create_index()
        elif atom == "int":
            indexrows = table.cols.var2.create_index()
        elif atom == "float":
            indexrows = table.cols.var3.create_index()
        else:
            raise ValueError("Index type not supported yet")
        if verbose:
            print("Number of indexed rows:", indexrows)
    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowswritten, rowsize)


def readFile(filename, atom, niter, verbose):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    table = fileh.root.table
    print("reading", table)
    if atom == "string":
        idxcol = table.cols.var1.index
    elif atom == "bool":
        idxcol = table.cols.var4.index
    elif atom == "int":
        idxcol = table.cols.var2.index
    else:
        idxcol = table.cols.var3.index
    if verbose:
        print("Max rows in buf:", table.nrowsinbuf)
        print("Rows in", table._v_pathname, ":", table.nrows)
        print("Buffersize:", table.rowsize * table.nrowsinbuf)
        print("MaxTuples:", table.nrowsinbuf)
        print("Chunk size:", idxcol.sorted.chunksize)
        print("Number of elements per slice:", idxcol.nelemslice)
        print("Slice number in", table._v_pathname, ":", idxcol.nrows)

    rowselected = 0
    if atom == "string":
        for i in range(niter):
            #results = [table.row["var3"] for i in table.where(2+i<=table.cols.var2 < 10+i)]
            #results = [table.row.nrow() for i in table.where(2<=table.cols.var2 < 10)]
            results = [p["var1"]  # p.nrow()
                       for p in table.where(table.cols.var1 == "1111")]
#                      for p in table.where("1000"<=table.cols.var1<="1010")]
            rowselected += len(results)
    elif atom == "bool":
        for i in range(niter):
            results = [p["var2"]  # p.nrow()
                       for p in table.where(table.cols.var4 == 0)]
            rowselected += len(results)
    elif atom == "int":
        for i in range(niter):
            #results = [table.row["var3"] for i in table.where(2+i<=table.cols.var2 < 10+i)]
            #results = [table.row.nrow() for i in table.where(2<=table.cols.var2 < 10)]
            results = [p["var2"]  # p.nrow()
                       # for p in table.where(110*i<=table.cols.var2<110*(i+1))]
                       # for p in table.where(1000-30<table.cols.var2<1000+60)]
                       for p in table.where(table.cols.var2 <= 400)]
            rowselected += len(results)
    elif atom == "float":
        for i in range(niter):
#         results = [(table.row.nrow(), table.row["var3"])
#                    for i in table.where(3<=table.cols.var3 < 5.)]
#             results = [(p.nrow(), p["var3"])
# for p in table.where(1000.-i<=table.cols.var3<1000.+i)]
            results = [
                p["var3"]  # (p.nrow(), p["var3"])
                for p in table.where(
                    100 * i <= table.cols.var3 < 100 * (i + 1))
            ]
#                        for p in table
#                        if 100*i<=p["var3"]<100*(i+1)]
#             results = [ (p.nrow(), p["var3"]) for p in table
#                         if (1000.-i <= p["var3"] < 1000.+i) ]
            rowselected += len(results)
        else:
            raise ValueError("Unsuported atom value")
    if verbose and 1:
        print("Values that fullfill the conditions:")
        print(results)

    rowsread = table.nrows * niter
    rowsize = table.rowsize

    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowsread, rowselected, rowsize)


def searchFile(filename, atom, verbose, item):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    rowsread = 0
    uncomprBytes = 0
    table = fileh.root.table
    if atom == "int":
        idxcol = table.cols.var2.index
    elif atom == "float":
        idxcol = table.cols.var3.index
    else:
        raise ValueError("Unsuported atom value")
    print("Searching", table, "...")
    if verbose:
        print("Chunk size:", idxcol.sorted.chunksize)
        print("Number of elements per slice:", idxcol.sorted.nelemslice)
        print("Slice number in", table._v_pathname, ":", idxcol.sorted.nrows)

    (positions, niter) = idxcol.search(item)
    if verbose:
        print("Positions for item", item, "==>", positions)
        print("Total iterations in search:", niter)

    rowsread += table.nrows
    uncomprBytes += idxcol.sorted.chunksize * niter * idxcol.sorted.itemsize

    results = table.read(coords=positions)
    print("results length:", len(results))
    if verbose:
        print("Values that fullfill the conditions:")
        print(results)

    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowsread, uncomprBytes, niter)


if __name__ == "__main__":
    import sys
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-R range] [-r] [-w] [-s recsize ] [-a
    atom] [-c level] [-l complib] [-S] [-F] [-i item] [-n nrows] [-x]
    [-k niter] file
            -v verbose
            -p use "psyco" if available
            -R select a range in a field in the form "start,stop,step"
            -r only read test
            -w only write test
            -s record size
            -a use [float], [int], [bool] or [string] atom
            -c sets a compression level (do not set it or 0 for no compression)
            -S activate shuffling filter
            -F activate fletcher32 filter
            -l sets the compression library to be used ("zlib", "lzo", "ucl", "bzip2")
            -i item to search
            -n set the number of rows in tables
            -x don't make indexes
            -k number of iterations for reading\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpSFR:rwxk:s:a:c:l:i:n:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    rng = None
    item = None
    atom = "int"
    fieldName = None
    testread = 1
    testwrite = 1
    usepsyco = 0
    complevel = 0
    shuffle = 0
    fletcher32 = 0
    complib = "zlib"
    nrows = 100
    recsize = "small"
    index = 1
    niter = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-S':
            shuffle = 1
        if option[0] == '-F':
            fletcher32 = 1
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-x':
            index = 0
        elif option[0] == '-s':
            recsize = option[1]
        elif option[0] == '-a':
            atom = option[1]
            if atom not in ["float", "int", "bool", "string"]:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-i':
            item = eval(option[1])
        elif option[0] == '-n':
            nrows = int(option[1])
        elif option[0] == '-k':
            niter = int(option[1])

    # Build the Filters instance
    filters = Filters(complevel=complevel, complib=complib,
                      shuffle=shuffle, fletcher32=fletcher32)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    if testwrite:
        print("Compression level:", complevel)
        if complevel > 0:
            print("Compression library:", complib)
            if shuffle:
                print("Suffling...")
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        (rowsw, rowsz) = createFile(file, nrows, filters,
                                    atom, recsize, index, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
            psyco.bind(searchFile)
        t1 = time.time()
        cpu1 = time.clock()
        if rng or item:
            (rowsr, uncomprB, niter) = searchFile(file, atom, verbose, item)
        else:
            for i in range(1):
                (rowsr, rowsel, rowsz) = readFile(file, atom, niter, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        tMrows = rowsr / (1000 * 1000.)
        sKrows = rowsel / 1000.
        print("Rows read:", rowsr, "Mread:", round(tMrows, 3), "Mrows")
        print("Rows selected:", rowsel, "Ksel:", round(sKrows, 3), "Krows")
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read Mrows/sec: ", round(tMrows / float(treadrows), 3))
        # print "Read KB/s :", int(rowsr * rowsz / (treadrows * 1024))
#       print "Uncompr MB :", int(uncomprB / (1024 * 1024))
#       print "Uncompr MB/s :", int(uncomprB / (treadrows * 1024 * 1024))
#       print "Total chunks uncompr :", int(niter)

########NEW FILE########
__FILENAME__ = searchsorted-bench2
#!/usr/bin/env python

from __future__ import print_function
import time
from tables import *


class Small(IsDescription):
    var1 = StringCol(itemsize=4)
    var2 = Int32Col()
    var3 = Float64Col()
    var4 = BoolCol()

# Define a user record to characterize some kind of particles


class Medium(IsDescription):
    var1 = StringCol(itemsize=16, dflt="")  # 16-character String
    #float1 = Float64Col(dflt=2.3)
    #float2 = Float64Col(dflt=2.3)
    # zADCcount = Int16Col()          # signed short integer
    var2 = Int32Col()               # signed short integer
    var3 = Float64Col()
    grid_i = Int32Col()             # integer
    grid_j = Int32Col()             # integer
    pressure = Float32Col()         # float  (single-precision)
    energy = Float64Col(shape=2)    # double (double-precision)


def createFile(filename, nrows, filters, atom, recsize, index, verbose):

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="Searchsorted Benchmark",
                      filters=filters)
    title = "This is the IndexArray title"
    # Create an IndexArray instance
    rowswritten = 0
    # Create an entry
    klass = {"small": Small, "medium": Medium}
    table = fileh.create_table(fileh.root, 'table', klass[recsize], title,
                               None, nrows)
    for i in range(nrows):
        #table.row['var1'] = str(i)
        #table.row['var2'] = random.randrange(nrows)
        table.row['var2'] = i
        table.row['var3'] = i
        #table.row['var4'] = i % 2
        table.row['var4'] = i > 2
        table.row.append()
    rowswritten += nrows
    table.flush()
    rowsize = table.rowsize
    indexrows = 0

    # Index one entry:
    if index:
        if atom == "string":
            indexrows = table.cols.var1.create_index()
        elif atom == "bool":
            indexrows = table.cols.var4.create_index()
        elif atom == "int":
            indexrows = table.cols.var2.create_index()
        elif atom == "float":
            indexrows = table.cols.var3.create_index()
        else:
            raise ValueError("Index type not supported yet")
        if verbose:
            print("Number of indexed rows:", indexrows)
    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowswritten, rowsize)


def readFile(filename, atom, niter, verbose):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    table = fileh.root.table
    print("reading", table)
    if atom == "string":
        idxcol = table.cols.var1.index
    elif atom == "bool":
        idxcol = table.cols.var4.index
    elif atom == "int":
        idxcol = table.cols.var2.index
    else:
        idxcol = table.cols.var3.index
    if verbose:
        print("Max rows in buf:", table.nrowsinbuf)
        print("Rows in", table._v_pathname, ":", table.nrows)
        print("Buffersize:", table.rowsize * table.nrowsinbuf)
        print("MaxTuples:", table.nrowsinbuf)
        print("Chunk size:", idxcol.sorted.chunksize)
        print("Number of elements per slice:", idxcol.nelemslice)
        print("Slice number in", table._v_pathname, ":", idxcol.nrows)

    rowselected = 0
    if atom == "string":
        for i in range(niter):
            #results = [table.row["var3"] for i in table(where=2+i<=table.cols.var2 < 10+i)]
            #results = [table.row.nrow() for i in table(where=2<=table.cols.var2 < 10)]
            results = [p["var1"]  # p.nrow()
                       for p in table(where=table.cols.var1 == "1111")]
#                      for p in table(where="1000"<=table.cols.var1<="1010")]
            rowselected += len(results)
    elif atom == "bool":
        for i in range(niter):
            results = [p["var2"]  # p.nrow()
                       for p in table(where=table.cols.var4 == 0)]
            rowselected += len(results)
    elif atom == "int":
        for i in range(niter):
            #results = [table.row["var3"] for i in table(where=2+i<=table.cols.var2 < 10+i)]
            #results = [table.row.nrow() for i in table(where=2<=table.cols.var2 < 10)]
            results = [p["var2"]  # p.nrow()
                       #                        for p in table(where=110*i<=table.cols.var2<110*(i+1))]
                       # for p in table(where=1000-30<table.cols.var2<1000+60)]
                       for p in table(where=table.cols.var2 <= 400)]
            rowselected += len(results)
    elif atom == "float":
        for i in range(niter):
#         results = [(table.row.nrow(), table.row["var3"])
#                    for i in table(where=3<=table.cols.var3 < 5.)]
#             results = [(p.nrow(), p["var3"])
# for p in table(where=1000.-i<=table.cols.var3<1000.+i)]
            results = [
                p["var3"]  # (p.nrow(), p["var3"])
                for p in table(
                    where=100 * i <= table.cols.var3 < 100 * (i + 1))
            ]
#                        for p in table
#                        if 100*i<=p["var3"]<100*(i+1)]
#             results = [ (p.nrow(), p["var3"]) for p in table
#                         if (1000.-i <= p["var3"] < 1000.+i) ]
            rowselected += len(results)
        else:
            raise ValueError("Unsuported atom value")
    if verbose and 1:
        print("Values that fullfill the conditions:")
        print(results)

    rowsread = table.nrows * niter
    rowsize = table.rowsize

    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowsread, rowselected, rowsize)


def searchFile(filename, atom, verbose, item):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    rowsread = 0
    uncomprBytes = 0
    table = fileh.root.table
    if atom == "int":
        idxcol = table.cols.var2.index
    elif atom == "float":
        idxcol = table.cols.var3.index
    else:
        raise ValueError("Unsuported atom value")
    print("Searching", table, "...")
    if verbose:
        print("Chunk size:", idxcol.sorted.chunksize)
        print("Number of elements per slice:", idxcol.sorted.nelemslice)
        print("Slice number in", table._v_pathname, ":", idxcol.sorted.nrows)

    (positions, niter) = idxcol.search(item)
    if verbose:
        print("Positions for item", item, "==>", positions)
        print("Total iterations in search:", niter)

    rowsread += table.nrows
    uncomprBytes += idxcol.sorted.chunksize * niter * idxcol.sorted.itemsize

    results = table.read(coords=positions)
    print("results length:", len(results))
    if verbose:
        print("Values that fullfill the conditions:")
        print(results)

    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowsread, uncomprBytes, niter)


if __name__ == "__main__":
    import sys
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-R range] [-r] [-w] [-s recsize ] [-a
    atom] [-c level] [-l complib] [-S] [-F] [-i item] [-n nrows] [-x]
    [-k niter] file
            -v verbose
            -p use "psyco" if available
            -R select a range in a field in the form "start,stop,step"
            -r only read test
            -w only write test
            -s record size
            -a use [float], [int], [bool] or [string] atom
            -c sets a compression level (do not set it or 0 for no compression)
            -S activate shuffling filter
            -F activate fletcher32 filter
            -l sets the compression library to be used ("zlib", "lzo", "ucl", "bzip2")
            -i item to search
            -n set the number of rows in tables
            -x don't make indexes
            -k number of iterations for reading\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpSFR:rwxk:s:a:c:l:i:n:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    rng = None
    item = None
    atom = "int"
    fieldName = None
    testread = 1
    testwrite = 1
    usepsyco = 0
    complevel = 0
    shuffle = 0
    fletcher32 = 0
    complib = "zlib"
    nrows = 100
    recsize = "small"
    index = 1
    niter = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-S':
            shuffle = 1
        if option[0] == '-F':
            fletcher32 = 1
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-x':
            index = 0
        elif option[0] == '-s':
            recsize = option[1]
        elif option[0] == '-a':
            atom = option[1]
            if atom not in ["float", "int", "bool", "string"]:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-i':
            item = eval(option[1])
        elif option[0] == '-n':
            nrows = int(option[1])
        elif option[0] == '-k':
            niter = int(option[1])

    # Build the Filters instance
    filters = Filters(complevel=complevel, complib=complib,
                      shuffle=shuffle, fletcher32=fletcher32)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    if testwrite:
        print("Compression level:", complevel)
        if complevel > 0:
            print("Compression library:", complib)
            if shuffle:
                print("Suffling...")
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        (rowsw, rowsz) = createFile(file, nrows, filters,
                                    atom, recsize, index, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
            psyco.bind(searchFile)
        t1 = time.time()
        cpu1 = time.clock()
        if rng or item:
            (rowsr, uncomprB, niter) = searchFile(file, atom, verbose, item)
        else:
            for i in range(1):
                (rowsr, rowsel, rowsz) = readFile(file, atom, niter, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        tMrows = rowsr / (1000 * 1000.)
        sKrows = rowsel / 1000.
        print("Rows read:", rowsr, "Mread:", round(tMrows, 3), "Mrows")
        print("Rows selected:", rowsel, "Ksel:", round(sKrows, 3), "Krows")
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read Mrows/sec: ", round(tMrows / float(treadrows), 3))
        # print "Read KB/s :", int(rowsr * rowsz / (treadrows * 1024))
#       print "Uncompr MB :", int(uncomprB / (1024 * 1024))
#       print "Uncompr MB/s :", int(uncomprB / (treadrows * 1024 * 1024))
#       print "Total chunks uncompr :", int(niter)

########NEW FILE########
__FILENAME__ = shelve-bench
#!/usr/bin/env python

from __future__ import print_function
from tables import *
import numpy as NA
import struct
import sys
import shelve
import psyco

# This class is accessible only for the examples


class Small(IsDescription):

    """Record descriptor.

    A record has several columns. They are represented here as class
    attributes, whose names are the column names and their values will
    become their types. The IsDescription class will take care the user
    will not add any new variables and that its type is correct.

    """

    var1 = StringCol(itemsize=4)
    var2 = Int32Col()
    var3 = Float64Col()

# Define a user record to characterize some kind of particles


class Medium(IsDescription):
    name = StringCol(itemsize=16)   # 16-character String
    float1 = Float64Col(shape=2, dflt=2.3)
    #float1 = Float64Col(dflt=1.3)
    #float2 = Float64Col(dflt=2.3)
    ADCcount = Int16Col()           # signed short integer
    grid_i = Int32Col()             # integer
    grid_j = Int32Col()             # integer
    pressure = Float32Col()         # float  (single-precision)
    energy = Float64Col()           # double (double-precision)

# Define a user record to characterize some kind of particles


class Big(IsDescription):
    name = StringCol(itemsize=16)   # 16-character String
    #float1 = Float64Col(shape=32, dflt=NA.arange(32))
    #float2 = Float64Col(shape=32, dflt=NA.arange(32))
    float1 = Float64Col(shape=32, dflt=range(32))
    float2 = Float64Col(shape=32, dflt=[2.2] * 32)
    ADCcount = Int16Col()           # signed short integer
    grid_i = Int32Col()             # integer
    grid_j = Int32Col()             # integer
    pressure = Float32Col()         # float  (single-precision)
    energy = Float64Col()           # double (double-precision)


def createFile(filename, totalrows, recsize):

    # Open a 'n'ew file
    fileh = shelve.open(filename, flag="n")

    rowswritten = 0
    # Get the record object associated with the new table
    if recsize == "big":
        d = Big()
        arr = NA.array(NA.arange(32), type=NA.Float64)
        arr2 = NA.array(NA.arange(32), type=NA.Float64)
    elif recsize == "medium":
        d = Medium()
    else:
        d = Small()
    # print d
    # sys.exit(0)
    for j in range(3):
        # Create a table
        # table = fileh.create_table(group, 'tuple'+str(j), Record(), title,
        #                          compress = 6, expectedrows = totalrows)
        # Create a Table instance
        tablename = 'tuple' + str(j)
        table = []
        # Fill the table
        if recsize == "big" or recsize == "medium":
            for i in range(totalrows):
                d.name = 'Particle: %6d' % (i)
                #d.TDCcount = i % 256
                d.ADCcount = (i * 256) % (1 << 16)
                if recsize == "big":
                    #d.float1 = NA.array([i]*32, NA.Float64)
                    #d.float2 = NA.array([i**2]*32, NA.Float64)
                    arr[0] = 1.1
                    d.float1 = arr
                    arr2[0] = 2.2
                    d.float2 = arr2
                    pass
                else:
                    d.float1 = NA.array([i ** 2] * 2, NA.Float64)
                    #d.float1 = float(i)
                    #d.float2 = float(i)
                d.grid_i = i
                d.grid_j = 10 - i
                d.pressure = float(i * i)
                d.energy = float(d.pressure ** 4)
                table.append((d.ADCcount, d.energy, d.float1, d.float2,
                              d.grid_i, d.grid_j, d.name, d.pressure))
                # Only on float case
                # table.append((d.ADCcount, d.energy, d.float1,
                #              d.grid_i, d.grid_j, d.name, d.pressure))
        else:
            for i in range(totalrows):
                d.var1 = str(i)
                d.var2 = i
                d.var3 = 12.1e10
                table.append((d.var1, d.var2, d.var3))

        # Save this table on disk
        fileh[tablename] = table
        rowswritten += totalrows

    # Close the file
    fileh.close()
    return (rowswritten, struct.calcsize(d._v_fmt))


def readFile(filename, recsize):
    # Open the HDF5 file in read-only mode
    fileh = shelve.open(filename, "r")
    for table in ['tuple0', 'tuple1', 'tuple2']:
        if recsize == "big" or recsize == "medium":
            e = [t[2] for t in fileh[table] if t[4] < 20]
            # if there is only one float (array)
            #e = [ t[1] for t in fileh[table] if t[3] < 20 ]
        else:
            e = [t[1] for t in fileh[table] if t[1] < 20]

        print("resulting selection list ==>", e)
        print("Total selected records ==> ", len(e))

    # Close the file (eventually destroy the extended type)
    fileh.close()


# Add code to test here
if __name__ == "__main__":
    import getopt
    import time

    usage = """usage: %s [-f] [-s recsize] [-i iterations] file
            -s use [big] record, [medium] or [small]
            -i sets the number of rows in each table\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 's:fi:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    recsize = "medium"
    iterations = 100

    # Get the options
    for option in opts:
        if option[0] == '-s':
            recsize = option[1]
            if recsize not in ["big", "medium", "small"]:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-i':
            iterations = int(option[1])

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    t1 = time.clock()
    psyco.bind(createFile)
    (rowsw, rowsz) = createFile(file, iterations, recsize)
    t2 = time.clock()
    tapprows = round(t2 - t1, 3)

    t1 = time.clock()
    psyco.bind(readFile)
    readFile(file, recsize)
    t2 = time.clock()
    treadrows = round(t2 - t1, 3)

    print("Rows written:", rowsw, " Row size:", rowsz)
    print("Time appending rows:", tapprows)
    print("Write rows/sec: ", int(iterations * 3 / float(tapprows)))
    print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))
    print("Time reading rows:", treadrows)
    print("Read rows/sec: ", int(iterations * 3 / float(treadrows)))
    print("Read KB/s :", int(rowsw * rowsz / (treadrows * 1024)))

########NEW FILE########
__FILENAME__ = split-file
"""
Split out a monolithic file with many different runs of
indexed_search.py. The resulting files are meant for use in
get-figures.py.

Usage: python split-file.py prefix filename
"""

import sys

prefix = sys.argv[1]
filename = sys.argv[2]
f = open(filename)
sf = None
for line in f:
    if line.startswith('Processing database:'):
        if sf:
            sf.close()
        line2 = line.split(':')[1]
        # Check if entry is compressed and if has to be processed
        line2 = line2[:line2.rfind('.')]
        params = line2.split('-')
        optlevel = 0
        complib = None
        for param in params:
            if param[0] == 'O' and param[1].isdigit():
                optlevel = int(param[1])
            elif param[:-1] in ('zlib', 'lzo'):
                complib = param
        if 'PyTables' in prefix:
            if complib:
                sfilename = "%s-O%s-%s.out" % (prefix, optlevel, complib)
            else:
                sfilename = "%s-O%s.out" % (prefix, optlevel,)
        else:
            sfilename = "%s.out" % (prefix,)
        sf = file(sfilename, 'a')
    if sf:
        sf.write(line)
f.close()

########NEW FILE########
__FILENAME__ = sqlite-search-bench
#!/usr/bin/python

from __future__ import print_function
import sqlite
import random
import time
import sys
import os
import os.path
from tables import *
import numpy as np

randomvalues = 0
standarddeviation = 10000
# Initialize the random generator always with the same integer
# in order to have reproductible results
random.seed(19)
np.random.seed((19, 20))

# defaults
psycon = 0
worst = 0


def createNewBenchFile(bfile, verbose):

    class Create(IsDescription):
        nrows = Int32Col(pos=0)
        irows = Int32Col(pos=1)
        tfill = Float64Col(pos=2)
        tidx = Float64Col(pos=3)
        tcfill = Float64Col(pos=4)
        tcidx = Float64Col(pos=5)
        rowsecf = Float64Col(pos=6)
        rowseci = Float64Col(pos=7)
        fsize = Float64Col(pos=8)
        isize = Float64Col(pos=9)
        psyco = BoolCol(pos=10)

    class Search(IsDescription):
        nrows = Int32Col(pos=0)
        rowsel = Int32Col(pos=1)
        time1 = Float64Col(pos=2)
        time2 = Float64Col(pos=3)
        tcpu1 = Float64Col(pos=4)
        tcpu2 = Float64Col(pos=5)
        rowsec1 = Float64Col(pos=6)
        rowsec2 = Float64Col(pos=7)
        psyco = BoolCol(pos=8)

    if verbose:
        print("Creating a new benchfile:", bfile)
    # Open the benchmarking file
    bf = open_file(bfile, "w")
    # Create groups
    for recsize in ["sqlite_small"]:
        group = bf.create_group("/", recsize, recsize + " Group")
        # Attach the row size of table as attribute
        if recsize == "small":
            group._v_attrs.rowsize = 16
        # Create a Table for writing bench
        bf.create_table(group, "create_indexed", Create, "indexed values")
        bf.create_table(group, "create_standard", Create, "standard values")
        # create a group for searching bench
        groupS = bf.create_group(group, "search", "Search Group")
        # Create Tables for searching
        for mode in ["indexed", "standard"]:
            group = bf.create_group(groupS, mode, mode + " Group")
            # for searching bench
            # for atom in ["string", "int", "float", "bool"]:
            for atom in ["string", "int", "float"]:
                bf.create_table(group, atom, Search, atom + " bench")
    bf.close()


def createFile(filename, nrows, filters, indexmode, heavy, noise, bfile,
               verbose):

    # Initialize some variables
    t1 = 0.
    t2 = 0.
    tcpu1 = 0.
    tcpu2 = 0.
    rowsecf = 0.
    rowseci = 0.
    size1 = 0.
    size2 = 0.

    if indexmode == "standard":
        print("Creating a new database:", dbfile)
        instd = os.popen("/usr/local/bin/sqlite " + dbfile, "w")
        CREATESTD = """
CREATE TABLE small (
-- Name         Type            -- Example
---------------------------------------
recnum  INTEGER PRIMARY KEY,  -- 345
var1            char(4),        -- Abronia villosa
var2            INTEGER,        -- 111
var3            FLOAT        --  12.32
);
"""
        CREATEIDX = """
CREATE TABLE small (
-- Name         Type            -- Example
---------------------------------------
recnum  INTEGER PRIMARY KEY,  -- 345
var1            char(4),        -- Abronia villosa
var2            INTEGER,        -- 111
var3            FLOAT        --  12.32
);
CREATE INDEX ivar1 ON small(var1);
CREATE INDEX ivar2 ON small(var2);
CREATE INDEX ivar3 ON small(var3);
"""
        # Creating the table first and indexing afterwards is a bit faster
        instd.write(CREATESTD)
        instd.close()

    conn = sqlite.connect(dbfile)
    cursor = conn.cursor()
    if indexmode == "standard":
        place_holders = ",".join(['%s'] * 3)
        # Insert rows
        SQL = "insert into small values(NULL, %s)" % place_holders
        time1 = time.time()
        cpu1 = time.clock()
        # This way of filling is to copy the PyTables benchmark
        nrowsbuf = 1000
        minimum = 0
        maximum = nrows
        for i in range(0, nrows, nrowsbuf):
            if i + nrowsbuf > nrows:
                j = nrows
            else:
                j = i + nrowsbuf
            if randomvalues:
                var3 = np.random.uniform(minimum, maximum, shape=[j - i])
            else:
                var3 = np.arange(i, j, type=np.Float64)
                if noise:
                    var3 += np.random.uniform(-3, 3, shape=[j - i])
            var2 = np.array(var3, type=np.Int32)
            var1 = np.array(None, shape=[j - i], dtype='s4')
            if not heavy:
                for n in range(j - i):
                    var1[n] = str("%.4s" % var2[n])
            for n in range(j - i):
                fields = (var1[n], var2[n], var3[n])
                cursor.execute(SQL, fields)
            conn.commit()
        t1 = round(time.time() - time1, 5)
        tcpu1 = round(time.clock() - cpu1, 5)
        rowsecf = nrows / t1
        size1 = os.stat(dbfile)[6]
        print("******** Results for writing nrows = %s" % (nrows), "*********")
        print(("Insert time:", t1, ", KRows/s:",
              round((nrows / 10. ** 3) / t1, 3),))
        print(", File size:", round(size1 / (1024. * 1024.), 3), "MB")

    # Indexem
    if indexmode == "indexed":
        time1 = time.time()
        cpu1 = time.clock()
        if not heavy:
            cursor.execute("CREATE INDEX ivar1 ON small(var1)")
            conn.commit()
        cursor.execute("CREATE INDEX ivar2 ON small(var2)")
        conn.commit()
        cursor.execute("CREATE INDEX ivar3 ON small(var3)")
        conn.commit()
        t2 = round(time.time() - time1, 5)
        tcpu2 = round(time.clock() - cpu1, 5)
        rowseci = nrows / t2
        print(("Index time:", t2, ", IKRows/s:",
              round((nrows / 10. ** 3) / t2, 3),))
        size2 = os.stat(dbfile)[6] - size1
        print((", Final size with index:",
              round(size2 / (1024. * 1024), 3), "MB"))

    conn.close()

    # Collect benchmark data
    bf = open_file(bfile, "a")
    recsize = "sqlite_small"
    if indexmode == "indexed":
        table = bf.get_node("/" + recsize + "/create_indexed")
    else:
        table = bf.get_node("/" + recsize + "/create_standard")
    table.row["nrows"] = nrows
    table.row["irows"] = nrows
    table.row["tfill"] = t1
    table.row["tidx"] = t2
    table.row["tcfill"] = tcpu1
    table.row["tcidx"] = tcpu2
    table.row["psyco"] = psycon
    table.row["rowsecf"] = rowsecf
    table.row["rowseci"] = rowseci
    table.row["fsize"] = size1
    table.row["isize"] = size2
    table.row.append()
    bf.close()

    return


def readFile(dbfile, nrows, indexmode, heavy, dselect, bfile, riter):
    # Connect to the database.
    conn = sqlite.connect(db=dbfile, mode=755)
    # Obtain a cursor
    cursor = conn.cursor()

    #      select count(*), avg(var2)
    SQL1 = """
    select recnum
    from small where var1 = %s
    """
    SQL2 = """
    select recnum
    from small where var2 >= %s and var2 < %s
    """
    SQL3 = """
    select recnum
    from small where var3 >= %s and var3 < %s
    """

    # Open the benchmark database
    bf = open_file(bfile, "a")
    # default values for the case that columns are not indexed
    t2 = 0.
    tcpu2 = 0.
    # Some previous computations for the case of random values
    if randomvalues:
        # algorithm to choose a value separated from mean
# If want to select fewer values, select this
#         if nrows/2 > standarddeviation*3:
# Choose five standard deviations away from mean value
#             dev = standarddeviation*5
# dev = standarddeviation*math.log10(nrows/1000.)

        # This algorithm give place to too asymmetric result values
#         if standarddeviation*10 < nrows/2:
# Choose four standard deviations away from mean value
#             dev = standarddeviation*4
#         else:
#             dev = 100
        # Yet Another Algorithm
        if nrows / 2 > standarddeviation * 10:
            dev = standarddeviation * 4.
        elif nrows / 2 > standarddeviation:
            dev = standarddeviation * 2.
        elif nrows / 2 > standarddeviation / 10.:
            dev = standarddeviation / 10.
        else:
            dev = standarddeviation / 100.

        valmax = int(round((nrows / 2.) - dev))
        # split the selection range in regular chunks
        if riter > valmax * 2:
            riter = valmax * 2
        chunksize = (valmax * 2 / riter) * 10
        # Get a list of integers for the intervals
        randlist = range(0, valmax, chunksize)
        randlist.extend(range(nrows - valmax, nrows, chunksize))
        # expand the list ten times so as to use the cache
        randlist = randlist * 10
        # shuffle the list
        random.shuffle(randlist)
        # reset the value of chunksize
        chunksize = chunksize / 10
        # print "chunksize-->", chunksize
        # randlist.sort();print "randlist-->", randlist
    else:
        chunksize = 3
    if heavy:
        searchmodelist = ["int", "float"]
    else:
        searchmodelist = ["string", "int", "float"]

    # Execute queries
    for atom in searchmodelist:
        time2 = 0
        cpu2 = 0
        rowsel = 0
        for i in range(riter):
            rnd = random.randrange(nrows)
            time1 = time.time()
            cpu1 = time.clock()
            if atom == "string":
                #cursor.execute(SQL1, "1111")
                cursor.execute(SQL1, str(rnd)[-4:])
            elif atom == "int":
                #cursor.execute(SQL2 % (rnd, rnd+3))
                cursor.execute(SQL2 % (rnd, rnd + dselect))
            elif atom == "float":
                #cursor.execute(SQL3 % (float(rnd), float(rnd+3)))
                cursor.execute(SQL3 % (float(rnd), float(rnd + dselect)))
            else:
                raise ValueError(
                    "atom must take a value in ['string','int','float']")
            if i == 0:
                t1 = time.time() - time1
                tcpu1 = time.clock() - cpu1
            else:
                if indexmode == "indexed":
                    # if indexed, wait until the 5th iteration to take
                    # times (so as to insure that the index is
                    # effectively cached)
                    if i >= 5:
                        time2 += time.time() - time1
                        cpu2 += time.clock() - cpu1
                else:
                    time2 += time.time() - time1
                    time2 += time.clock() - cpu1
        if riter > 1:
            if indexmode == "indexed" and riter >= 5:
                correction = 5
            else:
                correction = 1
            t2 = time2 / (riter - correction)
            tcpu2 = cpu2 / (riter - correction)

        print(("*** Query results for atom = %s, nrows = %s, "
              "indexmode = %s ***" % (atom, nrows, indexmode)))
        print("Query time:", round(t1, 5), ", cached time:", round(t2, 5))
        print("MRows/s:", round((nrows / 10. ** 6) / t1, 3), end=' ')
        if t2 > 0:
            print(", cached MRows/s:", round((nrows / 10. ** 6) / t2, 3))
        else:
            print()

        # Collect benchmark data
        recsize = "sqlite_small"
        tablepath = "/" + recsize + "/search/" + indexmode + "/" + atom
        table = bf.get_node(tablepath)
        table.row["nrows"] = nrows
        table.row["rowsel"] = rowsel
        table.row["time1"] = t1
        table.row["time2"] = t2
        table.row["tcpu1"] = tcpu1
        table.row["tcpu2"] = tcpu2
        table.row["psyco"] = psycon
        table.row["rowsec1"] = nrows / t1
        if t2 > 0:
            table.row["rowsec2"] = nrows / t2
        table.row.append()
        table.flush()  # Flush the data

    # Close the database
    conn.close()
    bf.close()  # the bench database

    return

if __name__ == "__main__":
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-R] [-h] [-t] [-r] [-w] [-n nrows] [-b file] [-k riter] [-m indexmode] [-N range] datafile
            -v verbose
            -p use "psyco" if available
            -R use Random values for filling
            -h heavy mode (exclude strings from timings)
            -t worsT searching case (to emulate PyTables worst cases)
            -r only read test
            -w only write test
            -n the number of rows (in krows)
            -b bench filename
            -N introduce (uniform) noise within range into the values
            -d the interval for look values (int, float) at. Default is 3.
            -k number of iterations for reading\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpRhtrwn:b:k:m:N:d:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    dselect = 3.
    noise = 0.
    verbose = 0
    heavy = 0
    testread = 1
    testwrite = 1
    usepsyco = 0
    nrows = 1000
    bfile = "sqlite-bench.h5"
    supported_imodes = ["indexed", "standard"]
    indexmode = "indexed"
    riter = 2

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        if option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-R':
            randomvalues = 1
        elif option[0] == '-h':
            heavy = 1
        elif option[0] == '-t':
            worst = 1
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-b':
            bfile = option[1]
        elif option[0] == '-N':
            noise = float(option[1])
        elif option[0] == '-m':
            indexmode = option[1]
            if indexmode not in supported_imodes:
                raise ValueError(
                    "Indexmode should be any of '%s' and you passed '%s'" %
                    (supported_imodes, indexmode))
        elif option[0] == '-n':
            nrows = int(float(option[1]) * 1000)
        elif option[0] == '-d':
            dselect = float(option[1])
        elif option[0] == '-k':
            riter = int(option[1])

    # remaining parameters
    dbfile = pargs[0]

    if worst:
        nrows -= 1  # the worst case

    # Create the benchfile (if needed)
    if not os.path.exists(bfile):
        createNewBenchFile(bfile, verbose)

    if testwrite:
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
            psycon = 1
        createFile(dbfile, nrows, None, indexmode, heavy, noise, bfile,
                   verbose)

    if testread:
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
            psycon = 1
        readFile(dbfile, nrows, indexmode, heavy, dselect, bfile, riter)

########NEW FILE########
__FILENAME__ = sqlite3-search-bench
from __future__ import print_function
import os
import os.path
from time import time
import numpy
import random

# in order to always generate the same random sequence
random.seed(19)


def fill_arrays(start, stop):
    col_i = numpy.arange(start, stop, dtype=numpy.int32)
    if userandom:
        col_j = numpy.random.uniform(0, nrows, stop - start)
    else:
        col_j = numpy.array(col_i, dtype=numpy.float64)
    return col_i, col_j

# Generator for ensure pytables benchmark compatibility


def int_generator(nrows):
    step = 1000 * 100
    j = 0
    for i in range(nrows):
        if i >= step * j:
            stop = (j + 1) * step
            if stop > nrows:  # Seems unnecessary
                stop = nrows
            col_i, col_j = fill_arrays(i, stop)
            j += 1
            k = 0
        yield (col_i[k], col_j[k])
        k += 1


def int_generator_slow(nrows):
    for i in range(nrows):
        if userandom:
            yield (i, float(random.randint(0, nrows)))
        else:
            yield (i, float(i))


def open_db(filename, remove=0):
    if remove and os.path.exists(filename):
        os.remove(filename)
    con = sqlite.connect(filename)
    cur = con.cursor()
    return con, cur


def create_db(filename, nrows):
    con, cur = open_db(filename, remove=1)
    cur.execute("create table ints(i integer, j real)")
    t1 = time()
    # This is twice as fast as a plain loop
    cur.executemany("insert into ints(i,j) values (?,?)", int_generator(nrows))
    con.commit()
    ctime = time() - t1
    if verbose:
        print("insert time:", round(ctime, 5))
        print("Krows/s:", round((nrows / 1000.) / ctime, 5))
    close_db(con, cur)


def index_db(filename):
    con, cur = open_db(filename)
    t1 = time()
    cur.execute("create index ij on ints(j)")
    con.commit()
    itime = time() - t1
    if verbose:
        print("index time:", round(itime, 5))
        print("Krows/s:", round(nrows / itime, 5))
    # Close the DB
    close_db(con, cur)


def query_db(filename, rng):
    con, cur = open_db(filename)
    t1 = time()
    ntimes = 10
    for i in range(ntimes):
        # between clause does not seem to take advantage of indexes
        # cur.execute("select j from ints where j between %s and %s" % \
        cur.execute("select i from ints where j >= %s and j <= %s" %
                    # cur.execute("select i from ints where i >= %s and i <=
                    # %s" %
                    (rng[0] + i, rng[1] + i))
        results = cur.fetchall()
    con.commit()
    qtime = (time() - t1) / ntimes
    if verbose:
        print("query time:", round(qtime, 5))
        print("Mrows/s:", round((nrows / 1000.) / qtime, 5))
        print(results)
    close_db(con, cur)


def close_db(con, cur):
    cur.close()
    con.close()

if __name__ == "__main__":
    import sys
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-v] [-p] [-m] [-i] [-q] [-c] [-R range] [-n nrows] file
            -v verbose
            -p use "psyco" if available
            -m use random values to fill the table
            -q do query
            -c create the database
            -i index the table
            -2 use sqlite2 (default is use sqlite3)
            -R select a range in a field in the form "start,stop" (def "0,10")
            -n sets the number of rows (in krows) in each table
            \n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpmiqc2R:n:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    usepsyco = 0
    userandom = 0
    docreate = 0
    createindex = 0
    doquery = 0
    sqlite_version = "3"
    rng = [0, 10]
    nrows = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-m':
            userandom = 1
        elif option[0] == '-i':
            createindex = 1
        elif option[0] == '-q':
            doquery = 1
        elif option[0] == '-c':
            docreate = 1
        elif option[0] == "-2":
            sqlite_version = "2"
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-n':
            nrows = int(option[1])

    # Catch the hdf5 file passed as the last argument
    filename = pargs[0]

    if sqlite_version == "2":
        import sqlite
    else:
        from pysqlite2 import dbapi2 as sqlite

    if verbose:
        print("pysqlite version:", sqlite.version)
        if userandom:
            print("using random values")

    if docreate:
        if verbose:
            print("writing %s krows" % nrows)
        if psyco_imported and usepsyco:
            psyco.bind(create_db)
        nrows *= 1000
        create_db(filename, nrows)

    if createindex:
        index_db(filename)

    if doquery:
        query_db(filename, rng)

########NEW FILE########
__FILENAME__ = stress-test
from __future__ import print_function
import gc
import sys
import time
#import types
import numpy
from tables import Group  # , MetaIsDescription
from tables import *


class Test(IsDescription):
    ngroup = Int32Col(pos=1)
    ntable = Int32Col(pos=2)
    nrow = Int32Col(pos=3)
    #string = StringCol(itemsize=500, pos=4)

TestDict = {
    "ngroup": Int32Col(pos=1),
    "ntable": Int32Col(pos=2),
    "nrow": Int32Col(pos=3),
}


def createFileArr(filename, ngroups, ntables, nrows):

    # First, create the groups

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="PyTables Stress Test")

    for k in range(ngroups):
        # Create the group
        fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

    fileh.close()

    # Now, create the arrays
    arr = numpy.arange(nrows)
    for k in range(ngroups):
        fileh = open_file(filename, mode="a", root_uep='group%04d' % k)
        for j in range(ntables):
            # Create the array
            fileh.create_array("/", 'array%04d' % j, arr, "Array %d" % j)
        fileh.close()

    return (ngroups * ntables * nrows, 4)


def readFileArr(filename, ngroups, recsize, verbose):

    rowsread = 0
    for ngroup in range(ngroups):
        fileh = open_file(filename, mode="r", root_uep='group%04d' % ngroup)
        # Get the group
        group = fileh.root
        narrai = 0
        if verbose:
            print("Group ==>", group)
        for arrai in fileh.list_nodes(group, 'Array'):
            if verbose > 1:
                print("Array ==>", arrai)
                print("Rows in", arrai._v_pathname, ":", arrai.shape)

            arr = arrai.read()

            rowsread += len(arr)
            narrai += 1

        # Close the file (eventually destroy the extended type)
        fileh.close()

    return (rowsread, 4, rowsread * 4)


def createFile(filename, ngroups, ntables, nrows, complevel, complib, recsize):

    # First, create the groups

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="PyTables Stress Test")

    for k in range(ngroups):
        # Create the group
        group = fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

    fileh.close()

    # Now, create the tables
    rowswritten = 0
    if not ntables:
        rowsize = 0

    for k in range(ngroups):
        print("Filling tables in group:", k)
        fileh = open_file(filename, mode="a", root_uep='group%04d' % k)
        # Get the group
        group = fileh.root
        for j in range(ntables):
            # Create a table
            # table = fileh.create_table(group, 'table%04d'% j, Test,
            table = fileh.create_table(group, 'table%04d' % j, TestDict,
                                       'Table%04d' % j,
                                       complevel, complib, nrows)
            rowsize = table.rowsize
            # Get the row object associated with the new table
            row = table.row
            # Fill the table
            for i in range(nrows):
                row['ngroup'] = k
                row['ntable'] = j
                row['nrow'] = i
                row.append()

            rowswritten += nrows
            table.flush()

        # Close the file
        fileh.close()

    return (rowswritten, rowsize)


def readFile(filename, ngroups, recsize, verbose):
    # Open the HDF5 file in read-only mode

    rowsize = 0
    buffersize = 0
    rowsread = 0
    for ngroup in range(ngroups):
        fileh = open_file(filename, mode="r", root_uep='group%04d' % ngroup)
        # Get the group
        group = fileh.root
        ntable = 0
        if verbose:
            print("Group ==>", group)
        for table in fileh.list_nodes(group, 'Table'):
            rowsize = table.rowsize
            buffersize = table.rowsize * table.nrowsinbuf
            if verbose > 1:
                print("Table ==>", table)
                print("Max rows in buf:", table.nrowsinbuf)
                print("Rows in", table._v_pathname, ":", table.nrows)
                print("Buffersize:", table.rowsize * table.nrowsinbuf)
                print("MaxTuples:", table.nrowsinbuf)

            nrow = 0
            if table.nrows > 0:  # only read if we have rows in tables
                for row in table:
                    try:
                        assert row["ngroup"] == ngroup
                        assert row["ntable"] == ntable
                        assert row["nrow"] == nrow
                    except:
                        print("Error in group: %d, table: %d, row: %d" %
                              (ngroup, ntable, nrow))
                        print("Record ==>", row)
                    nrow += 1

            assert nrow == table.nrows
            rowsread += table.nrows
            ntable += 1

        # Close the file (eventually destroy the extended type)
        fileh.close()

    return (rowsread, rowsize, buffersize)


class TrackRefs:

    """Object to track reference counts across test runs."""

    def __init__(self, verbose=0):
        self.type2count = {}
        self.type2all = {}
        self.verbose = verbose

    def update(self, verbose=0):
        obs = sys.getobjects(0)
        type2count = {}
        type2all = {}
        for o in obs:
            all = sys.getrefcount(o)
            t = type(o)
            if verbose:
                # if t == types.TupleType:
                if isinstance(o, Group):
                # if isinstance(o, MetaIsDescription):
                    print("-->", o, "refs:", all)
                    refrs = gc.get_referrers(o)
                    trefrs = []
                    for refr in refrs:
                        trefrs.append(type(refr))
                    print("Referrers -->", refrs)
                    print("Referrers types -->", trefrs)
            # if t == types.StringType: print "-->",o
            if t in type2count:
                type2count[t] += 1
                type2all[t] += all
            else:
                type2count[t] = 1
                type2all[t] = all

        ct = sorted([(type2count[t] - self.type2count.get(t, 0),
                      type2all[t] - self.type2all.get(t, 0),
                      t)
                     for t in type2count.keys()])
        ct.reverse()
        for delta1, delta2, t in ct:
            if delta1 or delta2:
                print("%-55s %8d %8d" % (t, delta1, delta2))

        self.type2count = type2count
        self.type2all = type2all


def dump_refs(preheat=10, iter1=10, iter2=10, *testargs):

    rc1 = rc2 = None
    # testMethod()
    for i in range(preheat):
        testMethod(*testargs)
    gc.collect()
    rc1 = sys.gettotalrefcount()
    track = TrackRefs()
    for i in range(iter1):
        testMethod(*testargs)
    print("First output of TrackRefs:")
    gc.collect()
    rc2 = sys.gettotalrefcount()
    track.update()
    print("Inc refs in function testMethod --> %5d" % (rc2 - rc1),
          file=sys.stderr)
    for i in range(iter2):
        testMethod(*testargs)
        track.update(verbose=1)
    print("Second output of TrackRefs:")
    gc.collect()
    rc3 = sys.gettotalrefcount()

    print("Inc refs in function testMethod --> %5d" % (rc3 - rc2),
          file=sys.stderr)


def dump_garbage():
    """show us waht the garbage is about."""
    # Force collection
    print("\nGARBAGE:")
    gc.collect()

    print("\nGARBAGE OBJECTS:")
    for x in gc.garbage:
        s = str(x)
        #if len(s) > 80: s = s[:77] + "..."
        print(type(x), "\n   ", s)

    # print "\nTRACKED OBJECTS:"
    # reportLoggedInstances("*")


def testMethod(file, usearray, testwrite, testread, complib, complevel,
               ngroups, ntables, nrows):

    if complevel > 0:
        print("Compression library:", complib)
    if testwrite:
        t1 = time.time()
        cpu1 = time.clock()
        if usearray:
            (rowsw, rowsz) = createFileArr(file, ngroups, ntables, nrows)
        else:
            (rowsw, rowsz) = createFile(file, ngroups, ntables, nrows,
                                        complevel, complib, recsize)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        t1 = time.time()
        cpu1 = time.clock()
        if usearray:
            (rowsr, rowsz, bufsz) = readFileArr(file,
                                                ngroups, recsize, verbose)
        else:
            (rowsr, rowsz, bufsz) = readFile(file, ngroups, recsize, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        print("Rows read:", rowsr, " Row size:", rowsz, "Buf size:", bufsz)
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read rows/sec: ", int(rowsr / float(treadrows)))
        print("Read KB/s :", int(rowsr * rowsz / (treadrows * 1024)))

if __name__ == "__main__":
    import getopt
    import profile
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-d debug] [-v level] [-p] [-r] [-w] [-l complib] [-c complevel] [-g ngroups] [-t ntables] [-i nrows] file
    -d debugging level
    -v verbosity level
    -p use "psyco" if available
    -a use Array objects instead of Table
    -r only read test
    -w only write test
    -l sets the compression library to be used ("zlib", "lzo", "ucl", "bzip2")
    -c sets a compression level (do not set it or 0 for no compression)
    -g number of groups hanging from "/"
    -t number of tables per group
    -i number of rows per table
"""

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'd:v:parwl:c:g:t:i:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    ngroups = 5
    ntables = 5
    nrows = 100
    verbose = 0
    debug = 0
    recsize = "medium"
    testread = 1
    testwrite = 1
    usepsyco = 0
    usearray = 0
    complevel = 0
    complib = "zlib"

    # Get the options
    for option in opts:
        if option[0] == '-d':
            debug = int(option[1])
        if option[0] == '-v':
            verbose = int(option[1])
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-a':
            usearray = 1
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-g':
            ngroups = int(option[1])
        elif option[0] == '-t':
            ntables = int(option[1])
        elif option[0] == '-i':
            nrows = int(option[1])

    if debug:
        gc.enable()

    if debug == 1:
        gc.set_debug(gc.DEBUG_LEAK)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    if psyco_imported and usepsyco:
        psyco.bind(createFile)
        psyco.bind(readFile)

    if debug == 2:
        dump_refs(10, 10, 15, file, usearray, testwrite, testread, complib,
                  complevel, ngroups, ntables, nrows)
    else:
#         testMethod(file, usearray, testwrite, testread, complib, complevel,
#                    ngroups, ntables, nrows)
        profile.run("testMethod(file, usearray, testwrite, testread, " +
                    "complib, complevel, ngroups, ntables, nrows)")

    # Show the dirt
    if debug == 1:
        dump_garbage()

########NEW FILE########
__FILENAME__ = stress-test2
from __future__ import print_function
import gc
import sys
import time
import random
from tables import *


class Test(IsDescription):
    ngroup = Int32Col(pos=1)
    ntable = Int32Col(pos=2)
    nrow = Int32Col(pos=3)
    time = Float64Col(pos=5)
    random = Float32Col(pos=4)


def createFile(filename, ngroups, ntables, nrows, complevel, complib, recsize):

    # First, create the groups

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="PyTables Stress Test")

    for k in range(ngroups):
        # Create the group
        group = fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

    fileh.close()

    # Now, create the tables
    rowswritten = 0
    for k in range(ngroups):
        fileh = open_file(filename, mode="a", root_uep='group%04d' % k)
        # Get the group
        group = fileh.root
        for j in range(ntables):
            # Create a table
            table = fileh.create_table(group, 'table%04d' % j, Test,
                                       'Table%04d' % j,
                                       complevel, complib, nrows)
            # Get the row object associated with the new table
            row = table.row
            # Fill the table
            for i in range(nrows):
                row['time'] = time.time()
                row['random'] = random.random() * 40 + 100
                row['ngroup'] = k
                row['ntable'] = j
                row['nrow'] = i
                row.append()

            rowswritten += nrows
            table.flush()

        # Close the file
        fileh.close()

    return (rowswritten, table.rowsize)


def readFile(filename, ngroups, recsize, verbose):
    # Open the HDF5 file in read-only mode

    rowsread = 0
    for ngroup in range(ngroups):
        fileh = open_file(filename, mode="r", root_uep='group%04d' % ngroup)
        # Get the group
        group = fileh.root
        ntable = 0
        if verbose:
            print("Group ==>", group)
        for table in fileh.list_nodes(group, 'Table'):
            rowsize = table.rowsize
            buffersize = table.rowsize * table.nrowsinbuf
            if verbose > 1:
                print("Table ==>", table)
                print("Max rows in buf:", table.nrowsinbuf)
                print("Rows in", table._v_pathname, ":", table.nrows)
                print("Buffersize:", table.rowsize * table.nrowsinbuf)
                print("MaxTuples:", table.nrowsinbuf)

            nrow = 0
            time_1 = 0.0
            for row in table:
                try:
                    # print "row['ngroup'], ngroup ==>", row["ngroup"], ngroup
                    assert row["ngroup"] == ngroup
                    assert row["ntable"] == ntable
                    assert row["nrow"] == nrow
                    # print "row['time'], time_1 ==>", row["time"], time_1
                    assert row["time"] >= (time_1 - 0.01)
                    #assert 100 <= row["random"] <= 139.999
                    assert 100 <= row["random"] <= 140
                except:
                    print("Error in group: %d, table: %d, row: %d" %
                          (ngroup, ntable, nrow))
                    print("Record ==>", row)
                time_1 = row["time"]
                nrow += 1

            assert nrow == table.nrows
            rowsread += table.nrows
            ntable += 1

        # Close the file (eventually destroy the extended type)
        fileh.close()

    return (rowsread, rowsize, buffersize)


def dump_garbage():
    """show us waht the garbage is about."""
    # Force collection
    print("\nGARBAGE:")
    gc.collect()

    print("\nGARBAGE OBJECTS:")
    for x in gc.garbage:
        s = str(x)
        #if len(s) > 80: s = s[:77] + "..."
        print(type(x), "\n   ", s)

if __name__ == "__main__":
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-d debug] [-v level] [-p] [-r] [-w] [-l complib] [-c complevel] [-g ngroups] [-t ntables] [-i nrows] file
    -d debugging level
    -v verbosity level
    -p use "psyco" if available
    -r only read test
    -w only write test
    -l sets the compression library to be used ("zlib", "lzo", "ucl", "bzip2")
    -c sets a compression level (do not set it or 0 for no compression)
    -g number of groups hanging from "/"
    -t number of tables per group
    -i number of rows per table
"""

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'd:v:prwl:c:g:t:i:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    ngroups = 5
    ntables = 5
    nrows = 100
    verbose = 0
    debug = 0
    recsize = "medium"
    testread = 1
    testwrite = 1
    usepsyco = 0
    complevel = 0
    complib = "zlib"

    # Get the options
    for option in opts:
        if option[0] == '-d':
            debug = int(option[1])
        if option[0] == '-v':
            verbose = int(option[1])
        if option[0] == '-p':
            usepsyco = 1
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-g':
            ngroups = int(option[1])
        elif option[0] == '-t':
            ntables = int(option[1])
        elif option[0] == '-i':
            nrows = int(option[1])

    if debug:
        gc.enable()
        gc.set_debug(gc.DEBUG_LEAK)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    print("Compression level:", complevel)
    if complevel > 0:
        print("Compression library:", complib)
    if testwrite:
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        (rowsw, rowsz) = createFile(file, ngroups, ntables, nrows,
                                    complevel, complib, recsize)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
        (rowsr, rowsz, bufsz) = readFile(file, ngroups, recsize, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        print("Rows read:", rowsr, " Row size:", rowsz, "Buf size:", bufsz)
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read rows/sec: ", int(rowsr / float(treadrows)))
        print("Read KB/s :", int(rowsr * rowsz / (treadrows * 1024)))

    # Show the dirt
    if debug > 1:
        dump_garbage()

########NEW FILE########
__FILENAME__ = stress-test3
#!/usr/bin/env python

"""This script allows to create arbitrarily large files with the desired
combination of groups, tables per group and rows per table.

Issue "python stress-test3.py" without parameters for a help on usage.

"""

from __future__ import print_function
import gc
import sys
import time
from tables import *


class Test(IsDescription):
    ngroup = Int32Col(pos=1)
    ntable = Int32Col(pos=2)
    nrow = Int32Col(pos=3)
    string = StringCol(500, pos=4)


def createFileArr(filename, ngroups, ntables, nrows):

    # First, create the groups

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="PyTables Stress Test")

    for k in range(ngroups):
        # Create the group
        fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

    fileh.close()

    return (0, 4)


def readFileArr(filename, ngroups, recsize, verbose):

    rowsread = 0
    for ngroup in range(ngroups):
        fileh = open_file(filename, mode="r", root_uep='group%04d' % ngroup)
        # Get the group
        group = fileh.root
        ntable = 0
        if verbose:
            print("Group ==>", group)
        for table in fileh.list_nodes(group, 'Array'):
            if verbose > 1:
                print("Array ==>", table)
                print("Rows in", table._v_pathname, ":", table.shape)

            arr = table.read()

            rowsread += len(arr)
            ntable += 1

        # Close the file (eventually destroy the extended type)
        fileh.close()

    return (rowsread, 4, 0)


def createFile(filename, ngroups, ntables, nrows, complevel, complib, recsize):

    # First, create the groups

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="PyTables Stress Test")

    for k in range(ngroups):
        # Create the group
        group = fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

    fileh.close()

    # Now, create the tables
    rowswritten = 0
    for k in range(ngroups):
        fileh = open_file(filename, mode="a", root_uep='group%04d' % k)
        # Get the group
        group = fileh.root
        for j in range(ntables):
            # Create a table
            table = fileh.create_table(group, 'table%04d' % j, Test,
                                       'Table%04d' % j,
                                       Filters(complevel, complib), nrows)
            rowsize = table.rowsize
            # Get the row object associated with the new table
            row = table.row
            # Fill the table
            for i in range(nrows):
                row['ngroup'] = k
                row['ntable'] = j
                row['nrow'] = i
                row.append()

            rowswritten += nrows
            table.flush()

        # Close the file
        fileh.close()

    return (rowswritten, rowsize)


def readFile(filename, ngroups, recsize, verbose):
    # Open the HDF5 file in read-only mode

    rowsread = 0
    for ngroup in range(ngroups):
        fileh = open_file(filename, mode="r", root_uep='group%04d' % ngroup)
        # Get the group
        group = fileh.root
        ntable = 0
        if verbose:
            print("Group ==>", group)
        for table in fileh.list_nodes(group, 'Table'):
            rowsize = table.rowsize
            buffersize = table.rowsize * table.nrowsinbuf
            if verbose > 1:
                print("Table ==>", table)
                print("Max rows in buf:", table.nrowsinbuf)
                print("Rows in", table._v_pathname, ":", table.nrows)
                print("Buffersize:", table.rowsize * table.nrowsinbuf)
                print("MaxTuples:", table.nrowsinbuf)

            nrow = 0
            for row in table:
                try:
                    assert row["ngroup"] == ngroup
                    assert row["ntable"] == ntable
                    assert row["nrow"] == nrow
                except:
                    print("Error in group: %d, table: %d, row: %d" %
                          (ngroup, ntable, nrow))
                    print("Record ==>", row)
                nrow += 1

            assert nrow == table.nrows
            rowsread += table.nrows
            ntable += 1

        # Close the file (eventually destroy the extended type)
        fileh.close()

    return (rowsread, rowsize, buffersize)


def dump_garbage():
    """show us waht the garbage is about."""
    # Force collection
    print("\nGARBAGE:")
    gc.collect()

    print("\nGARBAGE OBJECTS:")
    for x in gc.garbage:
        s = str(x)
        #if len(s) > 80: s = s[:77] + "..."
        print(type(x), "\n   ", s)

if __name__ == "__main__":
    import getopt
    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    usage = """usage: %s [-d debug] [-v level] [-p] [-r] [-w] [-l complib] [-c complevel] [-g ngroups] [-t ntables] [-i nrows] file
    -d debugging level
    -v verbosity level
    -p use "psyco" if available
    -a use Array objects instead of Table
    -r only read test
    -w only write test
    -l sets the compression library to be used ("zlib", "lzo", "ucl", "bzip2")
    -c sets a compression level (do not set it or 0 for no compression)
    -g number of groups hanging from "/"
    -t number of tables per group
    -i number of rows per table
"""

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'd:v:parwl:c:g:t:i:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    ngroups = 5
    ntables = 5
    nrows = 100
    verbose = 0
    debug = 0
    recsize = "medium"
    testread = 1
    testwrite = 1
    usepsyco = 0
    usearray = 0
    complevel = 0
    complib = "zlib"

    # Get the options
    for option in opts:
        if option[0] == '-d':
            debug = int(option[1])
        if option[0] == '-v':
            verbose = int(option[1])
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-a':
            usearray = 1
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-g':
            ngroups = int(option[1])
        elif option[0] == '-t':
            ntables = int(option[1])
        elif option[0] == '-i':
            nrows = int(option[1])

    if debug:
        gc.enable()
        gc.set_debug(gc.DEBUG_LEAK)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    print("Compression level:", complevel)
    if complevel > 0:
        print("Compression library:", complib)
    if testwrite:
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        if usearray:
            (rowsw, rowsz) = createFileArr(file, ngroups, ntables, nrows)
        else:
            (rowsw, rowsz) = createFile(file, ngroups, ntables, nrows,
                                        complevel, complib, recsize)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
        if usearray:
            (rowsr, rowsz, bufsz) = readFileArr(file,
                                                ngroups, recsize, verbose)
        else:
            (rowsr, rowsz, bufsz) = readFile(file, ngroups, recsize, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        print("Rows read:", rowsr, " Row size:", rowsz, "Buf size:", bufsz)
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read rows/sec: ", int(rowsr / float(treadrows)))
        print("Read KB/s :", int(rowsr * rowsz / (treadrows * 1024)))

    # Show the dirt
    if debug > 1:
        dump_garbage()

########NEW FILE########
__FILENAME__ = table-bench
#!/usr/bin/env python

from __future__ import print_function
import numpy as NP
from tables import *

# This class is accessible only for the examples


class Small(IsDescription):
    var1 = StringCol(itemsize=4, pos=2)
    var2 = Int32Col(pos=1)
    var3 = Float64Col(pos=0)

# Define a user record to characterize some kind of particles


class Medium(IsDescription):
    name = StringCol(itemsize=16, pos=0)    # 16-character String
    float1 = Float64Col(shape=2, dflt=NP.arange(2), pos=1)
    #float1 = Float64Col(dflt=2.3)
    #float2 = Float64Col(dflt=2.3)
    # zADCcount    = Int16Col()               # signed short integer
    ADCcount = Int32Col(pos=6)              # signed short integer
    grid_i = Int32Col(pos=7)                # integer
    grid_j = Int32Col(pos=8)                # integer
    pressure = Float32Col(pos=9)            # float  (single-precision)
    energy = Float64Col(pos=2)              # double (double-precision)
    # unalig      = Int8Col()                 # just to unalign data

# Define a user record to characterize some kind of particles


class Big(IsDescription):
    name = StringCol(itemsize=16)           # 16-character String
    float1 = Float64Col(shape=32, dflt=NP.arange(32))
    float2 = Float64Col(shape=32, dflt=2.2)
    TDCcount = Int8Col()                    # signed short integer
    #ADCcount    = Int32Col()
    # ADCcount = Int16Col()                   # signed short integer
    grid_i = Int32Col()                       # integer
    grid_j = Int32Col()                       # integer
    pressure = Float32Col()                   # float  (single-precision)
    energy = Float64Col()                     # double (double-precision)


def createFile(filename, totalrows, filters, recsize):

    # Open a file in "w"rite mode
    fileh = open_file(filename, mode="w", title="Table Benchmark",
                      filters=filters)

    # Table title
    title = "This is the table title"

    # Create a Table instance
    group = fileh.root
    rowswritten = 0
    for j in range(3):
        # Create a table
        if recsize == "big":
            table = fileh.create_table(group, 'tuple' + str(j), Big, title,
                                       None,
                                       totalrows)
        elif recsize == "medium":
            table = fileh.create_table(group, 'tuple' + str(j), Medium, title,
                                       None,
                                       totalrows)
        elif recsize == "small":
            table = fileh.create_table(group, 'tuple' + str(j), Small, title,
                                       None,
                                       totalrows)
        else:
            raise RuntimeError("This should never happen")

        table.attrs.test = 2
        rowsize = table.rowsize
        # Get the row object associated with the new table
        d = table.row
        # Fill the table
        if recsize == "big":
            for i in range(totalrows):
                # d['name']  = 'Part: %6d' % (i)
                d['TDCcount'] = i % 256
                #d['float1'] = NP.array([i]*32, NP.float64)
                #d['float2'] = NP.array([i**2]*32, NP.float64)
                #d['float1'][0] = float(i)
                #d['float2'][0] = float(i*2)
                # Common part with medium
                d['grid_i'] = i
                d['grid_j'] = 10 - i
                d['pressure'] = float(i * i)
                # d['energy'] = float(d['pressure'] ** 4)
                d['energy'] = d['pressure']
                # d['idnumber'] = i * (2 ** 34)
                d.append()
        elif recsize == "medium":
            for i in range(totalrows):
                #d['name']  = 'Part: %6d' % (i)
                #d['float1'] = NP.array([i]*2, NP.float64)
                #d['float1'] = arr
                #d['float1'] = i
                #d['float2'] = float(i)
                # Common part with big:
                d['grid_i'] = i
                d['grid_j'] = 10 - i
                d['pressure'] = i * 2
                # d['energy'] = float(d['pressure'] ** 4)
                d['energy'] = d['pressure']
                d.append()
        else:  # Small record
            for i in range(totalrows):
                #d['var1'] = str(random.randrange(1000000))
                #d['var3'] = random.randrange(10000000)
                d['var1'] = str(i)
                #d['var2'] = random.randrange(totalrows)
                d['var2'] = i
                #d['var3'] = 12.1e10
                d['var3'] = totalrows - i
                d.append()  # This is a 10% faster than table.append()
        rowswritten += totalrows

        if recsize == "small":
            # Testing with indexing
            pass
#            table._createIndex("var3", Filters(1,"zlib",shuffle=1))

        # table.flush()
        group._v_attrs.test2 = "just a test"
        # Create a new group
        group2 = fileh.create_group(group, 'group' + str(j))
        # Iterate over this new group (group2)
        group = group2
        table.flush()

    # Close the file (eventually destroy the extended type)
    fileh.close()
    return (rowswritten, rowsize)


def readFile(filename, recsize, verbose):
    # Open the HDF5 file in read-only mode

    fileh = open_file(filename, mode="r")
    rowsread = 0
    for groupobj in fileh.walk_groups(fileh.root):
        # print "Group pathname:", groupobj._v_pathname
        row = 0
        for table in fileh.list_nodes(groupobj, 'Table'):
            rowsize = table.rowsize
            print("reading", table)
            if verbose:
                print("Max rows in buf:", table.nrowsinbuf)
                print("Rows in", table._v_pathname, ":", table.nrows)
                print("Buffersize:", table.rowsize * table.nrowsinbuf)
                print("MaxTuples:", table.nrowsinbuf)

            if recsize == "big" or recsize == "medium":
                # e = [ p.float1 for p in table.iterrows()
                #      if p.grid_i < 2 ]
                #e = [ str(p) for p in table.iterrows() ]
                #      if p.grid_i < 2 ]
#                 e = [ p['grid_i'] for p in table.iterrows()
#                       if p['grid_j'] == 20 and p['grid_i'] < 20 ]
#                 e = [ p['grid_i'] for p in table
#                       if p['grid_i'] <= 2 ]
#                e = [ p['grid_i'] for p in table.where("grid_i<=20")]
#                 e = [ p['grid_i'] for p in
#                       table.where('grid_i <= 20')]
                e = [p['grid_i'] for p in
                     table.where('(grid_i <= 20) & (grid_j == 20)')]
#                 e = [ p['grid_i'] for p in table.iterrows()
#                       if p.nrow() == 20 ]
#                 e = [ table.delrow(p.nrow()) for p in table.iterrows()
#                       if p.nrow() == 20 ]
                # The version with a for loop is only 1% better than
                # comprenhension list
                #e = []
                # for p in table.iterrows():
                #    if p.grid_i < 20:
                #        e.append(p.grid_j)
            else:  # small record case
#                 e = [ p['var3'] for p in table.iterrows()
#                       if p['var2'] < 20 and p['var3'] < 20 ]
#                e = [ p['var3'] for p in table.where("var3 <= 20")
#                      if p['var2'] < 20 ]
#               e = [ p['var3'] for p in table.where("var3 <= 20")]
# Cuts 1) and 2) issues the same results but 2) is about 10 times faster
# Cut 1)
#                e = [ p.nrow() for p in
#                      table.where(table.cols.var2 > 5)
#                      if p["var2"] < 10]
# Cut 2)
#                 e = [ p.nrow() for p in
#                       table.where(table.cols.var2 < 10)
#                       if p["var2"] > 5]
#                e = [ (p._nrow,p["var3"]) for p in
#                e = [ p["var3"] for p in
#                      table.where(table.cols.var3 < 10)]
#                      table.where(table.cols.var3 < 10)]
#                      table if p["var3"] <= 10]
#               e = [ p['var3'] for p in table.where("var3 <= 20")]
#                e = [ p['var3'] for p in
# table.where(table.cols.var1 == "10")]  # More
                     # than ten times faster than the next one
#                e = [ p['var3'] for p in table
#                      if p['var1'] == "10"]
#                e = [ p['var3'] for p in table.where('var2 <= 20')]
                e = [p['var3']
                     for p in table.where('(var2 <= 20) & (var2 >= 3)')]
                # e = [ p[0] for p in table.where('var2 <= 20')]
                #e = [ p['var3'] for p in table if p['var2'] <= 20 ]
                # e = [ p[:] for p in table if p[1] <= 20 ]
#                  e = [ p['var3'] for p in table._whereInRange(table.cols.var2 <=20)]
                #e = [ p['var3'] for p in table.iterrows(0,21) ]
#                  e = [ p['var3'] for p in table.iterrows()
#                       if p.nrow() <= 20 ]
                #e = [ p['var3'] for p in table.iterrows(1,0,1000)]
                #e = [ p['var3'] for p in table.iterrows(1,100)]
                # e = [ p['var3'] for p in table.iterrows(step=2)
                #      if p.nrow() < 20 ]
                # e = [ p['var2'] for p in table.iterrows()
                #      if p['var2'] < 20 ]
                # for p in table.iterrows():
                #      pass
            if verbose:
                # print "Last record read:", p
                print("resulting selection list ==>", e)

            rowsread += table.nrows
            row += 1
            if verbose:
                print("Total selected records ==> ", len(e))

    # Close the file (eventually destroy the extended type)
    fileh.close()

    return (rowsread, rowsize)


def readField(filename, field, rng, verbose):
    fileh = open_file(filename, mode="r")
    rowsread = 0
    if rng is None:
        rng = [0, -1, 1]
    if field == "all":
        field = None
    for groupobj in fileh.walk_groups(fileh.root):
        for table in fileh.list_nodes(groupobj, 'Table'):
            rowsize = table.rowsize
            # table.nrowsinbuf = 3 # For testing purposes
            if verbose:
                print("Max rows in buf:", table.nrowsinbuf)
                print("Rows in", table._v_pathname, ":", table.nrows)
                print("Buffersize:", table.rowsize * table.nrowsinbuf)
                print("MaxTuples:", table.nrowsinbuf)
                print("(field, start, stop, step) ==>", (field, rng[0], rng[1],
                                                         rng[2]))

            e = table.read(rng[0], rng[1], rng[2], field)

            rowsread += table.nrows
            if verbose:
                print("Selected rows ==> ", e)
                print("Total selected rows ==> ", len(e))

    # Close the file (eventually destroy the extended type)
    fileh.close()
    return (rowsread, rowsize)

if __name__ == "__main__":
    import sys
    import getopt

    try:
        import psyco
        psyco_imported = 1
    except:
        psyco_imported = 0

    import time

    usage = """usage: %s [-v] [-p] [-P] [-R range] [-r] [-w] [-s recsize] [-f field] [-c level] [-l complib] [-i iterations] [-S] [-F] file
            -v verbose
            -p use "psyco" if available
            -P do profile
            -R select a range in a field in the form "start,stop,step"
            -r only read test
            -w only write test
            -s use [big] record, [medium] or [small]
            -f only read stated field name in tables ("all" means all fields)
            -c sets a compression level (do not set it or 0 for no compression)
            -S activate shuffling filter
            -F activate fletcher32 filter
            -l sets the compression library to be used ("zlib", "lzo", "blosc", "bzip2")
            -i sets the number of rows in each table\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpPSFR:rwf:s:c:l:i:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    profile = 0
    rng = None
    recsize = "medium"
    fieldName = None
    testread = 1
    testwrite = 1
    usepsyco = 0
    complevel = 0
    shuffle = 0
    fletcher32 = 0
    complib = "zlib"
    iterations = 100

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        if option[0] == '-p':
            usepsyco = 1
        if option[0] == '-P':
            profile = 1
        if option[0] == '-S':
            shuffle = 1
        if option[0] == '-F':
            fletcher32 = 1
        elif option[0] == '-R':
            rng = [int(i) for i in option[1].split(",")]
        elif option[0] == '-r':
            testwrite = 0
        elif option[0] == '-w':
            testread = 0
        elif option[0] == '-f':
            fieldName = option[1]
        elif option[0] == '-s':
            recsize = option[1]
            if recsize not in ["big", "medium", "small"]:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-c':
            complevel = int(option[1])
        elif option[0] == '-l':
            complib = option[1]
        elif option[0] == '-i':
            iterations = int(option[1])

    # Build the Filters instance
    filters = Filters(complevel=complevel, complib=complib,
                      shuffle=shuffle, fletcher32=fletcher32)

    # Catch the hdf5 file passed as the last argument
    file = pargs[0]

    if verbose:
        print("numpy version:", NP.__version__)
        if psyco_imported and usepsyco:
            print("Using psyco version:", psyco.version_info)

    if testwrite:
        print("Compression level:", complevel)
        if complevel > 0:
            print("Compression library:", complib)
            if shuffle:
                print("Suffling...")
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(createFile)
        if profile:
            import profile as prof
            import pstats
            prof.run(
                '(rowsw, rowsz) = createFile(file, iterations, filters, '
                'recsize)',
                'table-bench.prof')
            stats = pstats.Stats('table-bench.prof')
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            stats.print_stats(20)
        else:
            (rowsw, rowsz) = createFile(file, iterations, filters, recsize)
        t2 = time.time()
        cpu2 = time.clock()
        tapprows = round(t2 - t1, 3)
        cpuapprows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpuapprows / tapprows, 2) * 100)
        print("Rows written:", rowsw, " Row size:", rowsz)
        print("Time writing rows: %s s (real) %s s (cpu)  %s%%" %
              (tapprows, cpuapprows, tpercent))
        print("Write rows/sec: ", int(rowsw / float(tapprows)))
        print("Write KB/s :", int(rowsw * rowsz / (tapprows * 1024)))

    if testread:
        t1 = time.time()
        cpu1 = time.clock()
        if psyco_imported and usepsyco:
            psyco.bind(readFile)
            # psyco.bind(readField)
            pass
        if rng or fieldName:
            (rowsr, rowsz) = readField(file, fieldName, rng, verbose)
            pass
        else:
            for i in range(1):
                (rowsr, rowsz) = readFile(file, recsize, verbose)
        t2 = time.time()
        cpu2 = time.clock()
        treadrows = round(t2 - t1, 3)
        cpureadrows = round(cpu2 - cpu1, 3)
        tpercent = int(round(cpureadrows / treadrows, 2) * 100)
        print("Rows read:", rowsr, " Row size:", rowsz)
        print("Time reading rows: %s s (real) %s s (cpu)  %s%%" %
              (treadrows, cpureadrows, tpercent))
        print("Read rows/sec: ", int(rowsr / float(treadrows)))
        print("Read KB/s :", int(rowsr * rowsz / (treadrows * 1024)))

########NEW FILE########
__FILENAME__ = table-copy
from __future__ import print_function
import time

import numpy as np
import tables

N = 144000
#N = 144


def timed(func, *args, **kwargs):
    start = time.time()
    res = func(*args, **kwargs)
    print("%fs elapsed." % (time.time() - start))
    return res


def create_table(output_path):
    print("creating array...", end=' ')
    dt = np.dtype([('field%d' % i, int) for i in range(320)])
    a = np.zeros(N, dtype=dt)
    print("done.")

    output_file = tables.open_file(output_path, mode="w")
    table = output_file.create_table("/", "test", dt)  # , filters=blosc4)
    print("appending data...", end=' ')
    table.append(a)
    print("flushing...", end=' ')
    table.flush()
    print("done.")
    output_file.close()


def copy1(input_path, output_path):
    print("copying data from %s to %s..." % (input_path, output_path))
    input_file = tables.open_file(input_path, mode="r")
    output_file = tables.open_file(output_path, mode="w")

    # copy nodes as a batch
    input_file.copy_node("/", output_file.root, recursive=True)
    output_file.close()
    input_file.close()


def copy2(input_path, output_path):
    print("copying data from %s to %s..." % (input_path, output_path))
    input_file = tables.open_file(input_path, mode="r")
    input_file.copy_file(output_path, overwrite=True)
    input_file.close()


def copy3(input_path, output_path):
    print("copying data from %s to %s..." % (input_path, output_path))
    input_file = tables.open_file(input_path, mode="r")
    output_file = tables.open_file(output_path, mode="w")
    table = input_file.root.test
    table.copy(output_file.root)
    output_file.close()
    input_file.close()


def copy4(input_path, output_path, complib='zlib', complevel=0):
    print("copying data from %s to %s..." % (input_path, output_path))
    input_file = tables.open_file(input_path, mode="r")
    output_file = tables.open_file(output_path, mode="w")

    input_table = input_file.root.test
    print("reading data...", end=' ')
    data = input_file.root.test.read()
    print("done.")

    filter = tables.Filters(complevel=complevel, complib=complib)
    output_table = output_file.create_table("/", "test", input_table.dtype,
                                            filters=filter)
    print("appending data...", end=' ')
    output_table.append(data)
    print("flushing...", end=' ')
    output_table.flush()
    print("done.")

    input_file.close()
    output_file.close()


def copy5(input_path, output_path, complib='zlib', complevel=0):
    print("copying data from %s to %s..." % (input_path, output_path))
    input_file = tables.open_file(input_path, mode="r")
    output_file = tables.open_file(output_path, mode="w")

    input_table = input_file.root.test

    filter = tables.Filters(complevel=complevel, complib=complib)
    output_table = output_file.create_table("/", "test", input_table.dtype,
                                            filters=filter)
    chunksize = 10000
    rowsleft = len(input_table)
    start = 0
    for chunk in range((len(input_table) / chunksize) + 1):
        stop = start + min(chunksize, rowsleft)
        data = input_table.read(start, stop)
        output_table.append(data)
        output_table.flush()
        rowsleft -= chunksize
        start = stop

    input_file.close()
    output_file.close()


if __name__ == '__main__':
    timed(create_table, 'tmp.h5')
#    timed(copy1, 'tmp.h5', 'test1.h5')
    timed(copy2, 'tmp.h5', 'test2.h5')
#    timed(copy3, 'tmp.h5', 'test3.h5')
    timed(copy4, 'tmp.h5', 'test4.h5')
    timed(copy5, 'tmp.h5', 'test5.h5')

########NEW FILE########
__FILENAME__ = undo_redo
###########################################################################
# Benchmark for undo/redo. Run this program without parameters
# for mode of use.
#
# Francesc Alted
# 2005-03-09
###########################################################################

from __future__ import print_function
import numpy
from time import time
import tables

verbose = 0


class BasicBenchmark(object):

    def __init__(self, filename, testname, vecsize, nobjects, niter):

        self.file = filename
        self.test = testname
        self.vecsize = vecsize
        self.nobjects = nobjects
        self.niter = niter

        # Initialize the arrays
        self.a1 = numpy.arange(0, 1 * self.vecsize)
        self.a2 = numpy.arange(1 * self.vecsize, 2 * self.vecsize)
        self.a3 = numpy.arange(2 * self.vecsize, 3 * self.vecsize)

    def setUp(self):

        # Create an HDF5 file
        self.fileh = tables.open_file(self.file, mode="w")
        # open the do/undo
        self.fileh.enable_undo()

    def tearDown(self):
        self.fileh.disable_undo()
        self.fileh.close()
        # Remove the temporary file
        # os.remove(self.file)

    def createNode(self):
        """Checking a undo/redo create_array."""

        for i in range(self.nobjects):
            # Create a new array
            self.fileh.create_array('/', 'array' + str(i), self.a1)
            # Put a mark
            self.fileh.mark()
        # Unwind all marks sequentially
        for i in range(self.niter):
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.undo()
                if verbose:
                    print("u", end=' ')
            if verbose:
                print()
            undo = time() - t1
            # Rewind all marks sequentially
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.redo()
                if verbose:
                    print("r", end=' ')
            if verbose:
                print()
            redo = time() - t1

            print("Time for Undo, Redo (createNode):", undo, "s, ", redo, "s")

    def copy_children(self):
        """Checking a undo/redo copy_children."""

        # Create a group
        self.fileh.create_group('/', 'agroup')
        # Create several objects there
        for i in range(10):
            # Create a new array
            self.fileh.create_array('/agroup', 'array' + str(i), self.a1)
        # Excercise copy_children
        for i in range(self.nobjects):
            # Create another group for destination
            self.fileh.create_group('/', 'anothergroup' + str(i))
            # Copy children from /agroup to /anothergroup+i
            self.fileh.copy_children('/agroup', '/anothergroup' + str(i))
            # Put a mark
            self.fileh.mark()
        # Unwind all marks sequentially
        for i in range(self.niter):
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.undo()
                if verbose:
                    print("u", end=' ')
            if verbose:
                print()
            undo = time() - t1
            # Rewind all marks sequentially
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.redo()
                if verbose:
                    print("r", end=' ')
            if verbose:
                print()
            redo = time() - t1

            print(("Time for Undo, Redo (copy_children):", undo, "s, ",
                  redo, "s"))

    def set_attr(self):
        """Checking a undo/redo for setting attributes."""

        # Create a new array
        self.fileh.create_array('/', 'array', self.a1)
        for i in range(self.nobjects):
            # Set an attribute
            setattr(self.fileh.root.array.attrs, "attr" + str(i), str(self.a1))
            # Put a mark
            self.fileh.mark()
        # Unwind all marks sequentially
        for i in range(self.niter):
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.undo()
                if verbose:
                    print("u", end=' ')
            if verbose:
                print()
            undo = time() - t1
            # Rewind all marks sequentially
            t1 = time()
            for i in range(self.nobjects):
                self.fileh.redo()
                if verbose:
                    print("r", end=' ')
            if verbose:
                print()
            redo = time() - t1

            print("Time for Undo, Redo (set_attr):", undo, "s, ", redo, "s")

    def runall(self):

        if testname == "all":
            tests = [self.createNode, self.copy_children, self.set_attr]
        elif testname == "createNode":
            tests = [self.createNode]
        elif testname == "copy_children":
            tests = [self.copy_children]
        elif testname == "set_attr":
            tests = [self.set_attr]
        for meth in tests:
            self.setUp()
            meth()
            self.tearDown()


if __name__ == '__main__':
    import sys
    import getopt

    usage = """usage: %s [-v] [-p] [-t test] [-s vecsize] [-n niter] datafile
              -v verbose  (total dump of profiling)
              -p do profiling
              -t {createNode|copy_children|set_attr|all} run the specified test
              -s the size of vectors that are undone/redone
              -n number of objects in operations
              -i number of iterations for reading\n""" % sys.argv[0]

    try:
        opts, pargs = getopt.getopt(sys.argv[1:], 'vpt:s:n:i:')
    except:
        sys.stderr.write(usage)
        sys.exit(0)

    # if we pass too much parameters, abort
    if len(pargs) != 1:
        sys.stderr.write(usage)
        sys.exit(0)

    # default options
    verbose = 0
    profile = 0
    testname = "all"
    vecsize = 10
    nobjects = 1
    niter = 1

    # Get the options
    for option in opts:
        if option[0] == '-v':
            verbose = 1
        elif option[0] == '-p':
            profile = 1
        elif option[0] == '-t':
            testname = option[1]
            if testname not in ['createNode', 'copy_children', 'set_attr',
                                'all']:
                sys.stderr.write(usage)
                sys.exit(0)
        elif option[0] == '-s':
            vecsize = int(option[1])
        elif option[0] == '-n':
            nobjects = int(option[1])
        elif option[0] == '-i':
            niter = int(option[1])

    filename = pargs[0]

    bench = BasicBenchmark(filename, testname, vecsize, nobjects, niter)
    if profile:
        import hotshot
        import hotshot.stats
        prof = hotshot.Profile("do_undo.prof")
        prof.runcall(bench.runall)
        prof.close()
        stats = hotshot.stats.load("do_undo.prof")
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        if verbose:
            stats.print_stats()
        else:
            stats.print_stats(20)
    else:
        bench.runall()

# Local Variables:
# mode: python
# End:

########NEW FILE########
__FILENAME__ = widetree
from __future__ import print_function
import hotshot
import hotshot.stats

import unittest
import os
import tempfile

from tables import *

verbose = 0


class WideTreeTestCase(unittest.TestCase):
    """Checks for maximum number of childs for a Group."""

    def test00_Leafs(self):
        """Checking creation of large number of leafs (1024) per group.

        Variable 'maxchilds' controls this check. PyTables support up to
        4096 childs per group, but this would take too much memory (up
        to 64 MB) for testing purposes (may be we can add a test for big
        platforms). A 1024 childs run takes up to 30 MB. A 512 childs
        test takes around 25 MB.

        """

        import time
        maxchilds = 1000
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_wideTree..." % self.__class__.__name__)
            print("Maximum number of childs tested :", maxchilds)
        # Open a new empty HDF5 file
        #file = tempfile.mktemp(".h5")
        file = "test_widetree.h5"

        fileh = open_file(file, mode="w")
        if verbose:
            print("Children writing progress: ", end=' ')
        for child in range(maxchilds):
            if verbose:
                print("%3d," % (child), end=' ')
            a = [1, 1]
            fileh.create_group(fileh.root, 'group' + str(child),
                               "child: %d" % child)
            fileh.create_array("/group" + str(child), 'array' + str(child),
                               a, "child: %d" % child)
        if verbose:
            print()
        # Close the file
        fileh.close()

        t1 = time.time()
        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        print(("\nTime spent opening a file with %d groups + %d arrays: "
              "%s s" % (maxchilds, maxchilds, time.time() - t1)))
        if verbose:
            print("\nChildren reading progress: ", end=' ')
        # Close the file
        fileh.close()
        # Then, delete the file
        # os.remove(file)

    def test01_wideTree(self):
        """Checking creation of large number of groups (1024) per group.

        Variable 'maxchilds' controls this check. PyTables support up to
        4096 childs per group, but this would take too much memory (up
        to 64 MB) for testing purposes (may be we can add a test for big
        platforms). A 1024 childs run takes up to 30 MB. A 512 childs
        test takes around 25 MB.

        """

        import time
        maxchilds = 1000
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_wideTree..." % self.__class__.__name__)
            print("Maximum number of childs tested :", maxchilds)
        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        #file = "test_widetree.h5"

        fileh = open_file(file, mode="w")
        if verbose:
            print("Children writing progress: ", end=' ')
        for child in range(maxchilds):
            if verbose:
                print("%3d," % (child), end=' ')
            fileh.create_group(fileh.root, 'group' + str(child),
                               "child: %d" % child)
        if verbose:
            print()
        # Close the file
        fileh.close()

        t1 = time.time()
        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        print("\nTime spent opening a file with %d groups: %s s" %
              (maxchilds, time.time() - t1))
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()
    theSuite.addTest(unittest.makeSuite(WideTreeTestCase))

    return theSuite


if __name__ == '__main__':
    prof = hotshot.Profile("widetree.prof")
    benchtime, stones = prof.runcall(unittest.main(defaultTest='suite'))
    prof.close()
    stats = hotshot.stats.load("widetree.prof")
    stats.strip_dirs()
    stats.sort_stats('time', 'calls')
    stats.print_stats(20)

########NEW FILE########
__FILENAME__ = widetree2
from __future__ import print_function
import unittest

from tables import *
# Next imports are only necessary for this test suite
#from tables import Group, Leaf, Table, Array

verbose = 0


class Test(IsDescription):
    ngroup = Int32Col(pos=1)
    ntable = Int32Col(pos=2)
    nrow = Int32Col(pos=3)
    #string = StringCol(itemsize=500, pos=4)


class WideTreeTestCase(unittest.TestCase):

    def test00_Leafs(self):

        # Open a new empty HDF5 file
        filename = "test_widetree.h5"
        ngroups = 10
        ntables = 300
        nrows = 10
        complevel = 0
        complib = "lzo"

        print("Writing...")
        # Open a file in "w"rite mode
        fileh = open_file(filename, mode="w", title="PyTables Stress Test")

        for k in range(ngroups):
            # Create the group
            group = fileh.create_group("/", 'group%04d' % k, "Group %d" % k)

        fileh.close()

        # Now, create the tables
        rowswritten = 0
        for k in range(ngroups):
            print("Filling tables in group:", k)
            fileh = open_file(filename, mode="a", root_uep='group%04d' % k)
            # Get the group
            group = fileh.root
            for j in range(ntables):
                # Create a table
                table = fileh.create_table(group, 'table%04d' % j, Test,
                                           'Table%04d' % j,
                                           Filters(complevel, complib), nrows)
                # Get the row object associated with the new table
                row = table.row
                # Fill the table
                for i in range(nrows):
                    row['ngroup'] = k
                    row['ntable'] = j
                    row['nrow'] = i
                    row.append()

                rowswritten += nrows
                table.flush()

            # Close the file
            fileh.close()

        # read the file
        print("Reading...")
        rowsread = 0
        for ngroup in range(ngroups):
            fileh = open_file(filename, mode="r", root_uep='group%04d' %
                              ngroup)
            # Get the group
            group = fileh.root
            ntable = 0
            if verbose:
                print("Group ==>", group)
            for table in fileh.list_nodes(group, 'Table'):
                if verbose > 1:
                    print("Table ==>", table)
                    print("Max rows in buf:", table.nrowsinbuf)
                    print("Rows in", table._v_pathname, ":", table.nrows)
                    print("Buffersize:", table.rowsize * table.nrowsinbuf)
                    print("MaxTuples:", table.nrowsinbuf)

                nrow = 0
                for row in table:
                    try:
                        assert row["ngroup"] == ngroup
                        assert row["ntable"] == ntable
                        assert row["nrow"] == nrow
                    except:
                        print("Error in group: %d, table: %d, row: %d" %
                              (ngroup, ntable, nrow))
                        print("Record ==>", row)
                    nrow += 1

                assert nrow == table.nrows
                rowsread += table.nrows
                ntable += 1

            # Close the file (eventually destroy the extended type)
            fileh.close()


#----------------------------------------------------------------------
def suite():
    theSuite = unittest.TestSuite()
    theSuite.addTest(unittest.makeSuite(WideTreeTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = plot-speeds
"""Script for plotting the results of the 'suite' benchmark.
Invoke without parameters for usage hints.

:Author: Francesc Alted
:Date: 2010-06-01
"""

import matplotlib as mpl
from pylab import *

KB_ = 1024
MB_ = 1024*KB_
GB_ = 1024*MB_
NCHUNKS = 128    # keep in sync with bench.c

linewidth=2
#markers= ['+', ',', 'o', '.', 's', 'v', 'x', '>', '<', '^']
#markers= [ 'x', '+', 'o', 's', 'v', '^', '>', '<', ]
markers= [ 's', 'o', 'v', '^', '+', 'x', '>', '<', '.', ',' ]
markersize = 8

def get_values(filename):
    f = open(filename)
    values = {"memcpyw": [], "memcpyr": []}

    for line in f:
        if line.startswith('-->'):
            tmp = line.split('-->')[1]
            nthreads, size, elsize, sbits = [int(i) for i in tmp.split(', ')]
            values["size"] = size * NCHUNKS / MB_;
            values["elsize"] = elsize;
            values["sbits"] = sbits;
            # New run for nthreads
            (ratios, speedsw, speedsr) = ([], [], [])
            # Add a new entry for (ratios, speedw, speedr)
            values[nthreads] = (ratios, speedsw, speedsr)
            #print "-->", nthreads, size, elsize, sbits
        elif line.startswith('memcpy(write):'):
            tmp = line.split(',')[1]
            memcpyw = float(tmp.split(' ')[1])
            values["memcpyw"].append(memcpyw)
        elif line.startswith('memcpy(read):'):
            tmp = line.split(',')[1]
            memcpyr = float(tmp.split(' ')[1])
            values["memcpyr"].append(memcpyr)
        elif line.startswith('comp(write):'):
            tmp = line.split(',')[1]
            speedw = float(tmp.split(' ')[1])
            ratio = float(line.split(':')[-1])
            speedsw.append(speedw)
            ratios.append(ratio)
        elif line.startswith('decomp(read):'):
            tmp = line.split(',')[1]
            speedr = float(tmp.split(' ')[1])
            speedsr.append(speedr)
            if "OK" not in line:
                print "WARNING!  OK not found in decomp line!"

    f.close()
    return nthreads, values


def show_plot(plots, yaxis, legends, gtitle, xmax=None):
    xlabel('Compresssion ratio')
    ylabel('Speed (MB/s)')
    title(gtitle)
    xlim(0, xmax)
    #ylim(0, 10000)
    ylim(0, None)
    grid(True)

#     legends = [f[f.find('-'):f.index('.out')] for f in filenames]
#     legends = [l.replace('-', ' ') for l in legends]
    #legend([p[0] for p in plots], legends, loc = "upper left")
    legend([p[0] for p in plots
            if not isinstance(p, mpl.lines.Line2D)],
           legends, loc = "best")


    #subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2)
    if outfile:
        print "Saving plot to:", outfile
        savefig(outfile)
    else:
        show()

if __name__ == '__main__':

    from optparse import OptionParser

    usage = "usage: %prog [-o outfile] [-t title ] [-d|-c] filename"
    compress_title = 'Compression speed'
    decompress_title = 'Decompression speed'
    yaxis = 'No axis name'

    parser = OptionParser(usage=usage)
    parser.add_option('-o',
                      '--outfile',
                      dest='outfile',
                      help='filename for output ' + \
                      '(many extensions supported, e.g. .png, .jpg, .pdf)')

    parser.add_option('-t',
                      '--title',
                      dest='title',
                      help='title of the plot',)

    parser.add_option('-l',
                      '--limit',
                      dest='limit',
                      help='expression to limit number of threads shown',)

    parser.add_option('-x',
                      '--xmax',
                      dest='xmax',
                      help='limit the x-axis',
                      default=None)

    parser.add_option('-d', '--decompress', action='store_true',
            dest='dspeed',
            help='plot decompression data',
            default=False)
    parser.add_option('-c', '--compress', action='store_true',
            dest='cspeed',
            help='plot compression data',
            default=False)

    (options, args) = parser.parse_args()
    if len(args) == 0:
        parser.error("No input arguments")
    elif len(args) > 1:
        parser.error("Too many input arguments")
    else:
        pass

    if options.dspeed and options.cspeed:
        parser.error("Can only select one of [-d, -c]")
    elif options.cspeed:
        options.dspeed = False
        plot_title = compress_title
    else: # either neither or dspeed
        options.dspeed = True
        plot_title = decompress_title

    filename = args[0]
    outfile = options.outfile
    cspeed = options.cspeed
    dspeed = options.dspeed

    plots = []
    legends = []
    nthreads, values = get_values(filename)
    #print "Values:", values

    if options.limit:
        thread_range = eval(options.limit)
    else:
        thread_range = range(1, nthreads+1)

    if options.title:
        plot_title = options.title
    else:
        plot_title += " (%(size).1f MB, %(elsize)d bytes, %(sbits)d bits)" % values

    gtitle = plot_title

    for nt in thread_range:
        #print "Values for %s threads --> %s" % (nt, values[nt])
        (ratios, speedw, speedr) = values[nt]
        if cspeed:
            speed = speedw
        else:
            speed = speedr
        #plot_ = semilogx(ratios, speed, linewidth=2)
        plot_ = plot(ratios, speed, linewidth=2)
        plots.append(plot_)
        nmarker = nt
        if nt >= len(markers):
            nmarker = nt%len(markers)
        setp(plot_, marker=markers[nmarker], markersize=markersize,
             linewidth=linewidth)
        legends.append("%d threads" % nt)

    # Add memcpy lines
    if cspeed:
        mean = sum(values["memcpyw"]) / nthreads
        message = "memcpy (write to memory)"
    else:
        mean = sum(values["memcpyr"]) / nthreads
        message = "memcpy (read from memory)"
    plot_ = axhline(mean, linewidth=3, linestyle='-.', color='black')
    text(4.0, mean+50, message)
    plots.append(plot_)
    show_plot(plots, yaxis, legends, gtitle, xmax=int(options.xmax) if
            options.xmax else None)



########NEW FILE########
__FILENAME__ = make_hdf
#!/usr/bin/env python

from __future__ import generators

import tables, cPickle, time
#################################################################################


def is_scalar(item):
    try:
        iter(item)
        #could be a string
        try:
            item[:0]+'' #check for string
            return 'str'
        except:
            return 0
    except:
        return 'notstr'


def is_dict(item):
    try:
        item.iteritems()
        return 1
    except:
        return 0


def make_col(row_type, row_name, row_item, str_len):
    '''for strings it will always make at least 80 char or twice mac char size'''
    set_len=80
    if str_len:
        if 2*str_len>set_len:
            set_len=2*str_len
        row_type[row_name]=tables.Col("CharType", set_len)
    else:
        type_matrix={
            int: tables.Col("Int32", 1),
            float: tables.Col("Float32", 4), #Col("Int16", 1)
            }
        row_type[row_name]=type_matrix[type(row_item)]


def make_row(data):
    row_type={}
    scalar_type=is_scalar(data)
    if scalar_type:
        if scalar_type=='str':
            make_col(row_type, 'scalar', data, len(data))
        else:
            make_col(row_type, 'scalar', data, 0)
    else: #it is a list-like
        the_type=is_scalar(data[0])
        if the_type=='str':
            #get max length
            the_max=0
            for i in data:
              if len(i)>the_max:
                 the_max=len(i)
            make_col(row_type, 'col', data[0], the_max)
        elif the_type:
            make_col(row_type, 'col', data[0], 0)
        else: #list within the list, make many columns
            make_col(row_type, 'col_depth', 0, 0)
            count=0
            for col in data:
                the_type=is_scalar(col[0])
                if the_type=='str':
                    #get max length
                    the_max=0
                    for i in data:
                      if len(i)>the_max:
                         the_max=len(i)
                    make_col(row_type, 'col_'+str(count), col[0], the_max)
                elif the_type:
                    make_col(row_type, 'col_'+str(count), col[0], 0)
                else:
                    raise ValueError('too many nested levels of lists')
                count+=1
    return row_type


def add_table(fileh, group_obj, data, table_name):
    #figure out if it is a list of lists or a single list
    #get types of columns
    row_type=make_row(data)
    table1=fileh.createTable(group_obj, table_name, row_type, 'H', compress=1)
    row=table1.row

    if is_scalar(data):
        row['scalar']=data
        row.append()
    else:
        if is_scalar(data[0]):
            for i in data:
                row['col']=i
                row.append()
        else:
            count=0
            for col in data:
                row['col_depth']=len(col)
                for the_row in col:
                    if is_scalar(the_row):
                        row['col_'+str(count)]=the_row
                        row.append()
                    else:
                        raise ValueError('too many levels of lists')
                count+=1
    table1.flush()


def add_cache(fileh, cache):
    group_name='pytables_cache_v0';table_name='cache0'
    root=fileh.root
    group_obj=fileh.createGroup(root, group_name)
    cache_str=cPickle.dumps(cache, 0)
    cache_str=cache_str.replace('\n', chr(1))
    cache_pieces=[]
    while cache_str:
        cache_part=cache_str[:8000];cache_str=cache_str[8000:]
        if cache_part:
            cache_pieces.append(cache_part)
    row_type={}
    row_type['col_0']=tables.Col("CharType", 8000)
    #
    table_cache=fileh.createTable(group_obj, table_name, row_type, 'H', compress =1)
    for piece in cache_pieces:
        print len(piece)
        table_cache.row['col_0']=piece
        table_cache.row.append()
    table_cache.flush()


def save2(hdf_file, data):
    fileh=tables.openFile(hdf_file, mode='w', title='logon history')
    root=fileh.root;cache_root=cache={}
    root_path=root._v_pathname;root=0
    stack = [ (root_path, data, cache) ]
    table_num=0
    count=0

    while stack:
        (group_obj_path, data, cache)=stack.pop()
        #data='wilma':{'mother':[22,23,24]}}
        #grp_name wilma
        for grp_name in data:
            #print 'fileh=',fileh
            count+=1
            cache[grp_name]={}
            new_group_obj=fileh.createGroup(group_obj_path, grp_name)
            #print 'path=',new_group_obj._v_pathname
            new_path=new_group_obj._v_pathname
            #if dict, you have a bunch of groups
            if is_dict(data[grp_name]):#{'mother':[22,23,24]}
                stack.append((new_path, data[grp_name], cache[grp_name]))
            #you have a table
            else:
                #data[grp_name]=[110,130,140],[1,2,3]
                add_table(fileh, new_path, data[grp_name], 'tbl_'+str(table_num))
                table_num+=1

    #fileh=tables.openFile(hdf_file,mode='a',title='logon history')
    add_cache(fileh, cache_root)
    fileh.close()


########################
class Hdf_dict(dict):
    def __init__(self,hdf_file,hdf_dict={},stack=[]):
        self.hdf_file=hdf_file
        self.stack=stack
        if stack:
            self.hdf_dict=hdf_dict
        else:
            self.hdf_dict=self.get_cache()
        self.cur_dict=self.hdf_dict

    def get_cache(self):
        fileh=tables.openFile(self.hdf_file, rootUEP='pytables_cache_v0')
        table=fileh.root.cache0
        total=[]
        print 'reading'
        begin=time.time()
        for i in table.iterrows():
            total.append(i['col_0'])
        total=''.join(total)
        total=total.replace(chr(1), '\n')
        print 'loaded cache len=', len(total), time.time()-begin
        begin=time.time()
        a=cPickle.loads(total)
        print 'cache', time.time()-begin
        return a

    def has_key(self, k):
        return k in self.cur_dict

    def keys(self):
        return self.cur_dict.keys()

    def get(self,key,default=None):
        try:
            return self.__getitem__(key)
        except:
            return default

    def items(self):
        return list(self.iteritems())

    def values(self):
        return list(self.itervalues())


    ###########################################
    def __len__(self):
        return len(self.cur_dict)

    def __getitem__(self, k):
        if k in self.cur_dict:
            #now check if k has any data
            if self.cur_dict[k]:
                new_stack=self.stack[:]
                new_stack.append(k)
                return Hdf_dict(self.hdf_file, hdf_dict=self.cur_dict[k], stack=new_stack)
            else:
                new_stack=self.stack[:]
                new_stack.append(k)
                fileh=tables.openFile(self.hdf_file, rootUEP='/'.join(new_stack))
                #cur_data=getattr(self.cur_group,k) #/wilma (Group) '' =getattr(/ (Group) 'logon history',wilma)
                for table in fileh.root:
                    #return [ i['col_1'] for i in table.iterrows() ] #[9110,91]
                    #perhaps they stored a single item
                    try:
                        for item in table['scalar']:
                            return item
                    except:
                        #otherwise they stored a list of data
                        try:
                            return [ item for item in table['col']]
                        except:
                            cur_column=[]
                            total_columns=[]
                            col_num=0
                            cur_row=0
                            num_rows=0
                            for row in table:
                                if not num_rows:
                                    num_rows=row['col_depth']
                                if cur_row==num_rows:
                                    cur_row=num_rows=0
                                    col_num+=1
                                    total_columns.append(cur_column)
                                    cur_column=[]
                                cur_column.append( row['col_'+str(col_num)])
                                cur_row+=1
                            total_columns.append(cur_column)
                            return total_columns
        else:
            raise KeyError(k)

    def iterkeys(self):
        for key in self.iterkeys():
            yield key

    def __iter__(self):
        return self.iterkeys()

    def itervalues(self):
        for k in self.iterkeys():
            v=self.__getitem__(k)
            yield v

    def iteritems(self):
           # yield children
            for k in self.iterkeys():
                v=self.__getitem__(k)
                yield (k, v)

    def __repr__(self):
        return '{Hdf dict}'

    def __str__(self):
        return self.__repr__()

    #####
    def setdefault(self,key,default=None):
        try:
            return self.__getitem__(key)
        except:
            self.__setitem__(key)
            return default

    def update(self, d):
        for k, v in d.iteritems():
            self.__setitem__(k, v)

    def popitem(self):
        try:
            k, v = self.iteritems().next()
            del self[k]
            return k, v
        except StopIteration:
            raise KeyError("Hdf Dict is empty")

    def __setitem__(self, key, value):
        raise NotImplementedError

    def __delitem__(self, key):
        raise NotImplementedError

    def __hash__(self):
        raise TypeError("Hdf dict bjects are unhashable")


if __name__=='__main__':

    def write_small(file=''):
        data1={
        'fred':['a', 'b', 'c'],
        'barney':[[9110, 9130, 9140], [91, 92, 93]],
        'wilma':{'mother':{'pebbles':[22, 23, 24],'bambam':[67, 68, 69]}}
        }

        print 'saving'
        save2(file, data1)
        print 'saved'

    def read_small(file=''):
        #a=make_hdf.Hdf_dict(file)
        a=Hdf_dict(file)
        print a['wilma']
        b=a['wilma']
        for i in b:
            print i

        print a.keys()
        print 'has fred', bool('fred' in a)
        print 'length a', len(a)
        print 'get', a.get('fred'), a.get('not here')
        print 'wilma keys', a['wilma'].keys()
        print 'barney', a['barney']
        print 'get items'
        print a.items()
        for i in a.iteritems():
            print 'item', i
        for i in a.itervalues():
            print i

    a=raw_input('enter y to write out test file to test.hdf')
    if a.strip()=='y':
        print 'writing'
        write_small('test.hdf')
        print 'reading'
        read_small('test.hdf')

########NEW FILE########
__FILENAME__ = nctoh5
#!/usr/bin/env python

"""
convert netCDF file to HDF5 using Scientific.IO.NetCDF and PyTables.
Jeff Whitaker <jeffrey.s.whitaker@noaa.gov>

This requires Scientific from 
http://starship.python.net/~hinsen/ScientificPython

"""
from Scientific.IO import NetCDF
import tables, sys
# open netCDF file
ncfile = NetCDF.NetCDFFile(sys.argv[1], mode = "r")
# open h5 file.
h5file = tables.openFile(sys.argv[2], mode = "w")
# loop over variables in netCDF file.
for varname in ncfile.variables.keys():
    var = ncfile.variables[varname]
    vardims = list(var.dimensions)
    vardimsizes = [ncfile.dimensions[vardim] for vardim in vardims]
    # use long_name for title.
    if hasattr(var, 'long_name'):
       title = var.long_name
    else: # or, just use some bogus title.
       title = varname + ' array'
    # if variable has unlimited dimension or has rank>1,
    # make it enlargeable (with zlib compression).
    if vardimsizes[0] == None or len(vardimsizes) > 1:
        vardimsizes[0] = 0
        vardata = h5file.createEArray(h5file.root, varname,
        tables.Atom(shape=tuple(vardimsizes), dtype=var.typecode(),),
        title, filters=tables.Filters(complevel=6, complib='zlib'))
    # write data to enlargeable array on record at a time.
    # (so the whole array doesn't have to be kept in memory).
        for n in range(var.shape[0]):
            vardata.append(var[n:n+1])
    # or else, create regular array write data to it all at once.
    else:
        vardata=h5file.createArray(h5file.root, varname, var[:], title)
    # set variable attributes.
    for key, val in var.__dict__.iteritems():
        setattr(vardata.attrs, key, val)
    setattr(vardata.attrs, 'dimensions', tuple(vardims))
# set global (file) attributes.
for key, val in ncfile.__dict__.iteritems():
    setattr(h5file.root._v_attrs, key, val)
# Close the file
h5file.close()


########NEW FILE########
__FILENAME__ = filenode
# Copy this file into the clipboard and paste into 'script -c python'.

from __future__ import print_function
from tables.nodes import FileNode


import tables
h5file = tables.open_file('fnode.h5', 'w')


fnode = FileNode.new_node(h5file, where='/', name='fnode_test')


print(h5file.getAttrNode('/fnode_test', 'NODE_TYPE'))


print("This is a test text line.", file=fnode)
print("And this is another one.", file=fnode)
print(file=fnode)
fnode.write("Of course, file methods can also be used.")

fnode.seek(0)  # Go back to the beginning of file.

for line in fnode:
    print(repr(line))


fnode.close()
print(fnode.closed)


node = h5file.root.fnode_test
fnode = FileNode.open_node(node, 'a+')
print(repr(fnode.readline()))
print(fnode.tell())
print("This is a new line.", file=fnode)
print(repr(fnode.readline()))


fnode.seek(0)
for line in fnode:
    print(repr(line))


fnode.attrs.content_type = 'text/plain; charset=us-ascii'


fnode.attrs.author = "Ivan Vilata i Balaguer"
fnode.attrs.creation_date = '2004-10-20T13:25:25+0200'
fnode.attrs.keywords_en = ["FileNode", "test", "metadata"]
fnode.attrs.keywords_ca = ["FileNode", "prova", "metadades"]
fnode.attrs.owner = 'ivan'
fnode.attrs.acl = {'ivan': 'rw', '@users': 'r'}


fnode.close()
h5file.close()

########NEW FILE########
__FILENAME__ = pickletrouble
from __future__ import print_function
import tables


class MyClass(object):
    foo = 'bar'

# An object of my custom class.
myObject = MyClass()

h5f = tables.open_file('test.h5', 'w')
h5f.root._v_attrs.obj = myObject  # store the object
print(h5f.root._v_attrs.obj.foo)  # retrieve it
h5f.close()

# Delete class of stored object and reopen the file.
del MyClass, myObject

h5f = tables.open_file('test.h5', 'r')
print(h5f.root._v_attrs.obj.foo)
# Let us inspect the object to see what is happening.
print(repr(h5f.root._v_attrs.obj))
# Maybe unpickling the string will yield more information:
import pickle
pickle.loads(h5f.root._v_attrs.obj)
# So the problem was not in the stored object,
# but in the *environment* where it was restored.
h5f.close()

########NEW FILE########
__FILENAME__ = tutorial1
"""Small but quite comprehensive example showing the use of PyTables.

The program creates an output file, 'tutorial1.h5'.  You can view it
with any HDF5 generic utility.

"""


from __future__ import print_function
import os
import traceback

SECTION = "I HAVE NO TITLE"


def tutsep():
    print('----8<----', SECTION, '----8<----')


def tutprint(obj):
    tutsep()
    print(obj)


def tutrepr(obj):
    tutsep()
    print(repr(obj))


def tutexc():
    tutsep()
    traceback.print_exc(file=sys.stdout)


SECTION = "Importing tables objects"
from numpy import *
from tables import *


SECTION = "Declaring a Column Descriptor"
# Define a user record to characterize some kind of particles
class Particle(IsDescription):
    name      = StringCol(16)   # 16-character String
    idnumber  = Int64Col()      # Signed 64-bit integer
    ADCcount  = UInt16Col()     # Unsigned short integer
    TDCcount  = UInt8Col()      # unsigned byte
    grid_i    = Int32Col()      # integer
    grid_j    = IntCol()        # integer (equivalent to Int32Col)
    pressure  = Float32Col()    # float  (single-precision)
    energy    = FloatCol()      # double (double-precision)


SECTION = "Creating a PyTables file from scratch"
# Open a file in "w"rite mode
h5file = open_file('tutorial1.h5', mode="w", title="Test file")


SECTION = "Creating a new group"
# Create a new group under "/" (root)
group = h5file.create_group("/", 'detector', 'Detector information')


SECTION = "Creating a new table"
# Create one table on it
table = h5file.create_table(group, 'readout', Particle, "Readout example")

tutprint(h5file)
tutrepr(h5file)

# Get a shortcut to the record object in table
particle = table.row

# Fill the table with 10 particles
for i in range(10):
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i*i)
    particle['energy'] = float(particle['pressure'] ** 4)
    particle['idnumber'] = i * (2 ** 34)
    # Insert a new particle record
    particle.append()

# Flush the buffers for table
table.flush()


SECTION = "Reading (and selecting) data in a table"
# Read actual data from table. We are interested in collecting pressure values
# on entries where TDCcount field is greater than 3 and pressure less than 50
table = h5file.root.detector.readout
pressure = [
    x['pressure'] for x in table
    if x['TDCcount'] > 3 and 20 <= x['pressure'] < 50
]

tutrepr(pressure)

# Read also the names with the same cuts
names = [
    x['name'] for x in table
    if x['TDCcount'] > 3 and 20 <= x['pressure'] < 50
]

tutrepr(names)


SECTION = "Creating new array objects"
gcolumns = h5file.create_group(h5file.root, "columns", "Pressure and Name")

tutrepr(
    h5file.create_array(gcolumns, 'pressure', array(pressure),
                       "Pressure column selection")
)

tutrepr(
    h5file.create_array('/columns', 'name', names, "Name column selection")
)

tutprint(h5file)


SECTION = "Closing the file and looking at its content"
# Close the file
h5file.close()

tutsep()
os.system('h5ls -rd tutorial1.h5')
tutsep()
os.system('ptdump tutorial1.h5')


"""This example shows how to browse the object tree and enlarge tables.

Before to run this program you need to execute first tutorial1-1.py
that create the tutorial1.h5 file needed here.

"""


SECTION = "Traversing the object tree"
# Reopen the file in append mode
h5file = open_file("tutorial1.h5", "a")

# Print the object tree created from this filename
# List all the nodes (Group and Leaf objects) on tree
tutprint(h5file)

# List all the nodes (using File iterator) on tree
tutsep()
for node in h5file:
    print(node)

# Now, only list all the groups on tree
tutsep()
for group in h5file.walk_groups("/"):
    print(group)

# List only the arrays hanging from /
tutsep()
for group in h5file.walk_groups("/"):
    for array in h5file.list_nodes(group, classname='Array'):
        print(array)

# This gives the same result
tutsep()
for array in h5file.walk_nodes("/", "Array"):
    print(array)

# And finally, list only leafs on /detector group (there should be one!)
# Other way using iterators and natural naming
tutsep()
for leaf in h5file.root.detector('Leaf'):
    print(leaf)


SECTION = "Setting and getting user attributes"
# Get a pointer to '/detector/readout'
table = h5file.root.detector.readout

# Attach it a string (date) attribute
table.attrs.gath_date = "Wed, 06/12/2003 18:33"

# Attach a floating point attribute
table.attrs.temperature = 18.4
table.attrs.temp_scale = "Celsius"

# Get a pointer to '/detector'
detector = h5file.root.detector
# Attach a general object to the parent (/detector) group
detector._v_attrs.stuff = [5, (2.3, 4.5), "Integer and tuple"]

# Now, get the attributes
tutrepr(table.attrs.gath_date)
tutrepr(table.attrs.temperature)
tutrepr(table.attrs.temp_scale)
tutrepr(detector._v_attrs.stuff)

# Delete permanently the attribute gath_date of /detector/readout
del table.attrs.gath_date

# Print a representation of all attributes in  /detector/table
tutrepr(table.attrs)

# Get the (user) attributes of /detector/table
tutprint(table.attrs._f_list("user"))

# Get the (sys) attributes of /detector/table
tutprint(table.attrs._f_list("sys"))

# Rename an attribute
table.attrs._f_rename("temp_scale", "tempScale")
tutprint(table.attrs._f_list())

# Try to rename a system attribute:
try:
    table.attrs._f_rename("VERSION", "version")
except:
    tutexc()

h5file.flush()
tutsep()
os.system('h5ls -vr tutorial1.h5/detector/readout')


SECTION = "Getting object metadata"
# Get metadata from table
tutsep()
print("Object:", table)
tutsep()
print("Table name:", table.name)
tutsep()
print("Table title:", table.title)
tutsep()
print("Number of rows in table:", table.nrows)
tutsep()
print("Table variable names with their type and shape:")
tutsep()
for name in table.colnames:
    print(name, ':= %s, %s' % (table.coltypes[name], table.colshapes[name]))

tutprint(table.__doc__)

# Get the object in "/columns pressure"
pressureObject = h5file.get_node("/columns", "pressure")

# Get some metadata on this object
tutsep()
print("Info on the object:", repr(pressureObject))
tutsep()
print(" shape: ==>", pressureObject.shape)
tutsep()
print(" title: ==>", pressureObject.title)
tutsep()
print(" type: ==>", pressureObject.type)


SECTION = "Reading data from Array objects"
# Read the 'pressure' actual data
pressureArray = pressureObject.read()
tutrepr(pressureArray)
tutsep()
print("pressureArray is an object of type:", type(pressureArray))

# Read the 'name' Array actual data
nameArray = h5file.root.columns.name.read()
tutrepr(nameArray)
print("nameArray is an object of type:", type(nameArray))

# Print the data for both arrays
tutprint("Data on arrays nameArray and pressureArray:")
tutsep()
for i in range(pressureObject.shape[0]):
    print(nameArray[i], "-->", pressureArray[i])
tutrepr(pressureObject.name)


SECTION = "Appending data to an existing table"
# Create a shortcut to table object
table = h5file.root.detector.readout
# Get the object row from table
particle = table.row

# Append 5 new particles to table
for i in range(10, 15):
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i*i)
    particle['energy'] = float(particle['pressure'] ** 4)
    particle['idnumber'] = i * (2 ** 34)  # This exceeds long integer range
    particle.append()

# Flush this table
table.flush()

# Print the data using the table iterator:
tutsep()
for r in table:
    print("%-16s | %11.1f | %11.4g | %6d | %6d | %8d |" % \
          (r['name'], r['pressure'], r['energy'], r['grid_i'], r['grid_j'],
           r['TDCcount']))

# Delete some rows on the Table (yes, rows can be removed!)
tutrepr(table.remove_rows(5, 10))

# Close the file
h5file.close()

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# PyTables documentation build configuration file, created by
# sphinx-quickstart on Sun Feb  7 22:29:49 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../sphinxext'))
#sys.path.insert(0, os.path.abspath('../..'))


# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.doctest',
    'sphinx.ext.pngmath',
    'sphinx.ext.inheritance_diagram',
    'sphinx.ext.extlinks',
    'sphinx.ext.todo',
    'sphinx.ext.viewcode',
    'ipython_console_highlighting',
    'numpydoc',
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'PyTables'
copyright = u'2011-2014, PyTables maintainers'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
VERSION = open('../../VERSION').read().strip()
version = VERSION
# The full version, including alpha/beta/rc tags.
release = VERSION

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
#pygments_style = 'sphinx'
#pygments_style = 'friendly'
#pygments_style = 'bw'
#pygments_style = 'fruity'
#pygments_style = 'manni'
pygments_style = 'tango'


# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#html_theme = 'default'
#html_theme = 'altered_nature'
html_theme = 'cloud'
#html_theme = 'sphinxdoc'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
linkcolor = '#9F1E1E'
trimcolor = '#511755'
trimcolor = '#000000'

html_theme_options = {'sidebarbgcolor': 'rgba(213, 197, 229, 0.15)',
                      'sidebartextcolor': '#280941',
                      'sidebarlinkcolor': linkcolor,
                      'sidebartrimcolor': trimcolor,
                      'collapsiblesidebar': True,
                      'relbarbgcolor': '#006FFF',
                      'footerbgcolor': 'rgba(252, 255, 0, 0.125)',
                      'footertextcolor': '#504A4B',
                      'bodytrimcolor': trimcolor,
                      'linkcolor': linkcolor,
                      'textcolor': '#323039',
                      'sectionbgcolor': '#3CAD1C',
                      #'sectiontextcolor': '#777777',
                      #'sectiontrimcolor': trimcolor,
                      'codebgcolor': '#F1FFF0',
                      'codetextcolor': '#000000',
                      'quotebgcolor': '#f6fcfc',
                      'rubricbgcolor': '#D00000',
                      #'min_height': 'bottom',
                      }

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ["_theme"]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = '_static/logo-pytables-small.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'images/favicon.ico'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
html_sidebars = {
    'index': ['globaltoc.html', 'relations.html', 'sourcelink.html',
              'searchbox.html', 'travis-ci.html']
}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'pytablesdoc'



# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('usersguide/usersguide', 'usersguide-%s.tex' % version,
   u'PyTables User Guide', u'PyTables maintainers', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
latex_logo = 'usersguide/images/pytables-front-logo.pdf'

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
latex_use_parts = True

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# A dictionary that contains LaTeX snippets that override those Sphinx
# usually puts into the generated .tex files.
latex_elements = {
    'preamble': '\usepackage{bookmark,hyperref}',
}

# Documents to append as an appendix to all manuals.
#latex_appendices = [
#    'usersguide/datatypes',
#    'usersguide/condition_syntax',
#    'usersguide/parameter_files',
#    'usersguide/nested_rec_arrays',
#    'usersguide/utilities',
#    'usersguide/file_format',
#    'usersguide/bibliography',
#]

# If false, no module index is generated.
latex_domain_indices = False

# -- Options for autodocumentation ---------------------------------------------
autodoc_member_order = "groupwise"
autoclass_content = "class"
autosummary_generate = []

# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'PyTables'
epub_author = u'PyTables maintainers'
epub_publisher = u'PyTables maintainers'
epub_copyright = u'2011-2014, PyTables maintainers'

# -- External link oOptions ----------------------------------------------------
extlinks = {
    'issue': ('https://github.com/PyTables/PyTables/issues/%s', 'gh-'),
    'irc': ('http://pytables.github.io/irc/%s.html', ''),
}

########NEW FILE########
__FILENAME__ = pytables_test
from tables import *
import numarray

class Particle(IsDescription):
    name = StringCol(16) # 16-character String
    idnumber = Int64Col() # Signed 64-bit integer
    ADCcount = UInt16Col() # Unsigned short integer
    TDCcount = UInt8Col() # Unsigned byte
    grid_i = Int32Col() # Integer
    grid_j = IntCol() # Integer (equivalent to Int32Col)
    pressure = Float32Col() # Float (single-precision)
    energy = FloatCol() # Double (double-precision)

h5file = openFile("tutorial.h5", mode="w", title="Test file")
group = h5file.createGroup("/", "detector", "Detector information")
table = h5file.createTable(group, "readout", Particle, "Readout example")

print h5file

particle = table.row

for i in xrange(10):
    particle['name'] = 'Particle: %6d' % i
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i*256) % (1<<16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i*i)
    particle['energy'] = float(particle['pressure']**4)
    particle['idnumber'] = i * (2**34)
    particle.append()

table.flush()

table = h5file.root.detector.readout
pressure = [x['pressure'] for x in table.iterrows() if x['TDCcount']>3 and
                                                       20<=x['pressure']<50]

print pressure

h5file.close()

########NEW FILE########
__FILENAME__ = apigen
"""Attempt to generate templates for module reference with Sphinx

XXX - we exclude extension modules

To include extension modules, first identify them as valid in the
``_uri2path`` method, then handle them in the ``_parse_module`` script.

We get functions and classes by parsing the text of .py files.
Alternatively we could import the modules for discovery, and we'd have
to do that for extension modules.  This would involve changing the
``_parse_module`` method to work via import and introspection, and
might involve changing ``discover_modules`` (which determines which
files are modules, and therefore which module URIs will be passed to
``_parse_module``).

NOTE: this is a modified version of a script originally shipped with the
PyMVPA project, which we've adapted for NIPY use.  PyMVPA is an MIT-licensed
project."""

# Stdlib imports
import os
import re

# Functions and classes
class ApiDocWriter(object):
    ''' Class for automatic detection and parsing of API docs
    to Sphinx-parsable reST format'''

    # only separating first two levels
    rst_section_levels = ['*', '=', '-', '~', '^']

    def __init__(self,
                 package_name,
                 rst_extension='.rst',
                 package_skip_patterns=None,
                 module_skip_patterns=None,
                 ):
        ''' Initialize package for parsing

        Parameters
        ----------
        package_name : string
            Name of the top-level package.  *package_name* must be the
            name of an importable package
        rst_extension : string, optional
            Extension for reST files, default '.rst'
        package_skip_patterns : None or sequence of {strings, regexps}
            Sequence of strings giving URIs of packages to be excluded
            Operates on the package path, starting at (including) the
            first dot in the package path, after *package_name* - so,
            if *package_name* is ``sphinx``, then ``sphinx.util`` will
            result in ``.util`` being passed for earching by these
            regexps.  If is None, gives default. Default is:
            ['\.tests$']
        module_skip_patterns : None or sequence
            Sequence of strings giving URIs of modules to be excluded
            Operates on the module name including preceding URI path,
            back to the first dot after *package_name*.  For example
            ``sphinx.util.console`` results in the string to search of
            ``.util.console``
            If is None, gives default. Default is:
            ['\.setup$', '\._']
        '''
        if package_skip_patterns is None:
            package_skip_patterns = ['\\.tests$']
        if module_skip_patterns is None:
            module_skip_patterns = ['\\.setup$', '\\._']
        self.package_name = package_name
        self.rst_extension = rst_extension
        self.package_skip_patterns = package_skip_patterns
        self.module_skip_patterns = module_skip_patterns

    def get_package_name(self):
        return self._package_name

    def set_package_name(self, package_name):
        ''' Set package_name

        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> docwriter.root_path == sphinx.__path__[0]
        True
        >>> docwriter.package_name = 'docutils'
        >>> import docutils
        >>> docwriter.root_path == docutils.__path__[0]
        True
        '''
        # It's also possible to imagine caching the module parsing here
        self._package_name = package_name
        self.root_module = __import__(package_name)
        self.root_path = self.root_module.__path__[0]
        self.written_modules = None

    package_name = property(get_package_name, set_package_name, None,
                            'get/set package_name')

    def _get_object_name(self, line):
        ''' Get second token in line
        >>> docwriter = ApiDocWriter('sphinx')
        >>> docwriter._get_object_name("  def func():  ")
        'func'
        >>> docwriter._get_object_name("  class Klass(object):  ")
        'Klass'
        >>> docwriter._get_object_name("  class Klass:  ")
        'Klass'
        '''
        name = line.split()[1].split('(')[0].strip()
        # in case we have classes which are not derived from object
        # ie. old style classes
        return name.rstrip(':')

    def _uri2path(self, uri):
        ''' Convert uri to absolute filepath

        Parameters
        ----------
        uri : string
            URI of python module to return path for

        Returns
        -------
        path : None or string
            Returns None if there is no valid path for this URI
            Otherwise returns absolute file system path for URI

        Examples
        --------
        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> modpath = sphinx.__path__[0]
        >>> res = docwriter._uri2path('sphinx.builder')
        >>> res == os.path.join(modpath, 'builder.py')
        True
        >>> res = docwriter._uri2path('sphinx')
        >>> res == os.path.join(modpath, '__init__.py')
        True
        >>> docwriter._uri2path('sphinx.does_not_exist')

        '''
        if uri == self.package_name:
            return os.path.join(self.root_path, '__init__.py')
        path = uri.replace('.', os.path.sep)
        path = path.replace(self.package_name + os.path.sep, '')
        path = os.path.join(self.root_path, path)
        # XXX maybe check for extensions as well?
        if os.path.exists(path + '.py'): # file
            path += '.py'
        elif os.path.exists(os.path.join(path, '__init__.py')):
            path = os.path.join(path, '__init__.py')
        else:
            return None
        return path

    def _path2uri(self, dirpath):
        ''' Convert directory path to uri '''
        relpath = dirpath.replace(self.root_path, self.package_name)
        if relpath.startswith(os.path.sep):
            relpath = relpath[1:]
        return relpath.replace(os.path.sep, '.')

    def _parse_module(self, uri):
        ''' Parse module defined in *uri* '''
        filename = self._uri2path(uri)
        if filename is None:
            # nothing that we could handle here.
            return ([], [])
        f = open(filename, 'rt')
        functions, classes = self._parse_lines(f)
        f.close()
        return functions, classes
    
    def _parse_lines(self, linesource):
        ''' Parse lines of text for functions and classes '''
        functions = []
        classes = []
        for line in linesource:
            if line.startswith('def ') and line.count('('):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    functions.append(name)
            elif line.startswith('class '):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    classes.append(name)
            else:
                pass
        functions.sort()
        classes.sort()
        return functions, classes

    def generate_api_doc(self, uri):
        '''Make autodoc documentation template string for a module

        Parameters
        ----------
        uri : string
            python location of module - e.g 'sphinx.builder'

        Returns
        -------
        S : string
            Contents of API doc
        '''
        # get the names of all classes and functions
        functions, classes = self._parse_module(uri)
        if not len(functions) and not len(classes):
            print 'WARNING: Empty -', uri  # dbg
            return ''

        # Make a shorter version of the uri that omits the package name for
        # titles 
        uri_short = re.sub(r'^%s\.' % self.package_name, '', uri)
        
        ad = '.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n'

        chap_title = uri_short
        ad += (chap_title+'\n'+ self.rst_section_levels[1] * len(chap_title)
               + '\n\n')

        # Set the chapter title to read 'module' for all modules except for the
        # main packages
        if '.' in uri:
            title = 'Module: :mod:`' + uri_short + '`'
        else:
            title = ':mod:`' + uri_short + '`'
        ad += title + '\n' + self.rst_section_levels[2] * len(title)

        if len(classes):
            ad += '\nInheritance diagram for ``%s``:\n\n' % uri
            ad += '.. inheritance-diagram:: %s \n' % uri
            ad += '   :parts: 3\n'

        ad += '\n.. automodule:: ' + uri + '\n'
        ad += '\n.. currentmodule:: ' + uri + '\n'
        multi_class = len(classes) > 1
        multi_fx = len(functions) > 1
        if multi_class:
            ad += '\n' + 'Classes' + '\n' + \
                  self.rst_section_levels[2] * 7 + '\n'
        elif len(classes) and multi_fx:
            ad += '\n' + 'Class' + '\n' + \
                  self.rst_section_levels[2] * 5 + '\n'
        for c in classes:
            ad += '\n:class:`' + c + '`\n' \
                  + self.rst_section_levels[multi_class + 2 ] * \
                  (len(c)+9) + '\n\n'
            ad += '\n.. autoclass:: ' + c + '\n'
            # must NOT exclude from index to keep cross-refs working
            ad += '  :members:\n' \
                  '  :undoc-members:\n' \
                  '  :show-inheritance:\n' \
                  '  :inherited-members:\n' \
                  '\n' \
                  '  .. automethod:: __init__\n'
        if multi_fx:
            ad += '\n' + 'Functions' + '\n' + \
                  self.rst_section_levels[2] * 9 + '\n\n'
        elif len(functions) and multi_class:
            ad += '\n' + 'Function' + '\n' + \
                  self.rst_section_levels[2] * 8 + '\n\n'
        for f in functions:
            # must NOT exclude from index to keep cross-refs working
            ad += '\n.. autofunction:: ' + uri + '.' + f + '\n\n'
        return ad

    def _survives_exclude(self, matchstr, match_type):
        ''' Returns True if *matchstr* does not match patterns

        ``self.package_name`` removed from front of string if present

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> dw._survives_exclude('sphinx.okpkg', 'package')
        True
        >>> dw.package_skip_patterns.append('^\\.badpkg$')
        >>> dw._survives_exclude('sphinx.badpkg', 'package')
        False
        >>> dw._survives_exclude('sphinx.badpkg', 'module')
        True
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        True
        >>> dw.module_skip_patterns.append('^\\.badmod$')
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        False
        '''
        if match_type == 'module':
            patterns = self.module_skip_patterns
        elif match_type == 'package':
            patterns = self.package_skip_patterns
        else:
            raise ValueError('Cannot interpret match type "%s"' 
                             % match_type)
        # Match to URI without package name
        L = len(self.package_name)
        if matchstr[:L] == self.package_name:
            matchstr = matchstr[L:]
        for pat in patterns:
            try:
                pat.search
            except AttributeError:
                pat = re.compile(pat)
            if pat.search(matchstr):
                return False
        return True

    def discover_modules(self):
        ''' Return module sequence discovered from ``self.package_name`` 


        Parameters
        ----------
        None

        Returns
        -------
        mods : sequence
            Sequence of module names within ``self.package_name``

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> mods = dw.discover_modules()
        >>> 'sphinx.util' in mods
        True
        >>> dw.package_skip_patterns.append('\.util$')
        >>> 'sphinx.util' in dw.discover_modules()
        False
        >>> 
        '''
        modules = [self.package_name]
        # raw directory parsing
        for dirpath, dirnames, filenames in os.walk(self.root_path):
            # Check directory names for packages
            root_uri = self._path2uri(os.path.join(self.root_path,
                                                   dirpath))
            for dirname in dirnames[:]: # copy list - we modify inplace
                package_uri = '.'.join((root_uri, dirname))
                if (self._uri2path(package_uri) and
                    self._survives_exclude(package_uri, 'package')):
                    modules.append(package_uri)
                else:
                    dirnames.remove(dirname)
            # Check filenames for modules
            for filename in filenames:
                module_name = filename[:-3]
                module_uri = '.'.join((root_uri, module_name))
                if (self._uri2path(module_uri) and
                    self._survives_exclude(module_uri, 'module')):
                    modules.append(module_uri)
        return sorted(modules)
    
    def write_modules_api(self, modules, outdir):
        # write the list
        written_modules = []
        for m in modules:
            api_str = self.generate_api_doc(m)
            if not api_str:
                continue
            # write out to file
            outfile = os.path.join(outdir,
                                   m + self.rst_extension)
            fileobj = open(outfile, 'wt')
            fileobj.write(api_str)
            fileobj.close()
            written_modules.append(m)
        self.written_modules = written_modules

    def write_api_docs(self, outdir):
        """Generate API reST files.

        Parameters
        ----------
        outdir : string
            Directory name in which to store files
            We create automatic filenames for each module
            
        Returns
        -------
        None

        Notes
        -----
        Sets self.written_modules to list of written modules
        """
        if not os.path.exists(outdir):
            os.mkdir(outdir)
        # compose list of modules
        modules = self.discover_modules()
        self.write_modules_api(modules, outdir)
        
    def write_index(self, outdir, froot='gen', relative_to=None):
        """Make a reST API index file from written files

        Parameters
        ----------
        path : string
            Filename to write index to
        outdir : string
            Directory to which to write generated index file
        froot : string, optional
            root (filename without extension) of filename to write to
            Defaults to 'gen'.  We add ``self.rst_extension``.
        relative_to : string
            path to which written filenames are relative.  This
            component of the written file path will be removed from
            outdir, in the generated index.  Default is None, meaning,
            leave path as it is.
        """
        if self.written_modules is None:
            raise ValueError('No modules written')
        # Get full filename path
        path = os.path.join(outdir, froot+self.rst_extension)
        # Path written into index is relative to rootpath
        if relative_to is not None:
            relpath = outdir.replace(relative_to + os.path.sep, '')
        else:
            relpath = outdir
        idx = open(path, 'wt')
        w = idx.write
        w('.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n')
        w('.. toctree::\n\n')
        for f in self.written_modules:
            w('   %s\n' % os.path.join(relpath, f))
        idx.close()

########NEW FILE########
__FILENAME__ = comment_eater
from cStringIO import StringIO
import compiler
import inspect
import textwrap
import tokenize

from compiler_unparse import unparse


class Comment(object):
    """ A comment block.
    """
    is_comment = True
    def __init__(self, start_lineno, end_lineno, text):
        # int : The first line number in the block. 1-indexed.
        self.start_lineno = start_lineno
        # int : The last line number. Inclusive!
        self.end_lineno = end_lineno
        # str : The text block including '#' character but not any leading spaces.
        self.text = text

    def add(self, string, start, end, line):
        """ Add a new comment line.
        """
        self.start_lineno = min(self.start_lineno, start[0])
        self.end_lineno = max(self.end_lineno, end[0])
        self.text += string

    def __repr__(self):
        return '%s(%r, %r, %r)' % (self.__class__.__name__, self.start_lineno,
            self.end_lineno, self.text)


class NonComment(object):
    """ A non-comment block of code.
    """
    is_comment = False
    def __init__(self, start_lineno, end_lineno):
        self.start_lineno = start_lineno
        self.end_lineno = end_lineno

    def add(self, string, start, end, line):
        """ Add lines to the block.
        """
        if string.strip():
            # Only add if not entirely whitespace.
            self.start_lineno = min(self.start_lineno, start[0])
            self.end_lineno = max(self.end_lineno, end[0])

    def __repr__(self):
        return '%s(%r, %r)' % (self.__class__.__name__, self.start_lineno,
            self.end_lineno)


class CommentBlocker(object):
    """ Pull out contiguous comment blocks.
    """
    def __init__(self):
        # Start with a dummy.
        self.current_block = NonComment(0, 0)

        # All of the blocks seen so far.
        self.blocks = []

        # The index mapping lines of code to their associated comment blocks.
        self.index = {}

    def process_file(self, file):
        """ Process a file object.
        """
        for token in tokenize.generate_tokens(file.next):
            self.process_token(*token)
        self.make_index()

    def process_token(self, kind, string, start, end, line):
        """ Process a single token.
        """
        if self.current_block.is_comment:
            if kind == tokenize.COMMENT:
                self.current_block.add(string, start, end, line)
            else:
                self.new_noncomment(start[0], end[0])
        else:
            if kind == tokenize.COMMENT:
                self.new_comment(string, start, end, line)
            else:
                self.current_block.add(string, start, end, line)

    def new_noncomment(self, start_lineno, end_lineno):
        """ We are transitioning from a noncomment to a comment.
        """
        block = NonComment(start_lineno, end_lineno)
        self.blocks.append(block)
        self.current_block = block

    def new_comment(self, string, start, end, line):
        """ Possibly add a new comment.
        
        Only adds a new comment if this comment is the only thing on the line.
        Otherwise, it extends the noncomment block.
        """
        prefix = line[:start[1]]
        if prefix.strip():
            # Oops! Trailing comment, not a comment block.
            self.current_block.add(string, start, end, line)
        else:
            # A comment block.
            block = Comment(start[0], end[0], string)
            self.blocks.append(block)
            self.current_block = block

    def make_index(self):
        """ Make the index mapping lines of actual code to their associated
        prefix comments.
        """
        for prev, block in zip(self.blocks[:-1], self.blocks[1:]):
            if not block.is_comment:
                self.index[block.start_lineno] = prev

    def search_for_comment(self, lineno, default=None):
        """ Find the comment block just before the given line number.

        Returns None (or the specified default) if there is no such block.
        """
        if not self.index:
            self.make_index()
        block = self.index.get(lineno, None)
        text = getattr(block, 'text', default)
        return text


def strip_comment_marker(text):
    """ Strip # markers at the front of a block of comment text.
    """
    lines = []
    for line in text.splitlines():
        lines.append(line.lstrip('#'))
    text = textwrap.dedent('\n'.join(lines))
    return text


def get_class_traits(klass):
    """ Yield all of the documentation for trait definitions on a class object.
    """
    # FIXME: gracefully handle errors here or in the caller?
    source = inspect.getsource(klass)
    cb = CommentBlocker()
    cb.process_file(StringIO(source))
    mod_ast = compiler.parse(source)
    class_ast = mod_ast.node.nodes[0]
    for node in class_ast.code.nodes:
        # FIXME: handle other kinds of assignments?
        if isinstance(node, compiler.ast.Assign):
            name = node.nodes[0].name
            rhs = unparse(node.expr).strip()
            doc = strip_comment_marker(cb.search_for_comment(node.lineno, default=''))
            yield name, rhs, doc


########NEW FILE########
__FILENAME__ = compiler_unparse
""" Turn compiler.ast structures back into executable python code.

    The unparse method takes a compiler.ast tree and transforms it back into
    valid python code.  It is incomplete and currently only works for
    import statements, function calls, function definitions, assignments, and
    basic expressions.

    Inspired by python-2.5-svn/Demo/parser/unparse.py

    fixme: We may want to move to using _ast trees because the compiler for
           them is about 6 times faster than compiler.compile.
"""

import sys
import cStringIO
from compiler.ast import Const, Name, Tuple, Div, Mul, Sub, Add

def unparse(ast, single_line_functions=False):
    s = cStringIO.StringIO()
    UnparseCompilerAst(ast, s, single_line_functions)
    return s.getvalue().lstrip()

op_precedence = { 'compiler.ast.Power':3, 'compiler.ast.Mul':2, 'compiler.ast.Div':2,
                  'compiler.ast.Add':1, 'compiler.ast.Sub':1 }

class UnparseCompilerAst:
    """ Methods in this class recursively traverse an AST and
        output source code for the abstract syntax; original formatting
        is disregarged.
    """

    #########################################################################
    # object interface.
    #########################################################################

    def __init__(self, tree, file = sys.stdout, single_line_functions=False):
        """ Unparser(tree, file=sys.stdout) -> None.

            Print the source for tree to file.
        """
        self.f = file
        self._single_func = single_line_functions
        self._do_indent = True
        self._indent = 0
        self._dispatch(tree)
        self._write("\n")
        self.f.flush()

    #########################################################################
    # Unparser private interface.
    #########################################################################

    ### format, output, and dispatch methods ################################

    def _fill(self, text = ""):
        "Indent a piece of text, according to the current indentation level"
        if self._do_indent:
            self._write("\n"+"    "*self._indent + text)
        else:
            self._write(text)

    def _write(self, text):
        "Append a piece of text to the current line."
        self.f.write(text)

    def _enter(self):
        "Print ':', and increase the indentation."
        self._write(": ")
        self._indent += 1

    def _leave(self):
        "Decrease the indentation level."
        self._indent -= 1

    def _dispatch(self, tree):
        "_dispatcher function, _dispatching tree type T to method _T."
        if isinstance(tree, list):
            for t in tree:
                self._dispatch(t)
            return
        meth = getattr(self, "_"+tree.__class__.__name__)
        if tree.__class__.__name__ == 'NoneType' and not self._do_indent:
            return
        meth(tree)


    #########################################################################
    # compiler.ast unparsing methods.
    #
    # There should be one method per concrete grammar type. They are
    # organized in alphabetical order.
    #########################################################################

    def _Add(self, t):
        self.__binary_op(t, '+')

    def _And(self, t):
        self._write(" (")
        for i, node in enumerate(t.nodes):
            self._dispatch(node)
            if i != len(t.nodes)-1:
                self._write(") and (")
        self._write(")")
               
    def _AssAttr(self, t):
        """ Handle assigning an attribute of an object
        """
        self._dispatch(t.expr)
        self._write('.'+t.attrname)
 
    def _Assign(self, t):
        """ Expression Assignment such as "a = 1".

            This only handles assignment in expressions.  Keyword assignment
            is handled separately.
        """
        self._fill()
        for target in t.nodes:
            self._dispatch(target)
            self._write(" = ")
        self._dispatch(t.expr)
        if not self._do_indent:
            self._write('; ')

    def _AssName(self, t):
        """ Name on left hand side of expression.

            Treat just like a name on the right side of an expression.
        """
        self._Name(t)

    def _AssTuple(self, t):
        """ Tuple on left hand side of an expression.
        """

        # _write each elements, separated by a comma.
        for element in t.nodes[:-1]:
            self._dispatch(element)
            self._write(", ")

        # Handle the last one without writing comma
        last_element = t.nodes[-1]
        self._dispatch(last_element)

    def _AugAssign(self, t):
        """ +=,-=,*=,/=,**=, etc. operations
        """
        
        self._fill()
        self._dispatch(t.node)
        self._write(' '+t.op+' ')
        self._dispatch(t.expr)
        if not self._do_indent:
            self._write(';')
            
    def _Bitand(self, t):
        """ Bit and operation.
        """
        
        for i, node in enumerate(t.nodes):
            self._write("(")
            self._dispatch(node)
            self._write(")")
            if i != len(t.nodes)-1:
                self._write(" & ")
                
    def _Bitor(self, t):
        """ Bit or operation
        """
        
        for i, node in enumerate(t.nodes):
            self._write("(")
            self._dispatch(node)
            self._write(")")
            if i != len(t.nodes)-1:
                self._write(" | ")
                
    def _CallFunc(self, t):
        """ Function call.
        """
        self._dispatch(t.node)
        self._write("(")
        comma = False
        for e in t.args:
            if comma: self._write(", ")
            else: comma = True
            self._dispatch(e)
        if t.star_args:
            if comma: self._write(", ")
            else: comma = True
            self._write("*")
            self._dispatch(t.star_args)
        if t.dstar_args:
            if comma: self._write(", ")
            else: comma = True
            self._write("**")
            self._dispatch(t.dstar_args)
        self._write(")")

    def _Compare(self, t):
        self._dispatch(t.expr)
        for op, expr in t.ops:
            self._write(" " + op + " ")
            self._dispatch(expr)

    def _Const(self, t):
        """ A constant value such as an integer value, 3, or a string, "hello".
        """
        self._dispatch(t.value)

    def _Decorators(self, t):
        """ Handle function decorators (eg. @has_units)
        """
        for node in t.nodes:
            self._dispatch(node)

    def _Dict(self, t):
        self._write("{")
        for  i, (k, v) in enumerate(t.items):
            self._dispatch(k)
            self._write(": ")
            self._dispatch(v)
            if i < len(t.items)-1:
                self._write(", ")
        self._write("}")

    def _Discard(self, t):
        """ Node for when return value is ignored such as in "foo(a)".
        """
        self._fill()
        self._dispatch(t.expr)

    def _Div(self, t):
        self.__binary_op(t, '/')

    def _Ellipsis(self, t):
        self._write("...")

    def _From(self, t):
        """ Handle "from xyz import foo, bar as baz".
        """
        # fixme: Are From and ImportFrom handled differently?
        self._fill("from ")
        self._write(t.modname)
        self._write(" import ")
        for i, (name, asname) in enumerate(t.names):
            if i != 0:
                self._write(", ")
            self._write(name)
            if asname is not None:
                self._write(" as "+asname)
                
    def _Function(self, t):
        """ Handle function definitions
        """
        if t.decorators is not None:
            self._fill("@")
            self._dispatch(t.decorators)
        self._fill("def "+t.name + "(")
        defaults = [None] * (len(t.argnames) - len(t.defaults)) + list(t.defaults)
        for i, arg in enumerate(zip(t.argnames, defaults)):
            self._write(arg[0])
            if arg[1] is not None:
                self._write('=')
                self._dispatch(arg[1])
            if i < len(t.argnames)-1:
                self._write(', ')
        self._write(")")
        if self._single_func:
            self._do_indent = False
        self._enter()
        self._dispatch(t.code)
        self._leave()
        self._do_indent = True

    def _Getattr(self, t):
        """ Handle getting an attribute of an object
        """
        if isinstance(t.expr, (Div, Mul, Sub, Add)):
            self._write('(')
            self._dispatch(t.expr)
            self._write(')')
        else:
            self._dispatch(t.expr)
            
        self._write('.'+t.attrname)
        
    def _If(self, t):
        self._fill()
        
        for i, (compare, code) in enumerate(t.tests):
            if i == 0:
                self._write("if ")
            else:
                self._write("elif ")
            self._dispatch(compare)
            self._enter()
            self._fill()
            self._dispatch(code)
            self._leave()
            self._write("\n")

        if t.else_ is not None:
            self._write("else")
            self._enter()
            self._fill()
            self._dispatch(t.else_)
            self._leave()
            self._write("\n")
            
    def _IfExp(self, t):
        self._dispatch(t.then)
        self._write(" if ")
        self._dispatch(t.test)

        if t.else_ is not None:
            self._write(" else (")
            self._dispatch(t.else_)
            self._write(")")

    def _Import(self, t):
        """ Handle "import xyz.foo".
        """
        self._fill("import ")
        
        for i, (name, asname) in enumerate(t.names):
            if i != 0:
                self._write(", ")
            self._write(name)
            if asname is not None:
                self._write(" as "+asname)

    def _Keyword(self, t):
        """ Keyword value assignment within function calls and definitions.
        """
        self._write(t.name)
        self._write("=")
        self._dispatch(t.expr)
        
    def _List(self, t):
        self._write("[")
        for  i, node in enumerate(t.nodes):
            self._dispatch(node)
            if i < len(t.nodes)-1:
                self._write(", ")
        self._write("]")

    def _Module(self, t):
        if t.doc is not None:
            self._dispatch(t.doc)
        self._dispatch(t.node)

    def _Mul(self, t):
        self.__binary_op(t, '*')

    def _Name(self, t):
        self._write(t.name)

    def _NoneType(self, t):
        self._write("None")
        
    def _Not(self, t):
        self._write('not (')
        self._dispatch(t.expr)
        self._write(')')
        
    def _Or(self, t):
        self._write(" (")
        for i, node in enumerate(t.nodes):
            self._dispatch(node)
            if i != len(t.nodes)-1:
                self._write(") or (")
        self._write(")")
                
    def _Pass(self, t):
        self._write("pass\n")

    def _Printnl(self, t):
        self._fill("print ")
        if t.dest:
            self._write(">> ")
            self._dispatch(t.dest)
            self._write(", ")
        comma = False
        for node in t.nodes:
            if comma: self._write(', ')
            else: comma = True
            self._dispatch(node)

    def _Power(self, t):
        self.__binary_op(t, '**')

    def _Return(self, t):
        self._fill("return ")
        if t.value:
            if isinstance(t.value, Tuple):
                text = ', '.join([ name.name for name in t.value.asList() ])
                self._write(text)
            else:
                self._dispatch(t.value)
            if not self._do_indent:
                self._write('; ')

    def _Slice(self, t):
        self._dispatch(t.expr)
        self._write("[")
        if t.lower:
            self._dispatch(t.lower)
        self._write(":")
        if t.upper:
            self._dispatch(t.upper)
        #if t.step:
        #    self._write(":")
        #    self._dispatch(t.step)
        self._write("]")

    def _Sliceobj(self, t):
        for i, node in enumerate(t.nodes):
            if i != 0:
                self._write(":")
            if not (isinstance(node, Const) and node.value is None):
                self._dispatch(node)

    def _Stmt(self, tree):
        for node in tree.nodes:
            self._dispatch(node)

    def _Sub(self, t):
        self.__binary_op(t, '-')

    def _Subscript(self, t):
        self._dispatch(t.expr)
        self._write("[")
        for i, value in enumerate(t.subs):
            if i != 0:
                self._write(",")
            self._dispatch(value)
        self._write("]")

    def _TryExcept(self, t):
        self._fill("try")
        self._enter()
        self._dispatch(t.body)
        self._leave()

        for handler in t.handlers:
            self._fill('except ')
            self._dispatch(handler[0])
            if handler[1] is not None:
                self._write(', ')
                self._dispatch(handler[1])
            self._enter()
            self._dispatch(handler[2])
            self._leave()
            
        if t.else_:
            self._fill("else")
            self._enter()
            self._dispatch(t.else_)
            self._leave()

    def _Tuple(self, t):

        if not t.nodes:
            # Empty tuple.
            self._write("()")
        else:
            self._write("(")

            # _write each elements, separated by a comma.
            for element in t.nodes[:-1]:
                self._dispatch(element)
                self._write(", ")

            # Handle the last one without writing comma
            last_element = t.nodes[-1]
            self._dispatch(last_element)

            self._write(")")
            
    def _UnaryAdd(self, t):
        self._write("+")
        self._dispatch(t.expr)
        
    def _UnarySub(self, t):
        self._write("-")
        self._dispatch(t.expr)        

    def _With(self, t):
        self._fill('with ')
        self._dispatch(t.expr)
        if t.vars:
            self._write(' as ')
            self._dispatch(t.vars.name)
        self._enter()
        self._dispatch(t.body)
        self._leave()
        self._write('\n')
        
    def _int(self, t):
        self._write(repr(t))

    def __binary_op(self, t, symbol):
        # Check if parenthesis are needed on left side and then dispatch
        has_paren = False
        left_class = str(t.left.__class__)
        if (left_class in op_precedence.keys() and
            op_precedence[left_class] < op_precedence[str(t.__class__)]):
            has_paren = True
        if has_paren:
            self._write('(')
        self._dispatch(t.left)
        if has_paren:
            self._write(')')
        # Write the appropriate symbol for operator
        self._write(symbol)
        # Check if parenthesis are needed on the right side and then dispatch
        has_paren = False
        right_class = str(t.right.__class__)
        if (right_class in op_precedence.keys() and
            op_precedence[right_class] < op_precedence[str(t.__class__)]):
            has_paren = True
        if has_paren:
            self._write('(')
        self._dispatch(t.right)
        if has_paren:
            self._write(')')

    def _float(self, t):
        # if t is 0.1, str(t)->'0.1' while repr(t)->'0.1000000000001'
        # We prefer str here.
        self._write(str(t))

    def _str(self, t):
        self._write(repr(t))
        
    def _tuple(self, t):
        self._write(str(t))

    #########################################################################
    # These are the methods from the _ast modules unparse.
    #
    # As our needs to handle more advanced code increase, we may want to
    # modify some of the methods below so that they work for compiler.ast.
    #########################################################################

#    # stmt
#    def _Expr(self, tree):
#        self._fill()
#        self._dispatch(tree.value)
#
#    def _Import(self, t):
#        self._fill("import ")
#        first = True
#        for a in t.names:
#            if first:
#                first = False
#            else:
#                self._write(", ")
#            self._write(a.name)
#            if a.asname:
#                self._write(" as "+a.asname)
#
##    def _ImportFrom(self, t):
##        self._fill("from ")
##        self._write(t.module)
##        self._write(" import ")
##        for i, a in enumerate(t.names):
##            if i == 0:
##                self._write(", ")
##            self._write(a.name)
##            if a.asname:
##                self._write(" as "+a.asname)
##        # XXX(jpe) what is level for?
##
#
#    def _Break(self, t):
#        self._fill("break")
#
#    def _Continue(self, t):
#        self._fill("continue")
#
#    def _Delete(self, t):
#        self._fill("del ")
#        self._dispatch(t.targets)
#
#    def _Assert(self, t):
#        self._fill("assert ")
#        self._dispatch(t.test)
#        if t.msg:
#            self._write(", ")
#            self._dispatch(t.msg)
#
#    def _Exec(self, t):
#        self._fill("exec ")
#        self._dispatch(t.body)
#        if t.globals:
#            self._write(" in ")
#            self._dispatch(t.globals)
#        if t.locals:
#            self._write(", ")
#            self._dispatch(t.locals)
#
#    def _Print(self, t):
#        self._fill("print ")
#        do_comma = False
#        if t.dest:
#            self._write(">>")
#            self._dispatch(t.dest)
#            do_comma = True
#        for e in t.values:
#            if do_comma:self._write(", ")
#            else:do_comma=True
#            self._dispatch(e)
#        if not t.nl:
#            self._write(",")
#
#    def _Global(self, t):
#        self._fill("global")
#        for i, n in enumerate(t.names):
#            if i != 0:
#                self._write(",")
#            self._write(" " + n)
#
#    def _Yield(self, t):
#        self._fill("yield")
#        if t.value:
#            self._write(" (")
#            self._dispatch(t.value)
#            self._write(")")
#
#    def _Raise(self, t):
#        self._fill('raise ')
#        if t.type:
#            self._dispatch(t.type)
#        if t.inst:
#            self._write(", ")
#            self._dispatch(t.inst)
#        if t.tback:
#            self._write(", ")
#            self._dispatch(t.tback)
#
#
#    def _TryFinally(self, t):
#        self._fill("try")
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#
#        self._fill("finally")
#        self._enter()
#        self._dispatch(t.finalbody)
#        self._leave()
#
#    def _excepthandler(self, t):
#        self._fill("except ")
#        if t.type:
#            self._dispatch(t.type)
#        if t.name:
#            self._write(", ")
#            self._dispatch(t.name)
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#
#    def _ClassDef(self, t):
#        self._write("\n")
#        self._fill("class "+t.name)
#        if t.bases:
#            self._write("(")
#            for a in t.bases:
#                self._dispatch(a)
#                self._write(", ")
#            self._write(")")
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#
#    def _FunctionDef(self, t):
#        self._write("\n")
#        for deco in t.decorators:
#            self._fill("@")
#            self._dispatch(deco)
#        self._fill("def "+t.name + "(")
#        self._dispatch(t.args)
#        self._write(")")
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#
#    def _For(self, t):
#        self._fill("for ")
#        self._dispatch(t.target)
#        self._write(" in ")
#        self._dispatch(t.iter)
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#        if t.orelse:
#            self._fill("else")
#            self._enter()
#            self._dispatch(t.orelse)
#            self._leave
#
#    def _While(self, t):
#        self._fill("while ")
#        self._dispatch(t.test)
#        self._enter()
#        self._dispatch(t.body)
#        self._leave()
#        if t.orelse:
#            self._fill("else")
#            self._enter()
#            self._dispatch(t.orelse)
#            self._leave
#
#    # expr
#    def _Str(self, tree):
#        self._write(repr(tree.s))
##
#    def _Repr(self, t):
#        self._write("`")
#        self._dispatch(t.value)
#        self._write("`")
#
#    def _Num(self, t):
#        self._write(repr(t.n))
#
#    def _ListComp(self, t):
#        self._write("[")
#        self._dispatch(t.elt)
#        for gen in t.generators:
#            self._dispatch(gen)
#        self._write("]")
#
#    def _GeneratorExp(self, t):
#        self._write("(")
#        self._dispatch(t.elt)
#        for gen in t.generators:
#            self._dispatch(gen)
#        self._write(")")
#
#    def _comprehension(self, t):
#        self._write(" for ")
#        self._dispatch(t.target)
#        self._write(" in ")
#        self._dispatch(t.iter)
#        for if_clause in t.ifs:
#            self._write(" if ")
#            self._dispatch(if_clause)
#
#    def _IfExp(self, t):
#        self._dispatch(t.body)
#        self._write(" if ")
#        self._dispatch(t.test)
#        if t.orelse:
#            self._write(" else ")
#            self._dispatch(t.orelse)
#
#    unop = {"Invert":"~", "Not": "not", "UAdd":"+", "USub":"-"}
#    def _UnaryOp(self, t):
#        self._write(self.unop[t.op.__class__.__name__])
#        self._write("(")
#        self._dispatch(t.operand)
#        self._write(")")
#
#    binop = { "Add":"+", "Sub":"-", "Mult":"*", "Div":"/", "Mod":"%",
#                    "LShift":">>", "RShift":"<<", "BitOr":"|", "BitXor":"^", "BitAnd":"&",
#                    "FloorDiv":"//", "Pow": "**"}
#    def _BinOp(self, t):
#        self._write("(")
#        self._dispatch(t.left)
#        self._write(")" + self.binop[t.op.__class__.__name__] + "(")
#        self._dispatch(t.right)
#        self._write(")")
#
#    boolops = {_ast.And: 'and', _ast.Or: 'or'}
#    def _BoolOp(self, t):
#        self._write("(")
#        self._dispatch(t.values[0])
#        for v in t.values[1:]:
#            self._write(" %s " % self.boolops[t.op.__class__])
#            self._dispatch(v)
#        self._write(")")
#
#    def _Attribute(self,t):
#        self._dispatch(t.value)
#        self._write(".")
#        self._write(t.attr)
#
##    def _Call(self, t):
##        self._dispatch(t.func)
##        self._write("(")
##        comma = False
##        for e in t.args:
##            if comma: self._write(", ")
##            else: comma = True
##            self._dispatch(e)
##        for e in t.keywords:
##            if comma: self._write(", ")
##            else: comma = True
##            self._dispatch(e)
##        if t.starargs:
##            if comma: self._write(", ")
##            else: comma = True
##            self._write("*")
##            self._dispatch(t.starargs)
##        if t.kwargs:
##            if comma: self._write(", ")
##            else: comma = True
##            self._write("**")
##            self._dispatch(t.kwargs)
##        self._write(")")
#
#    # slice
#    def _Index(self, t):
#        self._dispatch(t.value)
#
#    def _ExtSlice(self, t):
#        for i, d in enumerate(t.dims):
#            if i != 0:
#                self._write(': ')
#            self._dispatch(d)
#
#    # others
#    def _arguments(self, t):
#        first = True
#        nonDef = len(t.args)-len(t.defaults)
#        for a in t.args[0:nonDef]:
#            if first:first = False
#            else: self._write(", ")
#            self._dispatch(a)
#        for a,d in zip(t.args[nonDef:], t.defaults):
#            if first:first = False
#            else: self._write(", ")
#            self._dispatch(a),
#            self._write("=")
#            self._dispatch(d)
#        if t.vararg:
#            if first:first = False
#            else: self._write(", ")
#            self._write("*"+t.vararg)
#        if t.kwarg:
#            if first:first = False
#            else: self._write(", ")
#            self._write("**"+t.kwarg)
#
##    def _keyword(self, t):
##        self._write(t.arg)
##        self._write("=")
##        self._dispatch(t.value)
#
#    def _Lambda(self, t):
#        self._write("lambda ")
#        self._dispatch(t.args)
#        self._write(": ")
#        self._dispatch(t.body)




########NEW FILE########
__FILENAME__ = docscrape
"""Extract reference documentation from the NumPy source tree.

"""

import inspect
import textwrap
import re
import pydoc
from StringIO import StringIO
from warnings import warn

class Reader(object):
    """A line-based string reader.

    """
    def __init__(self, data):
        """
        Parameters
        ----------
        data : str
           String with lines separated by '\n'.

        """
        if isinstance(data, list):
            self._str = data
        else:
            self._str = data.split('\n') # store string as list of lines

        self.reset()

    def __getitem__(self, n):
        return self._str[n]

    def reset(self):
        self._l = 0 # current line nr

    def read(self):
        if not self.eof():
            out = self[self._l]
            self._l += 1
            return out
        else:
            return ''

    def seek_next_non_empty_line(self):
        for l in self[self._l:]:
            if l.strip():
                break
            else:
                self._l += 1

    def eof(self):
        return self._l >= len(self._str)

    def read_to_condition(self, condition_func):
        start = self._l
        for line in self[start:]:
            if condition_func(line):
                return self[start:self._l]
            self._l += 1
            if self.eof():
                return self[start:self._l+1]
        return []

    def read_to_next_empty_line(self):
        self.seek_next_non_empty_line()
        def is_empty(line):
            return not line.strip()
        return self.read_to_condition(is_empty)

    def read_to_next_unindented_line(self):
        def is_unindented(line):
            return (line.strip() and (len(line.lstrip()) == len(line)))
        return self.read_to_condition(is_unindented)

    def peek(self,n=0):
        if self._l + n < len(self._str):
            return self[self._l + n]
        else:
            return ''

    def is_empty(self):
        return not ''.join(self._str).strip()


class NumpyDocString(object):
    def __init__(self, docstring, config={}):
        docstring = textwrap.dedent(docstring).split('\n')

        self._doc = Reader(docstring)
        self._parsed_data = {
            'Signature': '',
            'Summary': [''],
            'Extended Summary': [],
            'Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'Other Parameters': [],
            'Attributes': [],
            'Methods': [],
            'See Also': [],
            'Notes': [],
            'Warnings': [],
            'References': '',
            'Examples': '',
            'index': {}
            }

        self._parse()

    def __getitem__(self, key):
        return self._parsed_data[key]

    def __setitem__(self, key, val):
        if not self._parsed_data.has_key(key):
            warn("Unknown section %s" % key)
        else:
            self._parsed_data[key] = val

    def _is_at_section(self):
        self._doc.seek_next_non_empty_line()

        if self._doc.eof():
            return False

        l1 = self._doc.peek().strip()  # e.g. Parameters

        if l1.startswith('.. index::'):
            return True

        l2 = self._doc.peek(1).strip() #    ---------- or ==========
        return l2.startswith('-'*len(l1)) or l2.startswith('='*len(l1))

    def _strip(self, doc):
        i = 0
        j = 0
        for i, line in enumerate(doc):
            if line.strip(): break

        for j, line in enumerate(doc[::-1]):
            if line.strip(): break

        return doc[i:len(doc)-j]

    def _read_to_next_section(self):
        section = self._doc.read_to_next_empty_line()

        while not self._is_at_section() and not self._doc.eof():
            if not self._doc.peek(-1).strip(): # previous line was empty
                section += ['']

            section += self._doc.read_to_next_empty_line()

        return section

    def _read_sections(self):
        while not self._doc.eof():
            data = self._read_to_next_section()
            name = data[0].strip()

            if name.startswith('..'): # index section
                yield name, data[1:]
            elif len(data) < 2:
                yield StopIteration
            else:
                yield name, self._strip(data[2:])

    def _parse_param_list(self, content):
        r = Reader(content)
        params = []
        while not r.eof():
            header = r.read().strip()
            if ' : ' in header:
                arg_name, arg_type = header.split(' : ')[:2]
            else:
                arg_name, arg_type = header, ''

            desc = r.read_to_next_unindented_line()
            desc = dedent_lines(desc)

            params.append((arg_name, arg_type, desc))

        return params


    _name_rgx = re.compile(r"^\s*(:(?P<role>\w+):`(?P<name>[a-zA-Z0-9_.-]+)`|"
                           r" (?P<name2>[a-zA-Z0-9_.-]+))\s*", re.X)
    def _parse_see_also(self, content):
        """
        func_name : Descriptive text
            continued text
        another_func_name : Descriptive text
        func_name1, func_name2, :meth:`func_name`, func_name3

        """
        items = []

        def parse_item_name(text):
            """Match ':role:`name`' or 'name'"""
            m = self._name_rgx.match(text)
            if m:
                g = m.groups()
                if g[1] is None:
                    return g[3], None
                else:
                    return g[2], g[1]
            raise ValueError("%s is not a item name" % text)

        def push_item(name, rest):
            if not name:
                return
            name, role = parse_item_name(name)
            items.append((name, list(rest), role))
            del rest[:]

        current_func = None
        rest = []

        for line in content:
            if not line.strip(): continue

            m = self._name_rgx.match(line)
            if m and line[m.end():].strip().startswith(':'):
                push_item(current_func, rest)
                current_func, line = line[:m.end()], line[m.end():]
                rest = [line.split(':', 1)[1].strip()]
                if not rest[0]:
                    rest = []
            elif not line.startswith(' '):
                push_item(current_func, rest)
                current_func = None
                if ',' in line:
                    for func in line.split(','):
                        if func.strip():
                            push_item(func, [])
                elif line.strip():
                    current_func = line
            elif current_func is not None:
                rest.append(line.strip())
        push_item(current_func, rest)
        return items

    def _parse_index(self, section, content):
        """
        .. index: default
           :refguide: something, else, and more

        """
        def strip_each_in(lst):
            return [s.strip() for s in lst]

        out = {}
        section = section.split('::')
        if len(section) > 1:
            out['default'] = strip_each_in(section[1].split(','))[0]
        for line in content:
            line = line.split(':')
            if len(line) > 2:
                out[line[1]] = strip_each_in(line[2].split(','))
        return out

    def _parse_summary(self):
        """Grab signature (if given) and summary"""
        if self._is_at_section():
            return

        summary = self._doc.read_to_next_empty_line()
        summary_str = " ".join([s.strip() for s in summary]).strip()
        if re.compile('^([\w., ]+=)?\s*[\w\.]+\(.*\)$').match(summary_str):
            self['Signature'] = summary_str
            if not self._is_at_section():
                self['Summary'] = self._doc.read_to_next_empty_line()
        else:
            self['Summary'] = summary

        if not self._is_at_section():
            self['Extended Summary'] = self._read_to_next_section()

    def _parse(self):
        self._doc.reset()
        self._parse_summary()

        for (section, content) in self._read_sections():
            if not section.startswith('..'):
                section = ' '.join([s.capitalize() for s in section.split(' ')])
            if section in ('Parameters', 'Returns', 'Raises', 'Warns',
                           'Other Parameters', 'Attributes', 'Methods'):
                self[section] = self._parse_param_list(content)
            elif section.startswith('.. index::'):
                self['index'] = self._parse_index(section, content)
            elif section == 'See Also':
                self['See Also'] = self._parse_see_also(content)
            else:
                self[section] = content

    # string conversion routines

    def _str_header(self, name, symbol='-'):
        return [name, len(name)*symbol]

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        if self['Signature']:
            return [self['Signature'].replace('*', '\*')] + ['']
        else:
            return ['']

    def _str_summary(self):
        if self['Summary']:
            return self['Summary'] + ['']
        else:
            return []

    def _str_extended_summary(self):
        if self['Extended Summary']:
            return self['Extended Summary'] + ['']
        else:
            return []

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            for param, param_type, desc in self[name]:
                out += ['%s : %s' % (param, param_type)]
                out += self._str_indent(desc)
            out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += self[name]
            out += ['']
        return out

    def _str_see_also(self, func_role):
        if not self['See Also']: return []
        out = []
        out += self._str_header("See Also")
        last_had_desc = True
        for func, desc, role in self['See Also']:
            if role:
                link = ':%s:`%s`' % (role, func)
            elif func_role:
                link = ':%s:`%s`' % (func_role, func)
            else:
                link = "`%s`_" % func
            if desc or last_had_desc:
                out += ['']
                out += [link]
            else:
                out[-1] += ", %s" % link
            if desc:
                out += self._str_indent([' '.join(desc)])
                last_had_desc = True
            else:
                last_had_desc = False
        out += ['']
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        out += ['.. index:: %s' % idx.get('default', '')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            out += ['   :%s: %s' % (section, ', '.join(references))]
        return out

    def __str__(self, func_role=''):
        out = []
        out += self._str_signature()
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Returns', 'Other Parameters',
                           'Raises', 'Warns'):
            out += self._str_param_list(param_list)
        out += self._str_section('Warnings')
        out += self._str_see_also(func_role)
        for s in ('Notes', 'References', 'Examples'):
            out += self._str_section(s)
        for param_list in ('Attributes', 'Methods'):
            out += self._str_param_list(param_list)
        out += self._str_index()
        return '\n'.join(out)


def indent(str,indent=4):
    indent_str = ' '*indent
    if str is None:
        return indent_str
    lines = str.split('\n')
    return '\n'.join(indent_str + l for l in lines)

def dedent_lines(lines):
    """Deindent a list of lines maximally"""
    return textwrap.dedent("\n".join(lines)).split("\n")

def header(text, style='-'):
    return text + '\n' + style*len(text) + '\n'


class FunctionDoc(NumpyDocString):
    def __init__(self, func, role='func', doc=None, config={}):
        self._f = func
        self._role = role # e.g. "func" or "meth"

        if doc is None:
            if func is None:
                raise ValueError("No function or docstring given")
            doc = inspect.getdoc(func) or ''
        NumpyDocString.__init__(self, doc)

        if not self['Signature'] and func is not None:
            func, func_name = self.get_func()
            try:
                # try to read signature
                argspec = inspect.getargspec(func)
                argspec = inspect.formatargspec(*argspec)
                argspec = argspec.replace('*', '\*')
                signature = '%s%s' % (func_name, argspec)
            except TypeError, e:
                signature = '%s()' % func_name
            self['Signature'] = signature

    def get_func(self):
        func_name = getattr(self._f, '__name__', self.__class__.__name__)
        if inspect.isclass(self._f):
            func = getattr(self._f, '__call__', self._f.__init__)
        else:
            func = self._f
        return func, func_name

    def __str__(self):
        out = ''

        func, func_name = self.get_func()
        signature = self['Signature'].replace('*', '\*')

        roles = {'func': 'function',
                 'meth': 'method'}

        if self._role:
            if not roles.has_key(self._role):
                print "Warning: invalid role %s" % self._role
            out += '.. %s:: %s\n    \n\n' % (roles.get(self._role, ''),
                                             func_name)

        out += super(FunctionDoc, self).__str__(func_role=self._role)
        return out


class ClassDoc(NumpyDocString):
    def __init__(self, cls, doc=None, modulename='', func_doc=FunctionDoc,
                 config={}):
        if not inspect.isclass(cls) and cls is not None:
            raise ValueError("Expected a class or None, but got %r" % cls)
        self._cls = cls

        if modulename and not modulename.endswith('.'):
            modulename += '.'
        self._mod = modulename

        if doc is None:
            if cls is None:
                raise ValueError("No class or documentation string given")
            doc = pydoc.getdoc(cls)

        NumpyDocString.__init__(self, doc)

        if config.get('show_class_members', True):
            if not self['Methods']:
                self['Methods'] = [(name, '', '')
                                   for name in sorted(self.methods)]
            if not self['Attributes']:
                self['Attributes'] = [(name, '', '')
                                      for name in sorted(self.properties)]

    @property
    def methods(self):
        if self._cls is None:
            return []
        return [name for name, func in inspect.getmembers(self._cls)
                if not name.startswith('_') and callable(func)]

    @property
    def properties(self):
        if self._cls is None:
            return []
        return [name for name, func in inspect.getmembers(self._cls)
                if not name.startswith('_') and func is None]

########NEW FILE########
__FILENAME__ = docscrape_sphinx
import re, inspect, textwrap, pydoc
import sphinx
from docscrape import NumpyDocString, FunctionDoc, ClassDoc

class SphinxDocString(NumpyDocString):
    def __init__(self, docstring, config={}):
        self.use_plots = config.get('use_plots', False)
        NumpyDocString.__init__(self, docstring, config=config)

    # string conversion routines
    def _str_header(self, name, symbol='`'):
        return ['.. rubric:: ' + name, '']

    def _str_field_list(self, name):
        return [':' + name + ':']

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        return ['']
        if self['Signature']:
            return ['``%s``' % self['Signature']] + ['']
        else:
            return ['']

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Extended Summary'] + ['']

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_field_list(name)
            out += ['']
            for param, param_type, desc in self[name]:
                out += self._str_indent(['**%s** : %s' % (param.strip(),
                                                          param_type)])
                out += ['']
                out += self._str_indent(desc, 8)
                out += ['']
        return out

    @property
    def _obj(self):
        if hasattr(self, '_cls'):
            return self._cls
        elif hasattr(self, '_f'):
            return self._f
        return None

    def _str_member_list(self, name):
        """
        Generate a member listing, autosummary:: table where possible,
        and a table where not.

        """
        out = []
        if self[name]:
            out += ['.. rubric:: %s' % name, '']
            prefix = getattr(self, '_name', '')

            if prefix:
                prefix = '~%s.' % prefix

            autosum = []
            others = []
            for param, param_type, desc in self[name]:
                param = param.strip()
                if not self._obj or hasattr(self._obj, param):
                    autosum += ["   %s%s" % (prefix, param)]
                else:
                    others.append((param, param_type, desc))

            if autosum:
                out += ['.. autosummary::', '   :toctree:', '']
                out += autosum

            if others:
                maxlen_0 = max([len(x[0]) for x in others])
                maxlen_1 = max([len(x[1]) for x in others])
                hdr = "="*maxlen_0 + "  " + "="*maxlen_1 + "  " + "="*10
                fmt = '%%%ds  %%%ds  ' % (maxlen_0, maxlen_1)
                n_indent = maxlen_0 + maxlen_1 + 4
                out += [hdr]
                for param, param_type, desc in others:
                    out += [fmt % (param.strip(), param_type)]
                    out += self._str_indent(desc, n_indent)
                out += [hdr]
            out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += ['']
            content = textwrap.dedent("\n".join(self[name])).split("\n")
            out += content
            out += ['']
        return out

    def _str_see_also(self, func_role):
        out = []
        if self['See Also']:
            see_also = super(SphinxDocString, self)._str_see_also(func_role)
            out = ['.. seealso::', '']
            out += self._str_indent(see_also[2:])
        return out

    def _str_warnings(self):
        out = []
        if self['Warnings']:
            out = ['.. warning::', '']
            out += self._str_indent(self['Warnings'])
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        if len(idx) == 0:
            return out

        out += ['.. index:: %s' % idx.get('default', '')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            elif section == 'refguide':
                out += ['   single: %s' % (', '.join(references))]
            else:
                out += ['   %s: %s' % (section, ','.join(references))]
        return out

    def _str_references(self):
        out = []
        if self['References']:
            out += self._str_header('References')
            if isinstance(self['References'], str):
                self['References'] = [self['References']]
            out.extend(self['References'])
            out += ['']
            # Latex collects all references to a separate bibliography,
            # so we need to insert links to it
            if sphinx.__version__ >= "0.6":
                out += ['.. only:: latex', '']
            else:
                out += ['.. latexonly::', '']
            items = []
            for line in self['References']:
                m = re.match(r'.. \[([a-z0-9._-]+)\]', line, re.I)
                if m:
                    items.append(m.group(1))
            out += ['   ' + ", ".join(["[%s]_" % item for item in items]), '']
        return out

    def _str_examples(self):
        examples_str = "\n".join(self['Examples'])

        if (self.use_plots and 'import matplotlib' in examples_str
                and 'plot::' not in examples_str):
            out = []
            out += self._str_header('Examples')
            out += ['.. plot::', '']
            out += self._str_indent(self['Examples'])
            out += ['']
            return out
        else:
            return self._str_section('Examples')

    def __str__(self, indent=0, func_role="obj"):
        out = []
        out += self._str_signature()
        out += self._str_index() + ['']
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Returns', 'Other Parameters',
                           'Raises', 'Warns'):
            out += self._str_param_list(param_list)
        out += self._str_warnings()
        out += self._str_see_also(func_role)
        out += self._str_section('Notes')
        out += self._str_references()
        out += self._str_examples()
        #for param_list in ('Attributes', 'Methods'):
        #    out += self._str_member_list(param_list)
        out = self._str_indent(out, indent)
        return '\n'.join(out)

class SphinxFunctionDoc(SphinxDocString, FunctionDoc):
    def __init__(self, obj, doc=None, config={}):
        self.use_plots = config.get('use_plots', False)
        FunctionDoc.__init__(self, obj, doc=doc, config=config)

class SphinxClassDoc(SphinxDocString, ClassDoc):
    def __init__(self, obj, doc=None, func_doc=None, config={}):
        self.use_plots = config.get('use_plots', False)
        ClassDoc.__init__(self, obj, doc=doc, func_doc=None, config=config)

class SphinxObjDoc(SphinxDocString):
    def __init__(self, obj, doc=None, config={}):
        self._f = obj
        SphinxDocString.__init__(self, doc, config=config)

def get_doc_object(obj, what=None, doc=None, config={}):
    if what is None:
        if inspect.isclass(obj):
            what = 'class'
        elif inspect.ismodule(obj):
            what = 'module'
        elif callable(obj):
            what = 'function'
        else:
            what = 'object'
    if what == 'class':
        return SphinxClassDoc(obj, func_doc=SphinxFunctionDoc, doc=doc,
                              config=config)
    elif what in ('function', 'method'):
        return SphinxFunctionDoc(obj, doc=doc, config=config)
    else:
        if doc is None:
            doc = pydoc.getdoc(obj)
        return SphinxObjDoc(obj, doc, config=config)

########NEW FILE########
__FILENAME__ = inheritance_diagram
"""
Defines a docutils directive for inserting inheritance diagrams.

Provide the directive with one or more classes or modules (separated
by whitespace).  For modules, all of the classes in that module will
be used.

Example::

   Given the following classes:

   class A: pass
   class B(A): pass
   class C(A): pass
   class D(B, C): pass
   class E(B): pass

   .. inheritance-diagram: D E

   Produces a graph like the following:

               A
              / \
             B   C
            / \ /
           E   D

The graph is inserted as a PNG+image map into HTML and a PDF in
LaTeX.
"""

import inspect
import os
import re
import subprocess
try:
    from hashlib import md5
except ImportError:
    from md5 import md5

from docutils.nodes import Body, Element
from docutils.parsers.rst import directives
from sphinx.roles import xfileref_role

def my_import(name):
    """Module importer - taken from the python documentation.

    This function allows importing names with dots in them."""
    
    mod = __import__(name)
    components = name.split('.')
    for comp in components[1:]:
        mod = getattr(mod, comp)
    return mod

class DotException(Exception):
    pass

class InheritanceGraph(object):
    """
    Given a list of classes, determines the set of classes that
    they inherit from all the way to the root "object", and then
    is able to generate a graphviz dot graph from them.
    """
    def __init__(self, class_names, show_builtins=False):
        """
        *class_names* is a list of child classes to show bases from.

        If *show_builtins* is True, then Python builtins will be shown
        in the graph.
        """
        self.class_names = class_names
        self.classes = self._import_classes(class_names)
        self.all_classes = self._all_classes(self.classes)
        if len(self.all_classes) == 0:
            raise ValueError("No classes found for inheritance diagram")
        self.show_builtins = show_builtins

    py_sig_re = re.compile(r'''^([\w.]*\.)?    # class names
                           (\w+)  \s* $        # optionally arguments
                           ''', re.VERBOSE)

    def _import_class_or_module(self, name):
        """
        Import a class using its fully-qualified *name*.
        """
        try:
            path, base = self.py_sig_re.match(name).groups()
        except:
            raise ValueError(
                "Invalid class or module '%s' specified for inheritance diagram" % name)
        fullname = (path or '') + base
        path = (path and path.rstrip('.'))
        if not path:
            path = base
        try:
            module = __import__(path, None, None, [])
            # We must do an import of the fully qualified name.  Otherwise if a
            # subpackage 'a.b' is requested where 'import a' does NOT provide
            # 'a.b' automatically, then 'a.b' will not be found below.  This
            # second call will force the equivalent of 'import a.b' to happen
            # after the top-level import above.
            my_import(fullname)
            
        except ImportError:
            raise ValueError(
                "Could not import class or module '%s' specified for inheritance diagram" % name)

        try:
            todoc = module
            for comp in fullname.split('.')[1:]:
                todoc = getattr(todoc, comp)
        except AttributeError:
            raise ValueError(
                "Could not find class or module '%s' specified for inheritance diagram" % name)

        # If a class, just return it
        if inspect.isclass(todoc):
            return [todoc]
        elif inspect.ismodule(todoc):
            classes = []
            for cls in todoc.__dict__.values():
                if inspect.isclass(cls) and cls.__module__ == todoc.__name__:
                    classes.append(cls)
            return classes
        raise ValueError(
            "'%s' does not resolve to a class or module" % name)

    def _import_classes(self, class_names):
        """
        Import a list of classes.
        """
        classes = []
        for name in class_names:
            classes.extend(self._import_class_or_module(name))
        return classes

    def _all_classes(self, classes):
        """
        Return a list of all classes that are ancestors of *classes*.
        """
        all_classes = {}

        def recurse(cls):
            all_classes[cls] = None
            for c in cls.__bases__:
                if c not in all_classes:
                    recurse(c)

        for cls in classes:
            recurse(cls)

        return all_classes.keys()

    def class_name(self, cls, parts=0):
        """
        Given a class object, return a fully-qualified name.  This
        works for things I've tested in matplotlib so far, but may not
        be completely general.
        """
        module = cls.__module__
        if module == '__builtin__':
            fullname = cls.__name__
        else:
            fullname = "%s.%s" % (module, cls.__name__)
        if parts == 0:
            return fullname
        name_parts = fullname.split('.')
        return '.'.join(name_parts[-parts:])

    def get_all_class_names(self):
        """
        Get all of the class names involved in the graph.
        """
        return [self.class_name(x) for x in self.all_classes]

    # These are the default options for graphviz
    default_graph_options = {
        "rankdir": "LR",
        "size": '"8.0, 12.0"'
        }
    default_node_options = {
        "shape": "box",
        "fontsize": 10,
        "height": 0.25,
        "fontname": "Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans",
        "style": '"setlinewidth(0.5)"'
        }
    default_edge_options = {
        "arrowsize": 0.5,
        "style": '"setlinewidth(0.5)"'
        }

    def _format_node_options(self, options):
        return ','.join(["%s=%s" % x for x in options.items()])
    def _format_graph_options(self, options):
        return ''.join(["%s=%s;\n" % x for x in options.items()])

    def generate_dot(self, fd, name, parts=0, urls={},
                     graph_options={}, node_options={},
                     edge_options={}):
        """
        Generate a graphviz dot graph from the classes that
        were passed in to __init__.

        *fd* is a Python file-like object to write to.

        *name* is the name of the graph

        *urls* is a dictionary mapping class names to http urls

        *graph_options*, *node_options*, *edge_options* are
        dictionaries containing key/value pairs to pass on as graphviz
        properties.
        """
        g_options = self.default_graph_options.copy()
        g_options.update(graph_options)
        n_options = self.default_node_options.copy()
        n_options.update(node_options)
        e_options = self.default_edge_options.copy()
        e_options.update(edge_options)

        fd.write('digraph %s {\n' % name)
        fd.write(self._format_graph_options(g_options))

        for cls in self.all_classes:
            if not self.show_builtins and cls in __builtins__.values():
                continue

            name = self.class_name(cls, parts)

            # Write the node
            this_node_options = n_options.copy()
            url = urls.get(self.class_name(cls))
            if url is not None:
                this_node_options['URL'] = '"%s"' % url
            fd.write('  "%s" [%s];\n' %
                     (name, self._format_node_options(this_node_options)))

            # Write the edges
            for base in cls.__bases__:
                if not self.show_builtins and base in __builtins__.values():
                    continue

                base_name = self.class_name(base, parts)
                fd.write('  "%s" -> "%s" [%s];\n' %
                         (base_name, name,
                          self._format_node_options(e_options)))
        fd.write('}\n')

    def run_dot(self, args, name, parts=0, urls={},
                graph_options={}, node_options={}, edge_options={}):
        """
        Run graphviz 'dot' over this graph, returning whatever 'dot'
        writes to stdout.

        *args* will be passed along as commandline arguments.

        *name* is the name of the graph

        *urls* is a dictionary mapping class names to http urls

        Raises DotException for any of the many os and
        installation-related errors that may occur.
        """
        try:
            dot = subprocess.Popen(['dot'] + list(args),
                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                                   close_fds=True)
        except OSError:
            raise DotException("Could not execute 'dot'.  Are you sure you have 'graphviz' installed?")
        except ValueError:
            raise DotException("'dot' called with invalid arguments")
        except:
            raise DotException("Unexpected error calling 'dot'")

        self.generate_dot(dot.stdin, name, parts, urls, graph_options,
                          node_options, edge_options)
        dot.stdin.close()
        result = dot.stdout.read()
        returncode = dot.wait()
        if returncode != 0:
            raise DotException("'dot' returned the errorcode %d" % returncode)
        return result

class inheritance_diagram(Body, Element):
    """
    A docutils node to use as a placeholder for the inheritance
    diagram.
    """
    pass

def inheritance_diagram_directive(name, arguments, options, content, lineno,
                                  content_offset, block_text, state,
                                  state_machine):
    """
    Run when the inheritance_diagram directive is first encountered.
    """
    node = inheritance_diagram()

    class_names = arguments

    # Create a graph starting with the list of classes
    graph = InheritanceGraph(class_names)

    # Create xref nodes for each target of the graph's image map and
    # add them to the doc tree so that Sphinx can resolve the
    # references to real URLs later.  These nodes will eventually be
    # removed from the doctree after we're done with them.
    for name in graph.get_all_class_names():
        refnodes, x = xfileref_role(
            'class', ':class:`%s`' % name, name, 0, state)
        node.extend(refnodes)
    # Store the graph object so we can use it to generate the
    # dot file later
    node['graph'] = graph
    # Store the original content for use as a hash
    node['parts'] = options.get('parts', 0)
    node['content'] = " ".join(class_names)
    return [node]

def get_graph_hash(node):
    return md5(node['content'] + str(node['parts'])).hexdigest()[-10:]

def html_output_graph(self, node):
    """
    Output the graph for HTML.  This will insert a PNG with clickable
    image map.
    """
    graph = node['graph']
    parts = node['parts']

    graph_hash = get_graph_hash(node)
    name = "inheritance%s" % graph_hash
    path = '_images'
    dest_path = os.path.join(setup.app.builder.outdir, path)
    if not os.path.exists(dest_path):
        os.makedirs(dest_path)
    png_path = os.path.join(dest_path, name + ".png")
    path = setup.app.builder.imgpath

    # Create a mapping from fully-qualified class names to URLs.
    urls = {}
    for child in node:
        if child.get('refuri') is not None:
            urls[child['reftitle']] = child.get('refuri')
        elif child.get('refid') is not None:
            urls[child['reftitle']] = '#' + child.get('refid')

    # These arguments to dot will save a PNG file to disk and write
    # an HTML image map to stdout.
    image_map = graph.run_dot(['-Tpng', '-o%s' % png_path, '-Tcmapx'],
                              name, parts, urls)
    return ('<img src="%s/%s.png" usemap="#%s" class="inheritance"/>%s' %
            (path, name, name, image_map))

def latex_output_graph(self, node):
    """
    Output the graph for LaTeX.  This will insert a PDF.
    """
    graph = node['graph']
    parts = node['parts']

    graph_hash = get_graph_hash(node)
    name = "inheritance%s" % graph_hash
    dest_path = os.path.abspath(os.path.join(setup.app.builder.outdir, '_images'))
    if not os.path.exists(dest_path):
        os.makedirs(dest_path)
    pdf_path = os.path.abspath(os.path.join(dest_path, name + ".pdf"))

    graph.run_dot(['-Tpdf', '-o%s' % pdf_path],
                  name, parts, graph_options={'size': '"6.0,6.0"'})
    return '\n\\includegraphics{%s}\n\n' % pdf_path

def visit_inheritance_diagram(inner_func):
    """
    This is just a wrapper around html/latex_output_graph to make it
    easier to handle errors and insert warnings.
    """
    def visitor(self, node):
        try:
            content = inner_func(self, node)
        except DotException, e:
            # Insert the exception as a warning in the document
            warning = self.document.reporter.warning(str(e), line=node.line)
            warning.parent = node
            node.children = [warning]
        else:
            source = self.document.attributes['source']
            self.body.append(content)
            node.children = []
    return visitor

def do_nothing(self, node):
    pass

def setup(app):
    setup.app = app
    setup.confdir = app.confdir

    app.add_node(
        inheritance_diagram,
        latex=(visit_inheritance_diagram(latex_output_graph), do_nothing),
        html=(visit_inheritance_diagram(html_output_graph), do_nothing))
    app.add_directive(
        'inheritance-diagram', inheritance_diagram_directive,
        False, (1, 100, 0), parts = directives.nonnegative_int)

########NEW FILE########
__FILENAME__ = ipython_console_highlighting
"""reST directive for syntax-highlighting ipython interactive sessions.

XXX - See what improvements can be made based on the new (as of Sept 2009)
'pycon' lexer for the python console.  At the very least it will give better
highlighted tracebacks.
"""

#-----------------------------------------------------------------------------
# Needed modules

# Standard library
import re

# Third party
from pygments.lexer import Lexer, do_insertions
from pygments.lexers.agile import (PythonConsoleLexer, PythonLexer, 
                                   PythonTracebackLexer)
from pygments.token import Comment, Generic

from sphinx import highlighting

#-----------------------------------------------------------------------------
# Global constants
line_re = re.compile('.*?\n')

#-----------------------------------------------------------------------------
# Code begins - classes and functions

class IPythonConsoleLexer(Lexer):
    """
    For IPython console output or doctests, such as:

    .. sourcecode:: ipython

      In [1]: a = 'foo'

      In [2]: a
      Out[2]: 'foo'

      In [3]: print a
      foo

      In [4]: 1 / 0

    Notes:

      - Tracebacks are not currently supported.

      - It assumes the default IPython prompts, not customized ones.
    """
    
    name = 'IPython console session'
    aliases = ['ipython']
    mimetypes = ['text/x-ipython-console']
    input_prompt = re.compile("(In \[[0-9]+\]: )|(   \.\.\.+:)")
    output_prompt = re.compile("(Out\[[0-9]+\]: )|(   \.\.\.+:)")
    continue_prompt = re.compile("   \.\.\.+:")
    tb_start = re.compile("\-+")

    def get_tokens_unprocessed(self, text):
        pylexer = PythonLexer(**self.options)
        tblexer = PythonTracebackLexer(**self.options)

        curcode = ''
        insertions = []
        for match in line_re.finditer(text):
            line = match.group()
            input_prompt = self.input_prompt.match(line)
            continue_prompt = self.continue_prompt.match(line.rstrip())
            output_prompt = self.output_prompt.match(line)
            if line.startswith("#"):
                insertions.append((len(curcode),
                                   [(0, Comment, line)]))
            elif input_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, input_prompt.group())]))
                curcode += line[input_prompt.end():]
            elif continue_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, continue_prompt.group())]))
                curcode += line[continue_prompt.end():]
            elif output_prompt is not None:
                # Use the 'error' token for output.  We should probably make
                # our own token, but error is typicaly in a bright color like
                # red, so it works fine for our output prompts.
                insertions.append((len(curcode),
                                   [(0, Generic.Error, output_prompt.group())]))
                curcode += line[output_prompt.end():]
            else:
                if curcode:
                    for item in do_insertions(insertions,
                                              pylexer.get_tokens_unprocessed(curcode)):
                        yield item
                        curcode = ''
                        insertions = []
                yield match.start(), Generic.Output, line
        if curcode:
            for item in do_insertions(insertions,
                                      pylexer.get_tokens_unprocessed(curcode)):
                yield item


def setup(app):
    """Setup as a sphinx extension."""

    # This is only a lexer, so adding it below to pygments appears sufficient.
    # But if somebody knows that the right API usage should be to do that via
    # sphinx, by all means fix it here.  At least having this setup.py
    # suppresses the sphinx warning we'd get without it.
    pass

#-----------------------------------------------------------------------------
# Register the extension as a valid pygments lexer
highlighting.lexers['ipython'] = IPythonConsoleLexer()

########NEW FILE########
__FILENAME__ = ipython_directive
# -*- coding: utf-8 -*-
"""Sphinx directive to support embedded IPython code.

This directive allows pasting of entire interactive IPython sessions, prompts
and all, and their code will actually get re-executed at doc build time, with
all prompts renumbered sequentially.

To enable this directive, simply list it in your Sphinx ``conf.py`` file
(making sure the directory where you placed it is visible to sphinx, as is
needed for all Sphinx directives).

By default this directive assumes that your prompts are unchanged IPython ones,
but this can be customized.  For example, the following code in your Sphinx
config file will configure this directive for the following input/output
prompts ``Yade [1]:`` and ``-> [1]:``::

 import ipython_directive as id
 id.rgxin =re.compile(r'(?:In |Yade )\[(\d+)\]:\s?(.*)\s*')
 id.rgxout=re.compile(r'(?:Out| ->  )\[(\d+)\]:\s?(.*)\s*')
 id.fmtin ='Yade [%d]:'
 id.fmtout=' ->  [%d]:'

 from IPython import Config
 id.CONFIG = Config(
   prompt_in1="Yade [\#]:",
   prompt_in2="     .\D..",
   prompt_out=" ->  [\#]:"
 )
 id.reconfig_shell()

 import ipython_console_highlighting as ich
 ich.IPythonConsoleLexer.input_prompt=
    re.compile("(Yade \[[0-9]+\]: )|(   \.\.\.+:)")
 ich.IPythonConsoleLexer.output_prompt=
    re.compile("(( ->  )|(Out)\[[0-9]+\]: )|(   \.\.\.+:)")
 ich.IPythonConsoleLexer.continue_prompt=re.compile("   \.\.\.+:")


ToDo
----

- Turn the ad-hoc test() function into a real test suite.
- Break up ipython-specific functionality from matplotlib stuff into better
  separated code.
- Make sure %bookmarks used internally are removed on exit.


Authors
-------

- John D Hunter: orignal author.
- Fernando Perez: refactoring, documentation, cleanups, port to 0.11.
- Vclavmilauer <eudoxos-AT-arcig.cz>: Prompt generalizations.
"""

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

# Stdlib
import cStringIO
import os
import re
import sys

# To keep compatibility with various python versions
try:
    from hashlib import md5
except ImportError:
    from md5 import md5

# Third-party
import matplotlib
import sphinx
from docutils.parsers.rst import directives

matplotlib.use('Agg')

# Our own
from IPython import Config, InteractiveShell
from IPython.utils.io import Term

#-----------------------------------------------------------------------------
# Globals
#-----------------------------------------------------------------------------

sphinx_version = sphinx.__version__.split(".")
# The split is necessary for sphinx beta versions where the string is
# '6b1'
sphinx_version = tuple([int(re.split('[a-z]', x)[0])
                        for x in sphinx_version[:2]])

COMMENT, INPUT, OUTPUT =  range(3)
CONFIG = Config()
rgxin = re.compile('In \[(\d+)\]:\s?(.*)\s*')
rgxout = re.compile('Out\[(\d+)\]:\s?(.*)\s*')
fmtin = 'In [%d]:'
fmtout = 'Out[%d]:'

#-----------------------------------------------------------------------------
# Functions and class declarations
#-----------------------------------------------------------------------------
def block_parser(part):
    """
    part is a string of ipython text, comprised of at most one
    input, one ouput, comments, and blank lines.  The block parser
    parses the text into a list of::

      blocks = [ (TOKEN0, data0), (TOKEN1, data1), ...]

    where TOKEN is one of [COMMENT | INPUT | OUTPUT ] and
    data is, depending on the type of token::

      COMMENT : the comment string

      INPUT: the (DECORATOR, INPUT_LINE, REST) where
         DECORATOR: the input decorator (or None)
         INPUT_LINE: the input as string (possibly multi-line)
         REST : any stdout generated by the input line (not OUTPUT)


      OUTPUT: the output string, possibly multi-line
    """

    block = []
    lines = part.split('\n')
    N = len(lines)
    i = 0
    decorator = None
    while True:

        if i==N:
            # nothing left to parse -- the last line
            break

        line = lines[i]
        i += 1
        line_stripped = line.strip()
        if line_stripped.startswith('#'):
            block.append((COMMENT, line))
            continue

        if line_stripped.startswith('@'):
            # we're assuming at most one decorator -- may need to
            # rethink
            decorator = line_stripped
            continue

        # does this look like an input line?
        matchin = rgxin.match(line)
        if matchin:
            lineno, inputline = int(matchin.group(1)), matchin.group(2)

            # the ....: continuation string
            continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
            Nc = len(continuation)
            # input lines can continue on for more than one line, if
            # we have a '\' line continuation char or a function call
            # echo line 'print'.  The input line can only be
            # terminated by the end of the block or an output line, so
            # we parse out the rest of the input line if it is
            # multiline as well as any echo text

            rest = []
            while i<N:

                # look ahead; if the next line is blank, or a comment, or
                # an output line, we're done

                nextline = lines[i]
                matchout = rgxout.match(nextline)
                #print "nextline=%s, continuation=%s, starts=%s"%(nextline, continuation, nextline.startswith(continuation))
                if matchout or nextline.startswith('#'):
                    break
                elif nextline.startswith(continuation):
                    inputline += '\n' + nextline[Nc:]
                else:
                    rest.append(nextline)
                i+= 1

            block.append((INPUT, (decorator, inputline, '\n'.join(rest))))
            continue

        # if it looks like an output line grab all the text to the end
        # of the block
        matchout = rgxout.match(line)
        if matchout:
            lineno, output = int(matchout.group(1)), matchout.group(2)
            if i<N-1:
                output = '\n'.join([output] + lines[i:])

            block.append((OUTPUT, output))
            break

    return block


class EmbeddedSphinxShell(object):
    """An embedded IPython instance to run inside Sphinx"""

    def __init__(self):

        self.cout = cStringIO.StringIO()
        Term.cout = self.cout
        Term.cerr = self.cout

        # For debugging, so we can see normal output, use this:
        # from IPython.utils.io import Tee
        #Term.cout = Tee(self.cout, channel='stdout') # dbg
        #Term.cerr = Tee(self.cout, channel='stderr') # dbg

        # Create config object for IPython
        config = Config()
        config.Global.display_banner = False
        config.Global.exec_lines = ['import numpy as np',
                                    'from pylab import *'
                                    ]
        config.InteractiveShell.autocall = False
        config.InteractiveShell.autoindent = False
        config.InteractiveShell.colors = 'NoColor'

        # Create and initialize ipython, but don't start its mainloop
        IP = InteractiveShell.instance(config=config)

        # Store a few parts of IPython we'll need.
        self.IP = IP
        self.user_ns = self.IP.user_ns
        self.user_global_ns = self.IP.user_global_ns
                                    
        self.input = ''
        self.output = ''

        self.is_verbatim = False
        self.is_doctest = False
        self.is_suppress = False

        # on the first call to the savefig decorator, we'll import
        # pyplot as plt so we can make a call to the plt.gcf().savefig
        self._pyplot_imported = False

        # we need bookmark the current dir first so we can save
        # relative to it
        self.process_input_line('bookmark ipy_basedir')
        self.cout.seek(0)
        self.cout.truncate(0)

    def process_input_line(self, line):
        """process the input, capturing stdout"""
        #print "input='%s'"%self.input
        stdout = sys.stdout
        try:
            sys.stdout = self.cout
            self.IP.push_line(line)
        finally:
            sys.stdout = stdout

    # Callbacks for each type of token
    def process_input(self, data, input_prompt, lineno):
        """Process data block for INPUT token."""
        decorator, input, rest = data
        image_file = None
        #print 'INPUT:', data  # dbg
        is_verbatim = decorator=='@verbatim' or self.is_verbatim
        is_doctest = decorator=='@doctest' or self.is_doctest
        is_suppress = decorator=='@suppress' or self.is_suppress
        is_savefig = decorator is not None and \
                     decorator.startswith('@savefig')

        input_lines = input.split('\n')

        continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
        Nc = len(continuation)

        if is_savefig:
            saveargs = decorator.split(' ')
            filename = saveargs[1]
            outfile = os.path.join('_static/%s'%filename)
            # build out an image directive like
            # .. image:: somefile.png
            #    :width 4in
            #
            # from an input like
            # savefig somefile.png width=4in
            imagerows = ['.. image:: %s'%outfile]

            for kwarg in saveargs[2:]:
                arg, val = kwarg.split('=')
                arg = arg.strip()
                val = val.strip()
                imagerows.append('   :%s: %s'%(arg, val))

            image_file = outfile
            image_directive = '\n'.join(imagerows)

        # TODO: can we get "rest" from ipython
        #self.process_input_line('\n'.join(input_lines))

        ret = []
        is_semicolon = False

        for i, line in enumerate(input_lines):
            if line.endswith(';'):
                is_semicolon = True

            if i==0:
                # process the first input line
                if is_verbatim:
                    self.process_input_line('')
                else:
                    # only submit the line in non-verbatim mode
                    self.process_input_line(line)
                formatted_line = '%s %s'%(input_prompt, line)
            else:
                # process a continuation line
                if not is_verbatim:
                    self.process_input_line(line)

                formatted_line = '%s %s'%(continuation, line)

            if not is_suppress:
                ret.append(formatted_line)

        if not is_suppress:
            if len(rest.strip()):
                if is_verbatim:
                    # the "rest" is the standard output of the
                    # input, which needs to be added in
                    # verbatim mode
                    ret.append(rest)

        self.cout.seek(0)
        output = self.cout.read()
        if not is_suppress and not is_semicolon:
            ret.append(output)

        self.cout.truncate(0)
        return ret, input_lines, output, is_doctest, image_file
        #print 'OUTPUT', output  # dbg

    def process_output(self, data, output_prompt,
                       input_lines, output, is_doctest, image_file):
        """Process data block for OUTPUT token."""
        if is_doctest:
            submitted = data.strip()
            found = output
            if found is not None:
                found = found.strip()
                
                # XXX - fperez: in 0.11, 'output' never comes with the prompt
                # in it, just the actual output text.  So I think all this code
                # can be nuked...
                ## ind = found.find(output_prompt)
                ## if ind<0:
                ##     e='output prompt="%s" does not match out line=%s' % \
                ##        (output_prompt, found)
                ##     raise RuntimeError(e)
                ## found = found[len(output_prompt):].strip()

                if found!=submitted:
                    e = ('doctest failure for input_lines="%s" with '
                         'found_output="%s" and submitted output="%s"' %
                         (input_lines, found, submitted) )
                    raise RuntimeError(e)
                #print 'doctest PASSED for input_lines="%s" with found_output="%s" and submitted output="%s"'%(input_lines, found, submitted)

    def process_comment(self, data):
        """Process data block for COMMENT token."""
        if not self.is_suppress:
            return [data]

    def process_block(self, block):
        """
        process block from the block_parser and return a list of processed lines
        """

        ret = []
        output = None
        input_lines = None

        m = rgxin.match(str(self.IP.outputcache.prompt1).strip())
        lineno = int(m.group(1))

        input_prompt = fmtin%lineno
        output_prompt = fmtout%lineno
        image_file = None
        image_directive = None
        # XXX - This needs a second refactor.  There's too much state being
        # held globally, which makes for a very awkward interface and large,
        # hard to test functions.  I've already broken this up at least into
        # three separate processors to isolate the logic better, but this only
        # serves to highlight the coupling.  Next we need to clean it up...
        for token, data in block:
            if token==COMMENT:
                out_data = self.process_comment(data)
            elif token==INPUT:
                out_data, input_lines, output, is_doctest, image_file= \
                          self.process_input(data, input_prompt, lineno)
            elif token==OUTPUT:
                out_data = \
                    self.process_output(data, output_prompt,
                                        input_lines, output, is_doctest,
                                        image_file)
            if out_data:
                ret.extend(out_data)

        if image_file is not None:
            self.ensure_pyplot()
            command = 'plt.gcf().savefig("%s")'%image_file
            print 'SAVEFIG', command  # dbg
            self.process_input_line('bookmark ipy_thisdir')
            self.process_input_line('cd -b ipy_basedir')
            self.process_input_line(command)
            self.process_input_line('cd -b ipy_thisdir')
            self.cout.seek(0)
            self.cout.truncate(0)
        return ret, image_directive

    def ensure_pyplot(self):
        if self._pyplot_imported:
            return
        self.process_input_line('import matplotlib.pyplot as plt')

# A global instance used below. XXX: not sure why this can't be created inside
# ipython_directive itself.
shell = EmbeddedSphinxShell()

def reconfig_shell():
    """Called after setting module-level variables to re-instantiate
    with the set values (since shell is instantiated first at import-time
    when module variables have default values)"""
    global shell
    shell = EmbeddedSphinxShell()


def ipython_directive(name, arguments, options, content, lineno,
                      content_offset, block_text, state, state_machine,
                      ):

    debug = ipython_directive.DEBUG
    shell.is_suppress = options.has_key('suppress')
    shell.is_doctest = options.has_key('doctest')
    shell.is_verbatim = options.has_key('verbatim')

    #print 'ipy', shell.is_suppress, options
    parts = '\n'.join(content).split('\n\n')
    lines = ['.. sourcecode:: ipython', '']

    figures = []
    for part in parts:
        block = block_parser(part)

        if len(block):
            rows, figure = shell.process_block(block)
            for row in rows:
                lines.extend(['    %s'%line for line in row.split('\n')])

            if figure is not None:
                figures.append(figure)

    for figure in figures:
        lines.append('')
        lines.extend(figure.split('\n'))
        lines.append('')

    #print lines
    if len(lines)>2:
        if debug:
            print '\n'.join(lines)
        else:
            #print 'INSERTING %d lines'%len(lines)
            state_machine.insert_input(
                lines, state_machine.input_lines.source(0))

    return []

ipython_directive.DEBUG = False
ipython_directive.DEBUG = True  # dbg

# Enable as a proper Sphinx directive
def setup(app):
    setup.app = app
    options = {'suppress': directives.flag,
               'doctest': directives.flag,
               'verbatim': directives.flag,
               }

    app.add_directive('ipython', ipython_directive, True, (0, 2, 0), **options)


# Simple smoke test, needs to be converted to a proper automatic test.
def test():

    examples = [
        r"""
In [9]: pwd
Out[9]: '/home/jdhunter/py4science/book'

In [10]: cd bookdata/
/home/jdhunter/py4science/book/bookdata

In [2]: from pylab import *

In [2]: ion()

In [3]: im = imread('stinkbug.png')

@savefig mystinkbug.png width=4in
In [4]: imshow(im)
Out[4]: <matplotlib.image.AxesImage object at 0x39ea850>
        
""",
        r"""

In [1]: x = 'hello world'

# string methods can be
# used to alter the string
@doctest
In [2]: x.upper()
Out[2]: 'HELLO WORLD'

@verbatim
In [3]: x.st<TAB>
x.startswith  x.strip
""",
    r"""

In [130]: url = 'http://ichart.finance.yahoo.com/table.csv?s=CROX\
   .....: &d=9&e=22&f=2009&g=d&a=1&br=8&c=2006&ignore=.csv'

In [131]: print url.split('&')
['http://ichart.finance.yahoo.com/table.csv?s=CROX', 'd=9', 'e=22', 'f=2009', 'g=d', 'a=1', 'b=8', 'c=2006', 'ignore=.csv']

In [60]: import urllib

""",
    r"""\

In [133]: import numpy.random

@suppress
In [134]: numpy.random.seed(2358)

@doctest
In [135]: np.random.rand(10,2)
Out[135]:
array([[ 0.64524308,  0.59943846],
       [ 0.47102322,  0.8715456 ],
       [ 0.29370834,  0.74776844],
       [ 0.99539577,  0.1313423 ],
       [ 0.16250302,  0.21103583],
       [ 0.81626524,  0.1312433 ],
       [ 0.67338089,  0.72302393],
       [ 0.7566368 ,  0.07033696],
       [ 0.22591016,  0.77731835],
       [ 0.0072729 ,  0.34273127]])

""",

    r"""
In [106]: print x
jdh

In [109]: for i in range(10):
   .....:     print i
   .....:
   .....:
0
1
2
3
4
5
6
7
8
9
""",

        r"""

In [144]: from pylab import *

In [145]: ion()

# use a semicolon to suppress the output
@savefig test_hist.png width=4in
In [151]: hist(np.random.randn(10000), 100);


@savefig test_plot.png width=4in
In [151]: plot(np.random.randn(10000), 'o');
   """,

        r"""
# use a semicolon to suppress the output
In [151]: plt.clf()

@savefig plot_simple.png width=4in
In [151]: plot([1,2,3])

@savefig hist_simple.png width=4in
In [151]: hist(np.random.randn(10000), 100);

""",
     r"""
# update the current fig
In [151]: ylabel('number')

In [152]: title('normal distribution')


@savefig hist_with_text.png
In [153]: grid(True)

        """,
        ]

    #ipython_directive.DEBUG = True  # dbg
    #options = dict(suppress=True)  # dbg
    options = dict()
    for example in examples:
        content = example.split('\n')
        ipython_directive('debug', arguments=None, options=options,
                          content=content, lineno=0,
                          content_offset=None, block_text=None,
                          state=None, state_machine=None,
                          )

# Run test suite as a script
if __name__=='__main__':
    if not os.path.isdir('_static'):
        os.mkdir('_static')
    test()
    print 'All OK? Check figures in _static/'

########NEW FILE########
__FILENAME__ = numpydoc
"""
========
numpydoc
========

Sphinx extension that handles docstrings in the Numpy standard format. [1]

It will:

- Convert Parameters etc. sections to field lists.
- Convert See Also section to a See also entry.
- Renumber references.
- Extract the signature from the docstring, if it can't be determined otherwise.

.. [1] http://projects.scipy.org/numpy/wiki/CodingStyleGuidelines#docstring-standard

"""

import sphinx

if sphinx.__version__ < '1.0.1':
    raise RuntimeError("Sphinx 1.0.1 or newer is required")

import os, re, pydoc
from docscrape_sphinx import get_doc_object, SphinxDocString
from sphinx.util.compat import Directive
import inspect

def mangle_docstrings(app, what, name, obj, options, lines,
                      reference_offset=[0]):

    cfg = dict(use_plots=app.config.numpydoc_use_plots,
               show_class_members=app.config.numpydoc_show_class_members)

    if what == 'module':
        # Strip top title
        title_re = re.compile(ur'^\s*[#*=]{4,}\n[a-z0-9 -]+\n[#*=]{4,}\s*',
                              re.I|re.S)
        lines[:] = title_re.sub(u'', u"\n".join(lines)).split(u"\n")
    else:
        doc = get_doc_object(obj, what, u"\n".join(lines), config=cfg)
        lines[:] = unicode(doc).split(u"\n")

    if app.config.numpydoc_edit_link and hasattr(obj, '__name__') and \
           obj.__name__:
        if hasattr(obj, '__module__'):
            v = dict(full_name=u"%s.%s" % (obj.__module__, obj.__name__))
        else:
            v = dict(full_name=obj.__name__)
        lines += [u'', u'.. htmlonly::', '']
        lines += [u'    %s' % x for x in
                  (app.config.numpydoc_edit_link % v).split("\n")]

    # replace reference numbers so that there are no duplicates
    references = []
    for line in lines:
        line = line.strip()
        m = re.match(ur'^.. \[([a-z0-9_.-])\]', line, re.I)
        if m:
            references.append(m.group(1))

    # start renaming from the longest string, to avoid overwriting parts
    references.sort(key=lambda x: -len(x))
    if references:
        for i, line in enumerate(lines):
            for r in references:
                if re.match(ur'^\d+$', r):
                    new_r = u"R%d" % (reference_offset[0] + int(r))
                else:
                    new_r = u"%s%d" % (r, reference_offset[0])
                lines[i] = lines[i].replace(u'[%s]_' % r,
                                            u'[%s]_' % new_r)
                lines[i] = lines[i].replace(u'.. [%s]' % r,
                                            u'.. [%s]' % new_r)

    reference_offset[0] += len(references)

def mangle_signature(app, what, name, obj, options, sig, retann):
    # Do not try to inspect classes that don't define `__init__`
    if (inspect.isclass(obj) and
        (not hasattr(obj, '__init__') or
        'initializes x; see ' in pydoc.getdoc(obj.__init__))):
        return '', ''

    if not (callable(obj) or hasattr(obj, '__argspec_is_invalid_')): return
    if not hasattr(obj, '__doc__'): return

    doc = SphinxDocString(pydoc.getdoc(obj))
    if doc['Signature']:
        sig = re.sub(u"^[^(]*", u"", doc['Signature'])
        return sig, u''

def setup(app, get_doc_object_=get_doc_object):
    global get_doc_object
    get_doc_object = get_doc_object_

    app.connect('autodoc-process-docstring', mangle_docstrings)
    app.connect('autodoc-process-signature', mangle_signature)
    app.add_config_value('numpydoc_edit_link', None, False)
    app.add_config_value('numpydoc_use_plots', None, False)
    app.add_config_value('numpydoc_show_class_members', True, True)

    # Extra mangling domains
    app.add_domain(NumpyPythonDomain)
    app.add_domain(NumpyCDomain)

#------------------------------------------------------------------------------
# Docstring-mangling domains
#------------------------------------------------------------------------------

from docutils.statemachine import ViewList
from sphinx.domains.c import CDomain
from sphinx.domains.python import PythonDomain

class ManglingDomainBase(object):
    directive_mangling_map = {}

    def __init__(self, *a, **kw):
        super(ManglingDomainBase, self).__init__(*a, **kw)
        self.wrap_mangling_directives()

    def wrap_mangling_directives(self):
        for name, objtype in self.directive_mangling_map.items():
            self.directives[name] = wrap_mangling_directive(
                self.directives[name], objtype)

class NumpyPythonDomain(ManglingDomainBase, PythonDomain):
    name = 'np'
    directive_mangling_map = {
        'function': 'function',
        'class': 'class',
        'exception': 'class',
        'method': 'function',
        'classmethod': 'function',
        'staticmethod': 'function',
        'attribute': 'attribute',
    }

class NumpyCDomain(ManglingDomainBase, CDomain):
    name = 'np-c'
    directive_mangling_map = {
        'function': 'function',
        'member': 'attribute',
        'macro': 'function',
        'type': 'class',
        'var': 'object',
    }

def wrap_mangling_directive(base_directive, objtype):
    class directive(base_directive):
        def run(self):
            env = self.state.document.settings.env

            name = None
            if self.arguments:
                m = re.match(r'^(.*\s+)?(.*?)(\(.*)?', self.arguments[0])
                name = m.group(2).strip()

            if not name:
                name = self.arguments[0]

            lines = list(self.content)
            mangle_docstrings(env.app, objtype, name, None, None, lines)
            self.content = ViewList(lines, self.content.parent)

            return base_directive.run(self)

    return directive


########NEW FILE########
__FILENAME__ = phantom_import
"""
==============
phantom_import
==============

Sphinx extension to make directives from ``sphinx.ext.autodoc`` and similar
extensions to use docstrings loaded from an XML file.

This extension loads an XML file in the Pydocweb format [1] and
creates a dummy module that contains the specified docstrings. This
can be used to get the current docstrings from a Pydocweb instance
without needing to rebuild the documented module.

.. [1] http://code.google.com/p/pydocweb

"""
import imp, sys, compiler, types, os, inspect, re

def setup(app):
    app.connect('builder-inited', initialize)
    app.add_config_value('phantom_import_file', None, True)

def initialize(app):
    fn = app.config.phantom_import_file
    if (fn and os.path.isfile(fn)):
        print "[numpydoc] Phantom importing modules from", fn, "..."
        import_phantom_module(fn)

#------------------------------------------------------------------------------
# Creating 'phantom' modules from an XML description
#------------------------------------------------------------------------------
def import_phantom_module(xml_file):
    """
    Insert a fake Python module to sys.modules, based on a XML file.

    The XML file is expected to conform to Pydocweb DTD. The fake
    module will contain dummy objects, which guarantee the following:

    - Docstrings are correct.
    - Class inheritance relationships are correct (if present in XML).
    - Function argspec is *NOT* correct (even if present in XML).
      Instead, the function signature is prepended to the function docstring.
    - Class attributes are *NOT* correct; instead, they are dummy objects.

    Parameters
    ----------
    xml_file : str
        Name of an XML file to read
    
    """
    import lxml.etree as etree

    object_cache = {}

    tree = etree.parse(xml_file)
    root = tree.getroot()

    # Sort items so that
    # - Base classes come before classes inherited from them
    # - Modules come before their contents
    all_nodes = dict([(n.attrib['id'], n) for n in root])
    
    def _get_bases(node, recurse=False):
        bases = [x.attrib['ref'] for x in node.findall('base')]
        if recurse:
            j = 0
            while True:
                try:
                    b = bases[j]
                except IndexError: break
                if b in all_nodes:
                    bases.extend(_get_bases(all_nodes[b]))
                j += 1
        return bases

    type_index = ['module', 'class', 'callable', 'object']
    
    def base_cmp(a, b):
        x = cmp(type_index.index(a.tag), type_index.index(b.tag))
        if x != 0: return x

        if a.tag == 'class' and b.tag == 'class':
            a_bases = _get_bases(a, recurse=True)
            b_bases = _get_bases(b, recurse=True)
            x = cmp(len(a_bases), len(b_bases))
            if x != 0: return x
            if a.attrib['id'] in b_bases: return -1
            if b.attrib['id'] in a_bases: return 1
        
        return cmp(a.attrib['id'].count('.'), b.attrib['id'].count('.'))

    nodes = root.getchildren()
    nodes.sort(base_cmp)

    # Create phantom items
    for node in nodes:
        name = node.attrib['id']
        doc = (node.text or '').decode('string-escape') + "\n"
        if doc == "\n": doc = ""

        # create parent, if missing
        parent = name
        while True:
            parent = '.'.join(parent.split('.')[:-1])
            if not parent: break
            if parent in object_cache: break
            obj = imp.new_module(parent)
            object_cache[parent] = obj
            sys.modules[parent] = obj

        # create object
        if node.tag == 'module':
            obj = imp.new_module(name)
            obj.__doc__ = doc
            sys.modules[name] = obj
        elif node.tag == 'class':
            bases = [object_cache[b] for b in _get_bases(node)
                     if b in object_cache]
            bases.append(object)
            init = lambda self: None
            init.__doc__ = doc
            obj = type(name, tuple(bases), {'__doc__': doc, '__init__': init})
            obj.__name__ = name.split('.')[-1]
        elif node.tag == 'callable':
            funcname = node.attrib['id'].split('.')[-1]
            argspec = node.attrib.get('argspec')
            if argspec:
                argspec = re.sub('^[^(]*', '', argspec)
                doc = "%s%s\n\n%s" % (funcname, argspec, doc)
            obj = lambda: 0
            obj.__argspec_is_invalid_ = True
            obj.func_name = funcname
            obj.__name__ = name
            obj.__doc__ = doc
            if inspect.isclass(object_cache[parent]):
                obj.__objclass__ = object_cache[parent]
        else:
            class Dummy(object): pass
            obj = Dummy()
            obj.__name__ = name
            obj.__doc__ = doc
            if inspect.isclass(object_cache[parent]):
                obj.__get__ = lambda: None
        object_cache[name] = obj

        if parent:
            if inspect.ismodule(object_cache[parent]):
                obj.__module__ = parent
                setattr(object_cache[parent], name.split('.')[-1], obj)

    # Populate items
    for node in root:
        obj = object_cache.get(node.attrib['id'])
        if obj is None: continue
        for ref in node.findall('ref'):
            if node.tag == 'class':
                if ref.attrib['ref'].startswith(node.attrib['id'] + '.'):
                    setattr(obj, ref.attrib['name'],
                            object_cache.get(ref.attrib['ref']))
            else:
                setattr(obj, ref.attrib['name'],
                        object_cache.get(ref.attrib['ref']))

########NEW FILE########
__FILENAME__ = plot_directive
"""
A special directive for generating a matplotlib plot.

.. warning::

   This is a hacked version of plot_directive.py from Matplotlib.
   It's very much subject to change!


Usage
-----

Can be used like this::

    .. plot:: examples/example.py

    .. plot::

       import matplotlib.pyplot as plt
       plt.plot([1,2,3], [4,5,6])

    .. plot::

       A plotting example:

       >>> import matplotlib.pyplot as plt
       >>> plt.plot([1,2,3], [4,5,6])

The content is interpreted as doctest formatted if it has a line starting
with ``>>>``.

The ``plot`` directive supports the options

    format : {'python', 'doctest'}
        Specify the format of the input

    include-source : bool
        Whether to display the source code. Default can be changed in conf.py
    
and the ``image`` directive options ``alt``, ``height``, ``width``,
``scale``, ``align``, ``class``.

Configuration options
---------------------

The plot directive has the following configuration options:

    plot_include_source
        Default value for the include-source option

    plot_pre_code
        Code that should be executed before each plot.

    plot_basedir
        Base directory, to which plot:: file names are relative to.
        (If None or empty, file names are relative to the directoly where
        the file containing the directive is.)

    plot_formats
        File formats to generate. List of tuples or strings::

            [(suffix, dpi), suffix, ...]

        that determine the file format and the DPI. For entries whose
        DPI was omitted, sensible defaults are chosen.

    plot_html_show_formats
        Whether to show links to the files in HTML.

TODO
----

* Refactor Latex output; now it's plain images, but it would be nice
  to make them appear side-by-side, or in floats.

"""

import sys, os, glob, shutil, imp, warnings, cStringIO, re, textwrap, traceback
import sphinx

import warnings
warnings.warn("A plot_directive module is also available under "
              "matplotlib.sphinxext; expect this numpydoc.plot_directive "
              "module to be deprecated after relevant features have been "
              "integrated there.",
              FutureWarning, stacklevel=2)


#------------------------------------------------------------------------------
# Registration hook
#------------------------------------------------------------------------------

def setup(app):
    setup.app = app
    setup.config = app.config
    setup.confdir = app.confdir
    
    app.add_config_value('plot_pre_code', '', True)
    app.add_config_value('plot_include_source', False, True)
    app.add_config_value('plot_formats', ['png', 'hires.png', 'pdf'], True)
    app.add_config_value('plot_basedir', None, True)
    app.add_config_value('plot_html_show_formats', True, True)

    app.add_directive('plot', plot_directive, True, (0, 1, False),
                      **plot_directive_options)

#------------------------------------------------------------------------------
# plot:: directive
#------------------------------------------------------------------------------
from docutils.parsers.rst import directives
from docutils import nodes

def plot_directive(name, arguments, options, content, lineno,
                   content_offset, block_text, state, state_machine):
    return run(arguments, content, options, state_machine, state, lineno)
plot_directive.__doc__ = __doc__

def _option_boolean(arg):
    if not arg or not arg.strip():
        # no argument given, assume used as a flag
        return True
    elif arg.strip().lower() in ('no', '0', 'false'):
        return False
    elif arg.strip().lower() in ('yes', '1', 'true'):
        return True
    else:
        raise ValueError('"%s" unknown boolean' % arg)

def _option_format(arg):
    return directives.choice(arg, ('python', 'lisp'))

def _option_align(arg):
    return directives.choice(arg, ("top", "middle", "bottom", "left", "center",
                                   "right"))

plot_directive_options = {'alt': directives.unchanged,
                          'height': directives.length_or_unitless,
                          'width': directives.length_or_percentage_or_unitless,
                          'scale': directives.nonnegative_int,
                          'align': _option_align,
                          'class': directives.class_option,
                          'include-source': _option_boolean,
                          'format': _option_format,
                          }

#------------------------------------------------------------------------------
# Generating output
#------------------------------------------------------------------------------

from docutils import nodes, utils

try:
    # Sphinx depends on either Jinja or Jinja2
    import jinja2
    def format_template(template, **kw):
        return jinja2.Template(template).render(**kw)
except ImportError:
    import jinja
    def format_template(template, **kw):
        return jinja.from_string(template, **kw)

TEMPLATE = """
{{ source_code }}

{{ only_html }}

   {% if source_link or (html_show_formats and not multi_image) %}
   (
   {%- if source_link -%}
   `Source code <{{ source_link }}>`__
   {%- endif -%}
   {%- if html_show_formats and not multi_image -%}
     {%- for img in images -%}
       {%- for fmt in img.formats -%}
         {%- if source_link or not loop.first -%}, {% endif -%}
         `{{ fmt }} <{{ dest_dir }}/{{ img.basename }}.{{ fmt }}>`__
       {%- endfor -%}
     {%- endfor -%}
   {%- endif -%}
   )
   {% endif %}

   {% for img in images %}
   .. figure:: {{ build_dir }}/{{ img.basename }}.png
      {%- for option in options %}
      {{ option }}
      {% endfor %}

      {% if html_show_formats and multi_image -%}
        (
        {%- for fmt in img.formats -%}
        {%- if not loop.first -%}, {% endif -%}
        `{{ fmt }} <{{ dest_dir }}/{{ img.basename }}.{{ fmt }}>`__
        {%- endfor -%}
        )
      {%- endif -%}
   {% endfor %}

{{ only_latex }}

   {% for img in images %}
   .. image:: {{ build_dir }}/{{ img.basename }}.pdf
   {% endfor %}

"""

class ImageFile(object):
    def __init__(self, basename, dirname):
        self.basename = basename
        self.dirname = dirname
        self.formats = []

    def filename(self, format):
        return os.path.join(self.dirname, "%s.%s" % (self.basename, format))

    def filenames(self):
        return [self.filename(fmt) for fmt in self.formats]

def run(arguments, content, options, state_machine, state, lineno):
    if arguments and content:
        raise RuntimeError("plot:: directive can't have both args and content")

    document = state_machine.document
    config = document.settings.env.config

    options.setdefault('include-source', config.plot_include_source)

    # determine input
    rst_file = document.attributes['source']
    rst_dir = os.path.dirname(rst_file)

    if arguments:
        if not config.plot_basedir:
            source_file_name = os.path.join(rst_dir,
                                            directives.uri(arguments[0]))
        else:
            source_file_name = os.path.join(setup.confdir, config.plot_basedir,
                                            directives.uri(arguments[0]))
        code = open(source_file_name, 'r').read()
        output_base = os.path.basename(source_file_name)
    else:
        source_file_name = rst_file
        code = textwrap.dedent("\n".join(map(str, content)))
        counter = document.attributes.get('_plot_counter', 0) + 1
        document.attributes['_plot_counter'] = counter
        base, ext = os.path.splitext(os.path.basename(source_file_name))
        output_base = '%s-%d.py' % (base, counter)

    base, source_ext = os.path.splitext(output_base)
    if source_ext in ('.py', '.rst', '.txt'):
        output_base = base
    else:
        source_ext = ''

    # ensure that LaTeX includegraphics doesn't choke in foo.bar.pdf filenames
    output_base = output_base.replace('.', '-')

    # is it in doctest format?
    is_doctest = contains_doctest(code)
    if options.has_key('format'):
        if options['format'] == 'python':
            is_doctest = False
        else:
            is_doctest = True

    # determine output directory name fragment
    source_rel_name = relpath(source_file_name, setup.confdir)
    source_rel_dir = os.path.dirname(source_rel_name)
    while source_rel_dir.startswith(os.path.sep):
        source_rel_dir = source_rel_dir[1:]

    # build_dir: where to place output files (temporarily)
    build_dir = os.path.join(os.path.dirname(setup.app.doctreedir),
                             'plot_directive',
                             source_rel_dir)
    if not os.path.exists(build_dir):
        os.makedirs(build_dir)

    # output_dir: final location in the builder's directory
    dest_dir = os.path.abspath(os.path.join(setup.app.builder.outdir,
                                            source_rel_dir))

    # how to link to files from the RST file
    dest_dir_link = os.path.join(relpath(setup.confdir, rst_dir),
                                 source_rel_dir).replace(os.path.sep, '/')
    build_dir_link = relpath(build_dir, rst_dir).replace(os.path.sep, '/')
    source_link = dest_dir_link + '/' + output_base + source_ext

    # make figures
    try:
        results = makefig(code, source_file_name, build_dir, output_base,
                          config)
        errors = []
    except PlotError, err:
        reporter = state.memo.reporter
        sm = reporter.system_message(
            2, "Exception occurred in plotting %s: %s" % (output_base, err),
            line=lineno)
        results = [(code, [])]
        errors = [sm]

    # generate output restructuredtext
    total_lines = []
    for j, (code_piece, images) in enumerate(results):
        if options['include-source']:
            if is_doctest:
                lines = ['']
                lines += [row.rstrip() for row in code_piece.split('\n')]
            else:
                lines = ['.. code-block:: python', '']
                lines += ['    %s' % row.rstrip()
                          for row in code_piece.split('\n')]
            source_code = "\n".join(lines)
        else:
            source_code = ""

        opts = [':%s: %s' % (key, val) for key, val in options.items()
                if key in ('alt', 'height', 'width', 'scale', 'align', 'class')]

        only_html = ".. only:: html"
        only_latex = ".. only:: latex"

        if j == 0:
            src_link = source_link
        else:
            src_link = None

        result = format_template(
            TEMPLATE,
            dest_dir=dest_dir_link,
            build_dir=build_dir_link,
            source_link=src_link,
            multi_image=len(images) > 1,
            only_html=only_html,
            only_latex=only_latex,
            options=opts,
            images=images,
            source_code=source_code,
            html_show_formats=config.plot_html_show_formats)

        total_lines.extend(result.split("\n"))
        total_lines.extend("\n")

    if total_lines:
        state_machine.insert_input(total_lines, source=source_file_name)

    # copy image files to builder's output directory
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)

    for code_piece, images in results:
        for img in images:
            for fn in img.filenames():
                shutil.copyfile(fn, os.path.join(dest_dir,
                                                 os.path.basename(fn)))

    # copy script (if necessary)
    if source_file_name == rst_file:
        target_name = os.path.join(dest_dir, output_base + source_ext)
        f = open(target_name, 'w')
        f.write(unescape_doctest(code))
        f.close()

    return errors


#------------------------------------------------------------------------------
# Run code and capture figures
#------------------------------------------------------------------------------

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.image as image
from matplotlib import _pylab_helpers

import exceptions

def contains_doctest(text):
    try:
        # check if it's valid Python as-is
        compile(text, '<string>', 'exec')
        return False
    except SyntaxError:
        pass
    r = re.compile(r'^\s*>>>', re.M)
    m = r.search(text)
    return bool(m)

def unescape_doctest(text):
    """
    Extract code from a piece of text, which contains either Python code
    or doctests.

    """
    if not contains_doctest(text):
        return text

    code = ""
    for line in text.split("\n"):
        m = re.match(r'^\s*(>>>|\.\.\.) (.*)$', line)
        if m:
            code += m.group(2) + "\n"
        elif line.strip():
            code += "# " + line.strip() + "\n"
        else:
            code += "\n"
    return code

def split_code_at_show(text):
    """
    Split code at plt.show()

    """

    parts = []
    is_doctest = contains_doctest(text)

    part = []
    for line in text.split("\n"):
        if (not is_doctest and line.strip() == 'plt.show()') or \
               (is_doctest and line.strip() == '>>> plt.show()'):
            part.append(line)
            parts.append("\n".join(part))
            part = []
        else:
            part.append(line)
    if "\n".join(part).strip():
        parts.append("\n".join(part))
    return parts

class PlotError(RuntimeError):
    pass

def run_code(code, code_path, ns=None):
    # Change the working directory to the directory of the example, so
    # it can get at its data files, if any.
    pwd = os.getcwd()
    old_sys_path = list(sys.path)
    if code_path is not None:
        dirname = os.path.abspath(os.path.dirname(code_path))
        os.chdir(dirname)
        sys.path.insert(0, dirname)

    # Redirect stdout
    stdout = sys.stdout
    sys.stdout = cStringIO.StringIO()

    # Reset sys.argv
    old_sys_argv = sys.argv
    sys.argv = [code_path]
    
    try:
        try:
            code = unescape_doctest(code)
            if ns is None:
                ns = {}
            if not ns:
                exec setup.config.plot_pre_code in ns
            exec code in ns
        except (Exception, SystemExit), err:
            raise PlotError(traceback.format_exc())
    finally:
        os.chdir(pwd)
        sys.argv = old_sys_argv
        sys.path[:] = old_sys_path
        sys.stdout = stdout
    return ns


#------------------------------------------------------------------------------
# Generating figures
#------------------------------------------------------------------------------

def out_of_date(original, derived):
    """
    Returns True if derivative is out-of-date wrt original,
    both of which are full file paths.
    """
    return (not os.path.exists(derived)
            or os.stat(derived).st_mtime < os.stat(original).st_mtime)


def makefig(code, code_path, output_dir, output_base, config):
    """
    Run a pyplot script *code* and save the images under *output_dir*
    with file names derived from *output_base*

    """

    # -- Parse format list
    default_dpi = {'png': 80, 'hires.png': 200, 'pdf': 50}
    formats = []
    for fmt in config.plot_formats:
        if isinstance(fmt, str):
            formats.append((fmt, default_dpi.get(fmt, 80)))
        elif type(fmt) in (tuple, list) and len(fmt)==2:
            formats.append((str(fmt[0]), int(fmt[1])))
        else:
            raise PlotError('invalid image format "%r" in plot_formats' % fmt)

    # -- Try to determine if all images already exist

    code_pieces = split_code_at_show(code)

    # Look for single-figure output files first
    all_exists = True
    img = ImageFile(output_base, output_dir)
    for format, dpi in formats:
        if out_of_date(code_path, img.filename(format)):
            all_exists = False
            break
        img.formats.append(format)

    if all_exists:
        return [(code, [img])]

    # Then look for multi-figure output files
    results = []
    all_exists = True
    for i, code_piece in enumerate(code_pieces):
        images = []
        for j in xrange(1000):
            img = ImageFile('%s_%02d_%02d' % (output_base, i, j), output_dir)
            for format, dpi in formats:
                if out_of_date(code_path, img.filename(format)):
                    all_exists = False
                    break
                img.formats.append(format)

            # assume that if we have one, we have them all
            if not all_exists:
                all_exists = (j > 0)
                break
            images.append(img)
        if not all_exists:
            break
        results.append((code_piece, images))

    if all_exists:
        return results

    # -- We didn't find the files, so build them

    results = []
    ns = {}

    for i, code_piece in enumerate(code_pieces):
        # Clear between runs
        plt.close('all')

        # Run code
        run_code(code_piece, code_path, ns)

        # Collect images
        images = []
        fig_managers = _pylab_helpers.Gcf.get_all_fig_managers()
        for j, figman in enumerate(fig_managers):
            if len(fig_managers) == 1 and len(code_pieces) == 1:
                img = ImageFile(output_base, output_dir)
            else:
                img = ImageFile("%s_%02d_%02d" % (output_base, i, j),
                                output_dir)
            images.append(img)
            for format, dpi in formats:
                try:
                    figman.canvas.figure.savefig(img.filename(format), dpi=dpi)
                except exceptions.BaseException, err:
                    raise PlotError(traceback.format_exc())
                img.formats.append(format)

        # Results
        results.append((code_piece, images))

    return results


#------------------------------------------------------------------------------
# Relative pathnames
#------------------------------------------------------------------------------

try:
    from os.path import relpath
except ImportError:
    # Copied from Python 2.7
    if 'posix' in sys.builtin_module_names:
        def relpath(path, start=os.path.curdir):
            """Return a relative version of a path"""
            from os.path import sep, curdir, join, abspath, commonprefix, \
                 pardir

            if not path:
                raise ValueError("no path specified")

            start_list = abspath(start).split(sep)
            path_list = abspath(path).split(sep)

            # Work out how much of the filepath is shared by start and path.
            i = len(commonprefix([start_list, path_list]))

            rel_list = [pardir] * (len(start_list)-i) + path_list[i:]
            if not rel_list:
                return curdir
            return join(*rel_list)
    elif 'nt' in sys.builtin_module_names:
        def relpath(path, start=os.path.curdir):
            """Return a relative version of a path"""
            from os.path import sep, curdir, join, abspath, commonprefix, \
                 pardir, splitunc

            if not path:
                raise ValueError("no path specified")
            start_list = abspath(start).split(sep)
            path_list = abspath(path).split(sep)
            if start_list[0].lower() != path_list[0].lower():
                unc_path, rest = splitunc(path)
                unc_start, rest = splitunc(start)
                if bool(unc_path) ^ bool(unc_start):
                    raise ValueError("Cannot mix UNC and non-UNC paths (%s and %s)"
                                                                        % (path, start))
                else:
                    raise ValueError("path is on drive %s, start on drive %s"
                                                        % (path_list[0], start_list[0]))
            # Work out how much of the filepath is shared by start and path.
            for i in range(min(len(start_list), len(path_list))):
                if start_list[i].lower() != path_list[i].lower():
                    break
            else:
                i += 1

            rel_list = [pardir] * (len(start_list)-i) + path_list[i:]
            if not rel_list:
                return curdir
            return join(*rel_list)
    else:
        raise RuntimeError("Unsupported platform (no relpath available!)")

########NEW FILE########
__FILENAME__ = traitsdoc
"""
=========
traitsdoc
=========

Sphinx extension that handles docstrings in the Numpy standard format, [1]
and support Traits [2].

This extension can be used as a replacement for ``numpydoc`` when support
for Traits is required.

.. [1] http://projects.scipy.org/numpy/wiki/CodingStyleGuidelines#docstring-standard
.. [2] http://code.enthought.com/projects/traits/

"""

import inspect
import os
import pydoc

import docscrape
import docscrape_sphinx
from docscrape_sphinx import SphinxClassDoc, SphinxFunctionDoc, SphinxDocString

import numpydoc

import comment_eater

class SphinxTraitsDoc(SphinxClassDoc):
    def __init__(self, cls, modulename='', func_doc=SphinxFunctionDoc):
        if not inspect.isclass(cls):
            raise ValueError("Initialise using a class. Got %r" % cls)
        self._cls = cls

        if modulename and not modulename.endswith('.'):
            modulename += '.'
        self._mod = modulename
        self._name = cls.__name__
        self._func_doc = func_doc

        docstring = pydoc.getdoc(cls)
        docstring = docstring.split('\n')

        # De-indent paragraph
        try:
            indent = min(len(s) - len(s.lstrip()) for s in docstring
                         if s.strip())
        except ValueError:
            indent = 0

        for n, line in enumerate(docstring):
            docstring[n] = docstring[n][indent:]

        self._doc = docscrape.Reader(docstring)
        self._parsed_data = {
            'Signature': '',
            'Summary': '',
            'Description': [],
            'Extended Summary': [],
            'Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'Other Parameters': [],
            'Traits': [],
            'Methods': [],
            'See Also': [],
            'Notes': [],
            'References': '',
            'Example': '',
            'Examples': '',
            'index': {}
            }

        self._parse()

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Description'] + self['Extended Summary'] + ['']

    def __str__(self, indent=0, func_role="func"):
        out = []
        out += self._str_signature()
        out += self._str_index() + ['']
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Traits', 'Methods',
                           'Returns', 'Raises'):
            out += self._str_param_list(param_list)
        out += self._str_see_also("obj")
        out += self._str_section('Notes')
        out += self._str_references()
        out += self._str_section('Example')
        out += self._str_section('Examples')
        out = self._str_indent(out, indent)
        return '\n'.join(out)

def looks_like_issubclass(obj, classname):
    """ Return True if the object has a class or superclass with the given class
    name.

    Ignores old-style classes.
    """
    t = obj
    if t.__name__ == classname:
        return True
    for klass in t.__mro__:
        if klass.__name__ == classname:
            return True
    return False

def get_doc_object(obj, what=None, config=None):
    if what is None:
        if inspect.isclass(obj):
            what = 'class'
        elif inspect.ismodule(obj):
            what = 'module'
        elif callable(obj):
            what = 'function'
        else:
            what = 'object'
    if what == 'class':
        doc = SphinxTraitsDoc(obj, '', func_doc=SphinxFunctionDoc, config=config)
        if looks_like_issubclass(obj, 'HasTraits'):
            for name, trait, comment in comment_eater.get_class_traits(obj):
                # Exclude private traits.
                if not name.startswith('_'):
                    doc['Traits'].append((name, trait, comment.splitlines()))
        return doc
    elif what in ('function', 'method'):
        return SphinxFunctionDoc(obj, '', config=config)
    else:
        return SphinxDocString(pydoc.getdoc(obj), config=config)

def setup(app):
    # init numpydoc
    numpydoc.setup(app, get_doc_object)


########NEW FILE########
__FILENAME__ = add-column
"Example showing how to add a column on a existing column"

from __future__ import print_function
import tables


class Particle(tables.IsDescription):
    name = tables.StringCol(16, pos=1)      # 16-character String
    lati = tables.Int32Col(pos=2)           # integer
    longi = tables.Int32Col(pos=3)          # integer
    pressure = tables.Float32Col(pos=4)     # float  (single-precision)
    temperature = tables.Float64Col(pos=5)  # double (double-precision)

# Open a file in "w"rite mode
fileh = tables.open_file("add-column.h5", mode="w")
# Create a new group
group = fileh.create_group(fileh.root, "newgroup")

# Create a new table in newgroup group
table = fileh.create_table(group, 'table', Particle, "A table",
                           tables.Filters(1))

# Append several rows
table.append([("Particle:     10", 10, 0, 10 * 10, 10 ** 2),
              ("Particle:     11", 11, -1, 11 * 11, 11 ** 2),
              ("Particle:     12", 12, -2, 12 * 12, 12 ** 2)])

print("Contents of the original table:", fileh.root.newgroup.table[:])

# close the file
fileh.close()

# Open it again in append mode
fileh = tables.open_file("add-column.h5", "a")
group = fileh.root.newgroup
table = group.table

# Get a description of table in dictionary format
descr = table.description._v_colobjects
descr2 = descr.copy()

# Add a column to description
descr2["hot"] = tables.BoolCol(dflt=False)

# Create a new table with the new description
table2 = fileh.create_table(group, 'table2', descr2, "A table",
                            tables.Filters(1))

# Copy the user attributes
table.attrs._f_copy(table2)

# Fill the rows of new table with default values
for i in range(table.nrows):
    table2.row.append()
# Flush the rows to disk
table2.flush()

# Copy the columns of source table to destination
for col in descr:
    getattr(table2.cols, col)[:] = getattr(table.cols, col)[:]

# Fill the new column
table2.cols.hot[:] = [row["temperature"] > 11 ** 2 for row in table]

# Remove the original table
table.remove()

# Move table2 to table
table2.move('/newgroup', 'table')

# Print the new table
print("Contents of the table with column added:", fileh.root.newgroup.table[:])

# Finally, close the file
fileh.close()

########NEW FILE########
__FILENAME__ = array1
from __future__ import print_function
import numpy as np
import tables

# Open a new empty HDF5 file
fileh = tables.open_file("array1.h5", mode="w")
# Get the root group
root = fileh.root

# Create an Array
a = np.array([-1, 2, 4], np.int16)
# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'array_1', a, "Signed short array")

# Create a scalar Array
a = np.array(4, np.int16)
# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'array_s', a, "Scalar signed short array")

# Create a 3-d array of floats
a = np.arange(120, dtype=np.float64).reshape(20, 3, 2)
# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'array_f', a, "3-D float array")

# Close the file
fileh.close()

# Open the file for reading
fileh = tables.open_file("array1.h5", mode="r")
# Get the root group
root = fileh.root

a = root.array_1.read()
print("Signed byte array -->", repr(a), a.shape)

print("Testing iterator (works even over scalar arrays):", end=' ')
arr = root.array_s
for x in arr:
    print("nrow-->", arr.nrow)
    print("Element-->", repr(x))

# print "Testing getitem:"
# for i in range(root.array_1.nrows):
#     print "array_1["+str(i)+"]", "-->", root.array_1[i]

print("array_f[:,2:3,2::2]", repr(root.array_f[:, 2:3, 2::2]))
print("array_f[1,2:]", repr(root.array_f[1, 2:]))
print("array_f[1]", repr(root.array_f[1]))

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = array2
from __future__ import print_function
import numpy as np
import tables

# Open a new empty HDF5 file
fileh = tables.open_file("array2.h5", mode="w")
# Shortcut to the root group
root = fileh.root

# Create an array
a = np.array([1, 2.7182818284590451, 3.141592], float)
print("About to write array:", a)
print("  with shape: ==>", a.shape)
print("  and dtype ==>", a.dtype)

# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'carray', a, "Float array")

# Get metadata on the previously saved array
print()
print("Info on the object:", repr(root.carray))

# Close the file
fileh.close()

# Open the previous HDF5 file in read-only mode
fileh = tables.open_file("array2.h5", mode="r")
# Get the root group
root = fileh.root

# Get metadata on the previously saved array
print()
print("Info on the object:", repr(root.carray))

# Get the actual array
b = root.carray.read()
print()
print("Array read from file:", b)
print("  with shape: ==>", b.shape)
print("  and dtype ==>", b.dtype)

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = array3
from __future__ import print_function
import numpy as np
import tables

# Open a new empty HDF5 file
fileh = tables.open_file("array3.h5", mode="w")
# Get the root group
root = fileh.root

# Create a large array
# a = reshape(array(range(2**16), "s"), (2,) * 16)
a = np.ones((2,) * 8, np.int8)
print("About to write array a")
print("  with shape: ==>", a.shape)
print("  and dtype: ==>", a.dtype)

# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'carray', a, "Large array")

# Get metadata on the previously saved array
print()
print("Info on the object:", repr(root.carray))

# Close the file
fileh.close()

# Open the previous HDF5 file in read-only mode
fileh = tables.open_file("array3.h5", mode="r")
# Get the root group
root = fileh.root

# Get metadata on the previously saved array
print()
print("Getting info on retrieved /carray object:", repr(root.carray))

# Get the actual array
# b = fileh.readArray("/carray")
# You can obtain the same result with:
b = root.carray.read()
print()
print("Array b read from file")
print("  with shape: ==>", b.shape)
print("  with dtype: ==>", b.dtype)
# print "  contents:", b

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = array4
from __future__ import print_function
import numpy as np
import tables

basedim = 4
file = "array4.h5"
# Open a new empty HDF5 file
fileh = tables.open_file(file, mode="w")
# Get the root group
group = fileh.root
# Set the type codes to test
dtypes = [np.int8, np.uint8, np.int16, np.int, np.float32, np.float]
i = 1
for dtype in dtypes:
    # Create an array of dtype, with incrementally bigger ranges
    a = np.ones((basedim,) * i, dtype)
    # Save it on the HDF5 file
    dsetname = 'array_' + a.dtype.char
    hdfarray = fileh.create_array(group, dsetname, a, "Large array")
    print("Created dataset:", hdfarray)
    # Create a new group
    group = fileh.create_group(group, 'group' + str(i))
    # increment the range for next iteration
    i += 1

# Close the file
fileh.close()


# Open the previous HDF5 file in read-only mode
fileh = tables.open_file(file, mode="r")
# Get the root group
group = fileh.root
# Get the metadata on the previosly saved arrays
for i in range(len(dtypes)):
    # Create an array for later comparison
    a = np.ones((basedim,) * (i + 1), dtypes[i])
    # Get the dset object hangin from group
    dset = getattr(group, 'array_' + a.dtype.char)
    print("Info from dataset:", repr(dset))
    # Read the actual data in array
    b = dset.read()
    print("Array b read from file. Shape ==>", b.shape, end=' ')
    print(". Dtype ==> %s" % b.dtype)
    # Test if the original and read arrays are equal
    if np.allclose(a, b):
        print("Good: Read array is equal to the original")
    else:
        print("Error: Read array and the original differs!")
    # Iterate over the next group
    group = getattr(group, 'group' + str(i + 1))

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = attributes1
import numpy as np
import tables

# Open a new empty HDF5 file
fileh = tables.open_file("attributes1.h5", mode="w",
                         title="Testing attributes")
# Get the root group
root = fileh.root

# Create an array
a = np.array([1, 2, 4], np.int32)
# Save it on the HDF5 file
hdfarray = fileh.create_array(root, 'array', a, "Integer array")

# Assign user attributes

# A string
hdfarray.attrs.string = "This is an example"

# A Char
hdfarray.attrs.char = "1"

# An integer
hdfarray.attrs.int = 12

# A float
hdfarray.attrs.float = 12.32

# A generic object
hdfarray.attrs.object = {"a": 32.1, "b": 1, "c": [1, 2]}

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = carray1
from __future__ import print_function
import numpy
import tables

fileName = 'carray1.h5'
shape = (200, 300)
atom = tables.UInt8Atom()
filters = tables.Filters(complevel=5, complib='zlib')

h5f = tables.open_file(fileName, 'w')
ca = h5f.create_carray(h5f.root, 'carray', atom, shape, filters=filters)
# Fill a hyperslab in ``ca``.
ca[10:60, 20:70] = numpy.ones((50, 50))
h5f.close()

# Re-open and read another hyperslab
h5f = tables.open_file(fileName)
print(h5f)
print(h5f.root.carray[8:12, 18:22])
h5f.close()

########NEW FILE########
__FILENAME__ = earray1
from __future__ import print_function
import tables
import numpy

fileh = tables.open_file('earray1.h5', mode='w')
a = tables.StringAtom(itemsize=8)
# Use ``a`` as the object type for the enlargeable array.
array_c = fileh.create_earray(fileh.root, 'array_c', a, (0,), "Chars")
array_c.append(numpy.array(['a' * 2, 'b' * 4], dtype='S8'))
array_c.append(numpy.array(['a' * 6, 'b' * 8, 'c' * 10], dtype='S8'))

# Read the string ``EArray`` we have created on disk.
for s in array_c:
    print('array_c[%s] => %r' % (array_c.nrow, s))
# Close the file.
fileh.close()

########NEW FILE########
__FILENAME__ = earray2
#!/usr/bin/env python

"""Small example that shows how to work with extendeable arrays of different
types, strings included."""

from __future__ import print_function
import numpy as np
import tables

# Open a new empty HDF5 file
filename = "earray2.h5"
fileh = tables.open_file(filename, mode="w")
# Get the root group
root = fileh.root

# Create an string atom
a = tables.StringAtom(itemsize=1)
# Use it as a type for the enlargeable array
hdfarray = fileh.create_earray(root, 'array_c', a, (0,), "Character array")
hdfarray.append(np.array(['a', 'b', 'c']))
# The next is legal:
hdfarray.append(np.array(['c', 'b', 'c', 'd']))
# but these are not:
# hdfarray.append(array([['c', 'b'], ['c', 'd']]))
# hdfarray.append(array([[1,2,3],[3,2,1]], dtype=uint8).reshape(2,1,3))

# Create an atom
a = tables.UInt16Atom()
hdfarray = fileh.create_earray(root, 'array_e', a, (2, 0, 3),
                               "Unsigned short array")

# Create an enlargeable array
a = tables.UInt8Atom()
hdfarray = fileh.create_earray(root, 'array_b', a, (2, 0, 3),
                               "Unsigned byte array",
                               tables.Filters(complevel=1))

# Append an array to this table
hdfarray.append(
    np.array([[1, 2, 3], [3, 2, 1]], dtype=np.uint8).reshape(2, 1, 3))
hdfarray.append(
    np.array([[1, 2, 3], [3, 2, 1], [2, 4, 6], [6, 4, 2]],
             dtype=np.uint8).reshape(2, 2, 3) * 2)
# The next should give a type error:
# hdfarray.append(array([[1,0,1],[0,0,1]], dtype=Bool).reshape(2,1,3))

# Close the file
fileh.close()

# Open the file for reading
fileh = tables.open_file(filename, mode="r")
# Get the root group
root = fileh.root

a = root.array_c.read()
print("Character array -->", repr(a), a.shape)
a = root.array_e.read()
print("Empty array (yes, this is suported) -->", repr(a), a.shape)
a = root.array_b.read(step=2)
print("Int8 array, even rows (step = 2) -->", repr(a), a.shape)

print("Testing iterator:", end=' ')
# for x in root.array_b.iterrows(step=2):
for x in root.array_b:
    print("nrow-->", root.array_b.nrow)
    print("Element-->", x)

print("Testing getitem:")
for i in range(root.array_b.shape[0]):
    print("array_b[" + str(i) + "]", "-->", root.array_b[i])
# The nrows counts the growing dimension, which is different from the
# first index
for i in range(root.array_b.nrows):
    print("array_b[:," + str(i) + ",:]", "-->", root.array_b[:, i, :])
print("array_c[1:2]", repr(root.array_c[1:2]))
print("array_c[1:3]", repr(root.array_c[1:3]))
print("array_b[:]", root.array_b[:])

print(repr(root.array_c))
# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = enum
# Example on using enumerated types under PyTables.
# This file is intended to be run in an interactive Python session,
# since it contains some statements that raise exceptions.
# To run it, paste it as the input of ``python``.

from __future__ import print_function


def COMMENT(string):
    pass


COMMENT("**** Usage of the ``Enum`` class. ****")

COMMENT("Create an enumeration of colors with automatic concrete values.")
import tables
colorList = ['red', 'green', 'blue', 'white', 'black']
colors = tables.Enum(colorList)

COMMENT("Take a look at the name-value pairs.")
print("Colors:", [v for v in colors])

COMMENT("Access values as attributes.")
print("Value of 'red' and 'white':", (colors.red, colors.white))
print("Value of 'yellow':", colors.yellow)

COMMENT("Access values as items.")
print("Value of 'red' and 'white':", (colors['red'], colors['white']))
print("Value of 'yellow':", colors['yellow'])

COMMENT("Access names.")
print("Name of value %s:" % colors.red, colors(colors.red))
print("Name of value 1234:", colors(1234))


COMMENT("**** Enumerated columns. ****")

COMMENT("Create a new PyTables file.")
h5f = tables.open_file('enum.h5', 'w')

COMMENT("This describes a ball extraction.")


class BallExt(tables.IsDescription):
    ballTime = tables.Time32Col()
    ballColor = tables.EnumCol(colors, 'black', base='uint8')

COMMENT("Create a table of ball extractions.")
tbl = h5f.create_table(
    '/', 'extractions', BallExt, title="Random ball extractions")

COMMENT("Simulate some ball extractions.")
import time
import random
now = time.time()
row = tbl.row
for i in range(10):
    row['ballTime'] = now + i
    row['ballColor'] = colors[random.choice(colorList)]  # notice this
    row.append()

COMMENT("Try to append an invalid value.")
row['ballTime'] = now + 42
row['ballColor'] = 1234

tbl.flush()

COMMENT("Now print them!")
for r in tbl:
    ballTime = r['ballTime']
    ballColor = colors(r['ballColor'])  # notice this
    print("Ball extracted on %d is of color %s." % (ballTime, ballColor))


COMMENT("**** Enumerated arrays. ****")

COMMENT("This describes a range of working days.")
workingDays = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5}
dayRange = tables.EnumAtom(workingDays, 'Mon', base='uint16', shape=(0, 2))

COMMENT("Create an EArray of day ranges within a week.")
earr = h5f.create_earray('/', 'days', dayRange, title="Working day ranges")
earr.flavor = 'python'

COMMENT("Throw some day ranges in.")
wdays = earr.get_enum()
earr.append([(wdays.Mon, wdays.Fri), (wdays.Wed, wdays.Fri)])

COMMENT("The append method does not check values!")
earr.append([(wdays.Mon, 1234)])

COMMENT("Print the values.")
for (d1, d2) in earr:
    print("From %s to %s (%d days)." % (wdays(d1), wdays(d2), d2 - d1 + 1))

COMMENT("Close the PyTables file and remove it.")
import os
h5f.close()
os.remove('enum.h5')

########NEW FILE########
__FILENAME__ = filenodes1
from __future__ import print_function
from tables.nodes import filenode

import tables
h5file = tables.open_file('fnode.h5', 'w')


fnode = filenode.new_node(h5file, where='/', name='fnode_test')


print(h5file.get_node_attr('/fnode_test', 'NODE_TYPE'))


print("This is a test text line.", file=fnode)
print("And this is another one.", file=fnode)
print(file=fnode)
fnode.write("Of course, file methods can also be used.")

fnode.seek(0)  # Go back to the beginning of file.

for line in fnode:
    print(repr(line))


fnode.close()
print(fnode.closed)


node = h5file.root.fnode_test
fnode = filenode.open_node(node, 'a+')
print(repr(fnode.readline()))
print(fnode.tell())
print("This is a new line.", file=fnode)
print(repr(fnode.readline()))


fnode.seek(0)
for line in fnode:
    print(repr(line))


fnode.attrs.content_type = 'text/plain; charset=us-ascii'


fnode.attrs.author = "Ivan Vilata i Balaguer"
fnode.attrs.creation_date = '2004-10-20T13:25:25+0200'
fnode.attrs.keywords_en = ["FileNode", "test", "metadata"]
fnode.attrs.keywords_ca = ["FileNode", "prova", "metadades"]
fnode.attrs.owner = 'ivan'
fnode.attrs.acl = {'ivan': 'rw', '@users': 'r'}


fnode.close()
h5file.close()

########NEW FILE########
__FILENAME__ = index
from __future__ import print_function
import random
import tables
print('tables.__version__', tables.__version__)

nrows = 10000 - 1


class Distance(tables.IsDescription):
    frame = tables.Int32Col(pos=0)
    distance = tables.Float64Col(pos=1)

h5file = tables.open_file('index.h5', mode='w')
table = h5file.create_table(h5file.root, 'distance_table', Distance,
                            'distance table', expectedrows=nrows)
row = table.row
for i in range(nrows):
    # r['frame'] = nrows-i
    row['frame'] = random.randint(0, nrows)
    row['distance'] = float(i ** 2)
    row.append()
table.flush()

table.cols.frame.create_index(optlevel=9, _testmode=True, _verbose=True)
# table.cols.frame.optimizeIndex(level=5, verbose=1)

results = [r.nrow for r in table.where('frame < 2')]
print("frame<2 -->", table.read_coordinates(results))
# print("frame<2 -->", table.get_where_list('frame < 2'))

results = [r.nrow for r in table.where('(1 < frame) & (frame <= 5)')]
print("rows-->", results)
print("1<frame<=5 -->", table.read_coordinates(results))
# print("1<frame<=5 -->", table.get_where_list('(1 < frame) & (frame <= 5)'))

h5file.close()

########NEW FILE########
__FILENAME__ = inmemory
#!/usr/bin/env python
# encoding: utf-8

"""inmemory.py.

Example usage of creating in-memory HDF5 file with a specified chunksize
using PyTables 3.0.0+

See also Cookbook page
http://pytables.github.io/cookbook/inmemory_hdf5_files.html and available
drivers
http://pytables.github.io/usersguide/parameter_files.html#hdf5-driver-management

"""

import numpy as np
import tables

CHUNKY = 30
CHUNKX = 4320

if __name__ == '__main__':

    # create dataset and add global attrs
    file_path = 'demofile_chunk%sx%d.h5' % (CHUNKY, CHUNKX)

    with tables.open_file(file_path, 'w',
                          title='PyTables HDF5 In-memory example',
                          driver='H5FD_CORE') as h5f:

        # dummy some data
        lats = np.empty([2160])
        lons = np.empty([4320])

        # create some simple arrays
        lat_node = h5f.create_array('/', 'lat', lats, title='latitude')
        lon_node = h5f.create_array('/', 'lon', lons, title='longitude')

        # create a 365 x 4320 x 8640 CArray of 32bit float
        shape = (5, 2160, 4320)
        atom = tables.Float32Atom(dflt=np.nan)

        # chunk into daily slices and then further chunk days
        sst_node = h5f.create_carray(
            h5f.root, 'sst', atom, shape, chunkshape=(1, CHUNKY, CHUNKX))

        # dummy up an ndarray
        sst = np.empty([2160, 4320], dtype=np.float32)
        sst.fill(30.0)

        # write ndarray to a 2D plane in the HDF5
        sst_node[0] = sst

########NEW FILE########
__FILENAME__ = links
from __future__ import print_function
import tables as tb

# Create a new file with some structural groups
f1 = tb.open_file('links1.h5', 'w')
g1 = f1.create_group('/', 'g1')
g2 = f1.create_group(g1, 'g2')

# Create some datasets
a1 = f1.create_carray(g1, 'a1', tb.Int64Atom(), shape=(10000,))
t1 = f1.create_table(g2, 't1', {'f1': tb.IntCol(), 'f2': tb.FloatCol()})

# Create new group and a first hard link
gl = f1.create_group('/', 'gl')
ht = f1.create_hard_link(gl, 'ht', '/g1/g2/t1')  # ht points to t1
print("``%s`` is a hard link to: ``%s``" % (ht, t1))

# Remove the orginal link to the t1 table
t1.remove()
print("table continues to be accessible in: ``%s``" % f1.get_node('/gl/ht'))

# Let's continue with soft links
la1 = f1.create_soft_link(gl, 'la1', '/g1/a1')  # la1 points to a1
print("``%s`` is a soft link to: ``%s``" % (la1, la1.target))
lt = f1.create_soft_link(gl, 'lt', '/g1/g2/t1')  # lt points to t1 (dangling)
print("``%s`` is a soft link to: ``%s``" % (lt, lt.target))

# Recreate the '/g1/g2/t1' path
t1 = f1.create_hard_link('/g1/g2', 't1', '/gl/ht')
print("``%s`` is not dangling anymore" % (lt,))

# Dereferrencing
plt = lt()
print("dereferred lt node: ``%s``" % plt)
pla1 = la1()
print("dereferred la1 node: ``%s``" % pla1)

# Copy the array a1 into another file
f2 = tb.open_file('links2.h5', 'w')
new_a1 = a1.copy(f2.root, 'a1')
f2.close()  # close the other file

# Remove the original soft link and create an external link
la1.remove()
la1 = f1.create_external_link(gl, 'la1', 'links2.h5:/a1')
print("``%s`` is an external link to: ``%s``" % (la1, la1.target))
new_a1 = la1()  # dereferrencing la1 returns a1 in links2.h5
print("dereferred la1 node:  ``%s``" % new_a1)
print("new_a1 file:", new_a1._v_file.filename)

f1.close()

########NEW FILE########
__FILENAME__ = multiprocess_access_benchmarks
# Benchmark three methods of using PyTables with multiple processes, where data
# is read from a PyTables file in one process and then sent to another
#
# 1. using multiprocessing.Pipe
# 2. using a memory mapped file that's shared between two processes, passed as
#    out argument to tables.Array.read.
# 3. using a Unix domain socket (this uses the "abstract namespace" and will
#    work only on Linux).
# 4. using an IPv4 socket
#
# In all three cases, an array is loaded from a file in one process, sent to
# another, and then modified by incrementing each array element.  This is meant
# to simulate retrieving data and then modifying it.

from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import multiprocessing
import os
import random
import select
import socket
import time

import numpy as np
import tables


# create a PyTables file with a single int64 array with the specified number of
# elements
def create_file(array_size):
    array = np.ones(array_size, dtype='i8')
    with tables.open_file('test.h5', 'w') as fobj:
        array = fobj.create_array('/', 'test', array)
        print('file created, size: {0} MB'.format(array.size_on_disk / 1e6))


# process to receive an array using a multiprocessing.Pipe connection
class PipeReceive(multiprocessing.Process):

    def __init__(self, receiver_pipe, result_send):
        super(PipeReceive, self).__init__()
        self.receiver_pipe = receiver_pipe
        self.result_send = result_send

    def run(self):
        # block until something is received on the pipe
        array = self.receiver_pipe.recv()
        recv_timestamp = time.time()
        # perform an operation on the received array
        array += 1
        finish_timestamp = time.time()
        assert(np.all(array == 2))
        # send the measured timestamps back to the originating process
        self.result_send.send((recv_timestamp, finish_timestamp))


def read_and_send_pipe(send_type, array_size):
    # set up Pipe objects to send the actual array to the other process
    # and receive the timing results from the other process
    array_recv, array_send = multiprocessing.Pipe(False)
    result_recv, result_send = multiprocessing.Pipe(False)
    # start the other process and pause to allow it to start up
    recv_process = PipeReceive(array_recv, result_send)
    recv_process.start()
    time.sleep(0.15)
    with tables.open_file('test.h5', 'r') as fobj:
        array = fobj.get_node('/', 'test')
        start_timestamp = time.time()
        # read an array from the PyTables file and send it to the other process
        output = array.read(0, array_size, 1)
        array_send.send(output)
        assert(np.all(output + 1 == 2))
        # receive the timestamps from the other process
        recv_timestamp, finish_timestamp = result_recv.recv()
    print_results(send_type, start_timestamp, recv_timestamp, finish_timestamp)
    recv_process.join()


# process to receive an array using a shared memory mapped file
# for real use, this would require creating some protocol to specify the
# array's data type and shape
class MemmapReceive(multiprocessing.Process):

    def __init__(self, path_recv, result_send):
        super(MemmapReceive, self).__init__()
        self.path_recv = path_recv
        self.result_send = result_send

    def run(self):
        # block until the memmap file path is received from the other process
        path = self.path_recv.recv()
        # create a memmap array using the received file path
        array = np.memmap(path, 'i8', 'r+')
        recv_timestamp = time.time()
        # perform an operation on the array
        array += 1
        finish_timestamp = time.time()
        assert(np.all(array == 2))
        # send the timing results back to the other process
        self.result_send.send((recv_timestamp, finish_timestamp))


def read_and_send_memmap(send_type, array_size):
    # create a multiprocessing Pipe that will be used to send the memmap
    # file path to the receiving process
    path_recv, path_send = multiprocessing.Pipe(False)
    result_recv, result_send = multiprocessing.Pipe(False)
    # start the receiving process and pause to allow it to start up
    recv_process = MemmapReceive(path_recv, result_send)
    recv_process.start()
    time.sleep(0.15)
    with tables.open_file('test.h5', 'r') as fobj:
        array = fobj.get_node('/', 'test')
        start_timestamp = time.time()
        # memmap a file as a NumPy array in 'overwrite' mode
        output = np.memmap('/tmp/array1', 'i8', 'w+', shape=(array_size, ))
        # read an array from a PyTables file into the memmory mapped array
        array.read(0, array_size, 1, out=output)
        # use a multiprocessing.Pipe to send the file's path to the receiving
        # process
        path_send.send('/tmp/array1')
        # receive the timestamps from the other process
        recv_timestamp, finish_timestamp = result_recv.recv()
        # because 'output' is shared between processes, all elements should now
        # be equal to 2
        assert(np.all(output == 2))
    print_results(send_type, start_timestamp, recv_timestamp, finish_timestamp)
    recv_process.join()


# process to receive an array using a socket
# for real use, this would require creating some protocol to specify the
# array's data type and shape
class SocketReceive(multiprocessing.Process):

    def __init__(self, socket_family, address, result_send, array_nbytes):
        super(SocketReceive, self).__init__()
        self.socket_family = socket_family
        self.address = address
        self.result_send = result_send
        self.array_nbytes = array_nbytes

    def run(self):
        # create the socket, listen for a connection and use select to block
        # until a connection is made
        sock = socket.socket(self.socket_family, socket.SOCK_STREAM)
        sock.bind(self.address)
        sock.listen(1)
        readable, _, _ = select.select([sock], [], [])
        # accept the connection and read the sent data into a bytearray
        connection = sock.accept()[0]
        recv_buffer = bytearray(self.array_nbytes)
        view = memoryview(recv_buffer)
        bytes_recv = 0
        while bytes_recv < self.array_nbytes:
            bytes_recv += connection.recv_into(view[bytes_recv:])
        # convert the bytearray into a NumPy array
        array = np.frombuffer(recv_buffer, dtype='i8')
        recv_timestamp = time.time()
        # perform an operation on the received array
        array += 1
        finish_timestamp = time.time()
        assert(np.all(array == 2))
        # send the timestamps back to the originating process
        self.result_send.send((recv_timestamp, finish_timestamp))
        connection.close()
        sock.close()


def unix_socket_address():
    # create a Unix domain address in the abstract namespace
    # this will only work on Linux
    return b'\x00' + os.urandom(5)


def ipv4_socket_address():
    # create an IPv4 socket address
    return ('127.0.0.1', random.randint(9000, 10000))


def read_and_send_socket(send_type, array_size, array_bytes, address_func,
                         socket_family):
    address = address_func()
    # start the receiving process and pause to allow it to start up
    result_recv, result_send = multiprocessing.Pipe(False)
    recv_process = SocketReceive(socket_family, address, result_send,
                                 array_bytes)
    recv_process.start()
    time.sleep(0.15)
    with tables.open_file('test.h5', 'r') as fobj:
        array = fobj.get_node('/', 'test')
        start_timestamp = time.time()
        # connect to the receiving process' socket
        sock = socket.socket(socket_family, socket.SOCK_STREAM)
        sock.connect(address)
        # read the array from the PyTables file and send its
        # data buffer to the receiving process
        output = array.read(0, array_size, 1)
        sock.send(output.data)
        assert(np.all(output + 1 == 2))
        # receive the timestamps from the other process
        recv_timestamp, finish_timestamp = result_recv.recv()
    sock.close()
    print_results(send_type, start_timestamp, recv_timestamp, finish_timestamp)
    recv_process.join()


def print_results(send_type, start_timestamp, recv_timestamp,
                  finish_timestamp):
    msg = 'type: {0}\t receive: {1:5.5f}, add:{2:5.5f}, total: {3:5.5f}'
    print(msg.format(send_type,
                     recv_timestamp - start_timestamp,
                     finish_timestamp - recv_timestamp,
                     finish_timestamp - start_timestamp))


if __name__ == '__main__':

    random.seed(os.urandom(2))
    array_num_bytes = [int(x) for x in [1e5, 1e6, 1e7, 1e8]]

    for array_bytes in array_num_bytes:
        array_size = int(array_bytes // 8)

        create_file(array_size)
        read_and_send_pipe('multiproc.Pipe', array_size)
        read_and_send_memmap('memmap     ', array_size)
        # comment out this line to run on an OS other than Linux
        read_and_send_socket('Unix socket', array_size, array_bytes,
                             unix_socket_address, socket.AF_UNIX)
        read_and_send_socket('IPv4 socket', array_size, array_bytes,
                             ipv4_socket_address, socket.AF_INET)
        print()

########NEW FILE########
__FILENAME__ = multiprocess_access_queues
"""Example showing how to access a PyTables file from multiple processes using
queues."""

from __future__ import print_function
import Queue as queue
import multiprocessing
import os
import random
import time

import numpy
import tables


# this creates an HDF5 file with one array containing n rows
def make_file(file_path, n):

    with tables.open_file(file_path, 'w') as fobj:
        array = fobj.create_carray('/', 'array', tables.Int64Atom(), (n, n))
        for i in range(n):
            array[i, :] = i


# All access to the file goes through a single instance of this class.
# It contains several queues that are used to communicate with other
# processes.
# The read_queue is used for requests to read data from the HDF5 file.
# A list of result_queues is used to send data back to client processes.
# The write_queue is used for requests to modify the HDF5 file.
# One end of a pipe (shutdown) is used to signal the process to terminate.
class FileAccess(multiprocessing.Process):

    def __init__(self, h5_path, read_queue, result_queues, write_queue,
                 shutdown):
        self.h5_path = h5_path
        self.read_queue = read_queue
        self.result_queues = result_queues
        self.write_queue = write_queue
        self.shutdown = shutdown
        self.block_period = .01
        super(FileAccess, self).__init__()

    def run(self):
        self.h5_file = tables.open_file(self.h5_path, 'r+')
        self.array = self.h5_file.get_node('/array')
        another_loop = True
        while another_loop:

            # Check if the process has received the shutdown signal.
            if self.shutdown.poll():
                another_loop = False

            # Check for any data requests in the read_queue.
            try:
                row_num, proc_num = self.read_queue.get(
                    True, self.block_period)
                # look up the appropriate result_queue for this data processor
                # instance
                result_queue = self.result_queues[proc_num]
                print('processor {0} reading from row {1}'.format(proc_num,
                                                                  row_num))
                result_queue.put(self.read_data(row_num))
                another_loop = True
            except queue.Empty:
                pass

            # Check for any write requests in the write_queue.
            try:
                row_num, data = self.write_queue.get(True, self.block_period)
                print('writing row', row_num)
                self.write_data(row_num, data)
                another_loop = True
            except queue.Empty:
                pass

        # close the HDF5 file before shutting down
        self.h5_file.close()

    def read_data(self, row_num):
        return self.array[row_num, :]

    def write_data(self, row_num, data):
        self.array[row_num, :] = data


# This class represents a process that does work by reading and writing to the
# HDF5 file.  It does this by sending requests to the FileAccess class instance
# through its read and write queues.  The data results are sent back through
# the result_queue.
# Its actions are logged to a text file.
class DataProcessor(multiprocessing.Process):

    def __init__(self, read_queue, result_queue, write_queue, proc_num,
                 array_size, output_file):
        self.read_queue = read_queue
        self.result_queue = result_queue
        self.write_queue = write_queue
        self.proc_num = proc_num
        self.array_size = array_size
        self.output_file = output_file
        super(DataProcessor, self).__init__()

    def run(self):
        self.output_file = open(self.output_file, 'w')
        # read a random row from the file
        row_num = random.randint(0, self.array_size - 1)
        self.read_queue.put((row_num, self.proc_num))
        self.output_file.write(str(row_num) + '\n')
        self.output_file.write(str(self.result_queue.get()) + '\n')

        # modify a random row to equal 11 * (self.proc_num + 1)
        row_num = random.randint(0, self.array_size - 1)
        new_data = (numpy.zeros((1, self.array_size), 'i8') +
                    11 * (self.proc_num + 1))
        self.write_queue.put((row_num, new_data))

        # pause, then read the modified row
        time.sleep(0.015)
        self.read_queue.put((row_num, self.proc_num))
        self.output_file.write(str(row_num) + '\n')
        self.output_file.write(str(self.result_queue.get()) + '\n')
        self.output_file.close()


# this function starts the FileAccess class instance and
# sets up all the queues used to communicate with it
def make_queues(num_processors):
    read_queue = multiprocessing.Queue()
    write_queue = multiprocessing.Queue()
    shutdown_recv, shutdown_send = multiprocessing.Pipe(False)
    result_queues = [multiprocessing.Queue() for i in range(num_processors)]
    file_access = FileAccess(file_path, read_queue, result_queues, write_queue,
                             shutdown_recv)
    file_access.start()
    return read_queue, result_queues, write_queue, shutdown_send


if __name__ == '__main__':

    file_path = 'test.h5'
    n = 10
    make_file(file_path, n)

    num_processors = 3
    (read_queue, result_queues,
     write_queue, shutdown_send) = make_queues(num_processors)

    processors = []
    output_files = []
    for i in range(num_processors):
        result_queue = result_queues[i]
        output_file = str(i)
        processor = DataProcessor(read_queue, result_queue, write_queue, i, n,
                                  output_file)
        processors.append(processor)
        output_files.append(output_file)

    # start all DataProcessor instances
    for processor in processors:
        processor.start()

    # wait for all DataProcessor instances to finish
    for processor in processors:
        processor.join()

    # shut down the FileAccess instance
    shutdown_send.send(0)

    # print out contents of log files and delete them
    print()
    for output_file in output_files:
        print()
        print('contents of log file {0}'.format(output_file))
        print(open(output_file, 'r').read())
        os.remove(output_file)

    os.remove('test.h5')

########NEW FILE########
__FILENAME__ = nested-tut
"""Small example showing the use of nested types in PyTables.

The program creates an output file, 'nested-tut.h5'.  You can view it
with ptdump or any HDF5 generic utility.

:Author: F. Alted
:Date: 2005/06/10

"""

from __future__ import print_function
import numpy

import tables

#'-**-**-**-**- The sample nested class description  -**-**-**-**-**-'


class Info(tables.IsDescription):
    """A sub-structure of Test"""

    _v_pos = 2   # The position in the whole structure
    name = tables.StringCol(10)
    value = tables.Float64Col(pos=0)

colors = tables.Enum(['red', 'green', 'blue'])


class NestedDescr(tables.IsDescription):
    """A description that has several nested columns."""

    color = tables.EnumCol(colors, 'red', base='uint32')
    info1 = Info()

    class info2(tables.IsDescription):
        _v_pos = 1
        name = tables.StringCol(10)
        value = tables.Float64Col(pos=0)

        class info3(tables.IsDescription):
            x = tables.Float64Col(dflt=1)
            y = tables.UInt8Col(dflt=1)

print()
print('-**-**-**-**-**-**- file creation  -**-**-**-**-**-**-**-')

filename = "nested-tut.h5"

print("Creating file:", filename)
fileh = tables.open_file(filename, "w")

print()
print('-**-**-**-**-**- nested table creation  -**-**-**-**-**-')

table = fileh.create_table(fileh.root, 'table', NestedDescr)

# Fill the table with some rows
row = table.row
for i in range(10):
    row['color'] = colors[['red', 'green', 'blue'][i % 3]]
    row['info1/name'] = "name1-%s" % i
    row['info2/name'] = "name2-%s" % i
    row['info2/info3/y'] = i
    # All the rest will be filled with defaults
    row.append()

table.flush()  # flush the row buffer to disk
print(repr(table.nrows))

nra = table[::4]
print(repr(nra))
# Append some additional rows
table.append(nra)
print(repr(table.nrows))

# Create a new table
table2 = fileh.create_table(fileh.root, 'table2', nra)
print(repr(table2[:]))

# Read also the info2/name values with color == colors.red
names = [x['info2/name'] for x in table if x['color'] == colors.red]

print()
print("**** info2/name elements satisfying color == 'red':", repr(names))

print()
print('-**-**-**-**-**-**- table data reading & selection  -**-**-**-**-**-')

# Read the data
print()
print("**** table data contents:\n", table[:])

print()
print("**** table.info2 data contents:\n", repr(table.cols.info2[1:5]))

print()
print("**** table.info2.info3 data contents:\n",
      repr(table.cols.info2.info3[1:5]))

print("**** _f_col() ****")
print(repr(table.cols._f_col('info2')))
print(repr(table.cols._f_col('info2/info3/y')))

print()
print('-**-**-**-**-**-**- table metadata  -**-**-**-**-**-')

# Read description metadata
print()
print("**** table description (short):\n", repr(table.description))
print()
print("**** more from manual, period ***")
print(repr(table.description.info1))
print(repr(table.description.info2.info3))
print(repr(table.description._v_nested_names))
print(repr(table.description.info1._v_nested_names))
print()
print("**** now some for nested records, take that ****")
print(repr(table.description._v_nested_descr))
print(repr(numpy.rec.array(None, shape=0,
                           dtype=table.description._v_nested_descr)))
print(repr(numpy.rec.array(None, shape=0,
                           dtype=table.description.info2._v_nested_descr)))
print()
print("**** and some iteration over descriptions, too ****")
for coldescr in table.description._f_walk():
    print("column-->", coldescr)
print()
print("**** info2 sub-structure description:\n", table.description.info2)
print()
print("**** table representation (long form):\n", repr(table))

# Remember to always close the file
fileh.close()

########NEW FILE########
__FILENAME__ = nested1
# Example to show how nested types can be dealed with PyTables
# F. Alted 2005/05/27

from __future__ import print_function
import random
import tables

fileout = "nested1.h5"

# An example of enumerated structure
colors = tables.Enum(['red', 'green', 'blue'])


def read(file):
    fileh = tables.open_file(file, "r")

    print("table (short)-->", fileh.root.table)
    print("table (long)-->", repr(fileh.root.table))
    print("table (contents)-->", repr(fileh.root.table[:]))

    fileh.close()


def write(file, desc, indexed):
    fileh = tables.open_file(file, "w")
    table = fileh.create_table(fileh.root, 'table', desc)
    for colname in indexed:
        table.colinstances[colname].create_index()

    row = table.row
    for i in range(10):
        row['x'] = i
        row['y'] = 10.2 - i
        row['z'] = i
        row['color'] = colors[random.choice(['red', 'green', 'blue'])]
        row['info/name'] = "name%s" % i
        row['info/info2/info3/z4'] = i
        # All the rest will be filled with defaults
        row.append()

    fileh.close()

# The sample nested class description


class Info(tables.IsDescription):
    _v_pos = 2
    Name = tables.UInt32Col()
    Value = tables.Float64Col()


class Test(tables.IsDescription):
    """A description that has several columns."""

    x = tables.Int32Col(shape=2, dflt=0, pos=0)
    y = tables.Float64Col(dflt=1.2, shape=(2, 3))
    z = tables.UInt8Col(dflt=1)
    color = tables.EnumCol(colors, 'red', base='uint32', shape=(2,))
    Info = Info()

    class info(tables.IsDescription):
        _v_pos = 1
        name = tables.StringCol(10)
        value = tables.Float64Col(pos=0)
        y2 = tables.Float64Col(dflt=1, shape=(2, 3), pos=1)
        z2 = tables.UInt8Col(dflt=1)

        class info2(tables.IsDescription):
            y3 = tables.Float64Col(dflt=1, shape=(2, 3))
            z3 = tables.UInt8Col(dflt=1)
            name = tables.StringCol(10)
            value = tables.EnumCol(colors, 'blue', base='uint32', shape=(1,))

            class info3(tables.IsDescription):
                name = tables.StringCol(10)
                value = tables.Time64Col()
                y4 = tables.Float64Col(dflt=1, shape=(2, 3))
                z4 = tables.UInt8Col(dflt=1)

# Write the file and read it
write(fileout, Test, ['info/info2/z3'])
read(fileout)
print("You can have a look at '%s' output file now." % fileout)

########NEW FILE########
__FILENAME__ = objecttree
from __future__ import print_function
import tables


class Particle(tables.IsDescription):
    identity = tables.StringCol(itemsize=22, dflt=" ", pos=0)
                                                # character String
    idnumber = tables.Int16Col(dflt=1, pos=1)   # short integer
    speed = tables.Float32Col(dflt=1, pos=1)    # single-precision

# Open a file in "w"rite mode
fileh = tables.open_file("objecttree.h5", mode="w")
# Get the HDF5 root group
root = fileh.root

# Create the groups:
group1 = fileh.create_group(root, "group1")
group2 = fileh.create_group(root, "group2")

# Now, create an array in root group
array1 = fileh.create_array(
    root, "array1", ["string", "array"], "String array")
# Create 2 new tables in group1
table1 = fileh.create_table(group1, "table1", Particle)
table2 = fileh.create_table("/group2", "table2", Particle)
# Create the last table in group2
array2 = fileh.create_array("/group1", "array2", [1, 2, 3, 4])

# Now, fill the tables:
for table in (table1, table2):
    # Get the record object associated with the table:
    row = table.row
    # Fill the table with 10 records
    for i in range(10):
        # First, assign the values to the Particle record
        row['identity'] = 'This is particle: %2d' % (i)
        row['idnumber'] = i
        row['speed'] = i * 2.
        # This injects the Record values
        row.append()

    # Flush the table buffers
    table.flush()

# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()

########NEW FILE########
__FILENAME__ = particles
"""Beware! you need PyTables >= 2.3 to run this script!"""

from __future__ import print_function
from time import time  # use clock for Win
import numpy as np
import tables

# NEVENTS = 10000
NEVENTS = 20000
MAX_PARTICLES_PER_EVENT = 100

# Particle description


class Particle(tables.IsDescription):
    # event_id = tables.Int32Col(pos=1, indexed=True) # event id (indexed)
    event_id = tables.Int32Col(pos=1)               # event id (not indexed)
    particle_id = tables.Int32Col(pos=2)            # particle id in the event
    parent_id = tables.Int32Col(pos=3)              # the id of the parent
                                                    # particle (negative
                                                    # values means no parent)
    momentum = tables.Float64Col(shape=3, pos=4)    # momentum of the particle
    mass = tables.Float64Col(pos=5)                 # mass of the particle

# Create a new table for events
t1 = time()
print("Creating a table with %s entries aprox.. Wait please..." %
      (int(NEVENTS * (MAX_PARTICLES_PER_EVENT / 2.))))
fileh = tables.open_file("particles-pro.h5", mode="w")
group = fileh.create_group(fileh.root, "events")
table = fileh.create_table(group, 'table', Particle, "A table",
                           tables.Filters(0))
# Choose this line if you want data compression
# table = fileh.create_table(group, 'table', Particle, "A table", Filters(1))

# Fill the table with events
np.random.seed(1)  # In order to have reproducible results
particle = table.row
for i in range(NEVENTS):
    for j in range(np.random.randint(0, MAX_PARTICLES_PER_EVENT)):
        particle['event_id'] = i
        particle['particle_id'] = j
        particle['parent_id'] = j - 10     # 10 root particles (max)
        particle['momentum'] = np.random.normal(5.0, 2.0, size=3)
        particle['mass'] = np.random.normal(500.0, 10.0)
        # This injects the row values.
        particle.append()
table.flush()
print("Added %s entries --- Time: %s sec" %
      (table.nrows, round((time() - t1), 3)))

t1 = time()
print("Creating index...")
table.cols.event_id.create_index(optlevel=0, _verbose=True)
print("Index created --- Time: %s sec" % (round((time() - t1), 3)))
# Add the number of events as an attribute
table.attrs.nevents = NEVENTS

fileh.close()

# Open the file en read only mode and start selections
print("Selecting events...")
fileh = tables.open_file("particles-pro.h5", mode="r")
table = fileh.root.events.table

print("Particles in event 34:", end=' ')
nrows = 0
t1 = time()
for row in table.where("event_id == 34"):
        nrows += 1
print(nrows)
print("Done --- Time:", round((time() - t1), 3), "sec")

print("Root particles in event 34:", end=' ')
nrows = 0
t1 = time()
for row in table.where("event_id == 34"):
    if row['parent_id'] < 0:
        nrows += 1
print(nrows)
print("Done --- Time:", round((time() - t1), 3), "sec")

print("Sum of masses of root particles in event 34:", end=' ')
smass = 0.0
t1 = time()
for row in table.where("event_id == 34"):
    if row['parent_id'] < 0:
        smass += row['mass']
print(smass)
print("Done --- Time:", round((time() - t1), 3), "sec")

print(
    "Sum of masses of daughter particles for particle 3 in event 34:", end=' ')
smass = 0.0
t1 = time()
for row in table.where("event_id == 34"):
    if row['parent_id'] == 3:
        smass += row['mass']
print(smass)
print("Done --- Time:", round((time() - t1), 3), "sec")

print("Sum of module of momentum for particle 3 in event 34:", end=' ')
smomentum = 0.0
t1 = time()
# for row in table.where("(event_id == 34) & ((parent_id) == 3)"):
for row in table.where("event_id == 34"):
    if row['parent_id'] == 3:
        smomentum += np.sqrt(np.add.reduce(row['momentum'] ** 2))
print(smomentum)
print("Done --- Time:", round((time() - t1), 3), "sec")

# This is the same than above, but using generator expressions
# Python >= 2.4 needed here!
print("Sum of module of momentum for particle 3 in event 34 (2):", end=' ')
t1 = time()
print(sum(np.sqrt(np.add.reduce(row['momentum'] ** 2))
          for row in table.where("event_id == 34")
          if row['parent_id'] == 3))
print("Done --- Time:", round((time() - t1), 3), "sec")


fileh.close()

########NEW FILE########
__FILENAME__ = read_array_out_arg
# This script compares reading from an array in a loop using the
# tables.Array.read method.  In the first case, read is used without supplying
# an 'out' argument, which causes a new output buffer to be pre-allocated
# with each call.  In the second case, the buffer is created once, and then
# reused.


from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import time

import numpy as np
import tables


def create_file(array_size):
    array = np.ones(array_size, dtype='i8')
    with tables.open_file('test.h5', 'w') as fobj:
        array = fobj.create_array('/', 'test', array)
        print('file created, size: {0} MB'.format(array.size_on_disk / 1e6))


def standard_read(array_size):
    N = 10
    with tables.open_file('test.h5', 'r') as fobj:
        array = fobj.get_node('/', 'test')
        start = time.time()
        for i in range(N):
            output = array.read(0, array_size, 1)
        end = time.time()
        assert(np.all(output == 1))
        print('standard read   \t {0:5.5f}'.format((end - start) / N))


def pre_allocated_read(array_size):
    N = 10
    with tables.open_file('test.h5', 'r') as fobj:
        array = fobj.get_node('/', 'test')
        start = time.time()
        output = np.empty(array_size, 'i8')
        for i in range(N):
            array.read(0, array_size, 1, out=output)
        end = time.time()
        assert(np.all(output == 1))
        print('pre-allocated read\t {0:5.5f}'.format((end - start) / N))


if __name__ == '__main__':

    array_num_bytes = [int(x) for x in [1e5, 1e6, 1e7, 1e8]]

    for array_bytes in array_num_bytes:

        array_size = int(array_bytes // 8)

        create_file(array_size)
        standard_read(array_size)
        pre_allocated_read(array_size)
        print()

########NEW FILE########
__FILENAME__ = split
"""Use the H5FD_SPLIT driver to store metadata and raw data in separate files.

In this example, we store the metadata file in the current directory and
the raw data file in a subdirectory.

"""

import os
import errno
import numpy
import tables

FNAME = "split"
DRIVER = "H5FD_SPLIT"
RAW_DIR = "raw"
DRIVER_PROPS = {
    "driver_split_raw_ext": os.path.join(RAW_DIR, "%s-r.h5")
}
DATA_SHAPE = (2, 10)


class FooBar(tables.IsDescription):
    tag = tables.StringCol(16)
    data = tables.Float32Col(shape=DATA_SHAPE)

try:
    os.mkdir(RAW_DIR)
except OSError as e:
    if e.errno == errno.EEXIST:
        pass
with tables.open_file(FNAME, mode="w", driver=DRIVER, **DRIVER_PROPS) as f:
    group = f.create_group("/", "foo", "foo desc")
    table = f.create_table(group, "bar", FooBar, "bar desc")
    for i in range(5):
        table.row["tag"] = "t%d" % i
        table.row["data"] = numpy.random.random_sample(DATA_SHAPE)
        table.row.append()
    table.flush()

########NEW FILE########
__FILENAME__ = table-tree
from __future__ import print_function
import numpy as np
import tables


class Particle(tables.IsDescription):
    ADCcount = tables.Int16Col()                # signed short integer
    TDCcount = tables.UInt8Col()                # unsigned byte
    grid_i = tables.Int32Col()                  # integer
    grid_j = tables.Int32Col()                  # integer
    idnumber = tables.Int64Col()                # signed long long
    name = tables.StringCol(16, dflt="")        # 16-character String
    pressure = tables.Float32Col(shape=2)       # float  (single-precision)
    temperature = tables.Float64Col()           # double (double-precision)

Particle2 = {
    # You can also use any of the atom factories, i.e. the one which
    # accepts a PyTables type.
    "ADCcount": tables.Col.from_type("int16"),          # signed short integer
    "TDCcount": tables.Col.from_type("uint8"),          # unsigned byte
    "grid_i": tables.Col.from_type("int32"),            # integer
    "grid_j": tables.Col.from_type("int32"),            # integer
    "idnumber": tables.Col.from_type("int64"),          # signed long long
    "name": tables.Col.from_kind("string", 16),         # 16-character String
    "pressure": tables.Col.from_type("float32", (2,)),  # float
                                                        # (single-precision)
    "temperature": tables.Col.from_type("float64"),     # double
                                                        # (double-precision)
}

# The name of our HDF5 filename
filename = "table-tree.h5"

# Open a file in "w"rite mode
h5file = tables.open_file(filename, mode="w")

# Create a new group under "/" (root)
group = h5file.create_group("/", 'detector')

# Create one table on it
# table = h5file.create_table(group, 'table', Particle, "Title example")
# You can choose creating a Table from a description dictionary if you wish
table = h5file.create_table(group, 'table', Particle2, "Title example")

# Create a shortcut to the table record object
particle = table.row

# Fill the table with 10 particles
for i in range(10):
    # First, assign the values to the Particle record
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = [float(i * i), float(i * 2)]
    particle['temperature'] = float(i ** 2)
    particle['idnumber'] = i * (2 ** 34)  # This exceeds integer range
    # This injects the Record values.
    particle.append()

# Flush the buffers for table
table.flush()

# Get actual data from table. We are interested in column pressure.
pressure = [p['pressure'] for p in table.iterrows()]
print("Last record ==>", p)
print("Column pressure ==>", np.array(pressure))
print("Total records in table ==> ", len(pressure))
print()

# Create a new group to hold new arrays
gcolumns = h5file.create_group("/", "columns")
print("columns ==>", gcolumns, pressure)
# Create an array with this info under '/columns' having a 'list' flavor
h5file.create_array(gcolumns, 'pressure', pressure,
                    "Pressure column")
print("gcolumns.pressure type ==> ", gcolumns.pressure.atom.dtype)

# Do the same with TDCcount, but with a numpy object
TDC = [p['TDCcount'] for p in table.iterrows()]
print("TDC ==>", TDC)
print("TDC shape ==>", np.array(TDC).shape)
h5file.create_array('/columns', 'TDC', np.array(TDC), "TDCcount column")

# Do the same with name column
names = [p['name'] for p in table.iterrows()]
print("names ==>", names)
h5file.create_array('/columns', 'name', names, "Name column")
# This works even with homogeneous tuples or lists (!)
print("gcolumns.name shape ==>", gcolumns.name.shape)
print("gcolumns.name type ==> ", gcolumns.name.atom.dtype)

print("Table dump:")
for p in table.iterrows():
    print(p)

# Save a recarray object under detector
r = np.rec.array("a" * 300, formats='f4,3i4,a5,i2', shape=3)
recarrt = h5file.create_table("/detector", 'recarray', r, "RecArray example")
r2 = r[0:3:2]
# Change the byteorder property
recarrt = h5file.create_table("/detector", 'recarray2', r2,
                              "Non-contiguous recarray")
print(recarrt)
print()

print(h5file.root.detector.table.description)
# Close the file
h5file.close()

# sys.exit()

# Reopen it in append mode
h5file = tables.open_file(filename, "a")

# Ok. let's start browsing the tree from this filename
print("Reading info from filename:", h5file.filename)
print()

# Firstly, list all the groups on tree
print("Groups in file:")
for group in h5file.walk_groups("/"):
    print(group)
print()

# List all the nodes (Group and Leaf objects) on tree
print("List of all nodes in file:")
print(h5file)

# And finally, only the Arrays (Array objects)
print("Arrays in file:")
for array in h5file.walk_nodes("/", classname="Array"):
    print(array)
print()

# Get group /detector and print some info on it
detector = h5file.get_node("/detector")
print("detector object ==>", detector)

# List only leaves on detector
print("Leaves in group", detector, ":")
for leaf in h5file.list_nodes("/detector", 'Leaf'):
    print(leaf)
print()

# List only tables on detector
print("Tables in group", detector, ":")
for leaf in h5file.list_nodes("/detector", 'Table'):
    print(leaf)
print()

# List only arrays on detector (there should be none!)
print("Arrays in group", detector, ":")
for leaf in h5file.list_nodes("/detector", 'Array'):
    print(leaf)
print()

# Get "/detector" Group object
group = h5file.root.detector
print("/detector ==>", group)

# Get the "/detector/table
table = h5file.get_node("/detector/table")
print("/detector/table ==>", table)

# Get metadata from table
print("Object:", table)
print("Table name:", table.name)
print("Table title:", table.title)
print("Rows saved on table: %d" % (table.nrows))

print("Variable names on table with their type:")
for name in table.colnames:
    print("  ", name, ':=', table.coldtypes[name])
print()

# Read arrays in /columns/names and /columns/pressure

# Get the object in "/columns pressure"
pressureObject = h5file.get_node("/columns", "pressure")

# Get some metadata on this object
print("Info on the object:", pressureObject)
print("  shape ==>", pressureObject.shape)
print("  title ==>", pressureObject.title)
print("  type ==> ", pressureObject.atom.dtype)
print("  byteorder ==> ", pressureObject.byteorder)

# Read the pressure actual data
pressureArray = pressureObject.read()
print("  data type ==>", type(pressureArray))
print("  data ==>", pressureArray)
print()

# Get the object in "/columns/names"
nameObject = h5file.root.columns.name

# Get some metadata on this object
print("Info on the object:", nameObject)
print("  shape ==>", nameObject.shape)
print("  title ==>", nameObject.title)
print("  type ==> " % nameObject.atom.dtype)


# Read the 'name' actual data
nameArray = nameObject.read()
print("  data type ==>", type(nameArray))
print("  data ==>", nameArray)

# Print the data for both arrays
print("Data on arrays name and pressure:")
for i in range(pressureObject.shape[0]):
    print("".join(nameArray[i]), "-->", pressureArray[i])
print()


# Finally, append some new records to table
table = h5file.root.detector.table

# Append 5 new particles to table (yes, tables can be enlarged!)
particle = table.row
for i in range(10, 15):
    # First, assign the values to the Particle record
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = [float(i * i), float(i * 2)]
    particle['temperature'] = float(i ** 2)
    particle['idnumber'] = i * (2 ** 34)  # This exceeds integer range
    # This injects the Row values.
    particle.append()

# Flush this table
table.flush()

print("Columns name and pressure on expanded table:")
# Print some table columns, for comparison with array data
for p in table:
    print(p['name'], '-->', p['pressure'])
print()

# Put several flavors
oldflavor = table.flavor
print(table.read(field="ADCcount"))
table.flavor = "numpy"
print(table.read(field="ADCcount"))
table.flavor = oldflavor
print(table.read(0, 0, 1, "name"))
table.flavor = "python"
print(table.read(0, 0, 1, "name"))
table.flavor = oldflavor
print(table.read(0, 0, 2, "pressure"))
table.flavor = "python"
print(table.read(0, 0, 2, "pressure"))
table.flavor = oldflavor

# Several range selections
print("Extended slice in selection: [0:7:6]")
print(table.read(0, 7, 6))
print("Single record in selection: [1]")
print(table.read(1))
print("Last record in selection: [-1]")
print(table.read(-1))
print("Two records before the last in selection: [-3:-1]")
print(table.read(-3, -1))

# Print a recarray in table form
table = h5file.root.detector.recarray2
print("recarray2:", table)
print("  nrows:", table.nrows)
print("  byteorder:", table.byteorder)
print("  coldtypes:", table.coldtypes)
print("  colnames:", table.colnames)

print(table.read())
for p in table.iterrows():
    print(p['f1'], '-->', p['f2'])
print()

result = [rec['f1'] for rec in table if rec.nrow < 2]
print(result)

# Test the File.rename_node() method
# h5file.rename_node(h5file.root.detector.recarray2, "recarray3")
h5file.rename_node(table, "recarray3")
# Delete a Leaf from the HDF5 tree
h5file.remove_node(h5file.root.detector.recarray3)
# Delete the detector group and its leaves recursively
# h5file.remove_node(h5file.root.detector, recursive=1)
# Create a Group and then remove it
h5file.create_group(h5file.root, "newgroup")
h5file.remove_node(h5file.root, "newgroup")
h5file.rename_node(h5file.root.columns, "newcolumns")

print(h5file)

# Close this file
h5file.close()

########NEW FILE########
__FILENAME__ = table1
from __future__ import print_function
import tables


class Particle(tables.IsDescription):
    name = tables.StringCol(16, pos=1)      # 16-character String
    lati = tables.Int32Col(pos=2)           # integer
    longi = tables.Int32Col(pos=3)          # integer
    pressure = tables.Float32Col(pos=4)     # float  (single-precision)
    temperature = tables.Float64Col(pos=5)  # double (double-precision)

# Open a file in "w"rite mode
fileh = tables.open_file("table1.h5", mode="w")
# Create a new group
group = fileh.create_group(fileh.root, "newgroup")

# Create a new table in newgroup group
table = fileh.create_table(group, 'table', Particle, "A table",
                           tables.Filters(1))
particle = table.row

# Fill the table with 10 particles
for i in range(10):
    # First, assign the values to the Particle record
    particle['name'] = 'Particle: %6d' % (i)
    particle['lati'] = i
    particle['longi'] = 10 - i
    particle['pressure'] = float(i * i)
    particle['temperature'] = float(i ** 2)
    # This injects the row values.
    particle.append()

# We need to flush the buffers in table in order to get an
# accurate number of records on it.
table.flush()

# Add a couple of user attrs
table.attrs.user_attr1 = 1.023
table.attrs.user_attr2 = "This is the second user attr"

# Append several rows in only one call
table.append([("Particle:     10", 10, 0, 10 * 10, 10 ** 2),
              ("Particle:     11", 11, -1, 11 * 11, 11 ** 2),
              ("Particle:     12", 12, -2, 12 * 12, 12 ** 2)])

group = fileh.root.newgroup
print("Nodes under group", group, ":")
for node in fileh.list_nodes(group):
    print(node)
print()

print("Leaves everywhere in file", fileh.filename, ":")
for leaf in fileh.walk_nodes(classname="Leaf"):
    print(leaf)
print()

table = fileh.root.newgroup.table
print("Object:", table)
print("Table name: %s. Table title: %s" % (table.name, table.title))
print("Rows saved on table: %d" % (table.nrows))

print("Variable names on table with their type:")
for name in table.colnames:
    print("  ", name, ':=', table.coldtypes[name])

print("Table contents:")
for row in table:
    print(row[:])
print("Associated recarray:")
print(table.read())

# Finally, close the file
fileh.close()

########NEW FILE########
__FILENAME__ = table2
# This shows how to use the cols accessors for table columns
from __future__ import print_function
import tables


class Particle(tables.IsDescription):
    name = tables.StringCol(16, pos=1)          # 16-character String
    lati = tables.Int32Col(pos=2)               # integer
    longi = tables.Int32Col(pos=3)              # integer
    vector = tables.Int32Col(shape=(2,), pos=4)  # Integer
    matrix2D = tables.Float64Col(shape=(2, 2), pos=5)
                                                # double (double-precision)

# Open a file in "w"rite mode
fileh = tables.open_file("table2.h5", mode="w")
table = fileh.create_table(fileh.root, 'table', Particle, "A table")
# Append several rows in only one call
table.append(
    [("Particle:     10", 10, 0, (10 * 9, 1), [[10 ** 2, 11 * 3]] * 2),
     ("Particle:     11", 11, -1,
      (11 * 10, 2), [[11 ** 2, 10 * 3]] * 2),
     ("Particle:     12", 12, -2,
      (12 * 11, 3), [[12 ** 2, 9 * 3]] * 2),
     ("Particle:     13", 13, -3,
      (13 * 11, 4), [[13 ** 2, 8 * 3]] * 2),
     ("Particle:     14", 14, -4, (14 * 11, 5), [[14 ** 2, 7 * 3]] * 2)])

print("str(Cols)-->", table.cols)
print("repr(Cols)-->", repr(table.cols))
print("Column handlers:")
for name in table.colnames:
    print(table.cols._f_col(name))

print("Select table.cols.name[1]-->", table.cols.name[1])
print("Select table.cols.name[1:2]-->", table.cols.name[1:2])
print("Select table.cols.name[:]-->", table.cols.name[:])
print("Select table.cols._f_col('name')[:]-->", table.cols._f_col('name')[:])
print("Select table.cols.lati[1]-->", table.cols.lati[1])
print("Select table.cols.lati[1:2]-->", table.cols.lati[1:2])
print("Select table.cols.vector[:]-->", table.cols.vector[:])
print("Select table.cols['matrix2D'][:]-->", table.cols.matrix2D[:])

fileh.close()

########NEW FILE########
__FILENAME__ = table3
# This is an example on how to use complex columns
from __future__ import print_function
import tables


class Particle(tables.IsDescription):
    name = tables.StringCol(16, pos=1)   # 16-character String
    lati = tables.ComplexCol(itemsize=16, pos=2)
    longi = tables.ComplexCol(itemsize=8, pos=3)
    vector = tables.ComplexCol(itemsize=8, shape=(2,), pos=4)
    matrix2D = tables.ComplexCol(itemsize=16, shape=(2, 2), pos=5)

# Open a file in "w"rite mode
fileh = tables.open_file("table3.h5", mode="w")
table = fileh.create_table(fileh.root, 'table', Particle, "A table")
# Append several rows in only one call
table.append([
    ("Particle:     10", 10j, 0, (10 * 9 + 1j, 1), [[10 ** 2j, 11 * 3]] * 2),
    ("Particle:     11", 11j, -1, (11 * 10 + 2j, 2), [[11 ** 2j, 10 * 3]] * 2),
    ("Particle:     12", 12j, -2, (12 * 11 + 3j, 3), [[12 ** 2j, 9 * 3]] * 2),
    ("Particle:     13", 13j, -3, (13 * 11 + 4j, 4), [[13 ** 2j, 8 * 3]] * 2),
    ("Particle:     14", 14j, -4, (14 * 11 + 5j, 5), [[14 ** 2j, 7 * 3]] * 2)
])

print("str(Cols)-->", table.cols)
print("repr(Cols)-->", repr(table.cols))
print("Column handlers:")
for name in table.colnames:
    print(table.cols._f_col(name))

print("Select table.cols.name[1]-->", table.cols.name[1])
print("Select table.cols.name[1:2]-->", table.cols.name[1:2])
print("Select table.cols.name[:]-->", table.cols.name[:])
print("Select table.cols._f_col('name')[:]-->", table.cols._f_col('name')[:])
print("Select table.cols.lati[1]-->", table.cols.lati[1])
print("Select table.cols.lati[1:2]-->", table.cols.lati[1:2])
print("Select table.cols.vector[:]-->", table.cols.vector[:])
print("Select table.cols['matrix2D'][:]-->", table.cols.matrix2D[:])

fileh.close()

########NEW FILE########
__FILENAME__ = tutorial1-1
"""Small but quite comprehensive example showing the use of PyTables.

The program creates an output file, 'tutorial1.h5'.  You can view it
with any HDF5 generic utility.

"""

from __future__ import print_function
import numpy as np
import tables


        #'-**-**-**-**-**-**- user record definition  -**-**-**-**-**-**-**-'

# Define a user record to characterize some kind of particles
class Particle(tables.IsDescription):
    name = tables.StringCol(16)     # 16-character String
    idnumber = tables.Int64Col()    # Signed 64-bit integer
    ADCcount = tables.UInt16Col()   # Unsigned short integer
    TDCcount = tables.UInt8Col()    # unsigned byte
    grid_i = tables.Int32Col()      # integer
    grid_j = tables.Int32Col()      # integer
    pressure = tables.Float32Col()  # float  (single-precision)
    energy = tables.Float64Col()    # double (double-precision)

print()
print('-**-**-**-**-**-**- file creation  -**-**-**-**-**-**-**-')

# The name of our HDF5 filename
filename = "tutorial1.h5"

print("Creating file:", filename)

# Open a file in "w"rite mode
h5file = tables.open_file(filename, mode="w", title="Test file")

print()
print('-**-**-**-**-**- group and table creation  -**-**-**-**-**-**-**-')

# Create a new group under "/" (root)
group = h5file.create_group("/", 'detector', 'Detector information')
print("Group '/detector' created")

# Create one table on it
table = h5file.create_table(group, 'readout', Particle, "Readout example")
print("Table '/detector/readout' created")

# Print the file
print(h5file)
print()
print(repr(h5file))

# Get a shortcut to the record object in table
particle = table.row

# Fill the table with 10 particles
for i in range(10):
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i * i)
    particle['energy'] = float(particle['pressure'] ** 4)
    particle['idnumber'] = i * (2 ** 34)
    particle.append()

# Flush the buffers for table
table.flush()

print()
print('-**-**-**-**-**-**- table data reading & selection  -**-**-**-**-**-')

# Read actual data from table. We are interested in collecting pressure values
# on entries where TDCcount field is greater than 3 and pressure less than 50
pressure = [x['pressure'] for x in table.iterrows()
            if x['TDCcount'] > 3 and 20 <= x['pressure'] < 50]
print("Last record read:")
print(repr(x))
print("Field pressure elements satisfying the cuts:")
print(repr(pressure))

# Read also the names with the same cuts
names = [
    x['name'] for x in table.where(
        """(TDCcount > 3) & (20 <= pressure) & (pressure < 50)""")
]
print("Field names elements satisfying the cuts:")
print(repr(names))

print()
print('-**-**-**-**-**-**- array object creation  -**-**-**-**-**-**-**-')

print("Creating a new group called '/columns' to hold new arrays")
gcolumns = h5file.create_group(h5file.root, "columns", "Pressure and Name")

print("Creating an array called 'pressure' under '/columns' group")
h5file.create_array(gcolumns, 'pressure', np.array(pressure),
                    "Pressure column selection")
print(repr(h5file.root.columns.pressure))

print("Creating another array called 'name' under '/columns' group")
h5file.create_array(gcolumns, 'name', names, "Name column selection")
print(repr(h5file.root.columns.name))

print("HDF5 file:")
print(h5file)

# Close the file
h5file.close()
print("File '" + filename + "' created")

########NEW FILE########
__FILENAME__ = tutorial1-2
"""This example shows how to browse the object tree and enlarge tables.

Before to run this program you need to execute first tutorial1-1.py
that create the tutorial1.h5 file needed here.

"""

from __future__ import print_function
import tables

print()
print('-**-**-**-**- open the previous tutorial file -**-**-**-**-**-')

# Reopen the file in append mode
h5file = tables.open_file("tutorial1.h5", "a")

# Print the object tree created from this filename
print("Object tree from filename:", h5file.filename)
print(h5file)

print()
print('-**-**-**-**-**-**- traverse tree methods -**-**-**-**-**-**-**-')

# List all the nodes (Group and Leaf objects) on tree
print(h5file)

# List all the nodes (using File iterator) on tree
print("Nodes in file:")
for node in h5file:
    print(node)
print()

# Now, only list all the groups on tree
print("Groups in file:")
for group in h5file.walk_groups():
    print(group)
print()

# List only the arrays hanging from /
print("Arrays in file (I):")
for group in h5file.walk_groups("/"):
    for array in h5file.list_nodes(group, classname='Array'):
        print(array)

# This do the same result
print("Arrays in file (II):")
for array in h5file.walk_nodes("/", "Array"):
    print(array)
print()
# And finally, list only leafs on /detector group (there should be one!)
print("Leafs in group '/detector' (I):")
for leaf in h5file.list_nodes("/detector", 'Leaf'):
    print(leaf)

# Other way using iterators and natural naming
print("Leafs in group '/detector' (II):")
for leaf in h5file.root.detector._f_walknodes('Leaf'):
    print(leaf)


print()
print('-**-**-**-**-**-**- setting/getting object attributes -**-**--**-**-')

# Get a pointer to '/detector/readout' node
table = h5file.root.detector.readout
# Attach it a string (date) attribute
table.attrs.gath_date = "Wed, 06/12/2003 18:33"
# Attach a floating point attribute
table.attrs.temperature = 18.4
table.attrs.temp_scale = "Celsius"

# Get a pointer to '/detector' node
detector = h5file.root.detector
# Attach a general object to the parent (/detector) group
detector._v_attrs.stuff = [5, (2.3, 4.5), "Integer and tuple"]

# Now, get the attributes
print("gath_date attribute of /detector/readout:", table.attrs.gath_date)
print("temperature attribute of /detector/readout:", table.attrs.temperature)
print("temp_scale attribute of /detector/readout:", table.attrs.temp_scale)
print("stuff attribute in /detector:", detector._v_attrs.stuff)
print()

# Delete permanently the attribute gath_date of /detector/readout
print("Deleting /detector/readout gath_date attribute")
del table.attrs.gath_date

# Print a representation of all attributes in  /detector/table
print("AttributeSet instance in /detector/table:", repr(table.attrs))

# Get the (user) attributes of /detector/table
print("List of user attributes in /detector/table:", table.attrs._f_list())

# Get the (sys) attributes of /detector/table
print("List of user attributes in /detector/table:",
      table.attrs._f_list("sys"))
print()
# Rename an attribute
print("renaming 'temp_scale' attribute to 'tempScale'")
table.attrs._f_rename("temp_scale", "tempScale")
print(table.attrs._f_list())

# Try to rename a system attribute:
try:
    table.attrs._f_rename("VERSION", "version")
except:
    print("You can not rename a VERSION attribute: it is read only!.")

print()
print('-**-**-**-**-**-**- getting object metadata -**-**-**-**-**-**-')

# Get a pointer to '/detector/readout' data
table = h5file.root.detector.readout

# Get metadata from table
print("Object:", table)
print("Table name:", table.name)
print("Table title:", table.title)
print("Number of rows in table:", table.nrows)
print("Table variable names with their type and shape:")
for name in table.colnames:
    print(name, ':= %s, %s' % (table.coldtypes[name],
                               table.coldtypes[name].shape))
print()

# Get the object in "/columns pressure"
pressureObject = h5file.get_node("/columns", "pressure")

# Get some metadata on this object
print("Info on the object:", repr(pressureObject))
print("  shape: ==>", pressureObject.shape)
print("  title: ==>", pressureObject.title)
print("  atom: ==>", pressureObject.atom)
print()
print('-**-**-**-**-**- reading actual data from arrays -**-**-**-**-**-**-')

# Read the 'pressure' actual data
pressureArray = pressureObject.read()
print(repr(pressureArray))
# Check the kind of object we have created (it should be a numpy array)
print("pressureArray is an object of type:", type(pressureArray))

# Read the 'name' Array actual data
nameArray = h5file.root.columns.name.read()
# Check the kind of object we have created (it should be a numpy array)
print("nameArray is an object of type:", type(nameArray))

print()

# Print the data for both arrays
print("Data on arrays nameArray and pressureArray:")
for i in range(pressureObject.shape[0]):
    print(nameArray[i], "-->", pressureArray[i])

print()
print('-**-**-**-**-**- reading actual data from tables -**-**-**-**-**-**-')

# Create a shortcut to table object
table = h5file.root.detector.readout

# Read the 'energy' column of '/detector/readout'
print("Column 'energy' of '/detector/readout':\n", table.cols.energy)
print()
# Read the 3rd row of '/detector/readout'
print("Third row of '/detector/readout':\n", table[2])
print()
# Read the rows from 3 to 9 of row of '/detector/readout'
print("Rows from 3 to 9 of '/detector/readout':\n", table[2:9])

print()
print('-**-**-**-**- append records to existing table -**-**-**-**-**-')

# Get the object row from table
table = h5file.root.detector.readout
particle = table.row

# Append 5 new particles to table
for i in range(10, 15):
    particle['name'] = 'Particle: %6d' % (i)
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i * i)
    particle['energy'] = float(particle['pressure'] ** 4)
    particle['idnumber'] = i * (2 ** 34)  # This exceeds long integer range
    particle.append()

# Flush this table
table.flush()

# Print the data using the table iterator:
for r in table:
    print("%-16s | %11.1f | %11.4g | %6d | %6d | %8d |" %
          (r['name'], r['pressure'], r['energy'], r['grid_i'], r['grid_j'],
           r['TDCcount']))

print()
print("Total number of entries in resulting table:", table.nrows)

print()
print('-**-**-**-**- modify records of a table -**-**-**-**-**-')

# Single cells
print("First row of readout table.")
print("Before modif-->", table[0])
table.cols.TDCcount[0] = 1
print("After modifying first row of TDCcount-->", table[0])
table.cols.energy[0] = 2
print("After modifying first row of energy-->", table[0])

# Column slices
table.cols.TDCcount[2:5] = [2, 3, 4]
print("After modifying slice [2:5] of ADCcount-->", table[0:5])
table.cols.energy[1:9:3] = [2, 3, 4]
print("After modifying slice [1:9:3] of energy-->", table[0:9])

# Modifying complete Rows
table.modify_rows(start=1, step=3,
                  rows=[(1, 2, 3.0, 4, 5, 6, 'Particle:   None', 8.0),
                        (2, 4, 6.0, 8, 10, 12, 'Particle: None*2', 16.0)])
print("After modifying the complete third row-->", table[0:5])

# Modifying columns inside table iterators
for row in table.where('TDCcount <= 2'):
    row['energy'] = row['TDCcount'] * 2
    row.update()
print("After modifying energy column (where TDCcount <=2)-->", table[0:4])

print()
print('-**-**-**-**- modify elements of an array -**-**-**-**-**-')

print("pressure array")
pressureObject = h5file.root.columns.pressure
print("Before modif-->", pressureObject[:])
pressureObject[0] = 2
print("First modif-->", pressureObject[:])
pressureObject[1:3] = [2.1, 3.5]
print("Second modif-->", pressureObject[:])
pressureObject[::2] = [1, 2]
print("Third modif-->", pressureObject[:])

print("name array")
nameObject = h5file.root.columns.name
print("Before modif-->", nameObject[:])
nameObject[0] = 'Particle:   None'
print("First modif-->", nameObject[:])
nameObject[1:3] = ['Particle:      0', 'Particle:      1']
print("Second modif-->", nameObject[:])
nameObject[::2] = ['Particle:     -3', 'Particle:     -5']
print("Third modif-->", nameObject[:])

print()
print('-**-**-**-**- remove records from a table -**-**-**-**-**-')

# Delete some rows on the Table (yes, rows can be removed!)
table.remove_rows(5, 10)

# Print some table columns, for comparison with array data
print("Some columns in final table:")
print()
# Print the headers
print("%-16s | %11s | %11s | %6s | %6s | %8s |" %
     ('name', 'pressure', 'energy', 'grid_i', 'grid_j',
      'TDCcount'))

print("%-16s + %11s + %11s + %6s + %6s + %8s +" %
      ('-' * 16, '-' * 11, '-' * 11, '-' * 6, '-' * 6, '-' * 8))
# Print the data using the table iterator:
for r in table.iterrows():
    print("%-16s | %11.1f | %11.4g | %6d | %6d | %8d |" %
          (r['name'], r['pressure'], r['energy'], r['grid_i'], r['grid_j'],
           r['TDCcount']))

print()
print("Total number of entries in final table:", table.nrows)

# Close the file
h5file.close()

########NEW FILE########
__FILENAME__ = tutorial2
"""This program shows the different protections that PyTables offer to the user
in order to insure a correct data injection in tables.

Example to be used in the second tutorial in the User's Guide.

"""

from __future__ import print_function
import tables
import numpy as np

# Describe a particle record


class Particle(tables.IsDescription):
    name = tables.StringCol(itemsize=16)    # 16-character string
    lati = tables.Int32Col()                # integer
    longi = tables.Int32Col()               # integer
    pressure = tables.Float32Col(shape=(2, 3))      # array of floats
                                                    # (single-precision)
    temperature = tables.Float64Col(shape=(2, 3))   # array of doubles
                                                    # (double-precision)

# Native NumPy dtype instances are also accepted
Event = np.dtype([
    ("name", "S16"),
    ("TDCcount", np.uint8),
    ("ADCcount", np.uint16),
    ("xcoord", np.float32),
    ("ycoord", np.float32)
])

# And dictionaries too (this defines the same structure as above)
# Event = {
#     "name"     : StringCol(itemsize=16),
#     "TDCcount" : UInt8Col(),
#     "ADCcount" : UInt16Col(),
#     "xcoord"   : Float32Col(),
#     "ycoord"   : Float32Col(),
#     }

# Open a file in "w"rite mode
fileh = tables.open_file("tutorial2.h5", mode="w")
# Get the HDF5 root group
root = fileh.root
# Create the groups:
for groupname in ("Particles", "Events"):
    group = fileh.create_group(root, groupname)
# Now, create and fill the tables in Particles group
gparticles = root.Particles
# Create 3 new tables
for tablename in ("TParticle1", "TParticle2", "TParticle3"):
    # Create a table
    table = fileh.create_table("/Particles", tablename, Particle,
                               "Particles: " + tablename)
    # Get the record object associated with the table:
    particle = table.row
    # Fill the table with 257 particles
    for i in range(257):
        # First, assign the values to the Particle record
        particle['name'] = 'Particle: %6d' % (i)
        particle['lati'] = i
        particle['longi'] = 10 - i
        # Detectable errors start here. Play with them!
        particle['pressure'] = np.array(
            i * np.arange(2 * 3)).reshape((2, 4))  # Incorrect
        # particle['pressure'] = array(i*arange(2*3)).reshape((2,3))  # Correct
        # End of errors
        particle['temperature'] = (i ** 2)     # Broadcasting
        # This injects the Record values
        particle.append()
    # Flush the table buffers
    table.flush()

# Now, go for Events:
for tablename in ("TEvent1", "TEvent2", "TEvent3"):
    # Create a table in Events group
    table = fileh.create_table(root.Events, tablename, Event,
                               "Events: " + tablename)
    # Get the record object associated with the table:
    event = table.row
    # Fill the table with 257 events
    for i in range(257):
        # First, assign the values to the Event record
        event['name'] = 'Event: %6d' % (i)
        event['TDCcount'] = i % (1 << 8)   # Correct range
        # Detectable errors start here. Play with them!
        event['xcoor'] = float(i ** 2)     # Wrong spelling
        # event['xcoord'] = float(i**2)   # Correct spelling
        event['ADCcount'] = "sss"          # Wrong type
        # event['ADCcount'] = i * 2        # Correct type
        # End of errors
        event['ycoord'] = float(i) ** 4
        # This injects the Record values
        event.append()
    # Flush the buffers
    table.flush()

# Read the records from table "/Events/TEvent3" and select some
table = root.Events.TEvent3
e = [p['TDCcount'] for p in table
     if p['ADCcount'] < 20 and 4 <= p['TDCcount'] < 15]
print("Last record ==>", p)
print("Selected values ==>", e)
print("Total selected records ==> ", len(e))
# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()

########NEW FILE########
__FILENAME__ = tutorial3-1
"""Small example of do/undo capability with PyTables."""

import tables

# Create an HDF5 file
fileh = tables.open_file("tutorial3-1.h5", "w", title="Undo/Redo demo 1")

         #'-**-**-**-**-**-**- enable undo/redo log  -**-**-**-**-**-**-**-'
fileh.enable_undo()

# Create a new array
one = fileh.create_array('/', 'anarray', [3, 4], "An array")
# Mark this point
fileh.mark()
# Create a new array
another = fileh.create_array('/', 'anotherarray', [4, 5], "Another array")
# Now undo the past operation
fileh.undo()
# Check that anotherarray does not exist in the object tree but anarray does
assert "/anarray" in fileh
assert "/anotherarray" not in fileh
# Unwind once more
fileh.undo()
# Check that anarray does not exist in the object tree
assert "/anarray" not in fileh
assert "/anotherarray" not in fileh
# Go forward up to the next marker
fileh.redo()
# Check that anarray has come back to life in a sane state
assert "/anarray" in fileh
assert fileh.root.anarray.read() == [3, 4]
assert fileh.root.anarray.title == "An array"
assert fileh.root.anarray == one
# But anotherarray is not here yet
assert "/anotherarray" not in fileh
# Now, go rewind up to the end
fileh.redo()
assert "/anarray" in fileh
# Check that anotherarray has come back to life in a sane state
assert "/anotherarray" in fileh
assert fileh.root.anotherarray.read() == [4, 5]
assert fileh.root.anotherarray.title == "Another array"
assert fileh.root.anotherarray == another

         #'-**-**-**-**-**-**- disable undo/redo log  -**-**-**-**-**-**-**-'
fileh.disable_undo()

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = tutorial3-2
"""A more complex example of do/undo capability with PyTables.

Here, names has been assigned to the marks, and jumps are done between
marks.

"""

import tables

# Create an HDF5 file
fileh = tables.open_file('tutorial3-2.h5', 'w', title='Undo/Redo demo 2')

         #'-**-**-**-**-**-**- enable undo/redo log  -**-**-**-**-**-**-**-'
fileh.enable_undo()

# Start undoable operations
fileh.create_array('/', 'otherarray1', [3, 4], 'Another array 1')
fileh.create_group('/', 'agroup', 'Group 1')
# Create a 'first' mark
fileh.mark('first')
fileh.create_array('/agroup', 'otherarray2', [4, 5], 'Another array 2')
fileh.create_group('/agroup', 'agroup2', 'Group 2')
# Create a 'second' mark
fileh.mark('second')
fileh.create_array('/agroup/agroup2', 'otherarray3', [5, 6], 'Another array 3')
# Create a 'third' mark
fileh.mark('third')
fileh.create_array('/', 'otherarray4', [6, 7], 'Another array 4')
fileh.create_array('/agroup', 'otherarray5', [7, 8], 'Another array 5')

# Now go to mark 'first'
fileh.goto('first')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' not in fileh
assert '/agroup/otherarray2' not in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Go to mark 'third'
fileh.goto('third')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Now go to mark 'second'
fileh.goto('second')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Go to the end
fileh.goto(-1)
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' in fileh
assert '/agroup/otherarray5' in fileh
# Check that objects have come back to life in a sane state
assert fileh.root.otherarray1.read() == [3, 4]
assert fileh.root.agroup.otherarray2.read() == [4, 5]
assert fileh.root.agroup.agroup2.otherarray3.read() == [5, 6]
assert fileh.root.otherarray4.read() == [6, 7]
assert fileh.root.agroup.otherarray5.read() == [7, 8]


         #'-**-**-**-**-**-**- disable undo/redo log  -**-**-**-**-**-**-**-'
fileh.disable_undo()

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = undo-redo
"""Yet another couple of examples on do/undo feauture."""

import tables


def setUp(filename):
    # Create an HDF5 file
    fileh = tables.open_file(filename, mode="w", title="Undo/Redo demo")
    # Create some nodes in there
    fileh.create_group("/", "agroup", "Group 1")
    fileh.create_group("/agroup", "agroup2", "Group 2")
    fileh.create_array("/", "anarray", [1, 2], "Array 1")
    # Enable undo/redo.
    fileh.enable_undo()
    return fileh


def tearDown(fileh):
    # Disable undo/redo.
    fileh.disable_undo()
    # Close the file
    fileh.close()


def demo_6times3marks():
    """Checking with six ops and three marks."""

    # Initialize the data base with some nodes
    fileh = setUp("undo-redo-6times3marks.h5")

    # Create a new array
    fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
    fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
    # Put a mark
    fileh.mark()
    fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
    fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
    # Put a mark
    fileh.mark()
    fileh.create_array('/', 'otherarray5', [7, 8], "Another array 5")
    fileh.create_array('/', 'otherarray6', [8, 9], "Another array 6")
    # Unwind just one mark
    fileh.undo()
    assert "/otherarray1" in fileh
    assert "/otherarray2" in fileh
    assert "/otherarray3" in fileh
    assert "/otherarray4" in fileh
    assert "/otherarray5" not in fileh
    assert "/otherarray6" not in fileh
    # Unwind another mark
    fileh.undo()
    assert "/otherarray1" in fileh
    assert "/otherarray2" in fileh
    assert "/otherarray3" not in fileh
    assert "/otherarray4" not in fileh
    assert "/otherarray5" not in fileh
    assert "/otherarray6" not in fileh
    # Unwind all marks
    fileh.undo()
    assert "/otherarray1" not in fileh
    assert "/otherarray2" not in fileh
    assert "/otherarray3" not in fileh
    assert "/otherarray4" not in fileh
    assert "/otherarray5" not in fileh
    assert "/otherarray6" not in fileh
    # Redo until the next mark
    fileh.redo()
    assert "/otherarray1" in fileh
    assert "/otherarray2" in fileh
    assert "/otherarray3" not in fileh
    assert "/otherarray4" not in fileh
    assert "/otherarray5" not in fileh
    assert "/otherarray6" not in fileh
    # Redo until the next mark
    fileh.redo()
    assert "/otherarray1" in fileh
    assert "/otherarray2" in fileh
    assert "/otherarray3" in fileh
    assert "/otherarray4" in fileh
    assert "/otherarray5" not in fileh
    assert "/otherarray6" not in fileh
    # Redo until the end
    fileh.redo()
    assert "/otherarray1" in fileh
    assert "/otherarray2" in fileh
    assert "/otherarray3" in fileh
    assert "/otherarray4" in fileh
    assert "/otherarray5" in fileh
    assert "/otherarray6" in fileh

    # Tear down the file
    tearDown(fileh)


def demo_manyops():
    """Checking many operations together."""

    # Initialize the data base with some nodes
    fileh = setUp("undo-redo-manyops.h5")

    # Create an array
    fileh.create_array(fileh.root, 'anarray3', [3], "Array title 3")
    # Create a group
    fileh.create_group(fileh.root, 'agroup3', "Group title 3")
    # /anarray => /agroup/agroup3/
    new_node = fileh.copy_node('/anarray3', '/agroup/agroup2')
    new_node = fileh.copy_children('/agroup', '/agroup3', recursive=1)
    # rename anarray
    fileh.rename_node('/anarray', 'anarray4')
    # Move anarray
    new_node = fileh.copy_node('/anarray3', '/agroup')
    # Remove anarray4
    fileh.remove_node('/anarray4')
    # Undo the actions
    fileh.undo()
    assert '/anarray4' not in fileh
    assert '/anarray3' not in fileh
    assert '/agroup/agroup2/anarray3' not in fileh
    assert '/agroup3' not in fileh
    assert '/anarray4' not in fileh
    assert '/anarray' in fileh

    # Redo the actions
    fileh.redo()
    # Check that the copied node exists again in the object tree.
    assert '/agroup/agroup2/anarray3' in fileh
    assert '/agroup/anarray3' in fileh
    assert '/agroup3/agroup2/anarray3' in fileh
    assert '/agroup3/anarray3' not in fileh
    assert fileh.root.agroup.anarray3 is new_node
    assert '/anarray' not in fileh
    assert '/anarray4' not in fileh

    # Tear down the file
    tearDown(fileh)


if __name__ == '__main__':

    # run demos
    demo_6times3marks()
    demo_manyops()

########NEW FILE########
__FILENAME__ = vlarray1
from __future__ import print_function
import tables
import numpy as np

# Create a VLArray:
fileh = tables.open_file('vlarray1.h5', mode='w')
vlarray = fileh.create_vlarray(fileh.root, 'vlarray1',
                               tables.Int32Atom(shape=()),
                               "ragged array of ints",
                               filters=tables.Filters(1))
# Append some (variable length) rows:
vlarray.append(np.array([5, 6]))
vlarray.append(np.array([5, 6, 7]))
vlarray.append([5, 6, 9, 8])

# Now, read it through an iterator:
print('-->', vlarray.title)
for x in vlarray:
    print('%s[%d]--> %s' % (vlarray.name, vlarray.nrow, x))

# Now, do the same with native Python strings.
vlarray2 = fileh.create_vlarray(fileh.root, 'vlarray2',
                                tables.StringAtom(itemsize=2),
                                "ragged array of strings",
                                filters=tables.Filters(1))
vlarray2.flavor = 'python'
# Append some (variable length) rows:
print('-->', vlarray2.title)
vlarray2.append(['5', '66'])
vlarray2.append(['5', '6', '77'])
vlarray2.append(['5', '6', '9', '88'])

# Now, read it through an iterator:
for x in vlarray2:
    print('%s[%d]--> %s' % (vlarray2.name, vlarray2.nrow, x))

# Close the file.
fileh.close()

########NEW FILE########
__FILENAME__ = vlarray2
#!/usr/bin/env python

"""Small example that shows how to work with variable length arrays of
different types, UNICODE strings and general Python objects included."""

from __future__ import print_function
import numpy as np
import tables
import pickle

# Open a new empty HDF5 file
fileh = tables.open_file("vlarray2.h5", mode="w")
# Get the root group
root = fileh.root

# A test with VL length arrays:
vlarray = fileh.create_vlarray(root, 'vlarray1', tables.Int32Atom(),
                               "ragged array of ints")
vlarray.append(np.array([5, 6]))
vlarray.append(np.array([5, 6, 7]))
vlarray.append([5, 6, 9, 8])

# Test with lists of bidimensional vectors
vlarray = fileh.create_vlarray(root, 'vlarray2', tables.Int64Atom(shape=(2,)),
                               "Ragged array of vectors")
a = np.array([[1, 2], [1, 2]], dtype=np.int64)
vlarray.append(a)
vlarray.append(np.array([[1, 2], [3, 4]], dtype=np.int64))
vlarray.append(np.zeros(dtype=np.int64, shape=(0, 2)))
vlarray.append(np.array([[5, 6]], dtype=np.int64))
# This makes an error (shape)
# vlarray.append(array([[5], [6]], dtype=int64))
# This makes an error (type)
# vlarray.append(array([[5, 6]], dtype=uint64))

# Test with strings
vlarray = fileh.create_vlarray(root, 'vlarray3', tables.StringAtom(itemsize=3),
                               "Ragged array of strings")
vlarray.append(["123", "456", "3"])
vlarray.append(["456", "3"])
# This makes an error because of different string sizes than declared
# vlarray.append(["1234", "456", "3"])

# Python flavor
vlarray = fileh.create_vlarray(root, 'vlarray3b',
                               tables.StringAtom(itemsize=3),
                               "Ragged array of strings")
vlarray.flavor = "python"
vlarray.append(["123", "456", "3"])
vlarray.append(["456", "3"])

# Binary strings
vlarray = fileh.create_vlarray(root, 'vlarray4', tables.UInt8Atom(),
                               "pickled bytes")
data = pickle.dumps((["123", "456"], "3"))
vlarray.append(np.ndarray(buffer=data, dtype=np.uint8, shape=len(data)))

# The next is a way of doing the same than before
vlarray = fileh.create_vlarray(root, 'vlarray5', tables.ObjectAtom(),
                               "pickled object")
vlarray.append([["123", "456"], "3"])

# Boolean arrays are supported as well
vlarray = fileh.create_vlarray(root, 'vlarray6', tables.BoolAtom(),
                               "Boolean atoms")
# The next lines are equivalent...
vlarray.append([1, 0])
vlarray.append([1, 0, 3, 0])  # This will be converted to a boolean
# This gives a TypeError
# vlarray.append([1,0,1])

# Variable length strings
vlarray = fileh.create_vlarray(root, 'vlarray7', tables.VLStringAtom(),
                               "Variable Length String")
vlarray.append("asd")
vlarray.append("aaana")

# Unicode variable length strings
vlarray = fileh.create_vlarray(root, 'vlarray8', tables.VLUnicodeAtom(),
                               "Variable Length Unicode String")
vlarray.append("aaana")
vlarray.append("")   # The empty string
vlarray.append("asd")
vlarray.append("para\u0140lel")

# Close the file
fileh.close()

# Open the file for reading
fileh = tables.open_file("vlarray2.h5", mode="r")
# Get the root group
root = fileh.root

for object in fileh.list_nodes(root, "Leaf"):
    arr = object.read()
    print(object.name, "-->", arr)
    print("number of objects in this row:", len(arr))

# Close the file
fileh.close()

########NEW FILE########
__FILENAME__ = vlarray3
#!/usr/bin/env python

"""Example that shows how to easily save a variable number of atoms with a
VLArray."""

from __future__ import print_function
import numpy
import tables

N = 100
shape = (3, 3)

numpy.random.seed(10)  # For reproductible results
f = tables.open_file("vlarray3.h5", mode="w")
vlarray = f.create_vlarray(f.root, 'vlarray1',
                           tables.Float64Atom(shape=shape),
                           "ragged array of arrays")

k = 0
for i in range(N):
    l = []
    for j in range(numpy.random.randint(N)):
        l.append(numpy.random.randn(*shape))
        k += 1
    vlarray.append(l)

print("Total number of atoms:", k)
f.close()

########NEW FILE########
__FILENAME__ = vlarray4
#!/usr/bin/env python

"""Example that shows how to easily save a variable number of atoms with a
VLArray."""

from __future__ import print_function
import numpy
import tables

N = 100
shape = (3, 3)

numpy.random.seed(10)  # For reproductible results
f = tables.open_file("vlarray4.h5", mode="w")
vlarray = f.create_vlarray(f.root, 'vlarray1',
                           tables.Float64Atom(shape=shape),
                           "ragged array of arrays")

k = 0
for i in range(N):
    l = []
    for j in range(numpy.random.randint(N)):
        l.append(numpy.random.randn(*shape))
        k += 1
    vlarray.append(l)

print("Total number of atoms:", k)
f.close()

########NEW FILE########
__FILENAME__ = array
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: October 10, 2002
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Array class."""

import sys

import numpy

from tables import hdf5extension
from tables.filters import Filters
from tables.flavor import flavor_of, array_as_internal, internal_to_flavor

from tables.utils import (is_idx, convert_to_np_atom2, SizeType, lazyattr,
                          byteorders, quantize)
from tables.leaf import Leaf

from tables._past import previous_api, previous_api_property

# default version for ARRAY objects
# obversion = "1.0"    # initial version
# obversion = "2.0"    # Added an optional EXTDIM attribute
# obversion = "2.1"    # Added support for complex datatypes
# obversion = "2.2"    # This adds support for time datatypes.
# obversion = "2.3"    # This adds support for enumerated datatypes.
obversion = "2.4"    # Numeric and numarray flavors are gone.


class Array(hdf5extension.Array, Leaf):
    """This class represents homogeneous datasets in an HDF5 file.

    This class provides methods to write or read data to or from array objects
    in the file. This class does not allow you neither to enlarge nor compress
    the datasets on disk; use the EArray class (see :ref:`EArrayClassDescr`) if
    you want enlargeable dataset support or compression features, or CArray
    (see :ref:`CArrayClassDescr`) if you just want compression.

    An interesting property of the Array class is that it remembers the
    *flavor* of the object that has been saved so that if you saved, for
    example, a list, you will get a list during readings afterwards; if you
    saved a NumPy array, you will get a NumPy object, and so forth.

    Note that this class inherits all the public attributes and methods that
    Leaf (see :ref:`LeafClassDescr`) already provides. However, as Array
    instances have no internal I/O buffers, it is not necessary to use the
    flush() method they inherit from Leaf in order to save their internal state
    to disk.  When a writing method call returns, all the data is already on
    disk.

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*

    name : str
        The name of this node in its parent group.
    obj
        The array or scalar to be saved.  Accepted types are NumPy
        arrays and scalars as well as native Python sequences and
        scalars, provided that values are regular (i.e. they are not
        like ``[[1,2],2]``) and homogeneous (i.e. all the elements are
        of the same type).

        .. versionchanged:: 3.0
           Renamed form *object* into *obj*.
    title
        A description for this node (it sets the ``TITLE`` HDF5 attribute on
        disk).
    byteorder
        The byteorder of the data *on disk*, specified as 'little' or 'big'.
        If this is not specified, the byteorder is that of the given `object`.

    """

    # Class identifier.
    _c_classid = 'ARRAY'

    _c_classId = previous_api_property('_c_classid')
    _v_objectId = previous_api_property('_v_objectid')

    # Lazy read-only attributes
    # `````````````````````````
    @lazyattr
    def dtype(self):
        """The NumPy ``dtype`` that most closely matches this array."""

        return self.atom.dtype

    # Properties
    # ~~~~~~~~~~
    def _getnrows(self):
        if self.shape == ():
            return SizeType(1)  # scalar case
        else:
            return self.shape[self.maindim]
    nrows = property(
        _getnrows, None, None,
        "The number of rows in the array.")

    def _getrowsize(self):
        maindim = self.maindim
        rowsize = self.atom.size
        for i, dim in enumerate(self.shape):
            if i != maindim:
                rowsize *= dim
        return rowsize
    rowsize = property(
        _getrowsize, None, None,
        "The size of the rows in bytes in dimensions orthogonal to *maindim*.")

    size_in_memory = property(
        lambda self: self.nrows * self.rowsize, None, None,
        """The size of this array's data in bytes when it is fully loaded into
        memory.""")

    # Other methods
    # ~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 obj=None, title="",
                 byteorder=None, _log=True, _atom=None):

        self._v_version = None
        """The object version of this array."""
        self._v_new = new = obj is not None
        """Is this the first time the node has been created?"""
        self._v_new_title = title
        """New title for this node."""
        self._obj = obj
        """The object to be stored in the array.  It can be any of numpy,
        list, tuple, string, integer of floating point types, provided
        that they are regular (i.e. they are not like ``[[1, 2], 2]``).

        .. versionchanged:: 3.0
           Renamed form *_object* into *_obj*.

        """

        self._v_convert = True
        """Whether the ``Array`` object must be converted or not."""

        # Miscellaneous iteration rubbish.
        self._start = None
        """Starting row for the current iteration."""
        self._stop = None
        """Stopping row for the current iteration."""
        self._step = None
        """Step size for the current iteration."""
        self._nrowsread = None
        """Number of rows read up to the current state of iteration."""
        self._startb = None
        """Starting row for current buffer."""
        self._stopb = None
        """Stopping row for current buffer. """
        self._row = None
        """Current row in iterators (sentinel)."""
        self._init = False
        """Whether we are in the middle of an iteration or not (sentinel)."""
        self.listarr = None
        """Current buffer in iterators."""

        # Documented (*public*) attributes.
        self.atom = _atom
        """An Atom (see :ref:`AtomClassDescr`) instance representing the *type*
        and *shape* of the atomic objects to be saved.
        """
        self.shape = None
        """The shape of the stored array."""
        self.nrow = None
        """On iterators, this is the index of the current row."""
        self.extdim = -1   # ordinary arrays are not enlargeable
        """The index of the enlargeable dimension."""

        # Ordinary arrays have no filters: leaf is created with default ones.
        super(Array, self).__init__(parentnode, name, new, Filters(),
                                    byteorder, _log)

    def _g_create(self):
        """Save a new array in file."""

        self._v_version = obversion
        try:
            # `Leaf._g_post_init_hook()` should be setting the flavor on disk.
            self._flavor = flavor = flavor_of(self._obj)
            nparr = array_as_internal(self._obj, flavor)
        except:  # XXX
            # Problems converting data. Close the node and re-raise exception.
            self.close(flush=0)
            raise

        # Raise an error in case of unsupported object
        if nparr.dtype.kind in ['V', 'U', 'O']:  # in void, unicode, object
            raise TypeError("Array objects cannot currently deal with void, "
                            "unicode or object arrays")

        # Decrease the number of references to the object
        self._obj = None

        # Fix the byteorder of data
        nparr = self._g_fix_byteorder_data(nparr, nparr.dtype.byteorder)

        # Create the array on-disk
        try:
            # ``self._v_objectid`` needs to be set because would be
            # needed for setting attributes in some descendants later
            # on
            (self._v_objectid, self.shape, self.atom) = self._create_array(
                nparr, self._v_new_title, self.atom)
        except:  # XXX
            # Problems creating the Array on disk. Close node and re-raise.
            self.close(flush=0)
            raise

        # Compute the optimal buffer size
        self.nrowsinbuf = self._calc_nrowsinbuf()
        # Arrays don't have chunkshapes (so, set it to None)
        self._v_chunkshape = None

        return self._v_objectid

    def _g_open(self):
        """Get the metadata info for an array in file."""

        (oid, self.atom, self.shape, self._v_chunkshape) = self._open_array()

        self.nrowsinbuf = self._calc_nrowsinbuf()

        return oid

    def get_enum(self):
        """Get the enumerated type associated with this array.

        If this array is of an enumerated type, the corresponding Enum instance
        (see :ref:`EnumClassDescr`) is returned. If it is not of an enumerated
        type, a TypeError is raised.

        """

        if self.atom.kind != 'enum':
            raise TypeError("array ``%s`` is not of an enumerated type"
                            % self._v_pathname)

        return self.atom.enum

    getEnum = previous_api(get_enum)

    def iterrows(self, start=None, stop=None, step=None):
        """Iterate over the rows of the array.

        This method returns an iterator yielding an object of the current
        flavor for each selected row in the array.  The returned rows are taken
        from the *main dimension*.

        If a range is not supplied, *all the rows* in the array are iterated
        upon - you can also use the :meth:`Array.__iter__` special method for
        that purpose.  If you only want to iterate over a given *range of rows*
        in the array, you may use the start, stop and step parameters.

        Examples
        --------

        ::

            result = [row for row in arrayInstance.iterrows(step=4)]

        .. versionchanged:: 3.0
           If the *start* parameter is provided and *stop* is None then the
           array is iterated from *start* to the last line.
           In PyTables < 3.0 only one element was returned.

        """

        try:
            (self._start, self._stop, self._step) = self._process_range(
                start, stop, step)
        except IndexError:
            # If problems with indexes, silently return the null tuple
            return ()
        self._init_loop()
        return self

    def __iter__(self):
        """Iterate over the rows of the array.

        This is equivalent to calling :meth:`Array.iterrows` with default
        arguments, i.e. it iterates over *all the rows* in the array.

        Examples
        --------

        ::

            result = [row[2] for row in array]

        Which is equivalent to::

            result = [row[2] for row in array.iterrows()]

        """

        if not self._init:
            # If the iterator is called directly, assign default variables
            self._start = 0
            self._stop = self.nrows
            self._step = 1
            # and initialize the loop
            self._init_loop()
        return self

    def _init_loop(self):
        """Initialization for the __iter__ iterator."""

        self._nrowsread = self._start
        self._startb = self._start
        self._row = -1   # Sentinel
        self._init = True  # Sentinel
        self.nrow = SizeType(self._start - self._step)    # row number

    _initLoop = previous_api(_init_loop)

    def next(self):
        """Get the next element of the array during an iteration.

        The element is returned as an object of the current flavor.

        """

        # this could probably be sped up for long iterations by reusing the
        # listarr buffer
        if self._nrowsread >= self._stop:
            self._init = False
            self.listarr = None        # fixes issue #308
            raise StopIteration        # end of iteration
        else:
            # Read a chunk of rows
            if self._row + 1 >= self.nrowsinbuf or self._row < 0:
                self._stopb = self._startb + self._step * self.nrowsinbuf
                # Protection for reading more elements than needed
                if self._stopb > self._stop:
                    self._stopb = self._stop
                listarr = self._read(self._startb, self._stopb, self._step)
                # Swap the axes to easy the return of elements
                if self.extdim > 0:
                    listarr = listarr.swapaxes(self.extdim, 0)
                self.listarr = internal_to_flavor(listarr, self.flavor)
                self._row = -1
                self._startb = self._stopb
            self._row += 1
            self.nrow += self._step
            self._nrowsread += self._step
            # Fixes bug #968132
            # if self.listarr.shape:
            if self.shape:
                return self.listarr[self._row]
            else:
                return self.listarr    # Scalar case

    def _interpret_indexing(self, keys):
        """Internal routine used by __getitem__ and __setitem__"""

        maxlen = len(self.shape)
        shape = (maxlen,)
        startl = numpy.empty(shape=shape, dtype=SizeType)
        stopl = numpy.empty(shape=shape, dtype=SizeType)
        stepl = numpy.empty(shape=shape, dtype=SizeType)
        stop_None = numpy.zeros(shape=shape, dtype=SizeType)
        if not isinstance(keys, tuple):
            keys = (keys,)
        nkeys = len(keys)
        dim = 0
        # Here is some problem when dealing with [...,...] params
        # but this is a bit weird way to pass parameters anyway
        for key in keys:
            ellipsis = 0  # Sentinel
            if isinstance(key, type(Ellipsis)):
                ellipsis = 1
                for diml in xrange(dim, len(self.shape) - (nkeys - dim) + 1):
                    startl[dim] = 0
                    stopl[dim] = self.shape[diml]
                    stepl[dim] = 1
                    dim += 1
            elif dim >= maxlen:
                raise IndexError("Too many indices for object '%s'" %
                                 self._v_pathname)
            elif is_idx(key):
                # Protection for index out of range
                if key >= self.shape[dim]:
                    raise IndexError("Index out of range")
                if key < 0:
                    # To support negative values (Fixes bug #968149)
                    key += self.shape[dim]
                start, stop, step = self._process_range(
                    key, key + 1, 1, dim=dim)
                stop_None[dim] = 1
            elif isinstance(key, slice):
                start, stop, step = self._process_range(
                    key.start, key.stop, key.step, dim=dim)
            else:
                raise TypeError("Non-valid index or slice: %s" % key)
            if not ellipsis:
                startl[dim] = start
                stopl[dim] = stop
                stepl[dim] = step
                dim += 1

        # Complete the other dimensions, if needed
        if dim < len(self.shape):
            for diml in xrange(dim, len(self.shape)):
                startl[dim] = 0
                stopl[dim] = self.shape[diml]
                stepl[dim] = 1
                dim += 1

        # Compute the shape for the container properly. Fixes #1288792
        shape = []
        for dim in xrange(len(self.shape)):
            # The negative division operates differently with python scalars
            # and numpy scalars (which are similar to C conventions). See:
            # http://www.python.org/doc/faq/programming.html#why-does-22-10-return-3
            # and
            # http://www.peterbe.com/Integer-division-in-programming-languages
            # for more info on this issue.
            # I've finally decided to rely on the len(xrange) function.
            # F. Alted 2006-09-25
            # Switch to `lrange` to allow long ranges (see #99).
            # use xrange, since it supports large integers as of Python 2.6
            # see github #181
            new_dim = len(xrange(startl[dim], stopl[dim], stepl[dim]))
            if not (new_dim == 1 and stop_None[dim]):
                shape.append(new_dim)

        return startl, stopl, stepl, shape

    def _fancy_selection(self, args):
        """Performs a NumPy-style fancy selection in `self`.

        Implements advanced NumPy-style selection operations in
        addition to the standard slice-and-int behavior.

        Indexing arguments may be ints, slices or lists of indices.

        Note: This is a backport from the h5py project.

        """

        # Internal functions

        def validate_number(num, length):
            """Validate a list member for the given axis length."""

            try:
                num = long(num)
            except TypeError:
                raise TypeError("Illegal index: %r" % num)
            if num > length - 1:
                raise IndexError("Index out of bounds: %d" % num)

        def expand_ellipsis(args, rank):
            """Expand ellipsis objects and fill in missing axes."""

            n_el = sum(1 for arg in args if arg is Ellipsis)
            if n_el > 1:
                raise IndexError("Only one ellipsis may be used.")
            elif n_el == 0 and len(args) != rank:
                args = args + (Ellipsis,)

            final_args = []
            n_args = len(args)
            for idx, arg in enumerate(args):
                if arg is Ellipsis:
                    final_args.extend((slice(None),) * (rank - n_args + 1))
                else:
                    final_args.append(arg)

            if len(final_args) > rank:
                raise IndexError("Too many indices.")

            return final_args

        def translate_slice(exp, length):
            """Given a slice object, return a 3-tuple (start, count, step)

            This is for for use with the hyperslab selection routines.

            """

            start, stop, step = exp.start, exp.stop, exp.step
            if start is None:
                start = 0
            else:
                start = long(start)
            if stop is None:
                stop = length
            else:
                stop = long(stop)
            if step is None:
                step = 1
            else:
                step = long(step)

            if step < 1:
                raise IndexError("Step must be >= 1 (got %d)" % step)
            if stop == start:
                raise IndexError("Zero-length selections are not allowed")
            if stop < start:
                raise IndexError("Reverse-order selections are not allowed")
            if start < 0:
                start = length + start
            if stop < 0:
                stop = length + stop

            if not 0 <= start <= (length - 1):
                raise IndexError(
                    "Start index %s out of range (0-%d)" % (start, length - 1))
            if not 1 <= stop <= length:
                raise IndexError(
                    "Stop index %s out of range (1-%d)" % (stop, length))

            count = (stop - start) // step
            if (stop - start) % step != 0:
                count += 1

            if start + count > length:
                raise IndexError(
                    "Selection out of bounds (%d; axis has %d)" %
                    (start + count, length))

            return start, count, step

        # Main code for _fancy_selection
        mshape = []
        selection = []

        if not isinstance(args, tuple):
            args = (args,)

        args = expand_ellipsis(args, len(self.shape))

        list_seen = False
        reorder = None
        for idx, (exp, length) in enumerate(zip(args, self.shape)):
            if isinstance(exp, slice):
                start, count, step = translate_slice(exp, length)
                selection.append((start, count, step, idx, "AND"))
                mshape.append(count)
            else:
                try:
                    exp = list(exp)
                except TypeError:
                    exp = [exp]  # Handle scalar index as a list of length 1
                    mshape.append(0)  # Keep track of scalar index for NumPy
                else:
                    mshape.append(len(exp))
                if len(exp) == 0:
                    raise IndexError(
                        "Empty selections are not allowed (axis %d)" % idx)
                elif len(exp) > 1:
                    if list_seen:
                        raise IndexError(
                            "Only one selection list is allowed")
                    else:
                        list_seen = True
                nexp = numpy.asarray(exp, dtype="i8")
                # Convert negative values
                nexp = numpy.where(nexp < 0, length + nexp, nexp)
                # Check whether the list is ordered or not
                # (only one unordered list is allowed)
                if not len(nexp) == len(numpy.unique(nexp)):
                    raise IndexError(
                        "Selection lists cannot have repeated values")
                neworder = nexp.argsort()
                if not numpy.alltrue(neworder == numpy.arange(len(exp))):
                    if reorder is not None:
                        raise IndexError(
                            "Only one selection list can be unordered")
                    corrected_idx = sum(1 for x in mshape if x != 0) - 1
                    reorder = (corrected_idx, neworder)
                    nexp = nexp[neworder]
                for select_idx in xrange(len(nexp) + 1):
                    # This crazy piece of code performs a list selection
                    # using HDF5 hyperslabs.
                    # For each index, perform a "NOTB" selection on every
                    # portion of *this axis* which falls *outside* the list
                    # selection.  For this to work, the input array MUST be
                    # monotonically increasing.
                    if select_idx < len(nexp):
                        validate_number(nexp[select_idx], length)
                    if select_idx == 0:
                        start = 0
                        count = nexp[0]
                    elif select_idx == len(nexp):
                        start = nexp[-1] + 1
                        count = length - start
                    else:
                        start = nexp[select_idx - 1] + 1
                        count = nexp[select_idx] - start
                    if count > 0:
                        selection.append((start, count, 1, idx, "NOTB"))

        mshape = tuple(x for x in mshape if x != 0)
        return selection, reorder, mshape

    _fancySelection = previous_api(_fancy_selection)

    def __getitem__(self, key):
        """Get a row, a range of rows or a slice from the array.

        The set of tokens allowed for the key is the same as that for extended
        slicing in Python (including the Ellipsis or ... token).  The result is
        an object of the current flavor; its shape depends on the kind of slice
        used as key and the shape of the array itself.

        Furthermore, NumPy-style fancy indexing, where a list of indices in a
        certain axis is specified, is also supported.  Note that only one list
        per selection is supported right now.  Finally, NumPy-style point and
        boolean selections are supported as well.

        Examples
        --------

        ::

            array1 = array[4]                       # simple selection
            array2 = array[4:1000:2]                # slice selection
            array3 = array[1, ..., ::2, 1:4, 4:]    # general slice selection
            array4 = array[1, [1,5,10], ..., -1]    # fancy selection
            array5 = array[np.where(array[:] > 4)]  # point selection
            array6 = array[array[:] > 4]            # boolean selection

        """

        self._g_check_open()

        try:
            # First, try with a regular selection
            startl, stopl, stepl, shape = self._interpret_indexing(key)
            arr = self._read_slice(startl, stopl, stepl, shape)
        except TypeError:
            # Then, try with a point-wise selection
            try:
                coords = self._point_selection(key)
                arr = self._read_coords(coords)
            except TypeError:
                # Finally, try with a fancy selection
                selection, reorder, shape = self._fancy_selection(key)
                arr = self._read_selection(selection, reorder, shape)

        if self.flavor == "numpy" or not self._v_convert:
            return arr

        return internal_to_flavor(arr, self.flavor)

    def __setitem__(self, key, value):
        """Set a row, a range of rows or a slice in the array.

        It takes different actions depending on the type of the key parameter:
        if it is an integer, the corresponding array row is set to value (the
        value is broadcast when needed).  If key is a slice, the row slice
        determined by it is set to value (as usual, if the slice to be updated
        exceeds the actual shape of the array, only the values in the existing
        range are updated).

        If value is a multidimensional object, then its shape must be
        compatible with the shape determined by key, otherwise, a ValueError
        will be raised.

        Furthermore, NumPy-style fancy indexing, where a list of indices in a
        certain axis is specified, is also supported.  Note that only one list
        per selection is supported right now.  Finally, NumPy-style point and
        boolean selections are supported as well.

        Examples
        --------

        ::

            a1[0] = 333        # assign an integer to a Integer Array row
            a2[0] = 'b'        # assign a string to a string Array row
            a3[1:4] = 5        # broadcast 5 to slice 1:4
            a4[1:4:2] = 'xXx'  # broadcast 'xXx' to slice 1:4:2

            # General slice update (a5.shape = (4,3,2,8,5,10).
            a5[1, ..., ::2, 1:4, 4:] = numpy.arange(1728, shape=(4,3,2,4,3,6))
            a6[1, [1,5,10], ..., -1] = arr    # fancy selection
            a7[np.where(a6[:] > 4)] = 4       # point selection + broadcast
            a8[arr > 4] = arr2                # boolean selection

        """

        self._g_check_open()

        # Create an array compliant with the specified slice
        nparr = convert_to_np_atom2(value, self.atom)
        if nparr.size == 0:
            return

        # truncate data if least_significant_digit filter is set
        # TODO: add the least_significant_digit attribute to the array on disk
        if (self.filters.least_significant_digit is not None and
                not numpy.issubdtype(nparr.dtype, int)):
            nparr = quantize(nparr, self.filters.least_significant_digit)

        try:
            startl, stopl, stepl, shape = self._interpret_indexing(key)
            self._write_slice(startl, stopl, stepl, shape, nparr)
        except TypeError:
            # Then, try with a point-wise selection
            try:
                coords = self._point_selection(key)
                self._write_coords(coords, nparr)
            except TypeError:
                selection, reorder, shape = self._fancy_selection(key)
                self._write_selection(selection, reorder, shape, nparr)

    def _check_shape(self, nparr, slice_shape):
        """Test that nparr shape is consistent with underlying object.

        If not, try creating a new nparr object, using broadcasting if
        necessary.

        """

        if nparr.shape != (slice_shape + self.atom.dtype.shape):
            # Create an array compliant with the specified shape
            narr = numpy.empty(shape=slice_shape, dtype=self.atom.dtype)

            # Assign the value to it. It will raise a ValueError exception
            # if the objects cannot be broadcast to a single shape.
            narr[...] = nparr
            return narr
        else:
            return nparr

    _checkShape = previous_api(_check_shape)

    def _read_slice(self, startl, stopl, stepl, shape):
        """Read a slice based on `startl`, `stopl` and `stepl`."""

        nparr = numpy.empty(dtype=self.atom.dtype, shape=shape)
        # Protection against reading empty arrays
        if 0 not in shape:
            # Arrays that have non-zero dimensionality
            self._g_read_slice(startl, stopl, stepl, nparr)
        # For zero-shaped arrays, return the scalar
        if nparr.shape == ():
            nparr = nparr[()]
        return nparr

    _readSlice = previous_api(_read_slice)

    def _read_coords(self, coords):
        """Read a set of points defined by `coords`."""

        nparr = numpy.empty(dtype=self.atom.dtype, shape=len(coords))
        if len(coords) > 0:
            self._g_read_coords(coords, nparr)
        # For zero-shaped arrays, return the scalar
        if nparr.shape == ():
            nparr = nparr[()]
        return nparr

    _readCoords = previous_api(_read_coords)

    def _read_selection(self, selection, reorder, shape):
        """Read a `selection`.

        Reorder if necessary.

        """

        # Create the container for the slice
        nparr = numpy.empty(dtype=self.atom.dtype, shape=shape)
        # Arrays that have non-zero dimensionality
        self._g_read_selection(selection, nparr)
        # For zero-shaped arrays, return the scalar
        if nparr.shape == ():
            nparr = nparr[()]
        elif reorder is not None:
            # We need to reorder the array
            idx, neworder = reorder
            k = [slice(None)] * len(shape)
            k[idx] = neworder.argsort()
            # Apparently, a copy is not needed here, but doing it
            # for symmetry with the `_write_selection()` method.
            nparr = nparr[k].copy()
        return nparr

    _readSelection = previous_api(_read_selection)

    def _write_slice(self, startl, stopl, stepl, shape, nparr):
        """Write `nparr` in a slice based on `startl`, `stopl` and `stepl`."""

        nparr = self._check_shape(nparr, tuple(shape))
        countl = ((stopl - startl - 1) // stepl) + 1
        self._g_write_slice(startl, stepl, countl, nparr)

    _writeSlice = previous_api(_write_slice)

    def _write_coords(self, coords, nparr):
        """Write `nparr` values in points defined by `coords` coordinates."""

        if len(coords) > 0:
            nparr = self._check_shape(nparr, (len(coords),))
            self._g_write_coords(coords, nparr)

    _writeCoords = previous_api(_write_coords)

    def _write_selection(self, selection, reorder, shape, nparr):
        """Write `nparr` in `selection`.

        Reorder if necessary.

        """

        nparr = self._check_shape(nparr, tuple(shape))
        # Check whether we should reorder the array
        if reorder is not None:
            idx, neworder = reorder
            k = [slice(None)] * len(shape)
            k[idx] = neworder
            # For a reason a don't understand well, we need a copy of
            # the reordered array
            nparr = nparr[k].copy()
        self._g_write_selection(selection, nparr)

    _writeSelection = previous_api(_write_selection)

    def _read(self, start, stop, step, out=None):
        """Read the array from disk without slice or flavor processing."""

        nrowstoread = len(xrange(start, stop, step))
        shape = list(self.shape)
        if shape:
            shape[self.maindim] = nrowstoread
        if out is None:
            arr = numpy.empty(dtype=self.atom.dtype, shape=shape)
        else:
            bytes_required = self.rowsize * nrowstoread
            # if buffer is too small, it will segfault
            if bytes_required != out.nbytes:
                raise ValueError(('output array size invalid, got {0} bytes, '
                                  'need {1} bytes').format(out.nbytes,
                                                           bytes_required))
            if not out.flags['C_CONTIGUOUS']:
                raise ValueError('output array not C contiguous')
            arr = out
        # Protection against reading empty arrays
        if 0 not in shape:
            # Arrays that have non-zero dimensionality
            self._read_array(start, stop, step, arr)
        # data is always read in the system byteorder
        # if the out array's byteorder is different, do a byteswap
        if (out is not None and
                byteorders[arr.dtype.byteorder] != sys.byteorder):
            arr.byteswap(True)
        return arr

    def read(self, start=None, stop=None, step=None, out=None):
        """Get data in the array as an object of the current flavor.

        The start, stop and step parameters can be used to select only a
        *range of rows* in the array.  Their meanings are the same as in
        the built-in range() Python function, except that negative values
        of step are not allowed yet. Moreover, if only start is specified,
        then stop will be set to start + 1. If you do not specify neither
        start nor stop, then *all the rows* in the array are selected.

        The out parameter may be used to specify a NumPy array to receive
        the output data.  Note that the array must have the same size as
        the data selected with the other parameters.  Note that the array's
        datatype is not checked and no type casting is performed, so if it
        does not match the datatype on disk, the output will not be correct.
        Also, this parameter is only valid when the array's flavor is set
        to 'numpy'.  Otherwise, a TypeError will be raised.

        When data is read from disk in NumPy format, the output will be
        in the current system's byteorder, regardless of how it is stored
        on disk.
        The exception is when an output buffer is supplied, in which case
        the output will be in the byteorder of that output buffer.

        .. versionchanged:: 3.0
           Added the *out* parameter.

        """

        self._g_check_open()
        if out is not None and self.flavor != 'numpy':
            msg = ("Optional 'out' argument may only be supplied if array "
                   "flavor is 'numpy', currently is {0}").format(self.flavor)
            raise TypeError(msg)
        (start, stop, step) = self._process_range_read(start, stop, step)
        arr = self._read(start, stop, step, out)
        return internal_to_flavor(arr, self.flavor)

    def _g_copy_with_stats(self, group, name, start, stop, step,
                           title, filters, chunkshape, _log, **kwargs):
        """Private part of Leaf.copy() for each kind of leaf."""

        # Compute the correct indices.
        (start, stop, step) = self._process_range_read(start, stop, step)
        # Get the slice of the array
        # (non-buffered version)
        if self.shape:
            arr = self[start:stop:step]
        else:
            arr = self[()]
        # Build the new Array object.  Use the _atom reserved keyword
        # just in case the array is being copied from a native HDF5
        # with atomic types different from scalars.
        # For details, see #275 of trac.
        object_ = Array(group, name, arr, title=title, _log=_log,
                        _atom=self.atom)
        nbytes = numpy.prod(self.shape, dtype=SizeType) * self.atom.size

        return (object_, nbytes)

    _g_copyWithStats = previous_api(_g_copy_with_stats)

    def __repr__(self):
        """This provides more metainfo in addition to standard __str__"""

        return """%s
  atom := %r
  maindim := %r
  flavor := %r
  byteorder := %r
  chunkshape := %r""" % (self, self.atom, self.maindim,
                         self.flavor, self.byteorder,
                         self.chunkshape)


class ImageArray(Array):
    """Array containing an image.

    This class has no additional behaviour or functionality compared to
    that of an ordinary array.  It simply enables the user to open an
    ``IMAGE`` HDF5 node as a normal `Array` node in PyTables.

    """

    # Class identifier.
    _c_classid = 'IMAGE'

    _c_classId = previous_api_property('_c_classid')

########NEW FILE########
__FILENAME__ = atom
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: December 16, 2004
# Author: Ivan Vilata i Balaguer - ivan at selidor dot net
#
# $Id$
#
########################################################################

"""Atom classes for describing dataset contents."""

# Imports
# =======
import re
import sys
import inspect
import cPickle

import numpy

from tables.utils import SizeType
from tables.misc.enum import Enum

from tables._past import previous_api

# Public variables
# ================
__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""

all_types = set()  # filled as atom classes are created
"""Set of all PyTables types."""

atom_map = {}  # filled as atom classes are created
"""Maps atom kinds to item sizes and atom classes.

If there is a fixed set of possible item sizes for a given kind, the
kind maps to another mapping from item size in bytes to atom class.
Otherwise, the kind maps directly to the atom class.
"""

deftype_from_kind = {}  # filled as atom classes are created
"""Maps atom kinds to their default atom type (if any)."""


# Public functions
# ================
_type_re = re.compile(r'^([a-z]+)([0-9]*)$')


def split_type(type):
    """Split a PyTables type into a PyTables kind and an item size.

    Returns a tuple of (kind, itemsize). If no item size is present in the type
    (in the form of a precision), the returned item size is None::

        >>> split_type('int32')
        ('int', 4)
        >>> split_type('string')
        ('string', None)
        >>> split_type('int20')
        Traceback (most recent call last):
        ...
        ValueError: precision must be a multiple of 8: 20
        >>> split_type('foo bar')
        Traceback (most recent call last):
        ...
        ValueError: malformed type: 'foo bar'

    """

    match = _type_re.match(type)
    if not match:
        raise ValueError("malformed type: %r" % type)
    kind, precision = match.groups()
    itemsize = None
    if precision:
        precision = int(precision)
        itemsize, remainder = divmod(precision, 8)
        if remainder:  # 0 could be a valid item size
            raise ValueError("precision must be a multiple of 8: %d"
                             % precision)
    return (kind, itemsize)


# Private functions
# =================
def _invalid_itemsize_error(kind, itemsize, itemsizes):
    isizes = sorted(itemsizes)
    return ValueError("invalid item size for kind ``%s``: %r; "
                      "it must be one of ``%r``"
                      % (kind, itemsize, isizes))


def _abstract_atom_init(deftype, defvalue):
    """Return a constructor for an abstract `Atom` class."""

    defitemsize = split_type(deftype)[1]

    def __init__(self, itemsize=defitemsize, shape=(), dflt=defvalue):
        assert self.kind in atom_map
        try:
            atomclass = atom_map[self.kind][itemsize]
        except KeyError:
            raise _invalid_itemsize_error(self.kind, itemsize,
                                          atom_map[self.kind])
        self.__class__ = atomclass
        atomclass.__init__(self, shape, dflt)
    return __init__


def _normalize_shape(shape):
    """Check that the `shape` is safe to be used and return it as a tuple."""

    if isinstance(shape, (int, numpy.integer, long)):
        if shape < 1:
            raise ValueError("shape value must be greater than 0: %d"
                             % shape)
        shape = (shape,)  # N is a shorthand for (N,)
    try:
        shape = tuple(shape)
    except TypeError:
        raise TypeError("shape must be an integer or sequence: %r"
                        % (shape,))

    ## XXX Get from HDF5 library if possible.
    # HDF5 does not support ranks greater than 32
    if len(shape) > 32:
        raise ValueError(
            "shapes with rank > 32 are not supported: %r" % (shape,))

    return tuple(SizeType(s) for s in shape)


def _normalize_default(value, dtype):
    """Return `value` as a valid default of NumPy type `dtype`."""

    # Create NumPy objects as defaults
    # This is better in order to serialize them as attributes
    if value is None:
        value = 0
    basedtype = dtype.base
    try:
        default = numpy.array(value, dtype=basedtype)
    except ValueError:
        array = numpy.array(value)
        if array.shape != basedtype.shape:
            raise
        # Maybe nested dtype with "scalar" value.
        default = numpy.array(value, dtype=basedtype.base)
    # 0-dim arrays will be representented as NumPy scalars
    # (PyTables attribute convention)
    if default.shape == ():
        default = default[()]
    return default


def _cmp_dispatcher(other_method_name):
    """Dispatch comparisons to a method of the *other* object.

    Returns a new *rich comparison* method which dispatches calls to
    the method `other_method_name` of the *other* object.  If there is
    no such method in the object, ``False`` is returned.

    This is part of the implementation of a double dispatch pattern.
    """

    def dispatched_cmp(self, other):
        try:
            other_method = getattr(other, other_method_name)
        except AttributeError:
            return False
        return other_method(self)
    return dispatched_cmp


# Helper classes
# ==============
class MetaAtom(type):
    """Atom metaclass.

    This metaclass ensures that data about atom classes gets inserted
    into the suitable registries.

    """

    def __init__(class_, name, bases, dict_):
        super(MetaAtom, class_).__init__(name, bases, dict_)

        kind = dict_.get('kind')
        itemsize = dict_.get('itemsize')
        type_ = dict_.get('type')
        deftype = dict_.get('_deftype')

        if kind and deftype:
            deftype_from_kind[kind] = deftype

        if type_:
            all_types.add(type_)

        if kind and itemsize and not hasattr(itemsize, '__int__'):
            # Atom classes with a non-fixed item size do have an
            # ``itemsize``, but it's not a number (e.g. property).
            atom_map[kind] = class_
            return

        if kind:  # first definition of kind, make new entry
            atom_map[kind] = {}

        if itemsize and hasattr(itemsize, '__int__'):  # fixed
            kind = class_.kind  # maybe from superclasses
            atom_map[kind][int(itemsize)] = class_


# Atom classes
# ============
class Atom(object):
    """Defines the type of atomic cells stored in a dataset.

    The meaning of *atomic* is that individual elements of a cell can
    not be extracted directly by indexing (i.e.  __getitem__()) the
    dataset; e.g. if a dataset has shape (2, 2) and its atoms have
    shape (3,), to get the third element of the cell at (1, 0) one
    should use dataset[1,0][2] instead of dataset[1,0,2].

    The Atom class is meant to declare the different properties of the
    *base element* (also known as *atom*) of CArray, EArray and
    VLArray datasets, although they are also used to describe the base
    elements of Array datasets. Atoms have the property that their
    length is always the same.  However, you can grow datasets along
    the extensible dimension in the case of EArray or put a variable
    number of them on a VLArray row. Moreover, they are not restricted
    to scalar values, and they can be *fully multidimensional
    objects*.

    Parameters
    ----------
    itemsize : int
        For types with a non-fixed size, this sets the size in
        bytes of individual items in the atom.
    shape : tuple
        Sets the shape of the atom. An integer shape of
        N is equivalent to the tuple (N,).
    dflt
        Sets the default value for the atom.

    The following are the public methods and attributes of the Atom class.

    Notes
    -----
    A series of descendant classes are offered in order to make the
    use of these element descriptions easier. You should use a
    particular Atom descendant class whenever you know the exact type
    you will need when writing your code. Otherwise, you may use one
    of the Atom.from_*() factory Methods.

    .. rubric:: Arom attributes

    .. attribute:: dflt

        The default value of the atom.

        If the user does not supply a value for an element while
        filling a dataset, this default value will be written to disk.
        If the user supplies a scalar value for a multidimensional
        atom, this value is automatically *broadcast* to all the items
        in the atom cell. If dflt is not supplied, an appropriate zero
        value (or *null* string) will be chosen by default.  Please
        note that default values are kept internally as NumPy objects.

    .. attribute:: dtype

        The NumPy dtype that most closely matches this atom.

    .. attribute:: itemsize

        Size in bytes of a single item in the atom.
        Specially useful for atoms of the string kind.

    .. attribute:: kind

        The PyTables kind of the atom (a string).

    .. attribute:: shape

        The shape of the atom (a tuple for scalar atoms).

    .. attribute:: type

        The PyTables type of the atom (a string).

        Atoms can be compared with atoms and other objects for
        strict (in)equality without having to compare individual
        attributes::

            >>> atom1 = StringAtom(itemsize=10)  # same as ``atom2``
            >>> atom2 = Atom.from_kind('string', 10)  # same as ``atom1``
            >>> atom3 = IntAtom()
            >>> atom1 == 'foo'
            False
            >>> atom1 == atom2
            True
            >>> atom2 != atom1
            False
            >>> atom1 == atom3
            False
            >>> atom3 != atom2
            True

    """

    # Register data for all subclasses.
    __metaclass__ = MetaAtom

    # Class methods
    # ~~~~~~~~~~~~~
    @classmethod
    def prefix(class_):
        """Return the atom class prefix."""
        cname = class_.__name__
        return cname[:cname.rfind('Atom')]

    @classmethod
    def from_sctype(class_, sctype, shape=(), dflt=None):
        """Create an Atom from a NumPy scalar type sctype.

        Optional shape and default value may be specified as the
        shape and dflt
        arguments, respectively. Information in the
        sctype not represented in an Atom is ignored::

            >>> import numpy
            >>> Atom.from_sctype(numpy.int16, shape=(2, 2))
            Int16Atom(shape=(2, 2), dflt=0)
            >>> Atom.from_sctype('S5', dflt='hello')
            Traceback (most recent call last):
            ...
            ValueError: unknown NumPy scalar type: 'S5'
            >>> Atom.from_sctype('Float64')
            Float64Atom(shape=(), dflt=0.0)

        """
        if (not isinstance(sctype, type)
           or not issubclass(sctype, numpy.generic)):
            if sctype not in numpy.sctypeDict:
                raise ValueError("unknown NumPy scalar type: %r" % (sctype,))
            sctype = numpy.sctypeDict[sctype]
        return class_.from_dtype(numpy.dtype((sctype, shape)), dflt)

    @classmethod
    def from_dtype(class_, dtype, dflt=None):
        """Create an Atom from a NumPy dtype.

        An optional default value may be specified as the dflt
        argument. Information in the dtype not represented in an Atom is
        ignored::

            >>> import numpy
            >>> Atom.from_dtype(numpy.dtype((numpy.int16, (2, 2))))
            Int16Atom(shape=(2, 2), dflt=0)
            >>> Atom.from_dtype(numpy.dtype('Float64'))
            Float64Atom(shape=(), dflt=0.0)

        """
        basedtype = dtype.base
        if basedtype.names:
            raise ValueError("compound data types are not supported: %r"
                             % dtype)
        if basedtype.shape != ():
            raise ValueError("nested data types are not supported: %r"
                             % dtype)
        if basedtype.kind == 'S':  # can not reuse something like 'string80'
            itemsize = basedtype.itemsize
            return class_.from_kind('string', itemsize, dtype.shape, dflt)
        # Most NumPy types have direct correspondence with PyTables types.
        return class_.from_type(basedtype.name, dtype.shape, dflt)

    @classmethod
    def from_type(class_, type, shape=(), dflt=None):
        """Create an Atom from a PyTables type.

        Optional shape and default value may be specified as the
        shape and dflt arguments, respectively::

            >>> Atom.from_type('bool')
            BoolAtom(shape=(), dflt=False)
            >>> Atom.from_type('int16', shape=(2, 2))
            Int16Atom(shape=(2, 2), dflt=0)
            >>> Atom.from_type('string40', dflt='hello')
            Traceback (most recent call last):
            ...
            ValueError: unknown type: 'string40'
            >>> Atom.from_type('Float64')
            Traceback (most recent call last):
            ...
            ValueError: unknown type: 'Float64'

        """

        if type not in all_types:
            raise ValueError("unknown type: %r" % (type,))
        kind, itemsize = split_type(type)
        return class_.from_kind(kind, itemsize, shape, dflt)

    @classmethod
    def from_kind(class_, kind, itemsize=None, shape=(), dflt=None):
        """Create an Atom from a PyTables kind.

        Optional item size, shape and default value may be
        specified as the itemsize, shape and dflt
        arguments, respectively. Bear in mind that not all atoms support
        a default item size::

            >>> Atom.from_kind('int', itemsize=2, shape=(2, 2))
            Int16Atom(shape=(2, 2), dflt=0)
            >>> Atom.from_kind('int', shape=(2, 2))
            Int32Atom(shape=(2, 2), dflt=0)
            >>> Atom.from_kind('int', shape=1)
            Int32Atom(shape=(1,), dflt=0)
            >>> Atom.from_kind('string', dflt=b'hello')
            Traceback (most recent call last):
            ...
            ValueError: no default item size for kind ``string``
            >>> Atom.from_kind('Float')
            Traceback (most recent call last):
            ...
            ValueError: unknown kind: 'Float'

        Moreover, some kinds with atypical constructor signatures
        are not supported; you need to use the proper
        constructor::

            >>> Atom.from_kind('enum') #doctest: +ELLIPSIS
            Traceback (most recent call last):
            ...
            ValueError: the ``enum`` kind is not supported...

        """

        kwargs = {'shape': shape}
        if kind not in atom_map:
            raise ValueError("unknown kind: %r" % (kind,))
        # This incompatibility detection may get out-of-date and is
        # too hard-wired, but I couldn't come up with something
        # smarter.  -- Ivan (2007-02-08)
        if kind in ['enum']:
            raise ValueError("the ``%s`` kind is not supported; "
                             "please use the appropriate constructor"
                             % kind)
        # If no `itemsize` is given, try to get the default type of the
        # kind (which has a fixed item size).
        if itemsize is None:
            if kind not in deftype_from_kind:
                raise ValueError("no default item size for kind ``%s``"
                                 % kind)
            type_ = deftype_from_kind[kind]
            kind, itemsize = split_type(type_)
        kdata = atom_map[kind]
        # Look up the class and set a possible item size.
        if hasattr(kdata, 'kind'):  # atom class: non-fixed item size
            atomclass = kdata
            kwargs['itemsize'] = itemsize
        else:  # dictionary: fixed item size
            if itemsize not in kdata:
                raise _invalid_itemsize_error(kind, itemsize, kdata)
            atomclass = kdata[itemsize]
        # Only set a `dflt` argument if given (`None` may not be understood).
        if dflt is not None:
            kwargs['dflt'] = dflt

        return atomclass(**kwargs)

    # Properties
    # ~~~~~~~~~~
    size = property(
        lambda self: self.dtype.itemsize,
        None, None, "Total size in bytes of the atom.")
    recarrtype = property(
        lambda self: str(self.dtype.shape) + self.dtype.base.str[1:],
        None, None, "String type to be used in numpy.rec.array().")
    ndim = property(
        lambda self: len(self.shape), None, None,
        """The number of dimensions of the atom.

        .. versionadded:: 2.4""")

    # Special methods
    # ~~~~~~~~~~~~~~~
    def __init__(self, nptype, shape, dflt):
        if not hasattr(self, 'type'):
            raise NotImplementedError("``%s`` is an abstract class; "
                                      "please use one of its subclasses"
                                      % self.__class__.__name__)
        self.shape = shape = _normalize_shape(shape)
        """The shape of the atom (a tuple for scalar atoms)."""
        # Curiously enough, NumPy isn't generally able to accept NumPy
        # integers in a shape. ;(
        npshape = tuple(int(s) for s in shape)
        self.dtype = dtype = numpy.dtype((nptype, npshape))
        """The NumPy dtype that most closely matches this atom."""
        self.dflt = _normalize_default(dflt, dtype)
        """The default value of the atom.

        If the user does not supply a value for an element while
        filling a dataset, this default value will be written to
        disk. If the user supplies a scalar value for a
        multidimensional atom, this value is automatically *broadcast*
        to all the items in the atom cell. If dflt is not supplied, an
        appropriate zero value (or *null* string) will be chosen by
        default.  Please note that default values are kept internally
        as NumPy objects."""

    def __repr__(self):
        args = 'shape=%s, dflt=%r' % (self.shape, self.dflt)
        if not hasattr(self.__class__.itemsize, '__int__'):  # non-fixed
            args = 'itemsize=%s, %s' % (self.itemsize, args)
        return '%s(%s)' % (self.__class__.__name__, args)

    __eq__ = _cmp_dispatcher('_is_equal_to_atom')

    def __ne__(self, other):
        return not self.__eq__(other)

    # XXX: API incompatible change for PyTables 3 line
    # Overriding __eq__ blocks inheritance of __hash__ in 3.x
    # def __hash__(self):
    #    return hash((self.__class__, self.type, self.shape, self.itemsize,
    #                 self.dflt))

    # Public methods
    # ~~~~~~~~~~~~~~
    def copy(self, **override):
        """Get a copy of the atom, possibly overriding some arguments.

        Constructor arguments to be overridden must be passed as
        keyword arguments::

            >>> atom1 = Int32Atom(shape=12)
            >>> atom2 = atom1.copy()
            >>> print(atom1)
            Int32Atom(shape=(12,), dflt=0)
            >>> print(atom2)
            Int32Atom(shape=(12,), dflt=0)
            >>> atom1 is atom2
            False
            >>> atom3 = atom1.copy(shape=(2, 2))
            >>> print(atom3)
            Int32Atom(shape=(2, 2), dflt=0)
            >>> atom1.copy(foobar=42)
            Traceback (most recent call last):
            ...
            TypeError: __init__() got an unexpected keyword argument 'foobar'

        """
        newargs = self._get_init_args()
        newargs.update(override)
        return self.__class__(**newargs)

    # Private methods
    # ~~~~~~~~~~~~~~~
    def _get_init_args(self):
        """Get a dictionary of instance constructor arguments.

        This implementation works on classes which use the same names
        for both constructor arguments and instance attributes.

        """

        return dict((arg, getattr(self, arg))
                    for arg in inspect.getargspec(self.__init__)[0]
                    if arg != 'self')

    def _is_equal_to_atom(self, atom):
        """Is this object equal to the given `atom`?"""

        return (self.type == atom.type and self.shape == atom.shape
                and self.itemsize == atom.itemsize
                and numpy.all(self.dflt == atom.dflt))


class StringAtom(Atom):
    """Defines an atom of type string.

    The item size is the *maximum* length in characters of strings.

    """

    kind = 'string'
    itemsize = property(
        lambda self: self.dtype.base.itemsize,
        None, None, "Size in bytes of a sigle item in the atom.")
    type = 'string'
    _defvalue = b''

    def __init__(self, itemsize, shape=(), dflt=_defvalue):
        if not hasattr(itemsize, '__int__') or int(itemsize) < 0:
            raise ValueError("invalid item size for kind ``%s``: %r; "
                             "it must be a positive integer"
                             % ('string', itemsize))
        Atom.__init__(self, 'S%d' % itemsize, shape, dflt)


class BoolAtom(Atom):
    """Defines an atom of type bool."""

    kind = 'bool'
    itemsize = 1
    type = 'bool'
    _deftype = 'bool8'
    _defvalue = False

    def __init__(self, shape=(), dflt=_defvalue):
        Atom.__init__(self, self.type, shape, dflt)


class IntAtom(Atom):
    """Defines an atom of a signed integral type (int kind)."""

    kind = 'int'
    signed = True
    _deftype = 'int32'
    _defvalue = 0
    __init__ = _abstract_atom_init(_deftype, _defvalue)


class UIntAtom(Atom):
    """Defines an atom of an unsigned integral type (uint kind)."""

    kind = 'uint'
    signed = False
    _deftype = 'uint32'
    _defvalue = 0
    __init__ = _abstract_atom_init(_deftype, _defvalue)


class FloatAtom(Atom):
    """Defines an atom of a floating point type (float kind)."""

    kind = 'float'
    _deftype = 'float64'
    _defvalue = 0.0
    __init__ = _abstract_atom_init(_deftype, _defvalue)


def _create_numeric_class(baseclass, itemsize):
    """Create a numeric atom class with the given `baseclass` and an
    `itemsize`."""

    prefix = '%s%d' % (baseclass.prefix(), itemsize * 8)
    type_ = prefix.lower()
    classdict = {'itemsize': itemsize, 'type': type_,
                 '__doc__': "Defines an atom of type ``%s``." % type_}

    def __init__(self, shape=(), dflt=baseclass._defvalue):
        Atom.__init__(self, self.type, shape, dflt)
    classdict['__init__'] = __init__
    return type('%sAtom' % prefix, (baseclass,), classdict)


def _generate_integral_classes():
    """Generate all integral classes."""

    for baseclass in [IntAtom, UIntAtom]:
        for itemsize in [1, 2, 4, 8]:
            newclass = _create_numeric_class(baseclass, itemsize)
            yield newclass


def _generate_floating_classes():
    """Generate all floating classes."""

    itemsizes = [4, 8]

    # numpy >= 1.6
    if hasattr(numpy, 'float16'):
        itemsizes.insert(0, 2)
    if hasattr(numpy, 'float96'):
        itemsizes.append(12)
    if hasattr(numpy, 'float128'):
        itemsizes.append(16)

    for itemsize in itemsizes:
        newclass = _create_numeric_class(FloatAtom, itemsize)
        yield newclass


# Create all numeric atom classes.
for _classgen in [_generate_integral_classes, _generate_floating_classes]:
    for _newclass in _classgen():
        exec('%s = _newclass' % _newclass.__name__)
del _classgen, _newclass


class ComplexAtom(Atom):
    """Defines an atom of kind complex.

    Allowed item sizes are 8 (single precision) and 16 (double precision). This
    class must be used instead of more concrete ones to avoid confusions with
    numarray-like precision specifications used in PyTables 1.X.

    """

    # This definition is a little more complex (no pun intended)
    # because, although the complex kind is a normal numerical one,
    # the usage of bottom-level classes is artificially forbidden.
    # Everything will be back to normality when people has stopped
    # using the old bottom-level complex classes.

    kind = 'complex'
    itemsize = property(
        lambda self: self.dtype.base.itemsize,
        None, None, "Size in bytes of a sigle item in the atom.")
    _deftype = 'complex128'
    _defvalue = 0j
    _isizes = [8, 16]

    # Only instances have a `type` attribute, so complex types must be
    # registered by hand.
    all_types.add('complex64')
    all_types.add('complex128')
    if hasattr(numpy, 'complex192'):
        all_types.add('complex192')
        _isizes.append(24)
    if hasattr(numpy, 'complex256'):
        all_types.add('complex256')
        _isizes.append(32)

    def __init__(self, itemsize, shape=(), dflt=_defvalue):
        if itemsize not in self._isizes:
            raise _invalid_itemsize_error('complex', itemsize, self._isizes)
        self.type = '%s%d' % (self.kind, itemsize * 8)
        Atom.__init__(self, self.type, shape, dflt)


class _ComplexErrorAtom(ComplexAtom):
    """Reminds the user to stop using the old complex atom names."""

    __metaclass__ = type  # do not register anything about this class

    def __init__(self, shape=(), dflt=ComplexAtom._defvalue):
        raise TypeError(
            "to avoid confusions with PyTables 1.X complex atom names, "
            "please use ``ComplexAtom(itemsize=N)``, "
            "where N=8 for single precision complex atoms, "
            "and N=16 for double precision complex atoms")
Complex32Atom = Complex64Atom = Complex128Atom = _ComplexErrorAtom
if hasattr(numpy, 'complex192'):
    Complex192Atom = _ComplexErrorAtom
if hasattr(numpy, 'complex256'):
    Complex256Atom = _ComplexErrorAtom


class TimeAtom(Atom):
    """Defines an atom of time type (time kind).

    There are two distinct supported types of time: a 32 bit integer value and
    a 64 bit floating point value. Both of them reflect the number of seconds
    since the Unix epoch. This atom has the property of being stored using the
    HDF5 time datatypes.

    """

    kind = 'time'
    _deftype = 'time32'
    _defvalue = 0
    __init__ = _abstract_atom_init(_deftype, _defvalue)


class Time32Atom(TimeAtom):
    """Defines an atom of type time32."""

    itemsize = 4
    type = 'time32'
    _defvalue = 0

    def __init__(self, shape=(), dflt=_defvalue):
        Atom.__init__(self, 'int32', shape, dflt)


class Time64Atom(TimeAtom):
    """Defines an atom of type time64."""

    itemsize = 8
    type = 'time64'
    _defvalue = 0.0

    def __init__(self, shape=(), dflt=_defvalue):
        Atom.__init__(self, 'float64', shape, dflt)


class EnumAtom(Atom):
    """Description of an atom of an enumerated type.

    Instances of this class describe the atom type used to store enumerated
    values. Those values belong to an enumerated type, defined by the first
    argument (enum) in the constructor of the atom, which accepts the same
    kinds of arguments as the Enum class (see :ref:`EnumClassDescr`).  The
    enumerated type is stored in the enum attribute of the atom.

    A default value must be specified as the second argument (dflt) in the
    constructor; it must be the *name* (a string) of one of the enumerated
    values in the enumerated type. When the atom is created, the corresponding
    concrete value is broadcast and stored in the dflt attribute (setting
    different default values for items in a multidimensional atom is not
    supported yet). If the name does not match any value in the enumerated
    type, a KeyError is raised.

    Another atom must be specified as the base argument in order to determine
    the base type used for storing the values of enumerated values in memory
    and disk. This *storage atom* is kept in the base attribute of the created
    atom. As a shorthand, you may specify a PyTables type instead of the
    storage atom, implying that this has a scalar shape.

    The storage atom should be able to represent each and every concrete value
    in the enumeration. If it is not, a TypeError is raised. The default value
    of the storage atom is ignored.

    The type attribute of enumerated atoms is always enum.

    Enumerated atoms also support comparisons with other objects::

        >>> enum = ['T0', 'T1', 'T2']
        >>> atom1 = EnumAtom(enum, 'T0', 'int8')  # same as ``atom2``
        >>> atom2 = EnumAtom(enum, 'T0', Int8Atom())  # same as ``atom1``
        >>> atom3 = EnumAtom(enum, 'T0', 'int16')
        >>> atom4 = Int8Atom()
        >>> atom1 == enum
        False
        >>> atom1 == atom2
        True
        >>> atom2 != atom1
        False
        >>> atom1 == atom3
        False
        >>> atom1 == atom4
        False
        >>> atom4 != atom1
        True

    Examples
    --------

    The next C enum construction::

        enum myEnum {
            T0,
            T1,
            T2
        };

    would correspond to the following PyTables
    declaration::

        >>> my_enum_atom = EnumAtom(['T0', 'T1', 'T2'], 'T0', 'int32')

    Please note the dflt argument with a value of 'T0'. Since the concrete
    value matching T0 is unknown right now (we have not used explicit concrete
    values), using the name is the only option left for defining a default
    value for the atom.

    The chosen representation of values for this enumerated atom uses unsigned
    32-bit integers, which surely wastes quite a lot of memory. Another size
    could be selected by using the base argument (this time with a full-blown
    storage atom)::

        >>> my_enum_atom = EnumAtom(['T0', 'T1', 'T2'], 'T0', UInt8Atom())

    You can also define multidimensional arrays for data elements::

        >>> my_enum_atom = EnumAtom(
        ...    ['T0', 'T1', 'T2'], 'T0', base='uint32', shape=(3,2))

    for 3x2 arrays of uint32.

    """

    # Registering this class in the class map may be a little wrong,
    # since the ``Atom.from_kind()`` method fails miserably with
    # enumerations, as they don't support an ``itemsize`` argument.
    # However, resetting ``__metaclass__`` to ``type`` doesn't seem to
    # work and I don't feel like creating a subclass of ``MetaAtom``.

    kind = 'enum'
    type = 'enum'

    # Properties
    # ~~~~~~~~~~
    itemsize = property(
        lambda self: self.dtype.base.itemsize,
        None, None, "Size in bytes of a sigle item in the atom.")

    # Private methods
    # ~~~~~~~~~~~~~~~
    def _checkbase(self, base):
        """Check the `base` storage atom."""

        if base.kind == 'enum':
            raise TypeError("can not use an enumerated atom "
                            "as a storage atom: %r" % base)

        # Check whether the storage atom can represent concrete values
        # in the enumeration...
        basedtype = base.dtype
        pyvalues = [value for (name, value) in self.enum]
        try:
            npgenvalues = numpy.array(pyvalues)
        except ValueError:
            raise TypeError("concrete values are not uniformly-shaped")
        try:
            npvalues = numpy.array(npgenvalues, dtype=basedtype.base)
        except ValueError:
            raise TypeError("storage atom type is incompatible with "
                            "concrete values in the enumeration")
        if npvalues.shape[1:] != basedtype.shape:
            raise TypeError("storage atom shape does not match that of "
                            "concrete values in the enumeration")
        if npvalues.tolist() != npgenvalues.tolist():
            raise TypeError("storage atom type lacks precision for "
                            "concrete values in the enumeration")

        # ...with some implementation limitations.
        if not npvalues.dtype.kind in ['i', 'u']:
            raise NotImplementedError("only integer concrete values "
                                      "are supported for the moment, sorry")
        if len(npvalues.shape) > 1:
            raise NotImplementedError("only scalar concrete values "
                                      "are supported for the moment, sorry")

    _checkBase = previous_api(_checkbase)

    def _get_init_args(self):
        """Get a dictionary of instance constructor arguments."""

        return dict(enum=self.enum, dflt=self._defname,
                    base=self.base, shape=self.shape)

    def _is_equal_to_atom(self, atom):
        """Is this object equal to the given `atom`?"""

        return False

    def _is_equal_to_enumatom(self, enumatom):
        """Is this object equal to the given `enumatom`?"""

        return (self.enum == enumatom.enum and self.shape == enumatom.shape
                and numpy.all(self.dflt == enumatom.dflt)
                and self.base == enumatom.base)

    # Special methods
    # ~~~~~~~~~~~~~~~
    def __init__(self, enum, dflt, base, shape=()):
        if not isinstance(enum, Enum):
            enum = Enum(enum)
        self.enum = enum

        if isinstance(base, str):
            base = Atom.from_type(base)
        self._checkbase(base)
        self.base = base

        default = enum[dflt]  # check default value
        self._defname = dflt  # kept for representation purposes

        # These are kept to ease dumping this particular
        # representation of the enumeration to storage.
        names, values = [], []
        for (name, value) in enum:
            names.append(name)
            values.append(value)
        basedtype = self.base.dtype

        self._names = names
        self._values = numpy.array(values, dtype=basedtype.base)

        Atom.__init__(self, basedtype, shape, default)

    def __repr__(self):
        return ('EnumAtom(enum=%r, dflt=%r, base=%r, shape=%r)'
                % (self.enum, self._defname, self.base, self.shape))

    __eq__ = _cmp_dispatcher('_is_equal_to_enumatom')

    # XXX: API incompatible change for PyTables 3 line
    # Overriding __eq__ blocks inheritance of __hash__ in 3.x
    # def __hash__(self):
    #    return hash((self.__class__, self.enum, self.shape, self.dflt,
    #                 self.base))

# Pseudo-atom classes
# ===================
#
# Now, there come three special classes, `ObjectAtom`, `VLStringAtom`
# and `VLUnicodeAtom`, that actually do not descend from `Atom`, but
# which goal is so similar that they should be described here.
# Pseudo-atoms can only be used with `VLArray` datasets, and they do
# not support multidimensional values, nor multiple values per row.
#
# They can be recognised because they also have ``kind``, ``type`` and
# ``shape`` attributes, but no ``size``, ``itemsize`` or ``dflt``
# ones.  Instead, they have a ``base`` atom which defines the elements
# used for storage.
#
# See ``examples/vlarray1.py`` and ``examples/vlarray2.py`` for
# further examples on `VLArray` datasets, including object
# serialization and string management.


class PseudoAtom(object):
    """Pseudo-atoms can only be used in ``VLArray`` nodes.

    They can be recognised because they also have `kind`, `type` and
    `shape` attributes, but no `size`, `itemsize` or `dflt` ones.
    Instead, they have a `base` atom which defines the elements used
    for storage.
    """

    def __repr__(self):
        return '%s()' % self.__class__.__name__

    def toarray(self, object_):
        """Convert an `object_` into an array of base atoms."""

        raise NotImplementedError

    def fromarray(self, array):
        """Convert an `array` of base atoms into an object."""

        raise NotImplementedError


class _BufferedAtom(PseudoAtom):
    """Pseudo-atom which stores data as a buffer (flat array of uints)."""

    shape = ()

    def toarray(self, object_):
        buffer_ = self._tobuffer(object_)
        array = numpy.ndarray(buffer=buffer_, dtype=self.base.dtype,
                              shape=len(buffer_))
        return array

    def _tobuffer(self, object_):
        """Convert an `object_` into a buffer."""

        raise NotImplementedError


class VLStringAtom(_BufferedAtom):
    """Defines an atom of type ``vlstring``.

    This class describes a *row* of the VLArray class, rather than an atom. It
    differs from the StringAtom class in that you can only add *one instance of
    it to one specific row*, i.e. the :meth:`VLArray.append` method only
    accepts one object when the base atom is of this type.

    Like StringAtom, this class does not make assumptions on the encoding of
    the string, and raw bytes are stored as is.  Unicode strings are supported
    as long as no character is out of the ASCII set; otherwise, you will need
    to *explicitly* convert them to strings before you can save them.  For full
    Unicode support, using VLUnicodeAtom (see :ref:`VLUnicodeAtom`) is
    recommended.

    Variable-length string atoms do not accept parameters and they cause the
    reads of rows to always return Python strings.  You can regard vlstring
    atoms as an easy way to save generic variable length strings.

    """

    kind = 'vlstring'
    type = 'vlstring'
    base = UInt8Atom()

    def _tobuffer(self, object_):
        if not isinstance(object_, basestring):
            raise TypeError("object is not a string: %r" % (object_,))
        return numpy.string_(object_)

    def fromarray(self, array):
        return array.tostring()


class VLUnicodeAtom(_BufferedAtom):
    """Defines an atom of type vlunicode.

    This class describes a *row* of the VLArray class, rather than an atom.  It
    is very similar to VLStringAtom (see :ref:`VLStringAtom`), but it stores
    Unicode strings (using 32-bit characters a la UCS-4, so all strings of the
    same length also take up the same space).

    This class does not make assumptions on the encoding of plain input
    strings.  Plain strings are supported as long as no character is out of the
    ASCII set; otherwise, you will need to *explicitly* convert them to Unicode
    before you can save them.

    Variable-length Unicode atoms do not accept parameters and they cause the
    reads of rows to always return Python Unicode strings.  You can regard
    vlunicode atoms as an easy way to save variable length Unicode strings.

    """

    kind = 'vlunicode'
    type = 'vlunicode'
    base = UInt32Atom()

    if sys.version_info[0] > 2 or sys.maxunicode <= 0xffff:
        # numpy.unicode_ no more implements the buffer interface in Python 3
        #
        # When the Python build is UCS-2, we need to promote the
        # Unicode string to UCS-4.  We *must* use a 0-d array since
        # NumPy scalars inherit the UCS-2 encoding from Python (see
        # NumPy ticket #525).  Since ``_tobuffer()`` can't return an
        # array, we must override ``toarray()`` itself.
        def toarray(self, object_):
            if not isinstance(object_, basestring):
                raise TypeError("object is not a string: %r" % (object_,))
            ustr = unicode(object_)
            uarr = numpy.array(ustr, dtype='U')
            return numpy.ndarray(
                buffer=uarr, dtype=self.base.dtype, shape=len(ustr))

    def _tobuffer(self, object_):
        # This works (and is used) only with UCS-4 builds of Python,
        # where the width of the internal representation of a
        # character matches that of the base atoms.
        if not isinstance(object_, basestring):
            raise TypeError("object is not a string: %r" % (object_,))
        return numpy.unicode_(object_)

    def fromarray(self, array):
        length = len(array)
        if length == 0:
            return u''  # ``array.view('U0')`` raises a `TypeError`
        return array.view('U%d' % length).item()


class ObjectAtom(_BufferedAtom):
    """Defines an atom of type object.

    This class is meant to fit *any* kind of Python object in a row of a
    VLArray dataset by using pickle behind the scenes. Due to the fact that
    you can not foresee how long will be the output of the pickle
    serialization (i.e. the atom already has a *variable* length), you can only
    fit *one object per row*. However, you can still group several objects in a
    single tuple or list and pass it to the :meth:`VLArray.append` method.

    Object atoms do not accept parameters and they cause the reads of rows to
    always return Python objects. You can regard object atoms as an easy way to
    save an arbitrary number of generic Python objects in a VLArray dataset.

    """

    kind = 'object'
    type = 'object'
    base = UInt8Atom()

    def _tobuffer(self, object_):
        return cPickle.dumps(object_, cPickle.HIGHEST_PROTOCOL)

    def fromarray(self, array):
        # We have to check for an empty array because of a possible
        # bug in HDF5 which makes it claim that a dataset has one
        # record when in fact it is empty.
        if array.size == 0:
            return None
        return cPickle.loads(array.tostring())


# Main part
# =========
def _test():
    """Run ``doctest`` on this module."""

    import doctest
    doctest.testmod()


if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = attributeset
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: May 26, 2003
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the AttributeSet class."""

import re
import sys
import warnings
import cPickle
import numpy

from tables import hdf5extension
from tables.utils import SizeType
from tables.registry import class_name_dict
from tables.exceptions import ClosedNodeError, PerformanceWarning
from tables.path import check_name_validity
from tables.undoredo import attr_to_shadow
from tables.filters import Filters

from tables._past import previous_api

# System attributes
SYS_ATTRS = ["CLASS", "VERSION", "TITLE", "NROWS", "EXTDIM",
             "ENCODING", "PYTABLES_FORMAT_VERSION",
             "FLAVOR", "FILTERS", "AUTO_INDEX",
             "DIRTY", "NODE_TYPE", "NODE_TYPE_VERSION",
             "PSEUDOATOM"]
# Prefixes of other system attributes
SYS_ATTRS_PREFIXES = ["FIELD_"]
# RO_ATTRS will be disabled and let the user modify them if they
# want to. The user is still not allowed to remove or rename
# system attributes. Francesc Alted 2004-12-19
# Read-only attributes:
# RO_ATTRS = ["CLASS", "FLAVOR", "VERSION", "NROWS", "EXTDIM",
#             "PYTABLES_FORMAT_VERSION", "FILTERS",
#             "NODE_TYPE", "NODE_TYPE_VERSION"]
# RO_ATTRS = []

# The next attributes are not meant to be copied during a Node copy process
SYS_ATTRS_NOTTOBECOPIED = ["CLASS", "VERSION", "TITLE", "NROWS", "EXTDIM",
                           "PYTABLES_FORMAT_VERSION", "FILTERS", "ENCODING"]
# Attributes forced to be copied during node copies
FORCE_COPY_CLASS = ['CLASS', 'VERSION']
# Regular expression for column default values.
_field_fill_re = re.compile('^FIELD_[0-9]+_FILL$')
# Regular expression for fixing old pickled filters.
_old_filters_re = re.compile(br'\(([ic])tables\.Leaf\n')
# Fixed version of the previous string.
_new_filters_sub = br'(\1tables.filters\n'


def issysattrname(name):
    "Check if a name is a system attribute or not"

    if (name in SYS_ATTRS or
            numpy.prod([name.startswith(prefix)
                        for prefix in SYS_ATTRS_PREFIXES])):
        return True
    else:
        return False


class AttributeSet(hdf5extension.AttributeSet, object):
    """Container for the HDF5 attributes of a Node.

    This class provides methods to create new HDF5 node attributes,
    and to get, rename or delete existing ones.

    Like in Group instances (see :ref:`GroupClassDescr`), AttributeSet
    instances make use of the *natural naming* convention, i.e. you can
    access the attributes on disk as if they were normal Python
    attributes of the AttributeSet instance.

    This offers the user a very convenient way to access HDF5 node
    attributes. However, for this reason and in order not to pollute the
    object namespace, one can not assign *normal* attributes to
    AttributeSet instances, and their members use names which start by
    special prefixes as happens with Group objects.

    .. rubric:: Notes on native and pickled attributes

    The values of most basic types are saved as HDF5 native data in the
    HDF5 file.  This includes Python bool, int, float, complex and str
    (but not long nor unicode) values, as well as their NumPy scalar
    versions and homogeneous or *structured* NumPy arrays of them.  When
    read, these values are always loaded as NumPy scalar or array
    objects, as needed.

    For that reason, attributes in native HDF5 files will be always
    mapped into NumPy objects.  Specifically, a multidimensional
    attribute will be mapped into a multidimensional ndarray and a
    scalar will be mapped into a NumPy scalar object (for example, a
    scalar H5T_NATIVE_LLONG will be read and returned as a numpy.int64
    scalar).

    However, other kinds of values are serialized using pickle, so you
    only will be able to correctly retrieve them using a Python-aware
    HDF5 library.  Thus, if you want to save Python scalar values and
    make sure you are able to read them with generic HDF5 tools, you
    should make use of *scalar or homogeneous/structured array NumPy
    objects* (for example, numpy.int64(1) or numpy.array([1, 2, 3],
    dtype='int16')).

    One more advice: because of the various potential difficulties in
    restoring a Python object stored in an attribute, you may end up
    getting a pickle string where a Python object is expected. If this
    is the case, you may wish to run pickle.loads() on that string to
    get an idea of where things went wrong, as shown in this example::

        >>> import os, tempfile
        >>> import tables
        >>>
        >>> class MyClass(object):
        ...   foo = 'bar'
        ...
        >>> myObject = MyClass()  # save object of custom class in HDF5 attr
        >>> h5fname = tempfile.mktemp(suffix='.h5')
        >>> h5f = tables.open_file(h5fname, 'w')
        >>> h5f.root._v_attrs.obj = myObject  # store the object
        >>> print(h5f.root._v_attrs.obj.foo)  # retrieve it
        bar
        >>> h5f.close()
        >>>
        >>> del MyClass, myObject  # delete class of object and reopen file
        >>> h5f = tables.open_file(h5fname, 'r')
        >>> print(repr(h5f.root._v_attrs.obj))
        'ccopy_reg\\n_reconstructor...
        >>> import pickle  # let's unpickle that to see what went wrong
        >>> pickle.loads(h5f.root._v_attrs.obj)
        Traceback (most recent call last):
        ...
        AttributeError: 'module' object has no attribute 'MyClass'
        >>> # So the problem was not in the stored object,
        ... # but in the *environment* where it was restored.
        ... h5f.close()
        >>> os.remove(h5fname)


    .. rubric:: Notes on AttributeSet methods

    Note that this class overrides the __getattr__(), __setattr__() and
    __delattr__() special methods.  This allows you to read, assign or
    delete attributes on disk by just using the next constructs::

        leaf.attrs.myattr = 'str attr'    # set a string (native support)
        leaf.attrs.myattr2 = 3            # set an integer (native support)
        leaf.attrs.myattr3 = [3, (1, 2)]  # a generic object (Pickled)
        attrib = leaf.attrs.myattr        # get the attribute ``myattr``
        del leaf.attrs.myattr             # delete the attribute ``myattr``

    In addition, the dictionary-like __getitem__(), __setitem__() and
    __delitem__() methods are available, so you may write things like
    this::

        for name in :attr:`Node._v_attrs`._f_list():
            print("name: %s, value: %s" % (name, :attr:`Node._v_attrs`[name]))

    Use whatever idiom you prefer to access the attributes.

    If an attribute is set on a target node that already has a large
    number of attributes, a PerformanceWarning will be issued.


    .. rubric:: AttributeSet attributes

    .. attribute:: _v_attrnames

        A list with all attribute names.

    .. attribute:: _v_attrnamessys

        A list with system attribute names.

    .. attribute:: _v_attrnamesuser

        A list with user attribute names.

    .. attribute:: _v_unimplemented

        A list of attribute names with unimplemented native HDF5 types.

    """

    def _g_getnode(self):
        return self._v__nodefile._get_node(self._v__nodepath)

    _v_node = property(_g_getnode, None, None,
                       "The :class:`Node` instance this attribute set is "
                       "associated with.")

    def __init__(self, node):
        """Create the basic structures to keep the attribute information.

        Reads all the HDF5 attributes (if any) on disk for the node "node".

        Parameters
        ----------
        node
            The parent node

        """

        # Refuse to create an instance of an already closed node
        if not node._v_isopen:
            raise ClosedNodeError("the node for attribute set is closed")

        mydict = self.__dict__

        self._g_new(node)
        mydict["_v__nodefile"] = node._v_file
        mydict["_v__nodepath"] = node._v_pathname
        mydict["_v_attrnames"] = self._g_list_attr(node)
        # The list of unimplemented attribute names
        mydict["_v_unimplemented"] = []

        # Get the file version format. This is an optimization
        # in order to avoid accessing it too much.
        try:
            format_version = node._v_file.format_version
        except AttributeError:
            parsed_version = None
        else:
            if format_version == 'unknown':
                parsed_version = None
            else:
                parsed_version = tuple(map(int, format_version.split('.')))
        mydict["_v__format_version"] = parsed_version
        # Split the attribute list in system and user lists
        mydict["_v_attrnamessys"] = []
        mydict["_v_attrnamesuser"] = []
        for attr in self._v_attrnames:
            # put the attributes on the local dictionary to allow
            # tab-completion
            self.__getattr__(attr)
            if issysattrname(attr):
                self._v_attrnamessys.append(attr)
            else:
                self._v_attrnamesuser.append(attr)

        # Sort the attributes
        self._v_attrnames.sort()
        self._v_attrnamessys.sort()
        self._v_attrnamesuser.sort()

    def _g_update_node_location(self, node):
        """Updates the location information about the associated `node`."""

        myDict = self.__dict__
        myDict['_v__nodefile'] = node._v_file
        myDict['_v__nodepath'] = node._v_pathname
        # hdf5extension operations:
        self._g_new(node)

    _g_updateNodeLocation = previous_api(_g_update_node_location)

    def _f_list(self, attrset='user'):
        """Get a list of attribute names.

        The attrset string selects the attribute set to be used.  A
        'user' value returns only user attributes (this is the default).
        A 'sys' value returns only system attributes.  Finally, 'all'
        returns both system and user attributes.

        """

        if attrset == "user":
            return self._v_attrnamesuser[:]
        elif attrset == "sys":
            return self._v_attrnamessys[:]
        elif attrset == "all":
            return self._v_attrnames[:]

    def __getattr__(self, name):
        """Get the attribute named "name"."""

        # If attribute does not exist, raise AttributeError
        if not name in self._v_attrnames:
            raise AttributeError("Attribute '%s' does not exist in node: "
                                 "'%s'" % (name, self._v__nodepath))

        # Read the attribute from disk. This is an optimization to read
        # quickly system attributes that are _string_ values, but it
        # takes care of other types as well as for example NROWS for
        # Tables and EXTDIM for EArrays
        format_version = self._v__format_version
        value = self._g_getattr(self._v_node, name)

        # Check whether the value is pickled
        # Pickled values always seems to end with a "."
        maybe_pickled = (
            isinstance(value, numpy.generic) and  # NumPy scalar?
            value.dtype.type == numpy.bytes_ and  # string type?
            value.itemsize > 0 and value.endswith(b'.'))

        if (maybe_pickled and value in [b"0", b"0."]):
            # Workaround for a bug in many versions of Python (starting
            # somewhere after Python 2.6.1).  See ticket #253.
            retval = value
        elif (maybe_pickled and _field_fill_re.match(name)
              and format_version == (1, 5)):
            # This format was used during the first 1.2 releases, just
            # for string defaults.
            try:
                retval = cPickle.loads(value)
                retval = numpy.array(retval)
            except ImportError:
                retval = None  # signal error avoiding exception
        elif maybe_pickled and name == 'FILTERS' and format_version < (2, 0):
            # This is a big hack, but we don't have other way to recognize
            # pickled filters of PyTables 1.x files.
            value = _old_filters_re.sub(_new_filters_sub, value, 1)
            retval = cPickle.loads(value)  # pass unpickling errors through
        elif maybe_pickled:
            try:
                retval = cPickle.loads(value)
            # except cPickle.UnpicklingError:
            # It seems that pickle may raise other errors than UnpicklingError
            # Perhaps it would be better just an "except:" clause?
            # except (cPickle.UnpicklingError, ImportError):
            # Definitely (see SF bug #1254636)
            except:
                # ivb (2005-09-07): It is too hard to tell
                # whether the unpickling failed
                # because of the string not being a pickle one at all,
                # because of a malformed pickle string,
                # or because of some other problem in object reconstruction,
                # thus making inconvenient even the issuing of a warning here.
                # The documentation contains a note on this issue,
                # explaining how the user can tell where the problem was.
                retval = value
            # Additional check for allowing a workaround for #307
            if isinstance(retval, unicode) and retval == u'':
                retval = numpy.array(retval)[()]
        elif name == 'FILTERS' and format_version >= (2, 0):
            retval = Filters._unpack(value)
        elif (issysattrname(name) and isinstance(value, (bytes, unicode)) and
              not isinstance(value, str) and not _field_fill_re.match(name)):
            # system attributes should always be str
            if sys.version_info[0] < 3:
                retval = value.encode()
            else:
                # python 3, bytes and not "FIELD_[0-9]+_FILL"
                retval = value.decode('utf-8')
        else:
            retval = value

        # Put this value in local directory
        self.__dict__[name] = retval
        return retval

    def _g__setattr(self, name, value):
        """Set a PyTables attribute.

        Sets a (maybe new) PyTables attribute with the specified `name`
        and `value`.  If the attribute already exists, it is simply
        replaced.

        It does not log the change.

        """

        # Save this attribute to disk
        # (overwriting an existing one if needed)
        stvalue = value
        if issysattrname(name):
            if name in ["EXTDIM", "AUTO_INDEX", "DIRTY", "NODE_TYPE_VERSION"]:
                stvalue = numpy.array(value, dtype=numpy.int32)
                value = stvalue[()]
            elif name == "NROWS":
                stvalue = numpy.array(value, dtype=SizeType)
                value = stvalue[()]
            elif name == "FILTERS" and self._v__format_version >= (2, 0):
                stvalue = value._pack()
                # value will remain as a Filters instance here
        # Convert value from a Python scalar into a NumPy scalar
        # (only in case it has not been converted yet)
        # Fixes ticket #59
        if (stvalue is value and
                type(value) in (bool, bytes, int, float, complex, unicode,
                                numpy.unicode_)):
            # Additional check for allowing a workaround for #307
            if value == u'':
                stvalue = numpy.array(u'')
            else:
                stvalue = numpy.array(value)
            value = stvalue[()]

        self._g_setattr(self._v_node, name, stvalue)

        # New attribute or value. Introduce it into the local
        # directory
        self.__dict__[name] = value

        # Finally, add this attribute to the list if not present
        attrnames = self._v_attrnames
        if not name in attrnames:
            attrnames.append(name)
            attrnames.sort()
            if issysattrname(name):
                attrnamessys = self._v_attrnamessys
                attrnamessys.append(name)
                attrnamessys.sort()
            else:
                attrnamesuser = self._v_attrnamesuser
                attrnamesuser.append(name)
                attrnamesuser.sort()

    def __setattr__(self, name, value):
        """Set a PyTables attribute.

        Sets a (maybe new) PyTables attribute with the specified `name`
        and `value`.  If the attribute already exists, it is simply
        replaced.

        A ``ValueError`` is raised when the name starts with a reserved
        prefix or contains a ``/``.  A `NaturalNameWarning` is issued if
        the name is not a valid Python identifier.  A
        `PerformanceWarning` is issued when the recommended maximum
        number of attributes in a node is going to be exceeded.

        """

        nodeFile = self._v__nodefile
        attrnames = self._v_attrnames

        # Check for name validity
        check_name_validity(name)

        nodeFile._check_writable()

        # Check if there are too many attributes.
        maxNodeAttrs = nodeFile.params['MAX_NODE_ATTRS']
        if len(attrnames) >= maxNodeAttrs:
            warnings.warn("""\
node ``%s`` is exceeding the recommended maximum number of attributes (%d);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                          % (self._v__nodepath, maxNodeAttrs),
                          PerformanceWarning)

        undoEnabled = nodeFile.is_undo_enabled()
        # Log old attribute removal (if any).
        if undoEnabled and (name in attrnames):
            self._g_del_and_log(name)

        # Set the attribute.
        self._g__setattr(name, value)

        # Log new attribute addition.
        if undoEnabled:
            self._g_log_add(name)

    def _g_log_add(self, name):
        self._v__nodefile._log('ADDATTR', self._v__nodepath, name)

    _g_logAdd = previous_api(_g_log_add)

    def _g_del_and_log(self, name):
        nodeFile = self._v__nodefile
        nodePathname = self._v__nodepath
        # Log *before* moving to use the right shadow name.
        nodeFile._log('DELATTR', nodePathname, name)
        attr_to_shadow(nodeFile, nodePathname, name)

    _g_delAndLog = previous_api(_g_del_and_log)

    def _g__delattr(self, name):
        """Delete a PyTables attribute.

        Deletes the specified existing PyTables attribute.

        It does not log the change.

        """

        # Delete the attribute from disk
        self._g_remove(self._v_node, name)

        # Delete the attribute from local lists
        self._v_attrnames.remove(name)
        if name in self._v_attrnamessys:
            self._v_attrnamessys.remove(name)
        else:
            self._v_attrnamesuser.remove(name)

        # Delete the attribute from the local directory
        # closes (#1049285)
        del self.__dict__[name]

    def __delattr__(self, name):
        """Delete a PyTables attribute.

        Deletes the specified existing PyTables attribute from the
        attribute set.  If a nonexistent or system attribute is
        specified, an ``AttributeError`` is raised.

        """

        nodeFile = self._v__nodefile

        # Check if attribute exists
        if name not in self._v_attrnames:
            raise AttributeError(
                "Attribute ('%s') does not exist in node '%s'"
                % (name, self._v__nodepath))

        nodeFile._check_writable()

        # Remove the PyTables attribute or move it to shadow.
        if nodeFile.is_undo_enabled():
            self._g_del_and_log(name)
        else:
            self._g__delattr(name)

    def __getitem__(self, name):
        """The dictionary like interface for __getattr__()."""

        try:
            return self.__getattr__(name)
        except AttributeError:
            # Capture the AttributeError an re-raise a KeyError one
            raise KeyError(
                "Attribute ('%s') does not exist in node '%s'"
                % (name, self._v__nodepath))

    def __setitem__(self, name, value):
        """The dictionary like interface for __setattr__()."""

        self.__setattr__(name, value)

    def __delitem__(self, name):
        """The dictionary like interface for __delattr__()."""

        try:
            self.__delattr__(name)
        except AttributeError:
            # Capture the AttributeError an re-raise a KeyError one
            raise KeyError(
                "Attribute ('%s') does not exist in node '%s'"
                % (name, self._v__nodepath))

    def __contains__(self, name):
        """Is there an attribute with that name?

        A true value is returned if the attribute set has an attribute
        with the given name, false otherwise.

        """

        return name in self._v_attrnames

    def _f_rename(self, oldattrname, newattrname):
        """Rename an attribute from oldattrname to newattrname."""

        if oldattrname == newattrname:
            # Do nothing
            return

        # First, fetch the value of the oldattrname
        attrvalue = getattr(self, oldattrname)

        # Now, create the new attribute
        setattr(self, newattrname, attrvalue)

        # Finally, remove the old attribute
        delattr(self, oldattrname)

    def _g_copy(self, newset, set_attr=None, copyclass=False):
        """Copy set attributes.

        Copies all user and allowed system PyTables attributes to the
        given attribute set, replacing the existing ones.

        You can specify a *bound* method of the destination set that
        will be used to set its attributes.  Else, its `_g__setattr`
        method will be used.

        Changes are logged depending on the chosen setting method.  The
        default setting method does not log anything.

        .. versionchanged:: 3.0
           The *newSet* parameter has been renamed into *newset*.

        .. versionchanged:: 3.0
           The *copyClass* parameter has been renamed into *copyclass*.

        """

        copysysattrs = newset._v__nodefile.params['PYTABLES_SYS_ATTRS']
        if set_attr is None:
            set_attr = newset._g__setattr

        for attrname in self._v_attrnamesuser:
            # Do not copy the unimplemented attributes.
            if attrname not in self._v_unimplemented:
                set_attr(attrname, getattr(self, attrname))
        # Copy the system attributes that we are allowed to.
        if copysysattrs:
            for attrname in self._v_attrnamessys:
                if ((attrname not in SYS_ATTRS_NOTTOBECOPIED) and
                    # Do not copy the FIELD_ attributes in tables as this can
                    # be really *slow* (don't know exactly the reason).
                    # See #304.
                        not attrname.startswith("FIELD_")):
                    set_attr(attrname, getattr(self, attrname))
            # Copy CLASS and VERSION attributes if requested
            if copyclass:
                for attrname in FORCE_COPY_CLASS:
                    if attrname in self._v_attrnamessys:
                        set_attr(attrname, getattr(self, attrname))

    def _f_copy(self, where):
        """Copy attributes to the where node.

        Copies all user and certain system attributes to the given where
        node (a Node instance - see :ref:`NodeClassDescr`), replacing
        the existing ones.

        """

        # AttributeSet must be defined in order to define a Node.
        # However, we need to know Node here.
        # Using class_name_dict avoids a circular import.
        if not isinstance(where, class_name_dict['Node']):
            raise TypeError("destination object is not a node: %r" % (where,))
        self._g_copy(where._v_attrs, where._v_attrs.__setattr__)

    def _g_close(self):
        # Nothing will be done here, as the existing instance is completely
        # operative now.
        pass

    def __str__(self):
        """The string representation for this object."""

        # The pathname
        pathname = self._v__nodepath
        # Get this class name
        classname = self.__class__.__name__
        # The attribute names
        attrnumber = len([n for n in self._v_attrnames])
        return "%s._v_attrs (%s), %s attributes" % \
               (pathname, classname, attrnumber)

    def __repr__(self):
        """A detailed string representation for this object."""

        # print additional info only if there are attributes to show
        attrnames = [n for n in self._v_attrnames]
        if len(attrnames):
            rep = ['%s := %r' % (attr, getattr(self, attr))
                   for attr in attrnames]
            attrlist = '[%s]' % (',\n    '.join(rep))

            return "%s:\n   %s" % (str(self), attrlist)
        else:
            return str(self)


class NotLoggedAttributeSet(AttributeSet):
    def _g_log_add(self, name):
        pass

    _g_logAdd = previous_api(_g_log_add)

    def _g_del_and_log(self, name):
        self._g__delattr(name)

    _g_delAndLog = previous_api(_g_del_and_log)

## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = carray
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: June 15, 2005
# Author: Antonio Valentino
# Modified by: Francesc Alted
#
# $Id$
#
########################################################################

"""Here is defined the CArray class."""

import sys

import numpy

from tables.atom import Atom
from tables.array import Array
from tables.utils import correct_byteorder, SizeType

from tables._past import previous_api, previous_api_property

# default version for CARRAY objects
# obversion = "1.0"    # Support for time & enumerated datatypes.
obversion = "1.1"    # Numeric and numarray flavors are gone.


class CArray(Array):
    """This class represents homogeneous datasets in an HDF5 file.

    The difference between a CArray and a normal Array (see
    :ref:`ArrayClassDescr`), from which it inherits, is that a CArray
    has a chunked layout and, as a consequence, it supports compression.
    You can use datasets of this class to easily save or load arrays to
    or from disk, with compression support included.

    CArray includes all the instance variables and methods of Array.
    Only those with different behavior are mentioned here.

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.
    atom
       An `Atom` instance representing the *type* and *shape* of
       the atomic objects to be saved.

    shape
       The shape of the new array.

    title
       A description for this node (it sets the ``TITLE`` HDF5
       attribute on disk).

    filters
       An instance of the `Filters` class that provides
       information about the desired I/O filters to be applied
       during the life of this object.

    chunkshape
       The shape of the data chunk to be read or written in a
       single HDF5 I/O operation.  Filters are applied to those
       chunks of data.  The dimensionality of `chunkshape` must
       be the same as that of `shape`.  If ``None``, a sensible
       value is calculated (which is recommended).

    byteorder
        The byteorder of the data *on disk*, specified as 'little'
        or 'big'.  If this is not specified, the byteorder is that
        of the platform.

    Examples
    --------

    See below a small example of the use of the `CArray` class.
    The code is available in ``examples/carray1.py``::

        import numpy
        import tables

        fileName = 'carray1.h5'
        shape = (200, 300)
        atom = tables.UInt8Atom()
        filters = tables.Filters(complevel=5, complib='zlib')

        h5f = tables.open_file(fileName, 'w')
        ca = h5f.create_carray(h5f.root, 'carray', atom, shape,
                               filters=filters)

        # Fill a hyperslab in ``ca``.
        ca[10:60, 20:70] = numpy.ones((50, 50))
        h5f.close()

        # Re-open a read another hyperslab
        h5f = tables.open_file(fileName)
        print(h5f)
        print(h5f.root.carray[8:12, 18:22])
        h5f.close()

    The output for the previous script is something like::

        carray1.h5 (File) ''
        Last modif.: 'Thu Apr 12 10:15:38 2007'
        Object Tree:
        / (RootGroup) ''
        /carray (CArray(200, 300), shuffle, zlib(5)) ''

        [[0 0 0 0]
         [0 0 0 0]
         [0 0 1 1]
         [0 0 1 1]]

    """

    # Class identifier.
    _c_classid = 'CARRAY'

    _c_classId = previous_api_property('_c_classid')

    # Properties
    # ~~~~~~~~~~
    # Special methods
    # ~~~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 atom=None, shape=None,
                 title="", filters=None,
                 chunkshape=None, byteorder=None,
                 _log=True):

        self.atom = atom
        """An `Atom` instance representing the shape, type of the atomic
        objects to be saved.
        """
        self.shape = None
        """The shape of the stored array."""
        self.extdim = -1  # `CArray` objects are not enlargeable by default
        """The index of the enlargeable dimension."""

        # Other private attributes
        self._v_version = None
        """The object version of this array."""
        self._v_new = new = atom is not None
        """Is this the first time the node has been created?"""
        self._v_new_title = title
        """New title for this node."""
        self._v_convert = True
        """Whether the ``Array`` object must be converted or not."""
        self._v_chunkshape = chunkshape
        """Private storage for the `chunkshape` property of the leaf."""

        # Miscellaneous iteration rubbish.
        self._start = None
        """Starting row for the current iteration."""
        self._stop = None
        """Stopping row for the current iteration."""
        self._step = None
        """Step size for the current iteration."""
        self._nrowsread = None
        """Number of rows read up to the current state of iteration."""
        self._startb = None
        """Starting row for current buffer."""
        self._stopb = None
        """Stopping row for current buffer. """
        self._row = None
        """Current row in iterators (sentinel)."""
        self._init = False
        """Whether we are in the middle of an iteration or not (sentinel)."""
        self.listarr = None
        """Current buffer in iterators."""

        if new:
            if not isinstance(atom, Atom):
                raise ValueError("atom parameter should be an instance of "
                                 "tables.Atom and you passed a %s." %
                                 type(atom))
            if shape is None:
                raise ValueError("you must specify a non-empty shape")
            try:
                shape = tuple(shape)
            except TypeError:
                raise TypeError("`shape` parameter must be a sequence "
                                "and you passed a %s" % type(shape))
            self.shape = tuple(SizeType(s) for s in shape)

            if chunkshape is not None:
                try:
                    chunkshape = tuple(chunkshape)
                except TypeError:
                    raise TypeError(
                        "`chunkshape` parameter must be a sequence "
                        "and you passed a %s" % type(chunkshape))
                if len(shape) != len(chunkshape):
                    raise ValueError("the shape (%s) and chunkshape (%s) "
                                     "ranks must be equal." %
                                    (shape, chunkshape))
                elif min(chunkshape) < 1:
                    raise ValueError("chunkshape parameter cannot have "
                                     "zero-dimensions.")
                self._v_chunkshape = tuple(SizeType(s) for s in chunkshape)

        # The `Array` class is not abstract enough! :(
        super(Array, self).__init__(parentnode, name, new, filters,
                                    byteorder, _log)

    def _g_create(self):
        """Create a new array in file (specific part)."""

        if min(self.shape) < 1:
            raise ValueError(
                "shape parameter cannot have zero-dimensions.")
        # Finish the common part of creation process
        return self._g_create_common(self.nrows)

    def _g_create_common(self, expectedrows):
        """Create a new array in file (common part)."""

        self._v_version = obversion

        if self._v_chunkshape is None:
            # Compute the optimal chunk size
            self._v_chunkshape = self._calc_chunkshape(
                expectedrows, self.rowsize, self.atom.size)
        # Compute the optimal nrowsinbuf
        self.nrowsinbuf = self._calc_nrowsinbuf()
        # Correct the byteorder if needed
        if self.byteorder is None:
            self.byteorder = correct_byteorder(self.atom.type, sys.byteorder)

        try:
            # ``self._v_objectid`` needs to be set because would be
            # needed for setting attributes in some descendants later
            # on
            self._v_objectid = self._create_carray(self._v_new_title)
        except:  # XXX
            # Problems creating the Array on disk. Close node and re-raise.
            self.close(flush=0)
            raise

        return self._v_objectid

    def _g_copy_with_stats(self, group, name, start, stop, step,
                           title, filters, chunkshape, _log, **kwargs):
        """Private part of Leaf.copy() for each kind of leaf."""

        (start, stop, step) = self._process_range_read(start, stop, step)
        maindim = self.maindim
        shape = list(self.shape)
        shape[maindim] = len(xrange(start, stop, step))
        # Now, fill the new carray with values from source
        nrowsinbuf = self.nrowsinbuf
        # The slices parameter for self.__getitem__
        slices = [slice(0, dim, 1) for dim in self.shape]
        # This is a hack to prevent doing unnecessary conversions
        # when copying buffers
        self._v_convert = False
        # Build the new CArray object
        object = CArray(group, name, atom=self.atom, shape=shape,
                        title=title, filters=filters, chunkshape=chunkshape,
                        _log=_log)
        # Start the copy itself
        for start2 in xrange(start, stop, step * nrowsinbuf):
            # Save the records on disk
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            # Set the proper slice in the main dimension
            slices[maindim] = slice(start2, stop2, step)
            start3 = (start2 - start) // step
            stop3 = start3 + nrowsinbuf
            if stop3 > shape[maindim]:
                stop3 = shape[maindim]
            # The next line should be generalised if, in the future,
            # maindim is designed to be different from 0 in CArrays.
            # See ticket #199.
            object[start3:stop3] = self.__getitem__(tuple(slices))
        # Activate the conversion again (default)
        self._v_convert = True
        nbytes = numpy.prod(self.shape, dtype=SizeType) * self.atom.size

        return (object, nbytes)

    _g_copyWithStats = previous_api(_g_copy_with_stats)

########NEW FILE########
__FILENAME__ = conditions
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2006-09-19
# Author:  Ivan Vilata i Balaguer -- ivan@selidor.net
# :Notes:  Heavily modified by Francesc Alted for multi-index support.
#          2008-04-09
#          Combined common & pro version.
#          2011-06-04
#
# $Id$
#
########################################################################

"""Utility functions and classes for supporting query conditions.

Classes:

`CompileCondition`
    Container for a compiled condition.

Functions:

`compile_condition`
    Compile a condition and extract usable index conditions.
`call_on_recarr`
    Evaluate a function over a structured array.

"""

import re
from numexpr.necompiler import typecode_to_kind
from numexpr.necompiler import expressionToAST, typeCompileAst
from numexpr.necompiler import stringToExpression, NumExpr
from numexpr.expressions import ExpressionNode
from tables.utilsextension import get_nested_field
from tables.utils import lazyattr

_no_matching_opcode = re.compile(r"[^a-z]([a-z]+)_([a-z]+)[^a-z]")
# E.g. "gt" and "bfc" from "couldn't find matching opcode for 'gt_bfc'".


def _unsupported_operation_error(exception):
    """Make the \"no matching opcode\" Numexpr `exception` more clear.

    A new exception of the same kind is returned.

    """

    message = exception.args[0]
    op, types = _no_matching_opcode.search(message).groups()
    newmessage = "unsupported operand types for *%s*: " % op
    newmessage += ', '.join([typecode_to_kind[t] for t in types[1:]])
    return exception.__class__(newmessage)


def _check_indexable_cmp(getidxcmp):
    """Decorate `getidxcmp` to check the returned indexable comparison.

    This does some extra checking that Numexpr would perform later on
    the comparison if it was compiled within a complete condition.

    """

    def newfunc(exprnode, indexedcols):
        result = getidxcmp(exprnode, indexedcols)
        if result[0] is not None:
            try:
                typeCompileAst(expressionToAST(exprnode))
            except NotImplementedError as nie:
                # Try to make this Numexpr error less cryptic.
                raise _unsupported_operation_error(nie)
        return result
    newfunc.__name__ = getidxcmp.__name__
    newfunc.__doc__ = getidxcmp.__doc__
    return newfunc


@_check_indexable_cmp
def _get_indexable_cmp(exprnode, indexedcols):
    """Get the indexable variable-constant comparison in `exprnode`.

    A tuple of (variable, operation, constant) is returned if
    `exprnode` is a variable-constant (or constant-variable)
    comparison, and the variable is in `indexedcols`.  A normal
    variable can also be used instead of a constant: a tuple with its
    name will appear instead of its value.

    Otherwise, the values in the tuple are ``None``.
    """

    not_indexable = (None, None, None)
    turncmp = {'lt': 'gt',
               'le': 'ge',
               'eq': 'eq',
               'ge': 'le',
               'gt': 'lt', }

    def get_cmp(var, const, op):
        var_value, const_value = var.value, const.value
        if (var.astType == 'variable' and var_value in indexedcols
           and const.astType in ['constant', 'variable']):
            if const.astType == 'variable':
                const_value = (const_value, )
            return (var_value, op, const_value)
        return None

    def is_indexed_boolean(node):
        return (node.astType == 'variable'
                and node.astKind == 'bool'
                and node.value in indexedcols)

    # Boolean variables are indexable by themselves.
    if is_indexed_boolean(exprnode):
        return (exprnode.value, 'eq', True)
    # And so are negations of boolean variables.
    if exprnode.astType == 'op' and exprnode.value == 'invert':
        child = exprnode.children[0]
        if is_indexed_boolean(child):
            return (child.value, 'eq', False)
        # A negation of an expression will be returned as ``~child``.
        # The indexability of the negated expression will be decided later on.
        if child.astKind == "bool":
            return (child, 'invert', None)

    # Check node type.  Only comparisons are indexable from now on.
    if exprnode.astType != 'op':
        return not_indexable
    cmpop = exprnode.value
    if cmpop not in turncmp:
        return not_indexable

    # Look for a variable-constant comparison in both directions.
    left, right = exprnode.children
    cmp_ = get_cmp(left, right, cmpop)
    if cmp_:
        return cmp_
    cmp_ = get_cmp(right, left, turncmp[cmpop])
    if cmp_:
        return cmp_

    return not_indexable


def _equiv_expr_node(x, y):
    """Returns whether two ExpressionNodes are equivalent.

    This is needed because '==' is overridden on ExpressionNode to
    return a new ExpressionNode.

    """
    if not isinstance(x, ExpressionNode) and not isinstance(y, ExpressionNode):
        return x == y
    elif (type(x) is not type(y) or not isinstance(x, ExpressionNode)
            or not isinstance(y, ExpressionNode)
            or x.value != y.value or x.astKind != y.astKind
            or len(x.children) != len(y.children)):
        return False
    for xchild, ychild in zip(x.children, y.children):
        if not _equiv_expr_node(xchild, ychild):
            return False
    return True


def _get_idx_expr_recurse(exprnode, indexedcols, idxexprs, strexpr):
    """Here lives the actual implementation of the get_idx_expr() wrapper.

    'idxexprs' is a list of expressions in the form ``(var, (ops),
    (limits))``. 'strexpr' is the indexable expression in string format.
    These parameters will be received empty (i.e. [], ['']) for the
    first time and populated during the different recursive calls.
    Finally, they are returned in the last level to the original
    wrapper.  If 'exprnode' is not indexable, it will return the tuple
    ([], ['']) so as to signal this.

    """

    not_indexable = ([], [''])
    op_conv = {
        'and': '&',
        'or': '|',
        'not': '~',
    }
    negcmp = {
        'lt': 'ge',
        'le': 'gt',
        'ge': 'lt',
        'gt': 'le',
    }

    def fix_invert(idxcmp, exprnode, indexedcols):
        invert = False
        # Loop until all leading negations have been dealt with
        while idxcmp[1] == "invert":
            invert ^= True
            # The information about the negated node is in first position
            exprnode = idxcmp[0]
            idxcmp = _get_indexable_cmp(exprnode, indexedcols)
        return idxcmp, exprnode, invert

    # Indexable variable-constant comparison.
    idxcmp = _get_indexable_cmp(exprnode, indexedcols)
    idxcmp, exprnode, invert = fix_invert(idxcmp, exprnode, indexedcols)
    if idxcmp[0]:
        if invert:
            var, op, value = idxcmp
            if op == 'eq' and value in [True, False]:
                # ``var`` must be a boolean index.  Flip its value.
                value ^= True
            else:
                op = negcmp[op]
            expr = (var, (op,), (value,))
            invert = False
        else:
            expr = (idxcmp[0], (idxcmp[1],), (idxcmp[2],))
        return [expr]

    # For now negations of complex expressions will be not supported as
    # forming part of an indexable condition.  This might be supported in
    # the future.
    if invert:
        return not_indexable

    # Only conjunctions and disjunctions of comparisons are considered
    # for the moment.
    if exprnode.astType != 'op' or exprnode.value not in ['and', 'or']:
        return not_indexable

    left, right = exprnode.children
    # Get the expression at left
    lcolvar, lop, llim = _get_indexable_cmp(left, indexedcols)
    # Get the expression at right
    rcolvar, rop, rlim = _get_indexable_cmp(right, indexedcols)

    # Use conjunction of indexable VC comparisons like
    # ``(a <[=] x) & (x <[=] b)`` or ``(a >[=] x) & (x >[=] b)``
    # as ``a <[=] x <[=] b``, for the moment.
    op = exprnode.value
    if (lcolvar is not None and rcolvar is not None
            and _equiv_expr_node(lcolvar, rcolvar) and op == 'and'):
        if lop in ['gt', 'ge'] and rop in ['lt', 'le']:  # l <= x <= r
            expr = (lcolvar, (lop, rop), (llim, rlim))
            return [expr]
        if lop in ['lt', 'le'] and rop in ['gt', 'ge']:  # l >= x >= r
            expr = (rcolvar, (rop, lop), (rlim, llim))
            return [expr]

    # Recursively get the expressions at the left and the right
    lexpr = _get_idx_expr_recurse(left, indexedcols, idxexprs, strexpr)
    rexpr = _get_idx_expr_recurse(right, indexedcols, idxexprs, strexpr)

    def add_expr(expr, idxexprs, strexpr):
        """Add a single expression to the list."""

        if isinstance(expr, list):
            # expr is a single expression
            idxexprs.append(expr[0])
            lenexprs = len(idxexprs)
            # Mutate the strexpr string
            if lenexprs == 1:
                strexpr[:] = ["e0"]
            else:
                strexpr[:] = [
                    "(%s %s e%d)" % (strexpr[0], op_conv[op], lenexprs - 1)]

    # Add expressions to the indexable list when they are and'ed, or
    # they are both indexable.
    if lexpr != not_indexable and (op == "and" or rexpr != not_indexable):
        add_expr(lexpr, idxexprs, strexpr)
        if rexpr != not_indexable:
            add_expr(rexpr, idxexprs, strexpr)
        return (idxexprs, strexpr)
    if rexpr != not_indexable and op == "and":
        add_expr(rexpr, idxexprs, strexpr)
        return (idxexprs, strexpr)

    # Can not use indexed column.
    return not_indexable


def _get_idx_expr(expr, indexedcols):
    """Extract an indexable expression out of `exprnode`.

    Looks for variable-constant comparisons in the expression node
    `exprnode` involving variables in `indexedcols`.

    It returns a tuple of (idxexprs, strexpr) where 'idxexprs' is a
    list of expressions in the form ``(var, (ops), (limits))`` and
    'strexpr' is the indexable expression in string format.

    Expressions such as ``0 < c1 <= 1`` do not work as expected.

    Right now only some of the *indexable comparisons* are considered:

    * ``a <[=] x``, ``a == x`` and ``a >[=] x``
    * ``(a <[=] x) & (y <[=] b)`` and ``(a == x) | (b == y)``
    * ``~(~c_bool)``, ``~~c_bool`` and ``~(~c_bool) & (c_extra != 2)``

    (where ``a``, ``b`` and ``c_bool`` are indexed columns, but
    ``c_extra`` is not)

    Particularly, the ``!=`` operator and negations of complex boolean
    expressions are *not considered* as valid candidates:

    * ``a != 1`` and  ``c_bool != False``
    * ``~((a > 0) & (c_bool))``

    """

    return _get_idx_expr_recurse(expr, indexedcols, [], [''])


class CompiledCondition(object):
    """Container for a compiled condition."""

    # Lazy attributes
    # ```````````````
    @lazyattr
    def index_variables(self):
        """The columns participating in the index expression."""

        idxexprs = self.index_expressions
        idxvars = []
        for expr in idxexprs:
            idxvar = expr[0]
            if idxvar not in idxvars:
                idxvars.append(idxvar)
        return frozenset(idxvars)

    def __init__(self, func, params, idxexprs, strexpr):
        self.function = func
        """The compiled function object corresponding to this condition."""
        self.parameters = params
        """A list of parameter names for this condition."""
        self.index_expressions = idxexprs
        """A list of expressions in the form ``(var, (ops), (limits))``."""
        self.string_expression = strexpr
        """The indexable expression in string format."""

    def __repr__(self):
        return ("idxexprs: %s\nstrexpr: %s\nidxvars: %s"
                % (self.index_expressions, self.string_expression,
                   self.index_variables))

    def with_replaced_vars(self, condvars):
        """Replace index limit variables with their values in-place.

        A new compiled condition is returned.  Values are taken from
        the `condvars` mapping and converted to Python scalars.
        """

        exprs = self.index_expressions
        exprs2 = []
        for expr in exprs:
            idxlims = expr[2]  # the limits are in third place
            limit_values = []
            for idxlim in idxlims:
                if isinstance(idxlim, tuple):  # variable
                    idxlim = condvars[idxlim[0]]  # look up value
                    idxlim = idxlim.tolist()  # convert back to Python
                limit_values.append(idxlim)
            # Add this replaced entry to the new exprs2
            var, ops, _ = expr
            exprs2.append((var, ops, tuple(limit_values)))
        # Create a new container for the converted values
        newcc = CompiledCondition(
            self.function, self.parameters, exprs2, self.string_expression)
        return newcc


def _get_variable_names(expression):
    """Return the list of variable names in the Numexpr `expression`."""

    names = []
    stack = [expression]
    while stack:
        node = stack.pop()
        if node.astType == 'variable':
            names.append(node.value)
        elif hasattr(node, 'children'):
            stack.extend(node.children)
    return list(set(names))  # remove repeated names


def compile_condition(condition, typemap, indexedcols):
    """Compile a condition and extract usable index conditions.

    Looks for variable-constant comparisons in the `condition` string
    involving the indexed columns whose variable names appear in
    `indexedcols`.  The part of `condition` having usable indexes is
    returned as a compiled condition in a `CompiledCondition` container.

    Expressions such as '0 < c1 <= 1' do not work as expected.  The
    Numexpr types of *all* variables must be given in the `typemap`
    mapping.  The ``function`` of the resulting `CompiledCondition`
    instance is a Numexpr function object, and the ``parameters`` list
    indicates the order of its parameters.

    """

    # Get the expression tree and extract index conditions.
    expr = stringToExpression(condition, typemap, {})
    if expr.astKind != 'bool':
        raise TypeError("condition ``%s`` does not have a boolean type"
                        % condition)
    idxexprs = _get_idx_expr(expr, indexedcols)
    # Post-process the answer
    if isinstance(idxexprs, list):
        # Simple expression
        strexpr = ['e0']
    else:
        # Complex expression
        idxexprs, strexpr = idxexprs
    # Get rid of the unneccessary list wrapper for strexpr
    strexpr = strexpr[0]

    # Get the variable names used in the condition.
    # At the same time, build its signature.
    varnames = _get_variable_names(expr)
    signature = [(var, typemap[var]) for var in varnames]
    try:
        # See the comments in `numexpr.evaluate()` for the
        # reasons of inserting copy operators for unaligned,
        # *unidimensional* arrays.
        func = NumExpr(expr, signature)
    except NotImplementedError as nie:
        # Try to make this Numexpr error less cryptic.
        raise _unsupported_operation_error(nie)
    params = varnames

    # This is more comfortable to handle about than a tuple.
    return CompiledCondition(func, params, idxexprs, strexpr)


def call_on_recarr(func, params, recarr, param2arg=None):
    """Call `func` with `params` over `recarr`.

    The `param2arg` function, when specified, is used to get an argument
    given a parameter name; otherwise, the parameter itself is used as
    an argument.  When the argument is a `Column` object, the proper
    column from `recarr` is used as its value.

    """

    args = []
    for param in params:
        if param2arg:
            arg = param2arg(param)
        else:
            arg = param
        if hasattr(arg, 'pathname'):  # looks like a column
            arg = get_nested_field(recarr, arg.pathname)
        args.append(arg)
    return func(*args)

########NEW FILE########
__FILENAME__ = description
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: September 21, 2002
# Author: Francesc Alted
#
# $Id$
#
########################################################################

"""Classes for describing columns for ``Table`` objects."""

# Imports
# =======
from __future__ import print_function
import sys
import copy
import warnings

import numpy

from tables import atom
from tables.path import check_name_validity

from tables._past import previous_api, previous_api_property

# Public variables
# ================
__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


# Private functions
# =================
def same_position(oldmethod):
    """Decorate `oldmethod` to also compare the `_v_pos` attribute."""
    def newmethod(self, other):
        try:
            other._v_pos
        except AttributeError:
            return False  # not a column definition
        return self._v_pos == other._v_pos and oldmethod(self, other)
    newmethod.__name__ = oldmethod.__name__
    newmethod.__doc__ = oldmethod.__doc__
    return newmethod


# Column classes
# ==============
class Col(atom.Atom):
    """Defines a non-nested column.

    Col instances are used as a means to declare the different properties of a
    non-nested column in a table or nested column.  Col classes are descendants
    of their equivalent Atom classes (see :ref:`AtomClassDescr`), but their
    instances have an additional _v_pos attribute that is used to decide the
    position of the column inside its parent table or nested column (see the
    IsDescription class in :ref:`IsDescriptionClassDescr` for more information
    on column positions).

    In the same fashion as Atom, you should use a particular Col descendant
    class whenever you know the exact type you will need when writing your
    code. Otherwise, you may use one of the Col.from_*() factory methods.

    Each factory method inherited from the Atom class is available with the
    same signature, plus an additional pos parameter (placed in last position)
    which defaults to None and that may take an integer value.  This parameter
    might be used to specify the position of the column in the table.

    Besides, there are the next additional factory methods, available only for
    Col objects.

    The following parameters are available for most Col-derived constructors.

    Parameters
    ----------
    itemsize : int
        For types with a non-fixed size, this sets the size in bytes of
        individual items in the column.
    shape : tuple
        Sets the shape of the column. An integer shape of N is equivalent to
        the tuple (N,).
    dflt
        Sets the default value for the column.
    pos : int
        Sets the position of column in table.  If unspecified, the position
        will be randomly selected.

    """

    # Avoid mangling atom class data.
    __metaclass__ = type

    _class_from_prefix = {}  # filled as column classes are created
    """Maps column prefixes to column classes."""

    # Class methods
    # ~~~~~~~~~~~~~
    @classmethod
    def prefix(class_):
        """Return the column class prefix."""

        cname = class_.__name__
        return cname[:cname.rfind('Col')]

    @classmethod
    def from_atom(class_, atom, pos=None):
        """Create a Col definition from a PyTables atom.

        An optional position may be specified as the pos argument.

        """

        prefix = atom.prefix()
        kwargs = atom._get_init_args()
        colclass = class_._class_from_prefix[prefix]
        return colclass(pos=pos, **kwargs)

    @classmethod
    def from_sctype(class_, sctype, shape=(), dflt=None, pos=None):
        """Create a `Col` definition from a NumPy scalar type `sctype`.

        Optional shape, default value and position may be specified as
        the `shape`, `dflt` and `pos` arguments, respectively.
        Information in the `sctype` not represented in a `Col` is
        ignored.

        """

        newatom = atom.Atom.from_sctype(sctype, shape, dflt)
        return class_.from_atom(newatom, pos=pos)

    @classmethod
    def from_dtype(class_, dtype, dflt=None, pos=None):
        """Create a `Col` definition from a NumPy `dtype`.

        Optional default value and position may be specified as the
        `dflt` and `pos` arguments, respectively.  The `dtype` must have
        a byte order which is irrelevant or compatible with that of the
        system.  Information in the `dtype` not represented in a `Col`
        is ignored.

        """

        newatom = atom.Atom.from_dtype(dtype, dflt)
        return class_.from_atom(newatom, pos=pos)

    @classmethod
    def from_type(class_, type, shape=(), dflt=None, pos=None):
        """Create a `Col` definition from a PyTables `type`.

        Optional shape, default value and position may be specified as
        the `shape`, `dflt` and `pos` arguments, respectively.

        """

        newatom = atom.Atom.from_type(type, shape, dflt)
        return class_.from_atom(newatom, pos=pos)

    @classmethod
    def from_kind(class_, kind, itemsize=None, shape=(), dflt=None, pos=None):
        """Create a `Col` definition from a PyTables `kind`.

        Optional item size, shape, default value and position may be
        specified as the `itemsize`, `shape`, `dflt` and `pos`
        arguments, respectively.  Bear in mind that not all columns
        support a default item size.

        """

        newatom = atom.Atom.from_kind(kind, itemsize, shape, dflt)
        return class_.from_atom(newatom, pos=pos)

    @classmethod
    def _subclass_from_prefix(class_, prefix):
        """Get a column subclass for the given `prefix`."""

        cname = '%sCol' % prefix
        class_from_prefix = class_._class_from_prefix
        if cname in class_from_prefix:
            return class_from_prefix[cname]
        atombase = getattr(atom, '%sAtom' % prefix)

        class NewCol(class_, atombase):
            """Defines a non-nested column of a particular type.

            The constructor accepts the same arguments as the equivalent
            `Atom` class, plus an additional ``pos`` argument for
            position information, which is assigned to the `_v_pos`
            attribute.

            """

            def __init__(self, *args, **kwargs):
                pos = kwargs.pop('pos', None)
                class_from_prefix = self._class_from_prefix
                atombase.__init__(self, *args, **kwargs)
                # The constructor of an abstract atom may have changed
                # the class of `self` to something different of `NewCol`
                # and `atombase` (that's why the prefix map is saved).
                if self.__class__ is not NewCol:
                    colclass = class_from_prefix[self.prefix()]
                    self.__class__ = colclass
                self._v_pos = pos

            __eq__ = same_position(atombase.__eq__)
            _is_equal_to_atom = same_position(atombase._is_equal_to_atom)

            # XXX: API incompatible change for PyTables 3 line
            # Overriding __eq__ blocks inheritance of __hash__ in 3.x
            # def __hash__(self):
            #    return hash((self._v_pos, self.atombase))

            if prefix == 'Enum':
                _is_equal_to_enumatom = same_position(
                    atombase._is_equal_to_enumatom)

        NewCol.__name__ = cname

        class_from_prefix[prefix] = NewCol
        return NewCol

    # Special methods
    # ~~~~~~~~~~~~~~~
    def __repr__(self):
        # Reuse the atom representation.
        atomrepr = super(Col, self).__repr__()
        lpar = atomrepr.index('(')
        rpar = atomrepr.rindex(')')
        atomargs = atomrepr[lpar + 1:rpar]
        classname = self.__class__.__name__
        return '%s(%s, pos=%s)' % (classname, atomargs, self._v_pos)

    # Private methods
    # ~~~~~~~~~~~~~~~
    def _get_init_args(self):
        """Get a dictionary of instance constructor arguments."""

        kwargs = dict((arg, getattr(self, arg)) for arg in ('shape', 'dflt'))
        kwargs['pos'] = getattr(self, '_v_pos', None)
        return kwargs


def _generate_col_classes():
    """Generate all column classes."""

    # Abstract classes are not in the class map.
    cprefixes = ['Int', 'UInt', 'Float', 'Time']
    for (kind, kdata) in atom.atom_map.iteritems():
        if hasattr(kdata, 'kind'):  # atom class: non-fixed item size
            atomclass = kdata
            cprefixes.append(atomclass.prefix())
        else:  # dictionary: fixed item size
            for atomclass in kdata.itervalues():
                cprefixes.append(atomclass.prefix())

    # Bottom-level complex classes are not in the type map, of course.
    # We still want the user to get the compatibility warning, though.
    cprefixes.extend(['Complex32', 'Complex64', 'Complex128'])
    if hasattr(atom, 'Complex192Atom'):
        cprefixes.append('Complex192')
    if hasattr(atom, 'Complex256Atom'):
        cprefixes.append('Complex256')

    for cprefix in cprefixes:
        newclass = Col._subclass_from_prefix(cprefix)
        yield newclass

# Create all column classes.
for _newclass in _generate_col_classes():
    exec('%s = _newclass' % _newclass.__name__)
del _newclass


# Table description classes
# =========================
class Description(object):
    """This class represents descriptions of the structure of tables.

    An instance of this class is automatically bound to Table (see
    :ref:`TableClassDescr`) objects when they are created.  It provides a
    browseable representation of the structure of the table, made of non-nested
    (Col - see :ref:`ColClassDescr`) and nested (Description) columns.

    Column definitions under a description can be accessed as attributes of it
    (*natural naming*). For instance, if table.description is a Description
    instance with a column named col1 under it, the later can be accessed as
    table.description.col1. If col1 is nested and contains a col2 column, this
    can be accessed as table.description.col1.col2. Because of natural naming,
    the names of members start with special prefixes, like in the Group class
    (see :ref:`GroupClassDescr`).


    .. rubric:: Description attributes

    .. attribute:: _v_colobjects

        A dictionary mapping the names of the columns hanging
        directly from the associated table or nested column to their
        respective descriptions (Col - see :ref:`ColClassDescr` or
        Description - see :ref:`DescriptionClassDescr` instances).

        .. versionchanged:: 3.0
           The *_v_colObjects* attobute has been renamed into
           *_v_colobjects*.

    .. attribute:: _v_dflts

        A dictionary mapping the names of non-nested columns
        hanging directly from the associated table or nested column
        to their respective default values.

    .. attribute:: _v_dtype

        The NumPy type which reflects the structure of this
        table or nested column.  You can use this as the
        dtype argument of NumPy array factories.

    .. attribute:: _v_dtypes

        A dictionary mapping the names of non-nested columns
        hanging directly from the associated table or nested column
        to their respective NumPy types.

    .. attribute:: _v_is_nested

        Whether the associated table or nested column contains
        further nested columns or not.

    .. attribute:: _v_itemsize

        The size in bytes of an item in this table or nested column.

    .. attribute:: _v_name

        The name of this description group. The name of the
        root group is '/'.

    .. attribute:: _v_names

        A list of the names of the columns hanging directly
        from the associated table or nested column. The order of the
        names matches the order of their respective columns in the
        containing table.

    .. attribute:: _v_nested_descr

        A nested list of pairs of (name, format) tuples for all the columns
        under this table or nested column. You can use this as the dtype and
        descr arguments of NumPy array factories.

        .. versionchanged:: 3.0
           The *_v_nestedDescr* attribute has been renamed into
           *_v_nested_descr*.

    .. attribute:: _v_nested_formats

        A nested list of the NumPy string formats (and shapes) of all the
        columns under this table or nested column. You can use this as the
        formats argument of NumPy array factories.

        .. versionchanged:: 3.0
           The *_v_nestedFormats* attribute has been renamed into
           *_v_nested_formats*.

    .. attribute:: _v_nestedlvl

        The level of the associated table or nested column in the nested
        datatype.

    .. attribute:: _v_nested_names

        A nested list of the names of all the columns under this table or
        nested column. You can use this as the names argument of NumPy array
        factories.

        .. versionchanged:: 3.0
           The *_v_nestedNames* attribute has been renamed into
           *_v_nested_names*.

    .. attribute:: _v_pathname

        Pathname of the table or nested column.

    .. attribute:: _v_pathnames

        A list of the pathnames of all the columns under this table or nested
        column (in preorder).  If it does not contain nested columns, this is
        exactly the same as the :attr:`Description._v_names` attribute.

    .. attribute:: _v_types

        A dictionary mapping the names of non-nested columns hanging directly
        from the associated table or nested column to their respective PyTables
        types.

    """

    _v_colObjects = previous_api_property('_v_colobjects')
    _v_nestedFormats = previous_api_property('_v_nested_formats')
    _v_nestedNames = previous_api_property('_v_nested_names')
    _v_nestedDesct = previous_api_property('_v_nested_descr')

    def __init__(self, classdict, nestedlvl=-1, validate=True):

        if not classdict:
            raise ValueError("cannot create an empty data type")

        # Do a shallow copy of classdict just in case this is going to
        # be shared by other instances
        newdict = self.__dict__
        newdict["_v_name"] = "/"   # The name for root descriptor
        newdict["_v_names"] = []
        newdict["_v_dtypes"] = {}
        newdict["_v_types"] = {}
        newdict["_v_dflts"] = {}
        newdict["_v_colobjects"] = {}
        newdict["_v_is_nested"] = False
        nestedFormats = []
        nestedDType = []

        if not hasattr(newdict, "_v_nestedlvl"):
            newdict["_v_nestedlvl"] = nestedlvl + 1

        cols_with_pos = []  # colum (position, name) pairs
        cols_no_pos = []  # just column names

        # Check for special variables and convert column descriptions
        for (name, descr) in classdict.iteritems():
            if name.startswith('_v_'):
                if name in newdict:
                    # print("Warning!")
                    # special methods &c: copy to newdict, warn about conflicts
                    warnings.warn("Can't set attr %r in description class %r"
                                  % (name, self))
                else:
                    # print("Special variable!-->", name, classdict[name])
                    newdict[name] = descr
                continue  # This variable is not needed anymore

            columns = None
            if (type(descr) == type(IsDescription) and
                    issubclass(descr, IsDescription)):
                # print("Nested object (type I)-->", name)
                columns = descr().columns
            elif (type(descr.__class__) == type(IsDescription) and
                  issubclass(descr.__class__, IsDescription)):
                # print("Nested object (type II)-->", name)
                columns = descr.columns
            elif isinstance(descr, dict):
                # print("Nested object (type III)-->", name)
                columns = descr
            else:
                # print("Nested object (type IV)-->", name)
                descr = copy.copy(descr)
            # The copies above and below ensure that the structures
            # provided by the user will remain unchanged even if we
            # tamper with the values of ``_v_pos`` here.
            if columns is not None:
                descr = Description(copy.copy(columns), self._v_nestedlvl)
            classdict[name] = descr

            pos = getattr(descr, '_v_pos', None)
            if pos is None:
                cols_no_pos.append(name)
            else:
                cols_with_pos.append((pos, name))

        # Sort field names:
        #
        # 1. Fields with explicit positions, according to their
        #    positions (and their names if coincident).
        # 2. Fields with no position, in alfabetical order.
        cols_with_pos.sort()
        cols_no_pos.sort()
        keys = [name for (pos, name) in cols_with_pos] + cols_no_pos

        pos = 0
        # Get properties for compound types
        for k in keys:
            if validate:
                # Check for key name validity
                check_name_validity(k)
            # Class variables
            object = classdict[k]
            newdict[k] = object    # To allow natural naming
            if not (isinstance(object, Col) or
                    isinstance(object, Description)):
                raise TypeError('Passing an incorrect value to a table column.'
                                ' Expected a Col (or subclass) instance and '
                                'got: "%s". Please make use of the Col(), or '
                                'descendant, constructor to properly '
                                'initialize columns.' % object)
            object._v_pos = pos  # Set the position of this object
            object._v_parent = self  # The parent description
            pos += 1
            newdict['_v_colobjects'][k] = object
            newdict['_v_names'].append(k)
            object.__dict__['_v_name'] = k

            if not isinstance(k, str):
                # numpy only accepts "str" for field names
                if sys.version_info[0] < 3:
                    # Python 2.x: unicode --> str
                    kk = k.encode()  # use the default encoding
                else:
                    # Python 3.x: bytes --> str (unicode)
                    kk = k.decode()
            else:
                kk = k

            if isinstance(object, Col):
                dtype = object.dtype
                newdict['_v_dtypes'][k] = dtype
                newdict['_v_types'][k] = object.type
                newdict['_v_dflts'][k] = object.dflt
                nestedFormats.append(object.recarrtype)
                baserecarrtype = dtype.base.str[1:]
                nestedDType.append((kk, baserecarrtype, dtype.shape))
            else:  # A description
                nestedFormats.append(object._v_nested_formats)
                nestedDType.append((kk, object._v_dtype))

        # Assign the format list to _v_nested_formats
        newdict['_v_nested_formats'] = nestedFormats
        newdict['_v_dtype'] = numpy.dtype(nestedDType)
        # _v_itemsize is derived from the _v_dtype that already computes this
        newdict['_v_itemsize'] = newdict['_v_dtype'].itemsize
        if self._v_nestedlvl == 0:
            # Get recursively nested _v_nested_names and _v_nested_descr attrs
            self._g_set_nested_names_descr()
            # Get pathnames for nested groups
            self._g_set_path_names()
            # Check the _v_byteorder has been used an issue an Error
            if hasattr(self, "_v_byteorder"):
                raise ValueError(
                    "Using a ``_v_byteorder`` in the description is obsolete. "
                    "Use the byteorder parameter in the constructor instead.")

    def _g_set_nested_names_descr(self):
        """Computes the nested names and descriptions for nested datatypes."""

        names = self._v_names
        fmts = self._v_nested_formats
        self._v_nested_names = names[:]  # Important to do a copy!
        self._v_nested_descr = [(names[i], fmts[i]) for i in range(len(names))]
        for i in range(len(names)):
            name = names[i]
            new_object = self._v_colobjects[name]
            if isinstance(new_object, Description):
                new_object._g_set_nested_names_descr()
                # replace the column nested name by a correct tuple
                self._v_nested_names[i] = (name, new_object._v_nested_names)
                self._v_nested_descr[i] = (name, new_object._v_nested_descr)
                # set the _v_is_nested flag
                self._v_is_nested = True

    _g_setNestedNamesDescr = previous_api(_g_set_nested_names_descr)

    def _g_set_path_names(self):
        """Compute the pathnames for arbitrary nested descriptions.

        This method sets the ``_v_pathname`` and ``_v_pathnames``
        attributes of all the elements (both descriptions and columns)
        in this nested description.

        """

        def get_cols_in_order(description):
            return [description._v_colobjects[colname]
                    for colname in description._v_names]

        def join_paths(path1, path2):
            if not path1:
                return path2
            return '%s/%s' % (path1, path2)

        # The top of the stack always has a nested description
        # and a list of its child columns
        # (be they nested ``Description`` or non-nested ``Col`` objects).
        # In the end, the list contains only a list of column paths
        # under this one.
        #
        # For instance, given this top of the stack::
        #
        #   (<Description X>, [<Column A>, <Column B>])
        #
        # After computing the rest of the stack, the top is::
        #
        #   (<Description X>, ['a', 'a/m', 'a/n', ... , 'b', ...])

        stack = []

        # We start by pushing the top-level description
        # and its child columns.
        self._v_pathname = ''
        stack.append((self, get_cols_in_order(self)))

        while stack:
            desc, cols = stack.pop()
            head = cols[0]

            # What's the first child in the list?
            if isinstance(head, Description):
                # A nested description.  We remove it from the list and
                # push it with its child columns.  This will be the next
                # handled description.
                head._v_pathname = join_paths(desc._v_pathname, head._v_name)
                stack.append((desc, cols[1:]))  # alter the top
                stack.append((head, get_cols_in_order(head)))  # new top
            elif isinstance(head, Col):
                # A non-nested column.  We simply remove it from the
                # list and append its name to it.
                head._v_pathname = join_paths(desc._v_pathname, head._v_name)
                cols.append(head._v_name)  # alter the top
                stack.append((desc, cols[1:]))  # alter the top
            else:
                # Since paths and names are appended *to the end* of
                # children lists, a string signals that no more children
                # remain to be processed, so we are done with the
                # description at the top of the stack.
                assert isinstance(head, basestring)
                # Assign the computed set of descendent column paths.
                desc._v_pathnames = cols
                if len(stack) > 0:
                    # Compute the paths with respect to the parent node
                    # (including the path of the current description)
                    # and append them to its list.
                    descName = desc._v_name
                    colPaths = [join_paths(descName, path) for path in cols]
                    colPaths.insert(0, descName)
                    parentCols = stack[-1][1]
                    parentCols.extend(colPaths)
                # (Nothing is pushed, we are done with this description.)

    _g_setPathNames = previous_api(_g_set_path_names)

    def _f_walk(self, type='All'):
        """Iterate over nested columns.

        If type is 'All' (the default), all column description objects (Col and
        Description instances) are yielded in top-to-bottom order (preorder).

        If type is 'Col' or 'Description', only column descriptions of that
        type are yielded.

        """

        if type not in ["All", "Col", "Description"]:
            raise ValueError("""\
type can only take the parameters 'All', 'Col' or 'Description'.""")

        stack = [self]
        while stack:
            object = stack.pop(0)  # pop at the front so as to ensure the order
            if type in ["All", "Description"]:
                yield object  # yield description
            names = object._v_names
            for i in range(len(names)):
                new_object = object._v_colobjects[names[i]]
                if isinstance(new_object, Description):
                    stack.append(new_object)
                else:
                    if type in ["All", "Col"]:
                        yield new_object  # yield column

    def __repr__(self):
        """Gives a detailed Description column representation."""

        rep = ['%s\"%s\": %r' %
               ("  " * self._v_nestedlvl, k, self._v_colobjects[k])
               for k in self._v_names]
        return '{\n  %s}' % (',\n  '.join(rep))

    def __str__(self):
        """Gives a brief Description representation."""

        return 'Description(%s)' % self._v_nested_descr


class MetaIsDescription(type):
    """Helper metaclass to return the class variables as a dictionary."""

    def __new__(cls, classname, bases, classdict):
        """Return a new class with a "columns" attribute filled."""

        newdict = {"columns": {}, }
        if '__doc__' in classdict:
            newdict['__doc__'] = classdict['__doc__']
        for b in bases:
            if "columns" in b.__dict__:
                newdict["columns"].update(b.__dict__["columns"])
        for k in classdict:
            # if not (k.startswith('__') or k.startswith('_v_')):
            # We let pass _v_ variables to configure class behaviour
            if not (k.startswith('__')):
                newdict["columns"][k] = classdict[k]

        # Return a new class with the "columns" attribute filled
        return type.__new__(cls, classname, bases, newdict)

metaIsDescription = previous_api(MetaIsDescription)


class IsDescription(object):
    """Description of the structure of a table or nested column.

    This class is designed to be used as an easy, yet meaningful way to
    describe the structure of new Table (see :ref:`TableClassDescr`) datasets
    or nested columns through the definition of *derived classes*. In order to
    define such a class, you must declare it as descendant of IsDescription,
    with as many attributes as columns you want in your table. The name of each
    attribute will become the name of a column, and its value will hold a
    description of it.

    Ordinary columns can be described using instances of the Col class (see
    :ref:`ColClassDescr`). Nested columns can be described by using classes
    derived from IsDescription, instances of it, or name-description
    dictionaries. Derived classes can be declared in place (in which case the
    column takes the name of the class) or referenced by name.

    Nested columns can have a _v_pos special attribute which sets the
    *relative* position of the column among sibling columns *also having
    explicit positions*.  The pos constructor argument of Col instances is used
    for the same purpose.  Columns with no explicit position will be placed
    afterwards in alphanumeric order.

    Once you have created a description object, you can pass it to the Table
    constructor, where all the information it contains will be used to define
    the table structure.

    .. rubric:: IsDescription attributes

    .. attribute:: _v_pos

        Sets the position of a possible nested column description among its
        sibling columns.  This attribute can be specified *when declaring*
        an IsDescription subclass to complement its *metadata*.

    .. attribute:: columns

        Maps the name of each column in the description to its own descriptive
        object. This attribute is *automatically created* when an IsDescription
        subclass is declared.  Please note that declared columns can no longer
        be accessed as normal class variables after its creation.

    """

    __metaclass__ = MetaIsDescription


def descr_from_dtype(dtype_):
    """Get a description instance and byteorder from a (nested) NumPy dtype."""

    fields = {}
    fbyteorder = '|'
    for name in dtype_.names:
        dtype, pos = dtype_.fields[name][:2]
        kind = dtype.base.kind
        byteorder = dtype.base.byteorder
        if byteorder in '><=':
            if fbyteorder not in ['|', byteorder]:
                raise NotImplementedError(
                    "structured arrays with mixed byteorders "
                    "are not supported yet, sorry")
            fbyteorder = byteorder
        # Non-nested column
        if kind in 'biufSc':
            col = Col.from_dtype(dtype, pos=pos)
        # Nested column
        elif kind == 'V' and dtype.shape in [(), (1,)]:
            col, _ = descr_from_dtype(dtype)
            col._v_pos = pos
        else:
            raise NotImplementedError(
                "structured arrays with columns with type description ``%s`` "
                "are not supported yet, sorry" % dtype)
        fields[name] = col

    return Description(fields), fbyteorder


def dtype_from_descr(descr, byteorder=None):
    """Get a (nested) NumPy dtype from a description instance and byteorder.

    The descr parameter can be a Description or IsDescription
    instance, sub-class of IsDescription or a dictionary.

    """

    if isinstance(descr, dict):
        descr = Description(descr)
    elif (type(descr) == type(IsDescription)
          and issubclass(descr, IsDescription)):
        descr = Description(descr().columns)
    elif isinstance(descr, IsDescription):
        descr = Description(descr.columns)
    elif not isinstance(descr, Description):
        raise ValueError('invalid description: %r' % descr)

    dtype_ = descr._v_dtype

    if byteorder and byteorder != '|':
        dtype_ = dtype_.newbyteorder(byteorder)

    return dtype_


if __name__ == "__main__":
    """Test code."""

    class Info(IsDescription):
        _v_pos = 2
        Name = UInt32Col()
        Value = Float64Col()

    class Test(IsDescription):
        """A description that has several columns."""

        x = Col.from_type("int32", 2, 0, pos=0)
        y = Col.from_kind('float', dflt=1, shape=(2, 3))
        z = UInt8Col(dflt=1)
        color = StringCol(2, dflt=" ")
        # color = UInt32Col(2)
        Info = Info()

        class info(IsDescription):
            _v_pos = 1
            name = UInt32Col()
            value = Float64Col(pos=0)
            y2 = Col.from_kind('float', dflt=1, shape=(2, 3), pos=1)
            z2 = UInt8Col(dflt=1)

            class info2(IsDescription):
                y3 = Col.from_kind('float', dflt=1, shape=(2, 3))
                z3 = UInt8Col(dflt=1)
                name = UInt32Col()
                value = Float64Col()

                class info3(IsDescription):
                    name = UInt32Col()
                    value = Float64Col()
                    y4 = Col.from_kind('float', dflt=1, shape=(2, 3))
                    z4 = UInt8Col(dflt=1)

#     class Info(IsDescription):
#         _v_pos = 2
#         Name = StringCol(itemsize=2)
#         Value = ComplexCol(itemsize=16)

#     class Test(IsDescription):
#         """A description that has several columns"""
#         x = Col.from_type("int32", 2, 0, pos=0)
#         y = Col.from_kind('float', dflt=1, shape=(2,3))
#         z = UInt8Col(dflt=1)
#         color = StringCol(2, dflt=" ")
#         Info = Info()
#         class info(IsDescription):
#             _v_pos = 1
#             name = StringCol(itemsize=2)
#             value = ComplexCol(itemsize=16, pos=0)
#             y2 = Col.from_kind('float', dflt=1, shape=(2,3), pos=1)
#             z2 = UInt8Col(dflt=1)
#             class info2(IsDescription):
#                 y3 = Col.from_kind('float', dflt=1, shape=(2,3))
#                 z3 = UInt8Col(dflt=1)
#                 name = StringCol(itemsize=2)
#                 value = ComplexCol(itemsize=16)
#                 class info3(IsDescription):
#                     name = StringCol(itemsize=2)
#                     value = ComplexCol(itemsize=16)
#                     y4 = Col.from_kind('float', dflt=1, shape=(2,3))
#                     z4 = UInt8Col(dflt=1)

    # example cases of class Test
    klass = Test()
    # klass = Info()
    desc = Description(klass.columns)
    print("Description representation (short) ==>", desc)
    print("Description representation (long) ==>", repr(desc))
    print("Column names ==>", desc._v_names)
    print("Column x ==>", desc.x)
    print("Column Info ==>", desc.Info)
    print("Column Info.value ==>", desc.Info.Value)
    print("Nested column names  ==>", desc._v_nested_names)
    print("Defaults ==>", desc._v_dflts)
    print("Nested Formats ==>", desc._v_nested_formats)
    print("Nested Descriptions ==>", desc._v_nested_descr)
    print("Nested Descriptions (info) ==>", desc.info._v_nested_descr)
    print("Total size ==>", desc._v_dtype.itemsize)

    # check _f_walk
    for object in desc._f_walk():
        if isinstance(object, Description):
            print("******begin object*************", end=' ')
            print("name -->", object._v_name)
            # print("name -->", object._v_dtype.name)
            # print("object childs-->", object._v_names)
            # print("object nested childs-->", object._v_nested_names)
            print("totalsize-->", object._v_dtype.itemsize)
        else:
            # pass
            print("leaf -->", object._v_name, object.dtype)

    class testDescParent(IsDescription):
        c = Int32Col()

    class testDesc(testDescParent):
        pass

    assert 'c' in testDesc.columns

## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = earray
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: December 15, 2003
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the EArray class."""

import numpy

from tables.utils import convert_to_np_atom2, SizeType
from tables.carray import CArray

from tables._past import previous_api, previous_api_property

# default version for EARRAY objects
# obversion = "1.0"    # initial version
# obversion = "1.1"    # support for complex datatypes
# obversion = "1.2"    # This adds support for time datatypes.
# obversion = "1.3"    # This adds support for enumerated datatypes.
obversion = "1.4"    # Numeric and numarray flavors are gone.


class EArray(CArray):
    """This class represents extendable, homogeneous datasets in an HDF5 file.

    The main difference between an EArray and a CArray (see
    :ref:`CArrayClassDescr`), from which it inherits, is that the former
    can be enlarged along one of its dimensions, the *enlargeable
    dimension*.  That means that the :attr:`Leaf.extdim` attribute (see
    :class:`Leaf`) of any EArray instance will always be non-negative.
    Multiple enlargeable dimensions might be supported in the future.

    New rows can be added to the end of an enlargeable array by using the
    :meth:`EArray.append` method.

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.

    atom
        An `Atom` instance representing the *type* and *shape*
        of the atomic objects to be saved.

    shape
        The shape of the new array.  One (and only one) of
        the shape dimensions *must* be 0.  The dimension being 0
        means that the resulting `EArray` object can be extended
        along it.  Multiple enlargeable dimensions are not supported
        right now.

    title
        A description for this node (it sets the ``TITLE``
        HDF5 attribute on disk).

    filters
        An instance of the `Filters` class that provides information
        about the desired I/O filters to be applied during the life
        of this object.

    expectedrows
        A user estimate about the number of row elements that will
        be added to the growable dimension in the `EArray` node.
        If not provided, the default value is ``EXPECTED_ROWS_EARRAY``
        (see ``tables/parameters.py``).  If you plan to create either
        a much smaller or a much bigger `EArray` try providing a guess;
        this will optimize the HDF5 B-Tree creation and management
        process time and the amount of memory used.

    chunkshape
        The shape of the data chunk to be read or written in a single
        HDF5 I/O operation.  Filters are applied to those chunks of data.
        The dimensionality of `chunkshape` must be the same as that of
        `shape` (beware: no dimension should be 0 this time!).
        If ``None``, a sensible value is calculated based on the
        `expectedrows` parameter (which is recommended).

    byteorder
        The byteorder of the data *on disk*, specified as 'little' or
        'big'. If this is not specified, the byteorder is that of the
        platform.

    Examples
    --------

    See below a small example of the use of the `EArray` class.  The
    code is available in ``examples/earray1.py``::

        import tables
        import numpy

        fileh = tables.open_file('earray1.h5', mode='w')
        a = tables.StringAtom(itemsize=8)

        # Use ``a`` as the object type for the enlargeable array.
        array_c = fileh.create_earray(fileh.root, 'array_c', a, (0,),
                                      \"Chars\")
        array_c.append(numpy.array(['a'*2, 'b'*4], dtype='S8'))
        array_c.append(numpy.array(['a'*6, 'b'*8, 'c'*10], dtype='S8'))

        # Read the string ``EArray`` we have created on disk.
        for s in array_c:
            print('array_c[%s] => %r' % (array_c.nrow, s))
        # Close the file.
        fileh.close()

    The output for the previous script is something like::

        array_c[0] => 'aa'
        array_c[1] => 'bbbb'
        array_c[2] => 'aaaaaa'
        array_c[3] => 'bbbbbbbb'
        array_c[4] => 'cccccccc'

    """

    # Class identifier.
    _c_classid = 'EARRAY'

    _c_classId = previous_api_property('_c_classid')

    # Special methods
    # ~~~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 atom=None, shape=None, title="",
                 filters=None, expectedrows=None,
                 chunkshape=None, byteorder=None,
                 _log=True):

        # Specific of EArray
        if expectedrows is None:
            expectedrows = parentnode._v_file.params['EXPECTED_ROWS_EARRAY']
        self._v_expectedrows = expectedrows
        """The expected number of rows to be stored in the array."""

        # Call the parent (CArray) init code
        super(EArray, self).__init__(parentnode, name, atom, shape, title,
                                     filters, chunkshape, byteorder, _log)

    # Public and private methods
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~
    def _g_create(self):
        """Create a new array in file (specific part)."""

        # Pre-conditions and extdim computation
        zerodims = numpy.sum(numpy.array(self.shape) == 0)
        if zerodims > 0:
            if zerodims == 1:
                self.extdim = list(self.shape).index(0)
            else:
                raise NotImplementedError(
                    "Multiple enlargeable (0-)dimensions are not "
                    "supported.")
        else:
            raise ValueError(
                "When creating EArrays, you need to set one of "
                "the dimensions of the Atom instance to zero.")

        # Finish the common part of the creation process
        return self._g_create_common(self._v_expectedrows)

    def _check_shape_append(self, nparr):
        "Test that nparr shape is consistent with underlying EArray."

        # The arrays conforms self expandibility?
        myrank = len(self.shape)
        narank = len(nparr.shape) - len(self.atom.shape)
        if myrank != narank:
            raise ValueError(("the ranks of the appended object (%d) and the "
                              "``%s`` EArray (%d) differ")
                             % (narank, self._v_pathname, myrank))
        for i in range(myrank):
            if i != self.extdim and self.shape[i] != nparr.shape[i]:
                raise ValueError(("the shapes of the appended object and the "
                                  "``%s`` EArray differ in non-enlargeable "
                                  "dimension %d") % (self._v_pathname, i))

    _checkShapeAppend = previous_api(_check_shape_append)

    def append(self, sequence):
        """Add a sequence of data to the end of the dataset.

        The sequence must have the same type as the array; otherwise a
        TypeError is raised. In the same way, the dimensions of the
        sequence must conform to the shape of the array, that is, all
        dimensions must match, with the exception of the enlargeable
        dimension, which can be of any length (even 0!).  If the shape
        of the sequence is invalid, a ValueError is raised.

        """

        self._g_check_open()
        self._v_file._check_writable()

        # Convert the sequence into a NumPy object
        nparr = convert_to_np_atom2(sequence, self.atom)
        # Check if it has a consistent shape with underlying EArray
        self._check_shape_append(nparr)
        # If the size of the nparr is zero, don't do anything else
        if nparr.size > 0:
            self._append(nparr)

    def _g_copy_with_stats(self, group, name, start, stop, step,
                           title, filters, chunkshape, _log, **kwargs):
        """Private part of Leaf.copy() for each kind of leaf."""

        (start, stop, step) = self._process_range_read(start, stop, step)
        # Build the new EArray object
        maindim = self.maindim
        shape = list(self.shape)
        shape[maindim] = 0
        # The number of final rows
        nrows = len(xrange(start, stop, step))
        # Build the new EArray object
        object = EArray(
            group, name, atom=self.atom, shape=shape, title=title,
            filters=filters, expectedrows=nrows, chunkshape=chunkshape,
            _log=_log)
        # Now, fill the new earray with values from source
        nrowsinbuf = self.nrowsinbuf
        # The slices parameter for self.__getitem__
        slices = [slice(0, dim, 1) for dim in self.shape]
        # This is a hack to prevent doing unnecessary conversions
        # when copying buffers
        self._v_convert = False
        # Start the copy itself
        for start2 in xrange(start, stop, step * nrowsinbuf):
            # Save the records on disk
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            # Set the proper slice in the extensible dimension
            slices[maindim] = slice(start2, stop2, step)
            object._append(self.__getitem__(tuple(slices)))
        # Active the conversion again (default)
        self._v_convert = True
        nbytes = numpy.prod(self.shape, dtype=SizeType) * self.atom.itemsize

        return (object, nbytes)

    _g_copyWithStats = previous_api(_g_copy_with_stats)

## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = exceptions
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: December 17, 2004
# Author:  Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Declare exceptions and warnings that are specific to PyTables."""

__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


import os
import warnings
import traceback


class HDF5ExtError(RuntimeError):
    """A low level HDF5 operation failed.

    This exception is raised the low level PyTables components used for
    accessing HDF5 files.  It usually signals that something is not
    going well in the HDF5 library or even at the Input/Output level.

    Errors in the HDF5 C library may be accompanied by an extensive
    HDF5 back trace on standard error (see also
    :func:`tables.silence_hdf5_messages`).

    .. versionchanged:: 2.4

    Parameters
    ----------
    message
        error message
    h5bt
        This parameter (keyword only) controls the HDF5 back trace
        handling. Any keyword arguments other than h5bt is ignored.

        * if set to False the HDF5 back trace is ignored and the
          :attr:`HDF5ExtError.h5backtrace` attribute is set to None
        * if set to True the back trace is retrieved from the HDF5
          library and stored in the :attr:`HDF5ExtError.h5backtrace`
          attribute as a list of tuples
        * if set to "VERBOSE" (default) the HDF5 back trace is
          stored in the :attr:`HDF5ExtError.h5backtrace` attribute
          and also included in the string representation of the
          exception
        * if not set (or set to None) the default policy is used
          (see :attr:`HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY`)

    """

    # NOTE: in order to avoid circular dependencies between modules the
    #       _dump_h5_backtrace method is set at initialization time in
    #       the utilsExtenion.
    _dump_h5_backtrace = None

    DEFAULT_H5_BACKTRACE_POLICY = "VERBOSE"
    """Default policy for HDF5 backtrace handling

    * if set to False the HDF5 back trace is ignored and the
      :attr:`HDF5ExtError.h5backtrace` attribute is set to None
    * if set to True the back trace is retrieved from the HDF5
      library and stored in the :attr:`HDF5ExtError.h5backtrace`
      attribute as a list of tuples
    * if set to "VERBOSE" (default) the HDF5 back trace is
      stored in the :attr:`HDF5ExtError.h5backtrace` attribute
      and also included in the string representation of the
      exception

    This parameter can be set using the
    :envvar:`PT_DEFAULT_H5_BACKTRACE_POLICY` environment variable.
    Allowed values are "IGNORE" (or "FALSE"), "SAVE" (or "TRUE") and
    "VERBOSE" to set the policy to False, True and "VERBOSE"
    respectively.  The special value "DEFAULT" can be used to reset
    the policy to the default value

    .. versionadded:: 2.4
    """

    @classmethod
    def set_policy_from_env(cls):
        envmap = {
            "IGNORE": False,
            "FALSE": False,
            "SAVE": True,
            "TRUE": True,
            "VERBOSE": "VERBOSE",
            "DEFAULT": "VERBOSE",
        }
        oldvalue = cls.DEFAULT_H5_BACKTRACE_POLICY
        envvalue = os.environ.get("PT_DEFAULT_H5_BACKTRACE_POLICY", "DEFAULT")
        try:
            newvalue = envmap[envvalue.upper()]
        except KeyError:
            warnings.warn("Invalid value for the environment variable "
                          "'PT_DEFAULT_H5_BACKTRACE_POLICY'.  The default "
                          "policy for HDF5 back trace management in PyTables "
                          "will be: '%s'" % oldvalue)
        else:
            cls.DEFAULT_H5_BACKTRACE_POLICY = newvalue

        return oldvalue

    def __init__(self, *args, **kargs):

        super(HDF5ExtError, self).__init__(*args)

        self._h5bt_policy = kargs.get('h5bt', self.DEFAULT_H5_BACKTRACE_POLICY)

        if self._h5bt_policy and self._dump_h5_backtrace is not None:
            self.h5backtrace = self._dump_h5_backtrace()
            """HDF5 back trace.

            Contains the HDF5 back trace as a (possibly empty) list of
            tuples.  Each tuple has the following format::

                (filename, line number, function name, text)

            Depending on the value of the *h5bt* parameter passed to the
            initializer the h5backtrace attribute can be set to None.
            This means that the HDF5 back trace has been simply ignored
            (not retrieved from the HDF5 C library error stack) or that
            there has been an error (silently ignored) during the HDF5 back
            trace retrieval.

            .. versionadded:: 2.4

            See Also
            --------
            traceback.format_list : :func:`traceback.format_list`

            """

            # XXX: check _dump_h5_backtrace failures
        else:
            self.h5backtrace = None

    def __str__(self):
        """Returns a sting representation of the exception.

        The actual result depends on policy set in the initializer
        :meth:`HDF5ExtError.__init__`.

        .. versionadded:: 2.4

        """

        verbose = bool(self._h5bt_policy in ('VERBOSE', 'verbose'))

        if verbose and self.h5backtrace:
            bt = "\n".join([
                "HDF5 error back trace\n",
                self.format_h5_backtrace(),
                "End of HDF5 error back trace"
            ])

            if len(self.args) == 1 and isinstance(self.args[0], basestring):
                msg = super(HDF5ExtError, self).__str__()
                msg = "%s\n\n%s" % (bt, msg)
            elif self.h5backtrace[-1][-1]:
                msg = "%s\n\n%s" % (bt, self.h5backtrace[-1][-1])
            else:
                msg = bt
        else:
            msg = super(HDF5ExtError, self).__str__()

        return msg

    def format_h5_backtrace(self, backtrace=None):
        """Convert the HDF5 trace back represented as a list of tuples.
        (see :attr:`HDF5ExtError.h5backtrace`) into a string.

        .. versionadded:: 2.4

        """
        if backtrace is None:
            backtrace = self.h5backtrace

        if backtrace is None:
            return 'No HDF5 back trace available'
        else:
            return ''.join(traceback.format_list(backtrace))


# Initialize the policy for HDF5 back trace handling
HDF5ExtError.set_policy_from_env()


# The following exceptions are concretions of the ``ValueError`` exceptions
# raised by ``file`` objects on certain operations.

class ClosedNodeError(ValueError):
    """The operation can not be completed because the node is closed.

    For instance, listing the children of a closed group is not allowed.

    """

    pass


class ClosedFileError(ValueError):
    """The operation can not be completed because the hosting file is closed.

    For instance, getting an existing node from a closed file is not
    allowed.

    """

    pass


class FileModeError(ValueError):
    """The operation can not be carried out because the mode in which the
    hosting file is opened is not adequate.

    For instance, removing an existing leaf from a read-only file is not
    allowed.

    """

    pass


class NodeError(AttributeError, LookupError):
    """Invalid hierarchy manipulation operation requested.

    This exception is raised when the user requests an operation on the
    hierarchy which can not be run because of the current layout of the
    tree.  This includes accessing nonexistent nodes, moving or copying
    or creating over an existing node, non-recursively removing groups
    with children, and other similarly invalid operations.

    A node in a PyTables database cannot be simply overwritten by
    replacing it.  Instead, the old node must be removed explicitely
    before another one can take its place.  This is done to protect
    interactive users from inadvertedly deleting whole trees of data by
    a single erroneous command.

    """

    pass


class NoSuchNodeError(NodeError):
    """An operation was requested on a node that does not exist.

    This exception is raised when an operation gets a path name or a
    ``(where, name)`` pair leading to a nonexistent node.

    """

    pass


class UndoRedoError(Exception):
    """Problems with doing/redoing actions with Undo/Redo feature.

    This exception indicates a problem related to the Undo/Redo
    mechanism, such as trying to undo or redo actions with this
    mechanism disabled, or going to a nonexistent mark.

    """

    pass


class UndoRedoWarning(Warning):
    """Issued when an action not supporting Undo/Redo is run.

    This warning is only shown when the Undo/Redo mechanism is enabled.

    """

    pass


class NaturalNameWarning(Warning):
    """Issued when a non-pythonic name is given for a node.

    This is not an error and may even be very useful in certain
    contexts, but one should be aware that such nodes cannot be
    accessed using natural naming (instead, ``getattr()`` must be
    used explicitly).
    """

    pass


class PerformanceWarning(Warning):
    """Warning for operations which may cause a performance drop.

    This warning is issued when an operation is made on the database
    which may cause it to slow down on future operations (i.e. making
    the node tree grow too much).

    """

    pass


class FlavorError(ValueError):
    """Unsupported or unavailable flavor or flavor conversion.

    This exception is raised when an unsupported or unavailable flavor
    is given to a dataset, or when a conversion of data between two
    given flavors is not supported nor available.

    """

    pass


class FlavorWarning(Warning):
    """Unsupported or unavailable flavor conversion.

    This warning is issued when a conversion of data between two given
    flavors is not supported nor available, and raising an error would
    render the data inaccessible (e.g. on a dataset of an unavailable
    flavor in a read-only file).

    See the `FlavorError` class for more information.

    """

    pass


class FiltersWarning(Warning):
    """Unavailable filters.

    This warning is issued when a valid filter is specified but it is
    not available in the system.  It may mean that an available default
    filter is to be used instead.

    """

    pass


class OldIndexWarning(Warning):
    """Unsupported index format.

    This warning is issued when an index in an unsupported format is
    found.  The index will be marked as invalid and will behave as if
    doesn't exist.

    """

    pass


class DataTypeWarning(Warning):
    """Unsupported data type.

    This warning is issued when an unsupported HDF5 data type is found
    (normally in a file created with other tool than PyTables).

    """

    pass


class ExperimentalFeatureWarning(Warning):
    """Generic warning for experimental features.

    This warning is issued when using a functionality that is still
    experimental and that users have to use with care.

    """
    pass



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = expression
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: June 12, 2009
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Expr class."""

from __future__ import print_function
import sys
import warnings

import numpy as np
import tables as tb
from numexpr.necompiler import getContext, getExprNames, getType, NumExpr
from numexpr.expressions import functions as numexpr_functions
from tables.utilsextension import get_indices
from tables.exceptions import PerformanceWarning
from tables.parameters import IO_BUFFER_SIZE, BUFFER_TIMES

from tables._past import previous_api


class Expr(object):
    """A class for evaluating expressions with arbitrary array-like objects.

    Expr is a class for evaluating expressions containing array-like objects.
    With it, you can evaluate expressions (like "3 * a + 4 * b") that
    operate on arbitrary large arrays while optimizing the resources
    required to perform them (basically main memory and CPU cache memory).
    It is similar to the Numexpr package (see :ref:`[NUMEXPR] <NUMEXPR>`),
    but in addition to NumPy objects, it also accepts disk-based homogeneous
    arrays, like the Array, CArray, EArray and Column PyTables objects.

    All the internal computations are performed via the Numexpr package,
    so all the broadcast and upcasting rules of Numexpr applies here too.
    These rules are very similar to the NumPy ones, but with some exceptions
    due to the particularities of having to deal with potentially very large
    disk-based arrays.  Be sure to read the documentation of the Expr
    constructor and methods as well as that of Numexpr, if you want to fully
    grasp these particularities.

    Parameters
    ----------
    expr : str
        This specifies the expression to be evaluated, such as "2 * a + 3 * b".
    uservars : dict
        This can be used to define the variable names appearing in *expr*.
        This mapping should consist of identifier-like strings pointing to any
        `Array`, `CArray`, `EArray`, `Column` or NumPy ndarray instances (or
        even others which will tried to be converted to ndarrays).  When
        `uservars` is not provided or `None`, the current local and global
        namespace is sought instead of `uservars`.  It is also possible to pass
        just some of the variables in expression via the `uservars` mapping,
        and the rest will be retrieved from the current local and global
        namespaces.
    kwargs : dict
        This is meant to pass additional parameters to the Numexpr kernel.
        This is basically the same as the kwargs argument in
        Numexpr.evaluate(), and is mainly meant for advanced use.

    Examples
    --------
    The following shows an example of using Expr.

        >>> a = f.create_array('/', 'a', np.array([1,2,3]))
        >>> b = f.create_array('/', 'b', np.array([3,4,5]))
        >>> c = np.array([4,5,6])
        >>> expr = tb.Expr("2 * a + b * c")   # initialize the expression
        >>> expr.eval()                 # evaluate it
        array([14, 24, 36])
        >>> sum(expr)                   # use as an iterator
        74

    where you can see that you can mix different containers in
    the expression (whenever shapes are consistent).

    You can also work with multidimensional arrays::

        >>> a2 = f.create_array('/', 'a2', np.array([[1,2],[3,4]]))
        >>> b2 = f.create_array('/', 'b2', np.array([[3,4],[5,6]]))
        >>> c2 = np.array([4,5])           # This will be broadcasted
        >>> expr = tb.Expr("2 * a2 + b2-c2")
        >>> expr.eval()
        array([[1, 3],
               [7, 9]])
        >>> sum(expr)
        array([ 8, 12])

    .. rubric:: Expr attributes

    .. attribute:: append_mode

        The append mode for user-provided output containers.

    .. attribute:: maindim

        Common main dimension for inputs in expression.

    .. attribute:: names

        The names of variables in expression (list).

    .. attribute:: out

        The user-provided container (if any) for the expression outcome.

    .. attribute:: o_start

        The start range selection for the user-provided output.

    .. attribute:: o_stop

        The stop range selection for the user-provided output.

    .. attribute:: o_step

        The step range selection for the user-provided output.

    .. attribute:: shape

        Common shape for the arrays in expression.

    .. attribute:: values

        The values of variables in expression (list).

    """

    _exprvars_cache = {}
    """Cache of variables participating in expressions.

    .. versionadded:: 3.0

    """

    def __init__(self, expr, uservars=None, **kwargs):

        self.append_mode = False
        """The append mode for user-provided output containers."""
        self.maindim = 0
        """Common main dimension for inputs in expression."""
        self.names = []
        """The names of variables in expression (list)."""
        self.out = None
        """The user-provided container (if any) for the expression outcome."""
        self.o_start = None
        """The start range selection for the user-provided output."""
        self.o_stop = None
        """The stop range selection for the user-provided output."""
        self.o_step = None
        """The step range selection for the user-provided output."""
        self.shape = None
        """Common shape for the arrays in expression."""
        self.start, self.stop, self.step = (None,) * 3
        self.start = None
        """The start range selection for the input."""
        self.stop = None
        """The stop range selection for the input."""
        self.step = None
        """The step range selection for the input."""
        self.values = []
        """The values of variables in expression (list)."""

        self._compiled_expr = None
        """The compiled expression."""
        self._single_row_out = None
        """A sample of the output with just a single row."""

        # First, get the signature for the arrays in expression
        vars_ = self._required_expr_vars(expr, uservars)
        context = getContext(kwargs)
        self.names, _ = getExprNames(expr, context)

        # Raise a ValueError in case we have unsupported objects
        for name, var in vars_.iteritems():
            if type(var) in (int, long, float, str):
                continue
            if not isinstance(var, (tb.Leaf, tb.Column)):
                if hasattr(var, "dtype"):
                    # Quacks like a NumPy object
                    continue
                raise TypeError("Unsupported variable type: %r" % var)
            objname = var.__class__.__name__
            if objname not in ("Array", "CArray", "EArray", "Column"):
                raise TypeError("Unsupported variable type: %r" % var)

        # NumPy arrays to be copied? (we don't need to worry about
        # PyTables objects, as the reads always return contiguous and
        # aligned objects, or at least I think so).
        for name, var in vars_.iteritems():
            if isinstance(var, np.ndarray):
                # See numexpr.necompiler.evaluate for a rational
                # of the code below
                if not var.flags.aligned:
                    if var.ndim != 1:
                        # Do a copy of this variable
                        var = var.copy()
                        # Update the vars_ dictionary
                        vars_[name] = var

        # Get the variables and types
        values = self.values
        types_ = []
        for name in self.names:
            value = vars_[name]
            if hasattr(value, 'atom'):
                types_.append(value.atom)
            elif hasattr(value, 'dtype'):
                types_.append(value)
            else:
                # try to convert into a NumPy array
                value = np.array(value)
                types_.append(value)
            values.append(value)

        # Create a signature for the expression
        signature = [(name, getType(type_))
                     for (name, type_) in zip(self.names, types_)]

        # Compile the expression
        self._compiled_expr = NumExpr(expr, signature, **kwargs)

        # Guess the shape for the outcome and the maindim of inputs
        self.shape, self.maindim = self._guess_shape()

    # The next method is similar to their counterpart in `Table`, but
    # adapted to the `Expr` own requirements.
    def _required_expr_vars(self, expression, uservars, depth=2):
        """Get the variables required by the `expression`.

        A new dictionary defining the variables used in the `expression`
        is returned.  Required variables are first looked up in the
        `uservars` mapping, then in the set of top-level columns of the
        table.  Unknown variables cause a `NameError` to be raised.

        When `uservars` is `None`, the local and global namespace where
        the API callable which uses this method is called is sought
        instead.  To disable this mechanism, just specify a mapping as
        `uservars`.

        Nested columns and variables with an ``uint64`` type are not
        allowed (`TypeError` and `NotImplementedError` are raised,
        respectively).

        `depth` specifies the depth of the frame in order to reach local
        or global variables.

        """

        # Get the names of variables used in the expression.
        exprvars_cache = self._exprvars_cache
        if not expression in exprvars_cache:
            # Protection against growing the cache too much
            if len(exprvars_cache) > 256:
                # Remove 10 (arbitrary) elements from the cache
                for k in exprvars_cache.keys()[:10]:
                    del exprvars_cache[k]
            cexpr = compile(expression, '<string>', 'eval')
            exprvars = [var for var in cexpr.co_names
                        if var not in ['None', 'False', 'True']
                        and var not in numexpr_functions]
            exprvars_cache[expression] = exprvars
        else:
            exprvars = exprvars_cache[expression]

        # Get the local and global variable mappings of the user frame
        # if no mapping has been explicitly given for user variables.
        user_locals, user_globals = {}, {}
        if uservars is None:
            user_frame = sys._getframe(depth)
            user_locals = user_frame.f_locals
            user_globals = user_frame.f_globals

        # Look for the required variables first among the ones
        # explicitly provided by the user.
        reqvars = {}
        for var in exprvars:
            # Get the value.
            if uservars is not None and var in uservars:
                val = uservars[var]
            elif uservars is None and var in user_locals:
                val = user_locals[var]
            elif uservars is None and var in user_globals:
                val = user_globals[var]
            else:
                raise NameError("name ``%s`` is not defined" % var)

            # Check the value.
            if hasattr(val, 'dtype') and val.dtype.str[1:] == 'u8':
                raise NotImplementedError(
                    "variable ``%s`` refers to "
                    "a 64-bit unsigned integer object, that is "
                    "not yet supported in expressions, sorry; " % var)
            elif hasattr(val, '_v_colpathnames'):  # nested column
                # This branch is never reached because the compile step
                # above already raise a ``TypeError`` for nested
                # columns, but that could change in the future.  So it
                # is best to let this here.
                raise TypeError(
                    "variable ``%s`` refers to a nested column, "
                    "not allowed in expressions" % var)
            reqvars[var] = val
        return reqvars

    _requiredExprVars = previous_api(_required_expr_vars)

    def set_inputs_range(self, start=None, stop=None, step=None):
        """Define a range for all inputs in expression.

        The computation will only take place for the range defined by
        the start, stop and step parameters in the main dimension of
        inputs (or the leading one, if the object lacks the concept of
        main dimension, like a NumPy container).  If not a common main
        dimension exists for all inputs, the leading dimension will be
        used instead.

        """

        self.start = start
        self.stop = stop
        self.step = step

    setInputsRange = previous_api(set_inputs_range)

    def set_output(self, out, append_mode=False):
        """Set out as container for output as well as the append_mode.

        The out must be a container that is meant to keep the outcome of
        the expression.  It should be an homogeneous type container and
        can typically be an Array, CArray, EArray, Column or a NumPy ndarray.

        The append_mode specifies the way of which the output is filled.
        If true, the rows of the outcome are *appended* to the out container.
        Of course, for doing this it is necessary that out would have an
        append() method (like an EArray, for example).

        If append_mode is false, the output is set via the __setitem__()
        method (see the Expr.set_output_range() for info on how to select
        the rows to be updated).  If out is smaller than what is required
        by the expression, only the computations that are needed to fill
        up the container are carried out.  If it is larger, the excess
        elements are unaffected.

        """

        if not (hasattr(out, "shape") and hasattr(out, "__setitem__")):
            raise ValueError(
                "You need to pass a settable multidimensional container "
                "as output")
        self.out = out
        if append_mode and not hasattr(out, "append"):
            raise ValueError(
                "For activating the ``append`` mode, you need a container "
                "with an `append()` method (like the `EArray`)")
        self.append_mode = append_mode

    setOutput = previous_api(set_output)

    def set_output_range(self, start=None, stop=None, step=None):
        """Define a range for user-provided output object.

        The output object will only be modified in the range specified by the
        start, stop and step parameters in the main dimension of output (or the
        leading one, if the object does not have the concept of main dimension,
        like a NumPy container).

        """

        if self.out is None:
            raise IndexError(
                "You need to pass an output object to `setOut()` first")
        self.o_start = start
        self.o_stop = stop
        self.o_step = step

    setOutputRange = previous_api(set_output_range)

    # Although the next code is similar to the method in `Leaf`, it
    # allows the use of pure NumPy objects.
    def _calc_nrowsinbuf(self, object_):
        """Calculate the number of rows that will fit in a buffer."""

        # Compute the rowsize for the *leading* dimension
        shape_ = list(object_.shape)
        if shape_:
            shape_[0] = 1

        rowsize = np.prod(shape_) * object_.dtype.itemsize

        # Compute the nrowsinbuf
        # Multiplying the I/O buffer size by 4 gives optimal results
        # in my benchmarks with `tables.Expr` (see ``bench/poly.py``)
        buffersize = IO_BUFFER_SIZE * 4
        nrowsinbuf = buffersize // rowsize

        # Safeguard against row sizes being extremely large
        if nrowsinbuf == 0:
            nrowsinbuf = 1
            # If rowsize is too large, issue a Performance warning
            maxrowsize = BUFFER_TIMES * buffersize
            if rowsize > maxrowsize:
                warnings.warn("""\
The object ``%s`` is exceeding the maximum recommended rowsize (%d
bytes); be ready to see PyTables asking for *lots* of memory and
possibly slow I/O.  You may want to reduce the rowsize by trimming the
value of dimensions that are orthogonal (and preferably close) to the
*leading* dimension of this object."""
                              % (object, maxrowsize),
                              PerformanceWarning)

        return nrowsinbuf

    def _guess_shape(self):
        """Guess the shape of the output of the expression."""

        # First, compute the maximum dimension of inputs and maindim
        # (if it exists)
        maxndim = 0
        maindims = []
        for val in self.values:
            # Get the minimum of the lengths
            if len(val.shape) > maxndim:
                maxndim = len(val.shape)
            if hasattr(val, "maindim"):
                maindims.append(val.maindim)
        if maxndim == 0:
            self._single_row_out = out = self._compiled_expr(*self.values)
            return (), None
        if maindims and [maindims[0]] * len(maindims) == maindims:
            # If all maindims detected are the same, use this as maindim
            maindim = maindims[0]
        else:
            # If not, the main dimension will be the default one
            maindim = 0

        # The slices parameter for inputs
        slices = (slice(None),) * maindim + (0,)

        # Now, collect the values in first row of arrays with maximum dims
        vals = []
        lens = []
        for val in self.values:
            shape = val.shape
            # Warning: don't use len(val) below or it will raise an
            # `Overflow` error on 32-bit platforms for large enough arrays.
            if shape != () and shape[maindim] == 0:
                vals.append(val[:])
                lens.append(0)
            elif len(shape) < maxndim:
                vals.append(val)
            else:
                vals.append(val.__getitem__(slices))
                lens.append(shape[maindim])
        minlen = min(lens)
        self._single_row_out = out = self._compiled_expr(*vals)
        shape = list(out.shape)
        if minlen > 0:
            shape.insert(maindim, minlen)
        return shape, maindim

    def _get_info(self, shape, maindim, itermode=False):
        """Return various info needed for evaluating the computation loop."""

        # Compute the shape of the resulting container having
        # in account new possible values of start, stop and step in
        # the inputs range
        if maindim is not None:
            (start, stop, step) = get_indices(
                self.start, self.stop, self.step, shape[maindim])
            shape[maindim] = min(
                shape[maindim], len(xrange(start, stop, step)))
            i_nrows = shape[maindim]
        else:
            start, stop, step = 0, 0, None
            i_nrows = 0

        if not itermode:
            # Create a container for output if not defined yet
            o_maindim = 0    # Default maindim
            if self.out is None:
                out = np.empty(shape, dtype=self._single_row_out.dtype)
                # Get the trivial values for start, stop and step
                if maindim is not None:
                    (o_start, o_stop, o_step) = (0, shape[maindim], 1)
                else:
                    (o_start, o_stop, o_step) = (0, 0, 1)
            else:
                out = self.out
                # Out container already provided.  Do some sanity checks.
                if hasattr(out, "maindim"):
                    o_maindim = out.maindim

                # Refine the shape of the resulting container having in
                # account new possible values of start, stop and step in
                # the output range
                o_shape = list(out.shape)
                (o_start, o_stop, o_step) = get_indices(
                    self.o_start, self.o_stop, self.o_step, o_shape[o_maindim])
                o_shape[o_maindim] = min(o_shape[o_maindim],
                                         len(xrange(o_start, o_stop, o_step)))

                # Check that the shape of output is consistent with inputs
                tr_oshape = list(o_shape)   # this implies a copy
                olen_ = tr_oshape.pop(o_maindim)
                tr_shape = list(shape)      # do a copy
                if maindim is not None:
                    len_ = tr_shape.pop(o_maindim)
                else:
                    len_ = 1
                if tr_oshape != tr_shape:
                    raise ValueError(
                        "Shape for out container does not match expression")
                # Force the input length to fit in `out`
                if not self.append_mode and olen_ < len_:
                    shape[o_maindim] = olen_
                    stop = start + olen_

        # Get the positions of inputs that should be sliced (the others
        # will be broadcasted)
        ndim = len(shape)
        slice_pos = [i for i, val in enumerate(self.values)
                     if len(val.shape) == ndim]

        # The size of the I/O buffer
        nrowsinbuf = 1
        for i, val in enumerate(self.values):
            # Skip scalar values in variables
            if i in slice_pos:
                nrows = self._calc_nrowsinbuf(val)
                if nrows > nrowsinbuf:
                    nrowsinbuf = nrows

        if not itermode:
            return (i_nrows, slice_pos, start, stop, step, nrowsinbuf,
                    out, o_maindim, o_start, o_stop, o_step)
        else:
            # For itermode, we don't need the out info
            return (i_nrows, slice_pos, start, stop, step, nrowsinbuf)

    def eval(self):
        """Evaluate the expression and return the outcome.

        Because of performance reasons, the computation order tries to go along
        the common main dimension of all inputs.  If not such a common main
        dimension is found, the iteration will go along the leading dimension
        instead.

        For non-consistent shapes in inputs (i.e. shapes having a different
        number of dimensions), the regular NumPy broadcast rules applies.
        There is one exception to this rule though: when the dimensions
        orthogonal to the main dimension of the expression are consistent, but
        the main dimension itself differs among the inputs, then the shortest
        one is chosen for doing the computations.  This is so because trying to
        expand very large on-disk arrays could be too expensive or simply not
        possible.

        Also, the regular Numexpr casting rules (which are similar to those of
        NumPy, although you should check the Numexpr manual for the exceptions)
        are applied to determine the output type.

        Finally, if the setOuput() method specifying a user container has
        already been called, the output is sent to this user-provided
        container.  If not, a fresh NumPy container is returned instead.

        .. warning::

            When dealing with large on-disk inputs, failing to specify an
            on-disk container may consume all your available memory.

        """

        values, shape, maindim = self.values, self.shape, self.maindim

        # Get different info we need for the main computation loop
        (i_nrows, slice_pos, start, stop, step, nrowsinbuf,
         out, o_maindim, o_start, o_stop, o_step) = \
            self._get_info(shape, maindim)

        if i_nrows == 0:
            # No elements to compute
            return self._single_row_out

        # Create a key that selects every element in inputs and output
        # (including the main dimension)
        i_slices = [slice(None)] * (maindim + 1)
        o_slices = [slice(None)] * (o_maindim + 1)

        # This is a hack to prevent doing unnecessary flavor conversions
        # while reading buffers
        for val in values:
            if hasattr(val, 'maindim'):
                val._v_convert = False

        # Start the computation itself
        for start2 in xrange(start, stop, step * nrowsinbuf):
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            # Set the proper slice for inputs
            i_slices[maindim] = slice(start2, stop2, step)
            # Get the input values
            vals = []
            for i, val in enumerate(values):
                if i in slice_pos:
                    vals.append(val.__getitem__(tuple(i_slices)))
                else:
                    # A read of values is not apparently needed, as PyTables
                    # leaves seems to work just fine inside Numexpr
                    vals.append(val)
            # Do the actual computation for this slice
            rout = self._compiled_expr(*vals)
            # Set the values into the out buffer
            if self.append_mode:
                out.append(rout)
            else:
                # Compute the slice to be filled in output
                start3 = o_start + (start2 - start) // step
                stop3 = start3 + nrowsinbuf * o_step
                if stop3 > o_stop:
                    stop3 = o_stop
                o_slices[o_maindim] = slice(start3, stop3, o_step)
                # Set the slice
                out[tuple(o_slices)] = rout

        # Activate the conversion again (default)
        for val in values:
            if hasattr(val, 'maindim'):
                val._v_convert = True

        return out

    def __iter__(self):
        """Iterate over the rows of the outcome of the expression.

        This iterator always returns rows as NumPy objects, so a possible out
        container specified in :meth:`Expr.set_output` method is ignored here.

        """

        values, shape, maindim = self.values, self.shape, self.maindim

        # Get different info we need for the main computation loop
        (i_nrows, slice_pos, start, stop, step, nrowsinbuf) = \
            self._get_info(shape, maindim, itermode=True)

        if i_nrows == 0:
            # No elements to compute
            return

        # Create a key that selects every element in inputs
        # (including the main dimension)
        i_slices = [slice(None)] * (maindim + 1)

        # This is a hack to prevent doing unnecessary flavor conversions
        # while reading buffers
        for val in values:
            if hasattr(val, 'maindim'):
                val._v_convert = False

        # Start the computation itself
        for start2 in xrange(start, stop, step * nrowsinbuf):
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            # Set the proper slice in the main dimension
            i_slices[maindim] = slice(start2, stop2, step)
            # Get the values for computing the buffer
            vals = []
            for i, val in enumerate(values):
                if i in slice_pos:
                    vals.append(val.__getitem__(tuple(i_slices)))
                else:
                    # A read of values is not apparently needed, as PyTables
                    # leaves seems to work just fine inside Numexpr
                    vals.append(val)
            # Do the actual computation
            rout = self._compiled_expr(*vals)
            # Return one row per call
            for row in rout:
                yield row

        # Activate the conversion again (default)
        for val in values:
            if hasattr(val, 'maindim'):
                val._v_convert = True


if __name__ == "__main__":

    # shape = (10000,10000)
    shape = (10, 10000)

    f = tb.open_file("/tmp/expression.h5", "w")

    # Create some arrays
    a = f.create_carray(f.root, 'a', atom=tb.Float32Atom(dflt=1.), shape=shape)
    b = f.create_carray(f.root, 'b', atom=tb.Float32Atom(dflt=2.), shape=shape)
    c = f.create_carray(f.root, 'c', atom=tb.Float32Atom(dflt=3.), shape=shape)
    out = f.create_carray(f.root, 'out', atom=tb.Float32Atom(dflt=3.),
                          shape=shape)

    expr = Expr("a * b + c")
    expr.set_output(out)
    d = expr.eval()

    print("returned-->", repr(d))
    # print(`d[:]`)

    f.close()


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = file
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: September 4, 2002
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Create PyTables files and the object tree.

This module support importing generic HDF5 files, on top of which
PyTables files are created, read or extended. If a file exists, an
object tree mirroring their hierarchical structure is created in memory.
File class offer methods to traverse the tree, as well as to create new
nodes.

"""

from __future__ import print_function
import os
import sys
import time
import weakref
import warnings
import collections

import numexpr
import numpy

import tables.misc.proxydict
from tables import hdf5extension
from tables import utilsextension
from tables import parameters
from tables.exceptions import (ClosedFileError, FileModeError, NodeError,
                               NoSuchNodeError, UndoRedoError, ClosedNodeError,
                               PerformanceWarning)
from tables.registry import get_class_by_name
from tables.path import join_path, split_path
from tables import undoredo
from tables.description import (IsDescription, UInt8Col, StringCol,
                                descr_from_dtype, dtype_from_descr)
from tables.filters import Filters
from tables.node import Node, NotLoggedMixin
from tables.group import Group, RootGroup
from tables.group import TransactionGroupG, TransactionG, MarkG
from tables.leaf import Leaf
from tables.array import Array
from tables.carray import CArray
from tables.earray import EArray
from tables.vlarray import VLArray
from tables.table import Table
from tables import linkextension
from tables.utils import detect_number_of_cores
from tables import lrucacheextension
from tables.flavor import flavor_of, array_as_internal
from tables.atom import Atom

from tables.link import SoftLink, ExternalLink

from tables._past import previous_api, previous_api_property


# format_version = "1.0"  # Initial format
# format_version = "1.1"  # Changes in ucl compression
# format_version = "1.2"  # Support for enlargeable arrays and VLA's
#                         # 1.2 was introduced in PyTables 0.8
# format_version = "1.3"  # Support for indexes in Tables
#                         # 1.3 was introduced in PyTables 0.9
# format_version = "1.4"  # Support for multidimensional attributes
#                         # 1.4 was introduced in PyTables 1.1
# format_version = "1.5"  # Support for persistent defaults in tables
#                         # 1.5 was introduced in PyTables 1.2
# format_version = "1.6"  # Support for NumPy objects and new flavors for
#                         # objects.
#                         # 1.6 was introduced in pytables 1.3
#format_version = "2.0"   # Pickles are not used anymore in system attrs
#                         # 2.0 was introduced in PyTables 2.0
format_version = "2.1"  # Numeric and numarray flavors are gone.

compatible_formats = []  # Old format versions we can read
                         # Empty means that we support all the old formats


class _FileRegistry(object):
    def __init__(self):
        self._name_mapping = collections.defaultdict(set)
        self._handlers = set()

    @property
    def filenames(self):
        return self._name_mapping.keys()

    @property
    def handlers(self):
        #return set(self._handlers)  # return a copy
        return self._handlers

    def __len__(self):
        return len(self._handlers)

    def __contains__(self, filename):
        return filename in self.filenames

    def add(self, handler):
        self._name_mapping[handler.filename].add(handler)
        self._handlers.add(handler)

    def remove(self, handler):
        filename = handler.filename
        self._name_mapping[filename].remove(handler)
        # remove enpty keys
        if not self._name_mapping[filename]:
            del self._name_mapping[filename]
        self._handlers.remove(handler)

    def get_handlers_by_name(self, filename):
        #return set(self._name_mapping[filename])  # return a copy
        return self._name_mapping[filename]

    def close_all(self):
        are_open_files = len(self._handlers) > 0
        if are_open_files:
            sys.stderr.write("Closing remaining open files:")
        handlers = list(self._handlers)  # make a copy
        for fileh in handlers:
            sys.stderr.write("%s..." % fileh.filename)
            fileh.close()
            sys.stderr.write("done")
        if are_open_files:
            sys.stderr.write("\n")


# Dict of opened files (keys are filenames and values filehandlers)
_open_files = _FileRegistry()

# Opcodes for do-undo actions
_op_to_code = {
    "MARK": 0,
    "CREATE": 1,
    "REMOVE": 2,
    "MOVE": 3,
    "ADDATTR": 4,
    "DELATTR": 5,
}

_code_to_op = ["MARK", "CREATE", "REMOVE", "MOVE", "ADDATTR", "DELATTR"]


# Paths and names for hidden nodes related with transactions.
_trans_version = '1.0'

_trans_group_parent = '/'
_trans_group_name = '_p_transactions'
_trans_group_path = join_path(_trans_group_parent, _trans_group_name)

_action_log_parent = _trans_group_path
_action_log_name = 'actionlog'
_action_log_path = join_path(_action_log_parent, _action_log_name)

_trans_parent = _trans_group_path
_trans_name = 't%d'  # %d -> transaction number
_trans_path = join_path(_trans_parent, _trans_name)

_markParent = _trans_path
_markName = 'm%d'  # %d -> mark number
_markPath = join_path(_markParent, _markName)

_shadow_parent = _markPath
_shadow_name = 'a%d'  # %d -> action number
_shadow_path = join_path(_shadow_parent, _shadow_name)


def _checkfilters(filters):
    if not (filters is None or
            isinstance(filters, Filters)):
        raise TypeError("filter parameter has to be None or a Filter "
                        "instance and the passed type is: '%s'" %
                        type(filters))


def copy_file(srcfilename, dstfilename, overwrite=False, **kwargs):
    """An easy way of copying one PyTables file to another.

    This function allows you to copy an existing PyTables file named
    srcfilename to another file called dstfilename. The source file
    must exist and be readable. The destination file can be
    overwritten in place if existing by asserting the overwrite
    argument.

    This function is a shorthand for the :meth:`File.copy_file` method,
    which acts on an already opened file. kwargs takes keyword
    arguments used to customize the copying process. See the
    documentation of :meth:`File.copy_file` for a description of those
    arguments.

    """

    # Open the source file.
    srcfileh = open_file(srcfilename, mode="r")

    try:
        # Copy it to the destination file.
        srcfileh.copy_file(dstfilename, overwrite=overwrite, **kwargs)
    finally:
        # Close the source file.
        srcfileh.close()

copyFile = previous_api(copy_file)


if tuple(map(int, utilsextension.get_hdf5_version().split('-')[0].split('.'))) \
                                                                        < (1, 8, 7):
    _FILE_OPEN_POLICY = 'strict'
else:
    _FILE_OPEN_POLICY = 'default'


def open_file(filename, mode="r", title="", root_uep="/", filters=None,
              **kwargs):
    """Open a PyTables (or generic HDF5) file and return a File object.

    Parameters
    ----------
    filename : str
        The name of the file (supports environment variable expansion).
        It is suggested that file names have any of the .h5, .hdf or
        .hdf5 extensions, although this is not mandatory.
    mode : str
        The mode to open the file. It can be one of the
        following:

            * *'r'*: Read-only; no data can be modified.
            * *'w'*: Write; a new file is created (an existing file
              with the same name would be deleted).
            * *'a'*: Append; an existing file is opened for reading and
              writing, and if the file does not exist it is created.
            * *'r+'*: It is similar to 'a', but the file must already
              exist.

    title : str
        If the file is to be created, a TITLE string attribute will be
        set on the root group with the given value. Otherwise, the
        title will be read from disk, and this will not have any effect.
    root_uep : str
        The root User Entry Point. This is a group in the HDF5 hierarchy
        which will be taken as the starting point to create the object
        tree. It can be whatever existing group in the file, named by
        its HDF5 path. If it does not exist, an HDF5ExtError is issued.
        Use this if you do not want to build the *entire* object tree,
        but rather only a *subtree* of it.

        .. versionchanged:: 3.0
           The *rootUEP* parameter has been renamed into *root_uep*.

    filters : Filters
        An instance of the Filters (see :ref:`FiltersClassDescr`) class
        that provides information about the desired I/O filters
        applicable to the leaves that hang directly from the *root group*,
        unless other filter properties are specified for these leaves.
        Besides, if you do not specify filter properties for child groups,
        they will inherit these ones, which will in turn propagate to
        child nodes.

    Notes
    -----
    In addition, it recognizes the (lowercase) names of parameters
    present in :file:`tables/parameters.py` as additional keyword
    arguments.
    See :ref:`parameter_files` for a detailed info on the supported
    parameters.

    .. note::

        If you need to deal with a large number of nodes in an
        efficient way, please see :ref:`LRUOptim` for more info and
        advices about the integrated node cache engine.

    """

    # XXX filename normalization ??

    # Check already opened files
    if _FILE_OPEN_POLICY == 'strict':
        # This policy do not allows to open the same file multiple times
        # even in read-only mode
        if filename in _open_files:
            raise ValueError(
                "The file '%s' is already opened.  "
                "Please close it before reopening.  "
                "HDF5 v.%s, FILE_OPEN_POLICY = '%s'" % (
                    filename, utilsextension.get_hdf5_version(),
                    _FILE_OPEN_POLICY))
    else:
        for filehandle in _open_files.get_handlers_by_name(filename):
            omode = filehandle.mode
            # 'r' is incompatible with everything except 'r' itself
            if mode == 'r' and omode != 'r':
                raise ValueError(
                    "The file '%s' is already opened, but "
                    "not in read-only mode (as requested)." % filename)
            # 'a' and 'r+' are compatible with everything except 'r'
            elif mode in ('a', 'r+') and omode == 'r':
                raise ValueError(
                    "The file '%s' is already opened, but "
                    "in read-only mode.  Please close it before "
                    "reopening in append mode." % filename)
            # 'w' means that we want to destroy existing contents
            elif mode == 'w':
                raise ValueError(
                    "The file '%s' is already opened.  Please "
                    "close it before reopening in write mode." % filename)

    # Finally, create the File instance, and return it
    return File(filename, mode, title, root_uep, filters, **kwargs)

openFile = previous_api(open_file)


# A dumb class that doesn't keep nothing at all
class _NoCache(object):
    def __len__(self):
        return 0

    def __contains__(self, key):
        return False

    def __iter__(self):
        return iter([])

    def __setitem__(self, key, value):
        pass

    __marker = object()

    def pop(self, key, d=__marker):
        if d is not self.__marker:
            return d
        raise KeyError(key)


class _DictCache(dict):
    def __init__(self, nslots):
        if nslots < 1:
            raise ValueError("Invalid number of slots: %d" % nslots)
        self.nslots = nslots
        super(_DictCache, self).__init__()

    def __setitem__(self, key, value):
        # Check if we are running out of space
        if len(self) > self.nslots:
            warnings.warn(
                "the dictionary of node cache is exceeding the recommended "
                "maximum number (%d); be ready to see PyTables asking for "
                "*lots* of memory and possibly slow I/O." % (
                    self.nslots), PerformanceWarning)
        super(_DictCache, self).__setitem__(key, value)


class NodeManager(object):
    def __init__(self, nslots=64, node_factory=None):
        super(NodeManager, self).__init__()

        self.registry = weakref.WeakValueDictionary()

        if nslots > 0:
            cache = lrucacheextension.NodeCache(nslots)
        elif nslots == 0:
            cache = _NoCache()
        else:
            # nslots < 0
            cache = _DictCache(-nslots)

        self.cache = cache

        # node_factory(node_path)
        self.node_factory = node_factory

    def register_node(self, node, key):
        if key is None:
            key = node._v_pathname

        if key in self.registry:
            if not self.registry[key]._v_isopen:
                del self.registry[key]
            elif self.registry[key] is not node:
                raise RuntimeError('trying to ragister a node with an '
                                   'existing key: ``%s``' % key)
        else:
            self.registry[key] = node

    def cache_node(self, node, key=None):
        if key is None:
            key = node._v_pathname

        self.register_node(node, key)
        if key in self.cache:
            oldnode = self.cache.pop(key)
            if oldnode is not node and oldnode._v_isopen:
                raise RuntimeError('trying to cache a node with an '
                                   'existing key: ``%s``' % key)

        self.cache[key] = node

    def get_node(self, key):
        node = self.cache.pop(key, None)
        if node is not None:
            if node._v_isopen:
                self.cache_node(node, key)
                return node
            else:
                # this should not happen
                warnings.warn("a closed node found in the cache: ``%s``" % key)

        if key in self.registry:
            node = self.registry[key]
            if node is None:
                # this should not happen since WeakValueDictionary drops all
                # dead weakrefs
                warnings.warn("None is stored in the registry for key: "
                              "``%s``" % key)
            elif node._v_isopen:
                self.cache_node(node, key)
                return node
            else:
                # this should not happen
                warnings.warn("a closed node found in the registry: "
                              "``%s``" % key)
                del self.registry[key]
                node = None

        if self.node_factory:
            node = self.node_factory(key)
            self.cache_node(node, key)

        return node

    def rename_node(self, oldkey, newkey):
        for cache in (self.cache, self.registry):
            if oldkey in cache:
                node = cache.pop(oldkey)
                cache[newkey] = node

    def drop_from_cache(self, nodepath):
        '''Remove the node from cache'''

        # Remove the node from the cache.
        self.cache.pop(nodepath, None)

    def drop_node(self, node, check_unregistered=True):
        """Drop the `node`.

        Remove the node from the cache and, if it has no more references,
        close it.

        """

        # Remove all references to the node.
        nodepath = node._v_pathname

        self.drop_from_cache(nodepath)

        if nodepath in self.registry:
            if not node._v_isopen:
                del self.registry[nodepath]
        elif check_unregistered:
            # If the node is not in the registry (this should never happen)
            # we close it forcibly since it is not ensured that the __del__
            # method is called for object that are still alive when the
            # interpreter is shut down
            if node._v_isopen:
                warnings.warn("dropping a node that is not in the registry: "
                              "``%s``" % nodepath)

                node._g_pre_kill_hook()
                node._f_close()

    def flush_nodes(self):
        # Only iter on the nodes in the registry since nodes in the cahce
        # should always have an entry in the registry
        closed_keys = []
        for path, node in self.registry.items():
            if not node._v_isopen:
                closed_keys.append(path)
            elif '/_i_' not in path:  # Indexes are not necessary to be flushed
                if isinstance(node, Leaf):
                    node.flush()

        for path in closed_keys:
            # self.cache.pop(path, None)
            if path in self.cache:
                warnings.warn("closed node the cache: ``%s``" % path)
                self.cache.pop(path, None)
            self.registry.pop(path)

    @staticmethod
    def _close_nodes(nodepaths, get_node):
        for nodepath in nodepaths:
            try:
                node = get_node(nodepath)
            except KeyError:
                pass
            else:
                if not node._v_isopen or node._v__deleting:
                    continue

                try:
                    # Avoid descendent nodes to also iterate over
                    # their descendents, which are already to be
                    # closed by this loop.
                    if hasattr(node, '_f_get_child'):
                        node._g_close()
                    else:
                        node._f_close()
                    del node
                except ClosedNodeError:
                    #import traceback
                    #type_, value, tb = sys.exc_info()
                    #exception_dump = ''.join(
                    #    traceback.format_exception(type_, value, tb))
                    #warnings.warn(
                    #    "A '%s' exception occurred trying to close a node "
                    #    "that was supposed to be open.\n"
                    #    "%s" % (type_.__name__, exception_dump))
                    pass

    def close_subtree(self, prefix='/'):
        if not prefix.endswith('/'):
            prefix = prefix + '/'

        cache = self.cache
        registry = self.registry

        # Ensure tables are closed before their indices
        paths = [
            path for path in cache
            if path.startswith(prefix) and '/_i_' not in path
        ]
        self._close_nodes(paths, cache.pop)

        # Close everything else (i.e. indices)
        paths = [path for path in cache if path.startswith(prefix)]
        self._close_nodes(paths, cache.pop)

        # Ensure tables are closed before their indices
        paths = [
            path for path in registry
            if path.startswith(prefix) and '/_i_' not in path
        ]
        self._close_nodes(paths, registry.pop)

        # Close everything else (i.e. indices)
        paths = [path for path in registry if path.startswith(prefix)]
        self._close_nodes(paths, registry.pop)

    def shutdown(self):
        registry = self.registry
        cache = self.cache

        #self.close_subtree('/')

        keys = list(cache)  # copy
        for key in keys:
            node = cache.pop(key)
            if node._v_isopen:
                registry.pop(node._v_pathname, None)
                node._f_close()

        while registry:
            key, node = registry.popitem()
            if node._v_isopen:
                node._f_close()


class File(hdf5extension.File, object):
    """The in-memory representation of a PyTables file.

    An instance of this class is returned when a PyTables file is
    opened with the :func:`tables.open_file` function. It offers methods
    to manipulate (create, rename, delete...) nodes and handle their
    attributes, as well as methods to traverse the object tree.
    The *user entry point* to the object tree attached to the HDF5 file
    is represented in the root_uep attribute.
    Other attributes are available.

    File objects support an *Undo/Redo mechanism* which can be enabled
    with the :meth:`File.enable_undo` method. Once the Undo/Redo
    mechanism is enabled, explicit *marks* (with an optional unique
    name) can be set on the state of the database using the
    :meth:`File.mark`
    method. There are two implicit marks which are always available:
    the initial mark (0) and the final mark (-1).  Both the identifier
    of a mark and its name can be used in *undo* and *redo* operations.

    Hierarchy manipulation operations (node creation, movement and
    removal) and attribute handling operations (setting and deleting)
    made after a mark can be undone by using the :meth:`File.undo`
    method, which returns the database to the state of a past mark.
    If undo() is not followed by operations that modify the hierarchy
    or attributes, the :meth:`File.redo` method can be used to return
    the database to the state of a future mark. Else, future states of
    the database are forgotten.

    Note that data handling operations can not be undone nor redone by
    now. Also, hierarchy manipulation operations on nodes that do not
    support the Undo/Redo mechanism issue an UndoRedoWarning *before*
    changing the database.

    The Undo/Redo mechanism is persistent between sessions and can
    only be disabled by calling the :meth:`File.disable_undo` method.

    File objects can also act as context managers when using the with
    statement introduced in Python 2.5.  When exiting a context, the
    file is automatically closed.

    Parameters
    ----------
    filename : str
        The name of the file (supports environment variable expansion).
        It is suggested that file names have any of the .h5, .hdf or
        .hdf5 extensions, although this is not mandatory.

    mode : str
        The mode to open the file. It can be one of the
        following:

            * *'r'*: Read-only; no data can be modified.
            * *'w'*: Write; a new file is created (an existing file
              with the same name would be deleted).
            * *'a'*: Append; an existing file is opened for reading
              and writing, and if the file does not exist it is created.
            * *'r+'*: It is similar to 'a', but the file must already
              exist.

    title : str
        If the file is to be created, a TITLE string attribute will be
        set on the root group with the given value. Otherwise, the
        title will be read from disk, and this will not have any effect.

    root_uep : str
        The root User Entry Point. This is a group in the HDF5 hierarchy
        which will be taken as the starting point to create the object
        tree. It can be whatever existing group in the file, named by
        its HDF5 path. If it does not exist, an HDF5ExtError is issued.
        Use this if you do not want to build the *entire* object tree,
        but rather only a *subtree* of it.

        .. versionchanged:: 3.0
           The *rootUEP* parameter has been renamed into *root_uep*.

    filters : Filters
        An instance of the Filters (see :ref:`FiltersClassDescr`) class that
        provides information about the desired I/O filters applicable to the
        leaves that hang directly from the *root group*, unless other filter
        properties are specified for these leaves. Besides, if you do not
        specify filter properties for child groups, they will inherit these
        ones, which will in turn propagate to child nodes.

    Notes
    -----
    In addition, it recognizes the (lowercase) names of parameters
    present in :file:`tables/parameters.py` as additional keyword
    arguments.
    See :ref:`parameter_files` for a detailed info on the supported
    parameters.


    .. rubric:: File attributes

    .. attribute:: filename

        The name of the opened file.

    .. attribute:: format_version

        The PyTables version number of this file.

    .. attribute:: isopen

        True if the underlying file is open, false otherwise.

    .. attribute:: mode

        The mode in which the file was opened.

    .. attribute:: root

        The *root* of the object tree hierarchy (a Group instance).

    .. attribute:: root_uep

        The UEP (user entry point) group name in the file (see
        the :func:`open_file` function).

        .. versionchanged:: 3.0
           The *rootUEP* attribute has been renamed into *root_uep*.

    """

    ## <class variables>
    # The top level kinds. Group must go first!
    _node_kinds = ('Group', 'Leaf', 'Link', 'Unknown')
    rootUEP = previous_api_property('root_uep')
    _v_objectId = previous_api_property('_v_objectid')

    ## </class variables>

    ## <properties>

    def _gettitle(self):
        return self.root._v_title

    def _settitle(self, title):
        self.root._v_title = title

    def _deltitle(self):
        del self.root._v_title

    title = property(
        _gettitle, _settitle, _deltitle,
        "The title of the root group in the file.")

    def _getfilters(self):
        return self.root._v_filters

    def _setfilters(self, filters):
        self.root._v_filters = filters

    def _delfilters(self):
        del self.root._v_filters

    filters = property(
        _getfilters, _setfilters, _delfilters,
        ("Default filter properties for the root group "
         "(see :ref:`FiltersClassDescr`)."))

    open_count = property(
        lambda self: self._open_count, None, None,
        """The number of times this file handle has been opened.

        .. versionchanged:: 3.1
           The mechanism for caching and sharing file handles has been
           removed in PyTables 3.1.  Now this property should always
           be 1 (or 0 for closed files).

        .. deprecated:: 3.1

        """)

    ## </properties>

    def __init__(self, filename, mode="r", title="",
                 root_uep="/", filters=None, **kwargs):

        self.filename = filename
        """The name of the opened file."""

        self.mode = mode
        """The mode in which the file was opened."""

        if mode not in ('r', 'r+', 'a', 'w'):
            raise ValueError("invalid mode string ``%s``. Allowed modes are: "
                             "'r', 'r+', 'a' and 'w'" % mode)

        # Get all the parameters in parameter file(s)
        params = dict([(k, v) for k, v in parameters.__dict__.iteritems()
                       if k.isupper() and not k.startswith('_')])
        # Update them with possible keyword arguments
        if [k for k in kwargs if k.isupper()]:
            warnings.warn("The use of uppercase keyword parameters is "
                          "deprecated", DeprecationWarning)

        kwargs = dict([(k.upper(), v) for k, v in kwargs.iteritems()])
        params.update(kwargs)

        # If MAX_ * _THREADS is not set yet, set it to the number of cores
        # on this machine.

        if params['MAX_NUMEXPR_THREADS'] is None:
            params['MAX_NUMEXPR_THREADS'] = detect_number_of_cores()

        if params['MAX_BLOSC_THREADS'] is None:
            params['MAX_BLOSC_THREADS'] = detect_number_of_cores()

        self.params = params

        # Now, it is time to initialize the File extension
        self._g_new(filename, mode, **params)

        # Check filters and set PyTables format version for new files.
        new = self._v_new
        if new:
            _checkfilters(filters)
            self.format_version = format_version
            """The PyTables version number of this file."""

        # The node manager must be initialized before the root group
        # initialization but the node_factory attribute is set onl later
        # because it is a bount method of the root grop itself.
        node_cache_slots = params['NODE_CACHE_SLOTS']
        self._node_manager = NodeManager(nslots=node_cache_slots)

        # For the moment Undo/Redo is not enabled.
        self._undoEnabled = False

        # Set the flag to indicate that the file has been opened.
        # It must be set before opening the root group
        # to allow some basic access to its attributes.
        self.isopen = 1
        """True if the underlying file os open, False otherwise."""

        # Append the name of the file to the global dict of files opened.
        _open_files.add(self)

        # Set the number of times this file has been opened to 1
        self._open_count = 1

        # Get the root group from this file
        self.root = root = self.__get_root_group(root_uep, title, filters)
        """The *root* of the object tree hierarchy (a Group instance)."""
        # Complete the creation of the root node
        # (see the explanation in ``RootGroup.__init__()``.
        root._g_post_init_hook()
        self._node_manager.node_factory = self.root._g_load_child

        # Save the PyTables format version for this file.
        if new:
            if params['PYTABLES_SYS_ATTRS']:
                root._v_attrs._g__setattr(
                    'PYTABLES_FORMAT_VERSION', format_version)

        # If the file is old, and not opened in "read-only" mode,
        # check if it has a transaction log
        if not new and self.mode != "r" and _trans_group_path in self:
            # It does. Enable the undo.
            self.enable_undo()

        # Set the maximum number of threads for Numexpr
        numexpr.set_vml_num_threads(params['MAX_NUMEXPR_THREADS'])

    def __get_root_group(self, root_uep, title, filters):
        """Returns a Group instance which will act as the root group in the
        hierarchical tree.

        If file is opened in "r", "r+" or "a" mode, and the file already
        exists, this method dynamically builds a python object tree
        emulating the structure present on file.

        """

        self._v_objectid = self._get_file_id()

        if root_uep in [None, ""]:
            root_uep = "/"
        # Save the User Entry Point in a variable class
        self.root_uep = root_uep

        new = self._v_new

        # Get format version *before* getting the object tree
        if not new:
            # Firstly, get the PyTables format version for this file
            self.format_version = utilsextension.read_f_attr(
                self._v_objectid, 'PYTABLES_FORMAT_VERSION')
            if not self.format_version:
                # PYTABLES_FORMAT_VERSION attribute is not present
                self.format_version = "unknown"
                self._isPTFile = False
            elif not isinstance(self.format_version, str):
                # system attributes should always be str
                if sys.version_info[0] < 3:
                    self.format_version = self.format_version.encode()
                else:
                    self.format_version = self.format_version.decode('utf-8')

        # Create new attributes for the root Group instance and
        # create the object tree
        return RootGroup(self, root_uep, title=title, new=new, filters=filters)

    __getRootGroup = previous_api(__get_root_group)

    def _get_or_create_path(self, path, create):
        """Get the given `path` or create it if `create` is true.

        If `create` is true, `path` *must* be a string path and not a
        node, otherwise a `TypeError`will be raised.

        """

        if create:
            return self._create_path(path)
        else:
            return self.get_node(path)

    _getOrCreatePath = previous_api(_get_or_create_path)

    def _create_path(self, path):
        """Create the groups needed for the `path` to exist.

        The group associated with the given `path` is returned.

        """

        if not hasattr(path, 'split'):
            raise TypeError("when creating parents, parent must be a path")

        if path == '/':
            return self.root

        parent, create_group = self.root, self.create_group
        for pcomp in path.split('/')[1:]:
            try:
                child = parent._f_get_child(pcomp)
            except NoSuchNodeError:
                child = create_group(parent, pcomp)
            parent = child
        return parent

    _createPath = previous_api(_create_path)

    def create_group(self, where, name, title="", filters=None,
                     createparents=False):
        """Create a new group.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new group will hang. It can be a
            path string (for example '/level1/leaf5'), or a Group instance
            (see :ref:`GroupClassDescr`).
        name : str
            The name of the new group.
        title : str, optional
            A description for this node (it sets the TITLE HDF5 attribute on
            disk).
        filters : Filters
            An instance of the Filters class (see :ref:`FiltersClassDescr`)
            that provides information about the desired I/O filters applicable
            to the leaves that hang directly from this new group (unless other
            filter properties are specified for these leaves). Besides, if you
            do not specify filter properties for its child groups, they will
            inherit these ones.
        createparents : bool
            Whether to create the needed groups for the parent
            path to exist (not done by default).

        See Also
        --------
        Group : for more information on groups

        """

        parentnode = self._get_or_create_path(where, createparents)
        _checkfilters(filters)
        return Group(parentnode, name,
                     title=title, new=True, filters=filters)

    createGroup = previous_api(create_group)

    def create_table(self, where, name, description=None, title="",
                     filters=None, expectedrows=10000,
                     chunkshape=None, byteorder=None,
                     createparents=False, obj=None):
        """Create a new table with the given name in where location.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new table will hang. It can be a
            path string (for example '/level1/leaf5'), or a Group instance
            (see :ref:`GroupClassDescr`).
        name : str
            The name of the new table.
        description : Description
            This is an object that describes the table, i.e. how
            many columns it has, their names, types, shapes, etc.  It
            can be any of the following:

                * *A user-defined class*: This should inherit from the
                  IsDescription class (see :ref:`IsDescriptionClassDescr`)
                  where table fields are specified.
                * *A dictionary*: For example, when you do not know
                  beforehand which structure your table will have).
                * *A Description instance*: You can use the description
                  attribute of another table to create a new one with the
                  same structure.
                * *A NumPy dtype*: A completely general structured NumPy
                  dtype.
                * *A NumPy (structured) array instance*: The dtype of
                  this structured array will be used as the description.
                  Also, in case the array has actual data, it will be
                  injected into the newly created table.

            .. versionchanged:: 3.0
               The *description* parameter can be None (default) if *obj* is
               provided.  In that case the structure of the table is deduced
               by *obj*.

        title : str
            A description for this node (it sets the TITLE HDF5 attribute
            on disk).
        filters : Filters
            An instance of the Filters class (see :ref:`FiltersClassDescr`)
            that provides information about the desired I/O filters to be
            applied during the life of this object.
        expectedrows : int
            A user estimate of the number of records that will be in the table.
            If not provided, the default value is EXPECTED_ROWS_TABLE (see
            :file:`tables/parameters.py`). If you plan to create a bigger
            table try providing a guess; this will optimize the HDF5 B-Tree
            creation and management process time and memory used.
        chunkshape
            The shape of the data chunk to be read or written in a
            single HDF5 I/O operation. Filters are applied to those
            chunks of data. The rank of the chunkshape for tables must
            be 1. If None, a sensible value is calculated based on the
            expectedrows parameter (which is recommended).
        byteorder : str
            The byteorder of data *on disk*, specified as 'little' or 'big'.
            If this is not specified, the byteorder is that of the platform,
            unless you passed an array as the description, in which case
            its byteorder will be used.
        createparents : bool
            Whether to create the needed groups for the parent path to exist
            (not done by default).
        obj : python object
            The recarray to be saved.  Accepted types are NumPy record
            arrays, as well as native Python sequences convertible to numpy
            record arrays.

            The *obj* parameter is optional and it can be provided in
            alternative to the *description* parameter.
            If both *obj* and *description* are provided they must
            be consistent with each other.

            .. versionadded:: 3.0

        See Also
        --------
        Table : for more information on tables

        """

        if obj is not None:
            if not isinstance(obj, numpy.ndarray):
                raise TypeError('invalid obj parameter %r' % obj)

            descr, _ = descr_from_dtype(obj.dtype)
            if (description is not None and
                    dtype_from_descr(description) != obj.dtype):
                raise TypeError('the desctiption parameter is not consistent '
                                'with the data type of the obj parameter')
            elif description is None:
                description = descr

        parentnode = self._get_or_create_path(where, createparents)
        if description is None:
            raise ValueError("invalid table description: None")
        _checkfilters(filters)

        ptobj = Table(parentnode, name,
                      description=description, title=title,
                      filters=filters, expectedrows=expectedrows,
                      chunkshape=chunkshape, byteorder=byteorder)

        if obj is not None:
            ptobj.append(obj)

        return ptobj

    createTable = previous_api(create_table)

    def create_array(self, where, name, obj=None, title="",
                     byteorder=None, createparents=False,
                     atom=None, shape=None):
        """Create a new array.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new array will hang. It can be a
            path string (for example '/level1/leaf5'), or a Group instance
            (see :ref:`GroupClassDescr`).
        name : str
            The name of the new array
        obj : python object
            The array or scalar to be saved.  Accepted types are NumPy
            arrays and scalars, as well as native Python sequences and
            scalars, provided that values are regular (i.e. they are
            not like ``[[1,2],2]``) and homogeneous (i.e. all the
            elements are of the same type).

            Also, objects that have some of their dimensions equal to 0
            are not supported (use an EArray node (see
            :ref:`EArrayClassDescr`) if you want to store an array with
            one of its dimensions equal to 0).

            .. versionchanged:: 3.0
               The *Object parameter has been renamed into *obj*.*

        title : str
            A description for this node (it sets the TITLE HDF5 attribute on
            disk).
        byteorder : str
            The byteorder of the data *on disk*, specified as 'little' or
            'big'.  If this is not specified, the byteorder is that of the
            given object.
        createparents : bool, optional
            Whether to create the needed groups for the parent path to exist
            (not done by default).
        atom : Atom
            An Atom (see :ref:`AtomClassDescr`) instance representing
            the *type* and *shape* of the atomic objects to be saved.

            .. versionadded:: 3.0

        shape : tuple of ints
            The shape of the stored array.

            .. versionadded:: 3.0

        See Also
        --------
        Array : for more information on arrays
        create_table : for more information on the rest of parameters

        """

        if obj is None:
            if atom is None or shape is None:
                raise TypeError('if the obj parameter is not specified '
                                '(or None) then both the atom and shape '
                                'parametes should be provided.')
            else:
                # Making strides=(0,...) below is a trick to create the
                # array fast and without memory consumption
                dflt = numpy.zeros((), dtype=atom.dtype)
                obj = numpy.ndarray(shape, dtype=atom.dtype, buffer=dflt,
                                    strides=(0,)*len(shape))
        else:
            flavor = flavor_of(obj)
            # use a temporary object because converting obj at this stage
            # breaks some test. This is soultion performs a double,
            # potentially expensive, conversion of the obj parameter.
            _obj = array_as_internal(obj, flavor)

            if shape is not None and shape != _obj.shape:
                raise TypeError('the shape parameter do not match obj.shape')

            if atom is not None and atom.dtype != _obj.dtype:
                raise TypeError('the atom parameter is not consistent with '
                                'the data type of the obj parameter')

        parentnode = self._get_or_create_path(where, createparents)
        return Array(parentnode, name,
                     obj=obj, title=title, byteorder=byteorder)

    createArray = previous_api(create_array)

    def create_carray(self, where, name, atom=None, shape=None, title="",
                      filters=None, chunkshape=None,
                      byteorder=None, createparents=False, obj=None):
        """Create a new chunked array.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new array will hang. It can
            be a path string (for example '/level1/leaf5'), or a Group
            instance (see :ref:`GroupClassDescr`).
        name : str
            The name of the new array
        atom : Atom
            An Atom (see :ref:`AtomClassDescr`) instance representing
            the *type* and *shape* of the atomic objects to be saved.

            .. versionchanged:: 3.0
               The *atom* parameter can be None (default) if *obj* is
               provided.

        shape : tuple
            The shape of the new array.

            .. versionchanged:: 3.0
               The *shape* parameter can be None (default) if *obj* is
               provided.

        title : str, optional
            A description for this node (it sets the TITLE HDF5 attribute
            on disk).
        filters : Filters, optional
            An instance of the Filters class (see :ref:`FiltersClassDescr`)
            that provides information about the desired I/O filters to
            be applied during the life of this object.
        chunkshape : tuple or number or None, optional
            The shape of the data chunk to be read or written in a
            single HDF5 I/O operation.  Filters are applied to those
            chunks of data.  The dimensionality of chunkshape must be
            the same as that of shape.  If None, a sensible value is
            calculated (which is recommended).
        byteorder : str, optional
            The byteorder of the data *on disk*, specified as 'little'
            or 'big'.  If this is not specified, the byteorder is that
            of the given object.
        createparents : bool, optional
            Whether to create the needed groups for the parent path to
            exist (not done by default).
        obj : python object
            The array or scalar to be saved.  Accepted types are NumPy
            arrays and scalars, as well as native Python sequences and
            scalars, provided that values are regular (i.e. they are
            not like ``[[1,2],2]``) and homogeneous (i.e. all the
            elements are of the same type).

            Also, objects that have some of their dimensions equal to 0
            are not supported. Please use an EArray node (see
            :ref:`EArrayClassDescr`) if you want to store an array with
            one of its dimensions equal to 0.

            The *obj* parameter is optional and it can be provided in
            alternative to the *atom* and *shape* parameters.
            If both *obj* and *atom* and/or *shape* are provided they must
            be consistent with each other.

            .. versionadded:: 3.0

        See Also
        --------
        CArray : for more information on chunked arrays

        """

        if obj is not None:
            flavor = flavor_of(obj)
            obj = array_as_internal(obj, flavor)

            if shape is not None and shape != obj.shape:
                raise TypeError('the shape parameter do not match obj.shape')
            else:
                shape = obj.shape

            if atom is not None and atom.dtype != obj.dtype:
                raise TypeError('the atom parameter is not consistent with '
                                'the data type of the obj parameter')
            elif atom is None:
                atom = Atom.from_dtype(obj.dtype)

        parentnode = self._get_or_create_path(where, createparents)
        _checkfilters(filters)
        ptobj = CArray(parentnode, name,
                       atom=atom, shape=shape, title=title, filters=filters,
                       chunkshape=chunkshape, byteorder=byteorder)

        if obj is not None:
            ptobj[...] = obj

        return ptobj

    createCArray = previous_api(create_carray)

    def create_earray(self, where, name, atom=None, shape=None, title="",
                      filters=None, expectedrows=1000,
                      chunkshape=None, byteorder=None,
                      createparents=False, obj=None):
        """Create a new enlargeable array.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new array will hang. It can be a
            path string (for example '/level1/leaf5'), or a Group instance
            (see :ref:`GroupClassDescr`).
        name : str
            The name of the new array
        atom : Atom
            An Atom (see :ref:`AtomClassDescr`) instance representing the
            *type* and *shape* of the atomic objects to be saved.

            .. versionchanged:: 3.0
               The *atom* parameter can be None (default) if *obj* is
               provided.

        shape : tuple
            The shape of the new array.  One (and only one) of the shape
            dimensions *must* be 0.  The dimension being 0 means that the
            resulting EArray object can be extended along it.  Multiple
            enlargeable dimensions are not supported right now.

            .. versionchanged:: 3.0
               The *shape* parameter can be None (default) if *obj* is
               provided.

        title : str, optional
            A description for this node (it sets the TITLE HDF5 attribute on
            disk).
        expectedrows : int, optional
            A user estimate about the number of row elements that will be added
            to the growable dimension in the EArray node.  If not provided, the
            default value is EXPECTED_ROWS_EARRAY (see tables/parameters.py).
            If you plan to create either a much smaller or a much bigger array
            try providing a guess; this will optimize the HDF5 B-Tree creation
            and management process time and the amount of memory used.
        chunkshape : tuple, numeric, or None, optional
            The shape of the data chunk to be read or written in a single HDF5
            I/O operation.  Filters are applied to those chunks of data.  The
            dimensionality of chunkshape must be the same as that of shape
            (beware: no dimension should be 0 this time!).  If None, a sensible
            value is calculated based on the expectedrows parameter (which is
            recommended).
        byteorder : str, optional
            The byteorder of the data *on disk*, specified as 'little' or
            'big'. If this is not specified, the byteorder is that of the
            platform.
        createparents : bool, optional
            Whether to create the needed groups for the parent path to exist
            (not done by default).
        obj : python object
            The array or scalar to be saved.  Accepted types are NumPy
            arrays and scalars, as well as native Python sequences and
            scalars, provided that values are regular (i.e. they are
            not like ``[[1,2],2]``) and homogeneous (i.e. all the
            elements are of the same type).

            The *obj* parameter is optional and it can be provided in
            alternative to the *atom* and *shape* parameters.
            If both *obj* and *atom* and/or *shape* are provided they must
            be consistent with each other.

            .. versionadded:: 3.0

        See Also
        --------
        EArray : for more information on enlargeable arrays

        """

        if obj is not None:
            flavor = flavor_of(obj)
            obj = array_as_internal(obj, flavor)

            earray_shape = (0,) + obj.shape[1:]

            if shape is not None and shape != earray_shape:
                raise TypeError('the shape parameter is not compatible '
                                'with obj.shape.')
            else:
                shape = earray_shape

            if atom is not None and atom.dtype != obj.dtype:
                raise TypeError('the atom parameter is not consistent with '
                                'the data type of the obj parameter')
            elif atom is None:
                atom = Atom.from_dtype(obj.dtype)

        parentnode = self._get_or_create_path(where, createparents)
        _checkfilters(filters)
        ptobj = EArray(parentnode, name,
                       atom=atom, shape=shape, title=title,
                       filters=filters, expectedrows=expectedrows,
                       chunkshape=chunkshape, byteorder=byteorder)

        if obj is not None:
            ptobj.append(obj)

        return ptobj

    createEArray = previous_api(create_earray)

    def create_vlarray(self, where, name, atom=None, title="",
                       filters=None, expectedrows=None,
                       chunkshape=None, byteorder=None,
                       createparents=False, obj=None):
        """Create a new variable-length array.

        Parameters
        ----------
        where : str or Group
            The parent group from which the new array will hang. It can
            be a path string (for example '/level1/leaf5'), or a Group
            instance (see :ref:`GroupClassDescr`).
        name : str
            The name of the new array
        atom : Atom
            An Atom (see :ref:`AtomClassDescr`) instance representing
            the *type* and *shape* of the atomic objects to be saved.

            .. versionchanged:: 3.0
               The *atom* parameter can be None (default) if *obj* is
               provided.

        title : str, optional
            A description for this node (it sets the TITLE HDF5 attribute
            on disk).
        filters : Filters
            An instance of the Filters class (see :ref:`FiltersClassDescr`)
            that provides information about the desired I/O filters to
            be applied during the life of this object.
        expectedrows : int, optional
            A user estimate about the number of row elements that will
            be added to the growable dimension in the `VLArray` node.
            If not provided, the default value is ``EXPECTED_ROWS_VLARRAY``
            (see ``tables/parameters.py``).  If you plan to create either
            a much smaller or a much bigger `VLArray` try providing a guess;
            this will optimize the HDF5 B-Tree creation and management
            process time and the amount of memory used.

            .. versionadded:: 3.0

        chunkshape : int or tuple of int, optional
            The shape of the data chunk to be read or written in a
            single HDF5 I/O operation. Filters are applied to those
            chunks of data. The dimensionality of chunkshape must be 1.
            If None, a sensible value is calculated (which is recommended).
        byteorder : str, optional
            The byteorder of the data *on disk*, specified as 'little' or
            'big'. If this is not specified, the byteorder is that of the
            platform.
        createparents : bool, optional
            Whether to create the needed groups for the parent path to
            exist (not done by default).
        obj : python object
            The array or scalar to be saved.  Accepted types are NumPy
            arrays and scalars, as well as native Python sequences and
            scalars, provided that values are regular (i.e. they are
            not like ``[[1,2],2]``) and homogeneous (i.e. all the
            elements are of the same type).

            The *obj* parameter is optional and it can be provided in
            alternative to the *atom* parameter.
            If both *obj* and *atom* and are provided they must
            be consistent with each other.

            .. versionadded:: 3.0

        See Also
        --------
        VLArray : for more informationon variable-length arrays

        .. versionchanged:: 3.0
           The *expectedsizeinMB* parameter has been replaced by
           *expectedrows*.

        """

        if obj is not None:
            flavor = flavor_of(obj)
            obj = array_as_internal(obj, flavor)

            if atom is not None and atom.dtype != obj.dtype:
                raise TypeError('the atom parameter is not consistent with '
                                'the data type of the obj parameter')
            if atom is None:
                atom = Atom.from_dtype(obj.dtype)
        elif atom is None:
            raise ValueError('atom parameter cannot be None')

        parentnode = self._get_or_create_path(where, createparents)
        _checkfilters(filters)
        ptobj = VLArray(parentnode, name,
                        atom=atom, title=title, filters=filters,
                        expectedrows=expectedrows,
                        chunkshape=chunkshape, byteorder=byteorder)

        if obj is not None:
            ptobj.append(obj)

        return ptobj

    createVLArray = previous_api(create_vlarray)

    def create_hard_link(self, where, name, target, createparents=False):
        """Create a hard link.

        Create a hard link to a `target` node with the given `name` in
        `where` location.  `target` can be a node object or a path
        string.  If `createparents` is true, the intermediate groups
        required for reaching `where` are created (the default is not
        doing so).

        The returned node is a regular `Group` or `Leaf` instance.

        """

        targetnode = self.get_node(target)
        parentnode = self._get_or_create_path(where, createparents)
        linkextension._g_create_hard_link(parentnode, name, targetnode)
        # Refresh children names in link's parent node
        parentnode._g_add_children_names()
        # Return the target node
        return self.get_node(parentnode, name)

    createHardLink = previous_api(create_hard_link)

    def create_soft_link(self, where, name, target, createparents=False):
        """Create a soft link (aka symbolic link) to a `target` node.

        Create a soft link (aka symbolic link) to a `target` nodewith
        the given `name` in `where` location.  `target` can be a node
        object or a path string.  If `createparents` is true, the
        intermediate groups required for reaching `where` are created.

        (the default is not doing so).

        The returned node is a SoftLink instance.  See the SoftLink
        class (in :ref:`SoftLinkClassDescr`) for more information on
        soft links.

        """

        if not isinstance(target, str):
            if hasattr(target, '_v_pathname'):   # quacks like a Node
                target = target._v_pathname
            else:
                raise ValueError(
                    "`target` has to be a string or a node object")
        parentnode = self._get_or_create_path(where, createparents)
        slink = SoftLink(parentnode, name, target)
        # Refresh children names in link's parent node
        parentnode._g_add_children_names()
        return slink

    createSoftLink = previous_api(create_soft_link)

    def create_external_link(self, where, name, target, createparents=False):
        """Create an external link.

        Create an external link to a *target* node with the given *name*
        in *where* location.  *target* can be a node object in another
        file or a path string in the form 'file:/path/to/node'.  If
        *createparents* is true, the intermediate groups required for
        reaching *where* are created (the default is not doing so).

        The returned node is an :class:`ExternalLink` instance.

        """

        if not isinstance(target, str):
            if hasattr(target, '_v_pathname'):   # quacks like a Node
                target = target._v_file.filename + ':' + target._v_pathname
            else:
                raise ValueError(
                    "`target` has to be a string or a node object")
        elif target.find(':/') == -1:
            raise ValueError(
                "`target` must expressed as 'file:/path/to/node'")
        parentnode = self._get_or_create_path(where, createparents)
        elink = ExternalLink(parentnode, name, target)
        # Refresh children names in link's parent node
        parentnode._g_add_children_names()
        return elink

    createExternalLink = previous_api(create_external_link)

    def _get_node(self, nodepath):
        # The root node is always at hand.
        if nodepath == '/':
            return self.root

        node = self._node_manager.get_node(nodepath)
        assert node is not None, "unable to instantiate node ``%s``" % nodepath

        return node

    _getNode = previous_api(_get_node)

    def get_node(self, where, name=None, classname=None):
        """Get the node under where with the given name.

        where can be a Node instance (see :ref:`NodeClassDescr`) or a
        path string leading to a node. If no name is specified, that
        node is returned.

        If a name is specified, this must be a string with the name of
        a node under where.  In this case the where argument can only
        lead to a Group (see :ref:`GroupClassDescr`) instance (else a
        TypeError is raised). The node called name under the group
        where is returned.

        In both cases, if the node to be returned does not exist, a
        NoSuchNodeError is raised. Please note that hidden nodes are
        also considered.

        If the classname argument is specified, it must be the name of
        a class derived from Node. If the node is found but it is not
        an instance of that class, a NoSuchNodeError is also raised.

        """

        self._check_open()

        # For compatibility with old default arguments.
        if name == '':
            name = None

        # Get the parent path (and maybe the node itself).
        if isinstance(where, Node):
            node = where
            node._g_check_open()  # the node object must be open
            nodepath = where._v_pathname
        elif isinstance(where, (basestring, numpy.str_)):
            node = None
            if where.startswith('/'):
                nodepath = where
            else:
                raise NameError(
                    "``where`` must start with a slash ('/')")
        else:
            raise TypeError(
                "``where`` is not a string nor a node: %r" % (where,))

        # Get the name of the child node.
        if name is not None:
            node = None
            nodepath = join_path(nodepath, name)

        assert node is None or node._v_pathname == nodepath

        # Now we have the definitive node path, let us try to get the node.
        if node is None:
            node = self._get_node(nodepath)

        # Finally, check whether the desired node is an instance
        # of the expected class.
        if classname:
            class_ = get_class_by_name(classname)
            if not isinstance(node, class_):
                npathname = node._v_pathname
                nclassname = node.__class__.__name__
                # This error message is right since it can never be shown
                # for ``classname in [None, 'Node']``.
                raise NoSuchNodeError(
                    "could not find a ``%s`` node at ``%s``; "
                    "instead, a ``%s`` node has been found there"
                    % (classname, npathname, nclassname))

        return node

    getNode = previous_api(get_node)

    def is_visible_node(self, path):
        """Is the node under `path` visible?

        If the node does not exist, a NoSuchNodeError is raised.

        """

        # ``util.isvisiblepath()`` is still recommended for internal use.
        return self.get_node(path)._f_isvisible()

    isVisibleNode = previous_api(is_visible_node)

    def rename_node(self, where, newname, name=None, overwrite=False):
        """Change the name of the node specified by where and name to newname.

        Parameters
        ----------
        where, name
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted upon.
        newname : str
            The new name to be assigned to the node (a string).
        overwrite : bool
            Whether to recursively remove a node with the same
            newname if it already exists (not done by default).

        """

        obj = self.get_node(where, name=name)
        obj._f_rename(newname, overwrite)

    renameNode = previous_api(rename_node)

    def move_node(self, where, newparent=None, newname=None, name=None,
                  overwrite=False, createparents=False):
        """Move the node specified by where and name to newparent/newname.

        Parameters
        ----------
        where, name : path
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted upon.
        newparent
            The destination group the node will be moved into (a
            path name or a Group instance). If it is
            not specified or None, the current parent
            group is chosen as the new parent.
        newname
            The new name to be assigned to the node in its
            destination (a string). If it is not specified or
            None, the current name is chosen as the
            new name.

        Notes
        -----
        The other arguments work as in :meth:`Node._f_move`.

        """

        obj = self.get_node(where, name=name)
        obj._f_move(newparent, newname, overwrite, createparents)

    moveNode = previous_api(move_node)

    def copy_node(self, where, newparent=None, newname=None, name=None,
                  overwrite=False, recursive=False, createparents=False,
                  **kwargs):
        """Copy the node specified by where and name to newparent/newname.

        Parameters
        ----------
        where : str
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted
            upon.
        newparent : str or Group
            The destination group that the node will be copied
            into (a path name or a Group
            instance). If not specified or None, the
            current parent group is chosen as the new parent.
        newname : str
            The name to be assigned to the new copy in its
            destination (a string).  If it is not specified or
            None, the current name is chosen as the
            new name.
        name : str
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted
            upon.
        overwrite : bool, optional
            If True, the destination group will be overwritten if it already
            exists.  Defaults to False.
        recursive : bool, optional
            If True, all descendant nodes of srcgroup are recursively copied.
            Defaults to False.
        createparents : bool, optional
            If True, any necessary parents of dstgroup will be created.
            Defaults to False.
        kwargs
           Additional keyword arguments can be used to customize the copying
           process.  See the documentation of :meth:`Group._f_copy`
           for a description of those arguments.

        Returns
        -------
        node : Node
            The newly created copy of the source node (i.e. the destination
            node).  See :meth:`.Node._f_copy` for further details on the
            semantics of copying nodes.

        """

        obj = self.get_node(where, name=name)
        if obj._v_depth == 0 and newparent and not newname:
            npobj = self.get_node(newparent)
            if obj._v_file is not npobj._v_file:
                # Special case for copying file1:/ --> file2:/path
                self.root._f_copy_children(npobj, overwrite=overwrite,
                                           recursive=recursive, **kwargs)
                return npobj
            else:
                raise IOError(
                    "You cannot copy a root group over the same file")
        return obj._f_copy(newparent, newname,
                           overwrite, recursive, createparents, **kwargs)

    copyNode = previous_api(copy_node)

    def remove_node(self, where, name=None, recursive=False):
        """Remove the object node *name* under *where* location.

        Parameters
        ----------
        where, name
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted upon.
        recursive : bool
            If not supplied or false, the node will be removed
            only if it has no children; if it does, a
            NodeError will be raised. If supplied
            with a true value, the node and all its descendants will be
            completely removed.

        """

        obj = self.get_node(where, name=name)
        obj._f_remove(recursive)

    removeNode = previous_api(remove_node)

    def get_node_attr(self, where, attrname, name=None):
        """Get a PyTables attribute from the given node.

        Parameters
        ----------
        where, name
            These arguments work as in :meth:`File.get_node`, referencing the
            node to be acted upon.
        attrname
            The name of the attribute to retrieve.  If the named
            attribute does not exist, an AttributeError is raised.

        """

        obj = self.get_node(where, name=name)
        return obj._f_getattr(attrname)

    getNodeAttr = previous_api(get_node_attr)

    def set_node_attr(self, where, attrname, attrvalue, name=None):
        """Set a PyTables attribute for the given node.

        Parameters
        ----------
        where, name
            These arguments work as in
            :meth:`File.get_node`, referencing the node to be acted upon.
        attrname
            The name of the attribute to set.
        attrvalue
            The value of the attribute to set. Any kind of Python
            object (like strings, ints, floats, lists, tuples, dicts,
            small NumPy objects ...) can be stored as an attribute.
            However, if necessary, pickle is automatically used so as
            to serialize objects that you might want to save.
            See the :class:`AttributeSet` class for details.

        Notes
        -----
        If the node already has a large number of attributes, a
        PerformanceWarning is issued.

        """

        obj = self.get_node(where, name=name)
        obj._f_setattr(attrname, attrvalue)

    setNodeAttr = previous_api(set_node_attr)

    def del_node_attr(self, where, attrname, name=None):
        """Delete a PyTables attribute from the given node.

        Parameters
        ----------
        where, name
            These arguments work as in :meth:`File.get_node`, referencing the
            node to be acted upon.
        attrname
            The name of the attribute to delete.  If the named
            attribute does not exist, an AttributeError is raised.

        """

        obj = self.get_node(where, name=name)
        obj._f_delattr(attrname)

    delNodeAttr = previous_api(del_node_attr)

    def copy_node_attrs(self, where, dstnode, name=None):
        """Copy PyTables attributes from one node to another.

        Parameters
        ----------
        where, name
            These arguments work as in :meth:`File.get_node`, referencing the
            node to be acted upon.
        dstnode
            The destination node where the attributes will be copied to. It can
            be a path string or a Node instance (see :ref:`NodeClassDescr`).

        """

        srcobject = self.get_node(where, name=name)
        dstobject = self.get_node(dstnode)
        srcobject._v_attrs._f_copy(dstobject)

    copyNodeAttrs = previous_api(copy_node_attrs)

    def copy_children(self, srcgroup, dstgroup,
                      overwrite=False, recursive=False,
                      createparents=False, **kwargs):
        """Copy the children of a group into another group.

        Parameters
        ----------
        srcgroup : str
            The group to copy from.
        dstgroup : str
            The destination group.
        overwrite : bool, optional
            If True, the destination group will be overwritten if it already
            exists.  Defaults to False.
        recursive : bool, optional
            If True, all descendant nodes of srcgroup are recursively copied.
            Defaults to False.
        createparents : bool, optional
            If True, any necessary parents of dstgroup will be created.
            Defaults to False.
        kwargs : dict
           Additional keyword arguments can be used to customize the copying
           process.  See the documentation of :meth:`Group._f_copy_children`
           for a description of those arguments.

        """

        srcgroup = self.get_node(srcgroup)  # Does the source node exist?
        self._check_group(srcgroup)  # Is it a group?

        srcgroup._f_copy_children(
            dstgroup, overwrite, recursive, createparents, **kwargs)

    copyChildren = previous_api(copy_children)

    def copy_file(self, dstfilename, overwrite=False, **kwargs):
        """Copy the contents of this file to dstfilename.

        Parameters
        ----------
        dstfilename : str
            A path string indicating the name of the destination file. If
            it already exists, the copy will fail with an IOError, unless
            the overwrite argument is true.
        overwrite : bool, optional
            If true, the destination file will be overwritten if it already
            exists.  In this case, the destination file must be closed, or
            errors will occur.  Defaults to False.
        kwargs
            Additional keyword arguments discussed below.

        Notes
        -----
        Additional keyword arguments may be passed to customize the
        copying process. For instance, title and filters may be changed,
        user attributes may be or may not be copied, data may be
        sub-sampled, stats may be collected, etc. Arguments unknown to
        nodes are simply ignored. Check the documentation for copying
        operations of nodes to see which options they support.

        In addition, it recognizes the names of parameters present in
        :file:`tables/parameters.py` as additional keyword arguments.
        See :ref:`parameter_files` for a detailed info on the supported
        parameters.

        Copying a file usually has the beneficial side effect of
        creating a more compact and cleaner version of the original
        file.

        """

        self._check_open()

        # Check that we are not treading our own shoes
        if os.path.abspath(self.filename) == os.path.abspath(dstfilename):
            raise IOError("You cannot copy a file over itself")

        # Compute default arguments.
        # These are *not* passed on.
        filters = kwargs.pop('filters', None)
        if filters is None:
            # By checking the HDF5 attribute, we avoid setting filters
            # in the destination file if not explicitly set in the
            # source file.  Just by assigning ``self.filters`` we would
            # not be able to tell.
            filters = getattr(self.root._v_attrs, 'FILTERS', None)
        copyuserattrs = kwargs.get('copyuserattrs', True)
        title = kwargs.pop('title', self.title)

        if os.path.isfile(dstfilename) and not overwrite:
            raise IOError(("file ``%s`` already exists; "
                           "you may want to use the ``overwrite`` "
                           "argument") % dstfilename)

        # Create destination file, overwriting it.
        dstfileh = open_file(
            dstfilename, mode="w", title=title, filters=filters, **kwargs)

        try:
            # Maybe copy the user attributes of the root group.
            if copyuserattrs:
                self.root._v_attrs._f_copy(dstfileh.root)

            # Copy the rest of the hierarchy.
            self.root._f_copy_children(dstfileh.root, recursive=True, **kwargs)
        finally:
            dstfileh.close()

    copyFile = previous_api(copy_file)

    def list_nodes(self, where, classname=None):
        """Return a *list* with children nodes hanging from where.

        This is a list-returning version of :meth:`File.iter_nodes`.

        """

        group = self.get_node(where)  # Does the parent exist?
        self._check_group(group)  # Is it a group?

        return group._f_list_nodes(classname)

    listNodes = previous_api(list_nodes)

    def iter_nodes(self, where, classname=None):
        """Iterate over children nodes hanging from where.

        Parameters
        ----------
        where
            This argument works as in :meth:`File.get_node`, referencing the
            node to be acted upon.
        classname
            If the name of a class derived from
            Node (see :ref:`NodeClassDescr`) is supplied, only instances of
            that class (or subclasses of it) will be returned.

        Notes
        -----
        The returned nodes are alphanumerically sorted by their name.
        This is an iterator version of :meth:`File.list_nodes`.

        """

        group = self.get_node(where)  # Does the parent exist?
        self._check_group(group)  # Is it a group?

        return group._f_iter_nodes(classname)

    iterNodes = previous_api(iter_nodes)

    def __contains__(self, path):
        """Is there a node with that path?

        Returns True if the file has a node with the given path (a
        string), False otherwise.

        """

        try:
            self.get_node(path)
        except NoSuchNodeError:
            return False
        else:
            return True

    def __iter__(self):
        """Recursively iterate over the nodes in the tree.

        This is equivalent to calling :meth:`File.walk_nodes` with no
        arguments.

        Examples
        --------

        ::

            # Recursively list all the nodes in the object tree.
            h5file = tables.open_file('vlarray1.h5')
            print("All nodes in the object tree:")
            for node in h5file:
                print(node)

        """

        return self.walk_nodes('/')

    def walk_nodes(self, where="/", classname=None):
        """Recursively iterate over nodes hanging from where.

        Parameters
        ----------
        where : str or Group, optional
            If supplied, the iteration starts from (and includes)
            this group. It can be a path string or a
            Group instance (see :ref:`GroupClassDescr`).
        classname
            If the name of a class derived from
            Node (see :ref:`GroupClassDescr`) is supplied, only instances of
            that class (or subclasses of it) will be returned.

        Notes
        -----
        This version iterates over the leaves in the same group in order
        to avoid having a list referencing to them and thus, preventing
        the LRU cache to remove them after their use.

        Examples
        --------

        ::

            # Recursively print all the nodes hanging from '/detector'.
            print("Nodes hanging from group '/detector':")
            for node in h5file.walk_nodes('/detector', classname='EArray'):
                print(node)

        """

        class_ = get_class_by_name(classname)

        if class_ is Group:  # only groups
            for group in self.walk_groups(where):
                yield group
        elif class_ is Node:  # all nodes
            yield self.get_node(where)
            for group in self.walk_groups(where):
                for leaf in self.iter_nodes(group):
                    yield leaf
        else:  # only nodes of the named type
            for group in self.walk_groups(where):
                for leaf in self.iter_nodes(group, classname):
                    yield leaf

    walkNodes = previous_api(walk_nodes)

    def walk_groups(self, where="/"):
        """Recursively iterate over groups (not leaves) hanging from where.

        The where group itself is listed first (preorder), then each of its
        child groups (following an alphanumerical order) is also traversed,
        following the same procedure.  If where is not supplied, the root
        group is used.

        The where argument can be a path string
        or a Group instance (see :ref:`GroupClassDescr`).

        """

        group = self.get_node(where)  # Does the parent exist?
        self._check_group(group)  # Is it a group?
        return group._f_walk_groups()

    walkGroups = previous_api(walk_groups)

    def _check_open(self):
        """Check the state of the file.

        If the file is closed, a `ClosedFileError` is raised.

        """

        if not self.isopen:
            raise ClosedFileError("the file object is closed")

    _checkOpen = previous_api(_check_open)

    def _iswritable(self):
        """Is this file writable?"""

        return self.mode in ('w', 'a', 'r+')

    _isWritable = previous_api(_iswritable)

    def _check_writable(self):
        """Check whether the file is writable.

        If the file is not writable, a `FileModeError` is raised.

        """

        if not self._iswritable():
            raise FileModeError("the file is not writable")

    _checkWritable = previous_api(_check_writable)

    def _check_group(self, node):
        # `node` must already be a node.
        if not isinstance(node, Group):
            raise TypeError("node ``%s`` is not a group" % (node._v_pathname,))

    _checkGroup = previous_api(_check_group)

    # <Undo/Redo support>
    def is_undo_enabled(self):
        """Is the Undo/Redo mechanism enabled?

        Returns True if the Undo/Redo mechanism has been enabled for
        this file, False otherwise. Please note that this mechanism is
        persistent, so a newly opened PyTables file may already have
        Undo/Redo support enabled.

        """

        self._check_open()
        return self._undoEnabled

    isUndoEnabled = previous_api(is_undo_enabled)

    def _check_undo_enabled(self):
        if not self._undoEnabled:
            raise UndoRedoError("Undo/Redo feature is currently disabled!")

    _checkUndoEnabled = previous_api(_check_undo_enabled)

    def _create_transaction_group(self):
        tgroup = TransactionGroupG(
            self.root, _trans_group_name,
            "Transaction information container", new=True)
        # The format of the transaction container.
        tgroup._v_attrs._g__setattr('FORMATVERSION', _trans_version)
        return tgroup

    _createTransactionGroup = previous_api(_create_transaction_group)

    def _create_transaction(self, troot, tid):
        return TransactionG(
            troot, _trans_name % tid,
            "Transaction number %d" % tid, new=True)

    _createTransaction = previous_api(_create_transaction)

    def _create_mark(self, trans, mid):
        return MarkG(
            trans, _markName % mid,
            "Mark number %d" % mid, new=True)

    _createMark = previous_api(_create_mark)

    def enable_undo(self, filters=Filters(complevel=1)):
        """Enable the Undo/Redo mechanism.

        This operation prepares the database for undoing and redoing
        modifications in the node hierarchy. This
        allows :meth:`File.mark`, :meth:`File.undo`, :meth:`File.redo` and
        other methods to be called.

        The filters argument, when specified,
        must be an instance of class Filters (see :ref:`FiltersClassDescr`) and
        is meant for setting the compression values for the action log. The
        default is having compression enabled, as the gains in terms of
        space can be considerable. You may want to disable compression if
        you want maximum speed for Undo/Redo operations.

        Calling this method when the Undo/Redo mechanism is already
        enabled raises an UndoRedoError.

        """

        maxundo = self.params['MAX_UNDO_PATH_LENGTH']

        class ActionLog(NotLoggedMixin, Table):
            pass

        class ActionLogDesc(IsDescription):
            opcode = UInt8Col(pos=0)
            arg1 = StringCol(maxundo, pos=1, dflt=b"")
            arg2 = StringCol(maxundo, pos=2, dflt=b"")

        self._check_open()

        # Enabling several times is not allowed to avoid the user having
        # the illusion that a new implicit mark has been created
        # when calling enable_undo for the second time.

        if self.is_undo_enabled():
            raise UndoRedoError("Undo/Redo feature is already enabled!")

        self._markers = {}
        self._seqmarkers = []
        self._nmarks = 0
        self._curtransaction = 0
        self._curmark = -1  # No marks yet

        # Get the Group for keeping user actions
        try:
            tgroup = self.get_node(_trans_group_path)
        except NodeError:
            # The file is going to be changed.
            self._check_writable()

            # A transaction log group does not exist. Create it
            tgroup = self._create_transaction_group()

            # Create a transaction.
            self._trans = self._create_transaction(
                tgroup, self._curtransaction)

            # Create an action log
            self._actionlog = ActionLog(
                tgroup, _action_log_name, ActionLogDesc, "Action log",
                filters=filters)

            # Create an implicit mark
            self._actionlog.append([(_op_to_code["MARK"], str(0), '')])
            self._nmarks += 1
            self._seqmarkers.append(0)  # current action is 0

            # Create a group for mark 0
            self._create_mark(self._trans, 0)
            # Initialize the marker pointer
            self._curmark = int(self._nmarks - 1)
            # Initialize the action pointer
            self._curaction = self._actionlog.nrows - 1
        else:
            # The group seems to exist already
            # Get the default transaction
            self._trans = tgroup._f_get_child(
                _trans_name % self._curtransaction)
            # Open the action log and go to the end of it
            self._actionlog = tgroup.actionlog
            for row in self._actionlog:
                if row["opcode"] == _op_to_code["MARK"]:
                    name = row["arg2"].decode('utf-8')
                    self._markers[name] = self._nmarks
                    self._seqmarkers.append(row.nrow)
                    self._nmarks += 1
            # Get the current mark and current action
            self._curmark = int(self._actionlog.attrs.CURMARK)
            self._curaction = self._actionlog.attrs.CURACTION

        # The Undo/Redo mechanism has been enabled.
        self._undoEnabled = True

    enableUndo = previous_api(enable_undo)

    def disable_undo(self):
        """Disable the Undo/Redo mechanism.

        Disabling the Undo/Redo mechanism leaves the database in the
        current state and forgets past and future database states. This
        makes :meth:`File.mark`, :meth:`File.undo`, :meth:`File.redo` and other
        methods fail with an UndoRedoError.

        Calling this method when the Undo/Redo mechanism is already
        disabled raises an UndoRedoError.

        """

        self._check_open()

        if not self.is_undo_enabled():
            raise UndoRedoError("Undo/Redo feature is already disabled!")

        # The file is going to be changed.
        self._check_writable()

        del self._markers
        del self._seqmarkers
        del self._curmark
        del self._curaction
        del self._curtransaction
        del self._nmarks
        del self._actionlog
        # Recursively delete the transaction group
        tnode = self.get_node(_trans_group_path)
        tnode._g_remove(recursive=1)

        # The Undo/Redo mechanism has been disabled.
        self._undoEnabled = False

    disableUndo = previous_api(disable_undo)

    def mark(self, name=None):
        """Mark the state of the database.

        Creates a mark for the current state of the database. A unique (and
        immutable) identifier for the mark is returned. An optional name (a
        string) can be assigned to the mark. Both the identifier of a mark and
        its name can be used in :meth:`File.undo` and :meth:`File.redo`
        operations. When the name has already been used for another mark,
        an UndoRedoError is raised.

        This method can only be called when the Undo/Redo mechanism has been
        enabled. Otherwise, an UndoRedoError is raised.

        """

        self._check_open()
        self._check_undo_enabled()

        if name is None:
            name = ''
        else:
            if not isinstance(name, str):
                raise TypeError("Only strings are allowed as mark names. "
                                "You passed object: '%s'" % name)
            if name in self._markers:
                raise UndoRedoError("Name '%s' is already used as a marker "
                                    "name. Try another one." % name)

            # The file is going to be changed.
            self._check_writable()

            self._markers[name] = self._curmark + 1

        # Create an explicit mark
        # Insert the mark in the action log
        self._log("MARK", str(self._curmark + 1), name)
        self._curmark += 1
        self._nmarks = self._curmark + 1
        self._seqmarkers.append(self._curaction)
        # Create a group for the current mark
        self._create_mark(self._trans, self._curmark)
        return self._curmark

    def _log(self, action, *args):
        """Log an action.

        The `action` must be an all-uppercase string identifying it.
        Arguments must also be strings.

        This method should be called once the action has been completed.

        This method can only be called when the Undo/Redo mechanism has
        been enabled.  Otherwise, an `UndoRedoError` is raised.

        """

        assert self.is_undo_enabled()

        maxundo = self.params['MAX_UNDO_PATH_LENGTH']
        # Check whether we are at the end of the action log or not
        if self._curaction != self._actionlog.nrows - 1:
            # We are not, so delete the trailing actions
            self._actionlog.remove_rows(self._curaction + 1,
                                        self._actionlog.nrows)
            # Reset the current marker group
            mnode = self.get_node(_markPath % (self._curtransaction,
                                               self._curmark))
            mnode._g_reset()
            # Delete the marker groups with backup objects
            for mark in xrange(self._curmark + 1, self._nmarks):
                mnode = self.get_node(_markPath % (self._curtransaction, mark))
                mnode._g_remove(recursive=1)
            # Update the new number of marks
            self._nmarks = self._curmark + 1
            self._seqmarkers = self._seqmarkers[:self._nmarks]

        if action not in _op_to_code:  # INTERNAL
            raise UndoRedoError("Action ``%s`` not in ``_op_to_code`` "
                                "dictionary: %r" % (action, _op_to_code))

        arg1 = ""
        arg2 = ""
        if len(args) <= 1:
            arg1 = args[0]
        elif len(args) <= 2:
            arg1 = args[0]
            arg2 = args[1]
        else:  # INTERNAL
            raise UndoRedoError("Too many parameters for action log: "
                                "%r").with_traceback(args)
        if (len(arg1) > maxundo
                or len(arg2) > maxundo):  # INTERNAL
            raise UndoRedoError("Parameter arg1 or arg2 is too long: "
                                "(%r, %r)" % (arg1, arg2))
        # print("Logging-->", (action, arg1, arg2))
        self._actionlog.append([(_op_to_code[action],
                                 arg1.encode('utf-8'),
                                 arg2.encode('utf-8'))])
        self._curaction += 1

    def _get_mark_id(self, mark):
        """Get an integer markid from a mark sequence number or name."""

        if isinstance(mark, int):
            markid = mark
        elif isinstance(mark, str):
            if mark not in self._markers:
                lmarkers = sorted(self._markers.iterkeys())
                raise UndoRedoError("The mark that you have specified has not "
                                    "been found in the internal marker list: "
                                    "%r" % lmarkers)
            markid = self._markers[mark]
        else:
            raise TypeError("Parameter mark can only be an integer or a "
                            "string, and you passed a type <%s>" % type(mark))
        # print("markid, self._nmarks:", markid, self._nmarks)
        return markid

    _getMarkID = previous_api(_get_mark_id)

    def _get_final_action(self, markid):
        """Get the action to go.

        It does not touch the self private attributes

        """

        if markid > self._nmarks - 1:
            # The required mark is beyond the end of the action log
            # The final action is the last row
            return self._actionlog.nrows
        elif markid <= 0:
            # The required mark is the first one
            # return the first row
            return 0

        return self._seqmarkers[markid]

    _getFinalAction = previous_api(_get_final_action)

    def _doundo(self, finalaction, direction):
        """Undo/Redo actions up to final action in the specificed direction."""

        if direction < 0:
            actionlog = \
                self._actionlog[finalaction + 1:self._curaction + 1][::-1]
        else:
            actionlog = self._actionlog[self._curaction:finalaction]

        # Uncomment this for debugging
#         print("curaction, finalaction, direction", \
#               self._curaction, finalaction, direction)
        for i in xrange(len(actionlog)):
            if actionlog['opcode'][i] != _op_to_code["MARK"]:
                # undo/redo the action
                if direction > 0:
                    # Uncomment this for debugging
#                     print("redo-->", \
#                           _code_to_op[actionlog['opcode'][i]],\
#                           actionlog['arg1'][i],\
#                           actionlog['arg2'][i])
                    undoredo.redo(self,
                                  # _code_to_op[actionlog['opcode'][i]],
                                  # The next is a workaround for python < 2.5
                                  _code_to_op[int(actionlog['opcode'][i])],
                                  actionlog['arg1'][i].decode('utf8'),
                                  actionlog['arg2'][i].decode('utf8'))
                else:
                    # Uncomment this for debugging
                    # print("undo-->", \
                    #       _code_to_op[actionlog['opcode'][i]],\
                    #       actionlog['arg1'][i].decode('utf8'),\
                    #       actionlog['arg2'][i].decode('utf8'))
                    undoredo.undo(self,
                                  # _code_to_op[actionlog['opcode'][i]],
                                  # The next is a workaround for python < 2.5
                                  _code_to_op[int(actionlog['opcode'][i])],
                                  actionlog['arg1'][i].decode('utf8'),
                                  actionlog['arg2'][i].decode('utf8'))
            else:
                if direction > 0:
                    self._curmark = int(actionlog['arg1'][i])
                else:
                    self._curmark = int(actionlog['arg1'][i]) - 1
                    # Protection against negative marks
                    if self._curmark < 0:
                        self._curmark = 0
            self._curaction += direction

    def undo(self, mark=None):
        """Go to a past state of the database.

        Returns the database to the state associated with the specified mark.
        Both the identifier of a mark and its name can be used. If the mark is
        omitted, the last created mark is used. If there are no past
        marks, or the specified mark is not older than the current one, an
        UndoRedoError is raised.

        This method can only be called when the Undo/Redo mechanism
        has been enabled. Otherwise, an UndoRedoError
        is raised.

        """

        self._check_open()
        self._check_undo_enabled()

#         print("(pre)UNDO: (curaction, curmark) = (%s,%s)" % \
#               (self._curaction, self._curmark))
        if mark is None:
            markid = self._curmark
            # Correction if we are settled on top of a mark
            opcode = self._actionlog.cols.opcode
            if opcode[self._curaction] == _op_to_code["MARK"]:
                markid -= 1
        else:
            # Get the mark ID number
            markid = self._get_mark_id(mark)
        # Get the final action ID to go
        finalaction = self._get_final_action(markid)
        if finalaction > self._curaction:
            raise UndoRedoError("Mark ``%s`` is newer than the current mark. "
                                "Use `redo()` or `goto()` instead." % (mark,))

        # The file is going to be changed.
        self._check_writable()

        # Try to reach this mark by unwinding actions in the log
        self._doundo(finalaction - 1, -1)
        if self._curaction < self._actionlog.nrows - 1:
            self._curaction += 1
        self._curmark = int(self._actionlog.cols.arg1[self._curaction])
#         print("(post)UNDO: (curaction, curmark) = (%s,%s)" % \
#               (self._curaction, self._curmark))

    def redo(self, mark=None):
        """Go to a future state of the database.

        Returns the database to the state associated with the specified
        mark.  Both the identifier of a mark and its name can be used.
        If the `mark` is omitted, the next created mark is used.  If
        there are no future marks, or the specified mark is not newer
        than the current one, an UndoRedoError is raised.

        This method can only be called when the Undo/Redo mechanism has
        been enabled.  Otherwise, an UndoRedoError is raised.

        """

        self._check_open()
        self._check_undo_enabled()

#         print("(pre)REDO: (curaction, curmark) = (%s, %s)" % \
#               (self._curaction, self._curmark))
        if self._curaction >= self._actionlog.nrows - 1:
            # We are at the end of log, so no action
            return

        if mark is None:
            mark = self._curmark + 1
        elif mark == -1:
            mark = int(self._nmarks)  # Go beyond the mark bounds up to the end
        # Get the mark ID number
        markid = self._get_mark_id(mark)
        finalaction = self._get_final_action(markid)
        if finalaction < self._curaction + 1:
            raise UndoRedoError("Mark ``%s`` is older than the current mark. "
                                "Use `redo()` or `goto()` instead." % (mark,))

        # The file is going to be changed.
        self._check_writable()

        # Get the final action ID to go
        self._curaction += 1

        # Try to reach this mark by redoing the actions in the log
        self._doundo(finalaction, 1)
        # Increment the current mark only if we are not at the end of marks
        if self._curmark < self._nmarks - 1:
            self._curmark += 1
        if self._curaction > self._actionlog.nrows - 1:
            self._curaction = self._actionlog.nrows - 1
#         print("(post)REDO: (curaction, curmark) = (%s,%s)" % \
#               (self._curaction, self._curmark))

    def goto(self, mark):
        """Go to a specific mark of the database.

        Returns the database to the state associated with the specified mark.
        Both the identifier of a mark and its name can be used.

        This method can only be called when the Undo/Redo mechanism has been
        enabled. Otherwise, an UndoRedoError is raised.

        """

        self._check_open()
        self._check_undo_enabled()

        if mark == -1:  # Special case
            mark = self._nmarks  # Go beyond the mark bounds up to the end
        # Get the mark ID number
        markid = self._get_mark_id(mark)
        finalaction = self._get_final_action(markid)
        if finalaction < self._curaction:
            self.undo(mark)
        else:
            self.redo(mark)

    def get_current_mark(self):
        """Get the identifier of the current mark.

        Returns the identifier of the current mark. This can be used
        to know the state of a database after an application crash, or to
        get the identifier of the initial implicit mark after a call
        to :meth:`File.enable_undo`.

        This method can only be called when the Undo/Redo mechanism
        has been enabled. Otherwise, an UndoRedoError
        is raised.

        """

        self._check_open()
        self._check_undo_enabled()
        return self._curmark

    getCurrentMark = previous_api(get_current_mark)

    def _shadow_name(self):
        """Compute and return a shadow name.

        Computes the current shadow name according to the current
        transaction, mark and action.  It returns a tuple with the
        shadow parent node and the name of the shadow in it.

        """

        parent = self.get_node(
            _shadow_parent % (self._curtransaction, self._curmark))
        name = _shadow_name % (self._curaction,)

        return (parent, name)

    _shadowName = previous_api(_shadow_name)

    # </Undo/Redo support>

    def flush(self):
        """Flush all the alive leaves in the object tree."""

        self._check_open()

        # Flush the cache to disk
        self._node_manager.flush_nodes()
        self._flush_file(0)  # 0 means local scope, 1 global (virtual) scope

    def close(self):
        """Flush all the alive leaves in object tree and close the file."""

        # If the file is already closed, return immediately
        if not self.isopen:
            return

        # If this file has been opened more than once, decrease the
        # counter and return
        if self._open_count > 1:
            self._open_count -= 1
            return

        filename = self.filename

        if self._undoEnabled and self._iswritable():
            # Save the current mark and current action
            self._actionlog.attrs._g__setattr("CURMARK", self._curmark)
            self._actionlog.attrs._g__setattr("CURACTION", self._curaction)

        # Close all loaded nodes.
        self.root._f_close()

        self._node_manager.shutdown()

        # Post-conditions
        assert len(self._node_manager.cache) == 0, \
            ("cached nodes remain after closing: %s"
                % list(self._node_manager.cache))

        # No other nodes should have been revived.
        assert len(self._node_manager.registry) == 0, \
            ("alive nodes remain after closing: %s"
                % list(self._node_manager.registry))

        # Close the file
        self._close_file()

        # After the objects are disconnected, destroy the
        # object dictionary using the brute force ;-)
        # This should help to the garbage collector
        self.__dict__.clear()

        # Set the flag to indicate that the file is closed
        self.isopen = 0

        # Restore the filename attribute that is used by _FileRegistry
        self.filename = filename

        # Delete the entry from he registry of opened files
        _open_files.remove(self)

    def __enter__(self):
        """Enter a context and return the same file."""

        return self

    def __exit__(self, *exc_info):
        """Exit a context and close the file."""

        self.close()
        return False  # do not hide exceptions

    def __str__(self):
        """Return a short string representation of the object tree.

        Examples
        --------

        ::

            >>> f = tables.open_file('data/test.h5')
            >>> print(f)
            data/test.h5 (File) 'Table Benchmark'
            Last modif.: 'Mon Sep 20 12:40:47 2004'
            Object Tree:
            / (Group) 'Table Benchmark'
            /tuple0 (Table(100,)) 'This is the table title'
            /group0 (Group) ''
            /group0/tuple1 (Table(100,)) 'This is the table title'
            /group0/group1 (Group) ''
            /group0/group1/tuple2 (Table(100,)) 'This is the table title'
            /group0/group1/group2 (Group) ''

        """

        if not self.isopen:
            return "<closed File>"

        # Print all the nodes (Group and Leaf objects) on object tree
        try:
            date = time.asctime(time.localtime(os.stat(self.filename)[8]))
        except OSError:
            # in-memory file
            date = ""
        astring = self.filename + ' (File) ' + repr(self.title) + '\n'
#         astring += 'root_uep :=' + repr(self.root_uep) + '; '
#         astring += 'format_version := ' + self.format_version + '\n'
#         astring += 'filters :=' + repr(self.filters) + '\n'
        astring += 'Last modif.: ' + repr(date) + '\n'
        astring += 'Object Tree: \n'

        for group in self.walk_groups("/"):
            astring += str(group) + '\n'
            for kind in self._node_kinds[1:]:
                for node in self.list_nodes(group, kind):
                    astring += str(node) + '\n'
        return astring

    def __repr__(self):
        """Return a detailed string representation of the object tree."""

        if not self.isopen:
            return "<closed File>"

        # Print all the nodes (Group and Leaf objects) on object tree
        astring = 'File(filename=' + str(self.filename) + \
                  ', title=' + repr(self.title) + \
                  ', mode=' + repr(self.mode) + \
                  ', root_uep=' + repr(self.root_uep) + \
                  ', filters=' + repr(self.filters) + \
                  ')\n'
        for group in self.walk_groups("/"):
            astring += str(group) + '\n'
            for kind in self._node_kinds[1:]:
                for node in self.list_nodes(group, kind):
                    astring += repr(node) + '\n'
        return astring

    def _update_node_locations(self, oldpath, newpath):
        """Update location information of nodes under `oldpath`.

        This only affects *already loaded* nodes.

        """

        oldprefix = oldpath + '/'  # root node can not be renamed, anyway
        oldprefix_len = len(oldprefix)

        # Update alive and dead descendents.
        for cache in [self._node_manager.cache, self._node_manager.registry]:
            for nodepath in cache:
                if nodepath.startswith(oldprefix) and nodepath != oldprefix:
                    nodesuffix = nodepath[oldprefix_len:]
                    newnodepath = join_path(newpath, nodesuffix)
                    newnodeppath = split_path(newnodepath)[0]
                    descendent_node = self._get_node(nodepath)
                    descendent_node._g_update_location(newnodeppath)

    _updateNodeLocations = previous_api(_update_node_locations)


# If a user hits ^C during a run, it is wise to gracefully close the
# opened files.
import atexit
atexit.register(_open_files.close_all)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = filters
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2007-02-23
# Author: Ivan Vilata i Balaguer - ivan at selidor dot net
#
# $Id$
#
########################################################################

"""Functionality related with filters in a PyTables file."""

# Imports
# =======
import warnings
import numpy

from tables import (
    utilsextension, blosc_compressor_list, blosc_compcode_to_compname)
from tables.exceptions import FiltersWarning


# Public variables
# ================
__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""

all_complibs = ['zlib', 'lzo', 'bzip2', 'blosc']
all_complibs += ['blosc:%s' % cname for cname in blosc_compressor_list()]


"""List of all compression libraries."""

foreign_complibs = ['szip']
"""List of known but unsupported compression libraries."""

default_complib = 'zlib'
"""The default compression library."""


# Private variables
# =================
_shuffle_flag = 0x1
_fletcher32_flag = 0x2
_rounding_flag = 0x4


# Classes
# =======
class Filters(object):
    """Container for filter properties.

    This class is meant to serve as a container that keeps information about
    the filter properties associated with the chunked leaves, that is Table,
    CArray, EArray and VLArray.

    Instances of this class can be directly compared for equality.

    Parameters
    ----------
    complevel : int
        Specifies a compression level for data. The allowed
        range is 0-9. A value of 0 (the default) disables
        compression.
    complib : str
        Specifies the compression library to be used. Right now, 'zlib' (the
        default), 'lzo', 'bzip2' and 'blosc' are supported.  Additional
        compressors for Blosc like 'blosc:blosclz' ('blosclz' is the default
        in case the additional compressor is not specified), 'blosc:lz4',
        'blosc:lz4hc', 'blosc:snappy' and 'blosc:zlib' are supported too.
        Specifying a compression library which is not available in the
        system issues a FiltersWarning and sets the library to the default
        one.
    shuffle : bool
        Whether or not to use the *Shuffle*
        filter in the HDF5 library. This is normally used to improve
        the compression ratio. A false value disables shuffling and
        a true one enables it. The default value depends on whether
        compression is enabled or not; if compression is enabled,
        shuffling defaults to be enabled, else shuffling is
        disabled. Shuffling can only be used when compression is enabled.
    fletcher32 : bool
        Whether or not to use the
        *Fletcher32* filter in the HDF5 library.
        This is used to add a checksum on each data chunk. A false
        value (the default) disables the checksum.
    least_significant_digit : int
        If specified, data will be truncated (quantized). In conjunction
        with enabling compression, this produces 'lossy', but
        significantly more efficient compression. For example, if
        *least_significant_digit=1*, data will be quantized using
        ``around(scale*data)/scale``, where ``scale = 2**bits``, and
        bits is determined so that a precision of 0.1 is retained (in
        this case bits=4). Default is *None*, or no quantization.

        .. note::

            quantization is only applied if some form of compression is
            enabled

    Examples
    --------

    This is a small example on using the Filters class::

        import numpy
        from tables import *

        fileh = open_file('test5.h5', mode='w')
        atom = Float32Atom()
        filters = Filters(complevel=1, complib='blosc', fletcher32=True)
        arr = fileh.create_earray(fileh.root, 'earray', atom, (0,2),
                                 "A growable array", filters=filters)

        # Append several rows in only one call
        arr.append(numpy.array([[1., 2.],
                                [2., 3.],
                                [3., 4.]], dtype=numpy.float32))

        # Print information on that enlargeable array
        print("Result Array:")
        print(repr(arr))
        fileh.close()

    This enforces the use of the Blosc library, a compression level of 1 and a
    Fletcher32 checksum filter as well. See the output of this example::

        Result Array:
        /earray (EArray(3, 2), fletcher32, shuffle, blosc(1)) 'A growable array'
        type = float32
        shape = (3, 2)
        itemsize = 4
        nrows = 3
        extdim = 0
        flavor = 'numpy'
        byteorder = 'little'

    .. rubric:: Filters attributes

    .. attribute:: fletcher32

        Whether the *Fletcher32* filter is active or not.

    .. attribute:: complevel

        The compression level (0 disables compression).

    .. attribute:: complib

        The compression filter used (irrelevant when compression is not
        enabled).

    .. attribute:: shuffle

        Whether the *Shuffle* filter is active or not.

    """

    @classmethod
    def _from_leaf(class_, leaf):
        # Get a dictionary with all the filters
        parent = leaf._v_parent
        filters_dict = utilsextension.get_filters(parent._v_objectid,
                                                  leaf._v_name)
        if filters_dict is None:
            filters_dict = {}  # not chunked

        kwargs = dict(complevel=0, shuffle=False, fletcher32=False,  # all off
                      least_significant_digit=None, _new=False)
        for (name, values) in filters_dict.iteritems():
            if name == 'deflate':
                name = 'zlib'
            if name in all_complibs:
                kwargs['complib'] = name
                if name == "blosc":
                    kwargs['complevel'] = values[4]
                    # Shuffle filter is internal to blosc
                    if values[5]:
                        kwargs['shuffle'] = True
                    # In Blosc 1.3 another parameter is used for the compressor
                    if len(values) > 6:
                        cname = blosc_compcode_to_compname(values[6])
                        kwargs['complib'] = "blosc:%s" % cname
                else:
                    kwargs['complevel'] = values[0]
            elif name in foreign_complibs:
                kwargs['complib'] = name
                kwargs['complevel'] = 1  # any nonzero value will do
            elif name in ['shuffle', 'fletcher32']:
                kwargs[name] = True
        return class_(**kwargs)

    @classmethod
    def _unpack(class_, packed):
        """Create a new `Filters` object from a packed version.

        >>> Filters._unpack(0)
        Filters(complevel=0, shuffle=False, fletcher32=False, least_significant_digit=None)
        >>> Filters._unpack(0x101)
        Filters(complevel=1, complib='zlib', shuffle=False, fletcher32=False, least_significant_digit=None)
        >>> Filters._unpack(0x30109)
        Filters(complevel=9, complib='zlib', shuffle=True, fletcher32=True, least_significant_digit=None)
        >>> Filters._unpack(0x3010A)
        Traceback (most recent call last):
          ...
        ValueError: compression level must be between 0 and 9
        >>> Filters._unpack(0x1)
        Traceback (most recent call last):
          ...
        ValueError: invalid compression library id: 0

        """

        kwargs = {'_new': False}

        # Byte 0: compression level.
        kwargs['complevel'] = complevel = packed & 0xff
        packed >>= 8

        # Byte 1: compression library id (0 for none).
        if complevel > 0:
            complib_id = int(packed & 0xff)
            if not (0 < complib_id <= len(all_complibs)):
                raise ValueError("invalid compression library id: %d"
                                 % complib_id)
            kwargs['complib'] = all_complibs[complib_id - 1]
        packed >>= 8

        # Byte 2: parameterless filters.
        kwargs['shuffle'] = packed & _shuffle_flag
        kwargs['fletcher32'] = packed & _fletcher32_flag
        has_rounding = packed & _rounding_flag
        packed >>= 8

        # Byte 3: least significant digit.
        if has_rounding:
            kwargs['least_significant_digit'] = numpy.int8(packed & 0xff)
        else:
            kwargs['least_significant_digit'] = None

        return class_(**kwargs)

    def _pack(self):
        """Pack the `Filters` object into a 64-bit NumPy integer."""

        packed = numpy.int64(0)

        # Byte 3: least significant digit.
        if self.least_significant_digit is not None:
            #assert isinstance(self.least_significant_digit, numpy.int8)
            packed |= self.least_significant_digit
        packed <<= 8

        # Byte 2: parameterless filters.
        if self.shuffle:
            packed |= _shuffle_flag
        if self.fletcher32:
            packed |= _fletcher32_flag
        if self.least_significant_digit:
            packed |= _rounding_flag
        packed <<= 8

        # Byte 1: compression library id (0 for none).
        if self.complevel > 0:
            packed |= all_complibs.index(self.complib) + 1
        packed <<= 8

        # Byte 0: compression level.
        packed |= self.complevel

        return packed

    def __init__(self, complevel=0, complib=default_complib,
                 shuffle=True, fletcher32=False,
                 least_significant_digit=None, _new=True):

        if not (0 <= complevel <= 9):
            raise ValueError("compression level must be between 0 and 9")

        if _new and complevel > 0:
            # These checks are not performed when loading filters from disk.
            if complib not in all_complibs:
                raise ValueError(
                    "compression library ``%s`` is not supported; "
                    "it must be one of: %s"
                    % (complib, ", ".join(all_complibs)))
            if utilsextension.which_lib_version(complib) is None:
                warnings.warn("compression library ``%s`` is not available; "
                              "using ``%s`` instead"
                              % (complib, default_complib), FiltersWarning)
                complib = default_complib  # always available

        complevel = int(complevel)
        complib = str(complib)
        shuffle = bool(shuffle)
        fletcher32 = bool(fletcher32)
        if least_significant_digit is not None:
            least_significant_digit = numpy.int8(least_significant_digit)

        if complevel == 0:
            # Override some inputs when compression is not enabled.
            complib = None  # make it clear there is no compression
            shuffle = False  # shuffling and not compressing makes no sense
            least_significant_digit = None
        elif complib not in all_complibs:
            # Do not try to use a meaningful level for unsupported libs.
            complevel = -1

        self.complevel = complevel
        """The compression level (0 disables compression)."""

        self.complib = complib
        """The compression filter used (irrelevant when compression is
        not enabled).
        """

        self.shuffle = shuffle
        """Whether the *Shuffle* filter is active or not."""

        self.fletcher32 = fletcher32
        """Whether the *Fletcher32* filter is active or not."""

        self.least_significant_digit = least_significant_digit
        """The least significant digit to which data shall be truncated."""

    def __repr__(self):
        args, complevel = [], self.complevel
        if complevel >= 0:  # meaningful compression level
            args.append('complevel=%d' % complevel)
        if complevel != 0:  # compression enabled (-1 or > 0)
            args.append('complib=%r' % self.complib)
        args.append('shuffle=%s' % self.shuffle)
        args.append('fletcher32=%s' % self.fletcher32)
        args.append(
            'least_significant_digit=%s' % self.least_significant_digit)
        return '%s(%s)' % (self.__class__.__name__, ', '.join(args))

    def __str__(self):
        return repr(self)

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        for attr in self.__dict__:
            if getattr(self, attr) != getattr(other, attr):
                return False
        return True

    # XXX: API incompatible change for PyTables 3 line
    # Overriding __eq__ blocks inheritance of __hash__ in 3.x
    # def __hash__(self):
    #    return hash((self.__class__, self.complevel, self.complib,
    #                 self.shuffle, self.fletcher32))

    def copy(self, **override):
        """Get a copy of the filters, possibly overriding some arguments.

        Constructor arguments to be overridden must be passed as keyword
        arguments.

        Using this method is recommended over replacing the attributes of an
        instance, since instances of this class may become immutable in the
        future::

            >>> filters1 = Filters()
            >>> filters2 = filters1.copy()
            >>> filters1 == filters2
            True
            >>> filters1 is filters2
            False
            >>> filters3 = filters1.copy(complevel=1) #doctest: +ELLIPSIS
            Traceback (most recent call last):
            ...
            ValueError: compression library ``None`` is not supported...
            >>> filters3 = filters1.copy(complevel=1, complib='zlib')
            >>> print(filters1)
            Filters(complevel=0, shuffle=False, fletcher32=False, least_significant_digit=None)
            >>> print(filters3)
            Filters(complevel=1, complib='zlib', shuffle=False, fletcher32=False, least_significant_digit=None)
            >>> filters1.copy(foobar=42)
            Traceback (most recent call last):
            ...
            TypeError: __init__() got an unexpected keyword argument 'foobar'

        """

        newargs = self.__dict__.copy()
        newargs.update(override)
        return self.__class__(**newargs)


# Main part
# =========
def _test():
    """Run ``doctest`` on this module."""

    import doctest
    doctest.testmod()


if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = flavor
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: December 30, 2006
# Author: Ivan Vilata i Balaguer - ivan at selidor dot net
#
# $Id$
#
########################################################################

"""Utilities for handling different array flavors in PyTables.

Variables
=========

`__docformat`__
    The format of documentation strings in this module.
`internal_flavor`
    The flavor used internally by PyTables.
`all_flavors`
    List of all flavors available to PyTables.
`alias_map`
    Maps old flavor names to the most similar current flavor.
`description_map`
    Maps flavors to short descriptions of their supported objects.
`identifier_map`
    Maps flavors to functions that can identify their objects.

    The function associated with a given flavor will return a true
    value if the object passed to it can be identified as being of
    that flavor.

    See the `flavor_of()` function for a friendlier interface to
    flavor identification.

`converter_map`
    Maps (source, destination) flavor pairs to converter functions.

    Converter functions get an array of the source flavor and return
    an array of the destination flavor.

    See the `array_of_flavor()` and `flavor_to_flavor()` functions for
    friendlier interfaces to flavor conversion.

"""

# Imports
# =======
import warnings

from tables.exceptions import FlavorError, FlavorWarning


# Public variables
# ================
__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""

internal_flavor = 'numpy'
"""The flavor used internally by PyTables."""

# This is very slightly slower than a set for a small number of values
# in terms of (infrequent) lookup time, but allows `flavor_of()`
# (which may be called much more frequently) to check for flavors in
# order, beginning with the most common one.
all_flavors = []  # filled as flavors are registered
"""List of all flavors available to PyTables."""

alias_map = {}  # filled as flavors are registered
"""Maps old flavor names to the most similar current flavor."""

description_map = {}  # filled as flavors are registered
"""Maps flavors to short descriptions of their supported objects."""

identifier_map = {}  # filled as flavors are registered
"""Maps flavors to functions that can identify their objects.

The function associated with a given flavor will return a true value
if the object passed to it can be identified as being of that flavor.

See the `flavor_of()` function for a friendlier interface to flavor
identification.
"""

converter_map = {}  # filled as flavors are registered
"""Maps (source, destination) flavor pairs to converter functions.

Converter functions get an array of the source flavor and return an
array of the destination flavor.

See the `array_of_flavor()` and `flavor_to_flavor()` functions for
friendlier interfaces to flavor conversion.
"""


# Public functions
# ================
def check_flavor(flavor):
    """Raise a ``FlavorError`` if the `flavor` is not valid."""

    if flavor not in all_flavors:
        available_flavs = ", ".join(flav for flav in all_flavors)
        raise FlavorError(
            "flavor ``%s`` is unsupported or unavailable; "
            "available flavors in this system are: %s"
            % (flavor, available_flavs))


def array_of_flavor2(array, src_flavor, dst_flavor):
    """Get a version of the given `array` in a different flavor.

    The input `array` must be of the given `src_flavor`, and the
    returned array will be of the indicated `dst_flavor`.  Both
    flavors may be the same, but it is not guaranteed that the
    returned array will be the same object as the input one in this
    case.

    If the conversion is not supported, a ``FlavorError`` is raised.

    """

    convkey = (src_flavor, dst_flavor)
    if convkey not in converter_map:
        raise FlavorError("conversion from flavor ``%s`` to flavor ``%s`` "
                          "is unsupported or unavailable in this system"
                          % (src_flavor, dst_flavor))

    convfunc = converter_map[convkey]
    return convfunc(array)


def flavor_to_flavor(array, src_flavor, dst_flavor):
    """Get a version of the given `array` in a different flavor.

    The input `array` must be of the given `src_flavor`, and the
    returned array will be of the indicated `dst_flavor` (see below
    for an exception to this).  Both flavors may be the same, but it
    is not guaranteed that the returned array will be the same object
    as the input one in this case.

    If the conversion is not supported, a `FlavorWarning` is issued
    and the input `array` is returned as is.

    """

    try:
        return array_of_flavor2(array, src_flavor, dst_flavor)
    except FlavorError as fe:
        warnings.warn("%s; returning an object of the ``%s`` flavor instead"
                      % (fe.args[0], src_flavor), FlavorWarning)
        return array


def internal_to_flavor(array, dst_flavor):
    """Get a version of the given `array` in a different `dst_flavor`.

    The input `array` must be of the internal flavor, and the returned
    array will be of the given `dst_flavor`.  See `flavor_to_flavor()`
    for more information.

    """

    return flavor_to_flavor(array, internal_flavor, dst_flavor)


def array_as_internal(array, src_flavor):
    """Get a version of the given `array` in the internal flavor.

    The input `array` must be of the given `src_flavor`, and the
    returned array will be of the internal flavor.

    If the conversion is not supported, a ``FlavorError`` is raised.

    """

    return array_of_flavor2(array, src_flavor, internal_flavor)


def flavor_of(array):
    """Identify the flavor of a given `array`.

    If the `array` can not be matched with any flavor, a ``TypeError``
    is raised.

    """

    for flavor in all_flavors:
        if identifier_map[flavor](array):
            return flavor
    type_name = type(array).__name__
    supported_descs = "; ".join(description_map[fl] for fl in all_flavors)
    raise TypeError(
        "objects of type ``%s`` are not supported in this context, sorry; "
        "supported objects are: %s" % (type_name, supported_descs))


def array_of_flavor(array, dst_flavor):
    """Get a version of the given `array` in a different `dst_flavor`.

    The flavor of the input `array` is guessed, and the returned array
    will be of the given `dst_flavor`.

    If the conversion is not supported, a ``FlavorError`` is raised.

    """

    return array_of_flavor2(array, flavor_of(array), dst_flavor)


def restrict_flavors(keep=['python']):
    """Disable all flavors except those in keep.

    Providing an empty keep sequence implies disabling all flavors (but the
    internal one).  If the sequence is not specified, only optional flavors are
    disabled.

    .. important:: Once you disable a flavor, it can not be enabled again.

    """

    keep = set(keep).union([internal_flavor])
    remove = set(all_flavors).difference(keep)
    for flavor in remove:
        _disable_flavor(flavor)


# Flavor registration
# ===================
#
# The order in which flavors appear in `all_flavors` determines the
# order in which they will be tested for by `flavor_of()`, so place
# most frequent flavors first.
import numpy
all_flavors.append('numpy')  # this is the internal flavor

all_flavors.append('python')  # this is always supported


def _register_aliases():
    """Register aliases of *available* flavors."""

    for flavor in all_flavors:
        aliases = eval('_%s_aliases' % flavor)
        for alias in aliases:
            alias_map[alias] = flavor


def _register_descriptions():
    """Register descriptions of *available* flavors."""
    for flavor in all_flavors:
        description_map[flavor] = eval('_%s_desc' % flavor)


def _register_identifiers():
    """Register identifier functions of *available* flavors."""

    for flavor in all_flavors:
        identifier_map[flavor] = eval('_is_%s' % flavor)


def _register_converters():
    """Register converter functions between *available* flavors."""

    def identity(array):
        return array
    for src_flavor in all_flavors:
        for dst_flavor in all_flavors:
            # Converters with the same source and destination flavor
            # are used when available, since they may perform some
            # optimizations on the resulting array (e.g. making it
            # contiguous).  Otherwise, an identity function is used.
            convfunc = None
            try:
                convfunc = eval('_conv_%s_to_%s' % (src_flavor, dst_flavor))
            except NameError:
                if src_flavor == dst_flavor:
                    convfunc = identity
            if convfunc:
                converter_map[(src_flavor, dst_flavor)] = convfunc


def _register_all():
    """Register all *available* flavors."""

    _register_aliases()
    _register_descriptions()
    _register_identifiers()
    _register_converters()


def _deregister_aliases(flavor):
    """Deregister aliases of a given `flavor` (no checks)."""

    rm_aliases = []
    for (an_alias, a_flavor) in alias_map.iteritems():
        if a_flavor == flavor:
            rm_aliases.append(an_alias)
    for an_alias in rm_aliases:
        del alias_map[an_alias]


def _deregister_description(flavor):
    """Deregister description of a given `flavor` (no checks)."""

    del description_map[flavor]


def _deregister_identifier(flavor):
    """Deregister identifier function of a given `flavor` (no checks)."""

    del identifier_map[flavor]


def _deregister_converters(flavor):
    """Deregister converter functions of a given `flavor` (no checks)."""

    rm_flavor_pairs = []
    for flavor_pair in converter_map:
        if flavor in flavor_pair:
            rm_flavor_pairs.append(flavor_pair)
    for flavor_pair in rm_flavor_pairs:
        del converter_map[flavor_pair]


def _disable_flavor(flavor):
    """Completely disable the given `flavor` (no checks)."""

    _deregister_aliases(flavor)
    _deregister_description(flavor)
    _deregister_identifier(flavor)
    _deregister_converters(flavor)
    all_flavors.remove(flavor)


# Implementation of flavors
# =========================
_python_aliases = [
    'List', 'Tuple',
    'Int', 'Float', 'String',
    'VLString', 'Object',
]
_python_desc = ("homogeneous list or tuple, "
                "integer, float, complex or bytes")


def _is_python(array):
    return isinstance(array, (tuple, list, int, float, complex, bytes))

_numpy_aliases = []
_numpy_desc = "NumPy array, record or scalar"


def _is_numpy(array):
    return isinstance(array, (numpy.ndarray, numpy.generic))


def _numpy_contiguous(convfunc):
    """Decorate `convfunc` to return a *contiguous* NumPy array.

    Note: When arrays are 0-strided, the copy is avoided.  This allows
    to use `array` to still carry info about the dtype and shape.
    """

    def conv_to_numpy(array):
        nparr = convfunc(array)
        if (hasattr(nparr, 'flags') and
            not nparr.flags.contiguous and
            sum(nparr.strides) != 0):
            nparr = nparr.copy()  # copying the array makes it contiguous
        return nparr
    conv_to_numpy.__name__ = convfunc.__name__
    conv_to_numpy.__doc__ = convfunc.__doc__
    return conv_to_numpy


@_numpy_contiguous
def _conv_numpy_to_numpy(array):
    # Passes contiguous arrays through and converts scalars into
    # scalar arrays.
    return numpy.asarray(array)


@_numpy_contiguous
def _conv_python_to_numpy(array):
    return numpy.array(array)


def _conv_numpy_to_python(array):
    if array.shape != ():
        # Lists are the default for returning multidimensional objects
        array = array.tolist()
    else:
        # 0-dim or scalar case
        array = array.item()
    return array

# Now register everything related with *available* flavors.
_register_all()


# Main part
# =========
def _test():
    """Run ``doctest`` on this module."""

    import doctest
    doctest.testmod()


if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = group
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: September 4, 2002
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Group class."""

import warnings
import weakref

import tables.misc.proxydict
from tables import hdf5extension
from tables import utilsextension
from tables.registry import class_id_dict
from tables.exceptions import (
    NodeError, NoSuchNodeError, NaturalNameWarning, PerformanceWarning)
from tables.filters import Filters
from tables.registry import get_class_by_name
from tables.path import check_name_validity, join_path, isvisiblename
from tables.node import Node, NotLoggedMixin
from tables.leaf import Leaf
from tables.unimplemented import UnImplemented, Unknown

from tables.link import Link, SoftLink, ExternalLink

from tables._past import previous_api, previous_api_property

obversion = "1.0"


class _ChildrenDict(tables.misc.proxydict.ProxyDict):
    def _get_value_from_container(self, container, key):
        return container._f_get_child(key)

    _getValueFromContainer = previous_api(_get_value_from_container)


class Group(hdf5extension.Group, Node):
    """Basic PyTables grouping structure.

    Instances of this class are grouping structures containing *child*
    instances of zero or more groups or leaves, together with
    supporting metadata. Each group has exactly one *parent* group.

    Working with groups and leaves is similar in many ways to working
    with directories and files, respectively, in a Unix filesystem.
    As with Unix directories and files, objects in the object tree are
    often described by giving their full (or absolute) path names.
    This full path can be specified either as a string (like in
    '/group1/group2') or as a complete object path written in *natural
    naming* schema (like in file.root.group1.group2).

    A collateral effect of the *natural naming* schema is that the
    names of members in the Group class and its instances must be
    carefully chosen to avoid colliding with existing children node
    names.  For this reason and to avoid polluting the children
    namespace all members in a Group start with some reserved prefix,
    like _f_ (for public methods), _g_ (for private ones), _v_ (for
    instance variables) or _c_ (for class variables). Any attempt to
    create a new child node whose name starts with one of these
    prefixes will raise a ValueError exception.

    Another effect of natural naming is that children named after
    Python keywords or having names not valid as Python identifiers
    (e.g.  class, $a or 44) can not be accessed using the node.child
    syntax. You will be forced to use node._f_get_child(child) to
    access them (which is recommended for programmatic accesses).

    You will also need to use _f_get_child() to access an existing
    child node if you set a Python attribute in the Group with the
    same name as that node (you will get a NaturalNameWarning when
    doing this).

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*

    name : str
        The name of this node in its parent group.
    title
        The title for this group
    new
        If this group is new or has to be read from disk
    filters : Filters
        A Filters instance


    Notes
    -----
    The following documentation includes methods that are automatically
    called when a Group instance is accessed in a special way.

    For instance, this class defines the __setattr__, __getattr__, and
    __delattr__ methods, and they set, get and delete *ordinary Python
    attributes* as normally intended. In addition to that, __getattr__
    allows getting *child nodes* by their name for the sake of easy
    interaction on the command line, as long as there is no Python
    attribute with the same name. Groups also allow the interactive
    completion (when using readline) of the names of child nodes.
    For instance::

        # get a Python attribute
        nchild = group._v_nchildren

        # Add a Table child called 'table' under 'group'.
        h5file.create_table(group, 'table', myDescription)
        table = group.table          # get the table child instance
        group.table = 'foo'          # set a Python attribute

        # (PyTables warns you here about using the name of a child node.)
        foo = group.table            # get a Python attribute
        del group.table              # delete a Python attribute
        table = group.table          # get the table child instance again


    .. rubric:: Group attributes

    The following instance variables are provided in addition to those
    in Node (see :ref:`NodeClassDescr`):

    .. attribute:: _v_children

        Dictionary with all nodes hanging from this group.

    .. attribute:: _v_groups

        Dictionary with all groups hanging from this group.

    .. attribute:: _v_hidden

        Dictionary with all hidden nodes hanging from this group.

    .. attribute:: _v_leaves

        Dictionary with all leaves hanging from this group.

    .. attribute:: _v_links

        Dictionary with all links hanging from this group.

    .. attribute:: _v_unknown

        Dictionary with all unknown nodes hanging from this group.

    """

    # Class identifier.
    _c_classid = 'GROUP'

    _c_classId = previous_api_property('_c_classid')

    # Children containers that should be loaded only in a lazy way.
    # These are documented in the ``Group._g_add_children_names`` method.
    _c_lazy_children_attrs = (
        '__members__', '_v_children', '_v_groups', '_v_leaves',
        '_v_links', '_v_unknown', '_v_hidden')

    # <properties>

    # `_v_nchildren` is a direct read-only shorthand
    # for the number of *visible* children in a group.
    def _g_getnchildren(self):
        return len(self._v_children)

    _v_nchildren = property(_g_getnchildren, None, None,
                            "The number of children hanging from this group.")

    # `_v_filters` is a direct read-write shorthand for the ``FILTERS``
    # attribute with the default `Filters` instance as a default value.
    def _g_getfilters(self):
        filters = getattr(self._v_attrs, 'FILTERS', None)
        if filters is None:
            filters = Filters()
        return filters

    def _g_setfilters(self, value):
        if not isinstance(value, Filters):
            raise TypeError(
                "value is not an instance of `Filters`: %r" % (value,))
        self._v_attrs.FILTERS = value

    def _g_delfilters(self):
        del self._v_attrs.FILTERS

    _v_filters = property(
        _g_getfilters, _g_setfilters, _g_delfilters,
        """Default filter properties for child nodes.

        You can (and are encouraged to) use this property to get, set and
        delete the FILTERS HDF5 attribute of the group, which stores a Filters
        instance (see :ref:`FiltersClassDescr`). When the group has no such
        attribute, a default Filters instance is used.
        """)

    # </properties>

    _v_maxGroupWidth = previous_api_property('_v_max_group_width')

    def __init__(self, parentnode, name,
                 title="", new=False, filters=None,
                 _log=True):

        # Remember to assign these values in the root group constructor
        # if it does not use this one!

        # First, set attributes belonging to group objects.

        self._v_version = obversion
        """The object version of this group."""

        self._v_new = new
        """Is this the first time the node has been created?"""

        self._v_new_title = title
        """New title for this node."""

        self._v_new_filters = filters
        """New default filter properties for child nodes."""

        self._v_max_group_width = parentnode._v_file.params['MAX_GROUP_WIDTH']
        """Maximum number of children on each group before warning the user.

        .. versionchanged:: 3.0
           The *_v_maxGroupWidth* attribute has been renamed into
           *_v_max_group_width*.

        """

        # Finally, set up this object as a node.
        super(Group, self).__init__(parentnode, name, _log)

    def _g_post_init_hook(self):
        if self._v_new:
            if self._v_file.params['PYTABLES_SYS_ATTRS']:
                # Save some attributes for the new group on disk.
                set_attr = self._v_attrs._g__setattr
                # Set the title, class and version attributes.
                set_attr('TITLE', self._v_new_title)
                set_attr('CLASS', self._c_classid)
                set_attr('VERSION', self._v_version)

                # Set the default filter properties.
                newfilters = self._v_new_filters
                if newfilters is None:
                    # If no filters have been passed in the constructor,
                    # inherit them from the parent group, but only if they
                    # have been inherited or explicitly set.
                    newfilters = getattr(
                        self._v_parent._v_attrs, 'FILTERS', None)
                if newfilters is not None:
                    set_attr('FILTERS', newfilters)
        else:
            # If the file has PyTables format, get the VERSION attr
            if 'VERSION' in self._v_attrs._v_attrnamessys:
                self._v_version = self._v_attrs.VERSION
            else:
                self._v_version = "0.0 (unknown)"
            # We don't need to get more attributes from disk,
            # since the most important ones are defined as properties.

    _g_postInitHook = previous_api(_g_post_init_hook)

    def __del__(self):
        if (self._v_isopen and
            self._v_pathname in self._v_file._node_manager.registry and
                '_v_children' in self.__dict__):
            # The group is going to be killed.  Rebuild weak references
            # (that Python cancelled just before calling this method) so
            # that they are still usable if the object is revived later.
            selfref = weakref.ref(self)
            self._v_children.containerref = selfref
            self._v_groups.containerref = selfref
            self._v_leaves.containerref = selfref
            self._v_links.containerref = selfref
            self._v_unknown.containerref = selfref
            self._v_hidden.containerref = selfref

        super(Group, self).__del__()

    def _g_get_child_group_class(self, childname):
        """Get the class of a not-yet-loaded group child.

        `childname` must be the name of a *group* child.

        """

        childCID = self._g_get_gchild_attr(childname, 'CLASS')
        if childCID is not None and not isinstance(childCID, str):
            childCID = childCID.decode('utf-8')

        if childCID in class_id_dict:
            return class_id_dict[childCID]  # look up group class
        else:
            return Group  # default group class

    _g_getChildGroupClass = previous_api(_g_get_child_group_class)

    def _g_get_child_leaf_class(self, childname, warn=True):
        """Get the class of a not-yet-loaded leaf child.

        `childname` must be the name of a *leaf* child.  If the child
        belongs to an unknown kind of leaf, or if its kind can not be
        guessed, `UnImplemented` will be returned and a warning will be
        issued if `warn` is true.

        """

        if self._v_file.params['PYTABLES_SYS_ATTRS']:
            childCID = self._g_get_lchild_attr(childname, 'CLASS')
            if childCID is not None and not isinstance(childCID, str):
                childCID = childCID.decode('utf-8')
        else:
            childCID = None

        if childCID in class_id_dict:
            return class_id_dict[childCID]  # look up leaf class
        else:
            # Unknown or no ``CLASS`` attribute, try a guess.
            childCID2 = utilsextension.which_class(self._v_objectid, childname)
            if childCID2 == 'UNSUPPORTED':
                if warn:
                    if childCID is None:
                        warnings.warn(
                            "leaf ``%s`` is of an unsupported type; "
                            "it will become an ``UnImplemented`` node"
                            % self._g_join(childname))
                    else:
                        warnings.warn(
                            ("leaf ``%s`` has an unknown class ID ``%s``; "
                             "it will become an ``UnImplemented`` node")
                            % (self._g_join(childname), childCID))
                return UnImplemented
            assert childCID2 in class_id_dict
            return class_id_dict[childCID2]  # look up leaf class

    _g_getChildLeafClass = previous_api(_g_get_child_leaf_class)

    def _g_add_children_names(self):
        """Add children names to this group taking into account their
        visibility and kind."""

        mydict = self.__dict__

        # The names of the lazy attributes
        mydict['__members__'] = members = []
        """The names of visible children nodes for readline-style completion.
        """
        mydict['_v_children'] = children = _ChildrenDict(self)
        """The number of children hanging from this group."""
        mydict['_v_groups'] = groups = _ChildrenDict(self)
        """Dictionary with all groups hanging from this group."""
        mydict['_v_leaves'] = leaves = _ChildrenDict(self)
        """Dictionary with all leaves hanging from this group."""
        mydict['_v_links'] = links = _ChildrenDict(self)
        """Dictionary with all links hanging from this group."""
        mydict['_v_unknown'] = unknown = _ChildrenDict(self)
        """Dictionary with all unknown nodes hanging from this group."""
        mydict['_v_hidden'] = hidden = _ChildrenDict(self)
        """Dictionary with all hidden nodes hanging from this group."""

        # Get the names of *all* child groups and leaves.
        (group_names, leaf_names, link_names, unknown_names) = \
            self._g_list_group(self._v_parent)

        # Separate groups into visible groups and hidden nodes,
        # and leaves into visible leaves and hidden nodes.
        for (childnames, childdict) in ((group_names, groups),
                                        (leaf_names, leaves),
                                        (link_names, links),
                                        (unknown_names, unknown)):

            for childname in childnames:
                # See whether the name implies that the node is hidden.
                # (Assigned values are entirely irrelevant.)
                if isvisiblename(childname):
                    # Visible node.
                    members.insert(0, childname)
                    children[childname] = None
                    childdict[childname] = None
                else:
                    # Hidden node.
                    hidden[childname] = None

    _g_addChildrenNames = previous_api(_g_add_children_names)

    def _g_check_has_child(self, name):
        """Check whether 'name' is a children of 'self' and return its type."""

        # Get the HDF5 name matching the PyTables name.
        node_type = self._g_get_objinfo(name)
        if node_type == "NoSuchNode":
            raise NoSuchNodeError(
                "group ``%s`` does not have a child named ``%s``"
                % (self._v_pathname, name))
        return node_type

    _g_checkHasChild = previous_api(_g_check_has_child)

    def __iter__(self):
        """Iterate over the child nodes hanging directly from the group.

        This iterator is *not* recursive.

        Examples
        --------

        ::

            # Non-recursively list all the nodes hanging from '/detector'
            print("Nodes in '/detector' group:")
            for node in h5file.root.detector:
                print(node)

        """

        return self._f_iter_nodes()

    def __contains__(self, name):
        """Is there a child with that `name`?

        Returns a true value if the group has a child node (visible or
        hidden) with the given `name` (a string), false otherwise.

        """

        self._g_check_open()
        try:
            self._g_check_has_child(name)
        except NoSuchNodeError:
            return False
        return True

    def _f_walknodes(self, classname=None):
        """Iterate over descendant nodes.

        This method recursively walks *self* top to bottom (preorder),
        iterating over child groups in alphanumerical order, and yielding
        nodes.  If classname is supplied, only instances of the named class are
        yielded.

        If *classname* is Group, it behaves like :meth:`Group._f_walk_groups`,
        yielding only groups.  If you don't want a recursive behavior,
        use :meth:`Group._f_iter_nodes` instead.

        Examples
        --------

        ::

            # Recursively print all the arrays hanging from '/'
            print("Arrays in the object tree '/':")
            for array in h5file.root._f_walknodes('Array', recursive=True):
                print(array)

        """

        self._g_check_open()

        # For compatibility with old default arguments.
        if classname == '':
            classname = None

        if classname == "Group":
            # Recursive algorithm
            for group in self._f_walk_groups():
                yield group
        else:
            for group in self._f_walk_groups():
                for leaf in group._f_iter_nodes(classname):
                    yield leaf

    _f_walkNodes = previous_api(_f_walknodes)

    def _g_join(self, name):
        """Helper method to correctly concatenate a name child object with the
        pathname of this group."""

        if name == "/":
            # This case can happen when doing copies
            return self._v_pathname
        return join_path(self._v_pathname, name)

    def _g_width_warning(self):
        """Issue a :exc:`PerformanceWarning` on too many children."""

        warnings.warn("""\
group ``%s`` is exceeding the recommended maximum number of children (%d); \
be ready to see PyTables asking for *lots* of memory and possibly slow I/O."""
                      % (self._v_pathname, self._v_max_group_width),
                      PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)

    def _g_refnode(self, childnode, childname, validate=True):
        """Insert references to a `childnode` via a `childname`.

        Checks that the `childname` is valid and does not exist, then
        creates references to the given `childnode` by that `childname`.
        The validation of the name can be omitted by setting `validate`
        to a false value (this may be useful for adding already existing
        nodes to the tree).

        """

        # Check for name validity.
        if validate:
            check_name_validity(childname)
            childnode._g_check_name(childname)

        # Check if there is already a child with the same name.
        # This can be triggered because of the user
        # (via node construction or renaming/movement).
        # Links are not checked here because they are copied and referenced
        # using ``File.get_node`` so they already exist in `self`.
        if (not isinstance(childnode, Link)) and childname in self:
            raise NodeError(
                "group ``%s`` already has a child node named ``%s``"
                % (self._v_pathname, childname))

        # Show a warning if there is an object attribute with that name.
        if childname in self.__dict__:
            warnings.warn(
                "group ``%s`` already has an attribute named ``%s``; "
                "you will not be able to use natural naming "
                "to access the child node"
                % (self._v_pathname, childname), NaturalNameWarning)

        # Check group width limits.
        if (len(self._v_children) + len(self._v_hidden) >=
                self._v_max_group_width):
            self._g_width_warning()

        # Update members information.
        # Insert references to the new child.
        # (Assigned values are entirely irrelevant.)
        if isvisiblename(childname):
            # Visible node.
            self.__members__.insert(0, childname)  # enable completion
            self._v_children[childname] = None  # insert node
            if isinstance(childnode, Unknown):
                self._v_unknown[childname] = None
            elif isinstance(childnode, Link):
                self._v_links[childname] = None
            elif isinstance(childnode, Leaf):
                self._v_leaves[childname] = None
            elif isinstance(childnode, Group):
                self._v_groups[childname] = None
        else:
            # Hidden node.
            self._v_hidden[childname] = None  # insert node

    _g_refNode = previous_api(_g_refnode)

    def _g_unrefnode(self, childname):
        """Remove references to a node.

        Removes all references to the named node.

        """

        # This can *not* be triggered because of the user.
        assert childname in self, \
            ("group ``%s`` does not have a child node named ``%s``"
                % (self._v_pathname, childname))

        # Update members information, if needed
        if '_v_children' in self.__dict__:
            if childname in self._v_children:
                # Visible node.
                members = self.__members__
                member_index = members.index(childname)
                del members[member_index]  # disables completion

                del self._v_children[childname]  # remove node
                self._v_unknown.pop(childname, None)
                self._v_links.pop(childname, None)
                self._v_leaves.pop(childname, None)
                self._v_groups.pop(childname, None)
            else:
                # Hidden node.
                del self._v_hidden[childname]  # remove node

    _g_unrefNode = previous_api(_g_unrefnode)

    def _g_move(self, newparent, newname):
        # Move the node to the new location.
        oldpath = self._v_pathname
        super(Group, self)._g_move(newparent, newname)
        newpath = self._v_pathname

        # Update location information in children.  This node shouldn't
        # be affected since it has already been relocated.
        self._v_file._update_node_locations(oldpath, newpath)

    def _g_copy(self, newparent, newname, recursive, _log=True, **kwargs):
        # Compute default arguments.
        title = kwargs.get('title', self._v_title)
        filters = kwargs.get('filters', None)
        stats = kwargs.get('stats', None)

        # Fix arguments with explicit None values for backwards compatibility.
        if title is None:
            title = self._v_title
        # If no filters have been passed to the call, copy them from the
        # source group, but only if inherited or explicitly set.
        if filters is None:
            filters = getattr(self._v_attrs, 'FILTERS', None)

        # Create a copy of the object.
        new_node = Group(newparent, newname,
                         title, new=True, filters=filters, _log=_log)

        # Copy user attributes if needed.
        if kwargs.get('copyuserattrs', True):
            self._v_attrs._g_copy(new_node._v_attrs, copyclass=True)

        # Update statistics if needed.
        if stats is not None:
            stats['groups'] += 1

        if recursive:
            # Copy child nodes if a recursive copy was requested.
            # Some arguments should *not* be passed to children copy ops.
            kwargs = kwargs.copy()
            kwargs.pop('title', None)
            self._g_copy_children(new_node, **kwargs)

        return new_node

    def _g_copy_children(self, newparent, **kwargs):
        """Copy child nodes.

        Copies all nodes descending from this one into the specified
        `newparent`.  If the new parent has a child node with the same
        name as one of the nodes in this group, the copy fails with a
        `NodeError`, maybe resulting in a partial copy.  Nothing is
        logged.

        """

        # Recursive version of children copy.
        # for srcchild in self._v_children.itervalues():
        ##    srcchild._g_copy_as_child(newparent, **kwargs)

        # Non-recursive version of children copy.
        parentstack = [(self, newparent)]  # [(source, destination), ...]
        while parentstack:
            (srcparent, dstparent) = parentstack.pop()
            for srcchild in srcparent._v_children.itervalues():
                dstchild = srcchild._g_copy_as_child(dstparent, **kwargs)
                if isinstance(srcchild, Group):
                    parentstack.append((srcchild, dstchild))

    _g_copyChildren = previous_api(_g_copy_children)

    def _f_get_child(self, childname):
        """Get the child called childname of this group.

        If the child exists (be it visible or not), it is returned.  Else, a
        NoSuchNodeError is raised.

        Using this method is recommended over getattr() when doing programmatic
        accesses to children if childname is unknown beforehand or when its
        name is not a valid Python identifier.

        """

        self._g_check_open()

        self._g_check_has_child(childname)

        childpath = join_path(self._v_pathname, childname)
        return self._v_file._get_node(childpath)

    _f_getChild = previous_api(_f_get_child)

    def _f_list_nodes(self, classname=None):
        """Return a *list* with children nodes.

        This is a list-returning version of :meth:`Group._f_iter_nodes()`.

        """

        return list(self._f_iter_nodes(classname))

    _f_listNodes = previous_api(_f_list_nodes)

    def _f_iter_nodes(self, classname=None):
        """Iterate over children nodes.

        Child nodes are yielded alphanumerically sorted by node name.  If the
        name of a class derived from Node (see :ref:`NodeClassDescr`) is
        supplied in the classname parameter, only instances of that class (or
        subclasses of it) will be returned.

        This is an iterator version of :meth:`Group._f_list_nodes`.

        """

        self._g_check_open()

        if not classname:
            # Returns all the children alphanumerically sorted
            names = sorted(self._v_children.iterkeys())
            for name in names:
                yield self._v_children[name]
        elif classname == 'Group':
            # Returns all the groups alphanumerically sorted
            names = sorted(self._v_groups.iterkeys())
            for name in names:
                yield self._v_groups[name]
        elif classname == 'Leaf':
            # Returns all the leaves alphanumerically sorted
            names = sorted(self._v_leaves.iterkeys())
            for name in names:
                yield self._v_leaves[name]
        elif classname == 'Link':
            # Returns all the links alphanumerically sorted
            names = sorted(self._v_links.iterkeys())
            for name in names:
                yield self._v_links[name]
        elif classname == 'IndexArray':
            raise TypeError(
                "listing ``IndexArray`` nodes is not allowed")
        else:
            class_ = get_class_by_name(classname)

            children = self._v_children
            childnames = sorted(children.iterkeys())

            for childname in childnames:
                childnode = children[childname]
                if isinstance(childnode, class_):
                    yield childnode

    _f_iterNodes = previous_api(_f_iter_nodes)

    def _f_walk_groups(self):
        """Recursively iterate over descendent groups (not leaves).

        This method starts by yielding *self*, and then it goes on to
        recursively iterate over all child groups in alphanumerical order, top
        to bottom (preorder), following the same procedure.

        """

        self._g_check_open()

        stack = [self]
        yield self
        # Iterate over the descendants
        while stack:
            objgroup = stack.pop()
            groupnames = sorted(objgroup._v_groups.iterkeys())
            # Sort the groups before delivering. This uses the groups names
            # for groups in tree (in order to sort() can classify them).
            for groupname in groupnames:
                stack.append(objgroup._v_groups[groupname])
                yield objgroup._v_groups[groupname]

    _f_walkGroups = previous_api(_f_walk_groups)

    def __delattr__(self, name):
        """Delete a Python attribute called name.

        This method deletes an *ordinary Python attribute* from the object.
        It does *not* remove children nodes from this group; for that,
        use :meth:`File.remove_node` or :meth:`Node._f_remove`.
        It does *neither* delete a PyTables node attribute; for that,
        use :meth:`File.del_node_attr`, :meth:`Node._f_delattr` or
        :attr:`Node._v_attrs``.

        If there is an attribute and a child node with the same name,
        the child node will be made accessible again via natural naming.

        """

        try:
            super(Group, self).__delattr__(name)  # nothing particular
        except AttributeError as ae:
            hint = " (use ``node._f_remove()`` if you want to remove a node)"
            raise ae.__class__(str(ae) + hint)

    def __getattr__(self, name):
        """Get a Python attribute or child node called name.

        If the object has a Python attribute called name, its value is
        returned. Else, if the node has a child node called name, it is
        returned.  Else, an AttributeError is raised.

        """

        # That is true since a `NoSuchNodeError` is an `AttributeError`.
        mydict = self.__dict__
        if name in mydict:
            return mydict[name]
        elif name in self._c_lazy_children_attrs:
            self._g_add_children_names()
            return mydict[name]
        return self._f_get_child(name)

    def __setattr__(self, name, value):
        """Set a Python attribute called name with the given value.

        This method stores an *ordinary Python attribute* in the object. It
        does *not* store new children nodes under this group; for that, use the
        File.create*() methods (see the File class
        in :ref:`FileClassDescr`). It does *neither* store a PyTables node
        attribute; for that,
        use :meth:`File.set_node_attr`, :meth`:Node._f_setattr`
        or :attr:`Node._v_attrs`.

        If there is already a child node with the same name, a
        NaturalNameWarning will be issued and the child node will not be
        accessible via natural naming nor getattr(). It will still be available
        via :meth:`File.get_node`, :meth:`Group._f_get_child` and children
        dictionaries in the group (if visible).

        """

        # Show a warning if there is an child node with that name.
        #
        # ..note::
        #
        #   Using ``if name in self:`` is not right since that would
        #   require ``_v_children`` and ``_v_hidden`` to be already set
        #   when the very first attribute assignments are made.
        #   Moreover, this warning is only concerned about clashes with
        #   names used in natural naming, i.e. those in ``__members__``.
        #
        # ..note::
        #
        #   The check ``'__members__' in myDict`` allows attribute
        #   assignment to happen before calling `Group.__init__()`, by
        #   avoiding to look into the still not assigned ``__members__``
        #   attribute.  This allows subclasses to set up some attributes
        #   and then call the constructor of the superclass.  If the
        #   check above is disabled, that results in Python entering an
        #   endless loop on exit!

        mydict = self.__dict__
        if '__members__' in mydict and name in self.__members__:
            warnings.warn(
                "group ``%s`` already has a child node named ``%s``; "
                "you will not be able to use natural naming "
                "to access the child node"
                % (self._v_pathname, name), NaturalNameWarning)

        super(Group, self).__setattr__(name, value)

    def _f_flush(self):
        """Flush this Group."""

        self._g_check_open()
        self._g_flush_group()

    def _g_close_descendents(self):
        """Close all the *loaded* descendent nodes of this group."""

        node_manager = self._v_file._node_manager
        node_manager.close_subtree(self._v_pathname)

    _g_closeDescendents = previous_api(_g_close_descendents)

    def _g_close(self):
        """Close this (open) group."""

        if self._v_isopen:
            # hdf5extension operations:
            #   Close HDF5 group.
            self._g_close_group()

        # Close myself as a node.
        super(Group, self)._f_close()

    def _f_close(self):
        """Close this group and all its descendents.

        This method has the behavior described in :meth:`Node._f_close`.
        It should be noted that this operation closes all the nodes
        descending from this group.

        You should not need to close nodes manually because they are
        automatically opened/closed when they are loaded/evicted from
        the integrated LRU cache.

        """

        # If the group is already closed, return immediately
        if not self._v_isopen:
            return

        # First, close all the descendents of this group, unless a) the
        # group is being deleted (evicted from LRU cache) or b) the node
        # is being closed during an aborted creation, in which cases
        # this is not an explicit close issued by the user.
        if not (self._v__deleting or self._v_objectid is None):
            self._g_close_descendents()

        # When all the descendents have been closed, close this group.
        # This is done at the end because some nodes may still need to
        # be loaded during the closing process; thus this node must be
        # open until the very end.
        self._g_close()

    def _g_remove(self, recursive=False, force=False):
        """Remove (recursively if needed) the Group.

        This version correctly handles both visible and hidden nodes.

        """

        if self._v_nchildren > 0:
            if not (recursive or force):
                raise NodeError("group ``%s`` has child nodes; "
                                "please set `recursive` or `force` to true "
                                "to remove it"
                                % (self._v_pathname,))

            # First close all the descendents hanging from this group,
            # so that it is not possible to use a node that no longer exists.
            self._g_close_descendents()

        # Remove the node itself from the hierarchy.
        super(Group, self)._g_remove(recursive, force)

    def _f_copy(self, newparent=None, newname=None,
                overwrite=False, recursive=False, createparents=False,
                **kwargs):
        """Copy this node and return the new one.

        This method has the behavior described in :meth:`Node._f_copy`.
        In addition, it recognizes the following keyword arguments:

        Parameters
        ----------
        title
            The new title for the destination. If omitted or None, the
            original title is used. This only applies to the topmost
            node in recursive copies.
        filters : Filters
            Specifying this parameter overrides the original filter
            properties in the source node. If specified, it must be an
            instance of the Filters class (see :ref:`FiltersClassDescr`).
            The default is to copy the filter properties from the source
            node.
        copyuserattrs
            You can prevent the user attributes from being copied by setting
            thisparameter to False. The default is to copy them.
        stats
            This argument may be used to collect statistics on the copy
            process. When used, it should be a dictionary with keys 'groups',
            'leaves', 'links' and 'bytes' having a numeric value. Their values
            willbe incremented to reflect the number of groups, leaves and
            bytes, respectively, that have been copied during the operation.

        """

        return super(Group, self)._f_copy(
            newparent, newname,
            overwrite, recursive, createparents, **kwargs)

    def _f_copy_children(self, dstgroup, overwrite=False, recursive=False,
                         createparents=False, **kwargs):
        """Copy the children of this group into another group.

        Children hanging directly from this group are copied into dstgroup,
        which can be a Group (see :ref:`GroupClassDescr`) object or its
        pathname in string form. If createparents is true, the needed groups
        for the given destination group path to exist will be created.

        The operation will fail with a NodeError if there is a child node
        in the destination group with the same name as one of the copied
        children from this one, unless overwrite is true; in this case,
        the former child node is recursively removed before copying the
        later.

        By default, nodes descending from children groups of this node
        are not copied. If the recursive argument is true, all descendant
        nodes of this node are recursively copied.

        Additional keyword arguments may be passed to customize the
        copying process. For instance, title and filters may be changed,
        user attributes may be or may not be copied, data may be sub-sampled,
        stats may be collected, etc. Arguments unknown to nodes are simply
        ignored. Check the documentation for copying operations of nodes to
        see which options they support.

        """

        self._g_check_open()

        # `dstgroup` is used instead of its path to avoid accepting
        # `Node` objects when `createparents` is true.  Also, note that
        # there is no risk of creating parent nodes and failing later
        # because of destination nodes already existing.
        dstparent = self._v_file._get_or_create_path(dstgroup, createparents)
        self._g_check_group(dstparent)  # Is it a group?

        if not overwrite:
            # Abort as early as possible when destination nodes exist
            # and overwriting is not enabled.
            for childname in self._v_children:
                if childname in dstparent:
                    raise NodeError(
                        "destination group ``%s`` already has "
                        "a node named ``%s``; "
                        "you may want to use the ``overwrite`` argument"
                        % (dstparent._v_pathname, childname))

        for child in self._v_children.itervalues():
            child._f_copy(dstparent, None, overwrite, recursive, **kwargs)

    _f_copyChildren = previous_api(_f_copy_children)

    def __str__(self):
        """Return a short string representation of the group.

        Examples
        --------

        ::

            >>> f=tables.open_file('data/test.h5')
            >>> print(f.root.group0)
            /group0 (Group) 'First Group'

        """

        pathname = self._v_pathname
        classname = self.__class__.__name__
        title = self._v_title
        return "%s (%s) %r" % (pathname, classname, title)

    def __repr__(self):
        """Return a detailed string representation of the group.

        Examples
        --------

        ::

            >>> f = tables.open_file('data/test.h5')
            >>> f.root.group0
            /group0 (Group) 'First Group'
              children := ['tuple1' (Table), 'group1' (Group)]

        """

        rep = [
            '%r (%s)' % (childname, child.__class__.__name__)
            for (childname, child) in self._v_children.iteritems()
        ]
        childlist = '[%s]' % (', '.join(rep))

        return "%s\n  children := %s" % (str(self), childlist)


# Special definition for group root
class RootGroup(Group):

    _v_objectId = previous_api_property('_v_objectid')

    def __init__(self, ptfile, name, title, new, filters):
        mydict = self.__dict__

        # Set group attributes.
        self._v_version = obversion
        self._v_new = new
        if new:
            self._v_new_title = title
            self._v_new_filters = filters
        else:
            self._v_new_title = None
            self._v_new_filters = None

        # Set node attributes.
        self._v_file = ptfile
        self._v_isopen = True  # root is always open
        self._v_pathname = '/'
        self._v_name = '/'
        self._v_depth = 0
        self._v_max_group_width = ptfile.params['MAX_GROUP_WIDTH']
        self._v__deleting = False
        self._v_objectid = None  # later

        # Only the root node has the file as a parent.
        # Bypass __setattr__ to avoid the ``Node._v_parent`` property.
        mydict['_v_parent'] = ptfile
        ptfile._node_manager.register_node(self, '/')

        # hdf5extension operations (do before setting an AttributeSet):
        #   Update node attributes.
        self._g_new(ptfile, name, init=True)
        #   Open the node and get its object ID.
        self._v_objectid = self._g_open()

        # Set disk attributes and read children names.
        #
        # This *must* be postponed because this method needs the root node
        # to be created and bound to ``File.root``.
        # This is an exception to the rule, handled by ``File.__init()__``.
        #
        # self._g_post_init_hook()

    def _g_load_child(self, childname):
        """Load a child node from disk.

        The child node `childname` is loaded from disk and an adequate
        `Node` object is created and returned.  If there is no such
        child, a `NoSuchNodeError` is raised.

        """

        if self._v_file.root_uep != "/":
            childname = join_path(self._v_file.root_uep, childname)
        # Is the node a group or a leaf?
        node_type = self._g_check_has_child(childname)

        # Nodes that HDF5 report as H5G_UNKNOWN
        if node_type == 'Unknown':
            return Unknown(self, childname)

        # Guess the PyTables class suited to the node,
        # build a PyTables node and return it.
        if node_type == "Group":
            if self._v_file.params['PYTABLES_SYS_ATTRS']:
                ChildClass = self._g_get_child_group_class(childname)
            else:
                # Default is a Group class
                ChildClass = Group
            return ChildClass(self, childname, new=False)
        elif node_type == "Leaf":
            ChildClass = self._g_get_child_leaf_class(childname, warn=True)
            # Building a leaf may still fail because of unsupported types
            # and other causes.
            # return ChildClass(self, childname)  # uncomment for debugging
            try:
                return ChildClass(self, childname)
            except Exception as exc:  # XXX
                warnings.warn(
                    "problems loading leaf ``%s``::\n\n"
                    "  %s\n\n"
                    "The leaf will become an ``UnImplemented`` node."
                    % (self._g_join(childname), exc))
                # If not, associate an UnImplemented object to it
                return UnImplemented(self, childname)
        elif node_type == "SoftLink":
            return SoftLink(self, childname)
        elif node_type == "ExternalLink":
            return ExternalLink(self, childname)
        else:
            return UnImplemented(self, childname)

    _g_loadChild = previous_api(_g_load_child)

    def _f_rename(self, newname):
        raise NodeError("the root node can not be renamed")

    def _f_move(self, newparent=None, newname=None, createparents=False):
        raise NodeError("the root node can not be moved")

    def _f_remove(self, recursive=False):
        raise NodeError("the root node can not be removed")


class TransactionGroupG(NotLoggedMixin, Group):
    _c_classid = 'TRANSGROUP'

    _c_classId = previous_api_property('_c_classid')

    def _g_width_warning(self):
        warnings.warn("""\
the number of transactions is exceeding the recommended maximum (%d);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                      % (self._v_max_group_width,), PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)


class TransactionG(NotLoggedMixin, Group):
    _c_classid = 'TRANSG'

    _c_classId = previous_api_property('_c_classid')

    def _g_width_warning(self):
        warnings.warn("""\
transaction ``%s`` is exceeding the recommended maximum number of marks (%d);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                      % (self._v_pathname, self._v_max_group_width),
                      PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)


class MarkG(NotLoggedMixin, Group):
    # Class identifier.
    _c_classid = 'MARKG'

    _c_classId = previous_api_property('_c_classid')

    import re
    _c_shadow_name_re = re.compile(r'^a[0-9]+$')

    def _g_width_warning(self):
        warnings.warn("""\
mark ``%s`` is exceeding the recommended maximum action storage (%d nodes);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                      % (self._v_pathname, self._v_max_group_width),
                      PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)

    def _g_reset(self):
        """Empty action storage (nodes and attributes).

        This method empties all action storage kept in this node: nodes
        and attributes.

        """

        # Remove action storage nodes.
        for child in self._v_children.values():
            child._g_remove(True, True)

        # Remove action storage attributes.
        attrs = self._v_attrs
        shname = self._c_shadow_name_re
        for attrname in attrs._v_attrnamesuser[:]:
            if shname.match(attrname):
                attrs._g__delattr(attrname)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = hdf5Extension
from warnings import warn
from tables.hdf5extension import *

_warnmsg = ("hdf5Extension is pending deprecation, import hdf5extension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = idxutils
# -*- coding: utf-8 -*-

########################################################################
#
#       License: BSD
#       Created: April 02, 2007
#       Author:  Francesc Alted - faltet@pytables.com
#
#       $Id$
#
########################################################################

"""Utilities to be used mainly by the Index class."""

import sys
import math
import numpy

from tables._past import previous_api


# Hints for chunk/slice/block/superblock computations:
# - The slicesize should not exceed 2**32 elements (because of
# implementation reasons).  Such an extreme case would make the
# sorting algorithms to consume up to 64 GB of memory.
# - In general, one should favor a small chunksize ( < 128 KB) if one
# wants to reduce the latency for indexed queries. However, keep in
# mind that a very low value of chunksize for big datasets may hurt
# the performance by requering the HDF5 to use a lot of memory and CPU
# for its internal B-Tree.

def csformula(nrows):
    """Return the fitted chunksize (a float value) for nrows."""

    # This formula has been computed using two points:
    # 2**12 = m * 2**(n + log10(10**6))
    # 2**15 = m * 2**(n + log10(10**9))
    # where 2**12 and 2**15 are reasonable values for chunksizes for indexes
    # with 10**6 and 10**9 elements respectively.
    # Yes, return a floating point number!
    return 64 * 2**math.log10(nrows)


def limit_er(expectedrows):
    """Protection against creating too small or too large chunks or slices."""

    if expectedrows < 10**5:
        expectedrows = 10**5
    elif expectedrows > 10**12:
        expectedrows = 10**12
    return expectedrows


def computechunksize(expectedrows):
    """Get the optimum chunksize based on expectedrows."""

    expectedrows = limit_er(expectedrows)
    zone = int(math.log10(expectedrows))
    nrows = 10**zone
    return int(csformula(nrows))


def computeslicesize(expectedrows, memlevel):
    """Get the optimum slicesize based on expectedrows and memorylevel."""

    expectedrows = limit_er(expectedrows)
    # First, the optimum chunksize
    cs = csformula(expectedrows)
    # Now, the actual chunksize
    chunksize = computechunksize(expectedrows)
    # The optimal slicesize
    ss = int(cs * memlevel**2)
    # We *need* slicesize to be an exact multiple of the actual chunksize
    ss = (ss // chunksize) * chunksize
    ss *= 4    # slicesize should be at least divisible by 4
    # ss cannot be bigger than 2**31 - 1 elements because of fundamental
    # reasons (this limitation comes mainly from the way of compute
    # indices for indexes, but also because C keysort is not implemented
    # yet for the string type).  Besides, it cannot be larger than
    # 2**30, because limitiations of the optimized binary search code
    # (in idx-opt.c, the line ``mid = lo + (hi-lo)/2;`` will overflow
    # for values of ``lo`` and ``hi`` >= 2**30).  Finally, ss must be a
    # multiple of 4, so 2**30 must definitely be an upper limit.
    if ss > 2**30:
        ss = 2**30
    return ss


def computeblocksize(expectedrows, compoundsize, lowercompoundsize):
    """Calculate the optimum number of superblocks made from compounds blocks.

    This is useful for computing the sizes of both blocks and
    superblocks (using the PyTables terminology for blocks in indexes).

    """

    nlowerblocks = (expectedrows // lowercompoundsize) + 1
    if nlowerblocks > 2**20:
        # Protection against too large number of compound blocks
        nlowerblocks = 2**20
    size = lowercompoundsize * nlowerblocks
    # We *need* superblocksize to be an exact multiple of the actual
    # compoundblock size (a ceil must be performed here!)
    size = ((size // compoundsize) + 1) * compoundsize
    return size


def calc_chunksize(expectedrows, optlevel=6, indsize=4, memlevel=4):
    """Calculate the HDF5 chunk size for index and sorted arrays.

    The logic to do that is based purely in experiments playing with
    different chunksizes and compression flag. It is obvious that using
    big chunks optimizes the I/O speed, but if they are too large, the
    uncompressor takes too much time. This might (should) be further
    optimized by doing more experiments.

    """

    chunksize = computechunksize(expectedrows)
    slicesize = computeslicesize(expectedrows, memlevel)

    # Correct the slicesize and the chunksize based on optlevel
    if indsize == 1:  # ultralight
        chunksize, slicesize = ccs_ultralight(optlevel, chunksize, slicesize)
    elif indsize == 2:  # light
        chunksize, slicesize = ccs_light(optlevel, chunksize, slicesize)
    elif indsize == 4:  # medium
        chunksize, slicesize = ccs_medium(optlevel, chunksize, slicesize)
    elif indsize == 8:  # full
        chunksize, slicesize = ccs_full(optlevel, chunksize, slicesize)

    # Finally, compute blocksize and superblocksize
    blocksize = computeblocksize(expectedrows, slicesize, chunksize)
    superblocksize = computeblocksize(expectedrows, blocksize, slicesize)
    # The size for different blocks information
    sizes = (superblocksize, blocksize, slicesize, chunksize)
    return sizes

calcChunksize = previous_api(calc_chunksize)


def ccs_ultralight(optlevel, chunksize, slicesize):
    """Correct the slicesize and the chunksize based on optlevel."""

    if optlevel in (0, 1, 2):
        slicesize //= 2
        slicesize += optlevel * slicesize
    elif optlevel in (3, 4, 5):
        slicesize *= optlevel - 1
    elif optlevel in (6, 7, 8):
        slicesize *= optlevel - 1
    elif optlevel == 9:
        slicesize *= optlevel - 1
    return chunksize, slicesize


def ccs_light(optlevel, chunksize, slicesize):
    """Correct the slicesize and the chunksize based on optlevel."""

    if optlevel in (0, 1, 2):
        slicesize //= 2
    elif optlevel in (3, 4, 5):
        pass
    elif optlevel in (6, 7, 8):
        chunksize /= 2
    elif optlevel == 9:
        # Reducing the chunksize and enlarging the slicesize is the
        # best way to reduce the entropy with the current algorithm.
        chunksize /= 2
        slicesize *= 2
    return chunksize, slicesize


def ccs_medium(optlevel, chunksize, slicesize):
    """Correct the slicesize and the chunksize based on optlevel."""

    if optlevel in (0, 1, 2):
        slicesize //= 2
    elif optlevel in (3, 4, 5):
        pass
    elif optlevel in (6, 7, 8):
        chunksize //= 2
    elif optlevel == 9:
        # Reducing the chunksize and enlarging the slicesize is the
        # best way to reduce the entropy with the current algorithm.
        chunksize //= 2
        slicesize *= 2
    return chunksize, slicesize


def ccs_full(optlevel, chunksize, slicesize):
    """Correct the slicesize and the chunksize based on optlevel."""

    if optlevel in (0, 1, 2):
        slicesize //= 2
    elif optlevel in (3, 4, 5):
        pass
    elif optlevel in (6, 7, 8):
        chunksize //= 2
    elif optlevel == 9:
        # Reducing the chunksize and enlarging the slicesize is the
        # best way to reduce the entropy with the current algorithm.
        chunksize //= 2
        slicesize *= 2
    return chunksize, slicesize


def calcoptlevels(nblocks, optlevel, indsize):
    """Compute the optimizations to be done.

    The calculation is based on the number of blocks, optlevel and
    indexing mode.

    """

    if indsize == 2:  # light
        return col_light(nblocks, optlevel)
    elif indsize == 4:  # medium
        return col_medium(nblocks, optlevel)
    elif indsize == 8:  # full
        return col_full(nblocks, optlevel)


def col_light(nblocks, optlevel):
    """Compute the optimizations to be done for light indexes."""

    optmedian, optstarts, optstops, optfull = (False,) * 4

    if 0 < optlevel <= 3:
        optmedian = True
    elif 3 < optlevel <= 6:
        optmedian, optstarts = (True, True)
    elif 6 < optlevel <= 9:
        optmedian, optstarts, optstops = (True, True, True)

    return optmedian, optstarts, optstops, optfull


def col_medium(nblocks, optlevel):
    """Compute the optimizations to be done for medium indexes."""

    optmedian, optstarts, optstops, optfull = (False,) * 4

    # Medium case
    if nblocks <= 1:
        if 0 < optlevel <= 3:
            optmedian = True
        elif 3 < optlevel <= 6:
            optmedian, optstarts = (True, True)
        elif 6 < optlevel <= 9:
            optfull = 1
    else:  # More than a block
        if 0 < optlevel <= 3:
            optfull = 1
        elif 3 < optlevel <= 6:
            optfull = 2
        elif 6 < optlevel <= 9:
            optfull = 3

    return optmedian, optstarts, optstops, optfull


def col_full(nblocks, optlevel):
    """Compute the optimizations to be done for full indexes."""

    optmedian, optstarts, optstops, optfull = (False,) * 4

    # Full case
    if nblocks <= 1:
        if 0 < optlevel <= 3:
            optmedian = True
        elif 3 < optlevel <= 6:
            optmedian, optstarts = (True, True)
        elif 6 < optlevel <= 9:
            optfull = 1
    else:  # More than a block
        if 0 < optlevel <= 3:
            optfull = 1
        elif 3 < optlevel <= 6:
            optfull = 2
        elif 6 < optlevel <= 9:
            optfull = 3

    return optmedian, optstarts, optstops, optfull


def get_reduction_level(indsize, optlevel, slicesize, chunksize):
    """Compute the reduction level based on indsize and optlevel."""
    rlevels = [
        [8, 8, 8, 8, 4, 4, 4, 2, 2, 1],  # 8-bit indices (ultralight)
        [4, 4, 4, 4, 2, 2, 2, 1, 1, 1],  # 16-bit indices (light)
        [2, 2, 2, 2, 1, 1, 1, 1, 1, 1],  # 32-bit indices (medium)
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # 64-bit indices (full)
    ]
    isizes = {1: 0, 2: 1, 4: 2, 8: 3}
    rlevel = rlevels[isizes[indsize]][optlevel]
    # The next cases should only happen in tests
    if rlevel >= slicesize:
        rlevel = 1
    if slicesize <= chunksize * rlevel:
        rlevel = 1
    if indsize == 8:
        # Ensure that, for full indexes we will never perform a reduction.
        # This is required because of implementation assumptions.
        assert rlevel == 1
    return rlevel


# Python implementations of NextAfter and NextAfterF
#
# These implementations exist because the standard function
# nextafterf is not available on Microsoft platforms.
#
# These implementations are based on the IEEE representation of
# floats and doubles.
# Author:  Shack Toms - shack@livedata.com
#
# Thanks to Shack Toms shack@livedata.com for NextAfter and NextAfterF
# implementations in Python. 2004-10-01
# epsilon  = math.ldexp(1.0, -53) # smallest double such that
#                                 # 0.5 + epsilon != 0.5
# epsilonF = math.ldexp(1.0, -24) # smallest float such that 0.5 + epsilonF
# != 0.5
# maxFloat = float(2**1024 - 2**971)  # From the IEEE 754 standard
# maxFloatF = float(2**128 - 2**104)  # From the IEEE 754 standard
# minFloat  = math.ldexp(1.0, -1022) # min positive normalized double
# minFloatF = math.ldexp(1.0, -126)  # min positive normalized float
# smallEpsilon  = math.ldexp(1.0, -1074) # smallest increment for
#                                        # doubles < minFloat
# smallEpsilonF = math.ldexp(1.0, -149)  # smallest increment for
#                                        # floats < minFloatF
infinity = math.ldexp(1.0, 1023) * 2
infinityf = math.ldexp(1.0, 128)
# Finf = float("inf")  # Infinite in the IEEE 754 standard (not avail in Win)

# A portable representation of NaN
# if sys.byteorder == "little":
#     testNaN = struct.unpack("d", '\x01\x00\x00\x00\x00\x00\xf0\x7f')[0]
# elif sys.byteorder == "big":
#     testNaN = struct.unpack("d", '\x7f\xf0\x00\x00\x00\x00\x00\x01')[0]
# else:
#     raise ValueError("Byteorder '%s' not supported!" % sys.byteorder)
# This one seems better
# testNaN = infinity - infinity

# "infinity" for several types
infinitymap = {
    'bool': [0, 1],
    'int8': [-2**7, 2**7 - 1],
    'uint8': [0, 2**8 - 1],
    'int16': [-2**15, 2**15 - 1],
    'uint16': [0, 2**16 - 1],
    'int32': [-2**31, 2**31 - 1],
    'uint32': [0, 2**32 - 1],
    'int64': [-2**63, 2**63 - 1],
    'uint64': [0, 2**64 - 1],
    'float32': [-infinityf, infinityf],
    'float64': [-infinity, infinity],
}

if hasattr(numpy, 'float16'):
    infinitymap['float16'] = [-numpy.float16(numpy.inf),
                              numpy.float16(numpy.inf)]
if hasattr(numpy, 'float96'):
    infinitymap['float96'] = [-numpy.float96(numpy.inf),
                              numpy.float96(numpy.inf)]
if hasattr(numpy, 'float128'):
    infinitymap['float128'] = [-numpy.float128(numpy.inf),
                               numpy.float128(numpy.inf)]

# deprecated API
infinityMap = infinitymap
infinityF = infinityf

# Utility functions


def inftype(dtype, itemsize, sign=+1):
    """Return a superior limit for maximum representable data type."""

    assert sign in [-1, +1]

    if dtype.kind == "S":
        if sign < 0:
            return b"\x00" * itemsize
        else:
            return b"\xff" * itemsize
    try:
        return infinitymap[dtype.name][sign >= 0]
    except KeyError:
        raise TypeError("Type %s is not supported" % dtype.name)

infType = previous_api(inftype)


def string_next_after(x, direction, itemsize):
    """Return the next representable neighbor of x in the appropriate
    direction."""

    assert direction in [-1, +1]

    # Pad the string with \x00 chars until itemsize completion
    padsize = itemsize - len(x)
    if padsize > 0:
        x += b"\x00" * padsize
    if sys.version_info[0] < 3:
        xlist = list(x)
    else:
        # int.to_bytes is not available in Python < 3.2
        # xlist = [i.to_bytes(1, sys.byteorder) for i in x]
        xlist = [bytes([i]) for i in x]
    xlist.reverse()
    i = 0
    if direction > 0:
        if xlist == b"\xff" * itemsize:
            # Maximum value, return this
            return b"".join(xlist)
        for xchar in xlist:
            if ord(xchar) < 0xff:
                xlist[i] = chr(ord(xchar) + 1).encode('ascii')
                break
            else:
                xlist[i] = b"\x00"
            i += 1
    else:
        if xlist == b"\x00" * itemsize:
            # Minimum value, return this
            return b"".join(xlist)
        for xchar in xlist:
            if ord(xchar) > 0x00:
                xlist[i] = chr(ord(xchar) - 1).encode('ascii')
                break
            else:
                xlist[i] = b"\xff"
            i += 1
    xlist.reverse()
    return b"".join(xlist)

StringNextAfter = previous_api(string_next_after)


def int_type_next_after(x, direction, itemsize):
    """Return the next representable neighbor of x in the appropriate
    direction."""

    assert direction in [-1, +1]

    # x is guaranteed to be either an int or a float
    if direction < 0:
        if isinstance(x, int):
            return x - 1
        else:
            # return int(PyNextAfter(x, x - 1))
            return int(numpy.nextafter(x, x - 1))
    else:
        if isinstance(x, int):
            return x + 1
        else:
            # return int(PyNextAfter(x,x + 1)) + 1
            return int(numpy.nextafter(x, x + 1)) + 1

IntTypeNextAfter = previous_api(int_type_next_after)


def bool_type_next_after(x, direction, itemsize):
    """Return the next representable neighbor of x in the appropriate
    direction."""

    assert direction in [-1, +1]

    # x is guaranteed to be either a boolean
    if direction < 0:
        return False
    else:
        return True

BoolTypeNextAfter = previous_api(bool_type_next_after)


def nextafter(x, direction, dtype, itemsize):
    """Return the next representable neighbor of x in the appropriate
    direction."""

    assert direction in [-1, 0, +1]
    assert dtype.kind == "S" or type(x) in (bool, int, long, float)

    if direction == 0:
        return x

    if dtype.kind == "S":
        return string_next_after(x, direction, itemsize)

    if dtype.kind in ['b']:
        return bool_type_next_after(x, direction, itemsize)
    elif dtype.kind in ['i', 'u']:
        return int_type_next_after(x, direction, itemsize)
    elif dtype.kind == "f":
        if direction < 0:
            return numpy.nextafter(x, x - 1)
        else:
            return numpy.nextafter(x, x + 1)

    # elif dtype.name == "float32":
    #    if direction < 0:
    #        return PyNextAfterF(x,x-1)
    #    else:
    #        return PyNextAfterF(x,x + 1)
    # elif dtype.name == "float64":
    #    if direction < 0:
    #        return PyNextAfter(x,x-1)
    #    else:
    #        return PyNextAfter(x,x + 1)

    raise TypeError("data type ``%s`` is not supported" % dtype)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = index
# -*- coding: utf-8 -*-

#######################################################################
#
# License: BSD
# Created: June 08, 2004
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Index class."""

from __future__ import print_function
import sys
from bisect import bisect_left, bisect_right
from time import time, clock
import os
import os.path
import tempfile
import math
import warnings

import numpy

from tables.idxutils import (calc_chunksize, calcoptlevels,
                             get_reduction_level, nextafter, inftype)

from tables import indexesextension
from tables.node import NotLoggedMixin
from tables.atom import UIntAtom, Atom
from tables.earray import EArray
from tables.carray import CArray
from tables.leaf import Filters
from tables.indexes import CacheArray, LastRowArray, IndexArray
from tables.group import Group
from tables.path import join_path
from tables.exceptions import PerformanceWarning
from tables.utils import is_idx, idx2long, lazyattr
from tables.lrucacheextension import ObjectCache

from tables._past import previous_api, previous_api_property


# default version for INDEX objects
# obversion = "1.0"    # Version of indexes in PyTables 1.x series
# obversion = "2.0"    # Version of indexes in PyTables Pro 2.0 series
obversion = "2.1"     # Version of indexes in PyTables Pro 2.1 and up series,
                      # including the join 2.3 Std + Pro version


debug = False
# debug = True  # Uncomment this for printing sizes purposes
profile = False
# profile = True  # Uncomment for profiling
if profile:
    from tables.utils import show_stats


# The default method for sorting
defsort = "quicksort"
# defsort = "mergesort"

# Default policy for automatically updating indexes after a table
# append operation, or automatically reindexing after an
# index-invalidating operation like removing or modifying table rows.
default_auto_index = True
# Keep in sync with ``Table.autoindex`` docstring.

# Default filters used to compress indexes.  This is quite fast and
# compression is pretty good.
# Remember to keep these defaults in sync with the docstrings and UG.
default_index_filters = Filters(complevel=1, complib='zlib',
                                shuffle=True, fletcher32=False)

# Deprecated API
defaultAutoIndex = default_auto_index
defaultIndexFilters = default_index_filters

# The list of types for which an optimised search in cython and C has
# been implemented. Always add here the name of a new optimised type.
opt_search_types = ("int8", "int16", "int32", "int64",
                    "uint8", "uint16", "uint32", "uint64",
                    "float32", "float64")

# The upper limit for uint32 ints
max32 = 2**32


def _table_column_pathname_of_index(indexpathname):
    names = indexpathname.split("/")
    for i, name in enumerate(names):
        if name.startswith('_i_'):
            break
    tablepathname = "/".join(names[:i]) + "/" + name[3:]
    colpathname = "/".join(names[i + 1:])
    return (tablepathname, colpathname)

_tableColumnPathnameOfIndex = previous_api(_table_column_pathname_of_index)


class Index(NotLoggedMixin, indexesextension.Index, Group):
    """Represents the index of a column in a table.

    This class is used to keep the indexing information for columns in a Table
    dataset (see :ref:`TableClassDescr`). It is actually a descendant of the
    Group class (see :ref:`GroupClassDescr`), with some added functionality. An
    Index is always associated with one and only one column in the table.

    .. note::

        This class is mainly intended for internal use, but some of its
        documented attributes and methods may be interesting for the
        programmer.

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.
    atom : Atom
        An Atom object representing the shape and type of the atomic objects to
        be saved. Only scalar atoms are supported.
    title
        Sets a TITLE attribute of the Index entity.
    kind
        The desired kind for this index.  The 'full' kind specifies a complete
        track of the row position (64-bit), while the 'medium', 'light' or
        'ultralight' kinds only specify in which chunk the row is (using
        32-bit, 16-bit and 8-bit respectively).
    optlevel
        The desired optimization level for this index.
    filters : Filters
        An instance of the Filters class that provides information about the
        desired I/O filters to be applied during the life of this object.
    tmp_dir
        The directory for the temporary files.
    expectedrows
        Represents an user estimate about the number of row slices that will be
        added to the growable dimension in the IndexArray object.
    byteorder
        The byteorder of the index datasets *on-disk*.
    blocksizes
        The four main sizes of the compound blocks in index datasets (a low
        level parameter).

    """

    _c_classid = 'INDEX'

    _c_classId = previous_api_property('_c_classid')

    # <properties>
    kind = property(
        lambda self: {1: 'ultralight', 2: 'light',
                      4: 'medium', 8: 'full'}[self.indsize],
        None, None,
        "The kind of this index.")

    filters = property(
        lambda self: self._v_filters, None, None,
        """Filter properties for this index - see Filters in
        :ref:`FiltersClassDescr`.""")

    def _getdirty(self):
        if 'DIRTY' not in self._v_attrs:
            # If there is no ``DIRTY`` attribute, index should be clean.
            return False
        return self._v_attrs.DIRTY

    def _setdirty(self, dirty):
        wasdirty, isdirty = self.dirty, bool(dirty)
        self._v_attrs.DIRTY = dirty
        # If an *actual* change in dirtiness happens,
        # notify the condition cache by setting or removing a nail.
        conditioncache = self.table._condition_cache
        if not wasdirty and isdirty:
            conditioncache.nail()
        if wasdirty and not isdirty:
            conditioncache.unnail()

    dirty = property(
        _getdirty, _setdirty, None,
        """Whether the index is dirty or not.

        Dirty indexes are out of sync with column data, so they exist but they
        are not usable.
        """)

    def _getcolumn(self):
        tablepath, columnpath = _table_column_pathname_of_index(
            self._v_pathname)
        table = self._v_file._get_node(tablepath)
        column = table.cols._g_col(columnpath)
        return column

    column = property(_getcolumn, None, None,
        """The Column (see :ref:`ColumnClassDescr`) instance for the indexed
        column.""")

    def _gettable(self):
        tablepath, columnpath = _table_column_pathname_of_index(
            self._v_pathname)
        table = self._v_file._get_node(tablepath)
        return table

    table = property(_gettable, None, None,
                     "Accessor for the `Table` object of this index.")

    nblockssuperblock = property(
        lambda self: self.superblocksize // self.blocksize, None, None,
        "The number of blocks in a superblock.")

    nslicesblock = property(
        lambda self: self.blocksize // self.slicesize, None, None,
        "The number of slices in a block.")

    nchunkslice = property(
        lambda self: self.slicesize // self.chunksize, None, None,
        "The number of chunks in a slice.")

    def _g_nsuperblocks(self):
        # Last row should not be considered as a superblock
        nelements = self.nelements - self.nelementsILR
        nblocks = nelements // self.superblocksize
        if nelements % self.blocksize > 0:
            nblocks += 1
        return nblocks

    nsuperblocks = property(_g_nsuperblocks, None, None,
                            "The total number of superblocks in index.")

    def _g_nblocks(self):
        # Last row should not be considered as a block
        nelements = self.nelements - self.nelementsILR
        nblocks = nelements // self.blocksize
        if nelements % self.blocksize > 0:
            nblocks += 1
        return nblocks

    nblocks = property(_g_nblocks, None, None,
                       "The total number of blocks in index.")

    nslices = property(
        lambda self: self.nelements // self.slicesize, None, None,
        "The number of complete slices in index.")

    nchunks = property(
        lambda self: self.nelements // self.chunksize, None, None,
        "The number of complete chunks in index.")

    shape = property(
        lambda self: (self.nrows, self.slicesize), None, None,
        "The shape of this index (in slices and elements).")

    temp_required = property(
        lambda self: (self.indsize > 1 and
                      self.optlevel > 0 and
                      self.table.nrows > self.slicesize),
        None, None,
        "Whether a temporary file for indexes is required or not.")

    want_complete_sort = property(
        lambda self: (self.indsize == 8 and self.optlevel == 9),
        None, None,
        "Whether we should try to build a completely sorted index or not.")

    def _is_csi(self):
        if self.nelements == 0:
            # An index with 0 indexed elements is not a CSI one (by definition)
            return False
        if self.indsize < 8:
            # An index that is not full cannot be completely sorted
            return False
        # Try with the 'is_csi' attribute
        if 'is_csi' in self._v_attrs:
            return self._v_attrs.is_csi
        # If not, then compute the overlaps manually
        # (the attribute 'is_csi' will be set there)
        self.compute_overlaps(self, None, False)
        return self.noverlaps == 0

    _is_CSI = previous_api(_is_csi)

    is_csi = property(_is_csi, None, None,
        """Whether the index is completely sorted or not.

        .. versionchanged:: 3.0
           The *is_CSI* property has been renamed into *is_csi*.

        """)

    is_CSI = previous_api(is_csi)

    @lazyattr
    def nrowsinchunk(self):
        """The number of rows that fits in a *table* chunk."""

        return self.table.chunkshape[0]

    @lazyattr
    def lbucket(self):
        """Return the length of a bucket based index type."""

        # Avoid to set a too large lbucket size (mainly useful for tests)
        lbucket = min(self.nrowsinchunk, self.chunksize)
        if self.indsize == 1:
            # For ultra-light, we will never have to keep track of a
            # bucket outside of a slice.
            maxnb = 2**8
            if self.slicesize > maxnb * lbucket:
                lbucket = int(math.ceil(float(self.slicesize) / maxnb))
        elif self.indsize == 2:
            # For light, we will never have to keep track of a
            # bucket outside of a block.
            maxnb = 2**16
            if self.blocksize > maxnb * lbucket:
                lbucket = int(math.ceil(float(self.blocksize) / maxnb))
        else:
            # For medium and full indexes there should not be a need to
            # increase lbucket
            pass
        return lbucket

    # </properties>
    def __init__(self, parentnode, name,
                 atom=None, title="",
                 kind=None,
                 optlevel=None,
                 filters=None,
                 tmp_dir=None,
                 expectedrows=0,
                 byteorder=None,
                 blocksizes=None,
                 new=True):

        self._v_version = None
        """The object version of this index."""
        self.optlevel = optlevel
        """The optimization level for this index."""
        self.tmp_dir = tmp_dir
        """The directory for the temporary files."""
        self.expectedrows = expectedrows
        """The expected number of items of index arrays."""
        if byteorder in ["little", "big"]:
            self.byteorder = byteorder
        else:
            self.byteorder = sys.byteorder
        """The byteorder of the index datasets."""
        if atom is not None:
            self.dtype = atom.dtype.base
            self.type = atom.type
            """The datatypes to be stored by the sorted index array."""
            ############### Important note ###########################
            # The datatypes saved as index values are NumPy native
            # types, so we get rid of type metainfo like Time* or Enum*
            # that belongs to HDF5 types (actually, this metainfo is
            # not needed for sorting and looking-up purposes).
            ##########################################################
            indsize = {
                'ultralight': 1, 'light': 2, 'medium': 4, 'full': 8}[kind]
            assert indsize in (1, 2, 4, 8), "indsize should be 1, 2, 4 or 8!"
            self.indsize = indsize
            """The itemsize for the indices part of the index."""

        self.nrows = None
        """The total number of slices in the index."""
        self.nelements = None
        """The number of currently indexed rows for this column."""
        self.blocksizes = blocksizes
        """The four main sizes of the compound blocks (if specified)."""
        self.dirtycache = True
        """Dirty cache (for ranges, bounds & sorted) flag."""
        self.superblocksize = None
        """Size of the superblock for this index."""
        self.blocksize = None
        """Size of the block for this index."""
        self.slicesize = None
        """Size of the slice for this index."""
        self.chunksize = None
        """Size of the chunk for this index."""
        self.tmpfilename = None
        """Filename for temporary bounds."""
        self.opt_search_types = opt_search_types
        """The types for which and optimized search has been implemented."""
        self.noverlaps = -1
        """The number of overlaps in an index.  0 means a completely
        sorted index. -1 means that this number is not computed yet."""
        self.tprof = 0
        """Time counter for benchmarking purposes."""

        from tables.file import open_file
        self._openFile = open_file
        """The `open_file()` function, to avoid a circular import."""

        super(Index, self).__init__(parentnode, name, title, new, filters)

    def _g_post_init_hook(self):
        if self._v_new:
            # The version for newly created indexes
            self._v_version = obversion
        super(Index, self)._g_post_init_hook()

        # Index arrays must only be created for new indexes
        if not self._v_new:
            idxversion = self._v_version
            # Set-up some variables from info on disk and return
            attrs = self._v_attrs
            # Coerce NumPy scalars to Python scalars in order
            # to avoid undesired upcasting operations.
            self.superblocksize = long(attrs.superblocksize)
            self.blocksize = long(attrs.blocksize)
            self.slicesize = int(attrs.slicesize)
            self.chunksize = int(attrs.chunksize)
            self.blocksizes = (self.superblocksize, self.blocksize,
                               self.slicesize, self.chunksize)
            self.optlevel = int(attrs.optlevel)
            sorted = self.sorted
            indices = self.indices
            self.dtype = sorted.atom.dtype
            self.type = sorted.atom.type
            self.indsize = indices.atom.itemsize
            # Some sanity checks for slicesize, chunksize and indsize
            assert self.slicesize == indices.shape[1], "Wrong slicesize"
            assert self.chunksize == indices._v_chunkshape[
                1], "Wrong chunksize"
            assert self.indsize in (1, 2, 4, 8), "Wrong indices itemsize"
            if idxversion > "2.0":
                self.reduction = int(attrs.reduction)
                nelementsSLR = int(self.sortedLR.attrs.nelements)
                nelementsILR = int(self.indicesLR.attrs.nelements)
            else:
                self.reduction = 1
                nelementsILR = self.indicesLR[-1]
                nelementsSLR = nelementsILR
            self.nrows = sorted.nrows
            self.nelements = self.nrows * self.slicesize + nelementsILR
            self.nelementsSLR = nelementsSLR
            self.nelementsILR = nelementsILR
            if nelementsILR > 0:
                self.nrows += 1
            # Get the bounds as a cache (this has to remain here!)
            rchunksize = self.chunksize // self.reduction
            nboundsLR = (nelementsSLR - 1) // rchunksize
            if nboundsLR < 0:
                nboundsLR = 0  # correction for -1 bounds
            nboundsLR += 2  # bounds + begin + end
            # All bounds values (+begin + end) are at the end of sortedLR
            self.bebounds = self.sortedLR[
                nelementsSLR:nelementsSLR + nboundsLR]
            return

        # The index is new. Initialize the values
        self.nrows = 0
        self.nelements = 0
        self.nelementsSLR = 0
        self.nelementsILR = 0

        # The atom
        atom = Atom.from_dtype(self.dtype)

        # The filters
        filters = self.filters

        # Compute the superblocksize, blocksize, slicesize and chunksize values
        # (in case these parameters haven't been passed to the constructor)
        if self.blocksizes is None:
            self.blocksizes = calc_chunksize(
                self.expectedrows, self.optlevel, self.indsize)
        (self.superblocksize, self.blocksize,
         self.slicesize, self.chunksize) = self.blocksizes
        if debug:
            print("blocksizes:", self.blocksizes)
        # Compute the reduction level
        self.reduction = get_reduction_level(
            self.indsize, self.optlevel, self.slicesize, self.chunksize)
        rchunksize = self.chunksize // self.reduction
        rslicesize = self.slicesize // self.reduction

        # Save them on disk as attributes
        self._v_attrs.superblocksize = numpy.uint64(self.superblocksize)
        self._v_attrs.blocksize = numpy.uint64(self.blocksize)
        self._v_attrs.slicesize = numpy.uint32(self.slicesize)
        self._v_attrs.chunksize = numpy.uint32(self.chunksize)
        # Save the optlevel as well
        self._v_attrs.optlevel = self.optlevel
        # Save the reduction level
        self._v_attrs.reduction = self.reduction

        # Create the IndexArray for sorted values
        sorted = IndexArray(self, 'sorted', atom, "Sorted Values",
                            filters, self.byteorder)

        # Create the IndexArray for index values
        IndexArray(self, 'indices', UIntAtom(itemsize=self.indsize),
                   "Number of chunk in table", filters, self.byteorder)

        # Create the cache for range values  (1st order cache)
        CacheArray(self, 'ranges', atom, (0, 2), "Range Values", filters,
                   self.expectedrows // self.slicesize,
                   byteorder=self.byteorder)
        # median ranges
        EArray(self, 'mranges', atom, (0,), "Median ranges", filters,
               byteorder=self.byteorder, _log=False)

        # Create the cache for boundary values (2nd order cache)
        nbounds_inslice = (rslicesize - 1) // rchunksize
        CacheArray(self, 'bounds', atom, (0, nbounds_inslice),
                   "Boundary Values", filters, self.nchunks,
                   (1, nbounds_inslice), byteorder=self.byteorder)

        # begin, end & median bounds (only for numerical types)
        EArray(self, 'abounds', atom, (0,), "Start bounds", filters,
               byteorder=self.byteorder, _log=False)
        EArray(self, 'zbounds', atom, (0,), "End bounds", filters,
               byteorder=self.byteorder, _log=False)
        EArray(self, 'mbounds', atom, (0,), "Median bounds", filters,
               byteorder=self.byteorder, _log=False)

        # Create the Array for last (sorted) row values + bounds
        shape = (rslicesize + 2 + nbounds_inslice,)
        sortedLR = LastRowArray(self, 'sortedLR', atom, shape,
                                "Last Row sorted values + bounds",
                                filters, (rchunksize,),
                                byteorder=self.byteorder)

        # Create the Array for the number of chunk in last row
        shape = (self.slicesize,)     # enough for indexes and length
        indicesLR = LastRowArray(self, 'indicesLR',
                                 UIntAtom(itemsize=self.indsize),
                                 shape, "Last Row indices",
                                 filters, (self.chunksize,),
                                 byteorder=self.byteorder)

        # The number of elements in LR will be initialized here
        sortedLR.attrs.nelements = 0
        indicesLR.attrs.nelements = 0

        # All bounds values (+begin + end) are uninitialized in creation time
        self.bebounds = None

        # The starts and lengths initialization
        self.starts = numpy.empty(shape=self.nrows, dtype=numpy.int32)
        """Where the values fulfiling conditions starts for every slice."""
        self.lengths = numpy.empty(shape=self.nrows, dtype=numpy.int32)
        """Lengths of the values fulfilling conditions for every slice."""

        # Finally, create a temporary file for indexes if needed
        if self.temp_required:
            self.create_temp()

    _g_postInitHook = previous_api(_g_post_init_hook)

    def initial_append(self, xarr, nrow, reduction):
        """Compute an initial indices arrays for data to be indexed."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering initial_append", tref)
        arr = xarr.pop()
        indsize = self.indsize
        slicesize = self.slicesize
        nelementsILR = self.nelementsILR
        if profile:
            show_stats("Before creating idx", tref)
        if indsize == 8:
            idx = numpy.arange(0, len(arr), dtype="uint64") + nrow * slicesize
        elif indsize == 4:
            # For medium (32-bit) all the rows in tables should be
            # directly reachable.  But as len(arr) < 2**31, we can
            # choose uint32 for representing indices.  In this way, we
            # consume far less memory during the keysort process.  The
            # offset will be added in self.final_idx32() later on.
            #
            # This optimization also prevents the values in LR to
            # participate in the ``swap_chunks`` process, and this is
            # the main reason to not allow the medium indexes to create
            # completely sorted indexes.  However, I don't find this to
            # be a big limitation, as probably fully indexes are much
            # more suitable for producing completely sorted indexes
            # because in this case the indices part is usable for
            # getting the reverse indices of the index, and I forsee
            # this to be a common requirement in many operations (for
            # example, in table sorts).
            #
            # F. Alted 2008-09-15
            idx = numpy.arange(0, len(arr), dtype="uint32")
        else:
            idx = numpy.empty(len(arr), "uint%d" % (indsize * 8))
            lbucket = self.lbucket
            # Fill the idx with the bucket indices
            offset = lbucket - ((nrow * (slicesize % lbucket)) % lbucket)
            idx[0:offset] = 0
            for i in xrange(offset, slicesize, lbucket):
                idx[i:i + lbucket] = (i + lbucket - 1) // lbucket
            if indsize == 2:
                # Add a second offset in this case
                # First normalize the number of rows
                offset2 = (nrow % self.nslicesblock) * slicesize // lbucket
                idx += offset2
        # Add the last row at the beginning of arr & idx (if needed)
        if (indsize == 8 and nelementsILR > 0):
            # It is possible that the values in LR are already sorted.
            # Fetch them and override existing values in arr and idx.
            assert len(arr) > nelementsILR
            self.read_slice_lr(self.sortedLR, arr[:nelementsILR])
            self.read_slice_lr(self.indicesLR, idx[:nelementsILR])
        # In-place sorting
        if profile:
            show_stats("Before keysort", tref)
        indexesextension.keysort(arr, idx)
        larr = arr[-1]
        if reduction > 1:
            # It's important to do a copy() here in order to ensure that
            # sorted._append() will receive a contiguous array.
            if profile:
                show_stats("Before reduction", tref)
            reduc = arr[::reduction].copy()
            if profile:
                show_stats("After reduction", tref)
            arr = reduc
            if profile:
                show_stats("After arr <-- reduc", tref)
        # A completely sorted index is not longer possible after an
        # append of an index with already one slice.
        if nrow > 0:
            self._v_attrs.is_csi = False
        if profile:
            show_stats("Exiting initial_append", tref)
        return larr, arr, idx

    def final_idx32(self, idx, offset):
        """Perform final operations in 32-bit indices."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering final_idx32", tref)
        # Do an upcast first in order to add the offset.
        idx = idx.astype('uint64')
        idx += offset
        # The next partition is valid up to table sizes of
        # 2**30 * 2**18 = 2**48 bytes, that is, 256 Tera-elements,
        # which should be a safe figure, at least for a while.
        idx //= self.lbucket
        # After the division, we can downsize the indexes to 'uint32'
        idx = idx.astype('uint32')
        if profile:
            show_stats("Exiting final_idx32", tref)
        return idx

    def append(self, xarr, update=False):
        """Append the array to the index objects."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering append", tref)
        if not update and self.temp_required:
            where = self.tmp
            # The reduction will take place *after* the optimization process
            reduction = 1
        else:
            where = self
            reduction = self.reduction
        sorted = where.sorted
        indices = where.indices
        ranges = where.ranges
        mranges = where.mranges
        bounds = where.bounds
        mbounds = where.mbounds
        abounds = where.abounds
        zbounds = where.zbounds
        sortedLR = where.sortedLR
        indicesLR = where.indicesLR
        nrows = sorted.nrows  # before sorted.append()
        larr, arr, idx = self.initial_append(xarr, nrows, reduction)
        # Save the sorted array
        sorted.append(arr.reshape(1, arr.size))
        cs = self.chunksize // reduction
        ncs = self.nchunkslice
        # Save ranges & bounds
        ranges.append([[arr[0], larr]])
        bounds.append([arr[cs::cs]])
        abounds.append(arr[0::cs])
        zbounds.append(arr[cs - 1::cs])
        # Compute the medians
        smedian = arr[cs // 2::cs]
        mbounds.append(smedian)
        mranges.append([smedian[ncs // 2]])
        if profile:
            show_stats("Before deleting arr & smedian", tref)
        del arr, smedian   # delete references
        if profile:
            show_stats("After deleting arr & smedian", tref)
        # Now that arr is gone, we can upcast the indices and add the offset
        if self.indsize == 4:
            idx = self.final_idx32(idx, nrows * self.slicesize)
        indices.append(idx.reshape(1, idx.size))
        if profile:
            show_stats("Before deleting idx", tref)
        del idx
        # Update counters after a successful append
        self.nrows = nrows + 1
        self.nelements = self.nrows * self.slicesize
        self.nelementsSLR = 0  # reset the counter of the last row index to 0
        self.nelementsILR = 0  # reset the counter of the last row index to 0
        # The number of elements will be saved as an attribute.
        # This is necessary in case the LR arrays can remember its values
        # after a possible node preemtion/reload.
        sortedLR.attrs.nelements = self.nelementsSLR
        indicesLR.attrs.nelements = self.nelementsILR
        self.dirtycache = True   # the cache is dirty now
        if profile:
            show_stats("Exiting append", tref)

    def append_last_row(self, xarr, update=False):
        """Append the array to the last row index objects."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering appendLR", tref)
        # compute the elements in the last row sorted & bounds array
        nrows = self.nslices
        if not update and self.temp_required:
            where = self.tmp
            # The reduction will take place *after* the optimization process
            reduction = 1
        else:
            where = self
            reduction = self.reduction
        indicesLR = where.indicesLR
        sortedLR = where.sortedLR
        larr, arr, idx = self.initial_append(xarr, nrows, reduction)
        nelementsSLR = len(arr)
        nelementsILR = len(idx)
        # Build the cache of bounds
        rchunksize = self.chunksize // reduction
        self.bebounds = numpy.concatenate((arr[::rchunksize], [larr]))
        # The number of elements will be saved as an attribute
        sortedLR.attrs.nelements = nelementsSLR
        indicesLR.attrs.nelements = nelementsILR
        # Save the number of elements, bounds and sorted values
        # at the end of the sorted array
        offset2 = len(self.bebounds)
        sortedLR[nelementsSLR:nelementsSLR + offset2] = self.bebounds
        sortedLR[:nelementsSLR] = arr
        del arr
        # Now that arr is gone, we can upcast the indices and add the offset
        if self.indsize == 4:
            idx = self.final_idx32(idx, nrows * self.slicesize)
        # Save the reverse index array
        indicesLR[:len(idx)] = idx
        del idx
        # Update counters after a successful append
        self.nrows = nrows + 1
        self.nelements = nrows * self.slicesize + nelementsILR
        self.nelementsILR = nelementsILR
        self.nelementsSLR = nelementsSLR
        self.dirtycache = True   # the cache is dirty now
        if profile:
            show_stats("Exiting appendLR", tref)

    appendLastRow = previous_api(append_last_row)

    def optimize(self, verbose=False):
        """Optimize an index so as to allow faster searches.

        verbose
            If True, messages about the progress of the
            optimization process are printed out.

        """

        if not self.temp_required:
            return

        if verbose:
            self.verbose = True
        else:
            self.verbose = debug

        # Initialize last_tover and last_nover
        self.last_tover = 0
        self.last_nover = 0

        # Compute the correct optimizations for current optim level
        opts = calcoptlevels(self.nblocks, self.optlevel, self.indsize)
        optmedian, optstarts, optstops, optfull = opts

        if debug:
            print("optvalues:", opts)

        self.create_temp2()
        # Start the optimization process
        while True:
            if optfull:
                for niter in range(optfull):
                    if self.swap('chunks', 'median'):
                        break
                    if self.nblocks > 1:
                        # Swap slices only in the case that we have
                        # several blocks
                        if self.swap('slices', 'median'):
                            break
                        if self.swap('chunks', 'median'):
                            break
                    if self.swap('chunks', 'start'):
                        break
                    if self.swap('chunks', 'stop'):
                        break
            else:
                if optmedian:
                    if self.swap('chunks', 'median'):
                        break
                if optstarts:
                    if self.swap('chunks', 'start'):
                        break
                if optstops:
                    if self.swap('chunks', 'stop'):
                        break
            break  # If we reach this, exit the loop

        # Check if we require a complete sort.  Important: this step
        # should be carried out *after* the optimization process has
        # been completed (this is to guarantee that the complete sort
        # does not take too much memory).
        if self.want_complete_sort:
            if self.noverlaps > 0:
                self.do_complete_sort()
            # Check that we have effectively achieved the complete sort
            if self.noverlaps > 0:
                warnings.warn(
                    "OPSI was not able to achieve a completely sorted index."
                    "  Please report this to the authors.", UserWarning)

        # Close and delete the temporal optimization index file
        self.cleanup_temp()
        return

    def do_complete_sort(self):
        """Bring an already optimized index into a complete sorted state."""

        if self.verbose:
            t1 = time()
            c1 = clock()
        ss = self.slicesize
        tmp = self.tmp
        ranges = tmp.ranges[:]
        nslices = self.nslices

        nelementsLR = self.nelementsILR
        if nelementsLR > 0:
            # Add the ranges corresponding to the last row
            rangeslr = numpy.array([self.bebounds[0], self.bebounds[-1]])
            ranges = numpy.concatenate((ranges, [rangeslr]))
            nslices += 1

        sorted = tmp.sorted
        indices = tmp.indices
        sortedLR = tmp.sortedLR
        indicesLR = tmp.indicesLR
        sremain = numpy.array([], dtype=self.dtype)
        iremain = numpy.array([], dtype='u%d' % self.indsize)
        starts = numpy.zeros(shape=nslices, dtype=numpy.int_)
        for i in xrange(nslices):
            # Find the overlapping elements for slice i
            sover = numpy.array([], dtype=self.dtype)
            iover = numpy.array([], dtype='u%d' % self.indsize)
            prev_end = ranges[i, 1]
            for j in xrange(i + 1, nslices):
                stj = starts[j]
                if ((j < self.nslices and stj == ss) or
                        (j == self.nslices and stj == nelementsLR)):
                    # This slice has been already dealt with
                    continue
                if j < self.nslices:
                    assert stj < ss, \
                        "Two slices cannot overlap completely at this stage!"
                    next_beg = sorted[j, stj]
                else:
                    assert stj < nelementsLR, \
                        "Two slices cannot overlap completely at this stage!"
                    next_beg = sortedLR[stj]
                next_end = ranges[j, 1]
                if prev_end > next_end:
                    # Complete overlapping case
                    if j < self.nslices:
                        sover = numpy.concatenate((sover, sorted[j, stj:]))
                        iover = numpy.concatenate((iover, indices[j, stj:]))
                        starts[j] = ss
                    else:
                        n = nelementsLR
                        sover = numpy.concatenate((sover, sortedLR[stj:n]))
                        iover = numpy.concatenate((iover, indicesLR[stj:n]))
                        starts[j] = nelementsLR
                elif prev_end > next_beg:
                    idx = self.search_item_lt(tmp, prev_end, j, ranges[j], stj)
                    if j < self.nslices:
                        sover = numpy.concatenate((sover, sorted[j, stj:idx]))
                        iover = numpy.concatenate((iover, indices[j, stj:idx]))
                    else:
                        sover = numpy.concatenate((sover, sortedLR[stj:idx]))
                        iover = numpy.concatenate((iover, indicesLR[stj:idx]))
                    starts[j] = idx
            # Build the extended slices to sort out
            if i < self.nslices:
                ssorted = numpy.concatenate(
                    (sremain, sorted[i, starts[i]:], sover))
                sindices = numpy.concatenate(
                    (iremain, indices[i, starts[i]:], iover))
            else:
                ssorted = numpy.concatenate(
                    (sremain, sortedLR[starts[i]:nelementsLR], sover))
                sindices = numpy.concatenate(
                    (iremain, indicesLR[starts[i]:nelementsLR], iover))
            # Sort the extended slices
            indexesextension.keysort(ssorted, sindices)
            # Save the first elements of extended slices in the slice i
            if i < self.nslices:
                sorted[i] = ssorted[:ss]
                indices[i] = sindices[:ss]
                # Update caches for this slice
                self.update_caches(i, ssorted[:ss])
                # Save the remaining values in a separate array
                send = len(sover) + len(sremain)
                sremain = ssorted[ss:ss + send]
                iremain = sindices[ss:ss + send]
            else:
                # Still some elements remain for the last row
                n = len(ssorted)
                assert n == nelementsLR
                send = 0
                sortedLR[:n] = ssorted
                indicesLR[:n] = sindices
                # Update the caches for last row
                sortedlr = sortedLR[:nelementsLR]
                bebounds = numpy.concatenate(
                    (sortedlr[::self.chunksize], [sortedlr[-1]]))
                sortedLR[nelementsLR:nelementsLR + len(bebounds)] = bebounds
                self.bebounds = bebounds

        # Verify that we have dealt with all the remaining values
        assert send == 0

        # Compute the overlaps in order to verify that we have achieved
        # a complete sort.  This has to be executed always (and not only
        # in verbose mode!).
        self.compute_overlaps(self.tmp, "do_complete_sort()", self.verbose)
        if self.verbose:
            t = round(time() - t1, 4)
            c = round(clock() - c1, 4)
            print("time: %s. clock: %s" % (t, c))

    def swap(self, what, mode=None):
        """Swap chunks or slices using a certain bounds reference."""

        # Thresholds for avoiding continuing the optimization
        # thnover = 4 * self.slicesize  # minimum number of overlapping
        #                               # elements
        thnover = 40
        thmult = 0.1      # minimum ratio of multiplicity (a 10%)
        thtover = 0.01    # minimum overlaping index for slices (a 1%)

        if self.verbose:
            t1 = time()
            c1 = clock()
        if what == "chunks":
            self.swap_chunks(mode)
        elif what == "slices":
            self.swap_slices(mode)
        if mode:
            message = "swap_%s(%s)" % (what, mode)
        else:
            message = "swap_%s" % (what,)
        (nover, mult, tover) = self.compute_overlaps(
            self.tmp, message, self.verbose)
        rmult = len(mult.nonzero()[0]) / float(len(mult))
        if self.verbose:
            t = round(time() - t1, 4)
            c = round(clock() - c1, 4)
            print("time: %s. clock: %s" % (t, c))
        # Check that entropy is actually decreasing
        if what == "chunks" and self.last_tover > 0. and self.last_nover > 0:
            tover_var = (self.last_tover - tover) / self.last_tover
            nover_var = (self.last_nover - nover) / self.last_nover
            if tover_var < 0.05 and nover_var < 0.05:
                # Less than a 5% of improvement is too few
                return True
        self.last_tover = tover
        self.last_nover = nover
        # Check if some threshold has met
        if nover < thnover:
            return True
        if rmult < thmult:
            return True
        # Additional check for the overlap ratio
        if tover >= 0. and tover < thtover:
            return True
        return False

    def create_temp(self):
        """Create some temporary objects for slice sorting purposes."""

        # The index will be dirty during the index optimization process
        self.dirty = True
        # Build the name of the temporary file
        fd, self.tmpfilename = tempfile.mkstemp(
            ".tmp", "pytables-", self.tmp_dir)
        # Close the file descriptor so as to avoid leaks
        os.close(fd)
        # Create the proper PyTables file
        self.tmpfile = self._openFile(self.tmpfilename, "w")
        self.tmp = tmp = self.tmpfile.root
        cs = self.chunksize
        ss = self.slicesize
        filters = self.filters
        # temporary sorted & indices arrays
        shape = (0, ss)
        atom = Atom.from_dtype(self.dtype)
        EArray(tmp, 'sorted', atom, shape,
               "Temporary sorted", filters, chunkshape=(1, cs))
        EArray(tmp, 'indices', UIntAtom(itemsize=self.indsize), shape,
               "Temporary indices", filters, chunkshape=(1, cs))
        # temporary bounds
        nbounds_inslice = (ss - 1) // cs
        shape = (0, nbounds_inslice)
        EArray(tmp, 'bounds', atom, shape, "Temp chunk bounds",
               filters, chunkshape=(cs, nbounds_inslice))
        shape = (0,)
        EArray(tmp, 'abounds', atom, shape, "Temp start bounds",
               filters, chunkshape=(cs,))
        EArray(tmp, 'zbounds', atom, shape, "Temp end bounds",
               filters, chunkshape=(cs,))
        EArray(tmp, 'mbounds', atom, shape, "Median bounds",
               filters, chunkshape=(cs,))
        # temporary ranges
        EArray(tmp, 'ranges', atom, (0, 2),
               "Temporary range values", filters, chunkshape=(cs, 2))
        EArray(tmp, 'mranges', atom, (0,),
               "Median ranges", filters, chunkshape=(cs,))
        # temporary last row (sorted)
        shape = (ss + 2 + nbounds_inslice,)
        CArray(tmp, 'sortedLR', atom, shape,
               "Temp Last Row sorted values + bounds",
               filters, chunkshape=(cs,))
        # temporary last row (indices)
        shape = (ss,)
        CArray(tmp, 'indicesLR',
               UIntAtom(itemsize=self.indsize),
               shape, "Temp Last Row indices",
               filters, chunkshape=(cs,))

    def create_temp2(self):
        """Create some temporary objects for slice sorting purposes."""

        # The algorithms for doing the swap can be optimized so that
        # one should be necessary to create temporaries for keeping just
        # the contents of a single superblock.
        # F. Alted 2007-01-03
        cs = self.chunksize
        ss = self.slicesize
        filters = self.filters
        # temporary sorted & indices arrays
        shape = (self.nslices, ss)
        atom = Atom.from_dtype(self.dtype)
        tmp = self.tmp
        CArray(tmp, 'sorted2', atom, shape,
               "Temporary sorted 2", filters, chunkshape=(1, cs))
        CArray(tmp, 'indices2', UIntAtom(itemsize=self.indsize), shape,
               "Temporary indices 2", filters, chunkshape=(1, cs))
        # temporary bounds
        nbounds_inslice = (ss - 1) // cs
        shape = (self.nslices, nbounds_inslice)
        CArray(tmp, 'bounds2', atom, shape, "Temp chunk bounds 2",
               filters, chunkshape=(cs, nbounds_inslice))
        shape = (self.nchunks,)
        CArray(tmp, 'abounds2', atom, shape, "Temp start bounds 2",
               filters, chunkshape=(cs,))
        CArray(tmp, 'zbounds2', atom, shape, "Temp end bounds 2",
               filters, chunkshape=(cs,))
        CArray(tmp, 'mbounds2', atom, shape, "Median bounds 2",
               filters, chunkshape=(cs,))
        # temporary ranges
        CArray(tmp, 'ranges2', atom, (self.nslices, 2),
               "Temporary range values 2", filters, chunkshape=(cs, 2))
        CArray(tmp, 'mranges2', atom, (self.nslices,),
               "Median ranges 2", filters, chunkshape=(cs,))

    def cleanup_temp(self):
        """Copy the data and delete the temporaries for sorting purposes."""

        if self.verbose:
            print("Copying temporary data...")
        # tmp -> index
        reduction = self.reduction
        cs = self.chunksize // reduction
        ncs = self.nchunkslice
        tmp = self.tmp
        for i in xrange(self.nslices):
            # Copy sorted & indices slices
            sorted = tmp.sorted[i][::reduction].copy()
            self.sorted.append(sorted.reshape(1, sorted.size))
            # Compute ranges
            self.ranges.append([[sorted[0], sorted[-1]]])
            # Compute chunk bounds
            self.bounds.append([sorted[cs::cs]])
            # Compute start, stop & median bounds and ranges
            self.abounds.append(sorted[0::cs])
            self.zbounds.append(sorted[cs - 1::cs])
            smedian = sorted[cs // 2::cs]
            self.mbounds.append(smedian)
            self.mranges.append([smedian[ncs // 2]])
            del sorted, smedian   # delete references
            # Now that sorted is gone, we can copy the indices
            indices = tmp.indices[i]
            self.indices.append(indices.reshape(1, indices.size))

        # Now it is the last row turn (if needed)
        if self.nelementsSLR > 0:
            # First, the sorted values
            sortedLR = self.sortedLR
            indicesLR = self.indicesLR
            nelementsLR = self.nelementsILR
            sortedlr = tmp.sortedLR[:nelementsLR][::reduction].copy()
            nelementsSLR = len(sortedlr)
            sortedLR[:nelementsSLR] = sortedlr
            # Now, the bounds
            self.bebounds = numpy.concatenate((sortedlr[::cs], [sortedlr[-1]]))
            offset2 = len(self.bebounds)
            sortedLR[nelementsSLR:nelementsSLR + offset2] = self.bebounds
            # Finally, the indices
            indicesLR[:] = tmp.indicesLR[:]
            # Update the number of (reduced) sorted elements
            self.nelementsSLR = nelementsSLR
        # The number of elements will be saved as an attribute
        self.sortedLR.attrs.nelements = self.nelementsSLR
        self.indicesLR.attrs.nelements = self.nelementsILR

        if self.verbose:
            print("Deleting temporaries...")
        self.tmp = None
        self.tmpfile.close()
        os.remove(self.tmpfilename)
        self.tmpfilename = None

        # The optimization process has finished, and the index is ok now
        self.dirty = False
        # ...but the memory data cache is dirty now
        self.dirtycache = True

    def get_neworder(self, neworder, src_disk, tmp_disk,
                     lastrow, nslices, offset, dtype):
        """Get sorted & indices values in new order."""

        cs = self.chunksize
        ncs = ncs2 = self.nchunkslice
        self_nslices = self.nslices
        tmp = numpy.empty(shape=self.slicesize, dtype=dtype)
        for i in xrange(nslices):
            ns = offset + i
            if ns == self_nslices:
                # The number of complete chunks in the last row
                ncs2 = self.nelementsILR // cs
            # Get slices in new order
            for j in xrange(ncs2):
                idx = neworder[i * ncs + j]
                ins = idx // ncs
                inc = (idx - ins * ncs) * cs
                ins += offset
                nc = j * cs
                if ins == self_nslices:
                    tmp[nc:nc + cs] = lastrow[inc:inc + cs]
                else:
                    tmp[nc:nc + cs] = src_disk[ins, inc:inc + cs]
            if ns == self_nslices:
                # The number of complete chunks in the last row
                lastrow[:ncs2 * cs] = tmp[:ncs2 * cs]
                # The elements in the last chunk of the last row will
                # participate in the global reordering later on, during
                # the phase of sorting of *two* slices at a time
                # (including the last row slice, see
                # self.reorder_slices()).  The caches for last row will
                # be updated in self.reorder_slices() too.
                # F. Altet 2008-08-25
            else:
                tmp_disk[ns] = tmp

    def swap_chunks(self, mode="median"):
        """Swap & reorder the different chunks in a block."""

        boundsnames = {
            'start': 'abounds', 'stop': 'zbounds', 'median': 'mbounds'}
        tmp = self.tmp
        sorted = tmp.sorted
        indices = tmp.indices
        tmp_sorted = tmp.sorted2
        tmp_indices = tmp.indices2
        sortedLR = tmp.sortedLR
        indicesLR = tmp.indicesLR
        cs = self.chunksize
        ncs = self.nchunkslice
        nsb = self.nslicesblock
        ncb = ncs * nsb
        ncb2 = ncb
        boundsobj = tmp._f_get_child(boundsnames[mode])
        can_cross_bbounds = (self.indsize == 8 and self.nelementsILR > 0)
        for nblock in xrange(self.nblocks):
            # Protection for last block having less chunks than ncb
            remainingchunks = self.nchunks - nblock * ncb
            if remainingchunks < ncb:
                ncb2 = remainingchunks
            if ncb2 <= 1:
                # if only zero or one chunks remains we are done
                break
            nslices = ncb2 // ncs
            bounds = boundsobj[nblock * ncb:nblock * ncb + ncb2]
            # Do this only if lastrow elements can cross block boundaries
            if (nblock == self.nblocks - 1 and  # last block
                    can_cross_bbounds):
                nslices += 1
                ul = self.nelementsILR // cs
                bounds = numpy.concatenate((bounds, self.bebounds[:ul]))
            sbounds_idx = bounds.argsort(kind=defsort)
            offset = nblock * nsb
            # Swap sorted and indices following the new order
            self.get_neworder(sbounds_idx, sorted, tmp_sorted, sortedLR,
                              nslices, offset, self.dtype)
            self.get_neworder(sbounds_idx, indices, tmp_indices, indicesLR,
                              nslices, offset, 'u%d' % self.indsize)
        # Reorder completely the index at slice level
        self.reorder_slices(tmp=True)

    def read_slice(self, where, nslice, buffer, start=0):
        """Read a slice from the `where` dataset and put it in `buffer`."""

        # Create the buffers for specifying the coordinates
        self.startl = numpy.array([nslice, start], numpy.uint64)
        self.stopl = numpy.array([nslice + 1, start + buffer.size],
                                 numpy.uint64)
        self.stepl = numpy.ones(shape=2, dtype=numpy.uint64)
        where._g_read_slice(self.startl, self.stopl, self.stepl, buffer)

    def write_slice(self, where, nslice, buffer, start=0):
        """Write a `slice` to the `where` dataset with the `buffer` data."""

        self.startl = numpy.array([nslice, start], numpy.uint64)
        self.stopl = numpy.array([nslice + 1, start + buffer.size],
                                 numpy.uint64)
        self.stepl = numpy.ones(shape=2, dtype=numpy.uint64)
        countl = self.stopl - self.startl   # (1, self.slicesize)
        where._g_write_slice(self.startl, self.stepl, countl, buffer)

    # Read version for LastRow
    def read_slice_lr(self, where, buffer, start=0):
        """Read a slice from the `where` dataset and put it in `buffer`."""

        startl = numpy.array([start], dtype=numpy.uint64)
        stopl = numpy.array([start + buffer.size], dtype=numpy.uint64)
        stepl = numpy.array([1], dtype=numpy.uint64)
        where._g_read_slice(startl, stopl, stepl, buffer)

    # Write version for LastRow
    def write_sliceLR(self, where, buffer, start=0):
        """Write a slice from the `where` dataset with the `buffer` data."""

        startl = numpy.array([start], dtype=numpy.uint64)
        countl = numpy.array([start + buffer.size], dtype=numpy.uint64)
        stepl = numpy.array([1], dtype=numpy.uint64)
        where._g_write_slice(startl, stepl, countl, buffer)

    read_sliceLR = previous_api(read_slice_lr)

    def reorder_slice(self, nslice, sorted, indices, ssorted, sindices,
                      tmp_sorted, tmp_indices):
        """Copy & reorder the slice in source to final destination."""

        ss = self.slicesize
        # Load the second part in buffers
        self.read_slice(tmp_sorted, nslice, ssorted[ss:])
        self.read_slice(tmp_indices, nslice, sindices[ss:])
        indexesextension.keysort(ssorted, sindices)
        # Write the first part of the buffers to the regular leaves
        self.write_slice(sorted, nslice - 1, ssorted[:ss])
        self.write_slice(indices, nslice - 1, sindices[:ss])
        # Update caches
        self.update_caches(nslice - 1, ssorted[:ss])
        # Shift the slice in the end to the beginning
        ssorted[:ss] = ssorted[ss:]
        sindices[:ss] = sindices[ss:]

    def update_caches(self, nslice, ssorted):
        """Update the caches for faster lookups."""

        cs = self.chunksize
        ncs = self.nchunkslice
        tmp = self.tmp
        # update first & second cache bounds (ranges & bounds)
        tmp.ranges[nslice] = ssorted[[0, -1]]
        tmp.bounds[nslice] = ssorted[cs::cs]
        # update start & stop bounds
        tmp.abounds[nslice * ncs:(nslice + 1) * ncs] = ssorted[0::cs]
        tmp.zbounds[nslice * ncs:(nslice + 1) * ncs] = ssorted[cs - 1::cs]
        # update median bounds
        smedian = ssorted[cs // 2::cs]
        tmp.mbounds[nslice * ncs:(nslice + 1) * ncs] = smedian
        tmp.mranges[nslice] = smedian[ncs // 2]

    def reorder_slices(self, tmp):
        """Reorder completely the index at slice level.

        This method has to maintain the locality of elements in the
        ambit of ``blocks``, i.e. an element of a ``block`` cannot be
        sent to another ``block`` during this reordering.  This is
        *critical* for ``light`` indexes to be able to use this.

        This version of reorder_slices is optimized in that *two*
        complete slices are taken at a time (including the last row
        slice) so as to sort them.  Then, each new slice that is read is
        put at the end of this two-slice buffer, while the previous one
        is moved to the beginning of the buffer.  This is in order to
        better reduce the entropy of the regular part (i.e. all except
        the last row) of the index.

        A secondary effect of this is that it takes at least *twice* of
        memory than a previous version of reorder_slices() that only
        reorders on a slice-by-slice basis.  However, as this is more
        efficient than the old version, one can configure the slicesize
        to be smaller, so the memory consumption is barely similar.

        """

        tmp = self.tmp
        sorted = tmp.sorted
        indices = tmp.indices
        if tmp:
            tmp_sorted = tmp.sorted2
            tmp_indices = tmp.indices2
        else:
            tmp_sorted = tmp.sorted
            tmp_indices = tmp.indices
        cs = self.chunksize
        ss = self.slicesize
        nsb = self.blocksize // self.slicesize
        nslices = self.nslices
        nblocks = self.nblocks
        nelementsLR = self.nelementsILR
        # Create the buffer for reordering 2 slices at a time
        ssorted = numpy.empty(shape=ss * 2, dtype=self.dtype)
        sindices = numpy.empty(shape=ss * 2,
                               dtype=numpy.dtype('u%d' % self.indsize))

        if self.indsize == 8:
            # Bootstrap the process for reordering
            # Read the first slice in buffers
            self.read_slice(tmp_sorted, 0, ssorted[:ss])
            self.read_slice(tmp_indices, 0, sindices[:ss])

            nslice = 0   # Just in case the loop behind executes nothing
            # Loop over the remainding slices in block
            for nslice in xrange(1, sorted.nrows):
                self.reorder_slice(nslice, sorted, indices,
                                   ssorted, sindices,
                                   tmp_sorted, tmp_indices)

            # End the process (enrolling the lastrow if necessary)
            if nelementsLR > 0:
                sortedLR = self.tmp.sortedLR
                indicesLR = self.tmp.indicesLR
                # Shrink the ssorted and sindices arrays to the minimum
                ssorted2 = ssorted[:ss + nelementsLR]
                sortedlr = ssorted2[ss:]
                sindices2 = sindices[:ss + nelementsLR]
                indiceslr = sindices2[ss:]
                # Read the last row info in the second part of the buffer
                self.read_slice_lr(sortedLR, sortedlr)
                self.read_slice_lr(indicesLR, indiceslr)
                indexesextension.keysort(ssorted2, sindices2)
                # Write the second part of the buffers to the lastrow indices
                self.write_sliceLR(sortedLR, sortedlr)
                self.write_sliceLR(indicesLR, indiceslr)
                # Update the caches for last row
                bebounds = numpy.concatenate((sortedlr[::cs], [sortedlr[-1]]))
                sortedLR[nelementsLR:nelementsLR + len(bebounds)] = bebounds
                self.bebounds = bebounds
            # Write the first part of the buffers to the regular leaves
            self.write_slice(sorted, nslice, ssorted[:ss])
            self.write_slice(indices, nslice, sindices[:ss])
            # Update caches for this slice
            self.update_caches(nslice, ssorted[:ss])
        else:
            # Iterate over each block.  No data should cross block
            # boundaries to avoid adressing problems with short indices.
            for nb in xrange(nblocks):
                # Bootstrap the process for reordering
                # Read the first slice in buffers
                nrow = nb * nsb
                self.read_slice(tmp_sorted, nrow, ssorted[:ss])
                self.read_slice(tmp_indices, nrow, sindices[:ss])

                # Loop over the remainding slices in block
                lrb = nrow + nsb
                if lrb > nslices:
                    lrb = nslices
                nslice = nrow   # Just in case the loop behind executes nothing
                for nslice in xrange(nrow + 1, lrb):
                    self.reorder_slice(nslice, sorted, indices,
                                       ssorted, sindices,
                                       tmp_sorted, tmp_indices)

                # Write the first part of the buffers to the regular leaves
                self.write_slice(sorted, nslice, ssorted[:ss])
                self.write_slice(indices, nslice, sindices[:ss])
                # Update caches for this slice
                self.update_caches(nslice, ssorted[:ss])

    def swap_slices(self, mode="median"):
        """Swap slices in a superblock."""

        tmp = self.tmp
        sorted = tmp.sorted
        indices = tmp.indices
        tmp_sorted = tmp.sorted2
        tmp_indices = tmp.indices2
        ncs = self.nchunkslice
        nss = self.superblocksize // self.slicesize
        nss2 = nss
        for sblock in xrange(self.nsuperblocks):
            # Protection for last superblock having less slices than nss
            remainingslices = self.nslices - sblock * nss
            if remainingslices < nss:
                nss2 = remainingslices
            if nss2 <= 1:
                break
            if mode == "start":
                ranges = tmp.ranges[sblock * nss:sblock * nss + nss2, 0]
            elif mode == "stop":
                ranges = tmp.ranges[sblock * nss:sblock * nss + nss2, 1]
            elif mode == "median":
                ranges = tmp.mranges[sblock * nss:sblock * nss + nss2]
            sranges_idx = ranges.argsort(kind=defsort)
            # Don't swap the superblock at all if one doesn't need to
            ndiff = (sranges_idx != numpy.arange(nss2)).sum() / 2
            if ndiff * 50 < nss2:
                # The number of slices to rearrange is less than 2.5%,
                # so skip the reordering of this superblock
                # (too expensive for such a little improvement)
                if self.verbose:
                    print("skipping reordering of superblock ->", sblock)
                continue
            ns = sblock * nss2
            # Swap sorted and indices slices following the new order
            for i in xrange(nss2):
                idx = sranges_idx[i]
                # Swap sorted & indices slices
                oi = ns + i
                oidx = ns + idx
                tmp_sorted[oi] = sorted[oidx]
                tmp_indices[oi] = indices[oidx]
                # Swap start, stop & median ranges
                tmp.ranges2[oi] = tmp.ranges[oidx]
                tmp.mranges2[oi] = tmp.mranges[oidx]
                # Swap chunk bounds
                tmp.bounds2[oi] = tmp.bounds[oidx]
                # Swap start, stop & median bounds
                j = oi * ncs
                jn = (oi + 1) * ncs
                xj = oidx * ncs
                xjn = (oidx + 1) * ncs
                tmp.abounds2[j:jn] = tmp.abounds[xj:xjn]
                tmp.zbounds2[j:jn] = tmp.zbounds[xj:xjn]
                tmp.mbounds2[j:jn] = tmp.mbounds[xj:xjn]
            # tmp -> originals
            for i in xrange(nss2):
                # Copy sorted & indices slices
                oi = ns + i
                sorted[oi] = tmp_sorted[oi]
                indices[oi] = tmp_indices[oi]
                # Copy start, stop & median ranges
                tmp.ranges[oi] = tmp.ranges2[oi]
                tmp.mranges[oi] = tmp.mranges2[oi]
                # Copy chunk bounds
                tmp.bounds[oi] = tmp.bounds2[oi]
                # Copy start, stop & median bounds
                j = oi * ncs
                jn = (oi + 1) * ncs
                tmp.abounds[j:jn] = tmp.abounds2[j:jn]
                tmp.zbounds[j:jn] = tmp.zbounds2[j:jn]
                tmp.mbounds[j:jn] = tmp.mbounds2[j:jn]

    def search_item_lt(self, where, item, nslice, limits, start=0):
        """Search a single item in a specific sorted slice."""

        # This method will only works under the assumtion that item
        # *is to be found* in the nslice.
        assert limits[0] < item <= limits[1]
        cs = self.chunksize
        ss = self.slicesize
        nelementsLR = self.nelementsILR
        bstart = start // cs

        # Find the chunk
        if nslice < self.nslices:
            nchunk = bisect_left(where.bounds[nslice], item, bstart)
        else:
            # We need to subtract 1 chunk here because bebounds
            # has a leading value
            nchunk = bisect_left(self.bebounds, item, bstart) - 1
        assert nchunk >= 0

        # Find the element in chunk
        pos = nchunk * cs
        if nslice < self.nslices:
            pos += bisect_left(where.sorted[nslice, pos:pos + cs], item)
            assert pos <= ss
        else:
            end = pos + cs
            if end > nelementsLR:
                end = nelementsLR
            pos += bisect_left(self.sortedLR[pos:end], item)
            assert pos <= nelementsLR
        assert pos > 0
        return pos

    def compute_overlaps_finegrain(self, where, message, verbose):
        """Compute some statistics about overlaping of slices in index.

        It returns the following info:

        noverlaps : int
            The total number of elements that overlaps in index.
        multiplicity : array of int
            The number of times that a concrete slice overlaps with any other.
        toverlap : float
            An ovelap index: the sum of the values in segment slices that
            overlaps divided by the entire range of values.  This index is only
            computed for numerical types.

        """

        ss = self.slicesize
        ranges = where.ranges[:]
        sorted = where.sorted
        sortedLR = where.sortedLR
        nslices = self.nslices
        nelementsLR = self.nelementsILR
        if nelementsLR > 0:
            # Add the ranges corresponding to the last row
            rangeslr = numpy.array([self.bebounds[0], self.bebounds[-1]])
            ranges = numpy.concatenate((ranges, [rangeslr]))
            nslices += 1
        soverlap = 0.
        toverlap = -1.
        multiplicity = numpy.zeros(shape=nslices, dtype="int_")
        overlaps = multiplicity.copy()
        starts = multiplicity.copy()
        for i in xrange(nslices):
            prev_end = ranges[i, 1]
            for j in xrange(i + 1, nslices):
                stj = starts[j]
                assert stj <= ss
                if stj == ss:
                    # This slice has already been counted
                    continue
                if j < self.nslices:
                    next_beg = sorted[j, stj]
                else:
                    next_beg = sortedLR[stj]
                next_end = ranges[j, 1]
                if prev_end > next_end:
                    # Complete overlapping case
                    multiplicity[j - i] += 1
                    if j < self.nslices:
                        overlaps[i] += ss - stj
                        starts[j] = ss   # a sentinel
                    else:
                        overlaps[i] += nelementsLR - stj
                        starts[j] = nelementsLR   # a sentinel
                elif prev_end > next_beg:
                    multiplicity[j - i] += 1
                    idx = self.search_item_lt(
                        where, prev_end, j, ranges[j], stj)
                    nelem = idx - stj
                    overlaps[i] += nelem
                    starts[j] = idx
                    if self.type != "string":
                        # Convert ranges into floats in order to allow
                        # doing operations with them without overflows
                        soverlap += float(ranges[i, 1]) - float(ranges[j, 0])

        # Return the overlap as the ratio between overlaps and entire range
        if self.type != "string":
            erange = float(ranges[-1, 1]) - float(ranges[0, 0])
            # Check that there is an effective range of values
            # Beware, erange can be negative in situations where
            # the values are suffering overflow. This can happen
            # specially on big signed integer values (on overflows,
            # the end value will become negative!).
            # Also, there is no way to compute overlap ratios for
            # non-numerical types. So, be careful and always check
            # that toverlap has a positive value (it must have been
            # initialized to -1. before) before using it.
            # F. Alted 2007-01-19
            if erange > 0:
                toverlap = soverlap / erange
        if verbose and message != "init":
            print("toverlap (%s):" % message, toverlap)
            print("multiplicity:\n", multiplicity, multiplicity.sum())
            print("overlaps:\n", overlaps, overlaps.sum())
        noverlaps = overlaps.sum()
        # For full indexes, set the 'is_csi' flag
        if self.indsize == 8 and self._v_file._iswritable():
            self._v_attrs.is_csi = (noverlaps == 0)
        # Save the number of overlaps for future references
        self.noverlaps = noverlaps
        return (noverlaps, multiplicity, toverlap)

    def compute_overlaps(self, where, message, verbose):
        """Compute some statistics about overlaping of slices in index.

        It returns the following info:

        noverlaps : int
            The total number of slices that overlaps in index.
        multiplicity : array of int
            The number of times that a concrete slice overlaps with any other.
        toverlap : float
            An ovelap index: the sum of the values in segment slices that
            overlaps divided by the entire range of values.  This index is only
            computed for numerical types.

        """

        ranges = where.ranges[:]
        nslices = self.nslices
        if self.nelementsILR > 0:
            # Add the ranges corresponding to the last row
            rangeslr = numpy.array([self.bebounds[0], self.bebounds[-1]])
            ranges = numpy.concatenate((ranges, [rangeslr]))
            nslices += 1
        noverlaps = 0
        soverlap = 0.
        toverlap = -1.
        multiplicity = numpy.zeros(shape=nslices, dtype="int_")
        for i in xrange(nslices):
            for j in xrange(i + 1, nslices):
                if ranges[i, 1] > ranges[j, 0]:
                    noverlaps += 1
                    multiplicity[j - i] += 1
                    if self.type != "string":
                        # Convert ranges into floats in order to allow
                        # doing operations with them without overflows
                        soverlap += float(ranges[i, 1]) - float(ranges[j, 0])

        # Return the overlap as the ratio between overlaps and entire range
        if self.type != "string":
            erange = float(ranges[-1, 1]) - float(ranges[0, 0])
            # Check that there is an effective range of values
            # Beware, erange can be negative in situations where
            # the values are suffering overflow. This can happen
            # specially on big signed integer values (on overflows,
            # the end value will become negative!).
            # Also, there is no way to compute overlap ratios for
            # non-numerical types. So, be careful and always check
            # that toverlap has a positive value (it must have been
            # initialized to -1. before) before using it.
            # F. Altet 2007-01-19
            if erange > 0:
                toverlap = soverlap / erange
        if verbose:
            print("overlaps (%s):" % message, noverlaps, toverlap)
            print(multiplicity)
        # For full indexes, set the 'is_csi' flag
        if self.indsize == 8 and self._v_file._iswritable():
            self._v_attrs.is_csi = (noverlaps == 0)
        # Save the number of overlaps for future references
        self.noverlaps = noverlaps
        return (noverlaps, multiplicity, toverlap)

    def read_sorted_indices(self, what, start, stop, step):
        """Return the sorted or indices values in the specified range."""
        (start, stop, step) = self._process_range(start, stop, step)
        if start >= stop:
            return numpy.empty(0, self.dtype)
        # Correction for negative values of step (reverse indices)
        if step < 0:
            tmp = start
            start = self.nelements - stop
            stop = self.nelements - tmp
        if what == "sorted":
            values = self.sorted
            valuesLR = self.sortedLR
            buffer_ = numpy.empty(stop - start, dtype=self.dtype)
        else:
            values = self.indices
            valuesLR = self.indicesLR
            buffer_ = numpy.empty(stop - start, dtype="u%d" % self.indsize)
        ss = self.slicesize
        nrow_start = start // ss
        istart = start % ss
        nrow_stop = stop // ss
        tlen = stop - start
        bstart = 0
        ilen = 0
        for nrow in xrange(nrow_start, nrow_stop + 1):
            blen = ss - istart
            if ilen + blen > tlen:
                blen = tlen - ilen
            if blen <= 0:
                break
            if nrow < self.nslices:
                self.read_slice(
                    values, nrow, buffer_[bstart:bstart + blen], istart)
            else:
                self.read_slice_lr(
                    valuesLR, buffer_[bstart:bstart + blen], istart)
            istart = 0
            bstart += blen
            ilen += blen
        return buffer_[::step]

    def read_sorted(self, start=None, stop=None, step=None):
        """Return the sorted values of index in the specified range.

        The meaning of the start, stop and step arguments is the same as in
        :meth:`Table.read_sorted`.

        """

        return self.read_sorted_indices('sorted', start, stop, step)

    readSorted = previous_api(read_sorted)

    def read_indices(self, start=None, stop=None, step=None):
        """Return the indices values of index in the specified range.

        The meaning of the start, stop and step arguments is the same as in
        :meth:`Table.read_sorted`.

        """

        return self.read_sorted_indices('indices', start, stop, step)

    readIndices = previous_api(read_indices)

    def _process_range(self, start, stop, step):
        """Get a range specifc for the index usage."""

        if start is not None and stop is None:
            # Special case for the behaviour of PyTables iterators
            stop = idx2long(start + 1)
        if start is None:
            start = 0L
        else:
            start = idx2long(start)
        if stop is None:
            stop = idx2long(self.nelements)
        else:
            stop = idx2long(stop)
        if step is None:
            step = 1L
        else:
            step = idx2long(step)
        return (start, stop, step)

    _processRange = previous_api(_process_range)

    def __getitem__(self, key):
        """Return the indices values of index in the specified range.

        If key argument is an integer, the corresponding index is returned.  If
        key is a slice, the range of indices determined by it is returned.  A
        negative value of step in slice is supported, meaning that the results
        will be returned in reverse order.

        This method is equivalent to :meth:`Index.read_indices`.

        """

        if is_idx(key):
            if key < 0:
                # To support negative values
                key += self.nelements
            return self.read_indices(key, key + 1, 1)[0]
        elif isinstance(key, slice):
            return self.read_indices(key.start, key.stop, key.step)

    def __len__(self):
        return self.nelements

    def restorecache(self):
        "Clean the limits cache and resize starts and lengths arrays"

        params = self._v_file.params
        # The sorted IndexArray is absolutely required to be in memory
        # at the same time than the Index instance, so create a strong
        # reference to it.  We are not introducing leaks because the
        # strong reference will disappear when this Index instance is
        # to be closed.
        self._sorted = self.sorted
        self._sorted.boundscache = ObjectCache(params['BOUNDS_MAX_SLOTS'],
                                               params['BOUNDS_MAX_SIZE'],
                                               'non-opt types bounds')
        self.sorted.boundscache = ObjectCache(params['BOUNDS_MAX_SLOTS'],
                                              params['BOUNDS_MAX_SIZE'],
                                              'non-opt types bounds')
        """A cache for the bounds (2nd hash) data. Only used for
        non-optimized types searches."""
        self.limboundscache = ObjectCache(params['LIMBOUNDS_MAX_SLOTS'],
                                          params['LIMBOUNDS_MAX_SIZE'],
                                          'bounding limits')
        """A cache for bounding limits."""
        self.sortedLRcache = ObjectCache(params['SORTEDLR_MAX_SLOTS'],
                                         params['SORTEDLR_MAX_SIZE'],
                                         'last row chunks')
        """A cache for the last row chunks. Only used for searches in
        the last row, and mainly useful for small indexes."""
        self.starts = numpy.empty(shape=self.nrows, dtype=numpy.int32)
        self.lengths = numpy.empty(shape=self.nrows, dtype=numpy.int32)
        self.sorted._init_sorted_slice(self)
        self.dirtycache = False

    def search(self, item):
        """Do a binary search in this index for an item."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering search", tref)

        if self.dirtycache:
            self.restorecache()

        # An empty item or if left limit is larger than the right one
        # means that the number of records is always going to be empty,
        # so we avoid further computation (including looking up the
        # limits cache).
        if not item or item[0] > item[1]:
            self.starts[:] = 0
            self.lengths[:] = 0
            return 0

        tlen = 0
        # Check whether the item tuple is in the limits cache or not
        nslot = self.limboundscache.getslot(item)
        if nslot >= 0:
            startlengths = self.limboundscache.getitem(nslot)
            # Reset the lengths array (not necessary for starts)
            self.lengths[:] = 0
            # Now, set the interesting rows
            for nrow in xrange(len(startlengths)):
                nrow2, start, length = startlengths[nrow]
                self.starts[nrow2] = start
                self.lengths[nrow2] = length
                tlen = tlen + length
            return tlen
        # The item is not in cache. Do the real lookup.
        sorted = self.sorted
        if self.nslices > 0:
            if self.type in self.opt_search_types:
                # The next are optimizations. However, they hide the
                # CPU functions consumptions from python profiles.
                # You may want to de-activate them during profiling.
                if self.type == "int32":
                    tlen = sorted._search_bin_na_i(*item)
                elif self.type == "int64":
                    tlen = sorted._search_bin_na_ll(*item)
                elif self.type == "float16":
                    tlen = sorted._search_bin_na_e(*item)
                elif self.type == "float32":
                    tlen = sorted._search_bin_na_f(*item)
                elif self.type == "float64":
                    tlen = sorted._search_bin_na_d(*item)
                elif self.type == "float96":
                    tlen = sorted._search_bin_na_g(*item)
                elif self.type == "float128":
                    tlen = sorted._search_bin_na_g(*item)
                elif self.type == "uint32":
                    tlen = sorted._search_bin_na_ui(*item)
                elif self.type == "uint64":
                    tlen = sorted._search_bin_na_ull(*item)
                elif self.type == "int8":
                    tlen = sorted._search_bin_na_b(*item)
                elif self.type == "int16":
                    tlen = sorted._search_bin_na_s(*item)
                elif self.type == "uint8":
                    tlen = sorted._search_bin_na_ub(*item)
                elif self.type == "uint16":
                    tlen = sorted._search_bin_na_us(*item)
                else:
                    assert False, "This can't happen!"
            else:
                tlen = self.search_scalar(item, sorted)
        # Get possible remaining values in last row
        if self.nelementsSLR > 0:
            # Look for more indexes in the last row
            (start, stop) = self.search_last_row(item)
            self.starts[-1] = start
            self.lengths[-1] = stop - start
            tlen += stop - start

        if self.limboundscache.couldenablecache():
            # Get a startlengths tuple and save it in cache.
            # This is quite slow, but it is a good way to compress
            # the bounds info. Moreover, the .couldenablecache()
            # is doing a good work so as to avoid computing this
            # when it is not necessary to do it.
            startlengths = []
            for nrow, length in enumerate(self.lengths):
                if length > 0:
                    startlengths.append((nrow, self.starts[nrow], length))
            # Compute the size of the recarray (aproximately)
            # The +1 at the end is important to avoid 0 lengths
            # (remember, the object headers take some space)
            size = len(startlengths) * 8 * 2 + 1
            # Put this startlengths list in cache
            self.limboundscache.setitem(item, startlengths, size)

        if profile:
            show_stats("Exiting search", tref)
        return tlen

    # This is an scalar version of search. It works with strings as well.
    def search_scalar(self, item, sorted):
        """Do a binary search in this index for an item."""

        tlen = 0
        # Do the lookup for values fullfilling the conditions
        for i in xrange(self.nslices):
            (start, stop) = sorted._search_bin(i, item)
            self.starts[i] = start
            self.lengths[i] = stop - start
            tlen += stop - start
        return tlen

    def search_last_row(self, item):
        # Variable initialization
        item1, item2 = item
        bebounds = self.bebounds
        b0, b1 = bebounds[0], bebounds[-1]
        bounds = bebounds[1:-1]
        itemsize = self.dtype.itemsize
        sortedLRcache = self.sortedLRcache
        hi = self.nelementsSLR               # maximum number of elements
        rchunksize = self.chunksize // self.reduction

        nchunk = -1
        # Lookup for item1
        if item1 > b0:
            if item1 <= b1:
                # Search the appropriate chunk in bounds cache
                nchunk = bisect_left(bounds, item1)
                # Lookup for this chunk in cache
                nslot = sortedLRcache.getslot(nchunk)
                if nslot >= 0:
                    chunk = sortedLRcache.getitem(nslot)
                else:
                    begin = rchunksize * nchunk
                    end = rchunksize * (nchunk + 1)
                    if end > hi:
                        end = hi
                    # Read the chunk from disk
                    chunk = self.sortedLR._read_sorted_slice(
                        self.sorted, begin, end)
                    # Put it in cache.  It's important to *copy*
                    # the buffer, as it is reused in future reads!
                    sortedLRcache.setitem(nchunk, chunk.copy(),
                                          (end - begin) * itemsize)
                start = bisect_left(chunk, item1)
                start += rchunksize * nchunk
            else:
                start = hi
        else:
            start = 0
        # Lookup for item2
        if item2 >= b0:
            if item2 < b1:
                # Search the appropriate chunk in bounds cache
                nchunk2 = bisect_right(bounds, item2)
                if nchunk2 != nchunk:
                    # Lookup for this chunk in cache
                    nslot = sortedLRcache.getslot(nchunk2)
                    if nslot >= 0:
                        chunk = sortedLRcache.getitem(nslot)
                    else:
                        begin = rchunksize * nchunk2
                        end = rchunksize * (nchunk2 + 1)
                        if end > hi:
                            end = hi
                        # Read the chunk from disk
                        chunk = self.sortedLR._read_sorted_slice(
                            self.sorted, begin, end)
                        # Put it in cache.  It's important to *copy*
                        # the buffer, as it is reused in future reads!
                        # See bug #60 in xot.carabos.com
                        sortedLRcache.setitem(nchunk2, chunk.copy(),
                                              (end - begin) * itemsize)
                stop = bisect_right(chunk, item2)
                stop += rchunksize * nchunk2
            else:
                stop = hi
        else:
            stop = 0
        return (start, stop)

    searchLastRow = previous_api(search_last_row)

    def get_chunkmap(self):
        """Compute a map with the interesting chunks in index."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering get_chunkmap", tref)
        ss = self.slicesize
        nsb = self.nslicesblock
        nslices = self.nslices
        lbucket = self.lbucket
        indsize = self.indsize
        bucketsinblock = float(self.blocksize) / lbucket
        nchunks = long(math.ceil(float(self.nelements) / lbucket))
        chunkmap = numpy.zeros(shape=nchunks, dtype="bool")
        reduction = self.reduction
        starts = (self.starts - 1) * reduction + 1
        stops = (self.starts + self.lengths) * reduction
        starts[starts < 0] = 0    # All negative values set to zero
        indices = self.indices
        for nslice in xrange(self.nrows):
            start = starts[nslice]
            stop = stops[nslice]
            if stop > start:
                idx = numpy.empty(shape=stop - start, dtype='u%d' % indsize)
                if nslice < nslices:
                    indices._read_index_slice(nslice, start, stop, idx)
                else:
                    self.indicesLR._read_index_slice(start, stop, idx)
                if indsize == 8:
                    idx //= lbucket
                elif indsize == 2:
                    # The chunkmap size cannot be never larger than 'int_'
                    idx = idx.astype("int_")
                    offset = long((nslice // nsb) * bucketsinblock)
                    idx += offset
                elif indsize == 1:
                    # The chunkmap size cannot be never larger than 'int_'
                    idx = idx.astype("int_")
                    offset = (nslice * ss) // lbucket
                    idx += offset
                chunkmap[idx] = True
        # The case lbucket < nrowsinchunk should only happen in tests
        nrowsinchunk = self.nrowsinchunk
        if lbucket != nrowsinchunk:
            # Map the 'coarse grain' chunkmap into the 'true' chunkmap
            nelements = self.nelements
            tnchunks = long(math.ceil(float(nelements) / nrowsinchunk))
            tchunkmap = numpy.zeros(shape=tnchunks, dtype="bool")
            ratio = float(lbucket) / nrowsinchunk
            idx = chunkmap.nonzero()[0]
            starts = (idx * ratio).astype('int_')
            stops = numpy.ceil((idx + 1) * ratio).astype('int_')
            for i in range(len(idx)):
                tchunkmap[starts[i]:stops[i]] = True
            chunkmap = tchunkmap
        if profile:
            show_stats("Exiting get_chunkmap", tref)
        return chunkmap

    def get_lookup_range(self, ops, limits):
        assert len(ops) in [1, 2]
        assert len(limits) in [1, 2]
        assert len(ops) == len(limits)

        column = self.column
        coldtype = column.dtype.base
        itemsize = coldtype.itemsize

        if len(limits) == 1:
            assert ops[0] in ['lt', 'le', 'eq', 'ge', 'gt']
            limit = limits[0]
            op = ops[0]
            if op == 'lt':
                range_ = (inftype(coldtype, itemsize, sign=-1),
                          nextafter(limit, -1, coldtype, itemsize))
            elif op == 'le':
                range_ = (inftype(coldtype, itemsize, sign=-1),
                          limit)
            elif op == 'gt':
                range_ = (nextafter(limit, +1, coldtype, itemsize),
                          inftype(coldtype, itemsize, sign=+1))
            elif op == 'ge':
                range_ = (limit,
                          inftype(coldtype, itemsize, sign=+1))
            elif op == 'eq':
                range_ = (limit, limit)

        elif len(limits) == 2:
            assert ops[0] in ('gt', 'ge') and ops[1] in ('lt', 'le')

            lower, upper = limits
            if lower > upper:
                # ``a <[=] x <[=] b`` is always false if ``a > b``.
                return ()

            if ops == ('gt', 'lt'):  # lower < col < upper
                range_ = (nextafter(lower, +1, coldtype, itemsize),
                          nextafter(upper, -1, coldtype, itemsize))
            elif ops == ('ge', 'lt'):  # lower <= col < upper
                range_ = (lower, nextafter(upper, -1, coldtype, itemsize))
            elif ops == ('gt', 'le'):  # lower < col <= upper
                range_ = (nextafter(lower, +1, coldtype, itemsize), upper)
            elif ops == ('ge', 'le'):  # lower <= col <= upper
                range_ = (lower, upper)

        return range_

    getLookupRange = previous_api(get_lookup_range)

    def _f_remove(self, recursive=False):
        """Remove this Index object."""

        # Index removal is always recursive,
        # no matter what `recursive` says.
        super(Index, self)._f_remove(True)

    def __str__(self):
        """This provides a more compact representation than __repr__"""

        # The filters
        filters = ""
        if self.filters.complevel:
            if self.filters.shuffle:
                filters += ", shuffle"
            filters += ", %s(%s)" % (self.filters.complib,
                                     self.filters.complevel)
        return "Index(%s, %s%s).is_csi=%s" % \
               (self.optlevel, self.kind, filters, self.is_csi)

    def __repr__(self):
        """This provides more metainfo than standard __repr__"""

        cpathname = self.table._v_pathname + ".cols." + self.column.pathname
        retstr = """%s (Index for column %s)
  optlevel := %s
  kind := %s
  filters := %s
  is_csi := %s
  nelements := %s
  chunksize := %s
  slicesize := %s
  blocksize := %s
  superblocksize := %s
  filters := %s
  dirty := %s
  byteorder := %r""" % (self._v_pathname, cpathname,
                        self.optlevel, self.kind,
                        self.filters, self.is_csi,
                        self.nelements,
                        self.chunksize, self.slicesize,
                        self.blocksize, self.superblocksize,
                        self.filters, self.dirty,
                        self.byteorder)
        retstr += "\n  sorted := %s" % self.sorted
        retstr += "\n  indices := %s" % self.indices
        retstr += "\n  ranges := %s" % self.ranges
        retstr += "\n  bounds := %s" % self.bounds
        retstr += "\n  sortedLR := %s" % self.sortedLR
        retstr += "\n  indicesLR := %s" % self.indicesLR
        return retstr


class IndexesDescG(NotLoggedMixin, Group):
    _c_classid = 'DINDEX'

    _c_classId = previous_api_property('_c_classid')

    def _g_width_warning(self):
        warnings.warn(
            "the number of indexed columns on a single description group "
            "is exceeding the recommended maximum (%d); "
            "be ready to see PyTables asking for *lots* of memory "
            "and possibly slow I/O" % self._v_max_group_width,
            PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)


class IndexesTableG(NotLoggedMixin, Group):
    _c_classid = 'TINDEX'

    _c_classId = previous_api_property('_c_classid')

    def _getauto(self):
        if 'AUTO_INDEX' not in self._v_attrs:
            return default_auto_index
        return self._v_attrs.AUTO_INDEX

    def _setauto(self, auto):
        self._v_attrs.AUTO_INDEX = bool(auto)

    def _delauto(self):
        del self._v_attrs.AUTO_INDEX
    auto = property(_getauto, _setauto, _delauto)

    def _g_width_warning(self):
        warnings.warn(
            "the number of indexed columns on a single table "
            "is exceeding the recommended maximum (%d); "
            "be ready to see PyTables asking for *lots* of memory "
            "and possibly slow I/O" % self._v_max_group_width,
            PerformanceWarning)

    _g_widthWarning = previous_api(_g_width_warning)

    def _g_check_name(self, name):
        if not name.startswith('_i_'):
            raise ValueError(
                "names of index groups must start with ``_i_``: %s" % name)

    _g_checkName = previous_api(_g_check_name)

    def _gettable(self):
        names = self._v_pathname.split("/")
        tablename = names.pop()[3:]   # "_i_" is at the beginning
        parentpathname = "/".join(names)
        tablepathname = join_path(parentpathname, tablename)
        table = self._v_file._get_node(tablepathname)
        return table

    table = property(
        _gettable, None, None,
        "Accessor for the `Table` object of this `IndexesTableG` container.")


class OldIndex(NotLoggedMixin, Group):
    """This is meant to hide indexes of PyTables 1.x files."""

    _c_classid = 'CINDEX'

    _c_classId = previous_api_property('_c_classid')


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = indexes
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: June 02, 2004
# Author:  Francesc Alted - faltet@pytables.com
#
# $Source: /cvsroot/pytables/pytables/tables/indexes.py $
# $Id$
#
########################################################################

"""Here is defined the IndexArray class."""

from bisect import bisect_left, bisect_right

from tables.node import NotLoggedMixin
from tables.carray import CArray
from tables.earray import EArray
from tables import indexesextension

from tables._past import previous_api, previous_api_property

# Declarations for inheriting


class CacheArray(NotLoggedMixin, EArray, indexesextension.CacheArray):
    """Container for keeping index caches of 1st and 2nd level."""

    # Class identifier.
    _c_classid = 'CACHEARRAY'

    _c_classId = previous_api_property('_c_classid')


class LastRowArray(NotLoggedMixin, CArray, indexesextension.LastRowArray):
    """Container for keeping sorted and indices values of last row of an
    index."""

    # Class identifier.
    _c_classid = 'LASTROWARRAY'

    _c_classId = previous_api_property('_c_classid')


class IndexArray(NotLoggedMixin, EArray, indexesextension.IndexArray):
    """Represent the index (sorted or reverse index) dataset in HDF5 file.

    All NumPy typecodes are supported except for complex datatypes.

    Parameters
    ----------
    parentnode
        The Index class from which this object will hang off.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.
    atom
        An Atom object representing the shape and type of the atomic objects to
        be saved. Only scalar atoms are supported.
    title
        Sets a TITLE attribute on the array entity.
    filters : Filters
        An instance of the Filters class that provides information about the
        desired I/O filters to be applied during the life of this object.
    byteorder
        The byteroder of the data on-disk.

    """

    # Class identifier.
    _c_classid = 'INDEXARRAY'

    _c_classId = previous_api_property('_c_classid')

    # Properties
    # ~~~~~~~~~~
    chunksize = property(
        lambda self: self.chunkshape[1], None, None,
        """The chunksize for this object.""")

    slicesize = property(
        lambda self: self.shape[1], None, None,
        """The slicesize for this object.""")

    # Other methods
    # ~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 atom=None, title="",
                 filters=None, byteorder=None):
        """Create an IndexArray instance."""

        self._v_pathname = parentnode._g_join(name)
        if atom is not None:
            # The shape and chunkshape needs to be fixed here
            if name == "sorted":
                reduction = parentnode.reduction
                shape = (0, parentnode.slicesize // reduction)
                chunkshape = (1, parentnode.chunksize // reduction)
            else:
                shape = (0, parentnode.slicesize)
                chunkshape = (1, parentnode.chunksize)
        else:
            # The shape and chunkshape will be read from disk later on
            shape = None
            chunkshape = None

        super(IndexArray, self).__init__(
            parentnode, name, atom, shape, title, filters,
            chunkshape=chunkshape, byteorder=byteorder)

    # This version of searchBin uses both ranges (1st level) and
    # bounds (2nd level) caches. It uses a cache for boundary rows,
    # but not for 'sorted' rows (this is only supported for the
    # 'optimized' types).
    def _search_bin(self, nrow, item):
        item1, item2 = item
        result1 = -1
        result2 = -1
        hi = self.shape[1]
        ranges = self._v_parent.rvcache
        boundscache = self.boundscache
        # First, look at the beginning of the slice
        begin = ranges[nrow, 0]
        # Look for items at the beginning of sorted slices
        if item1 <= begin:
            result1 = 0
        if item2 < begin:
            result2 = 0
        if result1 >= 0 and result2 >= 0:
            return (result1, result2)
        # Then, look for items at the end of the sorted slice
        end = ranges[nrow, 1]
        if result1 < 0:
            if item1 > end:
                result1 = hi
        if result2 < 0:
            if item2 >= end:
                result2 = hi
        if result1 >= 0 and result2 >= 0:
            return (result1, result2)
        # Finally, do a lookup for item1 and item2 if they were not found
        # Lookup in the middle of slice for item1
        chunksize = self.chunksize  # Number of elements/chunksize
        nchunk = -1
        # Try to get the bounds row from the LRU cache
        nslot = boundscache.getslot(nrow)
        if nslot >= 0:
            # Cache hit. Use the row kept there.
            bounds = boundscache.getitem(nslot)
        else:
            # No luck with cached data. Read the row and put it in the cache.
            bounds = self._v_parent.bounds[nrow]
            size = bounds.size * bounds.itemsize
            boundscache.setitem(nrow, bounds, size)
        if result1 < 0:
            # Search the appropriate chunk in bounds cache
            nchunk = bisect_left(bounds, item1)
            chunk = self._read_sorted_slice(nrow, chunksize * nchunk,
                                            chunksize * (nchunk + 1))
            result1 = self._bisect_left(chunk, item1, chunksize)
            result1 += chunksize * nchunk
        # Lookup in the middle of slice for item2
        if result2 < 0:
            # Search the appropriate chunk in bounds cache
            nchunk2 = bisect_right(bounds, item2)
            if nchunk2 != nchunk:
                chunk = self._read_sorted_slice(nrow, chunksize * nchunk2,
                                                chunksize * (nchunk2 + 1))
            result2 = self._bisect_right(chunk, item2, chunksize)
            result2 += chunksize * nchunk2
        return (result1, result2)

    _searchBin = previous_api(_search_bin)

    def __str__(self):
        "A compact representation of this class"
        return "IndexArray(path=%s)" % self._v_pathname

    def __repr__(self):
        """A verbose representation of this class."""

        return """%s
  atom = %r
  shape = %s
  nrows = %s
  chunksize = %s
  slicesize = %s
  byteorder = %r""" % (self, self.atom, self.shape, self.nrows,
                       self.chunksize, self.slicesize, self.byteorder)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = indexesExtension
from warnings import warn
from tables.indexesextension import *

_warnmsg = ("indexesExtension is pending deprecation, import indexesextension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = leaf
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: October 14, 2002
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Leaf class."""

import warnings
import math

import numpy

from tables.flavor import (check_flavor, internal_flavor,
                           alias_map as flavor_alias_map)
from tables.node import Node
from tables.filters import Filters
from tables.utils import byteorders, lazyattr, SizeType
from tables.exceptions import PerformanceWarning
from tables import utilsextension
from tables._past import previous_api


def csformula(expected_mb):
    """Return the fitted chunksize for expected_mb."""

    # For a basesize of 8 KB, this will return:
    # 8 KB for datasets <= 1 MB
    # 1 MB for datasets >= 10 TB
    basesize = 8 * 1024   # 8 KB is a good minimum
    return basesize * int(2**math.log10(expected_mb))


def limit_es(expected_mb):
    """Protection against creating too small or too large chunks."""

    if expected_mb < 1:        # < 1 MB
        expected_mb = 1
    elif expected_mb > 10**7:  # > 10 TB
        expected_mb = 10**7
    return expected_mb


def calc_chunksize(expected_mb):
    """Compute the optimum HDF5 chunksize for I/O purposes.

    Rational: HDF5 takes the data in bunches of chunksize length to
    write the on disk. A BTree in memory is used to map structures on
    disk. The more chunks that are allocated for a dataset the larger
    the B-tree. Large B-trees take memory and causes file storage
    overhead as well as more disk I/O and higher contention for the meta
    data cache.  You have to balance between memory and I/O overhead
    (small B-trees) and time to access to data (big B-trees).

    The tuning of the chunksize parameter affects the performance and
    the memory consumed. This is based on my own experiments and, as
    always, your mileage may vary.

    """

    expected_mb = limit_es(expected_mb)
    zone = int(math.log10(expected_mb))
    expected_mb = 10**zone
    chunksize = csformula(expected_mb)
    return chunksize * 8     # XXX: Multiply by 8 seems optimal for
                           # sequential access


class Leaf(Node):
    """Abstract base class for all PyTables leaves.

    A leaf is a node (see the Node class in :class:`Node`) which hangs from a
    group (see the Group class in :class:`Group`) but, unlike a group, it can
    not have any further children below it (i.e. it is an end node).

    This definition includes all nodes which contain actual data (datasets
    handled by the Table - see :ref:`TableClassDescr`, Array -
    see :ref:`ArrayClassDescr`, CArray - see :ref:`CArrayClassDescr`, EArray -
    see :ref:`EArrayClassDescr`, and VLArray - see :ref:`VLArrayClassDescr`
    classes) and unsupported nodes (the UnImplemented
    class - :ref:`UnImplementedClassDescr`) these classes do in fact inherit
    from Leaf.


    .. rubric:: Leaf attributes

    These instance variables are provided in addition to those in Node
    (see :ref:`NodeClassDescr`):

    .. attribute:: byteorder

        The byte ordering of the leaf data *on disk*.  It will be either
        ``little`` or ``big``.

    .. attribute:: dtype

        The NumPy dtype that most closely matches this leaf type.

    .. attribute:: extdim

        The index of the enlargeable dimension (-1 if none).

    .. attribute:: nrows

        The length of the main dimension of the leaf data.

    .. attribute:: nrowsinbuf

        The number of rows that fit in internal input buffers.

        You can change this to fine-tune the speed or memory
        requirements of your application.

    .. attribute:: shape

        The shape of data in the leaf.

    """

    # Properties
    # ~~~~~~~~~~

    # Node property aliases
    # `````````````````````
    # These are a little hard to override, but so are properties.
    attrs = Node._v_attrs
    """The associated AttributeSet instance - see :ref:`AttributeSetClassDescr`
    (This is an easier-to-write alias of :attr:`Node._v_attrs`."""
    title = Node._v_title
    """A description for this node
    (This is an easier-to-write alias of :attr:`Node._v_title`)."""

    # Read-only node property aliases
    # ```````````````````````````````
    name = property(
        lambda self: self._v_name, None, None,
        """The name of this node in its parent group
        (This is an easier-to-write alias of :attr:`Node._v_name`).""")

    chunkshape = property(
        lambda self: getattr(self, '_v_chunkshape', None), None, None,
        """The HDF5 chunk size for chunked leaves (a tuple).

        This is read-only because you cannot change the chunk size of a
        leaf once it has been created.
        """)

    object_id = property(
        lambda self: self._v_objectid, None, None,
        """A node identifier, which may change from run to run.
        (This is an easier-to-write alias of :attr:`Node._v_objectid`).

        .. versionchanged:: 3.0
           The *objectID* property has been renamed into *object_id*.

        """)

    objectID = previous_api(object_id)

    ndim = property(
        lambda self: len(self.shape), None, None,
        """The number of dimensions of the leaf data.

        .. versionadded: 2.4""")

    # Lazy read-only attributes
    # `````````````````````````
    @lazyattr
    def filters(self):
        """Filter properties for this leaf.

        See Also
        --------
        Filters

        """

        return Filters._from_leaf(self)

    # Other properties
    # ````````````````
    def _getmaindim(self):
        if self.extdim < 0:
            return 0  # choose the first dimension
        return self.extdim

    maindim = property(
        _getmaindim, None, None,
        """The dimension along which iterators work.

        Its value is 0 (i.e. the first dimension) when the dataset is not
        extendable, and self.extdim (where available) for extendable ones.
        """)

    def _setflavor(self, flavor):
        self._v_file._check_writable()
        check_flavor(flavor)
        self._v_attrs.FLAVOR = self._flavor = flavor  # logs the change

    def _delflavor(self):
        del self._v_attrs.FLAVOR
        self._flavor = internal_flavor

    flavor = property(
        lambda self: self._flavor, _setflavor, _delflavor,
        """The type of data object read from this leaf.

        It can be any of 'numpy' or 'python'.

        You can (and are encouraged to) use this property to get, set
        and delete the FLAVOR HDF5 attribute of the leaf. When the leaf
        has no such attribute, the default flavor is used..
        """)

    size_on_disk = property(lambda self: self._get_storage_size(), None, None,
                            """
        The size of this leaf's data in bytes as it is stored on disk.  If the
        data is compressed, this shows the compressed size.  In the case of
        uncompressed, chunked data, this may be slightly larger than the amount
        of data, due to partially filled chunks.
        """)

    # Special methods
    # ~~~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 new=False, filters=None,
                 byteorder=None, _log=True):
        self._v_new = new
        """Is this the first time the node has been created?"""
        self.nrowsinbuf = None
        """
        The number of rows that fits in internal input buffers.

        You can change this to fine-tune the speed or memory
        requirements of your application.
        """
        self._flavor = None
        """Private storage for the `flavor` property."""

        if new:
            # Get filter properties from parent group if not given.
            if filters is None:
                filters = parentnode._v_filters
            self.__dict__['filters'] = filters  # bypass the property

            if byteorder not in (None, 'little', 'big'):
                raise ValueError(
                    "the byteorder can only take 'little' or 'big' values "
                    "and you passed: %s" % byteorder)
            self.byteorder = byteorder
            """The byte ordering of the leaf data *on disk*."""

        # Existing filters need not be read since `filters`
        # is a lazy property that automatically handles their loading.

        super(Leaf, self).__init__(parentnode, name, _log)

    def __len__(self):
        """Return the length of the main dimension of the leaf data.

        Please note that this may raise an OverflowError on 32-bit platforms
        for datasets having more than 2**31-1 rows.  This is a limitation of
        Python that you can work around by using the nrows or shape attributes.

        """

        return self.nrows

    def __str__(self):
        """The string representation for this object is its pathname in the
        HDF5 object tree plus some additional metainfo."""

        # Get this class name
        classname = self.__class__.__name__
        # The title
        title = self._v_title
        # The filters
        filters = ""
        if self.filters.fletcher32:
            filters += ", fletcher32"
        if self.filters.complevel:
            if self.filters.shuffle:
                filters += ", shuffle"
            filters += ", %s(%s)" % (self.filters.complib,
                                     self.filters.complevel)
        return "%s (%s%s%s) %r" % \
               (self._v_pathname, classname, self.shape, filters, title)

    # Private methods
    # ~~~~~~~~~~~~~~~
    def _g_post_init_hook(self):
        """Code to be run after node creation and before creation logging.

        This method gets or sets the flavor of the leaf.

        """

        super(Leaf, self)._g_post_init_hook()
        if self._v_new:  # set flavor of new node
            if self._flavor is None:
                self._flavor = internal_flavor
            else:  # flavor set at creation time, do not log
                if self._v_file.params['PYTABLES_SYS_ATTRS']:
                    self._v_attrs._g__setattr('FLAVOR', self._flavor)
        else:  # get flavor of existing node (if any)
            if self._v_file.params['PYTABLES_SYS_ATTRS']:
                flavor = getattr(self._v_attrs, 'FLAVOR', internal_flavor)
                self._flavor = flavor_alias_map.get(flavor, flavor)
            else:
                self._flavor = internal_flavor

    _g_postInitHook = previous_api(_g_post_init_hook)

    def _calc_chunkshape(self, expectedrows, rowsize, itemsize):
        """Calculate the shape for the HDF5 chunk."""

        # In case of a scalar shape, return the unit chunksize
        if self.shape == ():
            return (SizeType(1),)

        # Compute the chunksize
        MB = 1024 * 1024
        expected_mb = (expectedrows * rowsize) // MB
        chunksize = calc_chunksize(expected_mb)

        maindim = self.maindim
        # Compute the chunknitems
        chunknitems = chunksize // itemsize
        # Safeguard against itemsizes being extremely large
        if chunknitems == 0:
            chunknitems = 1
        chunkshape = list(self.shape)
        # Check whether trimming the main dimension is enough
        chunkshape[maindim] = 1
        newchunknitems = numpy.prod(chunkshape, dtype=SizeType)
        if newchunknitems <= chunknitems:
            chunkshape[maindim] = chunknitems // newchunknitems
        else:
            # No, so start trimming other dimensions as well
            for j in xrange(len(chunkshape)):
                # Check whether trimming this dimension is enough
                chunkshape[j] = 1
                newchunknitems = numpy.prod(chunkshape, dtype=SizeType)
                if newchunknitems <= chunknitems:
                    chunkshape[j] = chunknitems // newchunknitems
                    break
            else:
                # Ops, we ran out of the loop without a break
                # Set the last dimension to chunknitems
                chunkshape[-1] = chunknitems

        return tuple(SizeType(s) for s in chunkshape)

    def _calc_nrowsinbuf(self):
        """Calculate the number of rows that fits on a PyTables buffer."""

        params = self._v_file.params
        # Compute the nrowsinbuf
        rowsize = self.rowsize
        buffersize = params['IO_BUFFER_SIZE']
        nrowsinbuf = buffersize // rowsize

        # tableextension.pyx performs an assertion
        # to make sure nrowsinbuf is greater than or
        # equal to the chunksize.
        # See gh-206 and gh-238
        if self.chunkshape is not None:
            chunksize = self.chunkshape[self.maindim]
            if nrowsinbuf < chunksize:
                nrowsinbuf = chunksize

        # Safeguard against row sizes being extremely large
        if nrowsinbuf == 0:
            nrowsinbuf = 1
            # If rowsize is too large, issue a Performance warning
            maxrowsize = params['BUFFER_TIMES'] * buffersize
            if rowsize > maxrowsize:
                warnings.warn("""\
The Leaf ``%s`` is exceeding the maximum recommended rowsize (%d bytes);
be ready to see PyTables asking for *lots* of memory and possibly slow
I/O.  You may want to reduce the rowsize by trimming the value of
dimensions that are orthogonal (and preferably close) to the *main*
dimension of this leave.  Alternatively, in case you have specified a
very small/large chunksize, you may want to increase/decrease it."""
                              % (self._v_pathname, maxrowsize),
                              PerformanceWarning)
        return nrowsinbuf

    # This method is appropriate for calls to __getitem__ methods
    def _process_range(self, start, stop, step, dim=None, warn_negstep=True):
        if dim is None:
            nrows = self.nrows  # self.shape[self.maindim]
        else:
            nrows = self.shape[dim]

        if warn_negstep and step and step < 0:
            raise ValueError("slice step cannot be negative")
        # (start, stop, step) = slice(start, stop, step).indices(nrows)
        # The next function is a substitute for slice().indices in order to
        # support full 64-bit integer for slices even in 32-bit machines.
        # F. Alted 2005-05-08
        start, stop, step = utilsextension.get_indices(start, stop, step,
                                                       long(nrows))
        return (start, stop, step)

    _processRange = previous_api(_process_range)

    # This method is appropiate for calls to read() methods
    def _process_range_read(self, start, stop, step, warn_negstep=True):
        nrows = self.nrows
        if start is None and stop is None:
        #if start is None and stop is None and step is None:
            start = 0
            stop = nrows
            #step = 1
        if start is not None and stop is None and step is None:
            # Protection against start greater than available records
            # nrows == 0 is a special case for empty objects
            if nrows > 0 and start >= nrows:
                raise IndexError("start of range (%s) is greater than "
                                 "number of rows (%s)" % (start, nrows))
            step = 1
            if start == -1:  # corner case
                stop = nrows
            else:
                stop = start + 1
        # Finally, get the correct values (over the main dimension)
        start, stop, step = self._process_range(start, stop, step,
                                                warn_negstep=warn_negstep)
        return (start, stop, step)

    _processRangeRead = previous_api(_process_range_read)

    def _g_copy(self, newparent, newname, recursive, _log=True, **kwargs):
        # Compute default arguments.
        start = kwargs.pop('start', None)
        stop = kwargs.pop('stop', None)
        step = kwargs.pop('step', None)
        title = kwargs.pop('title', self._v_title)
        filters = kwargs.pop('filters', self.filters)
        chunkshape = kwargs.pop('chunkshape', self.chunkshape)
        copyuserattrs = kwargs.pop('copyuserattrs', True)
        stats = kwargs.pop('stats', None)
        if chunkshape == 'keep':
            chunkshape = self.chunkshape  # Keep the original chunkshape
        elif chunkshape == 'auto':
            chunkshape = None             # Will recompute chunkshape

        # Fix arguments with explicit None values for backwards compatibility.
        if title is None:
            title = self._v_title
        if filters is None:
            filters = self.filters

        # Create a copy of the object.
        (new_node, bytes) = self._g_copy_with_stats(
            newparent, newname, start, stop, step,
            title, filters, chunkshape, _log, **kwargs)

        # Copy user attributes if requested (or the flavor at least).
        if copyuserattrs:
            self._v_attrs._g_copy(new_node._v_attrs, copyclass=True)
        elif 'FLAVOR' in self._v_attrs:
            if self._v_file.params['PYTABLES_SYS_ATTRS']:
                new_node._v_attrs._g__setattr('FLAVOR', self._flavor)
        new_node._flavor = self._flavor  # update cached value

        # Update statistics if needed.
        if stats is not None:
            stats['leaves'] += 1
            stats['bytes'] += bytes

        return new_node

    def _g_fix_byteorder_data(self, data, dbyteorder):
        "Fix the byteorder of data passed in constructors."
        dbyteorder = byteorders[dbyteorder]
        # If self.byteorder has not been passed as an argument of
        # the constructor, then set it to the same value of data.
        if self.byteorder is None:
            self.byteorder = dbyteorder
        # Do an additional in-place byteswap of data if the in-memory
        # byteorder doesn't match that of the on-disk.  This is the only
        # place that we have to do the conversion manually. In all the
        # other cases, it will be HDF5 the responsible of doing the
        # byteswap properly.
        if dbyteorder in ['little', 'big']:
            if dbyteorder != self.byteorder:
                # if data is not writeable, do a copy first
                if not data.flags.writeable:
                    data = data.copy()
                data.byteswap(True)
        else:
            # Fix the byteorder again, no matter which byteorder have
            # specified the user in the constructor.
            self.byteorder = "irrelevant"
        return data

    def _point_selection(self, key):
        """Perform a point-wise selection.

        `key` can be any of the following items:

        * A boolean array with the same shape than self. Those positions
          with True values will signal the coordinates to be returned.

        * A numpy array (or list or tuple) with the point coordinates.
          This has to be a two-dimensional array of size len(self.shape)
          by num_elements containing a list of of zero-based values
          specifying the coordinates in the dataset of the selected
          elements. The order of the element coordinates in the array
          specifies the order in which the array elements are iterated
          through when I/O is performed. Duplicate coordinate locations
          are not checked for.

        Return the coordinates array.  If this is not possible, raise a
        `TypeError` so that the next selection method can be tried out.

        This is useful for whatever `Leaf` instance implementing a
        point-wise selection.

        """

        if type(key) in (list, tuple):
            if isinstance(key, tuple) and len(key) > len(self.shape):
                raise IndexError("Invalid index or slice: %r" % (key,))
            # Try to convert key to a numpy array.  If not possible,
            # a TypeError will be issued (to be catched later on).
            try:
                key = numpy.array(key)
            except ValueError:
                raise TypeError("Invalid index or slice: %r" % (key,))
        elif not isinstance(key, numpy.ndarray):
            raise TypeError("Invalid index or slice: %r" % (key,))

        # Protection against empty keys
        if len(key) == 0:
            return numpy.array([], dtype="i8")

        if key.dtype.kind == 'b':
            if not key.shape == self.shape:
                raise IndexError(
                    "Boolean indexing array has incompatible shape")
            # Get the True coordinates (64-bit indices!)
            coords = numpy.asarray(key.nonzero(), dtype='i8')
            coords = numpy.transpose(coords)
        elif key.dtype.kind == 'i' or key.dtype.kind == 'u':
            if len(key.shape) > 2:
                raise IndexError(
                    "Coordinate indexing array has incompatible shape")
            elif len(key.shape) == 2:
                if key.shape[0] != len(self.shape):
                    raise IndexError(
                        "Coordinate indexing array has incompatible shape")
                coords = numpy.asarray(key, dtype="i8")
                coords = numpy.transpose(coords)
            else:
                # For 1-dimensional datasets
                coords = numpy.asarray(key, dtype="i8")
        else:
            raise TypeError("Only integer coordinates allowed.")
        # We absolutely need a contiguous array
        if not coords.flags.contiguous:
            coords = coords.copy()
        return coords

    _pointSelection = previous_api(_point_selection)

    # Public methods
    # ~~~~~~~~~~~~~~
    # Tree manipulation
    # `````````````````
    def remove(self):
        """Remove this node from the hierarchy.

        This method has the behavior described
        in :meth:`Node._f_remove`. Please note that there is no recursive flag
        since leaves do not have child nodes.

        """

        self._f_remove(False)

    def rename(self, newname):
        """Rename this node in place.

        This method has the behavior described in :meth:`Node._f_rename()`.

        """

        self._f_rename(newname)

    def move(self, newparent=None, newname=None,
             overwrite=False, createparents=False):
        """Move or rename this node.

        This method has the behavior described in :meth:`Node._f_move`

        """

        self._f_move(newparent, newname, overwrite, createparents)

    def copy(self, newparent=None, newname=None,
             overwrite=False, createparents=False, **kwargs):
        """Copy this node and return the new one.

        This method has the behavior described in :meth:`Node._f_copy`. Please
        note that there is no recursive flag since leaves do not have child
        nodes.

        .. warning::

            Note that unknown parameters passed to this method will be
            ignored, so may want to double check the spelling of these
            (i.e. if you write them incorrectly, they will most probably
            be ignored).

        Parameters
        ----------
        title
            The new title for the destination. If omitted or None, the original
            title is used.
        filters : Filters
            Specifying this parameter overrides the original filter properties
            in the source node. If specified, it must be an instance of the
            Filters class (see :ref:`FiltersClassDescr`). The default is to
            copy the filter properties from the source node.
        copyuserattrs
            You can prevent the user attributes from being copied by setting
            this parameter to False. The default is to copy them.
        start, stop, step : int
            Specify the range of rows to be copied; the default is to copy all
            the rows.
        stats
            This argument may be used to collect statistics on the copy
            process. When used, it should be a dictionary with keys 'groups',
            'leaves' and 'bytes' having a numeric value. Their values will be
            incremented to reflect the number of groups, leaves and bytes,
            respectively, that have been copied during the operation.
        chunkshape
            The chunkshape of the new leaf.  It supports a couple of special
            values.  A value of keep means that the chunkshape will be the same
            than original leaf (this is the default).  A value of auto means
            that a new shape will be computed automatically in order to ensure
            best performance when accessing the dataset through the main
            dimension.  Any other value should be an integer or a tuple
            matching the dimensions of the leaf.

        """

        return self._f_copy(
            newparent, newname, overwrite, createparents, **kwargs)

    def truncate(self, size):
        """Truncate the main dimension to be size rows.

        If the main dimension previously was larger than this size, the extra
        data is lost.  If the main dimension previously was shorter, it is
        extended, and the extended part is filled with the default values.

        The truncation operation can only be applied to *enlargeable* datasets,
        else a TypeError will be raised.

        """

        # A non-enlargeable arrays (Array, CArray) cannot be truncated
        if self.extdim < 0:
            raise TypeError("non-enlargeable datasets cannot be truncated")
        self._g_truncate(size)

    def isvisible(self):
        """Is this node visible?

        This method has the behavior described in :meth:`Node._f_isvisible()`.

        """

        return self._f_isvisible()

    isVisible = previous_api(isvisible)

    # Attribute handling
    # ``````````````````
    def get_attr(self, name):
        """Get a PyTables attribute from this node.

        This method has the behavior described in :meth:`Node._f_getattr`.

        """

        return self._f_getattr(name)

    getAttr = previous_api(get_attr)

    def set_attr(self, name, value):
        """Set a PyTables attribute for this node.

        This method has the behavior described in :meth:`Node._f_setattr()`.

        """

        self._f_setattr(name, value)

    setAttr = previous_api(set_attr)

    def del_attr(self, name):
        """Delete a PyTables attribute from this node.

        This method has the behavior described in :meth:`Node_f_delAttr`.

        """

        self._f_delattr(name)

    delAttr = previous_api(del_attr)

    # Data handling
    # `````````````
    def flush(self):
        """Flush pending data to disk.

        Saves whatever remaining buffered data to disk. It also releases
        I/O buffers, so if you are filling many datasets in the same
        PyTables session, please call flush() extensively so as to help
        PyTables to keep memory requirements low.

        """

        self._g_flush()

    def _f_close(self, flush=True):
        """Close this node in the tree.

        This method has the behavior described in :meth:`Node._f_close`.
        Besides that, the optional argument flush tells whether to flush
        pending data to disk or not before closing.

        """

        if not self._v_isopen:
            return  # the node is already closed or not initialized

        # Only do a flush in case the leaf has an IO buffer.  The
        # internal buffers of HDF5 will be flushed afterwards during the
        # self._g_close() call.  Avoiding an unnecessary flush()
        # operation accelerates the closing for the unbuffered leaves.
        if flush and hasattr(self, "_v_iobuf"):
            self.flush()

        # Close the dataset and release resources
        self._g_close()

        # Close myself as a node.
        super(Leaf, self)._f_close()

    def close(self, flush=True):
        """Close this node in the tree.

        This method is completely equivalent to :meth:`Leaf._f_close`.

        """

        self._f_close(flush)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = link
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: November 25, 2009
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Create links in the HDF5 file.

This module implements containers for soft and external links.  Hard
links doesn't need a container as such as they are the same as regular
nodes (groups or leaves).

Classes:

    SoftLink
    ExternalLink

Functions:

Misc variables:

"""

import os
import tables
from tables import linkextension
from tables.node import Node
from tables.utils import lazyattr
from tables.attributeset import AttributeSet
import tables.file
from tables._past import previous_api, previous_api_property


def _g_get_link_class(parent_id, name):
    """Guess the link class."""

    return linkextension._get_link_class(parent_id, name)

_g_getLinkClass = previous_api(_g_get_link_class)


class Link(Node):
    """Abstract base class for all PyTables links.

    A link is a node that refers to another node.  The Link class inherits from
    Node class and the links that inherits from Link are SoftLink and
    ExternalLink.  There is not a HardLink subclass because hard links behave
    like a regular Group or Leaf.  Contrarily to other nodes, links cannot have
    HDF5 attributes.  This is an HDF5 library limitation that might be solved
    in future releases.

    See :ref:`LinksTutorial` for a small tutorial on how to work with links.

    .. rubric:: Link attributes

    .. attribute:: target

        The path string to the pointed node.

    """

    # Properties
    @lazyattr
    def _v_attrs(self):
        """
        A *NoAttrs* instance replacing the typical *AttributeSet* instance of
        other node objects.  The purpose of *NoAttrs* is to make clear that
        HDF5 attributes are not supported in link nodes.
        """
        class NoAttrs(AttributeSet):
            def __getattr__(self, name):
                raise KeyError("you cannot get attributes from this "
                               "`%s` instance" % self.__class__.__name__)

            def __setattr__(self, name, value):
                raise KeyError("you cannot set attributes to this "
                               "`%s` instance" % self.__class__.__name__)

            def _g_close(self):
                pass
        return NoAttrs(self)

    def __init__(self, parentnode, name, target=None, _log=False):
        self._v_new = target is not None
        self.target = target
        """The path string to the pointed node."""

        super(Link, self).__init__(parentnode, name, _log)

    # Public and tailored versions for copy, move, rename and remove methods
    def copy(self, newparent=None, newname=None,
             overwrite=False, createparents=False):
        """Copy this link and return the new one.

        See :meth:`Node._f_copy` for a complete explanation of the arguments.
        Please note that there is no recursive flag since links do not have
        child nodes.

        """

        newnode = self._f_copy(newparent=newparent, newname=newname,
                               overwrite=overwrite,
                               createparents=createparents)
        # Insert references to a `newnode` via `newname`
        newnode._v_parent._g_refnode(newnode, newname, True)
        return newnode

    def move(self, newparent=None, newname=None, overwrite=False):
        """Move or rename this link.

        See :meth:`Node._f_move` for a complete explanation of the arguments.

        """

        return self._f_move(newparent=newparent, newname=newname,
                            overwrite=overwrite)

    def remove(self):
        """Remove this link from the hierarchy."""

        return self._f_remove()

    def rename(self, newname=None, overwrite=False):
        """Rename this link in place.

        See :meth:`Node._f_rename` for a complete explanation of the arguments.

        """

        return self._f_rename(newname=newname, overwrite=overwrite)

    def __repr__(self):
        return str(self)


class SoftLink(linkextension.SoftLink, Link):
    """Represents a soft link (aka symbolic link).

    A soft link is a reference to another node in the *same* file hierarchy.
    Getting access to the pointed node (this action is called *dereferrencing*)
    is done via the __call__ special method (see below).

    """

    # Class identifier.
    _c_classid = 'SOFTLINK'

    _c_classId = previous_api_property('_c_classid')

    def __call__(self):
        """Dereference `self.target` and return the object.

        Examples
        --------

        ::

            >>> f=tables.open_file('data/test.h5')
            >>> print(f.root.link0)
            /link0 (SoftLink) -> /another/path
            >>> print(f.root.link0())
            /another/path (Group) ''

        """

        target = self.target
        # Check for relative pathnames
        if not self.target.startswith('/'):
            target = self._v_parent._g_join(self.target)
        return self._v_file._get_node(target)

    def __str__(self):
        """Return a short string representation of the link.

        Examples
        --------

        ::

            >>> f=tables.open_file('data/test.h5')
            >>> print(f.root.link0)
            /link0 (SoftLink) -> /path/to/node

        """

        classname = self.__class__.__name__
        target = self.target
        # Check for relative pathnames
        if not self.target.startswith('/'):
            target = self._v_parent._g_join(self.target)
        if target in self._v_file:
            dangling = ""
        else:
            dangling = " (dangling)"
        return "%s (%s) -> %s%s" % (self._v_pathname, classname,
                                    self.target, dangling)


class ExternalLink(linkextension.ExternalLink, Link):
    """Represents an external link.

    An external link is a reference to a node in *another* file.
    Getting access to the pointed node (this action is called
    *dereferencing*) is done via the :meth:`__call__` special method
    (see below).

    .. rubric:: ExternalLink attributes

    .. attribute:: extfile

        The external file handler, if the link has been dereferenced.
        In case the link has not been dereferenced yet, its value is
        None.

    """

    # Class identifier.
    _c_classid = 'EXTERNALLINK'

    _c_classId = previous_api_property('_c_classid')

    def __init__(self, parentnode, name, target=None, _log=False):
        self.extfile = None
        """The external file handler, if the link has been dereferenced.
        In case the link has not been dereferenced yet, its value is
        None."""
        super(ExternalLink, self).__init__(parentnode, name, target, _log)

    def _get_filename_node(self):
        """Return the external filename and nodepath from `self.target`."""

        # This is needed for avoiding the 'C:\\file.h5' filepath notation
        filename, target = self.target.split(':/')
        return filename, '/' + target

    def __call__(self, **kwargs):
        """Dereference self.target and return the object.

        You can pass all the arguments supported by the :func:`open_file`
        function (except filename, of course) so as to open the referenced
        external file.

        Examples
        --------

        ::

            >>> f=tables.open_file('data1/test1.h5')
            >>> print(f.root.link2)
            /link2 (ExternalLink) -> data2/test2.h5:/path/to/node
            >>> plink2 = f.root.link2('a')  # open in 'a'ppend mode
            >>> print(plink2)
            /path/to/node (Group) ''
            >>> print(plink2._v_filename)
            'data2/test2.h5'        # belongs to referenced file

        """

        filename, target = self._get_filename_node()

        if not os.path.isabs(filename):
            # Resolve the external link with respect to the this
            # file's directory.  See #306.
            base_directory = os.path.dirname(self._v_file.filename)
            filename = os.path.join(base_directory, filename)

        if self.extfile is None or not self.extfile.isopen:
            self.extfile = tables.open_file(filename, **kwargs)
        else:
            # XXX: implement better consistency checks
            assert self.extfile.filename == filename
            assert self.extfile.mode == kwargs.get('mode', 'r')

        return self.extfile._get_node(target)

    def umount(self):
        """Safely unmount self.extfile, if opened."""

        extfile = self.extfile
        # Close external file, if open
        if extfile is not None and extfile.isopen:
            extfile.close()
            self.extfile = None

    def _f_close(self):
        """Especific close for external links."""

        self.umount()
        super(ExternalLink, self)._f_close()

    def __str__(self):
        """Return a short string representation of the link.

        Examples
        --------

        ::

            >>> f=tables.open_file('data1/test1.h5')
            >>> print(f.root.link2)
            /link2 (ExternalLink) -> data2/test2.h5:/path/to/node

        """

        classname = self.__class__.__name__
        return "%s (%s) -> %s" % (self._v_pathname, classname, self.target)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = linkExtension
from warnings import warn
from tables.linkextension import *

_warnmsg = ("linkExtension is pending deprecation, import linextension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = lrucacheExtension
from warnings import warn
from tables.lrucacheextension import *

_warnmsg = ("lrucacheExtension is pending deprecation, import lrucacheextension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = enum
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: May 4, 2005
# Author:  Ivan Vilata i Balaguer - reverse:net.selidor@ivan
#
# $Id$
#
########################################################################

"""Implementation of enumerated types.

This module provides the `Enum` class, which can be used to construct
enumerated types.  Those types are defined by providing an *exhaustive
set or list* of possible, named values for a variable of that type.
Enumerated variables of the same type are usually compared between them
for equality and sometimes for order, but are not usually operated upon.

Enumerated values have an associated *name* and *concrete value*.  Every
name is unique and so are concrete values.  An enumerated variable
always takes the concrete value, not its name.  Usually, the concrete
value is not used directly, and frequently it is entirely irrelevant.
For the same reason, an enumerated variable is not usually compared with
concrete values out of its enumerated type.  For that kind of use,
standard variables and constants are more adequate.

"""

from tables._past import previous_api


__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


class Enum(object):
    """Enumerated type.

    Each instance of this class represents an enumerated type. The
    values of the type must be declared
    *exhaustively* and named with
    *strings*, and they might be given explicit
    concrete values, though this is not compulsory. Once the type is
    defined, it can not be modified.

    There are three ways of defining an enumerated type. Each one
    of them corresponds to the type of the only argument in the
    constructor of Enum:

    - *Sequence of names*: each enumerated
      value is named using a string, and its order is determined by
      its position in the sequence; the concrete value is assigned
      automatically::

          >>> boolEnum = Enum(['True', 'False'])

    - *Mapping of names*: each enumerated
      value is named by a string and given an explicit concrete value.
      All of the concrete values must be different, or a
      ValueError will be raised::

          >>> priority = Enum({'red': 20, 'orange': 10, 'green': 0})
          >>> colors = Enum({'red': 1, 'blue': 1})
          Traceback (most recent call last):
          ...
          ValueError: enumerated values contain duplicate concrete values: 1

    - *Enumerated type*: in that case, a copy
      of the original enumerated type is created. Both enumerated
      types are considered equal::

          >>> prio2 = Enum(priority)
          >>> priority == prio2
          True

    Please note that names starting with _ are
    not allowed, since they are reserved for internal usage::

        >>> prio2 = Enum(['_xx'])
        Traceback (most recent call last):
        ...
        ValueError: name of enumerated value can not start with ``_``: '_xx'

    The concrete value of an enumerated value is obtained by
    getting its name as an attribute of the Enum
    instance (see __getattr__()) or as an item (see
    __getitem__()). This allows comparisons between
    enumerated values and assigning them to ordinary Python
    variables::

        >>> redv = priority.red
        >>> redv == priority['red']
        True
        >>> redv > priority.green
        True
        >>> priority.red == priority.orange
        False

    The name of the enumerated value corresponding to a concrete
    value can also be obtained by using the
    __call__() method of the enumerated type. In this
    way you get the symbolic name to use it later with
    __getitem__()::

        >>> priority(redv)
        'red'
        >>> priority.red == priority[priority(priority.red)]
        True

    (If you ask, the __getitem__() method is
    not used for this purpose to avoid ambiguity in the case of using
    strings as concrete values.)

    """

    def __init__(self, enum):
        mydict = self.__dict__

        mydict['_names'] = {}
        mydict['_values'] = {}

        if isinstance(enum, list) or isinstance(enum, tuple):
            for (value, name) in enumerate(enum):  # values become 0, 1, 2...
                self._check_and_set_pair(name, value)
        elif isinstance(enum, dict):
            for (name, value) in enum.iteritems():
                self._check_and_set_pair(name, value)
        elif isinstance(enum, Enum):
            for (name, value) in enum._names.iteritems():
                self._check_and_set_pair(name, value)
        else:
            raise TypeError("""\
enumerations can only be created from \
sequences, mappings and other enumerations""")

    def _check_and_set_pair(self, name, value):
        """Check validity of enumerated value and insert it into type."""

        names = self._names
        values = self._values

        if not isinstance(name, basestring):
            raise TypeError(
                "name of enumerated value is not a string: %r" % (name,))
        if name.startswith('_'):
            raise ValueError(
                "name of enumerated value can not start with ``_``: %r"
                % name)
        # This check is only necessary with a sequence base object.
        if name in names:
            raise ValueError(
                "enumerated values contain duplicate names: %r" % name)
        # This check is only necessary with a mapping base object.
        if value in values:
            raise ValueError(
                "enumerated values contain duplicate concrete values: %r"
                % value)

        names[name] = value
        values[value] = name
        self.__dict__[name] = value

    _checkAndSetPair = previous_api(_check_and_set_pair)

    def __getitem__(self, name):
        """Get the concrete value of the enumerated value with that name.

        The name of the enumerated value must be a string. If there is no value
        with that name in the enumeration, a KeyError is raised.

        Examples
        --------

        Let ``enum`` be an enumerated type defined as:

        >>> enum = Enum({'T0': 0, 'T1': 2, 'T2': 5})

        then:

        >>> enum['T1']
        2
        >>> enum['foo']
        Traceback (most recent call last):
          ...
        KeyError: "no enumerated value with that name: 'foo'"

        """

        try:
            return self._names[name]
        except KeyError:
            raise KeyError("no enumerated value with that name: %r" % (name,))

    def __setitem__(self, name, value):
        """This operation is forbidden."""
        raise IndexError("operation not allowed")

    def __delitem__(self, name):
        """This operation is forbidden."""
        raise IndexError("operation not allowed")

    def __getattr__(self, name):
        """Get the concrete value of the enumerated value with that name.

        The name of the enumerated value must be a string. If there is no value
        with that name in the enumeration, an AttributeError is raised.

        Examples
        --------
        Let ``enum`` be an enumerated type defined as:

        >>> enum = Enum({'T0': 0, 'T1': 2, 'T2': 5})

        then:

        >>> enum.T1
        2
        >>> enum.foo
        Traceback (most recent call last):
          ...
        AttributeError: no enumerated value with that name: 'foo'

        """

        try:
            return self[name]
        except KeyError as ke:
            raise AttributeError(*ke.args)

    def __setattr__(self, name, value):
        """This operation is forbidden."""
        raise AttributeError("operation not allowed")

    def __delattr__(self, name):
        """This operation is forbidden."""
        raise AttributeError("operation not allowed")

    def __contains__(self, name):
        """Is there an enumerated value with that name in the type?

        If the enumerated type has an enumerated value with that name, True is
        returned.  Otherwise, False is returned. The name must be a string.

        This method does *not* check for concrete values matching a value in an
        enumerated type. For that, please use the :meth:`Enum.__call__` method.

        Examples
        --------
        Let ``enum`` be an enumerated type defined as:

        >>> enum = Enum({'T0': 0, 'T1': 2, 'T2': 5})

        then:

        >>> 'T1' in enum
        True
        >>> 'foo' in enum
        False
        >>> 0 in enum
        Traceback (most recent call last):
          ...
        TypeError: name of enumerated value is not a string: 0
        >>> enum.T1 in enum  # Be careful with this!
        Traceback (most recent call last):
          ...
        TypeError: name of enumerated value is not a string: 2

        """

        if not isinstance(name, basestring):
            raise TypeError(
                "name of enumerated value is not a string: %r" % (name,))
        return name in self._names

    def __call__(self, value, *default):
        """Get the name of the enumerated value with that concrete value.

        If there is no value with that concrete value in the enumeration and a
        second argument is given as a default, this is returned. Else, a
        ValueError is raised.

        This method can be used for checking that a concrete value belongs to
        the set of concrete values in an enumerated type.

        Examples
        --------
        Let ``enum`` be an enumerated type defined as:

        >>> enum = Enum({'T0': 0, 'T1': 2, 'T2': 5})

        then:

        >>> enum(5)
        'T2'
        >>> enum(42, None) is None
        True
        >>> enum(42)
        Traceback (most recent call last):
          ...
        ValueError: no enumerated value with that concrete value: 42

        """

        try:
            return self._values[value]
        except KeyError:
            if len(default) > 0:
                return default[0]
            raise ValueError(
                "no enumerated value with that concrete value: %r" % (value,))

    def __len__(self):
        """Return the number of enumerated values in the enumerated type.

        Examples
        --------
        >>> len(Enum(['e%d' % i for i in range(10)]))
        10

        """

        return len(self._names)

    def __iter__(self):
        """Iterate over the enumerated values.

        Enumerated values are returned as (name, value) pairs *in no particular
        order*.

        Examples
        --------
        >>> enumvals = {'red': 4, 'green': 2, 'blue': 1}
        >>> enum = Enum(enumvals)
        >>> enumdict = dict([(name, value) for (name, value) in enum])
        >>> enumvals == enumdict
        True

        """

        for name_value in self._names.iteritems():
            yield name_value

    def __eq__(self, other):
        """Is the other enumerated type equivalent to this one?

        Two enumerated types are equivalent if they have exactly the same
        enumerated values (i.e. with the same names and concrete values).

        Examples
        --------

        Let ``enum*`` be enumerated types defined as:

        >>> enum1 = Enum({'T0': 0, 'T1': 2})
        >>> enum2 = Enum(enum1)
        >>> enum3 = Enum({'T1': 2, 'T0': 0})
        >>> enum4 = Enum({'T0': 0, 'T1': 2, 'T2': 5})
        >>> enum5 = Enum({'T0': 0})
        >>> enum6 = Enum({'T0': 10, 'T1': 20})

        then:

        >>> enum1 == enum1
        True
        >>> enum1 == enum2 == enum3
        True
        >>> enum1 == enum4
        False
        >>> enum5 == enum1
        False
        >>> enum1 == enum6
        False

        Comparing enumerated types with other kinds of objects produces
        a false result:

        >>> enum1 == {'T0': 0, 'T1': 2}
        False
        >>> enum1 == ['T0', 'T1']
        False
        >>> enum1 == 2
        False

        """

        if not isinstance(other, Enum):
            return False
        return self._names == other._names

    def __ne__(self, other):
        """Is the `other` enumerated type different from this one?

        Two enumerated types are different if they don't have exactly
        the same enumerated values (i.e. with the same names and
        concrete values).

        Examples
        --------

        Let ``enum*`` be enumerated types defined as:

        >>> enum1 = Enum({'T0': 0, 'T1': 2})
        >>> enum2 = Enum(enum1)
        >>> enum3 = Enum({'T1': 2, 'T0': 0})
        >>> enum4 = Enum({'T0': 0, 'T1': 2, 'T2': 5})
        >>> enum5 = Enum({'T0': 0})
        >>> enum6 = Enum({'T0': 10, 'T1': 20})

        then:

        >>> enum1 != enum1
        False
        >>> enum1 != enum2 != enum3
        False
        >>> enum1 != enum4
        True
        >>> enum5 != enum1
        True
        >>> enum1 != enum6
        True

        """

        return not self.__eq__(other)

    # XXX: API incompatible change for PyTables 3 line
    # Overriding __eq__ blocks inheritance of __hash__ in 3.x
    # def __hash__(self):
    #    return hash((self.__class__, tuple(self._names.items())))
    def __repr__(self):
        """Return the canonical string representation of the enumeration. The
        output of this method can be evaluated to give a new enumeration object
        that will compare equal to this one.

        Examples
        --------
        >>> repr(Enum({'name': 10}))
        "Enum({'name': 10})"

        """

        return 'Enum(%s)' % self._names


def _test():
    import doctest
    return doctest.testmod()


if __name__ == '__main__':
    _test()



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = proxydict
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-07-07
# Author:  Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Proxy dictionary for objects stored in a container."""

import weakref

from tables._past import previous_api, previous_api_property


class ProxyDict(dict):
    """A dictionary which uses a container object to store its values."""

    containerRef = previous_api_property('containerref')

    def __init__(self, container):
        self.containerref = weakref.ref(container)
        """A weak reference to the container object.

        .. versionchanged:: 3.0
           The *containerRef* attribute has been renamed into
           *containerref*.

        """

    def __getitem__(self, key):
        if key not in self:
            raise KeyError(key)

        # Values are not actually stored to avoid extra references.
        return self._get_value_from_container(self._get_container(), key)

    def __setitem__(self, key, value):
        # Values are not actually stored to avoid extra references.
        super(ProxyDict, self).__setitem__(key, None)

    def __repr__(self):
        return object.__repr__(self)

    def __str__(self):
        # C implementation does not use `self.__getitem__()`. :(
        itemFormat = '%r: %r'
        itemReprs = [itemFormat % item for item in self.iteritems()]
        return '{%s}' % ', '.join(itemReprs)

    def values(self):
        # C implementation does not use `self.__getitem__()`. :(
        valueList = []
        for key in self.iterkeys():
            valueList.append(self[key])
        return valueList

    def itervalues(self):
        # C implementation does not use `self.__getitem__()`. :(
        for key in self.iterkeys():
            yield self[key]
        raise StopIteration

    def items(self):
        # C implementation does not use `self.__getitem__()`. :(
        itemList = []
        for key in self.iterkeys():
            itemList.append((key, self[key]))
        return itemList

    def iteritems(self):
        # C implementation does not use `self.__getitem__()`. :(
        for key in self.iterkeys():
            yield (key, self[key])
        raise StopIteration

    def _get_container(self):
        container = self.containerref()
        if container is None:
            raise ValueError("the container object does no longer exist")
        return container

    _getContainer = previous_api(_get_container)

########NEW FILE########
__FILENAME__ = node
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-02-11
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""PyTables nodes."""

import warnings

from tables.registry import class_name_dict, class_id_dict
from tables.exceptions import (ClosedNodeError, NodeError, UndoRedoWarning,
                               PerformanceWarning)
from tables.path import join_path, split_path, isvisiblepath
from tables.utils import lazyattr
from tables.undoredo import move_to_shadow
from tables.attributeset import AttributeSet, NotLoggedAttributeSet
from tables._past import previous_api, previous_api_property


__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


def _closedrepr(oldmethod):
    """Decorate string representation method to handle closed nodes.

    If the node is closed, a string like this is returned::

      <closed MODULE.CLASS at ADDRESS>

    instead of calling `oldmethod` and returning its result.

    """

    def newmethod(self):
        if not self._v_isopen:
            cmod = self.__class__.__module__
            cname = self.__class__.__name__
            addr = hex(id(self))
            return '<closed %s.%s at %s>' % (cmod, cname, addr)
        return oldmethod(self)
    newmethod.__name__ = oldmethod.__name__
    newmethod.__doc__ = oldmethod.__doc__
    return newmethod


class MetaNode(type):
    """Node metaclass.

    This metaclass ensures that their instance classes get registered
    into several dictionaries (namely the `tables.utils.class_name_dict`
    class name dictionary and the `tables.utils.class_id_dict` class
    identifier dictionary).

    It also adds sanity checks to some methods:

      * Check that the node is open when calling string representation
        and provide a default string if so.

    """

    def __new__(class_, name, bases, dict_):
        # Add default behaviour for representing closed nodes.
        for mname in ['__str__', '__repr__']:
            if mname in dict_:
                dict_[mname] = _closedrepr(dict_[mname])

        return type.__new__(class_, name, bases, dict_)

    def __init__(class_, name, bases, dict_):
        super(MetaNode, class_).__init__(name, bases, dict_)

        # Always register into class name dictionary.
        class_name_dict[class_.__name__] = class_

        # Register into class identifier dictionary only if the class
        # has an identifier and it is different from its parents'.
        cid = getattr(class_, '_c_classid', None)
        if cid is not None:
            for base in bases:
                pcid = getattr(base, '_c_classid', None)
                if pcid == cid:
                    break
            else:
                class_id_dict[cid] = class_


class Node(object):
    """Abstract base class for all PyTables nodes.

    This is the base class for *all* nodes in a PyTables hierarchy. It is an
    abstract class, i.e. it may not be directly instantiated; however, every
    node in the hierarchy is an instance of this class.

    A PyTables node is always hosted in a PyTables *file*, under a *parent
    group*, at a certain *depth* in the node hierarchy. A node knows its own
    *name* in the parent group and its own *path name* in the file.

    All the previous information is location-dependent, i.e. it may change when
    moving or renaming a node in the hierarchy. A node also has
    location-independent information, such as its *HDF5 object identifier* and
    its *attribute set*.

    This class gathers the operations and attributes (both location-dependent
    and independent) which are common to all PyTables nodes, whatever their
    type is. Nonetheless, due to natural naming restrictions, the names of all
    of these members start with a reserved prefix (see the Group class
    in :ref:`GroupClassDescr`).

    Sub-classes with no children (e.g. *leaf nodes*) may define new methods,
    attributes and properties to avoid natural naming restrictions. For
    instance, _v_attrs may be shortened to attrs and _f_rename to
    rename. However, the original methods and attributes should still be
    available.

    .. rubric:: Node attributes

    .. attribute:: _v_depth

        The depth of this node in the tree (an non-negative integer value).

    .. attribute:: _v_file

        The hosting File instance (see :ref:`FileClassDescr`).

    .. attribute:: _v_name

        The name of this node in its parent group (a string).

    .. attribute:: _v_pathname

        The path of this node in the tree (a string).

    .. attribute:: _v_objectid

        A node identifier (may change from run to run).

        .. versionchanged:: 3.0
           The *_v_objectID* attribute has been renamed into *_v_object_id*.

    """

    # This makes this class and all derived subclasses be handled by MetaNode.
    __metaclass__ = MetaNode

    # By default, attributes accept Undo/Redo.
    _AttributeSet = AttributeSet

    # <properties>
    # `_v_parent` is accessed via its file to avoid upwards references.
    def _g_getparent(self):
        (parentpath, nodename) = split_path(self._v_pathname)
        return self._v_file._get_node(parentpath)

    _v_parent = property(
        _g_getparent, None, None, ("The parent :class:`Group` instance"))

    # '_v_attrs' is defined as a lazy read-only attribute.
    # This saves 0.7s/3.8s.
    @lazyattr
    def _v_attrs(self):
        """The associated `AttributeSet` instance.

        See Also
        --------
        tables.attributeset.AttributeSet : container for the HDF5 attributes

        """

        return self._AttributeSet(self)

    # '_v_title' is a direct read-write shorthand for the 'TITLE' attribute
    # with the empty string as a default value.
    def _g_gettitle(self):
        if hasattr(self._v_attrs, 'TITLE'):
            return self._v_attrs.TITLE
        else:
            return ''

    def _g_settitle(self, title):
        self._v_attrs.TITLE = title

    _v_title = property(_g_gettitle, _g_settitle, None,
                        ("A description of this node. A shorthand for "
                         "TITLE attribute."))

    # </properties>

    # This may be looked up by ``__del__`` when ``__init__`` doesn't get
    # to be called.  See ticket #144 for more info.
    _v_isopen = False
    """Whehter this node is open or not."""

    _v_objectId = previous_api_property('_v_objectid')
    _v_maxTreeDepth = previous_api_property('_v_maxtreedepth')

    # The ``_log`` argument is only meant to be used by ``_g_copy_as_child()``
    # to avoid logging the creation of children nodes of a copied sub-tree.
    def __init__(self, parentnode, name, _log=True):
        # Remember to assign these values in the root group constructor
        # as it does not use this method implementation!

        self._v_file = None
        """The hosting File instance (see :ref:`FileClassDescr`)."""

        self._v_isopen = False
        """Whether this node is open or not."""

        self._v_pathname = None
        """The path of this node in the tree (a string)."""

        self._v_name = None
        """The name of this node in its parent group (a string)."""

        self._v_depth = None
        """The depth of this node in the tree (an non-negative integer value).
        """

        self._v_maxtreedepth = parentnode._v_file.params['MAX_TREE_DEPTH']
        """Maximum tree depth before warning the user.

        .. versionchanged:: 3.0
           Renamed into *_v_maxtreedepth* from *_v_maxTreeDepth*.

        """

        self._v__deleting = False
        """Is the node being deleted?"""

        self._v_objectid = None
        """A node identifier (may change from run to run).

        .. versionchanged:: 3.0
           The *_v_objectID* attribute has been renamed into *_v_objectid*.

        """

        validate = new = self._v_new  # set by subclass constructor

        # Is the parent node a group?  Is it open?
        self._g_check_group(parentnode)
        parentnode._g_check_open()
        file_ = parentnode._v_file

        # Will the file be able to host a new node?
        if new:
            file_._check_writable()

        # Bind to the parent node and set location-dependent information.
        if new:
            # Only new nodes need to be referenced.
            # Opened nodes are already known by their parent group.
            parentnode._g_refnode(self, name, validate)
        self._g_set_location(parentnode, name)

        try:
            # hdf5extension operations:
            #   Update node attributes.
            self._g_new(parentnode, name, init=True)
            #   Create or open the node and get its object ID.
            if new:
                self._v_objectid = self._g_create()
            else:
                self._v_objectid = self._g_open()

            # The node *has* been created, log that.
            if new and _log and file_.is_undo_enabled():
                self._g_log_create()

            # This allows extra operations after creating the node.
            self._g_post_init_hook()
        except:
            # If anything happens, the node must be closed
            # to undo every possible registration made so far.
            # We do *not* rely on ``__del__()`` doing it later,
            # since it might never be called anyway.
            self._f_close()
            raise

    def _g_log_create(self):
        self._v_file._log('CREATE', self._v_pathname)

    _g_logCreate = previous_api(_g_log_create)

    def __del__(self):
        # Closed `Node` instances can not be killed and revived.
        # Instead, accessing a closed and deleted (from memory, not
        # disk) one yields a *new*, open `Node` instance.  This is
        # because of two reasons:
        #
        # 1. Predictability.  After closing a `Node` and deleting it,
        #    only one thing can happen when accessing it again: a new,
        #    open `Node` instance is returned.  If closed nodes could be
        #    revived, one could get either a closed or an open `Node`.
        #
        # 2. Ease of use.  If the user wants to access a closed node
        #    again, the only condition would be that no references to
        #    the `Node` instance were left.  If closed nodes could be
        #    revived, the user would also need to force the closed
        #    `Node` out of memory, which is not a trivial task.
        #

        if not self._v_isopen:
            return  # the node is already closed or not initialized

        self._v__deleting = True

        # If we get here, the `Node` is still open.
        try:
            node_manager = self._v_file._node_manager
            node_manager.drop_node(self, check_unregistered=False)
        finally:
            # At this point the node can still be open if there is still some
            # alive reference around (e.g. if the __del__ method is called
            # explicitly by the user).
            if self._v_isopen:
                self._v__deleting = True
                self._f_close()

    def _g_pre_kill_hook(self):
        """Code to be called before killing the node."""
        pass

    _g_preKillHook = previous_api(_g_pre_kill_hook)

    def _g_create(self):
        """Create a new HDF5 node and return its object identifier."""
        raise NotImplementedError

    def _g_open(self):
        """Open an existing HDF5 node and return its object identifier."""
        raise NotImplementedError

    def _g_check_open(self):
        """Check that the node is open.

        If the node is closed, a `ClosedNodeError` is raised.

        """

        if not self._v_isopen:
            raise ClosedNodeError("the node object is closed")
        assert self._v_file.isopen, "found an open node in a closed file"

    _g_checkOpen = previous_api(_g_check_open)

    def _g_set_location(self, parentnode, name):
        """Set location-dependent attributes.

        Sets the location-dependent attributes of this node to reflect
        that it is placed under the specified `parentnode`, with the
        specified `name`.

        This also triggers the insertion of file references to this
        node.  If the maximum recommended tree depth is exceeded, a
        `PerformanceWarning` is issued.

        """

        file_ = parentnode._v_file
        parentdepth = parentnode._v_depth

        self._v_file = file_
        self._v_isopen = True

        root_uep = file_.root_uep
        if name.startswith(root_uep):
            # This has been called from File._get_node()
            assert parentdepth == 0
            if root_uep == "/":
                self._v_pathname = name
            else:
                self._v_pathname = name[len(root_uep):]
            _, self._v_name = split_path(name)
            self._v_depth = name.count("/") - root_uep.count("/") + 1
        else:
            # If we enter here is because this has been called elsewhere
            self._v_name = name
            self._v_pathname = join_path(parentnode._v_pathname, name)
            self._v_depth = parentdepth + 1

        # Check if the node is too deep in the tree.
        if parentdepth >= self._v_maxtreedepth:
            warnings.warn("""\
node ``%s`` is exceeding the recommended maximum depth (%d);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                          % (self._v_pathname, self._v_maxtreedepth),
                          PerformanceWarning)

        if self._v_pathname != '/':
            file_._node_manager.cache_node(self, self._v_pathname)

    _g_setLocation = previous_api(_g_set_location)

    def _g_update_location(self, newparentpath):
        """Update location-dependent attributes.

        Updates location data when an ancestor node has changed its
        location in the hierarchy to `newparentpath`.  In fact, this
        method is expected to be called by an ancestor of this node.

        This also triggers the update of file references to this node.
        If the maximum recommended node depth is exceeded, a
        `PerformanceWarning` is issued.  This warning is assured to be
        unique.

        """

        oldpath = self._v_pathname
        newpath = join_path(newparentpath, self._v_name)
        newdepth = newpath.count('/')

        self._v_pathname = newpath
        self._v_depth = newdepth

        # Check if the node is too deep in the tree.
        if newdepth > self._v_maxtreedepth:
            warnings.warn("""\
moved descendent node is exceeding the recommended maximum depth (%d);\
be ready to see PyTables asking for *lots* of memory and possibly slow I/O"""
                          % (self._v_maxtreedepth,), PerformanceWarning)

        node_manager = self._v_file._node_manager
        node_manager.rename_node(oldpath, newpath)

        # Tell dependent objects about the new location of this node.
        self._g_update_dependent()

    _g_updateLocation = previous_api(_g_update_location)

    def _g_del_location(self):
        """Clear location-dependent attributes.

        This also triggers the removal of file references to this node.

        """

        node_manager = self._v_file._node_manager
        pathname = self._v_pathname

        if not self._v__deleting:
            node_manager.drop_from_cache(pathname)
            # Note: node_manager.drop_node do not removes the node form the
            # registry if it is still open
            node_manager.registry.pop(pathname, None)

        self._v_file = None
        self._v_isopen = False
        self._v_pathname = None
        self._v_name = None
        self._v_depth = None

    _g_delLocation = previous_api(_g_del_location)

    def _g_post_init_hook(self):
        """Code to be run after node creation and before creation logging."""
        pass

    _g_postInitHook = previous_api(_g_post_init_hook)

    def _g_update_dependent(self):
        """Update dependent objects after a location change.

        All dependent objects (but not nodes!) referencing this node
        must be updated here.

        """

        if '_v_attrs' in self.__dict__:
            self._v_attrs._g_update_node_location(self)

    _g_updateDependent = previous_api(_g_update_dependent)

    def _f_close(self):
        """Close this node in the tree.

        This releases all resources held by the node, so it should not
        be used again.  On nodes with data, it may be flushed to disk.

        You should not need to close nodes manually because they are
        automatically opened/closed when they are loaded/evicted from
        the integrated LRU cache.

        """

        # After calling ``_f_close()``, two conditions are met:
        #
        #   1. The node object is detached from the tree.
        #   2. *Every* attribute of the node is removed.
        #
        # Thus, cleanup operations used in ``_f_close()`` in sub-classes
        # must be run *before* calling the method in the superclass.

        if not self._v_isopen:
            return  # the node is already closed

        myDict = self.__dict__

        # Close the associated `AttributeSet`
        # only if it has already been placed in the object's dictionary.
        if '_v_attrs' in myDict:
            self._v_attrs._g_close()

        # Detach the node from the tree if necessary.
        self._g_del_location()

        # Finally, clear all remaining attributes from the object.
        myDict.clear()

        # Just add a final flag to signal that the node is closed:
        self._v_isopen = False

    def _g_remove(self, recursive, force):
        """Remove this node from the hierarchy.

        If the node has children, recursive removal must be stated by
        giving `recursive` a true value; otherwise, a `NodeError` will
        be raised.

        If `force` is set to true, the node will be removed no matter it
        has children or not (useful for deleting hard links).

        It does not log the change.

        """

        # Remove the node from the PyTables hierarchy.
        parent = self._v_parent
        parent._g_unrefnode(self._v_name)
        # Close the node itself.
        self._f_close()
        # hdf5extension operations:
        # Remove the node from the HDF5 hierarchy.
        self._g_delete(parent)

    def _f_remove(self, recursive=False, force=False):
        """Remove this node from the hierarchy.

        If the node has children, recursive removal must be stated by giving
        recursive a true value; otherwise, a NodeError will be raised.

        If the node is a link to a Group object, and you are sure that you want
        to delete it, you can do this by setting the force flag to true.

        """

        self._g_check_open()
        file_ = self._v_file
        file_._check_writable()

        if file_.is_undo_enabled():
            self._g_remove_and_log(recursive, force)
        else:
            self._g_remove(recursive, force)

    def _g_remove_and_log(self, recursive, force):
        file_ = self._v_file
        oldpathname = self._v_pathname
        # Log *before* moving to use the right shadow name.
        file_._log('REMOVE', oldpathname)
        move_to_shadow(file_, oldpathname)

    _g_removeAndLog = previous_api(_g_remove_and_log)

    def _g_move(self, newparent, newname):
        """Move this node in the hierarchy.

        Moves the node into the given `newparent`, with the given
        `newname`.

        It does not log the change.

        """

        oldparent = self._v_parent
        oldname = self._v_name
        oldpathname = self._v_pathname  # to move the HDF5 node

        # Try to insert the node into the new parent.
        newparent._g_refnode(self, newname)
        # Remove the node from the new parent.
        oldparent._g_unrefnode(oldname)

        # Remove location information for this node.
        self._g_del_location()
        # Set new location information for this node.
        self._g_set_location(newparent, newname)

        # hdf5extension operations:
        #   Update node attributes.
        self._g_new(newparent, self._v_name, init=False)
        #   Move the node.
        # self._v_parent._g_move_node(oldpathname, self._v_pathname)
        self._v_parent._g_move_node(oldparent._v_objectid, oldname,
                                    newparent._v_objectid, newname,
                                    oldpathname, self._v_pathname)

        # Tell dependent objects about the new location of this node.
        self._g_update_dependent()

    def _f_rename(self, newname, overwrite=False):
        """Rename this node in place.

        Changes the name of a node to *newname* (a string).  If a node with the
        same newname already exists and overwrite is true, recursively remove
        it before renaming.

        """

        self._f_move(newname=newname, overwrite=overwrite)

    def _f_move(self, newparent=None, newname=None,
                overwrite=False, createparents=False):
        """Move or rename this node.

        Moves a node into a new parent group, or changes the name of the
        node. newparent can be a Group object (see :ref:`GroupClassDescr`) or a
        pathname in string form. If it is not specified or None, the current
        parent group is chosen as the new parent.  newname must be a string
        with a new name. If it is not specified or None, the current name is
        chosen as the new name. If createparents is true, the needed groups for
        the given new parent group path to exist will be created.

        Moving a node across databases is not allowed, nor it is moving a node
        *into* itself. These result in a NodeError. However, moving a node
        *over* itself is allowed and simply does nothing. Moving over another
        existing node is similarly not allowed, unless the optional overwrite
        argument is true, in which case that node is recursively removed before
        moving.

        Usually, only the first argument will be used, effectively moving the
        node to a new location without changing its name.  Using only the
        second argument is equivalent to renaming the node in place.

        """

        self._g_check_open()
        file_ = self._v_file
        oldparent = self._v_parent
        oldname = self._v_name

        # Set default arguments.
        if newparent is None and newname is None:
            raise NodeError("you should specify at least "
                            "a ``newparent`` or a ``newname`` parameter")
        if newparent is None:
            newparent = oldparent
        if newname is None:
            newname = oldname

        # Get destination location.
        if hasattr(newparent, '_v_file'):  # from node
            newfile = newparent._v_file
            newpath = newparent._v_pathname
        elif hasattr(newparent, 'startswith'):  # from path
            newfile = file_
            newpath = newparent
        else:
            raise TypeError("new parent is not a node nor a path: %r"
                            % (newparent,))

        # Validity checks on arguments.
        # Is it in the same file?
        if newfile is not file_:
            raise NodeError("nodes can not be moved across databases; "
                            "please make a copy of the node")

        # The movement always fails if the hosting file can not be modified.
        file_._check_writable()

        # Moving over itself?
        oldpath = oldparent._v_pathname
        if newpath == oldpath and newname == oldname:
            # This is equivalent to renaming the node to its current name,
            # and it does not change the referenced object,
            # so it is an allowed no-op.
            return

        # Moving into itself?
        self._g_check_not_contains(newpath)

        # Note that the previous checks allow us to go ahead and create
        # the parent groups if `createparents` is true.  `newparent` is
        # used instead of `newpath` to avoid accepting `Node` objects
        # when `createparents` is true.
        newparent = file_._get_or_create_path(newparent, createparents)
        self._g_check_group(newparent)  # Is it a group?

        # Moving over an existing node?
        self._g_maybe_remove(newparent, newname, overwrite)

        # Move the node.
        oldpathname = self._v_pathname
        self._g_move(newparent, newname)

        # Log the change.
        if file_.is_undo_enabled():
            self._g_log_move(oldpathname)

    def _g_log_move(self, oldpathname):
        self._v_file._log('MOVE', oldpathname, self._v_pathname)

    _g_logMove = previous_api(_g_log_move)

    def _g_copy(self, newparent, newname, recursive, _log=True, **kwargs):
        """Copy this node and return the new one.

        Creates and returns a copy of the node in the given `newparent`,
        with the given `newname`.  If `recursive` copy is stated, all
        descendents are copied as well.  Additional keyword argumens may
        affect the way that the copy is made.  Unknown arguments must be
        ignored.  On recursive copies, all keyword arguments must be
        passed on to the children invocation of this method.

        If `_log` is false, the change is not logged.  This is *only*
        intended to be used by ``_g_copy_as_child()`` as a means of
        optimising sub-tree copies.

        """

        raise NotImplementedError

    def _g_copy_as_child(self, newparent, **kwargs):
        """Copy this node as a child of another group.

        Copies just this node into `newparent`, not recursing children
        nor overwriting nodes nor logging the copy.  This is intended to
        be used when copying whole sub-trees.

        """

        return self._g_copy(newparent, self._v_name,
                            recursive=False, _log=False, **kwargs)

    _g_copyAsChild = previous_api(_g_copy_as_child)

    def _f_copy(self, newparent=None, newname=None,
                overwrite=False, recursive=False, createparents=False,
                **kwargs):
        """Copy this node and return the new node.

        Creates and returns a copy of the node, maybe in a different place in
        the hierarchy. newparent can be a Group object (see
        :ref:`GroupClassDescr`) or a pathname in string form. If it is not
        specified or None, the current parent group is chosen as the new
        parent.  newname must be a string with a new name. If it is not
        specified or None, the current name is chosen as the new name. If
        recursive copy is stated, all descendants are copied as well. If
        createparents is true, the needed groups for the given new parent group
        path to exist will be created.

        Copying a node across databases is supported but can not be
        undone. Copying a node over itself is not allowed, nor it is
        recursively copying a node into itself. These result in a
        NodeError. Copying over another existing node is similarly not allowed,
        unless the optional overwrite argument is true, in which case that node
        is recursively removed before copying.

        Additional keyword arguments may be passed to customize the copying
        process. For instance, title and filters may be changed, user
        attributes may be or may not be copied, data may be sub-sampled, stats
        may be collected, etc. See the documentation for the particular node
        type.

        Using only the first argument is equivalent to copying the node to a
        new location without changing its name. Using only the second argument
        is equivalent to making a copy of the node in the same group.

        """

        self._g_check_open()
        srcfile = self._v_file
        srcparent = self._v_parent
        srcname = self._v_name

        dstparent = newparent
        dstname = newname

        # Set default arguments.
        if dstparent is None and dstname is None:
            raise NodeError("you should specify at least "
                            "a ``newparent`` or a ``newname`` parameter")
        if dstparent is None:
            dstparent = srcparent
        if dstname is None:
            dstname = srcname

        # Get destination location.
        if hasattr(dstparent, '_v_file'):  # from node
            dstfile = dstparent._v_file
            dstpath = dstparent._v_pathname
        elif hasattr(dstparent, 'startswith'):  # from path
            dstfile = srcfile
            dstpath = dstparent
        else:
            raise TypeError("new parent is not a node nor a path: %r"
                            % (dstparent,))

        # Validity checks on arguments.
        if dstfile is srcfile:
            # Copying over itself?
            srcpath = srcparent._v_pathname
            if dstpath == srcpath and dstname == srcname:
                raise NodeError(
                    "source and destination nodes are the same node: ``%s``"
                    % self._v_pathname)

            # Recursively copying into itself?
            if recursive:
                self._g_check_not_contains(dstpath)

        # Note that the previous checks allow us to go ahead and create
        # the parent groups if `createparents` is true.  `dstParent` is
        # used instead of `dstPath` because it may be in other file, and
        # to avoid accepting `Node` objects when `createparents` is
        # true.
        dstparent = srcfile._get_or_create_path(dstparent, createparents)
        self._g_check_group(dstparent)  # Is it a group?

        # Copying to another file with undo enabled?
        if dstfile is not srcfile and srcfile.is_undo_enabled():
            warnings.warn("copying across databases can not be undone "
                          "nor redone from this database",
                          UndoRedoWarning)

        # Copying over an existing node?
        self._g_maybe_remove(dstparent, dstname, overwrite)

        # Copy the node.
        # The constructor of the new node takes care of logging.
        return self._g_copy(dstparent, dstname, recursive, **kwargs)

    def _f_isvisible(self):
        """Is this node visible?"""

        self._g_check_open()
        return isvisiblepath(self._v_pathname)

    _f_isVisible = previous_api(_f_isvisible)

    def _g_check_group(self, node):
        # Node must be defined in order to define a Group.
        # However, we need to know Group here.
        # Using class_name_dict avoids a circular import.
        if not isinstance(node, class_name_dict['Node']):
            raise TypeError("new parent is not a registered node: %s"
                            % node._v_pathname)
        if not isinstance(node, class_name_dict['Group']):
            raise TypeError("new parent node ``%s`` is not a group"
                            % node._v_pathname)

    _g_checkGroup = previous_api(_g_check_group)

    def _g_check_not_contains(self, pathname):
        # The not-a-TARDIS test. ;)
        mypathname = self._v_pathname
        if (mypathname == '/'  # all nodes fall below the root group
           or pathname == mypathname
           or pathname.startswith(mypathname + '/')):
            raise NodeError("can not move or recursively copy node ``%s`` "
                            "into itself" % mypathname)

    _g_checkNotContains = previous_api(_g_check_not_contains)

    def _g_maybe_remove(self, parent, name, overwrite):
        if name in parent:
            if not overwrite:
                raise NodeError("""\
destination group ``%s`` already has a node named ``%s``; \
you may want to use the ``overwrite`` argument""" % (parent._v_pathname, name))
            parent._f_get_child(name)._f_remove(True)

    _g_maybeRemove = previous_api(_g_maybe_remove)

    def _g_check_name(self, name):
        """Check validity of name for this particular kind of node.

        This is invoked once the standard HDF5 and natural naming checks
        have successfully passed.

        """

        if name.startswith('_i_'):
            # This is reserved for table index groups.
            raise ValueError(
                "node name starts with reserved prefix ``_i_``: %s" % name)

    _g_checkName = previous_api(_g_check_name)

    # <attribute handling>
    def _f_getattr(self, name):
        """Get a PyTables attribute from this node.

        If the named attribute does not exist, an AttributeError is
        raised.

        """

        return getattr(self._v_attrs, name)

    _f_getAttr = previous_api(_f_getattr)

    def _f_setattr(self, name, value):
        """Set a PyTables attribute for this node.

        If the node already has a large number of attributes, a
        PerformanceWarning is issued.

        """

        setattr(self._v_attrs, name, value)

    _f_setAttr = previous_api(_f_setattr)

    def _f_delattr(self, name):
        """Delete a PyTables attribute from this node.

        If the named attribute does not exist, an AttributeError is
        raised.

        """

        delattr(self._v_attrs, name)

    _f_delAttr = previous_api(_f_delattr)

    # </attribute handling>


class NotLoggedMixin:
    # Include this class in your inheritance tree
    # to avoid changes to instances of your class from being logged.

    _AttributeSet = NotLoggedAttributeSet

    def _g_log_create(self):
        pass

    _g_logCreate = previous_api(_g_log_create)

    def _g_log_move(self, oldpathname):
        pass

    _g_logMove = previous_api(_g_log_move)

    def _g_remove_and_log(self, recursive, force):
        self._g_remove(recursive, force)

    _g_removeAndLog = previous_api(_g_remove_and_log)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = filenode
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: October 2, 2004
# Author:  Ivan Vilata i Balaguer - reverse:net.selidor@ivan
#
# $Id$
#
########################################################################

"""A file interface to nodes for PyTables databases.

The FileNode module provides a file interface for using inside of
PyTables database files.  Use the new_node() function to create a brand
new file node which can be read and written as any ordinary Python
file.  Use the open_node() function to open an existing (i.e. created
with new_node()) node for read-only or read-write access.  Read acces
is always available.  Write access (enabled on new files and files
opened with mode 'a+') only allows appending data to a file node.

Currently only binary I/O is supported.

See :ref:`filenode_usersguide` for instructions on use.

.. versionchanged:: 3.0
   In version 3.0 the module as been completely rewritten to be fully
   compliant with the interfaces defined in the :mod:`io` module.

"""

import io
import os
import warnings

import numpy as np

import tables
from tables._past import previous_api


NodeType = 'file'
"""Value for NODE_TYPE node system attribute."""

NodeTypeVersions = [1, 2]
"""Supported values for NODE_TYPE_VERSION node system attribute."""


# have a Python2/3 compatible way to check for string
try:
    string_types = basestring
except NameError:
    string_types = str


class RawPyTablesIO(io.RawIOBase):
    """Base class for raw binary I/O on HDF5 files using PyTables."""

    # A lambda to turn a size into a shape, for each version.
    _size_to_shape = [
        None,
        lambda l: (l, 1),
        lambda l: (l, ),
    ]

    def __init__(self, node, mode=None):
        super(RawPyTablesIO, self).__init__()

        self._check_node(node)
        self._check_attributes(node)

        if mode is None:
            mode = node._v_file.mode
        else:
            self._check_mode(mode)
            self._cross_check_mode(mode, node._v_file.mode)

        self._node = node
        self._mode = mode
        self._pos = 0
        self._version = int(node.attrs.NODE_TYPE_VERSION)
        self._vshape = self._size_to_shape[self._version]
        self._vtype = node.atom.dtype.base.type

    # read only attribute
    @property
    def mode(self):
        """File mode."""

        return self._mode

    #def tell(self) -> int:
    def tell(self):
        """Return current stream position."""

        self._checkClosed()
        return self._pos

    #def seek(self, pos: int, whence: int = 0) -> int:
    def seek(self, pos, whence=0):
        """Change stream position.

        Change the stream position to byte offset offset. offset is
        interpreted relative to the position indicated by whence.  Values
        for whence are:

        * 0 -- start of stream (the default); offset should be zero or positive
        * 1 -- current stream position; offset may be negative
        * 2 -- end of stream; offset is usually negative

        Return the new absolute position.

        """

        self._checkClosed()
        try:
            pos = pos.__index__()
        #except AttributeError as err:
            #raise TypeError("an integer is required") from err
        except AttributeError:
            raise TypeError("an integer is required")
        if whence == 0:
            if pos < 0:
                raise ValueError("negative seek position %r" % (pos,))
            self._pos = pos
        elif whence == 1:
            self._pos = max(0, self._pos + pos)
        elif whence == 2:
            self._pos = max(0, self._node.nrows + pos)
        else:
            raise ValueError("invalid whence value")
        return self._pos

    #def seekable(self) -> bool:
    def seekable(self):
        """Return whether object supports random access.

        If False, seek(), tell() and truncate() will raise IOError. This
        method may need to do a test seek().

        """

        return True

    #def fileno(self) -> int:
    def fileno(self):
        """Returns underlying file descriptor if one exists.

        An IOError is raised if the IO object does not use a file
        descriptor.

        """

        self._checkClosed()
        self._node._v_file.fileno()

    #def close(self) -> None:
    def close(self):
        """Flush and close the IO object.

        This method has no effect if the file is already closed.

        """

        if not self.closed:
            if getattr(self._node, '_v_file', None) is None:
                warnings.warn("host PyTables file is already closed!")

        try:
            super(RawPyTablesIO, self).close()
        finally:
            # Release node object to allow closing the file.
            self._node = None

    def flush(self):
        """Flush write buffers, if applicable.

        This is not implemented for read-only and non-blocking streams.

        """

        self._checkClosed()
        self._node.flush()

    #def truncate(self, pos: int = None) -> int:
    def truncate(self, pos=None):
        """Truncate file to size bytes.

        Size defaults to the current IO position as reported by tell().
        Return the new size.

        Currently, this method only makes sense to grow the file node,
        since data can not be rewritten nor deleted.

        """

        self._checkClosed()
        self._checkWritable()

        if pos is None:
            pos = self._pos
        elif pos < 0:
            raise ValueError("negative truncate position %r" % (pos,))

        if pos < self._node.nrows:
            raise IOError("truncating is only allowed for growing a file")
        self._append_zeros(pos - self._node.nrows)

        return self.seek(pos)

    #def readable(self) -> bool:
    def readable(self):
        """Return whether object was opened for reading.

        If False, read() will raise IOError.

        """

        mode = self._mode
        return 'r' in mode or '+' in mode

    #def writable(self) -> bool:
    def writable(self):
        """Return whether object was opened for writing.

        If False, write() and truncate() will raise IOError.

        """

        mode = self._mode
        return 'w' in mode or 'a' in mode or '+' in mode

    #def readinto(self, b: bytearray) -> int:
    def readinto(self, b):
        """Read up to len(b) bytes into b.

        Returns number of bytes read (0 for EOF), or None if the object
        is set not to block as has no data to read.

        """

        self._checkClosed()
        self._checkReadable()

        if self._pos >= self._node.nrows:
            return 0

        n = len(b)
        start = self._pos
        stop = self._pos + n

        # XXX optimized path
        #if stop <= self._node.nrows and isinstance(b, np.ndarray):
        #    self._node.read(start, stop, out=b)
        #    self._pos += n
        #    return n

        if stop > self._node.nrows:
            stop = self._node.nrows
            n = stop - start

        # XXX This ought to work with anything that supports the buffer API
        b[:n] = self._node.read(start, stop).tostring()

        self._pos += n

        return n

    #def readline(self, limit: int = -1) -> bytes:
    def readline(self, limit=-1):
        """Read and return a line from the stream.

        If limit is specified, at most limit bytes will be read.

        The line terminator is always ``\\n`` for binary files; for text
        files, the newlines argument to open can be used to select the line
        terminator(s) recognized.

        """

        self._checkClosed()
        self._checkReadable()

        chunksize = self._node.chunkshape[0]

        # XXX: check
        lsep = b'\n'
        lseplen = len(lsep)

        # Set the remaining bytes to read to the specified size.
        remsize = limit

        partial = []
        finished = False

        while not finished:
            # Read a string limited by the remaining number of bytes.
            if limit <= 0:
                ibuff = self.read(chunksize)
            else:
                ibuff = self.read(min(remsize, chunksize))
            ibufflen = len(ibuff)
            remsize -= ibufflen

            if ibufflen >= lseplen:
                # Separator fits, look for EOL string.
                eolindex = ibuff.find(lsep)
            elif ibufflen == 0:
                # EOF was immediately reached.
                finished = True
                continue
            else:  # ibufflen < lseplen
                # EOF was hit and separator does not fit. ;)
                partial.append(ibuff)
                finished = True
                continue

            if eolindex >= 0:
                # Found an EOL. If there are trailing characters,
                # cut the input buffer and seek back;
                # else add the whole input buffer.
                trailing = ibufflen - lseplen - eolindex  # Bytes beyond EOL.
                if trailing > 0:
                    obuff = ibuff[:-trailing]
                    self.seek(-trailing, 1)
                    remsize += trailing
                else:
                    obuff = ibuff
                finished = True
            elif lseplen > 1 and (limit <= 0 or remsize > 0):
                # Seek back a little since the end of the read string
                # may have fallen in the middle of the line separator.
                obuff = ibuff[:-lseplen + 1]
                self.seek(-lseplen + 1, 1)
                remsize += lseplen - 1
            else:  # eolindex<0 and (lseplen<=1 or (limit>0 and remsize<=0))
                # Did not find an EOL, add the whole input buffer.
                obuff = ibuff

            # Append (maybe cut) buffer.
            partial.append(obuff)

            # If a limit has been specified and the remaining count
            # reaches zero, the reading is finished.
            if limit > 0 and remsize <= 0:
                finished = True

        return b''.join(partial)

    #def write(self, b: bytes) -> int:
    def write(self, b):
        """Write the given buffer to the IO stream.

        Returns the number of bytes written, which may be less than
        len(b).

        """

        self._checkClosed()
        self._checkWritable()

        if isinstance(b, unicode):
            raise TypeError("can't write str to binary stream")

        n = len(b)
        if n == 0:
            return 0

        pos = self._pos

        # Is the pointer beyond the real end of data?
        end2off = pos - self._node.nrows
        if end2off > 0:
            # Zero-fill the gap between the end of data and the pointer.
            self._append_zeros(end2off)

        # Append data.
        self._node.append(
            np.ndarray(buffer=b, dtype=self._vtype, shape=self._vshape(n)))

        self._pos += n

        return n

    def _checkClosed(self):
        """Checks if file node is open.

        Checks whether the file node is open or has been closed. In the
        second case, a ValueError is raised. If the host PyTables has
        been closed, ValueError is also raised.

        """

        super(RawPyTablesIO, self)._checkClosed()
        if getattr(self._node, '_v_file', None) is None:
            raise ValueError("host PyTables file is already closed!")

    def _check_node(self, node):
        if not isinstance(node, tables.EArray):
            raise TypeError('the "node" parameter should be a tables.EArray')
        if not isinstance(node.atom, tables.UInt8Atom):
            raise TypeError('only nodes with atom "UInt8Atom" are allowed')

    def _check_mode(self, mode):
        if not isinstance(mode, str):
            raise TypeError("invalid mode: %r" % mode)

        modes = set(mode)
        if modes - set("arwb+tU") or len(mode) > len(modes):
            raise ValueError("invalid mode: %r" % mode)

        reading = "r" in modes
        writing = "w" in modes
        appending = "a" in modes
        #updating = "+" in modes
        text = "t" in modes
        binary = "b" in modes

        if "U" in modes:
            if writing or appending:
                raise ValueError("can't use U and writing mode at once")
            reading = True

        if text and binary:
            raise ValueError("can't have text and binary mode at once")

        if reading + writing + appending > 1:
            raise ValueError("can't have read/write/append mode at once")

        if not (reading or writing or appending):
            raise ValueError("must have exactly one of read/write/append mode")

    def _cross_check_mode(self, mode, h5filemode):
        # XXX: check
        #readable = bool('r' in mode or '+' in mode)
        #h5readable = bool('r' in h5filemode or '+' in h5filemode)
        #
        #if readable and not h5readable:
        #    raise ValueError("RawPyTablesIO can't be open in read mode if "
        #                     "the underlying hdf5 file is not readable")

        writable = bool('w' in mode or 'a' in mode or '+' in mode)
        h5writable = bool('w' in h5filemode or 'a' in h5filemode or
                          '+' in h5filemode)

        if writable and not h5writable:
            raise ValueError("RawPyTablesIO can't be open in write mode if "
                             "the underlying hdf5 file is not writable")

    def _check_attributes(self, node):
        """Checks file node-specific attributes.

        Checks for the presence and validity
        of the system attributes 'NODE_TYPE' and 'NODE_TYPE_VERSION'
        in the specified PyTables node (leaf).
        ValueError is raised if an attribute is missing or incorrect.

        """

        attrs = node.attrs
        ltype = getattr(attrs, 'NODE_TYPE', None)
        ltypever = getattr(attrs, 'NODE_TYPE_VERSION', None)

        if ltype != NodeType:
            raise ValueError("invalid type of node object: %s" % (ltype,))
        if ltypever not in NodeTypeVersions:
            raise ValueError(
                "unsupported type version of node object: %s" % (ltypever,))

    _checkAttributes = previous_api(_check_attributes)

    def _append_zeros(self, size):
        """_append_zeros(size) -> None.  Appends a string of zeros.

        Appends a string of 'size' zeros to the array,
        without moving the file pointer.

        """

        # Appending an empty array would raise an error.
        if size == 0:
            return

        # XXX This may be redone to avoid a potentially large in-memory array.
        self._node.append(
            np.zeros(dtype=self._vtype, shape=self._vshape(size)))


class FileNodeMixin(object):
    """Mixin class for FileNode objects.

    It provides access to the attribute set of the node that becomes
    available via the attrs property. You can add attributes there, but
    try to avoid attribute names in all caps or starting with '_', since
    they may clash with internal attributes.

    """

    # The attribute set property methods.
    def _get_attrs(self):
        """Returns the attribute set of the file node."""

        #sefl._checkClosed()
        return self._node.attrs

    getAttrs = previous_api(_get_attrs)

    def _set_attrs(self, value):
        """set_attrs(string) -> None.  Raises ValueError."""

        raise ValueError("changing the whole attribute set is not allowed")

    setAttrs = previous_api(_set_attrs)

    def _del_attrs(self):
        """del_attrs() -> None.  Raises ValueError."""

        raise ValueError("deleting the whole attribute set is not allowed")

    delAttrs = previous_api(_del_attrs)

    # The attribute set property.
    attrs = property(
        _get_attrs, _set_attrs, _del_attrs,
        "A property pointing to the attribute set of the file node.")


class ROFileNode(FileNodeMixin, RawPyTablesIO):
    """Creates a new read-only file node.

    Creates a new read-only file node associated with the specified
    PyTables node, providing a standard Python file interface to it.
    The node has to have been created on a previous occasion
    using the new_node() function.

    The node used as storage is also made available via the read-only
    attribute node.  Please do not tamper with this object if it's
    avoidable, since you may break the operation of the file node object.

    The constructor is not intended to be used directly.
    Use the open_node() function in read-only mode ('r') instead.

    :Version 1:
        implements the file storage as a UInt8 uni-dimensional EArray.
    :Version 2:
        uses an UInt8 N vector EArray.

    .. versionchanged:: 3.0
       The offset attribute is no more available, please use seek/tell
       methods instead.

    .. versionchanged:: 3.0
       The line_separator property is no more available.
       The only line separator used for binary I/O is ``\\n``.

    """

    def __init__(self, node):
        RawPyTablesIO.__init__(self, node, 'r')
        self._checkReadable()

    @property
    def node(self):
        return self._node


class RAFileNode(FileNodeMixin, RawPyTablesIO):
    """Creates a new read-write file node.

    The first syntax opens the specified PyTables node, while the
    second one creates a new node in the specified PyTables file.
    In the second case, additional named arguments 'where' and 'name'
    must be passed to specify where the file node is to be created.
    Other named arguments such as 'title' and 'filters' may also be
    passed.  The special named argument 'expectedsize', indicating an
    estimate of the file size in bytes, may also be passed.

    Write access means reading as well as appending data is allowed.

    The node used as storage is also made available via the read-only
    attribute node.  Please do not tamper with this object if it's
    avoidable, since you may break the operation of the file node object.

    The constructor is not intended to be used directly.
    Use the new_node() or open_node() functions instead.

    :Version 1:
        implements the file storage as a UInt8 uni-dimensional EArray.
    :Version 2:
        uses an UInt8 N vector EArray.

    .. versionchanged:: 3.0
       The offset attribute is no more available, please use seek/tell
       methods instead.

    .. versionchanged:: 3.0
       The line_separator property is no more available.
       The only line separator used for binary I/O is ``\\n``.

    """

    # The atom representing a byte in the array, for each version.
    _byte_shape = [
        None,
        (0, 1),
        (0,),
    ]

    __allowed_init_kwargs = [
        'where', 'name', 'title', 'filters', 'expectedsize']

    def __init__(self, node, h5file, **kwargs):
        if node is not None:
            # Open an existing node and get its version.
            self._check_attributes(node)
            self._version = node.attrs.NODE_TYPE_VERSION
        elif h5file is not None:
            # Check for allowed keyword arguments,
            # to avoid unwanted arguments falling through to array constructor.
            for kwarg in kwargs:
                if kwarg not in self.__allowed_init_kwargs:
                    raise TypeError(
                        "%s keyword argument is not allowed" % repr(kwarg))

            # Turn 'expectedsize' into 'expectedrows'.
            if 'expectedsize' in kwargs:
                # These match since one byte is stored per row.
                expectedrows = kwargs['expectedsize']
                kwargs = kwargs.copy()
                del kwargs['expectedsize']
                kwargs['expectedrows'] = expectedrows

            # Create a new array in the specified PyTables file.
            self._version = NodeTypeVersions[-1]
            shape = self._byte_shape[self._version]
            node = h5file.create_earray(
                atom=tables.UInt8Atom(), shape=shape, **kwargs)

            # Set the node attributes, else remove the array itself.
            try:
                self._set_attributes(node)
            except RuntimeError:
                h5file.remove_node(kwargs['where'], kwargs['name'])
                raise

        RawPyTablesIO.__init__(self, node, 'a+')
        self._checkReadable()
        self._checkWritable()

    @property
    def node(self):
        return self._node

    def _set_attributes(self, node):
        """_set_attributes(node) -> None.  Adds file node-specific attributes.

        Sets the system attributes 'NODE_TYPE' and 'NODE_TYPE_VERSION'
        in the specified PyTables node (leaf).

        """

        attrs = node.attrs
        attrs.NODE_TYPE = NodeType
        attrs.NODE_TYPE_VERSION = NodeTypeVersions[-1]

    _setAttributes = previous_api(_set_attributes)


def new_node(h5file, **kwargs):
    """Creates a new file node object in the specified PyTables file object.

    Additional named arguments where and name must be passed to specify where
    the file node is to be created. Other named arguments such as title and
    filters may also be passed.

    The special named argument expectedsize, indicating an estimate of the
    file size in bytes, may also be passed. It returns the file node object.

    """

    return RAFileNode(None, h5file, **kwargs)


newNode = previous_api(new_node)


def open_node(node, mode='r'):
    """Opens an existing file node.

    Returns a file node object from the existing specified PyTables
    node. If mode is not specified or it is 'r', the file can only be
    read, and the pointer is positioned at the beginning of the file. If
    mode is 'a+', the file can be read and appended, and the pointer is
    positioned at the end of the file.

    """

    if mode == 'r':
        return ROFileNode(node)
    elif mode == 'a+':
        return RAFileNode(node, None)
    else:
        raise IOError("invalid mode: %s" % (mode,))


openNode = previous_api(open_node)


def save_to_filenode(h5file, filename, where, name=None, overwrite=False,
                     title="", filters=None):
    """Save a file's contents to a filenode inside a PyTables file.

    .. versionadded:: 3.2

    Parameters
    ----------
    h5file
      The PyTables file to be written to; can be either a string
      giving the file's location or a :class:`File` object.  If a file
      with name *h5file* already exists, it will be opened in
      mode ``a``.

    filename
      Path of the file which shall be stored within the PyTables file.

    where, name
      Location of the filenode where the data shall be stored.  If
      *name* is not given, and *where* is either a :class:`Group`
      object or a string ending on ``/``, the leaf name will be set to
      the file name of *filename*.

    overwrite
      Whether or not a possibly existing filenode of the specified
      name shall be overwritten.

    title
       A description for this node (it sets the ``TITLE`` HDF5
       attribute on disk).

    filters
       An instance of the :class:`Filters` class that provides
       information about the desired I/O filters to be applied
       during the life of this object.

    """
    # sanity checks
    if not os.access(filename, os.R_OK):
        raise IOError("The file '%s' could not be read" % filename)
    if isinstance(h5file, tables.file.File) and h5file.mode == "r":
        raise IOError("The file '%s' is opened read-only" % h5file.filename)

    # guess filenode's name if necessary
    if (name is None and (isinstance(where, tables.group.Group) or
                          (isinstance(where, string_types) and
                           where.endswith("/")))):
        name = os.path.split(filename)[1]

    new_h5file = not isinstance(h5file, tables.file.File)
    f = tables.File(h5file, "a") if new_h5file else h5file

    # check for already existing filenode
    try:
        n = f.get_node(where=where, name=name)
        if not overwrite:
            if new_h5file:
                f.close()
            raise IOError("Specified node already exists in file '%s'" %
                          f.filename)
    except tables.NoSuchNodeError:
        pass

    # read data from disk
    with open(filename, "rb") as fd:
        data = fd.read()

    if isinstance(where, string_types) and name is None:
        nodepath = where.split("/")
        where = "/" + "/".join(nodepath[:-1])
        name = nodepath[-1]

    # remove existing filenode if present
    try:
        f.remove_node(where=where, name=name)
    except tables.NoSuchNodeError:
        pass

    # write file's contents to filenode
    fnode = new_node(f, where=where, name=name, title=title, filters=filters)
    fnode.write(data)
    fnode.close()

    # cleanup
    if new_h5file:
        f.close()


def read_from_filenode(h5file, filename, where, name=None, overwrite=False,
                       create_target=False):
    """Read a filenode from a PyTables file and write its contents to a file.

    .. versionadded:: 3.2

    Parameters
    ----------
    h5file
      The PyTables file to be read from; can be either a string
      giving the file's location or a :class:`File` object.

    filename
      Path of the file where the contents of the filenode shall be
      written to.  If *filename* points to a directory or ends with
      ``/`` (``\`` on Windows), the filename will be set to the *name*
      attribute of the read filenode.

    where, name
      Location of the filenode where the data shall be read from.

    overwrite
      Whether or not a possibly existing file of the specified
      *filename* shall be overwritten.

    create_target
      Whether or not the folder hierarchy needed to accomodate the
      given target ``filename`` will be created.

    """
    new_h5file = not isinstance(h5file, tables.file.File)
    f = tables.File(h5file, "r") if new_h5file else h5file
    fnode = open_node(f.get_node(where=where, name=name))

    # guess output filename if necessary
    if os.path.isdir(filename) or filename.endswith(os.path.sep):
        filename = os.path.join(filename, fnode.node.name)

    if os.access(filename, os.R_OK) and not overwrite:
        if new_h5file:
            f.close()
        raise IOError("The file '%s' already exists" % filename)

    # create folder hierarchy if necessary
    if create_target and not os.path.isdir(os.path.split(filename)[0]):
        os.makedirs(os.path.split(filename)[0])

    if not os.access(os.path.split(filename)[0], os.W_OK):
        if new_h5file:
            f.close()
        raise IOError("The file '%s' cannot be written to" % filename)

    # read data from filenode
    data = fnode.read()
    fnode.close()

    # store data to file
    with open(filename, "wb") as fd:
        fd.write(data)

    # cleanup
    del data
    if new_h5file:
        f.close()


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = test_filenode
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: October 2, 2004
# Author:  Ivan Vilata i Balaguer - reverse:net.selidor@ivan
#
# $Id$
#
########################################################################

"""Unit test for the filenode module."""

import unittest
import tempfile
import os
import shutil
import warnings

import tables
from tables.nodes import filenode
from tables.tests import common


class NewFileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests creating a new file node with the new_node() function."

    def test00_NewFile(self):
        "Creation of a brand new file node."

        try:
            fnode = filenode.new_node(self.h5file, where='/', name='test')
            node = self.h5file.get_node('/test')
        except LookupError:
            self.fail("filenode.new_node() failed to create a new node.")
        else:
            self.assertEqual(
                fnode.node, node,
                "filenode.new_node() created a node in the wrong place.")

    def test01_NewFileTooFewArgs(self):
        "Creation of a new file node without arguments for node creation."

        self.assertRaises(TypeError, filenode.new_node, self.h5file)

    def test02_NewFileWithExpectedSize(self):
        "Creation of a new file node with 'expectedsize' argument."

        try:
            filenode.new_node(
                self.h5file, where='/', name='test', expectedsize=100000)
        except TypeError:
            self.fail("\
filenode.new_node() failed to accept 'expectedsize' argument.")

    def test03_NewFileWithExpectedRows(self):
        "Creation of a new file node with illegal 'expectedrows' argument."

        self.assertRaises(
            TypeError, filenode.new_node,
            self.h5file, where='/', name='test', expectedrows=100000)


class ClosedFileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests calling several methods on a closed file."

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, temporary HDF5 file with a '/test' node
          * 'fnode', the closed file node in '/test'
        """
        super(ClosedFileTestCase, self).setUp()
        self.fnode = filenode.new_node(self.h5file, where='/', name='test')
        self.fnode.close()

    def tearDown(self):
        """tearDown() -> None

        Closes 'h5file'; removes 'h5fname'.
        """
        self.fnode = None
        super(ClosedFileTestCase, self).tearDown()

    # All these tests mey seem odd, but Python (2.3) files
    # do test whether the file is not closed regardless of their mode.
    def test00_Close(self):
        "Closing a closed file."

        try:
            self.fnode.close()
        except ValueError:
            self.fail("Could not close an already closed file.")

    def test01_Flush(self):
        "Flushing a closed file."

        self.assertRaises(ValueError, self.fnode.flush)

    def test02_Next(self):
        "Getting the next line of a closed file."

        self.assertRaises(ValueError, self.fnode.next)

    def test03_Read(self):
        "Reading a closed file."

        self.assertRaises(ValueError, self.fnode.read)

    def test04_Readline(self):
        "Reading a line from a closed file."

        self.assertRaises(ValueError, self.fnode.readline)

    def test05_Readlines(self):
        "Reading lines from a closed file."

        self.assertRaises(ValueError, self.fnode.readlines)

    def test06_Seek(self):
        "Seeking a closed file."

        self.assertRaises(ValueError, self.fnode.seek, 0)

    def test07_Tell(self):
        "Getting the pointer position in a closed file."

        self.assertRaises(ValueError, self.fnode.tell)

    def test08_Truncate(self):
        "Truncating a closed file."

        self.assertRaises(ValueError, self.fnode.truncate)

    def test09_Write(self):
        "Writing a closed file."

        self.assertRaises(ValueError, self.fnode.write, b'foo')

    def test10_Writelines(self):
        "Writing lines to a closed file."

        self.assertRaises(ValueError, self.fnode.writelines, [b'foo\n'])


def copyFileToFile(srcfile, dstfile, blocksize=4096):
    """copyFileToFile(srcfile, dstfile[, blocksize]) -> None

    Copies a readable opened file 'srcfile' to a writable opened file 'destfile'
    in blocks of 'blocksize' bytes (4 KiB by default).
    """

    data = srcfile.read(blocksize)
    while len(data) > 0:
        dstfile.write(data)
        data = srcfile.read(blocksize)


class WriteFileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests writing, seeking and truncating a new file node."

    datafname = 'test_filenode.dat'

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, temporary HDF5 file with a '/test' node
          * 'fnode', the writable file node in '/test'
        """
        super(WriteFileTestCase, self).setUp()
        self.fnode = filenode.new_node(self.h5file, where='/', name='test')
        self.datafname = self._testFilename(self.datafname)

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode' and 'h5file'; removes 'h5fname'.
        """
        self.fnode.close()
        self.fnode = None
        super(WriteFileTestCase, self).tearDown()

    def test00_WriteFile(self):
        "Writing a whole file node."

        datafile = open(self.datafname, 'rb')
        try:
            copyFileToFile(datafile, self.fnode)
        finally:
            datafile.close()

    def test01_SeekFile(self):
        "Seeking and writing file node."

        self.fnode.write(b'0123')
        self.fnode.seek(8)
        self.fnode.write(b'4567')
        self.fnode.seek(3)
        data = self.fnode.read(6)
        self.assertEqual(
            data, b'3\0\0\0\0'b'4',
            "Gap caused by forward seek was not properly filled.")

        self.fnode.seek(0)
        self.fnode.write(b'test')

        self.fnode.seek(0)
        data = self.fnode.read(4)
        self.assertNotEqual(
            data, b'test', "Data was overwritten instead of appended.")

        self.fnode.seek(-4, 2)
        data = self.fnode.read(4)
        self.assertEqual(data, b'test', "Written data was not appended.")

        self.fnode.seek(0, 2)
        oldendoff = self.fnode.tell()
        self.fnode.seek(-2, 2)
        self.fnode.write(b'test')
        newendoff = self.fnode.tell()
        self.assertEqual(
            newendoff, oldendoff - 2 + 4,
            "Pointer was not correctly moved on append.")

    def test02_TruncateFile(self):
        "Truncating a file node."

        self.fnode.write(b'test')

        self.fnode.seek(2)
        self.assertRaises(IOError, self.fnode.truncate)

        self.fnode.seek(6)
        self.fnode.truncate()
        self.fnode.seek(0)
        data = self.fnode.read()
        self.assertEqual(data,
                         b'test\0\0', "File was not grown to the current offset.")

        self.fnode.truncate(8)
        self.fnode.seek(0)
        data = self.fnode.read()
        self.assertEqual(data,
                         b'test\0\0\0\0', "File was not grown to an absolute size.")


class OpenFileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests opening an existing file node for reading and writing."

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, temporary HDF5 file with a '/test' node
        """
        super(OpenFileTestCase, self).setUp()
        fnode = filenode.new_node(self.h5file, where='/', name='test')
        fnode.close()

    def test00_OpenFileRead(self):
        "Opening an existing file node for reading."

        node = self.h5file.get_node('/test')
        fnode = filenode.open_node(node)
        self.assertEqual(
            fnode.node, node, "filenode.open_node() opened the wrong node.")
        self.assertEqual(
            fnode.mode, 'r',
            "File was opened with an invalid mode %s." % repr(fnode.mode))
        self.assertEqual(
            fnode.tell(), 0L,
            "Pointer is not positioned at the beginning of the file.")
        fnode.close()

    def test01_OpenFileReadAppend(self):
        "Opening an existing file node for reading and appending."

        node = self.h5file.get_node('/test')
        fnode = filenode.open_node(node, 'a+')
        self.assertEqual(
            fnode.node, node, "filenode.open_node() opened the wrong node.")
        self.assertEqual(
            fnode.mode, 'a+',
            "File was opened with an invalid mode %s." % repr(fnode.mode))

        self.assertEqual(
            fnode.tell(), 0L,
            "Pointer is not positioned at the beginning of the file.")
        fnode.close()

    def test02_OpenFileInvalidMode(self):
        "Opening an existing file node with an invalid mode."

        self.assertRaises(
            IOError, filenode.open_node, self.h5file.get_node('/test'), 'w')


    # This no longer works since type and type version attributes
    # are now system attributes.  ivb(2004-12-29)
    # def test03_OpenFileNoAttrs(self):
    ##      "Opening a node with no type attributes."
    ##
    ##      node = self.h5file.get_node('/test')
    ##      self.h5file.del_node_attr('/test', '_type')
    ##      # Another way to get the same result is changing the value.
    ##      ##self.h5file.set_node_attr('/test', '_type', 'foobar')
    ##      self.assertRaises(ValueError, filenode.open_node, node)


class ReadFileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests reading from an existing file node."

    datafname = 'test_filenode.xbm'

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'datafile', the opened data file
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, temporary HDF5 file with a '/test' node
          * 'fnode', the readable file node in '/test', with data in it
        """

        self.datafname = self._testFilename(self.datafname)
        self.datafile = open(self.datafname, 'rb')

        super(ReadFileTestCase, self).setUp()

        fnode = filenode.new_node(self.h5file, where='/', name='test')
        copyFileToFile(self.datafile, fnode)
        fnode.close()

        self.datafile.seek(0)
        self.fnode = filenode.open_node(self.h5file.get_node('/test'))

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode', 'h5file' and 'datafile'; removes 'h5fname'.
        """

        self.fnode.close()
        self.fnode = None

        super(ReadFileTestCase, self).tearDown()

        self.datafile.close()
        self.datafile = None

    def test00_CompareFile(self):
        "Reading and comparing a whole file node."

        # Try to use hashlib (included from Python 2.5 on)
        try:
            import hashlib
            dfiledigest = hashlib.md5(self.datafile.read()).digest()
            fnodedigest = hashlib.md5(self.fnode.read()).digest()
        except ImportError:
            import md5
            dfiledigest = md5.new(self.datafile.read()).digest()
            fnodedigest = md5.new(self.fnode.read()).digest()

        self.assertEqual(
            dfiledigest, fnodedigest,
            "Data read from file node differs from that in the file on disk.")

    def test01_Write(self):
        "Writing on a read-only file."

        self.assertRaises(IOError, self.fnode.write, 'no way')

    def test02_UseAsImageFile(self):
        "Using a file node with Python Imaging Library."

        try:
            import Image

            Image.open(self.fnode)
        except ImportError:
            # PIL not available, nothing to do.
            pass
        except IOError:
            self.fail(
                "PIL was not able to create an image from the file node.")


class ReadlineTestCase(common.TempFileMixin, common.PyTablesTestCase):
    """
    Base class for text line-reading test cases.

    It provides a set of tests independent of the line separator string.
    Sub-classes must provide the 'line_separator' attribute.
    """

    def setUp(self):
        """
        This method sets the following instance attributes:

        * ``h5fname``: the name of the temporary HDF5 file.
        * ``h5file``: the writable, temporary HDF5 file with a ``/test`` node.
        * ``fnode``: the readable file node in ``/test``, with text in it.
        """

        super(ReadlineTestCase, self).setUp()

        linesep = self.line_separator

        # Fill the node file with some text.
        fnode = filenode.new_node(self.h5file, where='/', name='test')
        #fnode.line_separator = linesep
        fnode.write(linesep)
        data = 'short line%sshort line%s%s' % ((linesep.decode('ascii'),) * 3)
        data = data.encode('ascii')
        fnode.write(data)
        fnode.write(b'long line ' * 20 + linesep)
        fnode.write(b'unterminated')
        fnode.close()

        # Re-open it for reading.
        self.fnode = filenode.open_node(self.h5file.get_node('/test'))
        #self.fnode.line_separator = linesep

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode' and 'h5file'; removes 'h5fname'.
        """

        self.fnode.close()
        self.fnode = None
        super(ReadlineTestCase, self).tearDown()

    def test00_Readline(self):
        "Reading individual lines."

        linesep = self.line_separator

        line = self.fnode.readline()
        self.assertEqual(line, linesep)

        line = self.fnode.readline()  # 'short line' + linesep
        line = self.fnode.readline()
        self.assertEqual(line, b'short line' + linesep)
        line = self.fnode.readline()
        self.assertEqual(line, linesep)

        line = self.fnode.readline()
        self.assertEqual(line, b'long line ' * 20 + linesep)

        line = self.fnode.readline()
        self.assertEqual(line, b'unterminated')

        line = self.fnode.readline()
        self.assertEqual(line, b'')

        line = self.fnode.readline()
        self.assertEqual(line, b'')

    def test01_ReadlineSeek(self):
        "Reading individual lines and seeking back and forth."

        linesep = self.line_separator
        lseplen = len(linesep)

        self.fnode.readline()  # linesep
        self.fnode.readline()  # 'short line' + linesep

        self.fnode.seek(-(lseplen + 4), 1)
        line = self.fnode.readline()
        self.assertEqual(line, b'line' + linesep,
                         "Seeking back yielded different data.")

        self.fnode.seek(lseplen + 20, 1)  # Into the long line.
        line = self.fnode.readline()
        self.assertEqual(
            line[-(lseplen + 10):], b'long line ' + linesep,
            "Seeking forth yielded unexpected data.")

    def test02_Iterate(self):
        "Iterating over the lines."

        linesep = self.line_separator

        # Iterate to the end.
        for line in self.fnode:
            pass

        self.assertRaises(StopIteration, self.fnode.next)

        self.fnode.seek(0)

        line = next(self.fnode)
        self.assertEqual(line, linesep)

        line = next(self.fnode)
        self.assertEqual(line, b'short line' + linesep)

    def test03_Readlines(self):
        "Reading a list of lines."

        linesep = self.line_separator

        lines = self.fnode.readlines()
        self.assertEqual(lines, [
            linesep, b'short line' + linesep, b'short line' + linesep,
            linesep, b'long line ' * 20 + linesep, b'unterminated'])

    def test04_ReadlineSize(self):
        "Reading individual lines of limited size."

        linesep = self.line_separator
        lseplen = len(linesep)

        line = self.fnode.readline()  # linesep

        line = self.fnode.readline(lseplen + 20)
        self.assertEqual(line, b'short line' + linesep)

        line = self.fnode.readline(5)
        self.assertEqual(line, b'short')

        line = self.fnode.readline(lseplen + 20)
        self.assertEqual(line, b' line' + linesep)

        line = self.fnode.readline(lseplen)
        self.assertEqual(line, linesep)

        self.fnode.seek(-4, 2)
        line = self.fnode.readline(4)
        self.assertEqual(line, b'ated')

        self.fnode.seek(-4, 2)
        line = self.fnode.readline(20)
        self.assertEqual(line, b'ated')

    def test05_ReadlinesSize(self):
        "Reading a list of lines with a limited size."

        linesep = self.line_separator

        data = '%sshort line%sshort' % ((linesep.decode('ascii'),) * 2)
        data = data.encode('ascii')
        lines = self.fnode.readlines(len(data))
        #self.assertEqual(lines, [linesep, b'short line' + linesep, b'short'])
        #
        #line = self.fnode.readline()
        #self.assertEqual(line, b' line' + linesep)

        # NOTE: the test is relaxed because the *hint* parameter of
        # io.BaseIO.readlines controls the amout of read data in a coarse way
        self.assertEqual(len(lines), len(data.split(b'\n')))
        self.assertEqual(lines[:-1], [linesep, b'short line' + linesep])
        self.assertTrue(lines[-1].startswith(b'short'))


class MonoReadlineTestCase(ReadlineTestCase):
    "Tests reading one-byte-separated text lines from an existing file node."

    line_separator = b'\n'


#class MultiReadlineTestCase(ReadlineTestCase):
#    "Tests reading multibyte-separated text lines from an existing file node."
#
#    line_separator = b'<br/>'


#class LineSeparatorTestCase(common.TempFileMixin, common.PyTablesTestCase):
#    "Tests text line separator manipulation in a file node."
#
#    def setUp(self):
#        """setUp() -> None
#
#        This method sets the following instance attributes:
#          * 'h5fname', the name of the temporary HDF5 file
#          * 'h5file', the writable, temporary HDF5 file with a '/test' node
#          * 'fnode', the writable file node in '/test'
#        """
#        super(LineSeparatorTestCase, self).setUp()
#        self.fnode = filenode.new_node(self.h5file, where='/', name='test')
#
#    def tearDown(self):
#        """tearDown() -> None
#
#        Closes 'fnode' and 'h5file'; removes 'h5fname'.
#        """
#        self.fnode.close()
#        self.fnode = None
#        super(LineSeparatorTestCase, self).tearDown()
#
#    def test00_DefaultLineSeparator(self):
#        "Default line separator."
#
#        self.assertEqual(
#            self.fnode.line_separator, os.linesep.encode('ascii'),
#            "Default line separator does not match that in os.linesep.")
#
#    def test01_SetLineSeparator(self):
#        "Setting a valid line separator."
#
#        try:
#            self.fnode.line_separator = b'SEPARATOR'
#        except ValueError:
#            self.fail("Valid line separator was not accepted.")
#        else:
#            self.assertEqual(
#                self.fnode.line_separator, b'SEPARATOR',
#                "Line separator was not correctly set.")
#
#    def test02_SetInvalidLineSeparator(self):
#        "Setting an invalid line separator."
#
#        self.assertRaises(
#            ValueError, setattr, self.fnode, 'line_separator', b'')
#        self.assertRaises(
#            ValueError, setattr, self.fnode, 'line_separator', b'x' * 1024)
#        self.assertRaises(
#            TypeError, setattr, self.fnode, 'line_separator', u'x')


class AttrsTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests setting and getting file node attributes."

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, temporary HDF5 file with a '/test' node
          * 'fnode', the writable file node in '/test'
        """
        super(AttrsTestCase, self).setUp()
        self.fnode = filenode.new_node(self.h5file, where='/', name='test')

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode' and 'h5file'; removes 'h5fname'.
        """
        self.fnode.close()
        self.fnode = None
        super(AttrsTestCase, self).tearDown()

    # This no longer works since type and type version attributes
    # are now system attributes.  ivb(2004-12-29)
    # def test00_GetTypeAttr(self):
    ##      "Getting the type attribute of a file node."
    ##
    ##      self.assertEqual(
    ##              getattr(self.fnode.attrs, '_type', None), filenode.NodeType,
    ##              "File node has no '_type' attribute.")
    def test00_MangleTypeAttrs(self):
        "Mangling the type attributes on a file node."

        nodeType = getattr(self.fnode.attrs, 'NODE_TYPE', None)
        self.assertEqual(
            nodeType, filenode.NodeType,
            "File node does not have a valid 'NODE_TYPE' attribute.")

        nodeTypeVersion = getattr(self.fnode.attrs, 'NODE_TYPE_VERSION', None)
        self.assertTrue(
            nodeTypeVersion in filenode.NodeTypeVersions,
            "File node does not have a valid 'NODE_TYPE_VERSION' attribute.")

        # System attributes are now writable.  ivb(2004-12-30)
        # self.assertRaises(
        ##      AttributeError,
        ##      setattr, self.fnode.attrs, 'NODE_TYPE', 'foobar')
        # self.assertRaises(
        ##      AttributeError,
        ##      setattr, self.fnode.attrs, 'NODE_TYPE_VERSION', 'foobar')

        # System attributes are now removables.  F. Alted (2007-03-06)
#         self.assertRaises(
#                 AttributeError,
#                 delattr, self.fnode.attrs, 'NODE_TYPE')
#         self.assertRaises(
#                 AttributeError,
#                 delattr, self.fnode.attrs, 'NODE_TYPE_VERSION')

    # System attributes are now writable.  ivb(2004-12-30)
    # def test01_SetSystemAttr(self):
    ##      "Setting a system attribute on a file node."
    ##
    ##      self.assertRaises(
    # AttributeError, setattr, self.fnode.attrs, 'CLASS', 'foobar')
    def test02_SetGetDelUserAttr(self):
        "Setting a user attribute on a file node."

        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), None,
            "Inexistent attribute has a value that is not 'None'.")

        self.fnode.attrs.userAttr = 'foobar'
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), 'foobar',
            "User attribute was not correctly set.")

        self.fnode.attrs.userAttr = 'bazquux'
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), 'bazquux',
            "User attribute was not correctly changed.")

        del self.fnode.attrs.userAttr
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), None,
            "User attribute was not deleted.")
        # Another way is looking up the attribute in the attribute list.
        # if 'userAttr' in self.fnode.attrs._f_list():
        ##      self.fail("User attribute was not deleted.")

    def test03_AttrsOnClosedFile(self):
        "Accessing attributes on a closed file node."

        self.fnode.close()
        self.assertRaises(AttributeError, getattr, self.fnode, 'attrs')


class ClosedH5FileTestCase(common.TempFileMixin, common.PyTablesTestCase):
    "Tests accessing a file node in a closed PyTables file."

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the closed HDF5 file with a '/test' node
          * 'fnode', the writable file node in '/test'
        """
        super(ClosedH5FileTestCase, self).setUp()
        self.fnode = filenode.new_node(self.h5file, where='/', name='test')
        self.h5file.close()

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode'; removes 'h5fname'.
        """

        # ivilata:  We know that a UserWarning will be raised
        #   because the PyTables file has already been closed.
        #   However, we don't want it to pollute the test output.
        warnings.filterwarnings('ignore', category=UserWarning)
        try:
            self.fnode.close()
        except ValueError:
            pass
        finally:
            warnings.filterwarnings('default', category=UserWarning)

        self.fnode = None
        super(ClosedH5FileTestCase, self).tearDown()

    def test00_Write(self):
        "Writing to a file node in a closed PyTables file."

        self.assertRaises(ValueError, self.fnode.write, 'data')

    def test01_Attrs(self):
        "Accessing the attributes of a file node in a closed PyTables file."

        self.assertRaises(ValueError, getattr, self.fnode, 'attrs')


class OldVersionTestCase(common.PyTablesTestCase):
    """Base class for old version compatibility test cases.

    It provides some basic tests for file operations and attribute handling.
    Sub-classes must provide the 'oldversion' attribute
    and the 'oldh5fname' attribute.

    """

    def setUp(self):
        """
        This method sets the following instance attributes:

        * ``h5fname``: the name of the temporary HDF5 file.
        * ``h5file``: the writable, temporary HDF5 file with a ``/test`` node.
        * ``fnode``: the readable file node in ``/test``.
        """

        self.h5fname = tempfile.mktemp(suffix='.h5')

        self.oldh5fname = self._testFilename(self.oldh5fname)
        oldh5f = tables.open_file(self.oldh5fname)
        oldh5f.copy_file(self.h5fname)
        oldh5f.close()

        self.h5file = tables.open_file(
            self.h5fname, 'r+',
            title="Test for file node old version compatibility")
        self.fnode = filenode.open_node(self.h5file.root.test, 'a+')

    def tearDown(self):
        """Closes ``fnode`` and ``h5file``; removes ``h5fname``."""

        self.fnode.close()
        self.fnode = None
        self.h5file.close()
        self.h5file = None
        os.remove(self.h5fname)

    def test00_Read(self):
        "Reading an old version file node."

        #self.fnode.line_separator = '\n'

        line = self.fnode.readline()
        self.assertEqual(line, 'This is only\n')

        line = self.fnode.readline()
        self.assertEqual(line, 'a test file\n')

        line = self.fnode.readline()
        self.assertEqual(line, 'for FileNode version %d\n' % self.oldversion)

        line = self.fnode.readline()
        self.assertEqual(line, '')

        self.fnode.seek(0)
        line = self.fnode.readline()
        self.assertEqual(line, 'This is only\n')

    def test01_Write(self):
        "Writing an old version file node."

        #self.fnode.line_separator = '\n'

        self.fnode.write('foobar\n')
        self.fnode.seek(-7, 2)
        line = self.fnode.readline()
        self.assertEqual(line, 'foobar\n')

    def test02_Attributes(self):
        "Accessing attributes in an old version file node."

        self.fnode.attrs.userAttr = 'foobar'
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), 'foobar',
            "User attribute was not correctly set.")

        self.fnode.attrs.userAttr = 'bazquux'
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), 'bazquux',
            "User attribute was not correctly changed.")

        del self.fnode.attrs.userAttr
        self.assertEqual(
            getattr(self.fnode.attrs, 'userAttr', None), None,
            "User attribute was not deleted.")


class Version1TestCase(OldVersionTestCase):
    "Basic test for version 1 format compatibility."

    oldversion = 1
    oldh5fname = 'test_filenode_v1.h5'


class DirectReadWriteTestCase(common.TempFileMixin, common.PyTablesTestCase):

    datafname = 'test_filenode.dat'

    def setUp(self):
        """
        This method sets the following instance attributes:

        * ``h5fname``: the name of the temporary HDF5 file.
        * ``h5file``, the writable, temporary HDF5 file with a '/test' node
        * ``datafname``: the name of the data file to be stored in the
          temporary HDF5 file.
        * ``data``: the contents of the file ``datafname``
        * ``testfname``: the name of a temporary file to be written to.
        """

        super(DirectReadWriteTestCase, self).setUp()
        self.datafname = self._testFilename(self.datafname)
        self.testfname = tempfile.mktemp()
        self.testh5fname = tempfile.mktemp(suffix=".h5")
        with open(self.datafname, "rb") as fd:
            self.data = fd.read()
        self.testdir = tempfile.mkdtemp()

    def tearDown(self):
        """tearDown() -> None

        Closes 'fnode' and 'h5file'; removes 'h5fname'.
        """

        super(DirectReadWriteTestCase, self).tearDown()
        if os.access(self.testfname, os.R_OK):
            os.remove(self.testfname)
        if os.access(self.testh5fname, os.R_OK):
            os.remove(self.testh5fname)
        shutil.rmtree(self.testdir)

    def test01_WriteToFilename(self):
        # write contents of datafname to h5 testfile
        filenode.save_to_filenode(self.testh5fname, self.datafname, "/test1")
        # make sure writing to an existing node doesn't work ...
        self.assertRaises(IOError, filenode.save_to_filenode, self.testh5fname,
                          self.datafname, "/test1")
        # ... except if overwrite is True
        filenode.save_to_filenode(self.testh5fname, self.datafname, "/test1",
                                  overwrite=True)
        # write again, this time specifying a name
        filenode.save_to_filenode(self.testh5fname, self.datafname, "/",
                                  name="test2")
        # read from test h5file
        filenode.read_from_filenode(self.testh5fname, self.testfname, "/test1")
        # and compare result to what it should be
        with open(self.testfname, "rb") as fd:
            self.assertEqual(fd.read(), self.data)
        # make sure extracting to an existing file doesn't work ...
        self.assertRaises(IOError, filenode.read_from_filenode,
                          self.testh5fname, self.testfname, "/test1")
        # except overwrite is True.  And try reading with a name
        filenode.read_from_filenode(self.testh5fname, self.testfname, "/",
                                    name="test2", overwrite=True)
        # and compare to what it should be
        with open(self.testfname, "rb") as fd:
            self.assertEqual(fd.read(), self.data)
        # cleanup
        os.remove(self.testfname)
        os.remove(self.testh5fname)

    def test02_WriteToHDF5File(self):
        # write contents of datafname to h5 testfile
        filenode.save_to_filenode(self.h5file, self.datafname, "/test1")
        # make sure writing to an existing node doesn't work ...
        self.assertRaises(IOError, filenode.save_to_filenode, self.h5file,
                          self.datafname, "/test1")
        # ... except if overwrite is True
        filenode.save_to_filenode(self.h5file, self.datafname, "/test1",
                                  overwrite=True)
        # read from test h5file
        filenode.read_from_filenode(self.h5file, self.testfname, "/test1")
        # and compare result to what it should be
        with open(self.testfname, "rb") as fd:
            self.assertEqual(fd.read(), self.data)
        # make sure extracting to an existing file doesn't work ...
        self.assertRaises(IOError, filenode.read_from_filenode, self.h5file,
                          self.testfname, "/test1")
        # make sure the original h5file is still alive and kicking
        self.assertEqual(isinstance(self.h5file, tables.file.File), True)
        self.assertEqual(self.h5file.mode, "w")

    def test03_AutomaticNameGuessing(self):
        # write using the filename as node name
        filenode.save_to_filenode(self.testh5fname, self.datafname, "/")
        # and read again
        datafname = os.path.split(self.datafname)[1]
        filenode.read_from_filenode(self.testh5fname, self.testdir, "/",
                                    name=datafname)
        # test if the output file really has the expected name
        self.assertEqual(os.access(os.path.join(self.testdir, datafname),
                                   os.R_OK), True)
        # and compare result to what it should be
        with open(os.path.join(self.testdir, datafname), "rb") as fd:
            self.assertEqual(fd.read(), self.data)


#----------------------------------------------------------------------
def suite():
    """suite() -> test suite

    Returns a test suite consisting of all the test cases in the module.
    """

    theSuite = unittest.TestSuite()

    theSuite.addTest(unittest.makeSuite(NewFileTestCase))
    theSuite.addTest(unittest.makeSuite(ClosedFileTestCase))
    theSuite.addTest(unittest.makeSuite(WriteFileTestCase))
    theSuite.addTest(unittest.makeSuite(OpenFileTestCase))
    theSuite.addTest(unittest.makeSuite(ReadFileTestCase))
    theSuite.addTest(unittest.makeSuite(MonoReadlineTestCase))
    #theSuite.addTest(unittest.makeSuite(MultiReadlineTestCase))
    #theSuite.addTest(unittest.makeSuite(LineSeparatorTestCase))
    theSuite.addTest(unittest.makeSuite(AttrsTestCase))
    theSuite.addTest(unittest.makeSuite(ClosedH5FileTestCase))
    theSuite.addTest(unittest.makeSuite(DirectReadWriteTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = parameters
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: February 25, 2005
# Author:  Ivan Vilata - reverse:net.selidor@ivan
#
# $Id$
#
########################################################################

"""Parameters for PyTables."""

__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""

_KB = 1024
"""The size of a Kilobyte in bytes"""

_MB = 1024 * _KB
"""The size of a Megabyte in bytes"""

# Tunable parameters
# ==================
# Be careful when touching these!

# Parameters for different internal caches
# ----------------------------------------

BOUNDS_MAX_SIZE = 1 * _MB
"""The maximum size for bounds values cached during index lookups."""

BOUNDS_MAX_SLOTS = 4 * _KB
"""The maximum number of slots for the BOUNDS cache."""

ITERSEQ_MAX_ELEMENTS = 1 * _KB
"""The maximum number of iterator elements cached in data lookups."""

ITERSEQ_MAX_SIZE = 1 * _MB
"""The maximum space that will take ITERSEQ cache (in bytes)."""

ITERSEQ_MAX_SLOTS = 128
"""The maximum number of slots in ITERSEQ cache."""

LIMBOUNDS_MAX_SIZE = 256 * _KB
"""The maximum size for the query limits (for example, ``(lim1, lim2)``
in conditions like ``lim1 <= col < lim2``) cached during index lookups
(in bytes)."""

LIMBOUNDS_MAX_SLOTS = 128
"""The maximum number of slots for LIMBOUNDS cache."""

TABLE_MAX_SIZE = 1 * _MB
"""The maximum size for table chunks cached during index queries."""

SORTED_MAX_SIZE = 1 * _MB
"""The maximum size for sorted values cached during index lookups."""

SORTEDLR_MAX_SIZE = 8 * _MB
"""The maximum size for chunks in last row cached in index lookups (in
bytes)."""

SORTEDLR_MAX_SLOTS = 1 * _KB
"""The maximum number of chunks for SORTEDLR cache."""


# Parameters for general cache behaviour
# --------------------------------------
#
# The next parameters will not be effective if passed to the
# `open_file()` function (so, they can only be changed in a *global*
# way).  You can change them in the file, but this is strongly
# discouraged unless you know well what you are doing.

DISABLE_EVERY_CYCLES = 10
"""The number of cycles in which a cache will be forced to be disabled
if the hit ratio is lower than the LOWEST_HIT_RATIO (see below).  This
value should provide time enough to check whether the cache is being
efficient or not."""

ENABLE_EVERY_CYCLES = 50
"""The number of cycles in which a cache will be forced to be
(re-)enabled, irregardingly of the hit ratio. This will provide a chance
for checking if we are in a better scenario for doing caching again."""

LOWEST_HIT_RATIO = 0.6
"""The minimum acceptable hit ratio for a cache to avoid disabling (and
freeing) it."""


# Tunable parameters
# ==================
# Be careful when touching these!

# Recommended maximum values
# --------------------------

# Following are the recommended values for several limits.  However,
# these limits are somewhat arbitrary and can be increased if you have
# enough resources.

MAX_COLUMNS = 512
"""Maximum number of columns in :class:`tables.Table` objects before a
:exc:`tables.PerformanceWarning` is issued.  This limit is somewhat
arbitrary and can be increased.
"""

MAX_NODE_ATTRS = 4 * _KB
"""Maximum allowed number of attributes in a node."""

MAX_GROUP_WIDTH = 16 * _KB
"""Maximum allowed number of children hanging from a group."""

MAX_TREE_DEPTH = 2 * _KB
"""Maximum depth in object tree allowed."""

MAX_UNDO_PATH_LENGTH = 10 * _KB
"""Maximum length of paths allowed in undo/redo operations."""


# Cache limits
# ------------

COND_CACHE_SLOTS = 128
"""Maximum number of conditions for table queries to be kept in memory."""

CHUNK_CACHE_NELMTS = 521
"""Number of elements for HDF5 chunk cache."""

CHUNK_CACHE_PREEMPT = 0.0
"""Chunk preemption policy.  This value should be between 0 and 1
inclusive and indicates how much chunks that have been fully read are
favored for preemption. A value of zero means fully read chunks are
treated no differently than other chunks (the preemption is strictly
LRU) while a value of one means fully read chunks are always preempted
before other chunks."""

CHUNK_CACHE_SIZE = 2 * _MB
"""Size (in bytes) for HDF5 chunk cache."""

# Size for new metadata cache system
METADATA_CACHE_SIZE = 1 * _MB  # 1 MB is the default for HDF5
"""Size (in bytes) of the HDF5 metadata cache."""


# NODE_CACHE_SLOTS tells the number of nodes that fits in the cache.
#
# There are several forces driving the election of this number:
# 1.- As more nodes, better chances to re-use nodes
#     --> better performance
# 2.- As more nodes, the re-ordering of the LRU cache takes more time
#     --> less performance
# 3.- As more nodes, the memory needs for PyTables grows, specially for table
#     writings (that could take double of memory than table reads!).
#
# The default value here is quite conservative. If you have a system
# with tons of memory, and if you are touching regularly a very large
# number of leaves, try increasing this value and see if it fits better
# for you. Please report back your feedback.
NODE_CACHE_SLOTS = 64
"""Maximum number of nodes to be kept in the metadata cache.

It is the number of nodes to be kept in the metadata cache. Least recently
used nodes are unloaded from memory when this number of loaded nodes is
reached. To load a node again, simply access it as usual.
Nodes referenced by user variables and, in general, all nodes that are still
open are registered in the node manager and can be quickly accessed even
if they are not in the cache.

Negative value means that all the touched nodes will be kept in an
internal dictionary.  This is the faster way to load/retrieve nodes.
However, and in order to avoid a large memory comsumption, the user will
be warned when the number of loaded nodes will reach the
``-NODE_CACHE_SLOTS`` value.

Finally, a value of zero means that any cache mechanism is disabled.
"""


# Parameters for the I/O buffer in `Leaf` objects
# -----------------------------------------------

IO_BUFFER_SIZE = 1 * _MB
"""The PyTables internal buffer size for I/O purposes.  Should not
exceed the amount of highest level cache size in your CPU."""

BUFFER_TIMES = 100
"""The maximum buffersize/rowsize ratio before issuing a
:exc:`tables.PerformanceWarning`."""


# Miscellaneous
# -------------

EXPECTED_ROWS_EARRAY = 1000
"""Default expected number of rows for :class:`EArray` objects."""

EXPECTED_ROWS_VLARRAY = 1000
"""Default expected number of rows for :class:`VLArray` objects.

.. versionadded:: 3.0

"""

EXPECTED_ROWS_TABLE = 10000
"""Default expected number of rows for :class:`Table` objects."""

PYTABLES_SYS_ATTRS = True
"""Set this to ``False`` if you don't want to create PyTables system
attributes in datasets.  Also, if set to ``False`` the possible existing
system attributes are not considered for guessing the class of the node
during its loading from disk (this work is delegated to the PyTables'
class discoverer function for general HDF5 files)."""

MAX_NUMEXPR_THREADS = None
"""The maximum number of threads that PyTables should use internally in
Numexpr.  If `None`, it is automatically set to the number of cores in
your machine. In general, it is a good idea to set this to the number of
cores in your machine or, when your machine has many of them (e.g. > 4),
perhaps one less than this."""

MAX_BLOSC_THREADS = None
"""The maximum number of threads that PyTables should use internally in
Blosc.  If `None`, it is automatically set to the number of cores in
your machine. In general, it is a good idea to set this to the number of
cores in your machine or, when your machine has many of them (e.g. > 4),
perhaps one less than this."""

USER_BLOCK_SIZE = 0
"""Sets the user block size of a file.

The default user block size is 0; it may be set to any power of 2 equal
to 512 or greater (512, 1024, 2048, etc.).

.. versionadded:: 3.0

"""


# HDF5 driver management
# ----------------------
DRIVER = None
"""The HDF5 driver that should be used for reading/writing to the file.

Following drivers are supported:

    * H5FD_SEC2: this driver uses POSIX file-system functions like read
      and write to perform I/O to a single, permanent file on local
      disk with no system buffering.
      This driver is POSIX-compliant and is the default file driver for
      all systems.

    * H5FD_DIRECT: this is the H5FD_SEC2 driver except data is written
      to or read from the file synchronously without being cached by
      the system.

    * H5FD_WINDOWS: this driver was modified in HDF5-1.8.8 to be a
      wrapper of the POSIX driver, H5FD_SEC2. This change should not
      affect user applications.

    * H5FD_STDIO: this driver uses functions from the standard C
      stdio.h to perform I/O to a single, permanent file on local disk
      with additional system buffering.

    * H5FD_CORE: with this driver, an application can work with a file
      in memory for faster reads and writes. File contents are kept in
      memory until the file is closed. At closing, the memory version
      of the file can be written back to disk or abandoned.

    * H5FD_SPLIT: this file driver splits a file into two parts.
      One part stores metadata, and the other part stores raw data.
      This splitting a file into two parts is a limited case of the
      Multi driver.

The following drivers are not currently supported:

    * H5FD_LOG: this is the H5FD_SEC2 driver with logging capabilities.

    * H5FD_FAMILY: with this driver, the HDF5 files address space is
      partitioned into pieces and sent to separate storage files using
      an underlying driver of the users choice.
      This driver is for systems that do not support files larger than
      2 gigabytes.

    * H5FD_MULTI: with this driver, data can be stored in multiple
      files according to the type of the data. I/O might work better if
      data is stored in separate files based on the type of data.
      The Split driver is a special case of this driver.

    * H5FD_MPIO: this is the standard HDF5 file driver for parallel
      file systems. This driver uses the MPI standard for both
      communication and file I/O.

    * H5FD_MPIPOSIX: this parallel file system driver uses MPI for
      communication and POSIX file-system calls for file I/O.

    * H5FD_STREAM: this driver is no longer available.

.. seealso:: the `Drivers section`_ of the `HDF5 User's Guide`_ for
   more information.

.. note::

    not all supported drivers are always available. For example the
    H5FD_WINDOWS driver is not available on non Windows platforms.

    If the user try to use a driver that is not available on the target
    platform a :exc:`RuntimeError` is raised.

.. versionadded:: 3.0

.. _`Drivers section`:
    http://www.hdfgroup.org/HDF5/doc/UG/08_TheFile.html#Drivers
.. _`HDF5 User's Guide`: http://www.hdfgroup.org/HDF5/doc/UG/index.html

"""

DRIVER_DIRECT_ALIGNMENT = 0
"""Specifies the required alignment boundary in memory.

A value of 0 (zero) means to use HDF5 Librarys default value.

.. versionadded:: 3.0

"""

DRIVER_DIRECT_BLOCK_SIZE = 0
"""Specifies the file system block size.

A value of 0 (zero) means to use HDF5 Librarys default value of 4KB.

.. versionadded:: 3.0

"""

DRIVER_DIRECT_CBUF_SIZE = 0
"""Specifies the copy buffer size.

A value of 0 (zero) means to use HDF5 Librarys default value.

.. versionadded:: 3.0

"""

# DRIVER_LOG_FLAGS = 0x0001ffff
#"""Flags specifying the types of logging activity.
#
#.. versionadded:: 3.0
#
#.. seeealso::
#    http://www.hdfgroup.org/HDF5/doc/RM/RM_H5P.html#Property-SetFaplLog
#
#"""
#
# DRIVER_LOG_BUF_SIZE = 4 * _KB
#"""The size of the logging buffers, in bytes.
#
# One buffer of size DRIVER_LOG_BUF_SIZE will be created for each of
# H5FD_LOG_FILE_READ, H5FD_LOG_FILE_WRITE and H5FD_LOG_FLAVOR when those
# flags are set; these buffers will not grow as the file increases in
# size.
#
#.. versionadded:: 3.0
#
#"""

DRIVER_CORE_INCREMENT = 64 * _KB
"""Core driver memory increment.

Specifies the increment by which allocated memory is to be increased
each time more memory is required.

.. versionadded:: 3.0

"""

DRIVER_CORE_BACKING_STORE = 1
"""Enables backing store for the core driver.

With the H5FD_CORE driver, if the DRIVER_CORE_BACKING_STORE is set
to 1 (True), the file contents are flushed to a file with the same name
as this core file when the file is closed or access to the file is
terminated in memory.

The application is allowed to open an existing file with H5FD_CORE
driver. In that case, if the DRIVER_CORE_BACKING_STORE is set to 1 and
the flags for :func:`tables.open_file` is set to H5F_ACC_RDWR, any change
to the file contents are saved to the file when the file is closed.
If backing_store is set to 0 and the flags for :func:`tables.open_file`
is set to H5F_ACC_RDWR, any change to the file contents will be lost
when the file is closed. If the flags for :func:`tables.open_file` is
set to H5F_ACC_RDONLY, no change to the file is allowed either in
memory or on file.

.. versionadded:: 3.0

"""

DRIVER_CORE_IMAGE = None
"""String containing an HDF5 file image.

If this oprion is passed to the :func:`tables.open_file` function then the
returned file object is set up using the specified image.

A file image can be retrieved from an existing (and opened) file object
using the :meth:`tables.File.get_file_image` method.

.. note:: requires HDF5 >= 1.8.9.

.. versionadded:: 3.0

"""

DRIVER_SPLIT_META_EXT = '-m.h5'
"""The extension for the metadata file used by the H5FD_SPLIT driver.

If this option is passed to the :func:`tables.openFile` function along
with driver='H5FD_SPLIT', the extension is appended to the name passed
as the first parameter to form the name of the metadata file. If the
string '%s' is used in the extension, the metadata file name is formed
by replacing '%s' with the name passed as the first parameter instead.

.. versionadded:: 3.1

"""

DRIVER_SPLIT_RAW_EXT = '-r.h5'
"""The extension for the raw data file used by the H5FD_SPLIT driver.

If this option is passed to the :func:`tables.openFile` function along
with driver='H5FD_SPLIT', the extension is appended to the name passed
as the first parameter to form the name of the raw data file. If the
string '%s' is used in the extension, the raw data file name is formed
by replacing '%s' with the name passed as the first parameter instead.

.. versionadded:: 3.1

"""


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = path
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: January 15, 2007
# Author:  Ivan Vilata i Balaguer - ivan at selidor dot net
#
# $Id$
#
########################################################################

"""Functionality related with node paths in a PyTables file.

Variables
=========

`__docformat`__
    The format of documentation strings in this module.

"""

# Imports
# =======
import re
import warnings
import keyword

from tables.exceptions import NaturalNameWarning
from tables._past import previous_api

# Public variables
# ================
__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


# Private variables
# =================
_python_id_re = re.compile('^[a-zA-Z_][a-zA-Z0-9_]*$')
"""Python identifier regular expression."""

_reserved_id_re = re.compile('^_[cfgv]_')
"""PyTables reserved identifier regular expression.

- c: class variables
- f: class public methods
- g: class private methods
- v: instance variables
"""

_hidden_name_re = re.compile('^_[pi]_')
"""Nodes with a name *matching* this expression are considered hidden.

For instance, ``name`` whould be visible while ``_i_name`` would not.
"""

_hidden_path_re = re.compile('/_[pi]_')
"""Nodes with a path *containing* this expression are considered hidden.

For instance, a node with a pathname like ``/a/b/c`` would be visible
while nodes with pathnames like ``/a/c/_i_x`` or ``/a/_p_x/y`` would
not.
"""


# Public functions
# ================
def check_name_validity(name):
    """Check the validity of the `name` of an object.

    If the name is not valid, a ``ValueError`` is raised.  If it is
    valid but it can not be used with natural naming, a
    `NaturalNameWarning` is issued.

    """

    warnInfo = (
        "you will not be able to use natural naming to access this object; "
        "using ``getattr()`` will still work, though")

    if not isinstance(name, basestring):  # Python >= 2.3
        raise TypeError("object name is not a string: %r" % (name,))

    # Check whether `name` is a valid HDF5 name.
    # http://hdfgroup.org/HDF5/doc/UG/03_Model.html#Structure
    if name == '':
        raise ValueError("the empty string is not allowed as an object name")
    if name == '.':
        raise ValueError("``.`` is not allowed as an object name")
    if '/' in name:
        raise ValueError("the ``/`` character is not allowed "
                         "in object names: %r" % name)

    # Check whether `name` is a valid Python identifier.
    if not _python_id_re.match(name):
        warnings.warn("object name is not a valid Python identifier: %r; "
                      "it does not match the pattern ``%s``; %s"
                      % (name, _python_id_re.pattern, warnInfo),
                      NaturalNameWarning)
        return

    # However, Python identifiers and keywords have the same form.
    if keyword.iskeyword(name):
        warnings.warn("object name is a Python keyword: %r; %s"
                      % (name, warnInfo), NaturalNameWarning)
        return

    # Still, names starting with reserved prefixes are not allowed.
    if _reserved_id_re.match(name):
        raise ValueError("object name starts with a reserved prefix: %r; "
                         "it matches the pattern ``%s``"
                         % (name, _reserved_id_re.pattern))

    # ``__members__`` is the only exception to that rule.
    if name == '__members__':
        raise ValueError("``__members__`` is not allowed as an object name")

checkNameValidity = previous_api(check_name_validity)


def join_path(parentpath, name):
    """Join a *canonical* `parentpath` with a *non-empty* `name`.

    .. versionchanged:: 3.0
       The *parentPath* parameter has been renamed into *parentpath*.

    >>> join_path('/', 'foo')
    '/foo'
    >>> join_path('/foo', 'bar')
    '/foo/bar'
    >>> join_path('/foo', '/foo2/bar')
    '/foo/foo2/bar'
    >>> join_path('/foo', '/')
    '/foo'

    """

    if name.startswith('./'):  # Support relative paths (mainly for links)
        name = name[2:]
    if parentpath == '/' and name.startswith('/'):
        pstr = '%s' % name
    elif parentpath == '/' or name.startswith('/'):
        pstr = '%s%s' % (parentpath, name)
    else:
        pstr = '%s/%s' % (parentpath, name)
    if pstr.endswith('/'):
        pstr = pstr[:-1]
    return pstr

joinPath = previous_api(join_path)


def split_path(path):
    """Split a *canonical* `path` into a parent path and a node name.

    The result is returned as a tuple.  The parent path does not
    include a trailing slash.

    >>> split_path('/')
    ('/', '')
    >>> split_path('/foo/bar')
    ('/foo', 'bar')

    """

    lastslash = path.rfind('/')
    ppath = path[:lastslash]
    name = path[lastslash + 1:]

    if ppath == '':
        ppath = '/'

    return (ppath, name)

splitPath = previous_api(split_path)


def isvisiblename(name):
    """Does this `name` make the named node a visible one?"""

    return _hidden_name_re.match(name) is None

isVisibleName = previous_api(isvisiblename)


def isvisiblepath(path):
    """Does this `path` make the named node a visible one?"""

    return _hidden_path_re.search(path) is None

isVisiblePath = previous_api(isvisiblepath)


# Main part
# =========
def _test():
    """Run ``doctest`` on this module."""

    import doctest
    doctest.testmod()


if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = registry
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: March 18, 2005
# Author:  Ivan Vilata - reverse:net.selidor@ivan
#
# $Source$
# $Id$
#
########################################################################

"""Miscellaneous mappings used to avoid circular imports.

Variables:

`class_name_dict`
    Node class name to class object mapping.
`class_id_dict`
    Class identifier to class object mapping.

Misc variables:

`__docformat__`
    The format of documentation strings in this module.

"""

from tables._past import previous_api

# Important: no modules from PyTables should be imported here
# (but standard modules are OK), since the main reason for this module
# is avoiding circular imports!

__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""

class_name_dict = {}
"""Node class name to class object mapping.

This dictionary maps class names (e.g. ``'Group'``) to actual class
objects (e.g. `Group`).  Classes are registered here when they are
defined, and they are not expected to be unregistered (by now), but they
can be replaced when the module that defines them is reloaded.

.. versionchanged:: 3.0
   The *classNameDict* dictionary has been renamed into *class_name_dict*.

"""

class_id_dict = {}
"""Class identifier to class object mapping.

This dictionary maps class identifiers (e.g. ``'GROUP'``) to actual
class objects (e.g. `Group`).  Classes defining a new ``_c_classid``
attribute are registered here when they are defined, and they are not
expected to be unregistered (by now), but they can be replaced when the
module that defines them is reloaded.

.. versionchanged:: 3.0
   The *classIdDict* dictionary has been renamed into *class_id_dict*.

"""

# Deprecated API
classNameDict = class_name_dict
classIdDict = class_id_dict


def get_class_by_name(classname):
    """Get the node class matching the `classname`.

    If the name is not registered, a ``TypeError`` is raised.  The empty
    string and ``None`` are also accepted, and mean the ``Node`` class.

    .. versionadded:: 3.0

    """

    # The empty string is accepted for compatibility
    # with old default arguments.
    if classname is None or classname == '':
        classname = 'Node'

    # Get the class object corresponding to `classname`.
    if classname not in class_name_dict:
        raise TypeError("there is no registered node class named ``%s``"
                        % (classname,))

    return class_name_dict[classname]

getClassByName = previous_api(get_class_by_name)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = req_versions
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: November 5, 2010
# Author:  Francesc Alted - faltet@pytables.com
#
########################################################################

"""Required versions for PyTables dependencies."""

#***************************************************************
#  Keep these in sync with setup.py and user's guide and README
#***************************************************************

# Minimum recommended versions for mandatory packages
min_numpy_version = '1.4.1'
min_numexpr_version = '2.0.0'
min_cython_version = '0.13'

# The THG team has decided to fix an API inconsistency in the definition
# of the H5Z_class_t structure in version 1.8.3
min_hdf5_version = (1, 8, 4)  # necessary for allowing 1.8.10 > 1.8.5

########NEW FILE########
__FILENAME__ = pt2to3
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: April 9, 2013
# Author:  Anthony Scopatz - scopatz@gmail.com
#
# $Id$
#
########################################################################

"""This utility helps you migrate from PyTables 2.x APIs to 3.x APIs, which
are PEP 8 compliant.

"""
import os
import re
import sys
import argparse

from tables._past import old2newnames, new2oldnames

# Note that it is tempting to use the ast module here, but then this
# breaks transforming cython files.  So instead we are going to do the
# dumb thing with replace.


def make_subs(ns):
    names = new2oldnames if ns.reverse else old2newnames
    s = '(?<=\W)({0})(?=\W)'.format('|'.join(names.keys()))
    if ns.ignore_previous:
        s += '(?!\s*?=\s*?previous_api(_property)?\()'
        s += '(?!\* to \*\w+\*)'
        s += '(?!\* parameter has been renamed into \*\w+\*\.)'
        s += '(?! is pending deprecation, import \w+ instead\.)'
    subs = re.compile(s, flags=re.MULTILINE)

    def repl(m):
        return names.get(m.group(1), m.group(0))
    return subs, repl


def main():
    desc = ('PyTables 2.x -> 3.x API transition tool\n\n'
            'This tool displays to standard out, so it is \n'
            'common to pipe this to another file:\n\n'
            '$ pt2to3 oldfile.py > newfile.py')
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('-r', '--reverse', action='store_true', default=False,
                        dest='reverse',
                        help="reverts changes, going from 3.x -> 2.x.")
    parser.add_argument('-p', '--no-ignore-previous', action='store_false',
                        default=True, dest='ignore_previous',
                        help="ignores previous_api() calls.")
    parser.add_argument('-o', default=None, dest='output',
                        help="output file to write to.")
    parser.add_argument('-i', '--inplace', action='store_true', default=False,
                        dest='inplace', help="overwrites the file in-place.")
    parser.add_argument('filename', help='path to input file.')
    ns = parser.parse_args()

    if not os.path.isfile(ns.filename):
        sys.exit('file {0!r} not found'.format(ns.filename))
    with open(ns.filename, 'r') as f:
        src = f.read()

    subs, repl = make_subs(ns)
    targ = subs.sub(repl, src)

    ns.output = ns.filename if ns.inplace else ns.output
    if ns.output is None:
        sys.stdout.write(targ)
    else:
        with open(ns.output, 'w') as f:
            f.write(targ)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ptdump
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: February 10, 2004
# Author:  Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""This utility lets you look into the data and metadata of your data files.

Pass the flag -h to this for help on usage.

"""

from __future__ import print_function

import argparse

from tables.file import open_file
from tables.group import Group
from tables.leaf import Leaf
from tables.table import Table, Column
from tables.unimplemented import UnImplemented
from tables._past import previous_api

# default options
options = argparse.Namespace(
    rng=slice(None),
    showattrs=0,
    verbose=0,
    dump=0,
    colinfo=0,
    idxinfo=0,
)


def dump_leaf(leaf):
    if options.verbose:
        print(repr(leaf))
    else:
        print(str(leaf))
    if options.showattrs:
        print("  "+repr(leaf.attrs))
    if options.dump and not isinstance(leaf, UnImplemented):
        print("  Data dump:")
        # print((leaf.read(options.rng.start, options.rng.stop,
        #        options.rng.step))
        # This is better for large objects
        if options.rng.start is None:
            start = 0
        else:
            start = options.rng.start
        if options.rng.stop is None:
            if leaf.shape != ():
                stop = leaf.shape[0]
        else:
            stop = options.rng.stop
        if options.rng.step is None:
            step = 1
        else:
            step = options.rng.step
        if leaf.shape == ():
            print("[SCALAR] %s" % (leaf[()]))
        else:
            for i in range(start, stop, step):
                print("[%s] %s" % (i, leaf[i]))

    if isinstance(leaf, Table) and options.colinfo:
        # Show info of columns
        for colname in leaf.colnames:
            print(repr(leaf.cols._f_col(colname)))

    if isinstance(leaf, Table) and options.idxinfo:
        # Show info of indexes
        for colname in leaf.colnames:
            col = leaf.cols._f_col(colname)
            if isinstance(col, Column) and col.index is not None:
                idx = col.index
                print(repr(idx))

dumpLeaf = previous_api(dump_leaf)


def dump_group(pgroup):
    node_kinds = pgroup._v_file._node_kinds[1:]
    for group in pgroup._f_walk_groups():
        print(str(group))
        if options.showattrs:
            print("  "+repr(group._v_attrs))
        for kind in node_kinds:
            for node in group._f_list_nodes(kind):
                if options.verbose or options.dump:
                    dump_leaf(node)
                else:
                    print(str(node))


dumpGroup = previous_api(dump_group)


def _get_parser():
    parser = argparse.ArgumentParser(
        description='''The ptdump utility allows you look into the contents
        of your PyTables files. It lets you see not only the data but also
        the metadata (that is, the *structure* and additional information in
        the form of *attributes*).''')

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='dump more metainformation on nodes',
    )
    parser.add_argument(
        '-d', '--dump', action='store_true',
        help='dump data information on leaves',
    )
    parser.add_argument(
        '-a', '--showattrs', action='store_true',
        help='show attributes in nodes (only useful when -v or -d are active)',
    )
    parser.add_argument(
        '-c', '--colinfo', action='store_true',
        help='''show info of columns in tables (only useful when -v or -d
        are active)''',
    )
    parser.add_argument(
        '-i', '--idxinfo', action='store_true',
        help='''show info of indexed columns (only useful when -v or -d are
        active)''',
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help='''select a RANGE of rows (in the form "start,stop,step")
        during the copy of *all* the leaves.
        Default values are "None,None,1", which means a copy of all the
        rows.''',
    )
    parser.add_argument('src', metavar='filename[:nodepath]',
                        help='name of the HDF5 file to dump')

    return parser


def main():
    parser = _get_parser()

    args = parser.parse_args(namespace=options)

    # Get the options
    if isinstance(args.rng, basestring):
        try:
            options.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")
        else:
            args.dump = 1

    # Catch the files passed as the last arguments
    src = args.src.split(':')
    if len(src) == 1:
        filename, nodename = src[0], "/"
    else:
        filename, nodename = src
        if nodename == "":
            # case where filename == "filename:" instead of "filename:/"
            nodename = "/"

    # Check whether the specified node is a group or a leaf
    h5file = open_file(filename, 'r')
    nodeobject = h5file.get_node(nodename)
    if isinstance(nodeobject, Group):
        # Close the file again and reopen using the root_uep
        dump_group(nodeobject)
    elif isinstance(nodeobject, Leaf):
        # If it is not a Group, it must be a Leaf
        dump_leaf(nodeobject)
    else:
        # This should never happen
        print("Unrecognized object:", nodeobject)

    # Close the file
    h5file.close()

########NEW FILE########
__FILENAME__ = ptrepack
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: February 10, 2004
# Author:  Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""This utility lets you repack your data files in a flexible way.

Pass the flag -h to this for help on usage.

"""

from __future__ import print_function
import sys
import time
import os.path
import argparse
import warnings


from tables.file import open_file
from tables.group import Group
from tables.leaf import Filters
from tables.flavor import internal_flavor
from tables.exceptions import OldIndexWarning, NoSuchNodeError, FlavorWarning
from tables._past import previous_api

# Global variables
verbose = False
regoldindexes = True
createsysattrs = True

numpy_aliases = [
    'numeric',
    'Numeric',
    'numarray',
    'NumArray',
    'CharArray',
]


def newdst_group(dstfileh, dstgroup, title, filters):
    group = dstfileh.root
    # Now, create the new group. This works even if dstgroup == '/'
    for nodeName in dstgroup.split('/'):
        if nodeName == '':
            continue
        # First try if possible intermediate groups does already exist.
        try:
            group2 = dstfileh.get_node(group, nodeName)
        except NoSuchNodeError:
            # The group does not exist. Create it.
            group2 = dstfileh.create_group(group, nodeName,
                                           title=title,
                                           filters=filters)
        group = group2
    return group

newdstGroup = previous_api(newdst_group)


def recreate_indexes(table, dstfileh, dsttable):
    listoldindexes = table._listoldindexes
    if listoldindexes != []:
        if not regoldindexes:
            if verbose:
                print("[I]Not regenerating indexes for table: '%s:%s'" %
                      (dstfileh.filename, dsttable._v_pathname))
            return
        # Now, recreate the indexed columns
        if verbose:
            print("[I]Regenerating indexes for table: '%s:%s'" %
                  (dstfileh.filename, dsttable._v_pathname))
        for colname in listoldindexes:
            if verbose:
                print("[I]Indexing column: '%s'. Please wait..." % colname)
            colobj = dsttable.cols._f_col(colname)
            # We don't specify the filters for the indexes
            colobj.create_index(filters=None)

recreateIndexes = previous_api(recreate_indexes)


def copy_leaf(srcfile, dstfile, srcnode, dstnode, title,
              filters, copyuserattrs, overwritefile, overwrtnodes, stats,
              start, stop, step, chunkshape, sortby, checkCSI,
              propindexes, upgradeflavors):
    # Open the source file
    srcfileh = open_file(srcfile, 'r')
    # Get the source node (that should exist)
    srcNode = srcfileh.get_node(srcnode)

    # Get the destination node and its parent
    last_slash = dstnode.rindex('/')
    if last_slash == len(dstnode)-1:
        # print("Detected a trailing slash in destination node. "
        #       "Interpreting it as a destination group.")
        dstgroup = dstnode[:-1]
    elif last_slash > 0:
        dstgroup = dstnode[:last_slash]
    else:
        dstgroup = "/"
    dstleaf = dstnode[last_slash + 1:]
    if dstleaf == "":
        dstleaf = srcNode.name
    # Check whether the destination group exists or not
    if os.path.isfile(dstfile) and not overwritefile:
        dstfileh = open_file(dstfile, 'a', pytables_sys_attrs=createsysattrs)
        try:
            dstGroup = dstfileh.get_node(dstgroup)
        except:
            # The dstgroup does not seem to exist. Try creating it.
            dstGroup = newdst_group(dstfileh, dstgroup, title, filters)
        else:
            # The node exists, but it is really a group?
            if not isinstance(dstGroup, Group):
                # No. Should we overwrite it?
                if overwrtnodes:
                    parent = dstGroup._v_parent
                    last_slash = dstGroup._v_pathname.rindex('/')
                    dstgroupname = dstGroup._v_pathname[last_slash + 1:]
                    dstGroup.remove()
                    dstGroup = dstfileh.create_group(parent, dstgroupname,
                                                     title=title,
                                                     filters=filters)
                else:
                    raise RuntimeError("Please check that the node names are "
                                       "not duplicated in destination, and "
                                       "if so, add the --overwrite-nodes "
                                       "flag if desired.")
    else:
        # The destination file does not exist or will be overwritten.
        dstfileh = open_file(dstfile, 'w', title=title, filters=filters,
                             pytables_sys_attrs=createsysattrs)
        dstGroup = newdst_group(dstfileh, dstgroup, title="", filters=filters)

    # Finally, copy srcNode to dstNode
    try:
        dstNode = srcNode.copy(
            dstGroup, dstleaf, filters=filters,
            copyuserattrs=copyuserattrs, overwrite=overwrtnodes,
            stats=stats, start=start, stop=stop, step=step,
            chunkshape=chunkshape,
            sortby=sortby, checkCSI=checkCSI, propindexes=propindexes)
    except:
        (type, value, traceback) = sys.exc_info()
        print("Problems doing the copy from '%s:%s' to '%s:%s'" %
              (srcfile, srcnode, dstfile, dstnode))
        print("The error was --> %s: %s" % (type, value))
        print("The destination file looks like:\n", dstfileh)
        # Close all the open files:
        srcfileh.close()
        dstfileh.close()
        raise RuntimeError("Please check that the node names are not "
                           "duplicated in destination, and if so, add "
                           "the --overwrite-nodes flag if desired.")

    # Upgrade flavors in dstNode, if required
    if upgradeflavors:
        if srcfileh.format_version.startswith("1"):
            # Remove original flavor in case the source file has 1.x format
            dstNode.del_attr('FLAVOR')
        elif srcfileh.format_version < "2.1":
            if dstNode.get_attr('FLAVOR') in numpy_aliases:
                dstNode.set_attr('FLAVOR', internal_flavor)

    # Recreate possible old indexes in destination node
    if srcNode._c_classid == "TABLE":
        recreate_indexes(srcNode, dstfileh, dstNode)

    # Close all the open files:
    srcfileh.close()
    dstfileh.close()

copyLeaf = previous_api(copy_leaf)


def copy_children(srcfile, dstfile, srcgroup, dstgroup, title,
                  recursive, filters, copyuserattrs, overwritefile,
                  overwrtnodes, stats, start, stop, step,
                  chunkshape, sortby, checkCSI, propindexes,
                  upgradeflavors):
    "Copy the children from source group to destination group"
    # Open the source file with srcgroup as root_uep
    srcfileh = open_file(srcfile, 'r', root_uep=srcgroup)
    #  Assign the root to srcGroup
    srcGroup = srcfileh.root

    created_dstGroup = False
    # Check whether the destination group exists or not
    if os.path.isfile(dstfile) and not overwritefile:
        dstfileh = open_file(dstfile, 'a', pytables_sys_attrs=createsysattrs)
        try:
            dstGroup = dstfileh.get_node(dstgroup)
        except:
            # The dstgroup does not seem to exist. Try creating it.
            dstGroup = newdst_group(dstfileh, dstgroup, title, filters)
            created_dstGroup = True
        else:
            # The node exists, but it is really a group?
            if not isinstance(dstGroup, Group):
                # No. Should we overwrite it?
                if overwrtnodes:
                    parent = dstGroup._v_parent
                    last_slash = dstGroup._v_pathname.rindex('/')
                    dstgroupname = dstGroup._v_pathname[last_slash + 1:]
                    dstGroup.remove()
                    dstGroup = dstfileh.create_group(parent, dstgroupname,
                                                     title=title,
                                                     filters=filters)
                else:
                    raise RuntimeError("Please check that the node names are "
                                       "not duplicated in destination, and "
                                       "if so, add the --overwrite-nodes "
                                       "flag if desired.")
    else:
        # The destination file does not exist or will be overwritten.
        dstfileh = open_file(dstfile, 'w', title=title, filters=filters,
                             pytables_sys_attrs=createsysattrs)
        dstGroup = newdst_group(dstfileh, dstgroup, title="", filters=filters)
        created_dstGroup = True

    # Copy the attributes to dstGroup, if needed
    if created_dstGroup and copyuserattrs:
        srcGroup._v_attrs._f_copy(dstGroup)

    # Finally, copy srcGroup children to dstGroup
    try:
        srcGroup._f_copy_children(
            dstGroup, recursive=recursive, filters=filters,
            copyuserattrs=copyuserattrs, overwrite=overwrtnodes,
            stats=stats, start=start, stop=stop, step=step,
            chunkshape=chunkshape,
            sortby=sortby, checkCSI=checkCSI, propindexes=propindexes)
    except:
        (type, value, traceback) = sys.exc_info()
        print("Problems doing the copy from '%s:%s' to '%s:%s'" %
              (srcfile, srcgroup, dstfile, dstgroup))
        print("The error was --> %s: %s" % (type, value))
        print("The destination file looks like:\n", dstfileh)
        # Close all the open files:
        srcfileh.close()
        dstfileh.close()
        raise RuntimeError("Please check that the node names are not "
                           "duplicated in destination, and if so, add the "
                           "--overwrite-nodes flag if desired. In "
                           "particular, pay attention that root_uep is not "
                           "fooling you.")

    # Upgrade flavors in dstNode, if required
    if upgradeflavors:
        for dstNode in dstGroup._f_walknodes("Leaf"):
            if srcfileh.format_version.startswith("1"):
                # Remove original flavor in case the source file has 1.x format
                dstNode.del_attr('FLAVOR')
            elif srcfileh.format_version < "2.1":
                if dstNode.get_attr('FLAVOR') in numpy_aliases:
                    dstNode.set_attr('FLAVOR', internal_flavor)

    # Convert the remaining tables with old indexes (if any)
    for table in srcGroup._f_walknodes("Table"):
        dsttable = dstfileh.get_node(dstGroup, table._v_pathname)
        recreate_indexes(table, dstfileh, dsttable)

    # Close all the open files:
    srcfileh.close()
    dstfileh.close()

copyChildren = previous_api(copy_children)


def _get_parser():
    parser = argparse.ArgumentParser(
        description='''This utility is very powerful and lets you copy any
        leaf, group or complete subtree into another file.
        During the copy process you are allowed to change the filter
        properties if you want so. Also, in the case of duplicated pathnames,
        you can decide if you want to overwrite already existing nodes on the
        destination file. Generally speaking, ptrepack can be useful in may
        situations, like replicating a subtree in another file, change the
        filters in objects and see how affect this to the compression degree
        or I/O performance, consolidating specific data in repositories or
        even *importing* generic HDF5 files and create true PyTables
        counterparts.''')

    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='show verbose information',
    )
    parser.add_argument(
        '-o', '--overwrite', action='store_true', dest='overwritefile',
        help='overwrite destination file',
    )
    parser.add_argument(
        '-R', '--range', dest='rng', metavar='RANGE',
        help='''select a RANGE of rows (in the form "start,stop,step")
        during the copy of *all* the leaves.
        Default values are "None,None,1", which means a copy of all the
        rows.''',
    )
    parser.add_argument(
        '--non-recursive', action='store_false', default=True,
        dest='recursive',
        help='do not do a recursive copy. Default is to do it',
    )
    parser.add_argument(
        '--dest-title', dest='title', default='',
        help='title for the new file (if not specified, the source is copied)',
    )
    parser.add_argument(
        '--dont-create-sysattrs', action='store_false', default=True,
        dest='createsysattrs',
        help='do not create sys attrs (default is to do it)',
    )
    parser.add_argument(
        '--dont-copy-userattrs', action='store_false', default=True,
        dest='copyuserattrs',
        help='do not copy the user attrs (default is to do it)',
    )
    parser.add_argument(
        '--overwrite-nodes', action='store_true', dest='overwrtnodes',
        help='''overwrite destination nodes if they exist.
        Default is to not overwrite them''',
    )
    parser.add_argument(
        '--complevel', type=int, default=0,
        help='''set a compression level (0 for no compression, which is the
        default)''',
    )
    parser.add_argument(
        '--complib', choices=(
            "zlib", "lzo", "bzip2", "blosc", "blosc:blosclz",
            "blosc:lz4", "blosc:lz4hc", "blosc:snappy",
            "blosc:zlib"), default='zlib',
        help='''set the compression library to be used during the copy.
        Defaults to %(default)s''',
    )
    parser.add_argument(
        '--shuffle', type=int, choices=(0, 1),
        help='''activate or not the shuffling filter (default is active if
        complevel > 0)''',
    )
    parser.add_argument(
        '--fletcher32', type=int, choices=(0, 1),
        help='''whether to activate or not the fletcher32 filter (not active
        by default)''',
    )
    parser.add_argument(
        '--keep-source-filters', action='store_true', dest='keepfilters',
        help='''use the original filters in source files.
        The default is not doing that if any of --complevel, --complib,
        --shuffle or --fletcher32 option is specified''',
    )
    parser.add_argument(
        '--chunkshape', default='keep',
        help='''set a chunkshape.
        Possible options are: "keep" | "auto" | int | tuple.
        A value of "auto" computes a sensible value for the chunkshape of the
        leaves copied.  The default is to "keep" the original value''',
    )
    parser.add_argument(
        '--upgrade-flavors', action='store_true', dest='upgradeflavors',
        help='''when repacking PyTables 1.x or PyTables 2.x files, the flavor
        of leaves will be unset. With this, such a leaves will be serialized
        as objects with the internal flavor ('numpy' for 3.x series)''',
    )
    parser.add_argument(
        '--dont-regenerate-old-indexes', action='store_false', default=True,
        dest='regoldindexes',
        help='''disable regenerating old indexes.
        The default is to regenerate old indexes as they are found''',
    )
    parser.add_argument(
        '--sortby', metavar='COLUMN',
        help='''do a table copy sorted by the index in "column".
        For reversing the order, use a negative value in the "step" part of
        "RANGE" (see "-r" flag).  Only applies to table objects''',
    )
    parser.add_argument(
        '--checkCSI', action='store_true',
        help='Force the check for a CSI index for the --sortby column',
    )
    parser.add_argument(
        '--propindexes', action='store_true',
        help='''propagate the indexes existing in original tables. The default
        is to not propagate them.  Only applies to table objects''',
    )
    parser.add_argument(
        'src', metavar='sourcefile:sourcegroup', help='source file/group',
    )
    parser.add_argument(
        'dst', metavar='destfile:destgroup', help='destination file/group',
    )

    return parser


def main():
    global verbose
    global regoldindexes
    global createsysattrs

    parser = _get_parser()
    args = parser.parse_args()

    # check arguments
    if args.rng:
        try:
            args.rng = eval("slice(" + args.rng + ")")
        except Exception:
            parser.error("Error when getting the range parameter.")

    if args.chunkshape.isdigit() or args.chunkshape.startswith('('):
        args.chunkshape = eval(args.chunkshape)

    if args.complevel < 0 or args.complevel > 9:
        parser.error(
            'invalid "complevel" value, it sould be in te range [0, 9]'
        )

    # Catch the files passed as the last arguments
    src = args.src.split(':')
    dst = args.dst.split(':')
    if len(src) == 1:
        srcfile, srcnode = src[0], "/"
    else:
        srcfile, srcnode = src
    if len(dst) == 1:
        dstfile, dstnode = dst[0], "/"
    else:
        dstfile, dstnode = dst

    if srcnode == "":
        # case where filename == "filename:" instead of "filename:/"
        srcnode = "/"

    if dstnode == "":
        # case where filename == "filename:" instead of "filename:/"
        dstnode = "/"

    # Ignore the warnings for tables that contains oldindexes
    # (these will be handled by the copying routines)
    warnings.filterwarnings("ignore", category=OldIndexWarning)

    # Ignore the flavors warnings during upgrading flavor operations
    if args.upgradeflavors:
        warnings.filterwarnings("ignore", category=FlavorWarning)

    # Build the Filters instance
    filter_params = (
        args.complevel,
        args.complib,
        args.shuffle,
        args.fletcher32,
    )
    if (filter_params == (None,) * 4 or args.keepfilters):
        filters = None
    else:
        if args.complevel is None:
            args.complevel = 0
        if args.shuffle is None:
            if args.complevel > 0:
                args.shuffle = True
            else:
                args.shuffle = False
        if args.complib is None:
            args.complib = "zlib"
        if args.fletcher32 is None:
            args.fletcher32 = False
        filters = Filters(complevel=args.complevel, complib=args.complib,
                          shuffle=args.shuffle, fletcher32=args.fletcher32)

    # The start, stop and step params:
    start, stop, step = None, None, 1  # Defaults
    if args.rng:
        start, stop, step = args.rng.start, args.rng.stop, args.rng.step

    # Set globals
    verbose = args.verbose
    regoldindexes = args.regoldindexes
    createsysattrs = args.createsysattrs

    # Some timing
    t1 = time.time()
    cpu1 = time.clock()
    # Copy the file
    if verbose:
        print("+=+" * 20)
        print("Recursive copy:", args.recursive)
        print("Applying filters:", filters)
        if args.sortby is not None:
            print("Sorting table(s) by column:", args.sortby)
            print("Forcing a CSI creation:", args.checkCSI)
        if args.propindexes:
            print("Recreating indexes in copied table(s)")
        print("Start copying %s:%s to %s:%s" % (srcfile, srcnode,
                                                dstfile, dstnode))
        print("+=+" * 20)

    # Check whether the specified source node is a group or a leaf
    h5srcfile = open_file(srcfile, 'r')
    srcnodeobject = h5srcfile.get_node(srcnode)

    # Close the file again
    h5srcfile.close()

    stats = {'groups': 0, 'leaves': 0, 'links': 0, 'bytes': 0}
    if isinstance(srcnodeobject, Group):
        copy_children(
            srcfile, dstfile, srcnode, dstnode,
            title=args.title, recursive=args.recursive, filters=filters,
            copyuserattrs=args.copyuserattrs, overwritefile=args.overwritefile,
            overwrtnodes=args.overwrtnodes, stats=stats,
            start=start, stop=stop, step=step, chunkshape=args.chunkshape,
            sortby=args.sortby, checkCSI=args.checkCSI,
            propindexes=args.propindexes,
            upgradeflavors=args.upgradeflavors)
    else:
        # If not a Group, it should be a Leaf
        copy_leaf(
            srcfile, dstfile, srcnode, dstnode,
            title=args.title, filters=filters,
            copyuserattrs=args.copyuserattrs,
            overwritefile=args.overwritefile, overwrtnodes=args.overwrtnodes,
            stats=stats, start=start, stop=stop, step=step,
            chunkshape=args.chunkshape,
            sortby=args.sortby, checkCSI=args.checkCSI,
            propindexes=args.propindexes,
            upgradeflavors=args.upgradeflavors)

    # Gather some statistics
    t2 = time.time()
    cpu2 = time.clock()
    tcopy = round(t2 - t1, 3)
    cpucopy = round(cpu2 - cpu1, 3)
    tpercent = int(round(cpucopy / tcopy, 2) * 100)

    if verbose:
        ngroups = stats['groups']
        nleaves = stats['leaves']
        nlinks = stats['links']
        nbytescopied = stats['bytes']
        nnodes = ngroups + nleaves + nlinks

        print((
            "Groups copied:", ngroups,
            " Leaves copied:", nleaves,
            " Links copied:", nlinks,
        ))
        if args.copyuserattrs:
            print("User attrs copied")
        else:
            print("User attrs not copied")
        print("KBytes copied:", round(nbytescopied / 1024., 3))
        print("Time copying: %s s (real) %s s (cpu)  %s%%" % (
            tcopy, cpucopy, tpercent))
        print("Copied nodes/sec: ", round((nnodes) / float(tcopy), 1))
        print("Copied KB/s :", int(nbytescopied / (tcopy * 1024)))

########NEW FILE########
__FILENAME__ = table
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: September 4, 2002
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the Table class."""

import sys
import math
import warnings
import os.path
from time import time
from functools import reduce as _reduce

import numpy
import numexpr

from tables import tableextension
from tables.lrucacheextension import ObjectCache, NumCache
from tables.atom import Atom
from tables.conditions import compile_condition
from numexpr.necompiler import (
    getType as numexpr_getType, double, is_cpu_amd_intel)
from numexpr.expressions import functions as numexpr_functions
from tables.flavor import flavor_of, array_as_internal, internal_to_flavor
from tables.utils import is_idx, lazyattr, SizeType, NailedDict as CacheDict
from tables.leaf import Leaf
from tables.description import (
    IsDescription, Description, Col, descr_from_dtype)
from tables.exceptions import (NodeError, HDF5ExtError, PerformanceWarning,
                               OldIndexWarning, NoSuchNodeError)
from tables.utilsextension import get_nested_field

from tables.path import join_path, split_path
from tables.index import (
    OldIndex, default_index_filters, default_auto_index, Index, IndexesDescG,
    IndexesTableG)

profile = False
# profile = True  # Uncomment for profiling
if profile:
    from tables.utils import show_stats

from tables._past import previous_api, previous_api_property

# 2.2: Added support for complex types. Introduced in version 0.9.
# 2.2.1: Added suport for time types.
# 2.3: Changed the indexes naming schema.
# 2.4: Changed indexes naming schema (again).
# 2.5: Added the FIELD_%d_FILL attributes.
# 2.6: Added the FLAVOR attribute (optional).
# 2.7: Numeric and numarray flavors are gone.
obversion = "2.7"  # The Table VERSION number


try:
    # int_, long_ are only available in numexpr >= 2.1
    from numexpr.necompiler import int_, long_
except ImportError:
    int_ = int
    long_ = long

# Maps NumPy types to the types used by Numexpr.
_nxtype_from_nptype = {
    numpy.bool_: bool,
    numpy.int8: int_,
    numpy.int16: int_,
    numpy.int32: int_,
    numpy.int64: long_,
    numpy.uint8: int_,
    numpy.uint16: int_,
    numpy.uint32: long_,
    numpy.uint64: long_,
    numpy.float32: float,
    numpy.float64: double,
    numpy.complex64: complex,
    numpy.complex128: complex,
    numpy.bytes_: bytes,
}

if sys.version_info[0] > 2:
    _nxtype_from_nptype[numpy.str_] = str

if hasattr(numpy, 'float16'):
    _nxtype_from_nptype[numpy.float16] = float    # XXX: check
if hasattr(numpy, 'float96'):
    _nxtype_from_nptype[numpy.float96] = double   # XXX: check
if hasattr(numpy, 'float128'):
    _nxtype_from_nptype[numpy.float128] = double  # XXX: check
if hasattr(numpy, 'complec192'):
    _nxtype_from_nptype[numpy.complex192] = complex  # XXX: check
if hasattr(numpy, 'complex256'):
    _nxtype_from_nptype[numpy.complex256] = complex  # XXX: check


# The NumPy scalar type corresponding to `SizeType`.
_npsizetype = numpy.array(SizeType(0)).dtype.type


def _index_name_of(node):
    return '_i_%s' % node._v_name

_indexNameOf = previous_api(_index_name_of)


def _index_pathname_of(node):
    nodeParentPath = split_path(node._v_pathname)[0]
    return join_path(nodeParentPath, _index_name_of(node))

_indexPathnameOf = previous_api(_index_pathname_of)


def _index_pathname_of_column(table, colpathname):
    return join_path(_index_pathname_of(table), colpathname)

_indexPathnameOfColumn = previous_api(_index_pathname_of_column)

# The next are versions that work with just paths (i.e. we don't need
# a node instance for using them, which can be critical in certain
# situations)


def _index_name_of_(nodeName):
    return '_i_%s' % nodeName

_indexNameOf_ = previous_api(_index_name_of_)


def _index_pathname_of_(nodePath):
    nodeParentPath, nodeName = split_path(nodePath)
    return join_path(nodeParentPath, _index_name_of_(nodeName))

_indexPathnameOf_ = previous_api(_index_pathname_of_)


def _index_pathname_of_column_(tablePath, colpathname):
    return join_path(_index_pathname_of_(tablePath), colpathname)

_indexPathnameOfColumn_ = previous_api(_index_pathname_of_column_)


def _table__setautoindex(self, auto):
    auto = bool(auto)
    try:
        indexgroup = self._v_file._get_node(_index_pathname_of(self))
    except NoSuchNodeError:
        indexgroup = create_indexes_table(self)
    indexgroup.auto = auto
    # Update the cache in table instance as well
    self._autoindex = auto

_table__setautoIndex = previous_api(_table__setautoindex)


# **************** WARNING! ***********************
# This function can be called during the destruction time of a table
# so measures have been taken so that it doesn't have to revive
# another node (which can fool the LRU cache). The solution devised
# has been to add a cache for autoindex (Table._autoindex), populate
# it in creation time of the cache (which is a safe period) and then
# update the cache whenever it changes.
# This solves the error when running test_indexes.py ManyNodesTestCase.
# F. Alted 2007-04-20
# **************************************************
def _table__getautoindex(self):
    if self._autoindex is None:
        try:
            indexgroup = self._v_file._get_node(_index_pathname_of(self))
        except NoSuchNodeError:
            self._autoindex = default_auto_index  # update cache
            return self._autoindex
        else:
            self._autoindex = indexgroup.auto   # update cache
            return self._autoindex
    else:
        # The value is in cache, return it
        return self._autoindex

_table__getautoIndex = previous_api(_table__getautoindex)

_table__autoindex = property(
    _table__getautoindex, _table__setautoindex, None,
    """Automatically keep column indexes up to date?

    Setting this value states whether existing indexes should be
    automatically updated after an append operation or recomputed
    after an index-invalidating operation (i.e. removal and
    modification of rows).  The default is true.

    This value gets into effect whenever a column is altered.  If you
    don't have automatic indexing activated and you want to do an an
    immediate update use `Table.flush_rows_to_index()`; for an immediate
    reindexing of invalidated indexes, use `Table.reindex_dirty()`.

    This value is persistent.
    """)

_table__autoIndex = previous_api(_table__autoindex)


def restorecache(self):
    # Define a cache for sparse table reads
    params = self._v_file.params
    chunksize = self._v_chunkshape[0]
    nslots = params['TABLE_MAX_SIZE'] / (chunksize * self._v_dtype.itemsize)
    self._chunkcache = NumCache((nslots, chunksize), self._v_dtype,
                                'table chunk cache')
    self._seqcache = ObjectCache(params['ITERSEQ_MAX_SLOTS'],
                                 params['ITERSEQ_MAX_SIZE'],
                                 'Iter sequence cache')
    self._dirtycache = False


def _table__where_indexed(self, compiled, condition, condvars,
                          start, stop, step):
    if profile:
        tref = time()
    if profile:
        show_stats("Entering table_whereIndexed", tref)
    self._use_index = True
    # Clean the table caches for indexed queries if needed
    if self._dirtycache:
        restorecache(self)

    # Get the values in expression that are not columns
    values = []
    for key, value in condvars.iteritems():
        if isinstance(value, numpy.ndarray):
            values.append((key, value.item()))
    # Build a key for the sequence cache
    seqkey = (condition, tuple(values), (start, stop, step))
    # Do a lookup in sequential cache for this query
    nslot = self._seqcache.getslot(seqkey)
    if nslot >= 0:
        # Get the row sequence from the cache
        seq = self._seqcache.getitem(nslot)
        if len(seq) == 0:
            return None
        seq = numpy.array(seq, dtype='int64')
        # Correct the ranges in cached sequence
        if (start, stop, step) != (0, self.nrows, 1):
            seq = seq[(seq >= start) & (
                seq < stop) & ((seq - start) % step == 0)]
        return self.itersequence(seq)
    else:
        # No luck.  Set row sequence to empty.  It will be populated
        # in the iterator. If not possible, the slot entry will be
        # removed there.
        self._nslotseq = self._seqcache.setitem(seqkey, [], 1)

    # Compute the chunkmap for every index in indexed expression
    idxexprs = compiled.index_expressions
    strexpr = compiled.string_expression
    cmvars = {}
    tcoords = 0
    for i, idxexpr in enumerate(idxexprs):
        var, ops, lims = idxexpr
        col = condvars[var]
        index = col.index
        assert index is not None, "the chosen column is not indexed"
        assert not index.dirty, "the chosen column has a dirty index"

        # Get the number of rows that the indexed condition yields.
        range_ = index.get_lookup_range(ops, lims)
        ncoords = index.search(range_)
        tcoords += ncoords
        if index.reduction == 1 and ncoords == 0:
            # No values from index condition, thus the chunkmap should be empty
            nrowsinchunk = self.chunkshape[0]
            nchunks = long(math.ceil(float(self.nrows) / nrowsinchunk))
            chunkmap = numpy.zeros(shape=nchunks, dtype="bool")
        else:
            # Get the chunkmap from the index
            chunkmap = index.get_chunkmap()
        # Assign the chunkmap to the cmvars dictionary
        cmvars["e%d" % i] = chunkmap

    if index.reduction == 1 and tcoords == 0:
        # No candidates found in any indexed expression component, so leave now
        return None

    # Compute the final chunkmap
    chunkmap = numexpr.evaluate(strexpr, cmvars)
    # Method .any() is twice as faster than method .sum()
    if not chunkmap.any():
        # The chunkmap is empty
        return None

    if profile:
        show_stats("Exiting table_whereIndexed", tref)
    return chunkmap

_table__whereIndexed = previous_api(_table__where_indexed)


def create_indexes_table(table):
    itgroup = IndexesTableG(
        table._v_parent, _index_name_of(table),
        "Indexes container for table " + table._v_pathname, new=True)
    return itgroup

createIndexesTable = previous_api(create_indexes_table)


def create_indexes_descr(igroup, dname, iname, filters):
    idgroup = IndexesDescG(
        igroup, iname,
        "Indexes container for sub-description " + dname,
        filters=filters, new=True)
    return idgroup

createIndexesDescr = previous_api(create_indexes_descr)


def _column__create_index(self, optlevel, kind, filters, tmp_dir,
                          blocksizes, verbose):
    name = self.name
    table = self.table
    dtype = self.dtype
    descr = self.descr
    index = self.index
    get_node = table._v_file._get_node

    # Warn if the index already exists
    if index:
        raise ValueError("%s for column '%s' already exists. If you want to "
                         "re-create it, please, try with reindex() method "
                         "better" % (str(index), str(self.pathname)))

    # Check that the datatype is indexable.
    if dtype.str[1:] == 'u8':
        raise NotImplementedError(
            "indexing 64-bit unsigned integer columns "
            "is not supported yet, sorry")
    if dtype.kind == 'c':
        raise TypeError("complex columns can not be indexed")
    if dtype.shape != ():
        raise TypeError("multidimensional columns can not be indexed")

    # Get the indexes group for table, and if not exists, create it
    try:
        itgroup = get_node(_index_pathname_of(table))
    except NoSuchNodeError:
        itgroup = create_indexes_table(table)

    # Create the necessary intermediate groups for descriptors
    idgroup = itgroup
    dname = ""
    pathname = descr._v_pathname
    if pathname != '':
        inames = pathname.split('/')
        for iname in inames:
            if dname == '':
                dname = iname
            else:
                dname += '/' + iname
            try:
                idgroup = get_node('%s/%s' % (itgroup._v_pathname, dname))
            except NoSuchNodeError:
                idgroup = create_indexes_descr(idgroup, dname, iname, filters)

    # Create the atom
    assert dtype.shape == ()
    atom = Atom.from_dtype(numpy.dtype((dtype, (0,))))

    # Protection on tables larger than the expected rows (perhaps the
    # user forgot to pass this parameter to the Table constructor?)
    expectedrows = table._v_expectedrows
    if table.nrows > expectedrows:
        expectedrows = table.nrows

    # Create the index itself
    index = Index(
        idgroup, name, atom=atom,
        title="Index for %s column" % name,
        kind=kind,
        optlevel=optlevel,
        filters=filters,
        tmp_dir=tmp_dir,
        expectedrows=expectedrows,
        byteorder=table.byteorder,
        blocksizes=blocksizes)

    table._set_column_indexing(self.pathname, True)

    # Feed the index with values

    # Add rows to the index if necessary
    if table.nrows > 0:
        indexedrows = table._add_rows_to_index(
            self.pathname, 0, table.nrows, lastrow=True, update=False)
    else:
        indexedrows = 0
    index.dirty = False
    table._indexedrows = indexedrows
    table._unsaved_indexedrows = table.nrows - indexedrows

    # Optimize the index that has been already filled-up
    index.optimize(verbose=verbose)

    # We cannot do a flush here because when reindexing during a
    # flush, the indexes are created anew, and that creates a nested
    # call to flush().
    # table.flush()

    return indexedrows

_column__createIndex = previous_api(_column__create_index)


class _ColIndexes(dict):
    """Provides a nice representation of column indexes."""

    def __repr__(self):
        """Gives a detailed Description column representation."""

        rep = ['  \"%s\": %s' % (k, self[k]) for k in self.iterkeys()]
        return '{\n  %s}' % (',\n  '.join(rep))


class Table(tableextension.Table, Leaf):
    """This class represents heterogeneous datasets in an HDF5 file.

    Tables are leaves (see the Leaf class in :ref:`LeafClassDescr`) whose data
    consists of a unidimensional sequence of *rows*, where each row contains
    one or more *fields*.  Fields have an associated unique *name* and
    *position*, with the first field having position 0.  All rows have the same
    fields, which are arranged in *columns*.

    Fields can have any type supported by the Col class (see
    :ref:`ColClassDescr`) and its descendants, which support multidimensional
    data.  Moreover, a field can be *nested* (to an arbitrary depth), meaning
    that it includes further fields inside.  A field named x inside a nested
    field a in a table can be accessed as the field a/x (its *path name*) from
    the table.

    The structure of a table is declared by its description, which is made
    available in the Table.description attribute (see :class:`Table`).

    This class provides new methods to read, write and search table data
    efficiently.  It also provides special Python methods to allow accessing
    the table as a normal sequence or array (with extended slicing supported).

    PyTables supports *in-kernel* searches working simultaneously on several
    columns using complex conditions.  These are faster than selections using
    Python expressions.  See the :meth:`Table.where` method for more
    information on in-kernel searches.

    Non-nested columns can be *indexed*.  Searching an indexed column can be
    several times faster than searching a non-nested one.  Search methods
    automatically take advantage of indexing where available.

    When iterating a table, an object from the Row (see :ref:`RowClassDescr`)
    class is used.  This object allows to read and write data one row at a
    time, as well as to perform queries which are not supported by in-kernel
    syntax (at a much lower speed, of course).

    Objects of this class support access to individual columns via *natural
    naming* through the :attr:`Table.cols` accessor.  Nested columns are
    mapped to Cols instances, and non-nested ones to Column instances.
    See the Column class in :ref:`ColumnClassDescr` for examples of this
    feature.

    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.
    description
        An IsDescription subclass or a dictionary where the keys are the field
        names, and the values the type definitions. In addition, a pure NumPy
        dtype is accepted.  If None, the table metadata is read from disk,
        else, it's taken from previous parameters.
    title
        Sets a TITLE attribute on the HDF5 table entity.
    filters : Filters
        An instance of the Filters class that provides information about the
        desired I/O filters to be applied during the life of this object.
    expectedrows
        A user estimate about the number of rows that will be on table. If not
        provided, the default value is ``EXPECTED_ROWS_TABLE`` (see
        ``tables/parameters.py``).  If you plan to save bigger tables, try
        providing a guess; this will optimize the HDF5 B-Tree creation and
        management process time and memory used.
    chunkshape
        The shape of the data chunk to be read or written as a single HDF5 I/O
        operation. The filters are applied to those chunks of data. Its rank
        for tables has to be 1.  If ``None``, a sensible value is calculated
        based on the `expectedrows` parameter (which is recommended).
    byteorder
        The byteorder of the data *on-disk*, specified as 'little' or 'big'. If
        this is not specified, the byteorder is that of the platform, unless
        you passed a recarray as the `description`, in which case the recarray
        byteorder will be chosen.

    Notes
    -----
    The instance variables below are provided in addition to those in
    Leaf (see :ref:`LeafClassDescr`).  Please note that there are several
    col* dictionaries to ease retrieving information about a column
    directly by its path name, avoiding the need to walk through
    Table.description or Table.cols.


    .. rubric:: Table attributes

    .. attribute:: coldescrs

        Maps the name of a column to its Col description (see
        :ref:`ColClassDescr`).

    .. attribute:: coldflts

        Maps the name of a column to its default value.

    .. attribute:: coldtypes

        Maps the name of a column to its NumPy data type.

    .. attribute:: colindexed

        Is the column which name is used as a key indexed?

    .. attribute:: colinstances

        Maps the name of a column to its Column (see
        :ref:`ColumnClassDescr`) or Cols (see :ref:`ColsClassDescr`)
        instance.

    .. attribute:: colnames

        A list containing the names of *top-level* columns in the table.

    .. attribute:: colpathnames

        A list containing the pathnames of *bottom-level* columns in
        the table.

        These are the leaf columns obtained when walking the table
        description left-to-right, bottom-first. Columns inside a
        nested column have slashes (/) separating name components in
        their pathname.

    .. attribute:: cols

        A Cols instance that provides *natural naming* access to
        non-nested (Column, see :ref:`ColumnClassDescr`) and nested
        (Cols, see :ref:`ColsClassDescr`) columns.

    .. attribute:: coltypes

        Maps the name of a column to its PyTables data type.

    .. attribute:: description

        A Description instance (see :ref:`DescriptionClassDescr`)
        reflecting the structure of the table.

    .. attribute:: extdim

        The index of the enlargeable dimension (always 0 for tables).

    .. attribute:: indexed

        Does this table have any indexed columns?

    .. attribute:: nrows

        The current number of rows in the table.

    """

    # Class identifier.
    _c_classid = 'TABLE'

    _c_classId = previous_api_property('_c_classid')
    _v_objectId = previous_api_property('_v_objectid')

    # Properties
    # ~~~~~~~~~~
    @lazyattr
    def row(self):
        """The associated Row instance (see :ref:`RowClassDescr`)."""

        return tableextension.Row(self)

    @lazyattr
    def dtype(self):
        """The NumPy ``dtype`` that most closely matches this table."""

        return self.description._v_dtype

    # Read-only shorthands
    # ````````````````````

    shape = property(
        lambda self: (self.nrows,), None, None,
        "The shape of this table.")

    rowsize = property(
        lambda self: self.description._v_dtype.itemsize, None, None,
        "The size in bytes of each row in the table.")

    size_in_memory = property(
        lambda self: self.nrows * self.rowsize, None, None,
        """The size of this table's data in bytes when it is fully loaded into
        memory.  This may be used in combination with size_on_disk to calculate
        the compression ratio of the data.""")

    # Lazy attributes
    # ```````````````
    @lazyattr
    def _v_iobuf(self):
        """A buffer for doing I/O."""

        return self._get_container(self.nrowsinbuf)

    @lazyattr
    def _v_wdflts(self):
        """The defaults for writing in recarray format."""

        # First, do a check to see whether we need to set default values
        # different from 0 or not.
        for coldflt in self.coldflts.itervalues():
            if isinstance(coldflt, numpy.ndarray) or coldflt:
                break
        else:
            # No default different from 0 found.  Returning None.
            return None
        wdflts = self._get_container(1)
        for colname, coldflt in self.coldflts.iteritems():
            ra = get_nested_field(wdflts, colname)
            ra[:] = coldflt
        return wdflts

    @lazyattr
    def _colunaligned(self):
        """The pathnames of unaligned, *unidimensional* columns."""
        colunaligned, rarr = [], self._get_container(0)
        for colpathname in self.colpathnames:
            carr = get_nested_field(rarr, colpathname)
            if not carr.flags.aligned and carr.ndim == 1:
                colunaligned.append(colpathname)
        return frozenset(colunaligned)

    # Index-related properties
    # ````````````````````````
    autoindex = _table__autoindex
    """Automatically keep column indexes up to date?

    Setting this value states whether existing indexes should be automatically
    updated after an append operation or recomputed after an index-invalidating
    operation (i.e. removal and modification of rows). The default is true.

    This value gets into effect whenever a column is altered. If you don't have
    automatic indexing activated and you want to do an immediate update use
    :meth:`Table.flush_rows_to_index`; for immediate reindexing of invalidated
    indexes, use :meth:`Table.reindex_dirty`.

    This value is persistent.

    .. versionchanged:: 3.0
       The *autoIndex* property has been renamed into *autoindex*.

    """

    autoIndex = previous_api_property('autoindex')

    indexedcolpathnames = property(
        lambda self: [_colpname for _colpname in self.colpathnames
                      if self.colindexed[_colpname]],
        None, None,
        """List of pathnames of indexed columns in the table.""")

    colindexes = property(
        lambda self: _ColIndexes(
            ((_colpname, self.cols._f_col(_colpname).index)
             for _colpname in self.colpathnames
             if self.colindexed[_colpname])),
        None, None,
        """A dictionary with the indexes of the indexed columns.""")

    _dirtyindexes = property(
        lambda self: self._condition_cache._nailcount > 0,
        None, None,
        """Whether some index in table is dirty.""")

    # Other methods
    # ~~~~~~~~~~~~~
    def __init__(self, parentnode, name,
                 description=None, title="", filters=None,
                 expectedrows=None, chunkshape=None,
                 byteorder=None, _log=True):

        self._v_new = new = description is not None
        """Is this the first time the node has been created?"""
        self._v_new_title = title
        """New title for this node."""
        self._v_new_filters = filters
        """New filter properties for this node."""
        self.extdim = 0   # Tables only have one dimension currently
        """The index of the enlargeable dimension (always 0 for tables)."""
        self._v_recarray = None
        """A structured array to be stored in the table."""
        self._rabyteorder = None
        """The computed byteorder of the self._v_recarray."""
        if expectedrows is None:
            expectedrows = parentnode._v_file.params['EXPECTED_ROWS_TABLE']
        self._v_expectedrows = expectedrows
        """The expected number of rows to be stored in the table."""
        self.nrows = SizeType(0)
        """The current number of rows in the table."""
        self.description = None
        """A Description instance (see :ref:`DescriptionClassDescr`)
        reflecting the structure of the table."""
        self._time64colnames = []
        """The names of ``Time64`` columns."""
        self._strcolnames = []
        """The names of ``String`` columns."""
        self._colenums = {}
        """Maps the name of an enumerated column to its ``Enum`` instance."""
        self._v_chunkshape = None
        """Private storage for the `chunkshape` property of the leaf."""

        self.indexed = False
        """Does this table have any indexed columns?"""
        self._indexedrows = 0
        """Number of rows indexed in disk."""
        self._unsaved_indexedrows = 0
        """Number of rows indexed in memory but still not in disk."""
        self._listoldindexes = []
        """The list of columns with old indexes."""
        self._autoindex = None
        """Private variable that caches the value for autoindex."""

        self.colnames = []
        """A list containing the names of *top-level* columns in the table."""
        self.colpathnames = []
        """A list containing the pathnames of *bottom-level* columns in the
        table.

        These are the leaf columns obtained when walking the
        table description left-to-right, bottom-first.  Columns inside a
        nested column have slashes (/) separating name components in
        their pathname.
        """
        self.colinstances = {}
        """Maps the name of a column to its Column (see
        :ref:`ColumnClassDescr`) or Cols (see :ref:`ColsClassDescr`)
        instance."""
        self.coldescrs = {}
        """Maps the name of a column to its Col description (see
        :ref:`ColClassDescr`)."""
        self.coltypes = {}
        """Maps the name of a column to its PyTables data type."""
        self.coldtypes = {}
        """Maps the name of a column to its NumPy data type."""
        self.coldflts = {}
        """Maps the name of a column to its default value."""
        self.colindexed = {}
        """Is the column which name is used as a key indexed?"""

        self._use_index = False
        """Whether an index can be used or not in a search.  Boolean."""
        self._where_condition = None
        """Condition function and argument list for selection of values."""
        max_slots = parentnode._v_file.params['COND_CACHE_SLOTS']
        self._condition_cache = CacheDict(max_slots)
        """Cache of already compiled conditions."""
        self._exprvars_cache = {}
        """Cache of variables participating in numexpr expressions."""
        self._enabled_indexing_in_queries = True
        """Is indexing enabled in queries?  *Use only for testing.*"""
        self._empty_array_cache = {}
        """Cache of empty arrays."""

        self._v_dtype = None
        """The NumPy datatype fopr this table."""
        self.cols = None
        """
        A Cols instance that provides *natural naming* access to non-nested
        (Column, see :ref:`ColumnClassDescr`) and nested (Cols, see
        :ref:`ColsClassDescr`) columns.
        """
        self._dirtycache = True
        """Whether the data caches are dirty or not. Initially set to yes."""
        self._descflavor = None
        """Temporarily keeps the flavor of a description with data."""

        # Initialize this object in case is a new Table

        # Try purely descriptive description objects.
        if new and isinstance(description, dict):
            # Dictionary case
            self.description = Description(description)
        elif new and (type(description) == type(IsDescription)
                      and issubclass(description, IsDescription)):
            # IsDescription subclass case
            descr = description()
            self.description = Description(descr.columns)
        elif new and isinstance(description, Description):
            # It is a Description instance already
            self.description = description

        # No description yet?
        if new and self.description is None:
            # Try NumPy dtype instances
            if isinstance(description, numpy.dtype):
                self.description, self._rabyteorder = \
                    descr_from_dtype(description)

        # No description yet?
        if new and self.description is None:
            # Try structured array description objects.
            try:
                self._descflavor = flavor = flavor_of(description)
            except TypeError:  # probably not an array
                pass
            else:
                if flavor == 'python':
                    nparray = numpy.rec.array(description)
                else:
                    nparray = array_as_internal(description, flavor)
                self.nrows = nrows = SizeType(nparray.size)
                # If `self._v_recarray` is set, it will be used as the
                # initial buffer.
                if nrows > 0:
                    self._v_recarray = nparray
                self.description, self._rabyteorder = \
                    descr_from_dtype(nparray.dtype)

        # No description yet?
        if new and self.description is None:
            raise TypeError(
                "the ``description`` argument is not of a supported type: "
                "``IsDescription`` subclass, ``Description`` instance, "
                "dictionary, or structured array")

        # Check the chunkshape parameter
        if new and chunkshape is not None:
            if isinstance(chunkshape, (int, numpy.integer, long)):
                chunkshape = (chunkshape,)
            try:
                chunkshape = tuple(chunkshape)
            except TypeError:
                raise TypeError(
                    "`chunkshape` parameter must be an integer or sequence "
                    "and you passed a %s" % type(chunkshape))
            if len(chunkshape) != 1:
                raise ValueError("`chunkshape` rank (length) must be 1: %r"
                                 % (chunkshape,))
            self._v_chunkshape = tuple(SizeType(s) for s in chunkshape)

        super(Table, self).__init__(parentnode, name, new, filters,
                                    byteorder, _log)

    def _g_post_init_hook(self):
        # We are putting here the index-related issues
        # as well as filling general info for table
        # This is needed because we need first the index objects created

        # First, get back the flavor of input data (if any) for
        # `Leaf._g_post_init_hook()`.
        self._flavor, self._descflavor = self._descflavor, None
        super(Table, self)._g_post_init_hook()

        # Create a cols accessor.
        self.cols = Cols(self, self.description)

        # Place the `Cols` and `Column` objects into `self.colinstances`.
        colinstances, cols = self.colinstances, self.cols
        for colpathname in self.description._v_pathnames:
            colinstances[colpathname] = cols._g_col(colpathname)

        if self._v_new:
            # Columns are never indexed on creation.
            self.colindexed = dict((cpn, False) for cpn in self.colpathnames)
            return

        # The following code is only for opened tables.

        # Do the indexes group exist?
        indexesgrouppath = _index_pathname_of(self)
        igroup = indexesgrouppath in self._v_file
        oldindexes = False
        for colobj in self.description._f_walk(type="Col"):
            colname = colobj._v_pathname
            # Is this column indexed?
            if igroup:
                indexname = _index_pathname_of_column(self, colname)
                indexed = indexname in self._v_file
                self.colindexed[colname] = indexed
                if indexed:
                    column = self.cols._g_col(colname)
                    indexobj = column.index
                    if isinstance(indexobj, OldIndex):
                        indexed = False  # Not a vaild index
                        oldindexes = True
                        self._listoldindexes.append(colname)
                    else:
                        # Tell the condition cache about columns with dirty
                        # indexes.
                        if indexobj.dirty:
                            self._condition_cache.nail()
            else:
                indexed = False
                self.colindexed[colname] = False
            if indexed:
                self.indexed = True

        if oldindexes:  # this should only appear under 2.x Pro
            warnings.warn(
                "table ``%s`` has column indexes with PyTables 1.x format. "
                "Unfortunately, this format is not supported in "
                "PyTables 2.x series. Note that you can use the "
                "``ptrepack`` utility in order to recreate the indexes. "
                "The 1.x indexed columns found are: %s" %
                (self._v_pathname, self._listoldindexes),
                OldIndexWarning)

        # It does not matter to which column 'indexobj' belongs,
        # since their respective index objects share
        # the same number of elements.
        if self.indexed:
            self._indexedrows = indexobj.nelements
            self._unsaved_indexedrows = self.nrows - self._indexedrows
            # Put the autoindex value in a cache variable
            self._autoindex = self.autoindex

    _g_postInitHook = previous_api(_g_post_init_hook)

    def _getemptyarray(self, dtype):
        # Acts as a cache for empty arrays
        key = dtype
        if key in self._empty_array_cache:
            return self._empty_array_cache[key]
        else:
            self._empty_array_cache[
                key] = arr = numpy.empty(shape=0, dtype=key)
            return arr

    def _get_container(self, shape):
        "Get the appropriate buffer for data depending on table nestedness."

        # This is *much* faster than the numpy.rec.array counterpart
        return numpy.empty(shape=shape, dtype=self._v_dtype)

    def _get_type_col_names(self, type_):
        """Returns a list containing 'type_' column names."""

        return [colobj._v_pathname
                for colobj in self.description._f_walk('Col')
                if colobj.type == type_]

    _getTypeColNames = previous_api(_get_type_col_names)

    def _get_enum_map(self):
        """Return mapping from enumerated column names to `Enum` instances."""

        enumMap = {}
        for colobj in self.description._f_walk('Col'):
            if colobj.kind == 'enum':
                enumMap[colobj._v_pathname] = colobj.enum
        return enumMap

    _getEnumMap = previous_api(_get_enum_map)

    def _g_create(self):
        """Create a new table on disk."""

        # Warning against assigning too much columns...
        # F. Alted 2005-06-05
        maxColumns = self._v_file.params['MAX_COLUMNS']
        if (len(self.description._v_names) > maxColumns):
            warnings.warn(
                "table ``%s`` is exceeding the recommended "
                "maximum number of columns (%d); "
                "be ready to see PyTables asking for *lots* of memory "
                "and possibly slow I/O" % (self._v_pathname, maxColumns),
                PerformanceWarning)

        # 1. Create the HDF5 table (some parameters need to be computed).

        # Fix the byteorder of the recarray and update the number of
        # expected rows if necessary
        if self._v_recarray is not None:
            self._v_recarray = self._g_fix_byteorder_data(self._v_recarray,
                                                          self._rabyteorder)
            if len(self._v_recarray) > self._v_expectedrows:
                self._v_expectedrows = len(self._v_recarray)
        # Compute a sensible chunkshape
        if self._v_chunkshape is None:
            self._v_chunkshape = self._calc_chunkshape(
                self._v_expectedrows, self.rowsize, self.rowsize)
        # Correct the byteorder, if still needed
        if self.byteorder is None:
            self.byteorder = sys.byteorder

        # Cache some data which is already in the description.
        # This is necessary to happen before creation time in order
        # to be able to populate the self._v_wdflts
        self._cache_description_data()

        # After creating the table, ``self._v_objectid`` needs to be
        # set because it is needed for setting attributes afterwards.
        self._v_objectid = self._create_table(
            self._v_new_title, self.filters.complib or '', obversion)
        self._v_recarray = None  # not useful anymore
        self._rabyteorder = None  # not useful anymore

        # 2. Compute or get chunk shape and buffer size parameters.
        self.nrowsinbuf = self._calc_nrowsinbuf()

        # 3. Get field fill attributes from the table description and
        #    set them on disk.
        if self._v_file.params['PYTABLES_SYS_ATTRS']:
            set_attr = self._v_attrs._g__setattr
            for i, colobj in enumerate(self.description._f_walk(type="Col")):
                fieldname = "FIELD_%d_FILL" % i
                set_attr(fieldname, colobj.dflt)

        return self._v_objectid

    def _g_open(self):
        """Opens a table from disk and read the metadata on it.

        Creates an user description on the flight to easy the access to
        the actual data.

        """

        # 1. Open the HDF5 table and get some data from it.
        self._v_objectid, description, chunksize = self._get_info()
        self._v_expectedrows = self.nrows  # the actual number of rows

        # 2. Create an instance description to host the record fields.
        validate = not self._v_file._isPTFile  # only for non-PyTables files
        self.description = Description(description, validate=validate)

        # 3. Compute or get chunk shape and buffer size parameters.
        if chunksize == 0:
            self._v_chunkshape = self._calc_chunkshape(
                self._v_expectedrows, self.rowsize, self.rowsize)
        else:
            self._v_chunkshape = (chunksize,)
        self.nrowsinbuf = self._calc_nrowsinbuf()

        # 4. If there are field fill attributes, get them from disk and
        #    set them in the table description.
        if self._v_file.params['PYTABLES_SYS_ATTRS']:
            if "FIELD_0_FILL" in self._v_attrs._f_list("sys"):
                i = 0
                get_attr = self._v_attrs.__getattr__
                for objcol in self.description._f_walk(type="Col"):
                    colname = objcol._v_pathname
                    # Get the default values for each column
                    fieldname = "FIELD_%s_FILL" % i
                    defval = get_attr(fieldname)
                    if defval is not None:
                        objcol.dflt = defval
                    else:
                        warnings.warn("could not load default value "
                                      "for the ``%s`` column of table ``%s``; "
                                      "using ``%r`` instead"
                                      % (colname, self._v_pathname,
                                          objcol.dflt))
                        defval = objcol.dflt
                    i += 1

                # Set also the correct value in the desc._v_dflts dictionary
                for descr in self.description._f_walk(type="Description"):
                    names = descr._v_names
                    for i in range(len(names)):
                        objcol = descr._v_colobjects[names[i]]
                        if isinstance(objcol, Col):
                            descr._v_dflts[objcol._v_name] = objcol.dflt

        # 5. Cache some data which is already in the description.
        self._cache_description_data()

        return self._v_objectid

    def _cache_description_data(self):
        """Cache some data which is already in the description.

        Some information is extracted from `self.description` to build
        some useful (but redundant) structures:

        * `self.colnames`
        * `self.colpathnames`
        * `self.coldescrs`
        * `self.coltypes`
        * `self.coldtypes`
        * `self.coldflts`
        * `self._v_dtype`
        * `self._time64colnames`
        * `self._strcolnames`
        * `self._colenums`

        """

        self.colnames = list(self.description._v_names)
        self.colpathnames = [
            col._v_pathname for col in self.description._f_walk()
            if not hasattr(col, '_v_names')]  # bottom-level

        # Find ``time64`` column names.
        self._time64colnames = self._get_type_col_names('time64')
        # Find ``string`` column names.
        self._strcolnames = self._get_type_col_names('string')
        # Get a mapping of enumerated columns to their `Enum` instances.
        self._colenums = self._get_enum_map()

        # Get info about columns
        for colobj in self.description._f_walk(type="Col"):
            colname = colobj._v_pathname
            # Get the column types, types and defaults
            self.coldescrs[colname] = colobj
            self.coltypes[colname] = colobj.type
            self.coldtypes[colname] = colobj.dtype
            self.coldflts[colname] = colobj.dflt

        # Assign _v_dtype for this table
        self._v_dtype = self.description._v_dtype

    _cacheDescriptionData = previous_api(_cache_description_data)

    def _get_column_instance(self, colpathname):
        """Get the instance of the column with the given `colpathname`.

        If the column does not exist in the table, a `KeyError` is
        raised.

        """

        try:
            return _reduce(getattr, colpathname.split('/'), self.description)
        except AttributeError:
            raise KeyError("table ``%s`` does not have a column named ``%s``"
                           % (self._v_pathname, colpathname))

    _getColumnInstance = previous_api(_get_column_instance)

    _check_column = _get_column_instance

    def _disable_indexing_in_queries(self):
        """Force queries not to use indexing.

        *Use only for testing.*

        """

        if not self._enabled_indexing_in_queries:
            return  # already disabled
        # The nail avoids setting/getting compiled conditions in/from
        # the cache where indexing is used.
        self._condition_cache.nail()
        self._enabled_indexing_in_queries = False

    _disableIndexingInQueries = previous_api(_disable_indexing_in_queries)

    def _enable_indexing_in_queries(self):
        """Allow queries to use indexing.

        *Use only for testing.*

        """

        if self._enabled_indexing_in_queries:
            return  # already enabled
        self._condition_cache.unnail()
        self._enabled_indexing_in_queries = True

    _enableIndexingInQueries = previous_api(_enable_indexing_in_queries)

    def _required_expr_vars(self, expression, uservars, depth=1):
        """Get the variables required by the `expression`.

        A new dictionary defining the variables used in the `expression`
        is returned.  Required variables are first looked up in the
        `uservars` mapping, then in the set of top-level columns of the
        table.  Unknown variables cause a `NameError` to be raised.

        When `uservars` is `None`, the local and global namespace where
        the API callable which uses this method is called is sought
        instead.  This mechanism will not work as expected if this
        method is not used *directly* from an API callable.  To disable
        this mechanism, just specify a mapping as `uservars`.

        Nested columns and columns from other tables are not allowed
        (`TypeError` and `ValueError` are raised, respectively).  Also,
        non-column variable values are converted to NumPy arrays.

        `depth` specifies the depth of the frame in order to reach local
        or global variables.

        """

        # Get the names of variables used in the expression.
        exprvarscache = self._exprvars_cache
        if not expression in exprvarscache:
            # Protection against growing the cache too much
            if len(exprvarscache) > 256:
                # Remove 10 (arbitrary) elements from the cache
                for k in exprvarscache.keys()[:10]:
                    del exprvarscache[k]
            cexpr = compile(expression, '<string>', 'eval')
            exprvars = [var for var in cexpr.co_names
                        if var not in ['None', 'False', 'True']
                        and var not in numexpr_functions]
            exprvarscache[expression] = exprvars
        else:
            exprvars = exprvarscache[expression]

        # Get the local and global variable mappings of the user frame
        # if no mapping has been explicitly given for user variables.
        user_locals, user_globals = {}, {}
        if uservars is None:
            # We use specified depth to get the frame where the API
            # callable using this method is called.  For instance:
            #
            # * ``table._required_expr_vars()`` (depth 0) is called by
            # * ``table._where()`` (depth 1) is called by
            # * ``table.where()`` (depth 2) is called by
            # * user-space functions (depth 3)
            user_frame = sys._getframe(depth)
            user_locals = user_frame.f_locals
            user_globals = user_frame.f_globals

        colinstances = self.colinstances
        tblfile, tblpath = self._v_file, self._v_pathname
        # Look for the required variables first among the ones
        # explicitly provided by the user, then among implicit columns,
        # then among external variables (only if no explicit variables).
        reqvars = {}
        for var in exprvars:
            # Get the value.
            if uservars is not None and var in uservars:
                val = uservars[var]
            elif var in colinstances:
                val = colinstances[var]
            elif uservars is None and var in user_locals:
                val = user_locals[var]
            elif uservars is None and var in user_globals:
                val = user_globals[var]
            else:
                raise NameError("name ``%s`` is not defined" % var)

            # Check the value.
            if hasattr(val, 'pathname'):  # non-nested column
                if val.shape[1:] != ():
                    raise NotImplementedError(
                        "variable ``%s`` refers to "
                        "a multidimensional column, "
                        "not yet supported in conditions, sorry" % var)
                if (val._table_file is not tblfile or
                        val._table_path != tblpath):
                    raise ValueError("variable ``%s`` refers to a column "
                                     "which is not part of table ``%s``"
                                     % (var, tblpath))
                if val.dtype.str[1:] == 'u8':
                    raise NotImplementedError(
                        "variable ``%s`` refers to "
                        "a 64-bit unsigned integer column, "
                        "not yet supported in conditions, sorry; "
                        "please use regular Python selections" % var)
            elif hasattr(val, '_v_colpathnames'):  # nested column
                raise TypeError(
                    "variable ``%s`` refers to a nested column, "
                    "not allowed in conditions" % var)
            else:  # only non-column values are converted to arrays
                # XXX: not 100% sure about this
                if isinstance(val, unicode):
                    val = numpy.asarray(val.encode('ascii'))
                else:
                    val = numpy.asarray(val)
            reqvars[var] = val
        return reqvars

    _requiredExprVars = previous_api(_required_expr_vars)

    def _get_condition_key(self, condition, condvars):
        """Get the condition cache key for `condition` with `condvars`.

        Currently, the key is a tuple of `condition`, column variables
        names, normal variables names, column paths and variable paths
        (all are tuples).

        """

        # Variable names for column and normal variables.
        colnames, varnames = [], []
        # Column paths and types for each of the previous variable.
        colpaths, vartypes = [], []
        for (var, val) in condvars.iteritems():
            if hasattr(val, 'pathname'):  # column
                colnames.append(var)
                colpaths.append(val.pathname)
            else:  # array
                try:
                    varnames.append(var)
                    vartypes.append(numexpr_getType(val))  # expensive
                except ValueError:
                    # This is more clear than the error given by Numexpr.
                    raise TypeError("variable ``%s`` has data type ``%s``, "
                                    "not allowed in conditions"
                                    % (var, val.dtype.name))
        colnames, varnames = tuple(colnames), tuple(varnames)
        colpaths, vartypes = tuple(colpaths), tuple(vartypes)
        condkey = (condition, colnames, varnames, colpaths, vartypes)
        return condkey

    _getConditionKey = previous_api(_get_condition_key)

    def _compile_condition(self, condition, condvars):
        """Compile the `condition` and extract usable index conditions.

        This method returns an instance of ``CompiledCondition``.  See
        the ``compile_condition()`` function in the ``conditions``
        module for more information about the compilation process.

        This method makes use of the condition cache when possible.

        """

        # Look up the condition in the condition cache.
        condcache = self._condition_cache
        condkey = self._get_condition_key(condition, condvars)
        compiled = condcache.get(condkey)
        if compiled:
            return compiled.with_replaced_vars(condvars)  # bingo!

        # Bad luck, the condition must be parsed and compiled.
        # Fortunately, the key provides some valuable information. ;)
        (condition, colnames, varnames, colpaths, vartypes) = condkey

        # Extract more information from referenced columns.
        typemap = dict(zip(varnames, vartypes))  # start with normal variables
        indexedcols = []
        for colname in colnames:
            col = condvars[colname]

            # Extract types from *all* the given variables.
            coltype = col.dtype.type
            typemap[colname] = _nxtype_from_nptype[coltype]

            # Get the set of columns with usable indexes.
            if (self._enabled_indexing_in_queries  # not test in-kernel searches
                    and self.colindexed[col.pathname] and not col.index.dirty):
                indexedcols.append(colname)

        indexedcols = frozenset(indexedcols)
        # Now let ``compile_condition()`` do the Numexpr-related job.
        compiled = compile_condition(condition, typemap, indexedcols)

        # Check that there actually are columns in the condition.
        if not set(compiled.parameters).intersection(set(colnames)):
            raise ValueError("there are no columns taking part "
                             "in condition ``%s``" % (condition,))

        # Store the compiled condition in the cache and return it.
        condcache[condkey] = compiled
        return compiled.with_replaced_vars(condvars)

    _compileCondition = previous_api(_compile_condition)

    def will_query_use_indexing(self, condition, condvars=None):
        """Will a query for the condition use indexing?

        The meaning of the condition and *condvars* arguments is the same as in
        the :meth:`Table.where` method. If condition can use indexing, this
        method returns a frozenset with the path names of the columns whose
        index is usable. Otherwise, it returns an empty list.

        This method is mainly intended for testing. Keep in mind that changing
        the set of indexed columns or their dirtiness may make this method
        return different values for the same arguments at different times.

        """

        # Compile the condition and extract usable index conditions.
        condvars = self._required_expr_vars(condition, condvars, depth=2)
        compiled = self._compile_condition(condition, condvars)
        # Return the columns in indexed expressions
        idxcols = [condvars[var].pathname for var in compiled.index_variables]
        return frozenset(idxcols)

    willQueryUseIndexing = previous_api(will_query_use_indexing)

    def where(self, condition, condvars=None,
              start=None, stop=None, step=None):
        """Iterate over values fulfilling a condition.

        This method returns a Row iterator (see :ref:`RowClassDescr`) which
        only selects rows in the table that satisfy the given condition (an
        expression-like string).

        The condvars mapping may be used to define the variable names appearing
        in the condition. condvars should consist of identifier-like strings
        pointing to Column (see :ref:`ColumnClassDescr`) instances *of this
        table*, or to other values (which will be converted to arrays). A
        default set of condition variables is provided where each top-level,
        non-nested column with an identifier-like name appears. Variables in
        condvars override the default ones.

        When condvars is not provided or None, the current local and global
        namespace is sought instead of condvars. The previous mechanism is
        mostly intended for interactive usage. To disable it, just specify a
        (maybe empty) mapping as condvars.

        If a range is supplied (by setting some of the start, stop or step
        parameters), only the rows in that range and fulfilling the condition
        are used. The meaning of the start, stop and step parameters is the
        same as for Python slices.

        When possible, indexed columns participating in the condition will be
        used to speed up the search. It is recommended that you place the
        indexed columns as left and out in the condition as possible. Anyway,
        this method has always better performance than regular Python
        selections on the table.

        You can mix this method with regular Python selections in order to
        support even more complex queries. It is strongly recommended that you
        pass the most restrictive condition as the parameter to this method if
        you want to achieve maximum performance.

        .. warning::

            When in the middle of a table row iterator, you should not
            use methods that can change the number of rows in the table
            (like :meth:`Table.append` or :meth:`Table.remove_rows`) or
            unexpected errors will happen.

        Examples
        --------

        ::

            >>> passvalues = [ row['col3'] for row in
            ...                table.where('(col1 > 0) & (col2 <= 20)', step=5)
            ...                if your_function(row['col2']) ]
            >>> print("Values that pass the cuts:", passvalues)

        .. note::

            A special care should be taken when the query condition includes
            string literals.  Indeed Python 2 string literals are string of
            bytes while Python 3 strings are unicode objects.

            Let's assume that the table ``table`` has the following
            structure::

                class Record(IsDescription):
                    col1 = StringCol(4)  # 4-character String of bytes
                    col2 = IntCol()
                    col3 = FloatCol()

            The type of "col1" do not change depending on the Python version
            used (of course) and it always corresponds to strings of bytes.

            Any condition involving "col1" should be written using the
            appropriate type for string literals in order to avoid
            :exc:`TypeError`\ s.

            The code below will work fine in Python 2 but will fail with a
            :exc:`TypeError` in Python 3::

                condition = 'col1 == "AAAA"'
                for record in table.where(condition):  # TypeError in Python3
                    # do something with "record"

            The reason is that in Python 3 "condition" implies a comparison
            between a string of bytes ("col1" contents) and an unicode literal
            ("AAAA").

            The correct way to write the condition is::

                condition = 'col1 == b"AAAA"'

        .. versionchanged:: 3.0
           The start, stop and step parameters now behave like in slice.

        """

        return self._where(condition, condvars, start, stop, step)

    def _where(self, condition, condvars, start=None, stop=None, step=None):
        """Low-level counterpart of `self.where()`."""

        if profile:
            tref = time()
        if profile:
            show_stats("Entering table._where", tref)
        # Adjust the slice to be used.
        (start, stop, step) = self._process_range_read(start, stop, step)
        if start >= stop:  # empty range, reset conditions
            self._use_index = False
            self._where_condition = None
            return iter([])

        # Compile the condition and extract usable index conditions.
        condvars = self._required_expr_vars(condition, condvars, depth=3)
        compiled = self._compile_condition(condition, condvars)

        # Can we use indexes?
        if compiled.index_expressions:
            chunkmap = _table__where_indexed(
                self, compiled, condition, condvars, start, stop, step)
            if not isinstance(chunkmap, numpy.ndarray):
                # If it is not a NumPy array it should be an iterator
                # Reset conditions
                self._use_index = False
                self._where_condition = None
                # ...and return the iterator
                if chunkmap is not None:
                    return chunkmap
        else:
            chunkmap = None  # default to an in-kernel query

        args = [condvars[param] for param in compiled.parameters]
        self._where_condition = (compiled.function, args)
        row = tableextension.Row(self)
        if profile:
            show_stats("Exiting table._where", tref)
        return row._iter(start, stop, step, chunkmap=chunkmap)

    def read_where(self, condition, condvars=None, field=None,
                   start=None, stop=None, step=None):
        """Read table data fulfilling the given *condition*.

        This method is similar to :meth:`Table.read`, having their common
        arguments and return values the same meanings. However, only the rows
        fulfilling the *condition* are included in the result.

        The meaning of the other arguments is the same as in the
        :meth:`Table.where` method.

        """

        self._g_check_open()
        coords = [p.nrow for p in
                  self._where(condition, condvars, start, stop, step)]
        self._where_condition = None  # reset the conditions
        if len(coords) > 1:
            cstart, cstop = coords[0], coords[-1] + 1
            if cstop - cstart == len(coords):
                # Chances for monotonically increasing row values. Refine.
                inc_seq = numpy.alltrue(
                    numpy.arange(cstart, cstop) == numpy.array(coords))
                if inc_seq:
                    return self.read(cstart, cstop, field=field)
        return self.read_coordinates(coords, field)

    readWhere = previous_api(read_where)

    def append_where(self, dstTable, condition, condvars=None,
                     start=None, stop=None, step=None):
        """Append rows fulfilling the condition to the dstTable table.

        dstTable must be capable of taking the rows resulting from the query,
        i.e. it must have columns with the expected names and compatible
        types. The meaning of the other arguments is the same as in the
        :meth:`Table.where` method.

        The number of rows appended to dstTable is returned as a result.

        .. versionchanged:: 3.0
           The *whereAppend* method has been renamed into *append_where*.

        """

        self._g_check_open()

        # Check that the destination file is not in read-only mode.
        dstTable._v_file._check_writable()

        # Row objects do not support nested columns, so we must iterate
        # over the flat column paths.  When rows support nesting,
        # ``self.colnames`` can be directly iterated upon.
        colNames = [colName for colName in self.colpathnames]
        dstRow = dstTable.row
        nrows = 0
        for srcRow in self._where(condition, condvars, start, stop, step):
            for colName in colNames:
                dstRow[colName] = srcRow[colName]
            dstRow.append()
            nrows += 1
        dstTable.flush()
        return nrows

    whereAppend = previous_api(append_where)

    def get_where_list(self, condition, condvars=None, sort=False,
                       start=None, stop=None, step=None):
        """Get the row coordinates fulfilling the given condition.

        The coordinates are returned as a list of the current flavor.  sort
        means that you want to retrieve the coordinates ordered. The default is
        to not sort them.

        The meaning of the other arguments is the same as in the
        :meth:`Table.where` method.

        """

        self._g_check_open()

        coords = [p.nrow for p in
                  self._where(condition, condvars, start, stop, step)]
        coords = numpy.array(coords, dtype=SizeType)
        # Reset the conditions
        self._where_condition = None
        if sort:
            coords = numpy.sort(coords)
        return internal_to_flavor(coords, self.flavor)

    getWhereList = previous_api(get_where_list)

    def itersequence(self, sequence):
        """Iterate over a sequence of row coordinates.

        Notes
        -----
        This iterator can be nested (see :meth:`Table.where` for an example).

        """

        if not hasattr(sequence, '__getitem__'):
            raise TypeError(("Wrong 'sequence' parameter type. Only sequences "
                             "are suported."))
        # start, stop and step are necessary for the new iterator for
        # coordinates, and perhaps it would be useful to add them as
        # parameters in the future (not now, because I've just removed
        # the `sort` argument for 2.1).
        #
        # *Important note*: Negative values for step are not supported
        # for the general case, but only for the itersorted() and
        # read_sorted() purposes!  The self._process_range_read will raise
        # an appropiate error.
        # F. Alted 2008-09-18
        # A.V. 20130513: _process_range_read --> _process_range
        (start, stop, step) = self._process_range(None, None, None)
        if (start > stop) or (len(sequence) == 0):
            return iter([])
        row = tableextension.Row(self)
        return row._iter(start, stop, step, coords=sequence)

    def _check_sortby_csi(self, sortby, checkCSI):
        if isinstance(sortby, Column):
            icol = sortby
        elif isinstance(sortby, str):
            icol = self.cols._f_col(sortby)
        else:
            raise TypeError(
                "`sortby` can only be a `Column` or string object, "
                "but you passed an object of type: %s" % type(sortby))
        if icol.is_indexed and icol.index.kind == "full":
            if checkCSI and not icol.index.is_csi:
                # The index exists, but it is not a CSI one.
                raise ValueError(
                    "Field `%s` must have associated a CSI index "
                    "in table `%s`, but the existing one is not. "
                    % (sortby, self))
            return icol.index
        else:
            raise ValueError(
                "Field `%s` must have associated a 'full' index "
                "in table `%s`." % (sortby, self))

    _check_sortby_CSI = previous_api(_check_sortby_csi)

    def itersorted(self, sortby, checkCSI=False,
                   start=None, stop=None, step=None):
        """Iterate table data following the order of the index of sortby
        column.

        The sortby column must have associated a full index.  If you want to
        ensure a fully sorted order, the index must be a CSI one.  You may want
        to use the checkCSI argument in order to explicitly check for the
        existence of a CSI index.

        The meaning of the start, stop and step arguments is the same as in
        :meth:`Table.read`.

        .. versionchanged:: 3.0
           If the *start* parameter is provided and *stop* is None then the
           table is iterated from *start* to the last line.
           In PyTables < 3.0 only one element was returned.

        """

        index = self._check_sortby_csi(sortby, checkCSI)
        # Adjust the slice to be used.
        (start, stop, step) = self._process_range(start, stop, step,
                                                  warn_negstep=False)
        if (start > stop and 0 < step) or (start < stop and 0 > step):
            # Fall-back action is to return an empty iterator
            return iter([])
        row = tableextension.Row(self)
        return row._iter(start, stop, step, coords=index)

    def read_sorted(self, sortby, checkCSI=False, field=None,
                    start=None, stop=None, step=None):
        """Read table data following the order of the index of sortby column.

        The sortby column must have associated a full index.  If you want to
        ensure a fully sorted order, the index must be a CSI one.  You may want
        to use the checkCSI argument in order to explicitly check for the
        existence of a CSI index.

        If field is supplied only the named column will be selected.  If the
        column is not nested, an *array* of the current flavor will be
        returned; if it is, a *structured array* will be used instead.  If no
        field is specified, all the columns will be returned in a structured
        array of the current flavor.

        The meaning of the start, stop and step arguments is the same as in
        :meth:`Table.read`.

        .. versionchanged:: 3.0
           The start, stop and step parameters now behave like in slice.

        """

        self._g_check_open()
        index = self._check_sortby_csi(sortby, checkCSI)
        coords = index[start:stop:step]
        return self.read_coordinates(coords, field)

    readSorted = previous_api(read_sorted)

    def iterrows(self, start=None, stop=None, step=None):
        """Iterate over the table using a Row instance.

        If a range is not supplied, *all the rows* in the table are iterated
        upon - you can also use the :meth:`Table.__iter__` special method for
        that purpose. If you want to iterate over a given *range of rows* in
        the table, you may use the start, stop and step parameters.

        .. warning::

            When in the middle of a table row iterator, you should not
            use methods that can change the number of rows in the table
            (like :meth:`Table.append` or :meth:`Table.remove_rows`) or
            unexpected errors will happen.

        See Also
        --------
        tableextension.Row : the table row iterator and field accessor

        Examples
        --------

        ::

            result = [ row['var2'] for row in table.iterrows(step=5)
                                                    if row['var1'] <= 20 ]

        Notes
        -----
        This iterator can be nested (see :meth:`Table.where` for an example).

        .. versionchanged:: 3.0
           If the *start* parameter is provided and *stop* is None then the
           table is iterated from *start* to the last line.
           In PyTables < 3.0 only one element was returned.

        """
        (start, stop, step) = self._process_range(start, stop, step,
                                                  warn_negstep=False)
        if (start > stop and 0 < step) or (start < stop and 0 > step):
            # Fall-back action is to return an empty iterator
            return iter([])
        row = tableextension.Row(self)
        return row._iter(start, stop, step)

    def __iter__(self):
        """Iterate over the table using a Row instance.

        This is equivalent to calling :meth:`Table.iterrows` with default
        arguments, i.e. it iterates over *all the rows* in the table.

        See Also
        --------
        tableextension.Row : the table row iterator and field accessor

        Examples
        --------

        ::

            result = [ row['var2'] for row in table if row['var1'] <= 20 ]

        Which is equivalent to::

            result = [ row['var2'] for row in table.iterrows()
                                                    if row['var1'] <= 20 ]

        Notes
        -----
        This iterator can be nested (see :meth:`Table.where` for an example).

        """

        return self.iterrows()

    def _read(self, start, stop, step, field=None, out=None):
        """Read a range of rows and return an in-memory object."""

        select_field = None
        if field:
            if field not in self.coldtypes:
                if field in self.description._v_names:
                    # Remember to select this field
                    select_field = field
                    field = None
                else:
                    raise KeyError(("Field {0} not found in table "
                                    "{1}").format(field, self))
            else:
                # The column hangs directly from the top
                dtype_field = self.coldtypes[field]

        # Return a rank-0 array if start > stop
        if (start >= stop and 0 < step) or (start <= stop and 0 > step):
            if field is None:
                nra = self._get_container(0)
                return nra
            return numpy.empty(shape=0, dtype=dtype_field)

        nrows = len(xrange(start, stop, step))

        if out is None:
            # Compute the shape of the resulting column object
            if field:
                # Create a container for the results
                result = numpy.empty(shape=nrows, dtype=dtype_field)
            else:
                # Recarray case
                result = self._get_container(nrows)
        else:
            # there is no fast way to byteswap, since different columns may
            # have different byteorders
            if not out.dtype.isnative:
                raise ValueError(("output array must be in system's byteorder "
                                  "or results will be incorrect"))
            if field:
                bytes_required = dtype_field.itemsize * nrows
            else:
                bytes_required = self.rowsize * nrows
            if bytes_required != out.nbytes:
                raise ValueError(('output array size invalid, got {0} bytes, '
                                  'need {1} bytes').format(out.nbytes,
                                                           bytes_required))
            if not out.flags['C_CONTIGUOUS']:
                raise ValueError('output array not C contiguous')
            result = out

        # Call the routine to fill-up the resulting array
        if step == 1 and not field:
            # This optimization works three times faster than
            # the row._fill_col method (up to 170 MB/s on a pentium IV @ 2GHz)
            self._read_records(start, stop - start, result)
        # Warning!: _read_field_name should not be used until
        # H5TBread_fields_name in tableextension will be finished
        # F. Alted 2005/05/26
        # XYX Ho implementem per a PyTables 2.0??
        elif field and step > 15 and 0:
            # For step>15, this seems to work always faster than row._fill_col.
            self._read_field_name(result, start, stop, step, field)
        else:
            self.row._fill_col(result, start, stop, step, field)

        if select_field:
            return result[select_field]
        else:
            return result

    def read(self, start=None, stop=None, step=None, field=None, out=None):
        """Get data in the table as a (record) array.

        The start, stop and step parameters can be used to select only
        a *range of rows* in the table. Their meanings are the same as
        in the built-in Python slices.

        If field is supplied only the named column will be selected.
        If the column is not nested, an *array* of the current flavor
        will be returned; if it is, a *structured array* will be used
        instead.  If no field is specified, all the columns will be
        returned in a structured array of the current flavor.

        Columns under a nested column can be specified in the field
        parameter by using a slash character (/) as a separator (e.g.
        'position/x').

        The out parameter may be used to specify a NumPy array to
        receive the output data.  Note that the array must have the
        same size as the data selected with the other parameters.
        Note that the array's datatype is not checked and no type
        casting is performed, so if it does not match the datatype on
        disk, the output will not be correct.

        When specifying a single nested column with the field parameter,
        and supplying an output buffer with the out parameter, the
        output buffer must contain all columns in the table.
        The data in all columns will be read into the output buffer.
        However, only the specified nested column will be returned from
        the method call.

        When data is read from disk in NumPy format, the output will be
        in the current system's byteorder, regardless of how it is
        stored on disk. If the out parameter is specified, the output
        array also must be in the current system's byteorder.

        .. versionchanged:: 3.0
           Added the *out* parameter.  Also the start, stop and step
           parameters now behave like in slice.

        Examples
        --------

        Reading the entire table::

            t.read()

        Reading record n. 6::

            t.read(6, 7)

        Reading from record n. 6 to the end of the table::

            t.read(6)

        """

        self._g_check_open()

        if field:
            self._check_column(field)

        if out is not None and self.flavor != 'numpy':
            msg = ("Optional 'out' argument may only be supplied if array "
                   "flavor is 'numpy', currently is {0}").format(self.flavor)
            raise TypeError(msg)

        #(start, stop, step) = self._process_range_read(start, stop, step,
        (start, stop, step) = self._process_range(start, stop, step,
                                                  warn_negstep=False)

        arr = self._read(start, stop, step, field, out)
        return internal_to_flavor(arr, self.flavor)

    def _read_coordinates(self, coords, field=None):
        """Private part of `read_coordinates()` with no flavor conversion."""

        coords = self._point_selection(coords)

        ncoords = len(coords)
        # Create a read buffer only if needed
        if field is None or ncoords > 0:
            # Doing a copy is faster when ncoords is small (<1000)
            if ncoords < min(1000, self.nrowsinbuf):
                result = self._v_iobuf[:ncoords].copy()
            else:
                result = self._get_container(ncoords)

        # Do the real read
        if ncoords > 0:
            # Turn coords into an array of coordinate indexes, if necessary
            if not (isinstance(coords, numpy.ndarray) and
                    coords.dtype.type is _npsizetype and
                    coords.flags.contiguous and
                    coords.flags.aligned):
                # Get a contiguous and aligned coordinate array
                coords = numpy.array(coords, dtype=SizeType)
            self._read_elements(coords, result)

        # Do the final conversions, if needed
        if field:
            if ncoords > 0:
                result = get_nested_field(result, field)
            else:
                # Get an empty array from the cache
                result = self._getemptyarray(self.coldtypes[field])
        return result

    _readCoordinates = previous_api(_read_coordinates)

    def read_coordinates(self, coords, field=None):
        """Get a set of rows given their indexes as a (record) array.

        This method works much like the :meth:`Table.read` method, but it uses
        a sequence (coords) of row indexes to select the wanted columns,
        instead of a column range.

        The selected rows are returned in an array or structured array of the
        current flavor.

        """

        self._g_check_open()
        result = self._read_coordinates(coords, field)
        return internal_to_flavor(result, self.flavor)

    readCoordinates = previous_api(read_coordinates)

    def get_enum(self, colname):
        """Get the enumerated type associated with the named column.

        If the column named colname (a string) exists and is of an enumerated
        type, the corresponding Enum instance (see :ref:`EnumClassDescr`) is
        returned. If it is not of an enumerated type, a TypeError is raised. If
        the column does not exist, a KeyError is raised.

        """

        self._check_column(colname)

        try:
            return self._colenums[colname]
        except KeyError:
            raise TypeError(
                "column ``%s`` of table ``%s`` is not of an enumerated type"
                % (colname, self._v_pathname))

    getEnum = previous_api(get_enum)

    def col(self, name):
        """Get a column from the table.

        If a column called name exists in the table, it is read and returned as
        a NumPy object. If it does not exist, a KeyError is raised.

        Examples
        --------

        ::

            narray = table.col('var2')

        That statement is equivalent to::

            narray = table.read(field='var2')

        Here you can see how this method can be used as a shorthand for the
        :meth:`Table.read` method.

        """

        return self.read(field=name)

    def __getitem__(self, key):
        """Get a row or a range of rows from the table.

        If key argument is an integer, the corresponding table row is returned
        as a record of the current flavor. If key is a slice, the range of rows
        determined by it is returned as a structured array of the current
        flavor.

        In addition, NumPy-style point selections are supported.  In
        particular, if key is a list of row coordinates, the set of rows
        determined by it is returned.  Furthermore, if key is an array of
        boolean values, only the coordinates where key is True are returned.
        Note that for the latter to work it is necessary that key list would
        contain exactly as many rows as the table has.

        Examples
        --------

        ::

            record = table[4]
            recarray = table[4:1000:2]
            recarray = table[[4,1000]]   # only retrieves rows 4 and 1000
            recarray = table[[True, False, ..., True]]

        Those statements are equivalent to::

            record = table.read(start=4)[0]
            recarray = table.read(start=4, stop=1000, step=2)
            recarray = table.read_coordinates([4,1000])
            recarray = table.read_coordinates([True, False, ..., True])

        Here, you can see how indexing can be used as a shorthand for the
        :meth:`Table.read` and :meth:`Table.read_coordinates` methods.

        """

        self._g_check_open()

        if is_idx(key):
            # Index out of range protection
            if key >= self.nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += self.nrows
            (start, stop, step) = self._process_range(key, key + 1, 1)
            return self.read(start, stop, step)[0]
        elif isinstance(key, slice):
            (start, stop, step) = self._process_range(
                key.start, key.stop, key.step)
            return self.read(start, stop, step)
        # Try with a boolean or point selection
        elif type(key) in (list, tuple) or isinstance(key, numpy.ndarray):
            return self._read_coordinates(key, None)
        else:
            raise IndexError("Invalid index or slice: %r" % (key,))

    def __setitem__(self, key, value):
        """Set a row or a range of rows in the table.

        It takes different actions depending on the type of the *key*
        parameter: if it is an integer, the corresponding table row is
        set to *value* (a record or sequence capable of being converted
        to the table structure).  If *key* is a slice, the row slice
        determined by it is set to *value* (a record array or sequence
        capable of being converted to the table structure).

        In addition, NumPy-style point selections are supported.  In
        particular, if key is a list of row coordinates, the set of rows
        determined by it is set to value.  Furthermore, if key is an array of
        boolean values, only the coordinates where key is True are set to
        values from value.  Note that for the latter to work it is necessary
        that key list would contain exactly as many rows as the table has.

        Examples
        --------

        ::

            # Modify just one existing row
            table[2] = [456,'db2',1.2]

            # Modify two existing rows
            rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                                   formats='i4,a3,f8')
            table[1:30:2] = rows             # modify a table slice
            table[[1,3]] = rows              # only modifies rows 1 and 3
            table[[True,False,True]] = rows  # only modifies rows 0 and 2

        Which is equivalent to::

            table.modify_rows(start=2, rows=[456,'db2',1.2])
            rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                                   formats='i4,a3,f8')
            table.modify_rows(start=1, stop=3, step=2, rows=rows)
            table.modify_coordinates([1,3,2], rows)
            table.modify_coordinates([True, False, True], rows)

        Here, you can see how indexing can be used as a shorthand for the
        :meth:`Table.modify_rows` and :meth:`Table.modify_coordinates`
        methods.

        """

        self._g_check_open()
        self._v_file._check_writable()

        if is_idx(key):
            # Index out of range protection
            if key >= self.nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += self.nrows
            return self.modify_rows(key, key + 1, 1, [value])
        elif isinstance(key, slice):
            (start, stop, step) = self._process_range(
                key.start, key.stop, key.step)
            return self.modify_rows(start, stop, step, value)
        # Try with a boolean or point selection
        elif type(key) in (list, tuple) or isinstance(key, numpy.ndarray):
            return self.modify_coordinates(key, value)
        else:
            raise IndexError("Invalid index or slice: %r" % (key,))

    def _save_buffered_rows(self, wbufRA, lenrows):
        """Update the indexes after a flushing of rows."""

        self._open_append(wbufRA)
        self._append_records(lenrows)
        self._close_append()
        if self.indexed:
            self._unsaved_indexedrows += lenrows
            # The table caches for indexed queries are dirty now
            self._dirtycache = True
            if self.autoindex:
                # Flush the unindexed rows
                self.flush_rows_to_index(_lastrow=False)
            else:
                # All the columns are dirty now
                self._mark_columns_as_dirty(self.colpathnames)

    _saveBufferedRows = previous_api(_save_buffered_rows)

    def append(self, rows):
        """Append a sequence of rows to the end of the table.

        The rows argument may be any object which can be converted to
        a structured array compliant with the table structure
        (otherwise, a ValueError is raised).  This includes NumPy
        structured arrays, lists of tuples or array records, and a
        string or Python buffer.

        Examples
        --------

        ::

            from tables import *

            class Particle(IsDescription):
                name        = StringCol(16, pos=1) # 16-character String
                lati        = IntCol(pos=2)        # integer
                longi       = IntCol(pos=3)        # integer
                pressure    = Float32Col(pos=4)    # float  (single-precision)
                temperature = FloatCol(pos=5)      # double (double-precision)

            fileh = open_file('test4.h5', mode='w')
            table = fileh.create_table(fileh.root, 'table', Particle,
                                       "A table")

            # Append several rows in only one call
            table.append([("Particle:     10", 10, 0, 10 * 10, 10**2),
                          ("Particle:     11", 11, -1, 11 * 11, 11**2),
                          ("Particle:     12", 12, -2, 12 * 12, 12**2)])
            fileh.close()

        """

        self._g_check_open()
        self._v_file._check_writable()

        if not self._chunked:
            raise HDF5ExtError(
                "You cannot append rows to a non-chunked table.", h5bt=False)

        # Try to convert the object into a recarray compliant with table
        try:
            iflavor = flavor_of(rows)
            if iflavor != 'python':
                rows = array_as_internal(rows, iflavor)
            # Works for Python structures and always copies the original,
            # so the resulting object is safe for in-place conversion.
            wbufRA = numpy.rec.array(rows, dtype=self._v_dtype)
        except Exception as exc:  # XXX
            raise ValueError("rows parameter cannot be converted into a "
                             "recarray object compliant with table '%s'. "
                             "The error was: <%s>" % (str(self), exc))
        lenrows = wbufRA.shape[0]
        # If the number of rows to append is zero, don't do anything else
        if lenrows > 0:
            # Save write buffer to disk
            self._save_buffered_rows(wbufRA, lenrows)

    def _conv_to_recarr(self, obj):
        """Try to convert the object into a recarray."""

        try:
            iflavor = flavor_of(obj)
            if iflavor != 'python':
                obj = array_as_internal(obj, iflavor)
            if hasattr(obj, "shape") and obj.shape == ():
                # To allow conversion of scalars (void type) into arrays.
                # See http://projects.scipy.org/scipy/numpy/ticket/315
                # for discussion on how to pass buffers to constructors
                # See also http://projects.scipy.org/scipy/numpy/ticket/348
                recarr = numpy.array([obj], dtype=self._v_dtype)
            else:
                # Works for Python structures and always copies the original,
                # so the resulting object is safe for in-place conversion.
                recarr = numpy.rec.array(obj, dtype=self._v_dtype)
        except Exception as exc:  # XXX
            raise ValueError("Object cannot be converted into a recarray "
                             "object compliant with table format '%s'. "
                             "The error was: <%s>" %
                            (self.description._v_nested_descr, exc))

        return recarr

    def modify_coordinates(self, coords, rows):
        """Modify a series of rows in positions specified in coords.

        The values in the selected rows will be modified with the data given in
        rows.  This method returns the number of rows modified.

        The possible values for the rows argument are the same as in
        :meth:`Table.append`.

        """

        if rows is None:      # Nothing to be done
            return SizeType(0)

        # Convert the coordinates to something expected by HDF5
        coords = self._point_selection(coords)

        lcoords = len(coords)
        if len(rows) < lcoords:
            raise ValueError("The value has not enough elements to fill-in "
                             "the specified range")

        # Convert rows into a recarray
        recarr = self._conv_to_recarr(rows)

        if len(coords) > 0:
            # Do the actual update of rows
            self._update_elements(lcoords, coords, recarr)

        # Redo the index if needed
        self._reindex(self.colpathnames)

        return SizeType(lcoords)

    modifyCoordinates = previous_api(modify_coordinates)

    def modify_rows(self, start=None, stop=None, step=None, rows=None):
        """Modify a series of rows in the slice [start:stop:step].

        The values in the selected rows will be modified with the data given in
        rows.  This method returns the number of rows modified.  Should the
        modification exceed the length of the table, an IndexError is raised
        before changing data.

        The possible values for the rows argument are the same as in
        :meth:`Table.append`.

        """

        if step is None:
            step = 1
        if rows is None:      # Nothing to be done
            return SizeType(0)
        if start is None:
            start = 0

        if start < 0:
            raise ValueError("'start' must have a positive value.")
        if step < 1:
            raise ValueError(
                "'step' must have a value greater or equal than 1.")
        if stop is None:
            # compute the stop value. start + len(rows)*step does not work
            stop = start + (len(rows) - 1) * step + 1

        (start, stop, step) = self._process_range(start, stop, step)
        if stop > self.nrows:
            raise IndexError("This modification will exceed the length of "
                             "the table. Giving up.")
        # Compute the number of rows to read.
        nrows = len(xrange(start, stop, step))
        if len(rows) != nrows:
            raise ValueError("The value has different elements than the "
                             "specified range")

        # Convert rows into a recarray
        recarr = self._conv_to_recarr(rows)

        lenrows = len(recarr)
        if start + lenrows > self.nrows:
            raise IndexError("This modification will exceed the length of the "
                             "table. Giving up.")

        # Do the actual update
        self._update_records(start, stop, step, recarr)

        # Redo the index if needed
        self._reindex(self.colpathnames)

        return SizeType(lenrows)

    modifyRows = previous_api(modify_rows)

    def modify_column(self, start=None, stop=None, step=None,
                      column=None, colname=None):
        """Modify one single column in the row slice [start:stop:step].

        The colname argument specifies the name of the column in the
        table to be modified with the data given in column.  This
        method returns the number of rows modified.  Should the
        modification exceed the length of the table, an IndexError is
        raised before changing data.

        The *column* argument may be any object which can be converted
        to a (record) array compliant with the structure of the column
        to be modified (otherwise, a ValueError is raised).  This
        includes NumPy (record) arrays, lists of scalars, tuples or
        array records, and a string or Python buffer.

        """
        if step is None:
            step = 1
        if not isinstance(colname, str):
            raise TypeError("The 'colname' parameter must be a string.")
        self._v_file._check_writable()

        if column is None:      # Nothing to be done
            return SizeType(0)
        if start is None:
            start = 0

        if start < 0:
            raise ValueError("'start' must have a positive value.")
        if step < 1:
            raise ValueError(
                "'step' must have a value greater or equal than 1.")
        # Get the column format to be modified:
        objcol = self._get_column_instance(colname)
        descr = [objcol._v_parent._v_nested_descr[objcol._v_pos]]
        # Try to convert the column object into a NumPy ndarray
        try:
            # If the column is a recarray (or kind of), convert into ndarray
            if hasattr(column, 'dtype') and column.dtype.kind == 'V':
                column = numpy.rec.array(column, dtype=descr).field(0)
            else:
                # Make sure the result is always a *copy* of the original,
                # so the resulting object is safe for in-place conversion.
                iflavor = flavor_of(column)
                column = array_as_internal(column, iflavor)
        except Exception as exc:  # XXX
            raise ValueError("column parameter cannot be converted into a "
                             "ndarray object compliant with specified column "
                             "'%s'. The error was: <%s>" % (str(column), exc))

        # Get rid of single-dimensional dimensions
        column = column.squeeze()
        if column.shape == ():
            # Oops, stripped off to much dimensions
            column.shape = (1,)

        if stop is None:
            # compute the stop value. start + len(rows)*step does not work
            stop = start + (len(column) - 1) * step + 1
        (start, stop, step) = self._process_range(start, stop, step)
        if stop > self.nrows:
            raise IndexError("This modification will exceed the length of "
                             "the table. Giving up.")
        # Compute the number of rows to read.
        nrows = len(xrange(start, stop, step))
        if len(column) < nrows:
            raise ValueError("The value has not enough elements to fill-in "
                             "the specified range")
        # Now, read the original values:
        mod_recarr = self._read(start, stop, step)
        # Modify the appropriate column in the original recarray
        mod_col = get_nested_field(mod_recarr, colname)
        mod_col[:] = column
        # save this modified rows in table
        self._update_records(start, stop, step, mod_recarr)
        # Redo the index if needed
        self._reindex([colname])

        return SizeType(nrows)

    modifyColumn = previous_api(modify_column)

    def modify_columns(self, start=None, stop=None, step=None,
                       columns=None, names=None):
        """Modify a series of columns in the row slice [start:stop:step].

        The names argument specifies the names of the columns in the
        table to be modified with the data given in columns.  This
        method returns the number of rows modified.  Should the
        modification exceed the length of the table, an IndexError
        is raised before changing data.

        The columns argument may be any object which can be converted
        to a structured array compliant with the structure of the
        columns to be modified (otherwise, a ValueError is raised).
        This includes NumPy structured arrays, lists of tuples or array
        records, and a string or Python buffer.

        """
        if step is None:
            step = 1
        if type(names) not in (list, tuple):
            raise TypeError("The 'names' parameter must be a list of strings.")

        if columns is None:  # Nothing to be done
            return SizeType(0)
        if start is None:
            start = 0
        if start < 0:
            raise ValueError("'start' must have a positive value.")
        if step < 1:
            raise ValueError(("'step' must have a value greater or "
                              "equal than 1."))
        descr = []
        for colname in names:
            objcol = self._get_column_instance(colname)
            descr.append(objcol._v_parent._v_nested_descr[objcol._v_pos])
            # descr.append(objcol._v_parent._v_dtype[objcol._v_pos])
        # Try to convert the columns object into a recarray
        try:
            # Make sure the result is always a *copy* of the original,
            # so the resulting object is safe for in-place conversion.
            iflavor = flavor_of(columns)
            if iflavor != 'python':
                columns = array_as_internal(columns, iflavor)
                recarray = numpy.rec.array(columns, dtype=descr)
            else:
                recarray = numpy.rec.fromarrays(columns, dtype=descr)
        except Exception as exc:  # XXX
            raise ValueError("columns parameter cannot be converted into a "
                             "recarray object compliant with table '%s'. "
                             "The error was: <%s>" % (str(self), exc))

        if stop is None:
            # compute the stop value. start + len(rows)*step does not work
            stop = start + (len(recarray) - 1) * step + 1
        (start, stop, step) = self._process_range(start, stop, step)
        if stop > self.nrows:
            raise IndexError("This modification will exceed the length of "
                             "the table. Giving up.")
        # Compute the number of rows to read.
        nrows = len(xrange(start, stop, step))
        if len(recarray) < nrows:
            raise ValueError("The value has not enough elements to fill-in "
                             "the specified range")
        # Now, read the original values:
        mod_recarr = self._read(start, stop, step)
        # Modify the appropriate columns in the original recarray
        for i, name in enumerate(recarray.dtype.names):
            mod_col = get_nested_field(mod_recarr, names[i])
            mod_col[:] = recarray[name].squeeze()
        # save this modified rows in table
        self._update_records(start, stop, step, mod_recarr)
        # Redo the index if needed
        self._reindex(names)

        return SizeType(nrows)

    modifyColumns = previous_api(modify_columns)

    def flush_rows_to_index(self, _lastrow=True):
        """Add remaining rows in buffers to non-dirty indexes.

        This can be useful when you have chosen non-automatic indexing
        for the table (see the :attr:`Table.autoindex` property in
        :class:`Table`) and you want to update the indexes on it.

        """

        rowsadded = 0
        if self.indexed:
            # Update the number of unsaved indexed rows
            start = self._indexedrows
            nrows = self._unsaved_indexedrows
            for (colname, colindexed) in self.colindexed.iteritems():
                if colindexed:
                    col = self.cols._g_col(colname)
                    if nrows > 0 and not col.index.dirty:
                        rowsadded = self._add_rows_to_index(
                            colname, start, nrows, _lastrow, update=True)
            self._unsaved_indexedrows -= rowsadded
            self._indexedrows += rowsadded
        return rowsadded

    flushRowsToIndex = previous_api(flush_rows_to_index)

    def _add_rows_to_index(self, colname, start, nrows, lastrow, update):
        """Add more elements to the existing index."""

        # This method really belongs to Column, but since it makes extensive
        # use of the table, it gets dangerous when closing the file, since the
        # column may be accessing a table which is being destroyed.
        index = self.cols._g_col(colname).index
        slicesize = index.slicesize
        # The next loop does not rely on xrange so that it can
        # deal with long ints (i.e. more than 32-bit integers)
        # This allows to index columns with more than 2**31 rows
        # F. Alted 2005-05-09
        startLR = index.sorted.nrows * slicesize
        indexedrows = startLR - start
        stop = start + nrows - slicesize + 1
        while startLR < stop:
            index.append(
                [self._read(startLR, startLR + slicesize, 1, colname)],
                update=update)
            indexedrows += slicesize
            startLR += slicesize
        # index the remaining rows in last row
        if lastrow and startLR < self.nrows:
            index.append_last_row(
                [self._read(startLR, self.nrows, 1, colname)],
                update=update)
            indexedrows += self.nrows - startLR
        return indexedrows

    _addRowsToIndex = previous_api(_add_rows_to_index)

    def remove_rows(self, start=None, stop=None, step=None):
        """Remove a range of rows in the table.

        .. versionchanged:: 3.0
           The start, stop and step parameters now behave like in slice.

        .. seealso:: remove_row()

        Parameters
        ----------
        start : int
            Sets the starting row to be removed. It accepts negative values
            meaning that the count starts from the end.  A value of 0 means the
            first row.
        stop : int
            Sets the last row to be removed to stop-1, i.e. the end point is
            omitted (in the Python range() tradition). Negative values are also
            accepted.
        step : int
            The step size between rows to remove.

            .. versionadded:: 3.0

        Examples
        --------

        Removing rows from 5 to 10 (excluded)::

            t.remove_rows(5, 10)

        Removing all rows starting drom the 10th::

            t.remove_rows(10)

        Removing the 6th row::

            t.remove_rows(6, 7)

        .. note::

            removing a single row can be done using the specific
            :meth:`remove_row` method.

        """

        (start, stop, step) = self._process_range(start, stop, step)
        nrows = numpy.abs(stop - start)
        if nrows >= self.nrows:
            raise NotImplementedError('You are trying to delete all the rows '
                                      'in table "%s". This is not supported '
                                      'right now due to limitations on the '
                                      'underlying HDF5 library. Sorry!' %
                                      self._v_pathname)
        nrows = self._remove_rows(start, stop, step)
        # remove_rows is a invalidating index operation
        self._reindex(self.colpathnames)

        return SizeType(nrows)

    removeRows = previous_api(remove_rows)

    def remove_row(self, n):
        """Removes a row from the table.

        If only start is supplied, only this row is to be deleted.  If a range
        is supplied, i.e. both the start and stop parameters are passed, all
        the rows in the range are removed. A step parameter is not supported,
        and it is not foreseen to be implemented anytime soon.

        Parameters
        ----------
        n : int
            The index of the row to remove.

        .. versionadded:: 3.0

        """

        self.remove_rows(start=n, stop=n + 1)

    def _g_update_dependent(self):
        super(Table, self)._g_update_dependent()

        # Update the new path in columns
        self.cols._g_update_table_location(self)

        # Update the new path in the Row instance, if cached.  Fixes #224.
        if 'row' in self.__dict__:
            self.__dict__['row'] = tableextension.Row(self)

    _g_updateDependent = previous_api(_g_update_dependent)

    def _g_move(self, newparent, newname):
        """Move this node in the hierarchy.

        This overloads the Node._g_move() method.

        """

        itgpathname = _index_pathname_of(self)

        # First, move the table to the new location.
        super(Table, self)._g_move(newparent, newname)

        # Then move the associated index group (if any).
        try:
            itgroup = self._v_file._get_node(itgpathname)
        except NoSuchNodeError:
            pass
        else:
            newigroup = self._v_parent
            newiname = _index_name_of(self)
            itgroup._g_move(newigroup, newiname)

    def _g_remove(self, recursive=False, force=False):
        # Remove the associated index group (if any).
        itgpathname = _index_pathname_of(self)
        try:
            itgroup = self._v_file._get_node(itgpathname)
        except NoSuchNodeError:
            pass
        else:
            itgroup._f_remove(recursive=True)
            self.indexed = False   # there are indexes no more

        # Remove the leaf itself from the hierarchy.
        super(Table, self)._g_remove(recursive, force)

    def _set_column_indexing(self, colpathname, indexed):
        """Mark the referred column as indexed or non-indexed."""

        colindexed = self.colindexed
        isindexed, wasindexed = bool(indexed), colindexed[colpathname]
        if isindexed == wasindexed:
            return  # indexing state is unchanged

        # Changing the set of indexed columns invalidates the condition cache
        self._condition_cache.clear()
        colindexed[colpathname] = isindexed
        self.indexed = max(colindexed.values())  # this is an OR :)

    _setColumnIndexing = previous_api(_set_column_indexing)

    def _mark_columns_as_dirty(self, colnames):
        """Mark column indexes in `colnames` as dirty."""

        assert len(colnames) > 0
        if self.indexed:
            colindexed, cols = self.colindexed, self.cols
            # Mark the proper indexes as dirty
            for colname in colnames:
                if colindexed[colname]:
                    col = cols._g_col(colname)
                    col.index.dirty = True

    _markColumnsAsDirty = previous_api(_mark_columns_as_dirty)

    def _reindex(self, colnames):
        """Re-index columns in `colnames` if automatic indexing is true."""

        if self.indexed:
            colindexed, cols = self.colindexed, self.cols
            colstoindex = []
            # Mark the proper indexes as dirty
            for colname in colnames:
                if colindexed[colname]:
                    col = cols._g_col(colname)
                    col.index.dirty = True
                    colstoindex.append(colname)
            # Now, re-index the dirty ones
            if self.autoindex and colstoindex:
                self._do_reindex(dirty=True)
            # The table caches for indexed queries are dirty now
            self._dirtycache = True

    _reIndex = previous_api(_reindex)

    def _do_reindex(self, dirty):
        """Common code for `reindex()` and `reindex_dirty()`."""

        indexedrows = 0
        for (colname, colindexed) in self.colindexed.iteritems():
            if colindexed:
                indexcol = self.cols._g_col(colname)
                indexedrows = indexcol._do_reindex(dirty)
        # Update counters in case some column has been updated
        if indexedrows > 0:
            self._indexedrows = indexedrows
            self._unsaved_indexedrows = self.nrows - indexedrows

        return SizeType(indexedrows)

    _doReIndex = previous_api(_do_reindex)

    def reindex(self):
        """Recompute all the existing indexes in the table.

        This can be useful when you suspect that, for any reason, the
        index information for columns is no longer valid and want to
        rebuild the indexes on it.

        """

        self._do_reindex(dirty=False)

    reIndex = previous_api(reindex)

    def reindex_dirty(self):
        """Recompute the existing indexes in table, *if* they are dirty.

        This can be useful when you have set :attr:`Table.autoindex`
        (see :class:`Table`) to false for the table and you want to
        update the indexes after a invalidating index operation
        (:meth:`Table.remove_rows`, for example).

        """

        self._do_reindex(dirty=True)

    reIndexDirty = previous_api(reindex_dirty)

    def _g_copy_rows(self, object, start, stop, step, sortby, checkCSI):
        "Copy rows from self to object"
        if sortby is None:
            self._g_copy_rows_optim(object, start, stop, step)
            return
        lenbuf = self.nrowsinbuf
        absstep = abs(step)
        if sortby is not None:
            index = self._check_sortby_csi(sortby, checkCSI)
        for start2 in xrange(start, stop, absstep * lenbuf):
            stop2 = start2 + absstep * lenbuf
            if stop2 > stop:
                stop2 = stop
            # The next 'if' is not needed, but it doesn't bother either
            if sortby is None:
                rows = self[start2:stop2:step]
            else:
                coords = index[start2:stop2:step]
                rows = self.read_coordinates(coords)
            # Save the records on disk
            object.append(rows)
        object.flush()

    _g_copyRows = previous_api(_g_copy_rows)

    def _g_copy_rows_optim(self, object, start, stop, step):
        """Copy rows from self to object (optimized version)"""

        nrowsinbuf = self.nrowsinbuf
        object._open_append(self._v_iobuf)
        nrowsdest = object.nrows
        for start2 in xrange(start, stop, step * nrowsinbuf):
            # Save the records on disk
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            # Optimized version (it saves some conversions)
            nrows = ((stop2 - start2 - 1) // step) + 1
            self.row._fill_col(self._v_iobuf, start2, stop2, step, None)
            # The output buffer is created anew,
            # so the operation is safe to in-place conversion.
            object._append_records(nrows)
            nrowsdest += nrows
        object._close_append()

    _g_copyRows_optim = previous_api(_g_copy_rows_optim)

    def _g_prop_indexes(self, other):
        """Generate index in `other` table for every indexed column here."""

        oldcols, newcols = self.colinstances, other.colinstances
        for colname in newcols:
            if (isinstance(oldcols[colname], Column)):
                oldcolindexed = oldcols[colname].is_indexed
                if oldcolindexed:
                    oldcolindex = oldcols[colname].index
                    newcol = newcols[colname]
                    newcol.create_index(
                        kind=oldcolindex.kind, optlevel=oldcolindex.optlevel,
                        filters=oldcolindex.filters, tmp_dir=None)

    _g_propIndexes = previous_api(_g_prop_indexes)

    def _g_copy_with_stats(self, group, name, start, stop, step,
                           title, filters, chunkshape, _log, **kwargs):
        """Private part of Leaf.copy() for each kind of leaf."""

        # Get the private args for the Table flavor of copy()
        sortby = kwargs.pop('sortby', None)
        propindexes = kwargs.pop('propindexes', False)
        checkCSI = kwargs.pop('checkCSI', False)
        # Compute the correct indices.
        (start, stop, step) = self._process_range_read(
            start, stop, step, warn_negstep=sortby is None)
        # And the number of final rows
        nrows = len(xrange(start, stop, step))
        # Create the new table and copy the selected data.
        newtable = Table(group, name, self.description, title=title,
                         filters=filters, expectedrows=nrows,
                         chunkshape=chunkshape,
                         _log=_log)
        self._g_copy_rows(newtable, start, stop, step, sortby, checkCSI)
        nbytes = newtable.nrows * newtable.rowsize
        # Generate equivalent indexes in the new table, if required.
        if propindexes and self.indexed:
            self._g_prop_indexes(newtable)
        return (newtable, nbytes)

    _g_copyWithStats = previous_api(_g_copy_with_stats)

    # This overloading of copy is needed here in order to document
    # the additional keywords for the Table case.
    def copy(self, newparent=None, newname=None, overwrite=False,
             createparents=False, **kwargs):
        """Copy this table and return the new one.

        This method has the behavior and keywords described in
        :meth:`Leaf.copy`.  Moreover, it recognises the following additional
        keyword arguments.

        Parameters
        ----------
        sortby
            If specified, and sortby corresponds to a column with an index,
            then the copy will be sorted by this index.  If you want to ensure
            a fully sorted order, the index must be a CSI one.  A reverse
            sorted copy can be achieved by specifying a negative value for the
            step keyword.  If sortby is omitted or None, the original table
            order is used.
        checkCSI
            If true and a CSI index does not exist for the sortby column, an
            error will be raised.  If false (the default), it does nothing.
            You can use this flag in order to explicitly check for the
            existence of a CSI index.
        propindexes
            If true, the existing indexes in the source table are propagated
            (created) to the new one.  If false (the default), the indexes are
            not propagated.

        """

        return super(Table, self).copy(
            newparent, newname, overwrite, createparents, **kwargs)

    def flush(self):
        """Flush the table buffers."""

        # Flush rows that remains to be appended
        if 'row' in self.__dict__:
            self.row._flush_buffered_rows()
        if self.indexed and self.autoindex:
            # Flush any unindexed row
            rowsadded = self.flush_rows_to_index(_lastrow=True)
            assert rowsadded <= 0 or self._indexedrows == self.nrows, \
                ("internal error: the number of indexed rows (%d) "
                 "and rows in the table (%d) is not equal; "
                 "please report this to the authors."
                 % (self._indexedrows, self.nrows))
            if self._dirtyindexes:
                # Finally, re-index any dirty column
                self.reindex_dirty()

        super(Table, self).flush()

    def _g_pre_kill_hook(self):
        """Code to be called before killing the node."""

        # Flush the buffers before to clean-up them
        # self.flush()
        # It seems that flushing during the __del__ phase is a sure receipt for
        # bringing all kind of problems:
        # 1. Illegal Instruction
        # 2. Malloc(): trying to call free() twice
        # 3. Bus Error
        # 4. Segmentation fault
        # So, the best would be doing *nothing* at all in this __del__ phase.
        # As a consequence, the I/O will not be cleaned until a call to
        # Table.flush() would be done. This could lead to a potentially large
        # memory consumption.
        # NOTE: The user should make a call to Table.flush() whenever he has
        #       finished working with his table.
        # I've added a Performance warning in order to compel the user to
        # call self.flush() before the table is being preempted.
        # F. Alted 2006-08-03
        if (('row' in self.__dict__ and self.row._get_unsaved_nrows() > 0) or
            (self.indexed and self.autoindex and
             (self._unsaved_indexedrows > 0 or self._dirtyindexes))):
            warnings.warn(("table ``%s`` is being preempted from alive nodes "
                           "without its buffers being flushed or with some "
                           "index being dirty.  This may lead to very "
                           "ineficient use of resources and even to fatal "
                           "errors in certain situations.  Please do a call "
                           "to the .flush() or .reindex_dirty() methods on "
                           "this table before start using other nodes.")
                          % (self._v_pathname), PerformanceWarning)
        # Get rid of the IO buffers (if they have been created at all)
        mydict = self.__dict__
        if '_v_iobuf' in mydict:
            del mydict['_v_iobuf']
        if '_v_wdflts' in mydict:
            del mydict['_v_wdflts']

    _g_preKillHook = previous_api(_g_pre_kill_hook)

    def _f_close(self, flush=True):
        if not self._v_isopen:
            return  # the node is already closed

        # .. note::
        #
        #   As long as ``Table`` objects access their indices on closing,
        #   ``File.close()`` will need to make *two separate passes*
        #   to first close ``Table`` objects and then ``Index`` hierarchies.
        #

        # Flush right now so the row object does not get in the middle.
        if flush:
            self.flush()

        # Some warnings can be issued after calling `self._g_set_location()`
        # in `self.__init__()`.  If warnings are turned into exceptions,
        # `self._g_post_init_hook` may not be called and `self.cols` not set.
        # One example of this is
        # ``test_create.createTestCase.test05_maxFieldsExceeded()``.
        cols = self.cols
        if cols is not None:
            cols._g_close()

        # Close myself as a leaf.
        super(Table, self)._f_close(False)

    def __repr__(self):
        """This provides column metainfo in addition to standard __str__"""

        if self.indexed:
            format = """\
%s
  description := %r
  byteorder := %r
  chunkshape := %r
  autoindex := %r
  colindexes := %r"""
            return format % (str(self), self.description, self.byteorder,
                             self.chunkshape, self.autoindex,
                             _ColIndexes(self.colindexes))
        else:
            return """\
%s
  description := %r
  byteorder := %r
  chunkshape := %r""" % \
                (str(self), self.description, self.byteorder, self.chunkshape)


class Cols(object):
    """Container for columns in a table or nested column.

    This class is used as an *accessor* to the columns in a table or nested
    column.  It supports the *natural naming* convention, so that you can
    access the different columns as attributes which lead to Column instances
    (for non-nested columns) or other Cols instances (for nested columns).

    For instance, if table.cols is a Cols instance with a column named col1
    under it, the later can be accessed as table.cols.col1. If col1 is nested
    and contains a col2 column, this can be accessed as table.cols.col1.col2
    and so on. Because of natural naming, the names of members start with
    special prefixes, like in the Group class (see :ref:`GroupClassDescr`).

    Like the Column class (see :ref:`ColumnClassDescr`), Cols supports item
    access to read and write ranges of values in the table or nested column.


    .. rubric:: Cols attributes

    .. attribute:: _v_colnames

        A list of the names of the columns hanging directly
        from the associated table or nested column.  The order of
        the names matches the order of their respective columns in
        the containing table.

    .. attribute:: _v_colpathnames

        A list of the pathnames of all the columns under the
        associated table or nested column (in preorder).  If it does
        not contain nested columns, this is exactly the same as the
        :attr:`Cols._v_colnames` attribute.

    .. attribute:: _v_desc

        The associated Description instance (see
        :ref:`DescriptionClassDescr`).

    """

    def _g_gettable(self):
        return self._v__tableFile._get_node(self._v__tablePath)

    _v_table = property(
        _g_gettable, None, None,
        "The parent Table instance (see :ref:`TableClassDescr`).")

    def __init__(self, table, desc):

        myDict = self.__dict__
        myDict['_v__tableFile'] = table._v_file
        myDict['_v__tablePath'] = table._v_pathname
        myDict['_v_desc'] = desc
        myDict['_v_colnames'] = desc._v_names
        myDict['_v_colpathnames'] = table.description._v_pathnames
        # Put the column in the local dictionary
        for name in desc._v_names:
            if name in desc._v_types:
                myDict[name] = Column(table, name, desc)
            else:
                myDict[name] = Cols(table, desc._v_colobjects[name])

    def _g_update_table_location(self, table):
        """Updates the location information about the associated `table`."""

        myDict = self.__dict__
        myDict['_v__tableFile'] = table._v_file
        myDict['_v__tablePath'] = table._v_pathname

        # Update the locations in individual columns.
        for colname in self._v_colnames:
            myDict[colname]._g_update_table_location(table)

    _g_updateTableLocation = previous_api(_g_update_table_location)

    def __len__(self):
        """Get the number of top level columns in table."""

        return len(self._v_colnames)

    def _f_col(self, colname):
        """Get an accessor to the column colname.

        This method returns a Column instance (see :ref:`ColumnClassDescr`) if
        the requested column is not nested, and a Cols instance (see
        :ref:`ColsClassDescr`) if it is.  You may use full column pathnames in
        colname.

        Calling cols._f_col('col1/col2') is equivalent to using cols.col1.col2.
        However, the first syntax is more intended for programmatic use.  It is
        also better if you want to access columns with names that are not valid
        Python identifiers.

        """

        if not isinstance(colname, str):
            raise TypeError("Parameter can only be an string. You passed "
                            "object: %s" % colname)
        if ((colname.find('/') > -1 and
             not colname in self._v_colpathnames) and
                not colname in self._v_colnames):
            raise KeyError(("Cols accessor ``%s.cols%s`` does not have a "
                            "column named ``%s``")
                           % (self._v__tablePath, self._v_desc._v_pathname,
                              colname))

        return self._g_col(colname)

    def _g_col(self, colname):
        """Like `self._f_col()` but it does not check arguments."""

        # Get the Column or Description object
        inames = colname.split('/')
        cols = self
        for iname in inames:
            cols = cols.__dict__[iname]
        return cols

    def __getitem__(self, key):
        """Get a row or a range of rows from a table or nested column.

        If key argument is an integer, the corresponding nested type row is
        returned as a record of the current flavor. If key is a slice, the
        range of rows determined by it is returned as a structured array of the
        current flavor.

        Examples
        --------

        ::

            record = table.cols[4]  # equivalent to table[4]
            recarray = table.cols.Info[4:1000:2]

        Those statements are equivalent to::

            nrecord = table.read(start=4)[0]
            nrecarray = table.read(start=4, stop=1000, step=2).field('Info')

        Here you can see how a mix of natural naming, indexing and slicing can
        be used as shorthands for the :meth:`Table.read` method.

        """

        table = self._v_table
        nrows = table.nrows
        if is_idx(key):
            # Index out of range protection
            if key >= nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += nrows
            (start, stop, step) = table._process_range(key, key + 1, 1)
            colgroup = self._v_desc._v_pathname
            if colgroup == "":  # The root group
                return table.read(start, stop, step)[0]
            else:
                crecord = table.read(start, stop, step)[0]
                return crecord[colgroup]
        elif isinstance(key, slice):
            (start, stop, step) = table._process_range(
                key.start, key.stop, key.step)
            colgroup = self._v_desc._v_pathname
            if colgroup == "":  # The root group
                return table.read(start, stop, step)
            else:
                crecarray = table.read(start, stop, step)
                if hasattr(crecarray, "field"):
                    return crecarray.field(colgroup)  # RecArray case
                else:
                    return get_nested_field(crecarray, colgroup)  # numpy case
        else:
            raise TypeError("invalid index or slice: %r" % (key,))

    def __setitem__(self, key, value):
        """Set a row or a range of rows in a table or nested column.

        If key argument is an integer, the corresponding row is set to
        value. If key is a slice, the range of rows determined by it is set to
        value.

        Examples
        --------

        ::

            table.cols[4] = record
            table.cols.Info[4:1000:2] = recarray

        Those statements are equivalent to::

            table.modify_rows(4, rows=record)
            table.modify_column(4, 1000, 2, colname='Info', column=recarray)

        Here you can see how a mix of natural naming, indexing and slicing
        can be used as shorthands for the :meth:`Table.modify_rows` and
        :meth:`Table.modify_column` methods.

        """

        table = self._v_table
        nrows = table.nrows
        if is_idx(key):
            # Index out of range protection
            if key >= nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += nrows
            (start, stop, step) = table._process_range(key, key + 1, 1)
        elif isinstance(key, slice):
            (start, stop, step) = table._process_range(
                key.start, key.stop, key.step)
        else:
            raise TypeError("invalid index or slice: %r" % (key,))

        # Actually modify the correct columns
        colgroup = self._v_desc._v_pathname
        if colgroup == "":  # The root group
            table.modify_rows(start, stop, step, rows=value)
        else:
            table.modify_column(
                start, stop, step, colname=colgroup, column=value)

    def _g_close(self):
        # First, close the columns (ie possible indices open)
        for col in self._v_colnames:
            colobj = self._g_col(col)
            if isinstance(colobj, Column):
                colobj.close()
                # Delete the reference to column
                del self.__dict__[col]
            else:
                colobj._g_close()

        self.__dict__.clear()

    def __str__(self):
        """The string representation for this object."""

        # The pathname
        tablepathname = self._v__tablePath
        descpathname = self._v_desc._v_pathname
        if descpathname:
            descpathname = "." + descpathname
        # Get this class name
        classname = self.__class__.__name__
        # The number of columns
        ncols = len(self._v_colnames)
        return "%s.cols%s (%s), %s columns" % \
               (tablepathname, descpathname, classname, ncols)

    def __repr__(self):
        """A detailed string representation for this object."""

        out = str(self) + "\n"
        for name in self._v_colnames:
            # Get this class name
            classname = getattr(self, name).__class__.__name__
            # The type
            if name in self._v_desc._v_dtypes:
                tcol = self._v_desc._v_dtypes[name]
                # The shape for this column
                shape = (self._v_table.nrows,) + \
                    self._v_desc._v_dtypes[name].shape
            else:
                tcol = "Description"
                # Description doesn't have a shape currently
                shape = ()
            out += "  %s (%s%s, %s)" % (name, classname, shape, tcol) + "\n"
        return out


class Column(object):
    """Accessor for a non-nested column in a table.

    Each instance of this class is associated with one *non-nested* column of a
    table. These instances are mainly used to read and write data from the
    table columns using item access (like the Cols class - see
    :ref:`ColsClassDescr`), but there are a few other associated methods to
    deal with indexes.

    .. rubric:: Column attributes

    .. attribute:: descr

        The Description (see :ref:`DescriptionClassDescr`) instance of the
        parent table or nested column.

    .. attribute:: name

        The name of the associated column.

    .. attribute:: pathname

        The complete pathname of the associated column (the same as
        Column.name if the column is not inside a nested column).

    Parameters
    ----------
    table
        The parent table instance
    name
        The name of the column that is associated with this object
    descr
        The parent description object

    """

    # Lazy read-only attributes
    # `````````````````````````
    @lazyattr
    def dtype(self):
        """The NumPy dtype that most closely matches this column."""

        return self.descr._v_dtypes[self.name].base  # Get rid of shape info

    @lazyattr
    def type(self):
        """The PyTables type of the column (a string)."""

        return self.descr._v_types[self.name]

    # Properties
    # ~~~~~~~~~~
    def _gettable(self):
        return self._table_file._get_node(self._table_path)

    table = property(_gettable, None, None,
                     """The parent Table instance (see
                     :ref:`TableClassDescr`).""")

    def _getindex(self):
        indexPath = _index_pathname_of_column_(self._table_path, self.pathname)
        try:
            index = self._table_file._get_node(indexPath)
        except NodeError:
            index = None  # The column is not indexed
        return index

    index = property(_getindex, None, None,
                     """The Index instance (see :ref:`IndexClassDescr`)
                     associated with this column (None if the column is not
                     indexed).""")

    def _getshape(self):
        return (self.table.nrows,) + self.descr._v_dtypes[self.name].shape

    shape = property(_getshape, None, None, "The shape of this column.")

    def _isindexed(self):
        if self.index is None:
            return False
        else:
            return True

    is_indexed = property(_isindexed, None, None,
                          "True if the column is indexed, false otherwise.")

    maindim = property(
        lambda self: 0, None, None,
        """"The dimension along which iterators work. Its value is 0 (i.e. the
        first dimension).""")

    def __init__(self, table, name, descr):

        self._table_file = table._v_file
        self._table_path = table._v_pathname
        self.name = name
        """The name of the associated column."""
        self.pathname = descr._v_colobjects[name]._v_pathname
        """The complete pathname of the associated column (the same as
        Column.name if the column is not inside a nested column)."""
        self.descr = descr
        """The Description (see :ref:`DescriptionClassDescr`) instance of the
        parent table or nested column."""

    def _g_update_table_location(self, table):
        """Updates the location information about the associated `table`."""

        self._table_file = table._v_file
        self._table_path = table._v_pathname

    _g_updateTableLocation = previous_api(_g_update_table_location)

    def __len__(self):
        """Get the number of elements in the column.

        This matches the length in rows of the parent table.

        """

        return self.table.nrows

    def __getitem__(self, key):
        """Get a row or a range of rows from a column.

        If key argument is an integer, the corresponding element in the column
        is returned as an object of the current flavor.  If key is a slice, the
        range of elements determined by it is returned as an array of the
        current flavor.

        Examples
        --------

        ::

            print("Column handlers:")
            for name in table.colnames:
                print(table.cols._f_col(name))
                print("Select table.cols.name[1]-->", table.cols.name[1])
                print("Select table.cols.name[1:2]-->", table.cols.name[1:2])
                print("Select table.cols.name[:]-->", table.cols.name[:])
                print("Select table.cols._f_col('name')[:]-->",
                                                table.cols._f_col('name')[:])

        The output of this for a certain arbitrary table is::

            Column handlers:
            /table.cols.name (Column(), string, idx=None)
            /table.cols.lati (Column(), int32, idx=None)
            /table.cols.longi (Column(), int32, idx=None)
            /table.cols.vector (Column(2,), int32, idx=None)
            /table.cols.matrix2D (Column(2, 2), float64, idx=None)
            Select table.cols.name[1]--> Particle:     11
            Select table.cols.name[1:2]--> ['Particle:     11']
            Select table.cols.name[:]--> ['Particle:     10'
             'Particle:     11' 'Particle:     12'
             'Particle:     13' 'Particle:     14']
            Select table.cols._f_col('name')[:]--> ['Particle:     10'
             'Particle:     11' 'Particle:     12'
             'Particle:     13' 'Particle:     14']

        See the :file:`examples/table2.py` file for a more complete example.

        """

        table = self.table

        # Generalized key support not there yet, but at least allow
        # for a tuple with one single element (the main dimension).
        # (key,) --> key
        if isinstance(key, tuple) and len(key) == 1:
            key = key[0]

        if is_idx(key):
            # Index out of range protection
            if key >= table.nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += table.nrows
            (start, stop, step) = table._process_range(key, key + 1, 1)
            return table.read(start, stop, step, self.pathname)[0]
        elif isinstance(key, slice):
            (start, stop, step) = table._process_range(
                key.start, key.stop, key.step)
            return table.read(start, stop, step, self.pathname)
        else:
            raise TypeError(
                "'%s' key type is not valid in this context" % key)

    def __iter__(self):
        """Iterate through all items in the column."""

        table = self.table
        itemsize = self.dtype.itemsize
        nrowsinbuf = table._v_file.params['IO_BUFFER_SIZE'] // itemsize
        buf = numpy.empty((nrowsinbuf, ), self.dtype)
        max_row = len(self)
        for start_row in xrange(0, len(self), nrowsinbuf):
            end_row = min(start_row + nrowsinbuf, max_row)
            buf_slice = buf[0:end_row - start_row]
            table.read(start_row, end_row, 1, field=self.pathname,
                       out=buf_slice)
            for row in buf_slice:
                yield row

    def __setitem__(self, key, value):
        """Set a row or a range of rows in a column.

        If key argument is an integer, the corresponding element is set to
        value.  If key is a slice, the range of elements determined by it is
        set to value.

        Examples
        --------

        ::

            # Modify row 1
            table.cols.col1[1] = -1

            # Modify rows 1 and 3
            table.cols.col1[1::2] = [2,3]

        Which is equivalent to::

            # Modify row 1
            table.modify_columns(start=1, columns=[[-1]], names=['col1'])

            # Modify rows 1 and 3
            columns = numpy.rec.fromarrays([[2,3]], formats='i4')
            table.modify_columns(start=1, step=2, columns=columns,
                                 names=['col1'])

        """

        table = self.table
        table._v_file._check_writable()

        # Generalized key support not there yet, but at least allow
        # for a tuple with one single element (the main dimension).
        # (key,) --> key
        if isinstance(key, tuple) and len(key) == 1:
            key = key[0]

        if is_idx(key):
            # Index out of range protection
            if key >= table.nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += table.nrows
            return table.modify_column(key, key + 1, 1,
                                       [[value]], self.pathname)
        elif isinstance(key, slice):
            (start, stop, step) = table._process_range(
                key.start, key.stop, key.step)
            return table.modify_column(start, stop, step,
                                       value, self.pathname)
        else:
            raise ValueError("Non-valid index or slice: %s" % key)

    def create_index(self, optlevel=6, kind="medium", filters=None,
                     tmp_dir=None, _blocksizes=None, _testmode=False,
                     _verbose=False):
        """Create an index for this column.

        .. warning::

            In some situations it is useful to get a completely sorted
            index (CSI).  For those cases, it is best to use the
            :meth:`Column.create_csindex` method instead.

        Parameters
        ----------
        optlevel : int
            The optimization level for building the index.  The levels ranges
            from 0 (no optimization) up to 9 (maximum optimization).  Higher
            levels of optimization mean better chances for reducing the entropy
            of the index at the price of using more CPU, memory and I/O
            resources for creating the index.
        kind : str
            The kind of the index to be built.  It can take the 'ultralight',
            'light', 'medium' or 'full' values.  Lighter kinds ('ultralight'
            and 'light') mean that the index takes less space on disk, but will
            perform queries slower.  Heavier kinds ('medium' and 'full') mean
            better chances for reducing the entropy of the index (increasing
            the query speed) at the price of using more disk space as well as
            more CPU, memory and I/O resources for creating the index.

            Note that selecting a full kind with an optlevel of 9 (the maximum)
            guarantees the creation of an index with zero entropy, that is, a
            completely sorted index (CSI) - provided that the number of rows in
            the table does not exceed the 2**48 figure (that is more than 100
            trillions of rows).  See :meth:`Column.create_csindex` method for a
            more direct way to create a CSI index.
        filters : Filters
            Specify the Filters instance used to compress the index.  If None,
            default index filters will be used (currently, zlib level 1 with
            shuffling).
        tmp_dir
            When kind is other than 'ultralight', a temporary file is created
            during the index build process.  You can use the tmp_dir argument
            to specify the directory for this temporary file.  The default is
            to create it in the same directory as the file containing the
            original table.

        """

        kinds = ['ultralight', 'light', 'medium', 'full']
        if kind not in kinds:
            raise ValueError("Kind must have any of these values: %s" % kinds)
        if (not isinstance(optlevel, (int, long)) or
                (optlevel < 0 or optlevel > 9)):
            raise ValueError("Optimization level must be an integer in the "
                             "range 0-9")
        if filters is None:
            filters = default_index_filters
        if tmp_dir is None:
            tmp_dir = os.path.dirname(self._table_file.filename)
        else:
            if not os.path.isdir(tmp_dir):
                raise ValueError("Temporary directory '%s' does not exist" %
                                 tmp_dir)
        if (_blocksizes is not None and
                (not isinstance(_blocksizes, tuple) or len(_blocksizes) != 4)):
            raise ValueError("_blocksizes must be a tuple with exactly 4 "
                             "elements")
        idxrows = _column__create_index(self, optlevel, kind, filters,
                                        tmp_dir, _blocksizes, _verbose)
        return SizeType(idxrows)

    createIndex = previous_api(create_index)

    def create_csindex(self, filters=None, tmp_dir=None,
                       _blocksizes=None, _testmode=False, _verbose=False):
        """Create a completely sorted index (CSI) for this column.

        This method guarantees the creation of an index with zero entropy, that
        is, a completely sorted index (CSI) -- provided that the number of rows
        in the table does not exceed the 2**48 figure (that is more than 100
        trillions of rows).  A CSI index is needed for some table methods (like
        :meth:`Table.itersorted` or :meth:`Table.read_sorted`) in order to
        ensure completely sorted results.

        For the meaning of filters and tmp_dir arguments see
        :meth:`Column.create_index`.

        Notes
        -----
        This method is equivalent to
        Column.create_index(optlevel=9, kind='full', ...).

        """

        return self.create_index(
            kind='full', optlevel=9, filters=filters, tmp_dir=tmp_dir,
            _blocksizes=_blocksizes, _testmode=_testmode, _verbose=_verbose)

    createCSIndex = previous_api(create_csindex)

    def _do_reindex(self, dirty):
        """Common code for reindex() and reindex_dirty() codes."""

        index = self.index
        dodirty = True
        if dirty and not index.dirty:
            dodirty = False
        if index is not None and dodirty:
            self._table_file._check_writable()
            # Get the old index parameters
            kind = index.kind
            optlevel = index.optlevel
            filters = index.filters
            # We *need* to tell the index that it is going to be undirty.
            # This is needed here so as to unnail() the condition cache.
            index.dirty = False
            # Delete the existing Index
            index._f_remove()
            # Create a new Index with the previous parameters
            return SizeType(self.create_index(
                kind=kind, optlevel=optlevel, filters=filters))
        else:
            return SizeType(0)  # The column is not intended for indexing

    _doReIndex = previous_api(_do_reindex)

    def reindex(self):
        """Recompute the index associated with this column.

        This can be useful when you suspect that, for any reason,
        the index information is no longer valid and you want to rebuild it.

        This method does nothing if the column is not indexed.

        """

        self._do_reindex(dirty=False)

    reIndex = previous_api(reindex)

    def reindex_dirty(self):
        """Recompute the associated index only if it is dirty.

        This can be useful when you have set :attr:`Table.autoindex` to false
        for the table and you want to update the column's index after an
        invalidating index operation (like :meth:`Table.remove_rows`).

        This method does nothing if the column is not indexed.

        """

        self._do_reindex(dirty=True)

    reIndexDirty = previous_api(reindex_dirty)

    def remove_index(self):
        """Remove the index associated with this column.

        This method does nothing if the column is not indexed. The removed
        index can be created again by calling the :meth:`Column.create_index`
        method.

        """

        self._table_file._check_writable()

        # Remove the index if existing.
        if self.is_indexed:
            index = self.index
            index._f_remove()
            self.table._set_column_indexing(self.pathname, False)

    removeIndex = previous_api(remove_index)

    def close(self):
        """Close this column."""

        self.__dict__.clear()

    def __str__(self):
        """The string representation for this object."""

        # The pathname
        tablepathname = self._table_path
        pathname = self.pathname.replace('/', '.')
        # Get this class name
        classname = self.__class__.__name__
        # The shape for this column
        shape = self.shape
        # The type
        tcol = self.descr._v_types[self.name]
        return "%s.cols.%s (%s%s, %s, idx=%s)" % \
               (tablepathname, pathname, classname, shape, tcol, self.index)

    def __repr__(self):
        """A detailed string representation for this object."""

        return str(self)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = tableExtension
from warnings import warn
from tables.tableextension import *

_warnmsg = ("tableExtension is pending deprecation, import tableextension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = check_leaks
# -*- coding: utf-8 -*-

from __future__ import print_function
import os
import time

import tables

tref = time.time()
trel = tref


def show_mem(explain):
    global tref, trel

    filename = "/proc/%s/status" % os.getpid()
    with open(filename) as fd:
        for line in fd:
            if line.startswith("VmSize:"):
                vmsize = int(line.split()[1])
            elif line.startswith("VmRSS:"):
                vmrss = int(line.split()[1])
            elif line.startswith("VmData:"):
                vmdata = int(line.split()[1])
            elif line.startswith("VmStk:"):
                vmstk = int(line.split()[1])
            elif line.startswith("VmExe:"):
                vmexe = int(line.split()[1])
            elif line.startswith("VmLib:"):
                vmlib = int(line.split()[1])

    print("\nMemory usage: ******* %s *******" % explain)
    print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
    print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
    print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))
    print("WallClock time:", time.time() - tref, end=' ')
    print("  Delta time:", time.time() - trel)
    trel = time.time()


def write_group(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            fileh.create_group(fileh.root, 'group' + str(child),
                               "child: %d" % child)
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_group(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'group' + str(child))
            assert node is not None
            # flavor = node._v_attrs.CLASS
#         for child in fileh.walk_nodes():
#             pass
        show_mem("After reading metadata. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_array(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            fileh.create_array(fileh.root, 'array' + str(child),
                               [1, 1], "child: %d" % child)
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_array(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'array' + str(child))
            # flavor = node._v_attrs.FLAVOR
            data = node[:]  # Read data
            assert data is not None
        show_mem("After reading data. Iter %s" % i)
#         for child in range(nchildren):
#             node = fileh.get_node(fileh.root, 'array' + str(child))
#             flavor = node._v_attrs.FLAVOR
            # flavor = node._v_attrs
#         for child in fileh.walk_nodes():
#             pass
#         show_mem("After reading metadata. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_carray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            fileh.create_carray(fileh.root, 'array' + str(child),
                                tables.IntAtom(), (2,), "child: %d" % child)
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_carray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'array' + str(child))
            # flavor = node._v_attrs.FLAVOR
            data = node[:]  # Read data
            assert data is not None
            # print("data-->", data)
        show_mem("After reading data. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_earray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            ea = fileh.create_earray(fileh.root, 'array' + str(child),
                                     tables.IntAtom(), shape=(0,),
                                     title="child: %d" % child)
            ea.append([1, 2, 3])
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_earray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'array' + str(child))
            # flavor = node._v_attrs.FLAVOR
            data = node[:]  # Read data
            assert data is not None
            # print("data-->", data)
        show_mem("After reading data. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_vlarray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            vl = fileh.create_vlarray(fileh.root, 'array' + str(child),
                                      tables.IntAtom(), "child: %d" % child)
            vl.append([1, 2, 3])
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_vlarray(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'array' + str(child))
            # flavor = node._v_attrs.FLAVOR
            data = node[:]  # Read data
            assert data is not None
            # print("data-->", data)
        show_mem("After reading data. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_table(filename, nchildren, niter):

    class Record(tables.IsDescription):
        var1 = tables.IntCol(pos=1)
        var2 = tables.StringCol(length=1, pos=2)
        var3 = tables.FloatCol(pos=3)

    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            t = fileh.create_table(fileh.root, 'table' + str(child),
                                   Record, "child: %d" % child)
            t.append([[1, "2", 3.]])
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_table(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'table' + str(child))
            # klass = node._v_attrs.CLASS
            data = node[:]  # Read data
            assert data is not None
            # print("data-->", data)
        show_mem("After reading data. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def write_xtable(filename, nchildren, niter):

    class Record(tables.IsDescription):
        var1 = tables.IntCol(pos=1)
        var2 = tables.StringCol(length=1, pos=2)
        var3 = tables.FloatCol(pos=3)

    for i in range(niter):
        fileh = tables.open_file(filename, mode="w")
        for child in range(nchildren):
            t = fileh.create_table(fileh.root, 'table' + str(child),
                                   Record, "child: %d" % child)
            t.append([[1, "2", 3.]])
            t.cols.var1.create_index()
        show_mem("After creating. Iter %s" % i)
        fileh.close()
        show_mem("After close")


def read_xtable(filename, nchildren, niter):
    for i in range(niter):
        fileh = tables.open_file(filename, mode="r")
        for child in range(nchildren):
            node = fileh.get_node(fileh.root, 'table' + str(child))
            # klass = node._v_attrs.CLASS
            # data = node[:]  # Read data
            # print("data-->", data)
        show_mem("After reading data. Iter %s" % i)
        fileh.close()
        show_mem("After close")
        del node


if __name__ == '__main__':
    import pstats
    import argparse
    import profile as prof

    def _get_parser():
        parser = argparse.ArgumentParser(
            description='Check for PyTables memory leaks.')
        parser.add_argument('-v', '--verbose', action='store_true',
                            help='enable verbose mode')
        parser.add_argument('-p', '--profile', action='store_true',
                            help='profile')
        parser.add_argument('-a', '--array', action='store_true',
                            help='create/read arrays (default)')
        parser.add_argument('-c', '--carray', action='store_true',
                            help='create/read carrays')
        parser.add_argument('-e', '--earray', action='store_true',
                            help='create/read earrays')
        parser.add_argument('-l', '--vlarray', action='store_true',
                            help='create/read vlarrays')
        parser.add_argument('-t', '--table', action='store_true',
                            help='create/read tables')
        parser.add_argument('-x', '--indexed-table', action='store_true',
                            dest='xtable', help='create/read indexed-tables')
        parser.add_argument('-g', '--group', action='store_true',
                            help='create/read groups')
        parser.add_argument('-r', '--read', action='store_true',
                            help='only read test')
        parser.add_argument('-w', '--write', action='store_true',
                            help='only write test')
        parser.add_argument('-n', '--nchildren', type=int, default=1000,
                            help='number of children (%(default)d is the '
                                 'default)')
        parser.add_argument('-i', '--niter', type=int, default=3,
                            help='number of iterations (default: %(default)d)')

        parser.add_argument('filename', help='HDF5 file name')

        return parser

    parser = _get_parser()
    args = parser.parse_args()

    # set 'array' as default value if no ather option has been specified
    for name in ('carray', 'earray', 'vlarray', 'table', 'xtable', 'group'):
        if getattr(args, name):
            break
    else:
        args.array = True

    filename = args.filename
    nchildren = args.nchildren
    niter = args.niter

    if args.array:
        fwrite = 'write_array'
        fread = 'read_array'
    elif args.carray:
        fwrite = 'write_carray'
        fread = 'read_carray'
    elif args.earray:
        fwrite = 'write_earray'
        fread = 'read_earray'
    elif args.vlarray:
        fwrite = 'write_vlarray'
        fread = 'read_vlarray'
    elif args.table:
        fwrite = 'write_table'
        fread = 'read_table'
    elif args.xtable:
        fwrite = 'write_xtable'
        fread = 'read_xtable'
    elif args.group:
        fwrite = 'write_group'
        fread = 'read_group'

    show_mem("Before open")
    if args.write:
        if args.profile:
            prof.run(str(fwrite)+'(filename, nchildren, niter)',
                     'write_file.prof')
            stats = pstats.Stats('write_file.prof')
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            if args.verbose:
                stats.print_stats()
            else:
                stats.print_stats(20)
        else:
            eval(fwrite+'(filename, nchildren, niter)')
    if args.read:
        if args.profile:
            prof.run(fread+'(filename, nchildren, niter)', 'read_file.prof')
            stats = pstats.Stats('read_file.prof')
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            if args.verbose:
                print('profile -verbose')
                stats.print_stats()
            else:
                stats.print_stats(20)
        else:
            eval(fread+'(filename, nchildren, niter)')

########NEW FILE########
__FILENAME__ = common
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-05-24
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Utilities for PyTables' test suites."""

from __future__ import print_function
import os
import sys
import time
import unittest
import tempfile
import warnings
import os.path

import numpy
import tables

verbose = False
"""Show detailed output of the testing process."""

heavy = False
"""Run all tests even when they take long to complete."""

show_memory = False
"""Show the progress of memory consumption."""

if 'verbose' in sys.argv:
    verbose = True
    sys.argv.remove('verbose')

if 'silent' in sys.argv:  # take care of old flag, just in case
    verbose = False
    sys.argv.remove('silent')

if '--heavy' in sys.argv:
    heavy = True
    sys.argv.remove('--heavy')


def verbosePrint(string, nonl=False):
    """Print out the `string` if verbose output is enabled."""
    if not verbose:
        return
    if nonl:
        print(string, end=' ')
    else:
        print(string)


def cleanup(klass):
    # klass.__dict__.clear()     # This is too hard. Don't do that
#    print("Class attributes deleted")
    for key in klass.__dict__:
        if not klass.__dict__[key].__class__.__name__ in ('instancemethod'):
            klass.__dict__[key] = None


def allequal(a, b, flavor="numpy"):
    """Checks if two numerical objects are equal."""

    # print("a-->", repr(a))
    # print("b-->", repr(b))
    if not hasattr(b, "shape"):
        # Scalar case
        return a == b

    if ((not hasattr(a, "shape") or a.shape == ()) and
            (not hasattr(b, "shape") or b.shape == ())):
        return a == b

    if a.shape != b.shape:
        if verbose:
            print("Shape is not equal:", a.shape, "!=", b.shape)
        return 0

    # Way to check the type equality without byteorder considerations
    if hasattr(b, "dtype") and a.dtype.str[1:] != b.dtype.str[1:]:
        if verbose:
            print("dtype is not equal:", a.dtype, "!=", b.dtype)
        return 0

    # Rank-0 case
    if len(a.shape) == 0:
        if a[()] == b[()]:
            return 1
        else:
            if verbose:
                print("Shape is not equal:", a.shape, "!=", b.shape)
            return 0

    # null arrays
    if a.size == 0:  # len(a) is not correct for generic shapes
        if b.size == 0:
            return 1
        else:
            if verbose:
                print("length is not equal")
                print("len(a.data) ==>", len(a.data))
                print("len(b.data) ==>", len(b.data))
            return 0

    # Multidimensional case
    result = (a == b)
    result = numpy.all(result)
    if not result and verbose:
        print("Some of the elements in arrays are not equal")

    return result


def areArraysEqual(arr1, arr2):
    """Are both `arr1` and `arr2` equal arrays?

    Arguments can be regular NumPy arrays, chararray arrays or
    structured arrays (including structured record arrays). They are
    checked for type and value equality.

    """

    t1 = type(arr1)
    t2 = type(arr2)

    if not ((hasattr(arr1, 'dtype') and arr1.dtype == arr2.dtype) or
            issubclass(t1, t2) or issubclass(t2, t1)):
        return False

    return numpy.all(arr1 == arr2)


def pyTablesTest(oldmethod):
    def newmethod(self, *args, **kwargs):
        self._verboseHeader()
        try:
            try:
                return oldmethod(self, *args, **kwargs)
            except SkipTest as se:
                if se.args:
                    msg = se.args[0]
                else:
                    msg = "<skipped>"
                verbosePrint("\nSkipped test: %s" % msg)
            except self.failureException as fe:
                if fe.args:
                    msg = fe.args[0]
                else:
                    msg = "<failed>"
                verbosePrint("\nTest failed: %s" % msg)
                raise
            except Exception as exc:
                cname = exc.__class__.__name__
                verbosePrint("\nError in test::\n\n  %s: %s" % (cname, exc))
                raise
        finally:
            verbosePrint('')  # separator line between tests
    newmethod.__name__ = oldmethod.__name__
    newmethod.__doc__ = oldmethod.__doc__
    return newmethod


class SkipTest(Exception):
    """When this exception is raised, the test is skipped successfully."""
    pass


class MetaPyTablesTestCase(type):

    """Metaclass for PyTables test case classes."""

    # http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/198078

    def __new__(class_, name, bases, dict_):
        newdict = {}
        for (aname, avalue) in dict_.iteritems():
            if callable(avalue) and aname.startswith('test'):
                avalue = pyTablesTest(avalue)
            newdict[aname] = avalue
        return type.__new__(class_, name, bases, newdict)


class PyTablesTestCase(unittest.TestCase):

    """Abstract test case with useful methods."""

    __metaclass__ = MetaPyTablesTestCase

    def _getName(self):
        """Get the name of this test case."""
        return self.id().split('.')[-2]

    def _getMethodName(self):
        """Get the name of the method currently running in the test case."""
        return self.id().split('.')[-1]

    def _verboseHeader(self):
        """Print a nice header for the current test method if verbose."""

        if verbose:
            name = self._getName()
            methodName = self._getMethodName()

            title = "Running %s.%s" % (name, methodName)
            print('%s\n%s' % (title, '-' * len(title)))

    @classmethod
    def _testFilename(class_, filename):
        """Returns an absolute version of the `filename`, taking care of the
        location of the calling test case class."""
        modname = class_.__module__
        # When the definitive switch to ``setuptools`` is made,
        # this should definitely use the ``pkg_resouces`` API::
        #
        #   return pkg_resources.resource_filename(modname, filename)
        #
        modfile = sys.modules[modname].__file__
        dirname = os.path.dirname(modfile)
        return os.path.join(dirname, filename)

    def failUnlessWarns(self, warnClass, callableObj, *args, **kwargs):
        """Fail unless a warning of class `warnClass` is issued.

        This method will fail if no warning belonging to the given
        `warnClass` is issued when invoking `callableObj` with arguments
        `args` and keyword arguments `kwargs`.  Warnings of the
        `warnClass` are hidden, while others are shown.

        This method returns the value returned by the call to
        `callableObj`.

        """

        issued = [False]  # let's avoid scoping problems ;)

        # Save the original warning-showing function.
        showwarning = warnings.showwarning

        # This warning-showing function hides and takes note
        # of expected warnings and acts normally on others.
        def myShowWarning(message, category, filename, lineno,
                          file=None, line=None):
            if issubclass(category, warnClass):
                issued[0] = True
                verbosePrint(
                    "Great!  The following ``%s`` was caught::\n"
                    "\n"
                    "  %s\n"
                    "\n"
                    "In file ``%s``, line number %d.\n"
                    % (category.__name__, message, filename, lineno))
            else:
                showwarning(message, category, filename, lineno, file, line)

        # By forcing Python to always show warnings of the wanted class,
        # and replacing the warning-showing function with a tailored one,
        # we can check for *every* occurence of the warning.
        warnings.filterwarnings('always', category=warnClass)
        warnings.showwarning = myShowWarning
        try:
            # Run code and see what happens.
            ret = callableObj(*args, **kwargs)
        finally:
            # Restore the original warning-showing function
            # and warning filter.
            warnings.showwarning = showwarning
            warnings.filterwarnings('default', category=warnClass)

        if not issued[0]:
            raise self.failureException(
                "``%s`` was not issued" % warnClass.__name__)

        # We only get here if the call to `callableObj` was successful
        # and it issued the expected warning.
        return ret

    assertWarns = failUnlessWarns

    def failUnlessRaises(self, excClass, callableObj, *args, **kwargs):
        if not verbose:
            # Use the ordinary implementation from `unittest.TestCase`.
            return super(PyTablesTestCase, self).assertRaises(
                excClass, callableObj, *args, **kwargs)

        try:
            callableObj(*args, **kwargs)
        except excClass as exc:
            print((
                "Great!  The following ``%s`` was caught::\n"
                "\n"
                "  %s\n"
                % (exc.__class__.__name__, exc)))
        else:
            raise self.failureException(
                "``%s`` was not raised" % excClass.__name__)

    assertRaises = failUnlessRaises

    def _checkEqualityGroup(self, node1, node2, hardlink=False):
        if verbose:
            print("Group 1:", node1)
            print("Group 2:", node2)
        if hardlink:
            self.assertTrue(node1._v_pathname != node2._v_pathname,
                            "node1 and node2 have the same pathnames.")
        else:
            self.assertTrue(node1._v_pathname == node2._v_pathname,
                "node1 and node2 does not have the same pathnames.")
        self.assertTrue(node1._v_children == node2._v_children,
                "node1 and node2 does not have the same children.")

    def _checkEqualityLeaf(self, node1, node2, hardlink=False):
        if verbose:
            print("Leaf 1:", node1)
            print("Leaf 2:", node2)
        if hardlink:
            self.assertTrue(node1._v_pathname != node2._v_pathname,
                "node1 and node2 have the same pathnames.")
        else:
            self.assertTrue(node1._v_pathname == node2._v_pathname,
                "node1 and node2 does not have the same pathnames.")
        self.assertTrue(areArraysEqual(node1[:], node2[:]),
            "node1 and node2 does not have the same values.")


class TempFileMixin:
    def setUp(self):
        """Set ``h5file`` and ``h5fname`` instance attributes.

        * ``h5fname``: the name of the temporary HDF5 file.
        * ``h5file``: the writable, empty, temporary HDF5 file.

        """

        self.h5fname = tempfile.mktemp(suffix='.h5')
        self.h5file = tables.open_file(
            self.h5fname, 'w', title=self._getName())

    def tearDown(self):
        """Close ``h5file`` and remove ``h5fname``."""

        self.h5file.close()
        self.h5file = None
        os.remove(self.h5fname)   # comment this for debugging purposes only

    def _reopen(self, mode='r'):
        """Reopen ``h5file`` in the specified ``mode``.

        Returns a true or false value depending on whether the file was
        reopenend or not.  If not, nothing is changed.

        """

        self.h5file.close()
        self.h5file = tables.open_file(self.h5fname, mode)
        return True


class ShowMemTime(PyTablesTestCase):
    tref = time.time()
    """Test for showing memory and time consumption."""

    def test00(self):
        """Showing memory and time consumption."""

        # Obtain memory info (only for Linux 2.6.x)
        for line in open("/proc/self/status"):
            if line.startswith("VmSize:"):
                vmsize = int(line.split()[1])
            elif line.startswith("VmRSS:"):
                vmrss = int(line.split()[1])
            elif line.startswith("VmData:"):
                vmdata = int(line.split()[1])
            elif line.startswith("VmStk:"):
                vmstk = int(line.split()[1])
            elif line.startswith("VmExe:"):
                vmexe = int(line.split()[1])
            elif line.startswith("VmLib:"):
                vmlib = int(line.split()[1])
        print("\nWallClock time:", time.time() - self.tref)
        print("Memory usage: ******* %s *******" % self._getName())
        print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
        print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
        print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = create_backcompat_indexes
# -*- coding: utf-8 -*-

# Script for creating different kind of indexes in a small space as possible.
# This is intended for testing purposes.

from tables import *


class Descr(IsDescription):
    var1 = StringCol(itemsize=4, shape=(), dflt='', pos=0)
    var2 = BoolCol(shape=(), dflt=False, pos=1)
    var3 = Int32Col(shape=(), dflt=0, pos=2)
    var4 = Float64Col(shape=(), dflt=0.0, pos=3)

# Parameters for the table and index creation
small_chunkshape = (2,)
small_blocksizes = (64, 32, 16, 8)
nrows = 43

# Create the new file
f = open_file('indexes_2_1.h5', 'w')
t1 = f.create_table(f.root, 'table1', Descr)
row = t1.row
for i in range(nrows):
    row['var1'] = i
    row['var2'] = i
    row['var3'] = i
    row['var4'] = i
    row.append()
t1.flush()

# Do a copy of table1
t1.copy(f.root, 'table2')

# Create indexes of all kinds
t1.cols.var1.create_index(0, 'ultralight', _blocksizes=small_blocksizes)
t1.cols.var2.create_index(3, 'light', _blocksizes=small_blocksizes)
t1.cols.var3.create_index(6, 'medium', _blocksizes=small_blocksizes)
t1.cols.var4.create_index(9, 'full', _blocksizes=small_blocksizes)

f.close()

########NEW FILE########
__FILENAME__ = test_all
# -*- coding: utf-8 -*-

"""Run all test cases."""

from __future__ import print_function
import re
import sys
import locale
import unittest
import platform

import numpy

import numexpr
import tables
from tables.req_versions import *
from tables.tests import common
from tables.utils import detect_number_of_cores


def get_tuple_version(hexversion):
    """Get a tuple from a compact version in hex."""
    h = hexversion
    return(h & 0xff0000) >> 16, (h & 0xff00) >> 8, h & 0xff


def suite():
    test_modules = [
        'tables.tests.test_attributes',
        'tables.tests.test_basics',
        'tables.tests.test_create',
        'tables.tests.test_backcompat',
        'tables.tests.test_types',
        'tables.tests.test_lists',
        'tables.tests.test_tables',
        'tables.tests.test_tablesMD',
        'tables.tests.test_array',
        'tables.tests.test_earray',
        'tables.tests.test_carray',
        'tables.tests.test_vlarray',
        'tables.tests.test_tree',
        'tables.tests.test_timetype',
        'tables.tests.test_do_undo',
        'tables.tests.test_enum',
        'tables.tests.test_nestedtypes',
        'tables.tests.test_hdf5compat',
        'tables.tests.test_numpy',
        'tables.tests.test_queries',
        'tables.tests.test_expression',
        'tables.tests.test_links',
        'tables.tests.test_indexes',
        'tables.tests.test_indexvalues',
        'tables.tests.test_index_backcompat',
        # Sub-packages
        'tables.nodes.tests.test_filenode',
    ]

    # print('-=' * 38)

    # The test for garbage must be run *in the last place*.
    # Else, it is not as useful.
    test_modules.append('tables.tests.test_garbage')

    alltests = unittest.TestSuite()
    if common.show_memory:
        # Add a memory report at the beginning
        alltests.addTest(unittest.makeSuite(common.ShowMemTime))
    for name in test_modules:
        # Unexpectedly, the following code doesn't seem to work anymore
        # in python 3
        # exec('from %s import suite as test_suite' % name)
        __import__(name)
        test_suite = sys.modules[name].suite

        alltests.addTest(test_suite())
        if common.show_memory:
            # Add a memory report after each test module
            alltests.addTest(unittest.makeSuite(common.ShowMemTime))
    return alltests


def print_versions():
    """Print all the versions of software that PyTables relies on."""

    print('-=' * 38)
    print("PyTables version:  %s" % tables.__version__)
    print("HDF5 version:      %s" % tables.which_lib_version("hdf5")[1])
    print("NumPy version:     %s" % numpy.__version__)
    tinfo = tables.which_lib_version("zlib")
    if numexpr.use_vml:
        # Get only the main version number and strip out all the rest
        vml_version = numexpr.get_vml_version()
        vml_version = re.findall("[0-9.]+", vml_version)[0]
        vml_avail = "using VML/MKL %s" % vml_version
    else:
        vml_avail = "not using Intel's VML/MKL"
    print("Numexpr version:   %s (%s)" % (numexpr.__version__, vml_avail))
    if tinfo is not None:
        print("Zlib version:      %s (%s)" % (tinfo[1],
                                              "in Python interpreter"))
    tinfo = tables.which_lib_version("lzo")
    if tinfo is not None:
        print("LZO version:       %s (%s)" % (tinfo[1], tinfo[2]))
    tinfo = tables.which_lib_version("bzip2")
    if tinfo is not None:
        print("BZIP2 version:     %s (%s)" % (tinfo[1], tinfo[2]))
    tinfo = tables.which_lib_version("blosc")
    if tinfo is not None:
        blosc_date = tinfo[2].split()[1]
        print("Blosc version:     %s (%s)" % (tinfo[1], blosc_date))
        blosc_cnames = tables.blosc_compressor_list()
        print("Blosc compressors: %s" % (blosc_cnames,))
    try:
        from Cython.Compiler.Main import Version as Cython_Version
        print('Cython version:    %s' % Cython_Version.version)
    except:
        pass
    print('Python version:    %s' % sys.version)
    print('Platform:          %s' % platform.platform())
    #if os.name == 'posix':
    #    (sysname, nodename, release, version, machine) = os.uname()
    #    print('Platform:          %s-%s' % (sys.platform, machine))
    print('Byte-ordering:     %s' % sys.byteorder)
    print('Detected cores:    %s' % detect_number_of_cores())
    print('Default encoding:  %s' % sys.getdefaultencoding())
    print('Default locale:    (%s, %s)' % locale.getdefaultlocale())
    print('-=' * 38)

    # This should improve readability whan tests are run by CI tools
    sys.stdout.flush()


def print_heavy(heavy):
    if heavy:
        print("""\
Performing the complete test suite!""")
    else:
        print("""\
Performing only a light (yet comprehensive) subset of the test suite.
If you want a more complete test, try passing the --heavy flag to this script
(or set the 'heavy' parameter in case you are using tables.test() call).
The whole suite will take more than 4 hours to complete on a relatively
modern CPU and around 512 MB of main memory.""")
    print('-=' * 38)


def test(verbose=False, heavy=False):
    """Run all the tests in the test suite.

    If *verbose* is set, the test suite will emit messages with full
    verbosity (not recommended unless you are looking into a certain
    problem).

    If *heavy* is set, the test suite will be run in *heavy* mode (you
    should be careful with this because it can take a lot of time and
    resources from your computer).

    Return 0 (os.EX_OK) if all tests pass, 1 in case of failure

    """

    print_versions()
    print_heavy(heavy)

    # What a context this is!
    #oldverbose, common.verbose = common.verbose, verbose
    oldheavy, common.heavy = common.heavy, heavy
    try:
        result = unittest.TextTestRunner(verbosity=1+int(verbose)).run(suite())
        if result.wasSuccessful():
            return 0
        else:
            return 1
    finally:
        #common.verbose = oldverbose
        common.heavy = oldheavy  # there are pretty young heavies, too ;)


if __name__ == '__main__':

    hdf5_version = get_tuple_version(tables.which_lib_version("hdf5")[0])
    if hdf5_version < min_hdf5_version:
        print("*Warning*: HDF5 version is lower than recommended: %s < %s" %
              (hdf5_version, min_hdf5_version))

    if numpy.__version__ < min_numpy_version:
        print("*Warning*: NumPy version is lower than recommended: %s < %s" %
              (numpy.__version__, min_numpy_version))

    # Handle some global flags (i.e. only useful for test_all.py)
    only_versions = 0
    args = sys.argv[:]
    for arg in args:
        # Remove 'show-versions' for PyTables 2.3 or higher
        if arg in ['--print-versions', '--show-versions']:
            only_versions = True
            sys.argv.remove(arg)
        elif arg == '--show-memory':
            common.show_memory = True
            sys.argv.remove(arg)

    print_versions()
    if not only_versions:
        print_heavy(common.heavy)
        unittest.main(defaultTest='tables.tests.suite')

########NEW FILE########
__FILENAME__ = test_array
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile
import warnings

import numpy

from tables import *

from tables.tests import common
from tables.utils import byteorders
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

warnings.resetwarnings()


class BasicTestCase(unittest.TestCase):
    """Basic test for all the supported typecodes present in numpy.

    All of them are included on pytables.

    """
    endiancheck = False

    def write_read(self, testarray):
        a = testarray
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running test for array with type '%s'" % a.dtype.type,
                  end=' ')
            print("for class check:", self.title)

        # Create an instance of HDF5 file
        filename = tempfile.mktemp(".h5")
        try:
            with open_file(filename, mode="w") as fileh:
                root = fileh.root

                # Create the array under root and name 'somearray'
                if self.endiancheck and a.dtype.kind != "S":
                    b = a.byteswap()
                    b.dtype = a.dtype.newbyteorder()
                    a = b

                fileh.create_array(root, 'somearray', a, "Some array")

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                root = fileh.root

                # Read the saved array
                b = root.somearray.read()

                # Compare them. They should be equal.
                if common.verbose and not allequal(a, b):
                    print("Write and read arrays differ!")
                    # print("Array written:", a)
                    print("Array written shape:", a.shape)
                    print("Array written itemsize:", a.itemsize)
                    print("Array written type:", a.dtype.type)
                    # print("Array read:", b)
                    print("Array read shape:", b.shape)
                    print("Array read itemsize:", b.itemsize)
                    print("Array read type:", b.dtype.type)
                    if a.dtype.kind != "S":
                        print("Array written byteorder:", a.dtype.byteorder)
                        print("Array read byteorder:", b.dtype.byteorder)

                # Check strictly the array equality
                self.assertEqual(a.shape, b.shape)
                self.assertEqual(a.shape, root.somearray.shape)
                if a.dtype.kind == "S":
                    self.assertEqual(root.somearray.atom.type, "string")
                else:
                    self.assertEqual(a.dtype.type, b.dtype.type)
                    self.assertEqual(a.dtype.type,
                                     root.somearray.atom.dtype.type)
                    abo = byteorders[a.dtype.byteorder]
                    bbo = byteorders[b.dtype.byteorder]
                    if abo != "irrelevant":
                        self.assertEqual(abo, root.somearray.byteorder)
                        self.assertEqual(bbo, sys.byteorder)
                        if self.endiancheck:
                            self.assertNotEqual(bbo, abo)

                obj = root.somearray
                self.assertEqual(obj.flavor, 'numpy')
                self.assertEqual(obj.shape, a.shape)
                self.assertEqual(obj.ndim, a.ndim)
                self.assertEqual(obj.chunkshape, None)
                if a.shape:
                    nrows = a.shape[0]
                else:
                    # scalar
                    nrows = 1

                self.assertEqual(obj.nrows, nrows)

                self.assertTrue(allequal(a, b))
        finally:
            # Then, delete the file
            os.remove(filename)

    def write_read_out_arg(self, testarray):
        a = testarray

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running test for array with type '%s'" % a.dtype.type,
                  end=' ')
            print("for class check:", self.title)

        # Create an instance of HDF5 file
        filename = tempfile.mktemp(".h5")
        try:
            with open_file(filename, mode="w") as fileh:
                root = fileh.root

                # Create the array under root and name 'somearray'
                if self.endiancheck and a.dtype.kind != "S":
                    b = a.byteswap()
                    b.dtype = a.dtype.newbyteorder()
                    a = b

                fileh.create_array(root, 'somearray', a, "Some array")

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                root = fileh.root

                # Read the saved array
                b = numpy.empty_like(a, dtype=a.dtype)
                root.somearray.read(out=b)

                # Check strictly the array equality
                self.assertEqual(a.shape, b.shape)
                self.assertEqual(a.shape, root.somearray.shape)
                if a.dtype.kind == "S":
                    self.assertEqual(root.somearray.atom.type, "string")
                else:
                    self.assertEqual(a.dtype.type, b.dtype.type)
                    self.assertEqual(a.dtype.type,
                                     root.somearray.atom.dtype.type)
                    abo = byteorders[a.dtype.byteorder]
                    bbo = byteorders[b.dtype.byteorder]
                    if abo != "irrelevant":
                        self.assertEqual(abo, root.somearray.byteorder)
                        self.assertEqual(abo, bbo)
                        if self.endiancheck:
                            self.assertNotEqual(bbo, sys.byteorder)

                self.assertTrue(allequal(a, b))
        finally:
            # Then, delete the file
            os.remove(filename)

    def write_read_atom_shape_args(self, testarray):
        a = testarray
        atom = Atom.from_dtype(a.dtype)
        shape = a.shape
        byteorder = None

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running test for array with type '%s'" % a.dtype.type,
                  end=' ')
            print("for class check:", self.title)

        # Create an instance of HDF5 file
        filename = tempfile.mktemp(".h5")
        try:
            with open_file(filename, mode="w") as fileh:
                root = fileh.root

                # Create the array under root and name 'somearray'
                if self.endiancheck and a.dtype.kind != "S":
                    b = a.byteswap()
                    b.dtype = a.dtype.newbyteorder()
                    if b.dtype.byteorder in ('>', '<'):
                        byteorder = byteorders[b.dtype.byteorder]
                    a = b

                ptarr = fileh.create_array(root, 'somearray',
                                           atom=atom, shape=shape,
                                           title="Some array",
                                           # specify the byteorder explicitly
                                           # since there is no way to deduce
                                           # it in this case
                                           byteorder=byteorder)
                self.assertEqual(shape, ptarr.shape)
                self.assertEqual(atom, ptarr.atom)
                ptarr[...] = a

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                root = fileh.root

                # Read the saved array
                b = root.somearray.read()

                # Compare them. They should be equal.
                if common.verbose and not allequal(a, b):
                    print("Write and read arrays differ!")
                    # print("Array written:", a)
                    print("Array written shape:", a.shape)
                    print("Array written itemsize:", a.itemsize)
                    print("Array written type:", a.dtype.type)
                    # print("Array read:", b)
                    print("Array read shape:", b.shape)
                    print("Array read itemsize:", b.itemsize)
                    print("Array read type:", b.dtype.type)
                    if a.dtype.kind != "S":
                        print("Array written byteorder:", a.dtype.byteorder)
                        print("Array read byteorder:", b.dtype.byteorder)

                # Check strictly the array equality
                self.assertEqual(a.shape, b.shape)
                self.assertEqual(a.shape, root.somearray.shape)
                if a.dtype.kind == "S":
                    self.assertEqual(root.somearray.atom.type, "string")
                else:
                    self.assertEqual(a.dtype.type, b.dtype.type)
                    self.assertEqual(a.dtype.type,
                                     root.somearray.atom.dtype.type)
                    abo = byteorders[a.dtype.byteorder]
                    bbo = byteorders[b.dtype.byteorder]
                    if abo != "irrelevant":
                        self.assertEqual(abo, root.somearray.byteorder)
                        self.assertEqual(bbo, sys.byteorder)
                        if self.endiancheck:
                            self.assertNotEqual(bbo, abo)

                obj = root.somearray
                self.assertEqual(obj.flavor, 'numpy')
                self.assertEqual(obj.shape, a.shape)
                self.assertEqual(obj.ndim, a.ndim)
                self.assertEqual(obj.chunkshape, None)
                if a.shape:
                    nrows = a.shape[0]
                else:
                    # scalar
                    nrows = 1

                self.assertEqual(obj.nrows, nrows)

                self.assertTrue(allequal(a, b))
        finally:
            # Then, delete the file
            os.remove(filename)

    def setup00_char(self):
        """Data integrity during recovery (character objects)"""

        if not isinstance(self.tupleChar, numpy.ndarray):
            a = numpy.array(self.tupleChar, dtype="S")
        else:
            a = self.tupleChar

        return a

    def test00_char(self):
        a = self.setup00_char()
        self.write_read(a)

    def test00_char_out_arg(self):
        a = self.setup00_char()
        self.write_read_out_arg(a)

    def test00_char_atom_shape_args(self):
        a = self.setup00_char()
        self.write_read_atom_shape_args(a)

    def test00b_char(self):
        """Data integrity during recovery (string objects)"""

        a = self.tupleChar

        filename = tempfile.mktemp(".h5")
        try:
            # Create an instance of HDF5 file
            with open_file(filename, mode="w") as fileh:
                fileh.create_array(fileh.root, 'somearray', a, "Some array")

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                # Read the saved array
                b = fileh.root.somearray.read()
                if isinstance(a, bytes):
                    self.assertEqual(type(b), bytes)
                    self.assertEqual(a, b)
                else:
                    # If a is not a python string, then it should be a list
                    # or ndarray
                    self.assertTrue(type(b) in [list, numpy.ndarray])
        finally:
            # Then, delete the file
            os.remove(filename)

    def test00b_char_out_arg(self):
        """Data integrity during recovery (string objects)"""

        a = self.tupleChar

        filename = tempfile.mktemp(".h5")
        try:
            # Create an instance of HDF5 file
            with open_file(filename, mode="w") as fileh:
                fileh.create_array(fileh.root, 'somearray', a, "Some array")

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                # Read the saved array
                b = numpy.empty_like(a)
                if fileh.root.somearray.flavor != 'numpy':
                    self.assertRaises(TypeError,
                                      lambda: fileh.root.somearray.read(out=b))
                else:
                    fileh.root.somearray.read(out=b)
                self.assertTrue(type(b), numpy.ndarray)
        finally:
            # Then, delete the file
            os.remove(filename)

    def test00b_char_atom_shape_args(self):
        """Data integrity during recovery (string objects)"""

        a = self.tupleChar

        filename = tempfile.mktemp(".h5")
        try:
            # Create an instance of HDF5 file
            with open_file(filename, mode="w") as fileh:
                nparr = numpy.asarray(a)
                atom = Atom.from_dtype(nparr.dtype)
                shape = nparr.shape
                if nparr.dtype.byteorder in ('>', '<'):
                    byteorder = byteorders[nparr.dtype.byteorder]
                else:
                    byteorder = None

                ptarr = fileh.create_array(fileh.root, 'somearray',
                                           atom=atom, shape=shape,
                                           byteorder=byteorder,
                                           title="Some array")
                self.assertEqual(shape, ptarr.shape)
                self.assertEqual(atom, ptarr.atom)
                ptarr[...] = a

            # Re-open the file in read-only mode
            with open_file(filename, mode="r") as fileh:
                # Read the saved array
                b = numpy.empty_like(a)
                if fileh.root.somearray.flavor != 'numpy':
                    self.assertRaises(TypeError,
                                      lambda: fileh.root.somearray.read(out=b))
                else:
                    fileh.root.somearray.read(out=b)
                self.assertTrue(type(b), numpy.ndarray)
        finally:
            # Then, delete the file
            os.remove(filename)

    def setup01_char_nc(self):
        """Data integrity during recovery (non-contiguous character objects)"""

        if not isinstance(self.tupleChar, numpy.ndarray):
            a = numpy.array(self.tupleChar, dtype="S")
        else:
            a = self.tupleChar
        if a.ndim == 0:
            b = a.copy()
        else:
            b = a[::2]
            # Ensure that this numpy string is non-contiguous
            if len(b) > 1:
                self.assertEqual(b.flags.contiguous, False)
        return b

    def test01_char_nc(self):
        b = self.setup01_char_nc()
        self.write_read(b)

    def test01_char_nc_out_arg(self):
        b = self.setup01_char_nc()
        self.write_read_out_arg(b)

    def test01_char_nc_atom_shape_args(self):
        b = self.setup01_char_nc()
        self.write_read_atom_shape_args(b)

    def test02_types(self):
        """Data integrity during recovery (numerical types)"""

        typecodes = ['int8', 'int16', 'int32', 'int64',
                     'uint8', 'uint16', 'uint32', 'uint64',
                     'float32', 'float64',
                     'complex64', 'complex128']

        for name in ('float16', 'float96', 'float128',
                     'complex192', 'complex256'):
            atomname = name.capitalize() + 'Atom'
            if atomname in globals():
                typecodes.append(name)

        for typecode in typecodes:
            a = numpy.array(self.tupleInt, typecode)
            self.write_read(a)
            b = numpy.array(self.tupleInt, typecode)
            self.write_read_out_arg(b)
            c = numpy.array(self.tupleInt, typecode)
            self.write_read_atom_shape_args(c)

    def test03_types_nc(self):
        """Data integrity during recovery (non-contiguous numerical types)"""

        typecodes = ['int8', 'int16', 'int32', 'int64',
                     'uint8', 'uint16', 'uint32', 'uint64',
                     'float32', 'float64',
                     'complex64', 'complex128', ]

        for name in ('float16', 'float96', 'float128',
                     'complex192', 'complex256'):
            atomname = name.capitalize() + 'Atom'
            if atomname in globals():
                typecodes.append(name)

        for typecode in typecodes:
            a = numpy.array(self.tupleInt, typecode)
            if a.ndim == 0:
                b1 = a.copy()
                b2 = a.copy()
                b3 = a.copy()
            else:
                b1 = a[::2]
                b2 = a[::2]
                b3 = a[::2]
                # Ensure that this array is non-contiguous
                if len(b1) > 1:
                    self.assertEqual(b1.flags.contiguous, False)
                if len(b2) > 1:
                    self.assertEqual(b2.flags.contiguous, False)
                if len(b3) > 1:
                    self.assertEqual(b3.flags.contiguous, False)
            self.write_read(b1)
            self.write_read_out_arg(b2)
            self.write_read_atom_shape_args(b3)


class Basic0DOneTestCase(BasicTestCase):
    # Scalar case
    title = "Rank-0 case 1"
    tupleInt = 3
    tupleChar = b"3"
    endiancheck = True


class Basic0DTwoTestCase(BasicTestCase):
    # Scalar case
    title = "Rank-0 case 2"
    tupleInt = 33
    tupleChar = b"33"
    endiancheck = True


class Basic1DZeroTestCase(BasicTestCase):
    # This test case is not supported by PyTables (HDF5 limitations)
    # 1D case
    title = "Rank-1 case 0"
    tupleInt = ()
    tupleChar = ()
    endiancheck = False


class Basic1DOneTestCase(BasicTestCase):
    "Method doc"
    # 1D case
    title = "Rank-1 case 1"
    tupleInt = (3,)
    tupleChar = (b"a",)
    endiancheck = True


class Basic1DTwoTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 2"
    tupleInt = (3, 4)
    tupleChar = (b"aaa",)
    endiancheck = True


class Basic1DThreeTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 3"
    tupleInt = (3, 4, 5)
    tupleChar = (b"aaa", b"bbb",)
    endiancheck = True


class Basic2DOneTestCase(BasicTestCase):
    # 2D case
    title = "Rank-2 case 1"
    tupleInt = numpy.array(numpy.arange((4)**2))
    tupleInt.shape = (4,)*2
    tupleChar = numpy.array(["abc"]*3**2, dtype="S3")
    tupleChar.shape = (3,)*2
    endiancheck = True


class Basic2DTwoTestCase(BasicTestCase):
    # 2D case, with a multidimensional dtype
    title = "Rank-2 case 2"
    tupleInt = numpy.array(numpy.arange((4)), dtype=(numpy.int_, (4,)))
    tupleChar = numpy.array(["abc"]*3, dtype=("S3", (3,)))
    endiancheck = True


class Basic10DTestCase(BasicTestCase):
    # 10D case
    title = "Rank-10 test"
    tupleInt = numpy.array(numpy.arange((2)**10))
    tupleInt.shape = (2,)*10
    tupleChar = numpy.array(
        ["abc"]*2**10, dtype="S3")
    tupleChar.shape = (2,)*10
    endiancheck = True


class Basic32DTestCase(BasicTestCase):
    # 32D case (maximum)
    title = "Rank-32 test"
    tupleInt = numpy.array((32,))
    tupleInt.shape = (1,)*32
    tupleChar = numpy.array(["121"], dtype="S3")
    tupleChar.shape = (1,)*32


class ReadOutArgumentTests(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode='w')
        self.size = 1000

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def create_array(self):
        array = numpy.arange(self.size, dtype='f8')
        disk_array = self.fileh.create_array('/', 'array', array)
        return array, disk_array

    def test_read_entire_array(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size, ), 'f8')
        disk_array.read(out=out_buffer)
        numpy.testing.assert_equal(out_buffer, array)

    def test_read_contiguous_slice1(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.arange(self.size, dtype='f8')
        out_buffer = numpy.random.permutation(out_buffer)
        out_buffer_orig = out_buffer.copy()
        start = self.size // 2
        disk_array.read(start=start, stop=self.size, out=out_buffer[start:])
        numpy.testing.assert_equal(out_buffer[start:], array[start:])
        numpy.testing.assert_equal(out_buffer[:start], out_buffer_orig[:start])

    def test_read_contiguous_slice2(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.arange(self.size, dtype='f8')
        out_buffer = numpy.random.permutation(out_buffer)
        out_buffer_orig = out_buffer.copy()
        start = self.size // 4
        stop = self.size - start
        disk_array.read(start=start, stop=stop, out=out_buffer[start:stop])
        numpy.testing.assert_equal(out_buffer[start:stop], array[start:stop])
        numpy.testing.assert_equal(out_buffer[:start], out_buffer_orig[:start])
        numpy.testing.assert_equal(out_buffer[stop:], out_buffer_orig[stop:])

    def test_read_non_contiguous_slice_contiguous_buffer(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size // 2, ), dtype='f8')
        disk_array.read(start=0, stop=self.size, step=2, out=out_buffer)
        numpy.testing.assert_equal(out_buffer, array[0:self.size:2])

    def test_read_non_contiguous_buffer(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size, ), 'f8')
        out_buffer_slice = out_buffer[0:self.size:2]
        # once Python 2.6 support is dropped, this could change
        # to assertRaisesRegexp to check exception type and message at once
        self.assertRaises(ValueError, disk_array.read, 0, self.size, 2,
                          out_buffer_slice)
        try:
            disk_array.read(0, self.size, 2, out_buffer_slice)
        except ValueError as exc:
            self.assertEqual('output array not C contiguous', str(exc))

    def test_buffer_too_small(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size // 2, ), 'f8')
        self.assertRaises(ValueError, disk_array.read, 0, self.size, 1,
                          out_buffer)
        try:
            disk_array.read(0, self.size, 1, out_buffer)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))

    def test_buffer_too_large(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size + 1, ), 'f8')
        self.assertRaises(ValueError, disk_array.read, 0, self.size, 1,
                          out_buffer)
        try:
            disk_array.read(0, self.size, 1, out_buffer)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))


class SizeOnDiskInMemoryPropertyTestCase(unittest.TestCase):

    def setUp(self):
        self.array_size = (10, 10)
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.array = self.fileh.create_array(
            '/', 'somearray', numpy.zeros(self.array_size, 'i4'))

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test_all_zeros(self):
        self.assertEqual(self.array.size_on_disk, 10 * 10 * 4)
        self.assertEqual(self.array.size_in_memory, 10 * 10 * 4)


class UnalignedAndComplexTestCase(unittest.TestCase):
    """Basic test for all the supported typecodes present in numpy.

    Most of them are included on PyTables.

    """

    def setUp(self):
        # Create an instance of HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.root = self.fileh.root

    def tearDown(self):
        self.fileh.close()

        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def write_read(self, testArray):
        if common.verbose:
            print('\n', '-=' * 30)
            print("\nRunning test for array with type '%s'" %
                  testArray.dtype.type)

        # Create the array under root and name 'somearray'
        a = testArray
        if self.endiancheck:
            byteorder = {"little": "big", "big": "little"}[sys.byteorder]
        else:
            byteorder = sys.byteorder

        self.fileh.create_array(self.root, 'somearray', a, "Some array",
                                byteorder=byteorder)

        if self.reopen:
            self.fileh.close()
            # Re-open the file in read-only mode
            self.fileh = open_file(self.file, mode="r")
            self.root = self.fileh.root

        # Read the saved array
        b = self.root.somearray.read()

        # Get an array to be compared in the correct byteorder
        c = a.newbyteorder(byteorder)

        # Compare them. They should be equal.
        if not allequal(c, b) and common.verbose:
            print("Write and read arrays differ!")
            print("Array written:", a)
            print("Array written shape:", a.shape)
            print("Array written itemsize:", a.itemsize)
            print("Array written type:", a.dtype.type)
            print("Array read:", b)
            print("Array read shape:", b.shape)
            print("Array read itemsize:", b.itemsize)
            print("Array read type:", b.dtype.type)

        # Check strictly the array equality
        self.assertEqual(a.shape, b.shape)
        self.assertEqual(a.shape, self.root.somearray.shape)
        if a.dtype.byteorder != "|":
            self.assertEqual(a.dtype, b.dtype)
            self.assertEqual(a.dtype, self.root.somearray.atom.dtype)
            self.assertEqual(byteorders[b.dtype.byteorder], sys.byteorder)
            self.assertEqual(self.root.somearray.byteorder, byteorder)

        self.assertTrue(allequal(c, b))

    def test01_signedShort_unaligned(self):
        "Checking an unaligned signed short integer array"

        r = numpy.rec.array(b'a'*200, formats='i1,f4,i2', shape=10)
        a = r["f2"]
        # Ensure that this array is non-aligned
        self.assertEqual(a.flags.aligned, False)
        self.assertEqual(a.dtype.type, numpy.int16)
        self.write_read(a)

    def test02_float_unaligned(self):
        "Checking an unaligned single precision array"

        r = numpy.rec.array(b'a'*200, formats='i1,f4,i2', shape=10)
        a = r["f1"]
        # Ensure that this array is non-aligned
        self.assertEqual(a.flags.aligned, 0)
        self.assertEqual(a.dtype.type, numpy.float32)
        self.write_read(a)

    def test03_byte_offset(self):
        "Checking an offsetted byte array"

        r = numpy.arange(100, dtype=numpy.int8)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test04_short_offset(self):
        "Checking an offsetted unsigned short int precision array"

        r = numpy.arange(100, dtype=numpy.uint32)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test05_int_offset(self):
        "Checking an offsetted integer array"

        r = numpy.arange(100, dtype=numpy.int32)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test06_longlongint_offset(self):
        "Checking an offsetted long long integer array"

        r = numpy.arange(100, dtype=numpy.int64)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test07_float_offset(self):
        "Checking an offsetted single precision array"

        r = numpy.arange(100, dtype=numpy.float32)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test08_double_offset(self):
        "Checking an offsetted double precision array"

        r = numpy.arange(100, dtype=numpy.float64)
        r.shape = (10, 10)
        a = r[2]
        self.write_read(a)

    def test09_float_offset_unaligned(self):
        "Checking an unaligned and offsetted single precision array"

        r = numpy.rec.array(b'a'*200, formats='i1,3f4,i2', shape=10)
        a = r["f1"][3]
        # Ensure that this array is non-aligned
        self.assertEqual(a.flags.aligned, False)
        self.assertEqual(a.dtype.type, numpy.float32)
        self.write_read(a)

    def test10_double_offset_unaligned(self):
        "Checking an unaligned and offsetted double precision array"

        r = numpy.rec.array(b'a'*400, formats='i1,3f8,i2', shape=10)
        a = r["f1"][3]
        # Ensure that this array is non-aligned
        self.assertEqual(a.flags.aligned, False)
        self.assertEqual(a.dtype.type, numpy.float64)
        self.write_read(a)

    def test11_int_byteorder(self):
        "Checking setting data with different byteorder in a range (integer)"

        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Save an array with the reversed byteorder on it
        a = numpy.arange(25, dtype=numpy.int32).reshape(5, 5)
        a = a.byteswap()
        a = a.newbyteorder()
        array = fileh.create_array(fileh.root, 'array', a, "byteorder (int)")
        # Read a subarray (got an array with the machine byteorder)
        b = array[2:4, 3:5]
        b = b.byteswap()
        b = b.newbyteorder()
        # Set this subarray back to the array
        array[2:4, 3:5] = b
        b = b.byteswap()
        b = b.newbyteorder()
        # Set this subarray back to the array
        array[2:4, 3:5] = b
        # Check that the array is back in the correct byteorder
        c = array[...]
        if common.verbose:
            print("byteorder of array on disk-->", array.byteorder)
            print("byteorder of subarray-->", b.dtype.byteorder)
            print("subarray-->", b)
            print("retrieved array-->", c)
        self.assertTrue(allequal(a, c))
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test12_float_byteorder(self):
        "Checking setting data with different byteorder in a range (float)"

        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Save an array with the reversed byteorder on it
        a = numpy.arange(25, dtype=numpy.float64).reshape(5, 5)
        a = a.byteswap()
        a = a.newbyteorder()
        array = fileh.create_array(fileh.root, 'array', a, "byteorder (float)")
        # Read a subarray (got an array with the machine byteorder)
        b = array[2:4, 3:5]
        b = b.byteswap()
        b = b.newbyteorder()
        # Set this subarray back to the array
        array[2:4, 3:5] = b
        b = b.byteswap()
        b = b.newbyteorder()
        # Set this subarray back to the array
        array[2:4, 3:5] = b
        # Check that the array is back in the correct byteorder
        c = array[...]
        if common.verbose:
            print("byteorder of array on disk-->", array.byteorder)
            print("byteorder of subarray-->", b.dtype.byteorder)
            print("subarray-->", b)
            print("retrieved array-->", c)
        self.assertTrue(allequal(a, c))
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)


class ComplexNotReopenNotEndianTestCase(UnalignedAndComplexTestCase):
    endiancheck = False
    reopen = False


class ComplexReopenNotEndianTestCase(UnalignedAndComplexTestCase):
    endiancheck = False
    reopen = True


class ComplexNotReopenEndianTestCase(UnalignedAndComplexTestCase):
    endiancheck = True
    reopen = False


class ComplexReopenEndianTestCase(UnalignedAndComplexTestCase):
    endiancheck = True
    reopen = True


class GroupsArrayTestCase(unittest.TestCase):
    """This test class checks combinations of arrays with groups."""

    def test00_iterativeGroups(self):
        """Checking combinations of arrays with groups."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_iterativeGroups..." %
                  self.__class__.__name__)

        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")

        # Get the root group
        group = fileh.root

        # Set the type codes to test
        # The typecodes below does expose an ambiguity that is reported in:
        # http://projects.scipy.org/scipy/numpy/ticket/283 and
        # http://projects.scipy.org/scipy/numpy/ticket/290
        typecodes = ['b', 'B', 'h', 'H', 'i', 'I', 'l', 'L', 'q', 'f', 'd',
                     'F', 'D']
        if 'Float16Atom' in globals():
            typecodes.append('e')
        if 'Float96Atom' in globals() or 'Float128Atom' in globals():
            typecodes.append('g')
        if 'Complex192Atom' in globals() or 'Complex256Atom' in globals():
            typecodes.append('G')

        for i, typecode in enumerate(typecodes):
            a = numpy.ones((3,), typecode)
            dsetname = 'array_' + typecode
            if common.verbose:
                print("Creating dataset:", group._g_join(dsetname))
            fileh.create_array(group, dsetname, a, "Large array")
            group = fileh.create_group(group, 'group' + str(i))

        # Close the file
        fileh.close()

        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        # Get the root group
        group = fileh.root

        # Get the metadata on the previosly saved arrays
        for i in range(len(typecodes)):
            # Create an array for later comparison
            a = numpy.ones((3,), typecodes[i])
            # Get the dset object hanging from group
            dset = getattr(group, 'array_' + typecodes[i])
            # Get the actual array
            b = dset.read()
            if common.verbose:
                print("Info from dataset:", dset._v_pathname)
                print("  shape ==>", dset.shape, end=' ')
                print("  type ==> %s" % dset.atom.dtype)
                print("Array b read from file. Shape: ==>", b.shape, end=' ')
                print(". Type ==> %s" % b.dtype)
            self.assertEqual(a.shape, b.shape)
            self.assertEqual(a.dtype, b.dtype)
            self.assertTrue(allequal(a, b))

            # Iterate over the next group
            group = getattr(group, 'group' + str(i))

        # Close the file
        fileh.close()

        # Then, delete the file
        os.remove(file)
        del a, b, fileh

    def test01_largeRankArrays(self):
        """Checking creation of large rank arrays (0 < rank <= 32)
        It also uses arrays ranks which ranges until maxrank.
        """

        # maximum level of recursivity (deepest group level) achieved:
        # maxrank = 32 (for a effective maximum rank of 32)
        # This limit is due to HDF5 library limitations.
        minrank = 1
        maxrank = 32

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_largeRankArrays..." %
                  self.__class__.__name__)
            print("Maximum rank for tested arrays:", maxrank)
        # Open a new empty HDF5 file
        # file = tempfile.mktemp(".h5")
        file = "test_array.h5"
        fileh = open_file(file, mode="w")
        group = fileh.root
        if common.verbose:
            print("Rank array writing progress: ", end=' ')
        for rank in range(minrank, maxrank + 1):
            # Create an array of integers, with incrementally bigger ranges
            a = numpy.ones((1,) * rank, numpy.int32)
            if common.verbose:
                print("%3d," % (rank), end=' ')
            fileh.create_array(group, "array", a, "Rank: %s" % rank)
            group = fileh.create_group(group, 'group' + str(rank))
        # Flush the buffers
        fileh.flush()
        # Close the file
        fileh.close()

        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        group = fileh.root
        if common.verbose:
            print()
            print("Rank array reading progress: ")
        # Get the metadata on the previosly saved arrays
        for rank in range(minrank, maxrank + 1):
            # Create an array for later comparison
            a = numpy.ones((1,) * rank, numpy.int32)
            # Get the actual array
            b = group.array.read()
            if common.verbose:
                print("%3d," % (rank), end=' ')
            if common.verbose and not allequal(a, b):
                print("Info from dataset:", dset._v_pathname)
                print("  Shape: ==>", dset.shape, end=' ')
                print("  typecode ==> %c" % dset.typecode)
                print("Array b read from file. Shape: ==>", b.shape, end=' ')
                print(". Type ==> %c" % b.dtype)

            self.assertEqual(a.shape, b.shape)
            self.assertEqual(a.dtype, b.dtype)
            self.assertTrue(allequal(a, b))

            # print(fileh)
            # Iterate over the next group
            group = fileh.get_node(group, 'group' + str(rank))

        if common.verbose:
            print()  # This flush the stdout buffer
        # Close the file
        fileh.close()

        # Delete the file
        os.remove(file)


class CopyTestCase(unittest.TestCase):

    def test01_copy(self):
        """Checking Array.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        arr = numpy.array([[456, 2], [3, 457]], dtype='int16')
        array1 = fileh.create_array(fileh.root, 'array1', arr, "title array1")

        # Copy to another Array
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_copy(self):
        """Checking Array.copy() method (where specified)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        arr = numpy.array([[456, 2], [3, 457]], dtype='int16')
        array1 = fileh.create_array(fileh.root, 'array1', arr, "title array1")

        # Copy to another Array
        group1 = fileh.create_group("/", "group1")
        array2 = array1.copy(group1, 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.group1.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03_copy(self):
        """Checking Array.copy() method (checking title copying)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        arr = numpy.array([[456, 2], [3, 457]], dtype='int16')
        array1 = fileh.create_array(fileh.root, 'array1', arr, "title array1")
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2
        # Copy it to another Array
        array2 = array1.copy('/', 'array2', title="title array2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        # Assert user attributes
        if common.verbose:
            print("title of destination array-->", array2.title)
        self.assertEqual(array2.title, "title array2")

        # Close the file
        fileh.close()
        os.remove(file)

    def test04_copy(self):
        """Checking Array.copy() method (user attributes copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        arr = numpy.array([[456, 2], [3, 457]], dtype='int16')
        array1 = fileh.create_array(fileh.root, 'array1', arr, "title array1")
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2
        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(array2.attrs.attr1, "attr1")
        self.assertEqual(array2.attrs.attr2, 2)

        # Close the file
        fileh.close()
        os.remove(file)

    def test04b_copy(self):
        """Checking Array.copy() method (user attributes not copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        arr = numpy.array([[456, 2], [3, 457]], dtype='int16')
        array1 = fileh.create_array(fileh.root, 'array1', arr, "title array1")
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2
        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(hasattr(array2.attrs, "attr1"), 0)
        self.assertEqual(hasattr(array2.attrs, "attr2"), 0)

        # Close the file
        fileh.close()
        os.remove(file)


class CloseCopyTestCase(CopyTestCase):
    close = 1


class OpenCopyTestCase(CopyTestCase):
    close = 0


class CopyIndexTestCase(unittest.TestCase):

    def test01_index(self):
        """Checking Array.copy() method with indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_index..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a numpy
        r = numpy.arange(200, dtype='int32')
        r.shape = (100, 2)
        # Save it in a array:
        array1 = fileh.create_array(fileh.root, 'array1', r, "title array1")

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_indexclosef(self):
        """Checking Array.copy() method with indexes (close file version)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_indexclosef..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a numpy
        r = numpy.arange(200, dtype='int32')
        r.shape = (100, 2)
        # Save it in a array:
        array1 = fileh.create_array(fileh.root, 'array1', r, "title array1")

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        # Close and reopen the file
        fileh.close()
        fileh = open_file(file, mode="r")
        array1 = fileh.root.array1
        array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)


class CopyIndex1TestCase(CopyIndexTestCase):
    start = 0
    stop = 7
    step = 1


class CopyIndex2TestCase(CopyIndexTestCase):
    start = 0
    stop = -1
    step = 1


class CopyIndex3TestCase(CopyIndexTestCase):
    start = 1
    stop = 7
    step = 1


class CopyIndex4TestCase(CopyIndexTestCase):
    start = 0
    stop = 6
    step = 1


class CopyIndex5TestCase(CopyIndexTestCase):
    start = 3
    stop = 7
    step = 1


class CopyIndex6TestCase(CopyIndexTestCase):
    start = 3
    stop = 6
    step = 2


class CopyIndex7TestCase(CopyIndexTestCase):
    start = 0
    stop = 7
    step = 10


class CopyIndex8TestCase(CopyIndexTestCase):
    start = 6
    stop = -1  # Negative values means starting from the end
    step = 1


class CopyIndex9TestCase(CopyIndexTestCase):
    start = 3
    stop = 4
    step = 1


class CopyIndex10TestCase(CopyIndexTestCase):
    start = 3
    stop = 4
    step = 2


class CopyIndex11TestCase(CopyIndexTestCase):
    start = -3
    stop = -1
    step = 2


class CopyIndex12TestCase(CopyIndexTestCase):
    start = -1   # Should point to the last element
    stop = None  # None should mean the last element (including it)
    step = 1


class GetItemTestCase(unittest.TestCase):

    def test00_single(self):
        "Single element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0], type(a[0]))
            print("Read first element:", arr[0], type(arr[0]))
        self.assertTrue(allequal(a[0], arr[0]))
        self.assertEqual(type(a[0]), type(arr[0]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test01_single(self):
        "Single element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0], type(a[0]))
            print("Read first element:", arr[0], type(arr[0]))
        self.assertEqual(a[0], arr[0])
        self.assertEqual(type(a[0]), type(arr[0]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test02_range(self):
        "Range element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertTrue(allequal(a[1:4], arr[1:4]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test03_range(self):
        "Range element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertTrue(allequal(a[1:4], arr[1:4]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test04_range(self):
        "Range element access, strided (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertTrue(allequal(a[1:4:2], arr[1:4:2]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test05_range(self):
        "Range element access, strided (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertTrue(allequal(a[1:4:2], arr[1:4:2]))
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test06_negativeIndex(self):
        "Negative Index element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original last element:", a[-1])
            print("Read last element:", arr[-1])
        self.assertTrue(allequal(a[-1], arr[-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test07_negativeIndex(self):
        "Negative Index element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original before last element:", a[-2])
            print("Read before last element:", arr[-2])
        if isinstance(a[-2], numpy.ndarray):
            self.assertTrue(allequal(a[-2], arr[-2]))
        else:
            self.assertEqual(a[-2], arr[-2])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test08_negativeRange(self):
        "Negative range element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertTrue(allequal(a[-4:-1], arr[-4:-1]))
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test09_negativeRange(self):
        "Negative range element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertTrue(allequal(a[-4:-1], arr[-4:-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return


class GI1NATestCase(GetItemTestCase):
    title = "Rank-1 case 1"
    numericalList = numpy.array([3])
    numericalListME = numpy.array([3, 2, 1, 0, 4, 5, 6])
    charList = numpy.array(["3"], 'S')
    charListME = numpy.array(
        ["321", "221", "121", "021", "421", "521", "621"], 'S')


class GI1NAOpenTestCase(GI1NATestCase):
    close = 0


class GI1NACloseTestCase(GI1NATestCase):
    close = 1


class GI2NATestCase(GetItemTestCase):
    # A more complex example
    title = "Rank-1,2 case 2"
    numericalList = numpy.array([3, 4])
    numericalListME = numpy.array([[3, 2, 1, 0, 4, 5, 6],
                                   [2, 1, 0, 4, 5, 6, 7],
                                   [4, 3, 2, 1, 0, 4, 5],
                                   [3, 2, 1, 0, 4, 5, 6],
                                   [3, 2, 1, 0, 4, 5, 6]])

    charList = numpy.array(["a", "b"], 'S')
    charListME = numpy.array(
        [["321", "221", "121", "021", "421", "521", "621"],
         ["21", "21", "11", "02", "42", "21", "61"],
         ["31", "21", "12", "21", "41", "51", "621"],
         ["321", "221", "121", "021",
          "421", "521", "621"],
         ["3241", "2321", "13216",
          "0621", "4421", "5421", "a621"],
         ["a321", "s221", "d121", "g021", "b421", "5vvv21", "6zxzxs21"]], 'S')


class GI2NAOpenTestCase(GI2NATestCase):
    close = 0


class GI2NACloseTestCase(GI2NATestCase):
    close = 1


class SetItemTestCase(unittest.TestCase):

    def test00_single(self):
        "Single element update (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify a single element of a and arr:
        a[0] = b"b"
        arr[0] = b"b"

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0])
            print("Read first element:", arr[0])
        self.assertTrue(allequal(a[0], arr[0]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test01_single(self):
        "Single element update (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        a[0] = 333
        arr[0] = 333

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0])
            print("Read first element:", arr[0])
        self.assertEqual(a[0], arr[0])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test02_range(self):
        "Range element update (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        a[1:3] = b"xXx"
        arr[1:3] = b"xXx"

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertTrue(allequal(a[1:4], arr[1:4]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test03_range(self):
        "Range element update (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = slice(1, 3, None)
        rng = numpy.arange(a[s].size)*2 + 3
        rng.shape = a[s].shape
        a[s] = rng
        arr[s] = rng

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertTrue(allequal(a[1:4], arr[1:4]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test04_range(self):
        "Range element update, strided (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = slice(1, 4, 2)
        a[s] = b"xXx"
        arr[s] = b"xXx"

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertTrue(allequal(a[1:4:2], arr[1:4:2]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test05_range(self):
        "Range element update, strided (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = slice(1, 4, 2)
        rng = numpy.arange(a[s].size)*2 + 3
        rng.shape = a[s].shape
        a[s] = rng
        arr[s] = rng

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertTrue(allequal(a[1:4:2], arr[1:4:2]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test06_negativeIndex(self):
        "Negative Index element update (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = -1
        a[s] = b"xXx"
        arr[s] = b"xXx"

        # Get and compare an element
        if common.verbose:
            print("Original last element:", a[-1])
            print("Read last element:", arr[-1])
        self.assertTrue(allequal(a[-1], arr[-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test07_negativeIndex(self):
        "Negative Index element update (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = -2
        a[s] = a[s]*2 + 3
        arr[s] = arr[s]*2 + 3

        # Get and compare an element
        if common.verbose:
            print("Original before last element:", a[-2])
            print("Read before last element:", arr[-2])
        if isinstance(a[-2], numpy.ndarray):
            self.assertTrue(allequal(a[-2], arr[-2]))
        else:
            self.assertEqual(a[-2], arr[-2])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test08_negativeRange(self):
        "Negative range element update (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = slice(-4, -1, None)
        a[s] = b"xXx"
        arr[s] = b"xXx"

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertTrue(allequal(a[-4:-1], arr[-4:-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test09_negativeRange(self):
        "Negative range element update (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of a and arr:
        s = slice(-3, -1, None)
        rng = numpy.arange(a[s].size)*2 + 3
        rng.shape = a[s].shape
        a[s] = rng
        arr[s] = rng

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertTrue(allequal(a[-4:-1], arr[-4:-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test10_outOfRange(self):
        "Out of range update (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file, 'a')
            arr = fileh.root.somearray

        # Modify elements of arr that are out of range:
        s = slice(1, a.shape[0]+1, None)
        s2 = slice(1, 1000, None)
        rng = numpy.arange(a[s].size)*2 + 3
        rng.shape = a[s].shape
        a[s] = rng
        rng2 = numpy.arange(a[s2].size)*2 + 3
        rng2.shape = a[s2].shape
        arr[s2] = rng2

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertTrue(allequal(a[-4:-1], arr[-4:-1]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return


class SI1NATestCase(SetItemTestCase):
    title = "Rank-1 case 1"
    numericalList = numpy.array([3])
    numericalListME = numpy.array([3, 2, 1, 0, 4, 5, 6])
    charList = numpy.array(["3"], 'S')
    charListME = numpy.array(
        ["321", "221", "121", "021", "421", "521", "621"], 'S')


class SI1NAOpenTestCase(SI1NATestCase):
    close = 0


class SI1NACloseTestCase(SI1NATestCase):
    close = 1


class SI2NATestCase(SetItemTestCase):
    # A more complex example
    title = "Rank-1,2 case 2"
    numericalList = numpy.array([3, 4])
    numericalListME = numpy.array([[3, 2, 1, 0, 4, 5, 6],
                                   [2, 1, 0, 4, 5, 6, 7],
                                   [4, 3, 2, 1, 0, 4, 5],
                                   [3, 2, 1, 0, 4, 5, 6],
                                   [3, 2, 1, 0, 4, 5, 6]])

    charList = numpy.array(["a", "b"], 'S')
    charListME = numpy.array(
        [["321", "221", "121", "021", "421", "521", "621"],
         ["21", "21", "11", "02", "42", "21", "61"],
         ["31", "21", "12", "21", "41", "51", "621"],
         ["321", "221", "121", "021",
          "421", "521", "621"],
         ["3241", "2321", "13216",
          "0621", "4421", "5421", "a621"],
         ["a321", "s221", "d121", "g021", "b421", "5vvv21", "6zxzxs21"]], 'S')


class SI2NAOpenTestCase(SI2NATestCase):
    close = 0


class SI2NACloseTestCase(SI2NATestCase):
    close = 1


class GeneratorTestCase(unittest.TestCase):

    def test00a_single(self):
        "Testing generator access to Arrays, single elements (char)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test00b_me(self):
        "Testing generator access to Arrays, multiple elements (char)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]

        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        for i in range(len(ga)):
            self.assertTrue(allequal(ga[i], garr[i]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test01a_single(self):
        "Testing generator access to Arrays, single elements (numeric)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return

    def test01b_me(self):
        "Testing generator access to Arrays, multiple elements (numeric)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        if self.close:
            fileh.close()
            fileh = open_file(file)
            arr = fileh.root.somearray

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        for i in range(len(ga)):
            self.assertTrue(allequal(ga[i], garr[i]))

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)
        return


class GE1NATestCase(GeneratorTestCase):
    title = "Rank-1 case 1"
    numericalList = numpy.array([3])
    numericalListME = numpy.array([3, 2, 1, 0, 4, 5, 6])
    charList = numpy.array(["3"], 'S')
    charListME = numpy.array(
        ["321", "221", "121", "021", "421", "521", "621"], 'S')


class GE1NAOpenTestCase(GE1NATestCase):
    close = 0


class GE1NACloseTestCase(GE1NATestCase):
    close = 1


class GE2NATestCase(GeneratorTestCase):
    # A more complex example
    title = "Rank-1,2 case 2"
    numericalList = numpy.array([3, 4])
    numericalListME = numpy.array([[3, 2, 1, 0, 4, 5, 6],
                                   [2, 1, 0, 4, 5, 6, 7],
                                   [4, 3, 2, 1, 0, 4, 5],
                                   [3, 2, 1, 0, 4, 5, 6],
                                   [3, 2, 1, 0, 4, 5, 6]])

    charList = numpy.array(["a", "b"], 'S')
    charListME = numpy.array(
        [["321", "221", "121", "021", "421", "521", "621"],
         ["21", "21", "11", "02", "42", "21", "61"],
         ["31", "21", "12", "21", "41", "51", "621"],
         ["321", "221", "121", "021",
          "421", "521", "621"],
         ["3241", "2321", "13216",
          "0621", "4421", "5421", "a621"],
         ["a321", "s221", "d121", "g021", "b421", "5vvv21", "6zxzxs21"]], 'S')


class GE2NAOpenTestCase(GE2NATestCase):
    close = 0


class GE2NACloseTestCase(GE2NATestCase):
    close = 1


class NonHomogeneousTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test(self):
        """Test for creation of non-homogeneous arrays."""
        # This checks ticket #12.
        h5file = self.h5file
        self.assertRaises(ValueError, h5file.create_array, '/', 'test',
                          [1, [2, 3]])
        self.assertRaises(NoSuchNodeError, h5file.remove_node, '/test')


class TruncateTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test(self):
        """Test for unability to truncate Array objects."""
        array1 = self.h5file.create_array('/', 'array1', [0, 2])
        self.assertRaises(TypeError, array1.truncate, 0)


class PointSelectionTestCase(common.PyTablesTestCase):

    def setUp(self):
        # Limits for selections
        self.limits = [
            (0, 1),  # just one element
            (20, -10),  # no elements
            (-10, 4),  # several elements
            (0, 10),   # several elements (again)
        ]
        # Create an instance of an HDF5 Array
        self.file = tempfile.mktemp(".h5")
        self.fileh = fileh = open_file(self.file, "w")
        # Create a sample array
        size = numpy.prod(self.shape)
        nparr = numpy.arange(size, dtype=numpy.int32).reshape(self.shape)
        self.nparr = nparr
        self.tbarr = fileh.create_array(fileh.root, 'array', nparr)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01a_read(self):
        """Test for point-selections (read, boolean keys)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = (nparr >= value1) & (nparr < value2)
            if common.verbose:
                print("Selection to test:", key)
            a = nparr[key]
            b = tbarr[key]
#             if common.verbose:
#                 print("NumPy selection:", a)
#                 print("PyTables selection:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables selections does not match.")

    def test01b_read(self):
        """Test for point-selections (read, integer keys)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = numpy.where((nparr >= value1) & (nparr < value2))
            if common.verbose:
                print("Selection to test:", key)
            a = nparr[key]
            b = tbarr[key]
#             if common.verbose:
#                 print("NumPy selection:", a)
#                 print("PyTables selection:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables selections does not match.")

    def test01c_read(self):
        """Test for point-selections (read, float keys)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = numpy.where((nparr >= value1) & (nparr < value2))
            if common.verbose:
                print("Selection to test:", key)
            # a = nparr[key]
            fkey = numpy.array(key, "f4")
            self.assertRaises(IndexError, tbarr.__getitem__, fkey)

    def test02a_write(self):
        """Test for point-selections (write, boolean keys)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = (nparr >= value1) & (nparr < value2)
            if common.verbose:
                print("Selection to test:", key)
            s = nparr[key]
            nparr[key] = s * 2
            tbarr[key] = s * 2
            a = nparr[:]
            b = tbarr[:]
#             if common.verbose:
#                 print("NumPy modified array:", a)
#                 print("PyTables modifyied array:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables modifications does not match.")

    def test02b_write(self):
        """Test for point-selections (write, integer keys)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = numpy.where((nparr >= value1) & (nparr < value2))
            if common.verbose:
                print("Selection to test:", key)
            s = nparr[key]
            nparr[key] = s * 2
            tbarr[key] = s * 2
            a = nparr[:]
            b = tbarr[:]
#             if common.verbose:
#                 print("NumPy modified array:", a)
#                 print("PyTables modifyied array:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables modifications does not match.")

    def test02c_write(self):
        """Test for point-selections (write, integer values, broadcast)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for value1, value2 in self.limits:
            key = numpy.where((nparr >= value1) & (nparr < value2))
            if common.verbose:
                print("Selection to test:", key)
            # s = nparr[key]
            nparr[key] = 2   # force a broadcast
            tbarr[key] = 2   # force a broadcast
            a = nparr[:]
            b = tbarr[:]
#             if common.verbose:
#                 print("NumPy modified array:", a)
#                 print("PyTables modifyied array:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables modifications does not match.")


class PointSelection1(PointSelectionTestCase):
    shape = (5, 3, 3)


class PointSelection2(PointSelectionTestCase):
    shape = (7, 3)


class PointSelection3(PointSelectionTestCase):
    shape = (4, 3, 2, 1)


class PointSelection4(PointSelectionTestCase):
    shape = (1, 3, 2, 5, 6)


class FancySelectionTestCase(common.PyTablesTestCase):

    def setUp(self):
        M, N, O = self.shape

        # The next are valid selections for both NumPy and PyTables
        self.working_keyset = [
            ([1, 3], slice(1, N-1), 2),
            ([M-1, 1, 3, 2], slice(None), 2),  # unordered lists supported
            (slice(M), [N-1, 1, 0], slice(None)),
            (slice(1, M, 3), slice(1, N), [O-1, 1, 0]),
            (M-1, [2, 1], 1),
            (1, 2, 1),              # regular selection
            ([1, 2], -2, -1),     # negative indices
            ([1, -2], 2, -1),     # more negative indices
            ([1, -2], 2, Ellipsis),     # one ellipsis
            (Ellipsis, [1, 2]),    # one ellipsis
            (numpy.array(
                [1, -2], 'i4'), 2, -1),  # array 32-bit instead of list
            (numpy.array(
                [-1, 2], 'i8'), 2, -1),  # array 64-bit instead of list
        ]

        # Using booleans instead of ints is deprecated since numpy 1.8
        # Tests for keys that have to support the __index__ attribute
        #if (sys.version_info[0] >= 2 and sys.version_info[1] >= 5):
        #    self.working_keyset.append(
        #        (False, True),  # equivalent to (0,1) ;-)
        #    )

        # Valid selections for NumPy, but not for PyTables (yet)
        # The next should raise an IndexError
        self.not_working_keyset = [
            numpy.array([False, True], dtype="b1"),  # boolean arrays
            ([1, 2, 1], 2, 1),    # repeated values
            ([1, 2], 2, [1, 2]),  # several lists
            ([], 2, 1),         # empty selections
            (Ellipsis, [1, 2], Ellipsis),  # several ellipsis
            # Using booleans instead of ints is deprecated since numpy 1.8
            ([False, True]),    # boolean values with incompatible shape
        ]

        # The next should raise an IndexError in both NumPy and PyTables
        self.not_working_oob = [
            ([1, 2], 2, 1000),         # out-of-bounds selections
            ([1, 2], 2000, 1),         # out-of-bounds selections
        ]

        # The next should raise a IndexError in both NumPy and PyTables
        self.not_working_too_many = [
            ([1, 2], 2, 1, 1),
        ]

        # Create an instance of an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = fileh = open_file(self.file, "w")
        # Create a sample array
        nparr = numpy.empty(self.shape, dtype=numpy.int32)
        data = numpy.arange(N * O, dtype=numpy.int32).reshape(N, O)
        for i in xrange(M):
            nparr[i] = data * i
        self.nparr = nparr
        self.tbarr = fileh.create_array(fileh.root, 'array', nparr)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01a_read(self):
        """Test for fancy-selections (working selections, read)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for key in self.working_keyset:
            if common.verbose:
                print("Selection to test:", key)
            a = nparr[key]
            b = tbarr[key]
#             if common.verbose:
#                 print("NumPy selection:", a)
#                 print("PyTables selection:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables selections does not match.")

    def test01b_read(self):
        """Test for fancy-selections (not working selections, read)."""
        # nparr = self.nparr
        tbarr = self.tbarr
        for key in self.not_working_keyset:
            if common.verbose:
                print("Selection to test:", key)
            # a = nparr[key]
            self.assertRaises(IndexError, tbarr.__getitem__, key)

    def test01c_read(self):
        """Test for fancy-selections (out-of-bound indexes, read)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for key in self.not_working_oob:
            if common.verbose:
                print("Selection to test:", key)
            self.assertRaises(IndexError, nparr.__getitem__, key)
            self.assertRaises(IndexError, tbarr.__getitem__, key)

    def test01d_read(self):
        """Test for fancy-selections (too many indexes, read)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for key in self.not_working_too_many:
            if common.verbose:
                print("Selection to test:", key)
            # ValueError for numpy 1.6.x and earlier
            # IndexError in numpy > 1.8.0
            self.assertRaises((ValueError, IndexError), nparr.__getitem__, key)
            self.assertRaises(IndexError, tbarr.__getitem__, key)

    def test02a_write(self):
        """Test for fancy-selections (working selections, write)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for key in self.working_keyset:
            if common.verbose:
                print("Selection to test:", key)
            s = nparr[key]
            nparr[key] = s * 2
            tbarr[key] = s * 2
            a = nparr[:]
            b = tbarr[:]
#             if common.verbose:
#                 print("NumPy modified array:", a)
#                 print("PyTables modifyied array:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables modifications does not match.")

    def test02b_write(self):
        """Test for fancy-selections (working selections, write, broadcast)."""
        nparr = self.nparr
        tbarr = self.tbarr
        for key in self.working_keyset:
            if common.verbose:
                print("Selection to test:", key)
            # s = nparr[key]
            nparr[key] = 2   # broadcast value
            tbarr[key] = 2   # broadcast value
            a = nparr[:]
            b = tbarr[:]
#             if common.verbose:
#                 print("NumPy modified array:", a)
#                 print("PyTables modifyied array:", b)
            self.assertTrue(
                numpy.alltrue(a == b),
                "NumPy array and PyTables modifications does not match.")


class FancySelection1(FancySelectionTestCase):
    shape = (5, 3, 3)  # Minimum values


class FancySelection2(FancySelectionTestCase):
    # shape = (5, 3, 3)  # Minimum values
    shape = (7, 3, 3)


class FancySelection3(FancySelectionTestCase):
    # shape = (5, 3, 3)  # Minimum values
    shape = (7, 4, 5)


class FancySelection4(FancySelectionTestCase):
    # shape = (5, 3, 3)  # Minimum values
    shape = (5, 3, 10)


class CopyNativeHDF5MDAtom(common.PyTablesTestCase):

    def setUp(self):
        filename = self._testFilename("array_mdatom.h5")
        self.fileh = open_file(filename, "r")
        self.arr = self.fileh.root.arr
        self.copy = tempfile.mktemp(".h5")
        self.copyh = open_file(self.copy, mode="w")
        self.arr2 = self.arr.copy(self.copyh.root, newname="arr2")

    def tearDown(self):
        self.fileh.close()
        self.copyh.close()
        os.remove(self.copy)

    def test01_copy(self):
        """Checking that native MD atoms are copied as-is"""
        self.assertEqual(self.arr.atom, self.arr2.atom)
        self.assertEqual(self.arr.shape, self.arr2.shape)

    def test02_reopen(self):
        """Checking that native MD atoms are copied as-is (re-open)"""
        self.copyh.close()
        self.copyh = open_file(self.copy, mode="r")
        self.arr2 = self.copyh.root.arr2
        self.assertEqual(self.arr.atom, self.arr2.atom)
        self.assertEqual(self.arr.shape, self.arr2.shape)


class AccessClosedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(AccessClosedTestCase, self).setUp()

        a = numpy.zeros((10, 10))
        self.array = self.h5file.create_array(self.h5file.root, 'array', a)

    def test_read(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.read)

    def test_getitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__getitem__, 0)

    def test_setitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__setitem__, 0, 0)


class BroadcastTest(common.TempFileMixin, common.PyTablesTestCase):

    def test(self):
        """Test correct broadcasting when the array atom is not scalar."""

        array_shape = (2, 3)
        element_shape = (3,)

        dtype = numpy.dtype((numpy.int, element_shape))
        atom = Atom.from_dtype(dtype)
        h5arr = self.h5file.create_carray(self.h5file.root, 'array',
                                          atom, array_shape)

        size = numpy.prod(element_shape)
        nparr = numpy.arange(size).reshape(element_shape)

        h5arr[0] = nparr
        self.assertTrue(numpy.all(h5arr[0] == nparr))


class TestCreateArrayArgs(common.TempFileMixin, common.PyTablesTestCase):
    where = '/'
    name = 'array'
    obj = numpy.array([[1, 2], [3, 4]])
    title = 'title'
    byteorder = None
    createparents = False
    atom = Atom.from_dtype(obj.dtype)
    shape = obj.shape

    def test_positional_args(self):
        self.h5file.create_array(self.where, self.name, self.obj, self.title)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_positional_args_atom_shape(self):
        self.h5file.create_array(self.where, self.name, None, self.title,
                                 self.byteorder, self.createparents,
                                 self.atom, self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(numpy.zeros_like(self.obj), nparr))

    def test_kwargs_obj(self):
        self.h5file.create_array(self.where, self.name, title=self.title,
                                 obj=self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_01(self):
        ptarr = self.h5file.create_array(self.where, self.name,
                                         title=self.title,
                                         atom=self.atom, shape=self.shape)
        ptarr[...] = self.obj
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_02(self):
        ptarr = self.h5file.create_array(self.where, self.name,
                                         title=self.title,
                                         atom=self.atom, shape=self.shape)
        #ptarr[...] = self.obj
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(numpy.zeros_like(self.obj), nparr))

    def test_kwargs_obj_atom(self):
        ptarr = self.h5file.create_array(self.where, self.name,
                                         title=self.title,
                                         obj=self.obj,
                                         atom=self.atom)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_shape(self):
        ptarr = self.h5file.create_array(self.where, self.name,
                                         title=self.title,
                                         obj=self.obj,
                                         shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_shape(self):
        ptarr = self.h5file.create_array(self.where, self.name,
                                         title=self.title,
                                         obj=self.obj,
                                         atom=self.atom,
                                         shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_error(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_array,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom)

    def test_kwargs_obj_shape_error(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_array,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_01(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_array,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=self.shape)

    def test_kwargs_obj_atom_shape_error_02(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_array,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=self.atom,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_03(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_array,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=shape)


#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    for i in range(niter):
        # The scalar case test should be refined in order to work
        theSuite.addTest(unittest.makeSuite(Basic0DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic0DTwoTestCase))
        # theSuite.addTest(unittest.makeSuite(Basic1DZeroTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DTwoTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DThreeTestCase))
        theSuite.addTest(unittest.makeSuite(Basic2DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic2DTwoTestCase))
        theSuite.addTest(unittest.makeSuite(Basic10DTestCase))
        # The 32 dimensions case is tested on GroupsArray
        # theSuite.addTest(unittest.makeSuite(Basic32DTestCase))
        theSuite.addTest(unittest.makeSuite(ReadOutArgumentTests))
        theSuite.addTest(unittest.makeSuite(
            SizeOnDiskInMemoryPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(GroupsArrayTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexNotReopenNotEndianTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexReopenNotEndianTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexNotReopenEndianTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexReopenEndianTestCase))
        theSuite.addTest(unittest.makeSuite(CloseCopyTestCase))
        theSuite.addTest(unittest.makeSuite(OpenCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex1TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex2TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex3TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex4TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex5TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex6TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex7TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex8TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex9TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex10TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex11TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex12TestCase))
        theSuite.addTest(unittest.makeSuite(GI1NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(GI1NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(GI2NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(GI2NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(SI1NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(SI1NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(SI2NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(SI2NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(GE1NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(GE1NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(GE2NAOpenTestCase))
        theSuite.addTest(unittest.makeSuite(GE2NACloseTestCase))
        theSuite.addTest(unittest.makeSuite(NonHomogeneousTestCase))
        theSuite.addTest(unittest.makeSuite(TruncateTestCase))
        theSuite.addTest(unittest.makeSuite(FancySelection1))
        theSuite.addTest(unittest.makeSuite(FancySelection2))
        theSuite.addTest(unittest.makeSuite(FancySelection3))
        theSuite.addTest(unittest.makeSuite(FancySelection4))
        theSuite.addTest(unittest.makeSuite(PointSelection1))
        theSuite.addTest(unittest.makeSuite(PointSelection2))
        theSuite.addTest(unittest.makeSuite(PointSelection3))
        theSuite.addTest(unittest.makeSuite(PointSelection4))
        theSuite.addTest(unittest.makeSuite(CopyNativeHDF5MDAtom))
        theSuite.addTest(unittest.makeSuite(AccessClosedTestCase))
        theSuite.addTest(unittest.makeSuite(TestCreateArrayArgs))
        theSuite.addTest(unittest.makeSuite(BroadcastTest))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_attributes
# -*- coding: utf-8 -*-

"""This test unit checks node attributes that are persistent (AttributeSet)."""

from __future__ import print_function
import os
import sys
import unittest
import tempfile

import numpy
from numpy.testing import assert_array_equal, assert_almost_equal

from tables.parameters import NODE_CACHE_SLOTS
from tables import *
from tables.tests import common
from tables.tests.common import PyTablesTestCase
from tables.exceptions import DataTypeWarning

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class Record(IsDescription):
    var1 = StringCol(itemsize=4)  # 4-character String
    var2 = IntCol()               # integer
    var3 = Int16Col()             # short integer
    var4 = FloatCol()             # double (double-precision)
    var5 = Float32Col()           # float  (single-precision)


class CreateTestCase(unittest.TestCase):

    def setUp(self):
        # Create an instance of HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(
            self.file, mode="w", node_cache_slots=self.node_cache_slots)
        self.root = self.fileh.root

        # Create a table object
        self.table = self.fileh.create_table(self.root, 'atable',
                                             Record, "Table title")
        # Create an array object
        self.array = self.fileh.create_array(self.root, 'anarray',
                                             [1], "Array title")
        # Create a group object
        self.group = self.fileh.create_group(self.root, 'agroup',
                                             "Group title")

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

#---------------------------------------

    def test01_setAttributes(self):
        """Checking setting large string attributes (File methods)"""

        attrlength = 2048
        # Try to put a long string attribute on a group object
        self.fileh.set_node_attr(self.root.agroup, "attr1", "p" * attrlength)

        # Now, try with a Table object
        self.fileh.set_node_attr(self.root.atable, "attr1", "a" * attrlength)

        # Finally, try with an Array object
        self.fileh.set_node_attr(self.root.anarray, "attr1", "n" * attrlength)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        self.assertEqual(self.fileh.get_node_attr(self.root.agroup, 'attr1'),
                         "p" * attrlength)
        self.assertEqual(self.fileh.get_node_attr(self.root.atable, 'attr1'),
                         "a" * attrlength)
        self.assertEqual(self.fileh.get_node_attr(self.root.anarray, 'attr1'),
                         "n" * attrlength)

    def test02_setAttributes(self):
        """Checking setting large string attributes (Node methods)"""

        attrlength = 2048
        # Try to put a long string attribute on a group object
        self.root.agroup._f_setattr('attr1', "p" * attrlength)
        # Now, try with a Table object
        self.root.atable.set_attr('attr1', "a" * attrlength)

        # Finally, try with an Array object
        self.root.anarray.set_attr('attr1', "n" * attrlength)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        self.assertEqual(self.root.agroup._f_getattr(
            'attr1'), "p" * attrlength)
        self.assertEqual(self.root.atable.get_attr("attr1"), "a" * attrlength)
        self.assertEqual(self.root.anarray.get_attr("attr1"), "n" * attrlength)

    def test03_setAttributes(self):
        """Checking setting large string attributes (AttributeSet methods)"""

        attrlength = 2048
        # Try to put a long string attribute on a group object
        self.group._v_attrs.attr1 = "p" * attrlength
        # Now, try with a Table object
        self.table.attrs.attr1 = "a" * attrlength
        # Finally, try with an Array object
        self.array.attrs.attr1 = "n" * attrlength

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        # This should work even when the node cache is disabled
        self.assertEqual(self.root.agroup._v_attrs.attr1, "p" * attrlength)
        self.assertEqual(self.root.atable.attrs.attr1, "a" * attrlength)
        self.assertEqual(self.root.anarray.attrs.attr1, "n" * attrlength)

    def test04_listAttributes(self):
        """Checking listing attributes."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        if common.verbose:
            print("Attribute list:", self.group._v_attrs._f_list())

        # Now, try with a Table object
        self.table.attrs.a = "1"
        self.table.attrs.c = "2"
        self.table.attrs.b = "3"
        if common.verbose:
            print("Attribute list:", self.table.attrs._f_list())

        # Finally, try with an Array object
        self.array.attrs.k = "1"
        self.array.attrs.j = "2"
        self.array.attrs.i = "3"
        if common.verbose:
            print("Attribute list:", self.array.attrs._f_list())

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup = self.root.agroup
        self.assertEqual(agroup._v_attrs._f_list("user"), ["pq", "qr", "rs"])
        self.assertEqual(agroup._v_attrs._f_list("sys"),
                         ['CLASS', 'TITLE', 'VERSION'])
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "pq", "qr", "rs"])

        atable = self.root.atable
        self.assertEqual(atable.attrs._f_list(), ["a", "b", "c"])
        self.assertEqual(atable.attrs._f_list("sys"),
                         ['CLASS',
                          'FIELD_0_FILL', 'FIELD_0_NAME',
                          'FIELD_1_FILL', 'FIELD_1_NAME',
                          'FIELD_2_FILL', 'FIELD_2_NAME',
                          'FIELD_3_FILL', 'FIELD_3_NAME',
                          'FIELD_4_FILL', 'FIELD_4_NAME',
                          'NROWS',
                          'TITLE', 'VERSION'])
        self.assertEqual(atable.attrs._f_list("all"),
                         ['CLASS',
                          'FIELD_0_FILL', 'FIELD_0_NAME',
                          'FIELD_1_FILL', 'FIELD_1_NAME',
                          'FIELD_2_FILL', 'FIELD_2_NAME',
                          'FIELD_3_FILL', 'FIELD_3_NAME',
                          'FIELD_4_FILL', 'FIELD_4_NAME',
                          'NROWS',
                          'TITLE', 'VERSION',
                          "a", "b", "c"])

        anarray = self.root.anarray
        self.assertEqual(anarray.attrs._f_list(), ["i", "j", "k"])
        self.assertEqual(
            anarray.attrs._f_list("sys"),
            ['CLASS', 'FLAVOR', 'TITLE', 'VERSION'])
        self.assertEqual(
            anarray.attrs._f_list("all"),
            ['CLASS', 'FLAVOR', 'TITLE', 'VERSION', "i", "j", "k"])

    def test05_removeAttributes(self):
        """Checking removing attributes."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # delete an attribute
        del self.group._v_attrs.pq

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup = self.root.agroup
        if common.verbose:
            print("Attribute list:", agroup._v_attrs._f_list())
        # Check the local attributes names
        self.assertEqual(agroup._v_attrs._f_list(), ["qr", "rs"])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list("all"))
        # Check the disk attribute names
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "qr", "rs"])

        # delete an attribute (__delattr__ method)
        del agroup._v_attrs.qr
        if common.verbose:
            print("Attribute list:", agroup._v_attrs._f_list())
        # Check the local attributes names
        self.assertEqual(agroup._v_attrs._f_list(), ["rs"])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list())
        # Check the disk attribute names
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "rs"])

    def test05b_removeAttributes(self):
        """Checking removing attributes (using File.del_node_attr())"""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # delete an attribute
        self.fileh.del_node_attr(self.group, "pq")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup = self.root.agroup
        if common.verbose:
            print("Attribute list:", agroup._v_attrs._f_list())
        # Check the local attributes names
        self.assertEqual(agroup._v_attrs._f_list(), ["qr", "rs"])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list("all"))
        # Check the disk attribute names
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "qr", "rs"])

        # delete an attribute (File.del_node_attr method)
        self.fileh.del_node_attr(self.root, "qr", "agroup")
        if common.verbose:
            print("Attribute list:", agroup._v_attrs._f_list())
        # Check the local attributes names
        self.assertEqual(agroup._v_attrs._f_list(), ["rs"])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list())
        # Check the disk attribute names
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "rs"])

    def test06_removeAttributes(self):
        """Checking removing system attributes."""

        # remove a system attribute
        if common.verbose:
            print("Before removing CLASS attribute")
            print("System attrs:", self.group._v_attrs._v_attrnamessys)
        del self.group._v_attrs.CLASS
        self.assertEqual(self.group._v_attrs._f_list("sys"),
                         ['TITLE', 'VERSION'])
        if common.verbose:
            print("After removing CLASS attribute")
            print("System attrs:", self.group._v_attrs._v_attrnamessys)

    def test07_renameAttributes(self):
        """Checking renaming attributes."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # rename an attribute
        self.group._v_attrs._f_rename("pq", "op")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup = self.root.agroup
        if common.verbose:
            print("Attribute list:", agroup._v_attrs._f_list())
        # Check the local attributes names (alphabetically sorted)
        self.assertEqual(agroup._v_attrs._f_list(), ["op", "qr", "rs"])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list("all"))
        # Check the disk attribute names (not sorted)
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "op", "qr", "rs"])

    def test08_renameAttributes(self):
        """Checking renaming system attributes."""

        if common.verbose:
            print("Before renaming CLASS attribute")
            print("All attrs:", self.group._v_attrs._v_attrnames)
        # rename a system attribute
        self.group._v_attrs._f_rename("CLASS", "op")
        if common.verbose:
            print("After renaming CLASS attribute")
            print("All attrs:", self.group._v_attrs._v_attrnames)

        # Check the disk attribute names (not sorted)
        agroup = self.root.agroup
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['TITLE', 'VERSION', "op"])

    def test09_overwriteAttributes(self):
        """Checking overwriting attributes."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # overwrite attributes
        self.group._v_attrs.pq = "4"
        self.group._v_attrs.qr = 2
        self.group._v_attrs.rs = [1, 2, 3]

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup = self.root.agroup
        if common.verbose:
            print("Value of Attribute pq:", agroup._v_attrs.pq)
        # Check the local attributes names (alphabetically sorted)
        self.assertEqual(agroup._v_attrs.pq, "4")
        self.assertEqual(agroup._v_attrs.qr, 2)
        self.assertEqual(agroup._v_attrs.rs, [1, 2, 3])
        if common.verbose:
            print("Attribute list in disk:", agroup._v_attrs._f_list("all"))
        # Check the disk attribute names (not sorted)
        self.assertEqual(agroup._v_attrs._f_list("all"),
                         ['CLASS', 'TITLE', 'VERSION', "pq", "qr", "rs"])

    def test10a_copyAttributes(self):
        """Checking copying attributes."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # copy all attributes from "/agroup" to "/atable"
        self.group._v_attrs._f_copy(self.root.atable)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        atable = self.root.atable
        if common.verbose:
            print("Attribute list:", atable._v_attrs._f_list())
        # Check the local attributes names (alphabetically sorted)
        self.assertEqual(atable._v_attrs._f_list(), ["pq", "qr", "rs"])
        if common.verbose:
            print("Complete attribute list:", atable._v_attrs._f_list("all"))
        # Check the disk attribute names (not sorted)
        self.assertEqual(atable._v_attrs._f_list("all"),
                         ['CLASS',
                          'FIELD_0_FILL', 'FIELD_0_NAME',
                          'FIELD_1_FILL', 'FIELD_1_NAME',
                          'FIELD_2_FILL', 'FIELD_2_NAME',
                          'FIELD_3_FILL', 'FIELD_3_NAME',
                          'FIELD_4_FILL', 'FIELD_4_NAME',
                          'NROWS',
                          'TITLE', 'VERSION',
                          "pq", "qr", "rs"])

    def test10b_copyAttributes(self):
        """Checking copying attributes (copy_node_attrs)"""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        # copy all attributes from "/agroup" to "/atable"
        self.fileh.copy_node_attrs(self.group, self.root.atable)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        atable = self.root.atable
        if common.verbose:
            print("Attribute list:", atable._v_attrs._f_list())
        # Check the local attributes names (alphabetically sorted)
        self.assertEqual(atable._v_attrs._f_list(), ["pq", "qr", "rs"])
        if common.verbose:
            print("Complete attribute list:", atable._v_attrs._f_list("all"))
        # Check the disk attribute names (not sorted)
        self.assertEqual(atable._v_attrs._f_list("all"),
                         ['CLASS',
                          'FIELD_0_FILL', 'FIELD_0_NAME',
                          'FIELD_1_FILL', 'FIELD_1_NAME',
                          'FIELD_2_FILL', 'FIELD_2_NAME',
                          'FIELD_3_FILL', 'FIELD_3_NAME',
                          'FIELD_4_FILL', 'FIELD_4_NAME',
                          'NROWS',
                          'TITLE', 'VERSION',
                          "pq", "qr", "rs"])

    def test10c_copyAttributes(self):
        """Checking copying attributes during group copies."""

        # With a Group object
        self.group._v_attrs['CLASS'] = "GROUP2"
        self.group._v_attrs['VERSION'] = "1.3"
        # copy "/agroup" to "/agroup2"
        self.fileh.copy_node(self.group, self.root, "agroup2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        agroup2 = self.root.agroup2
        if common.verbose:
            print("Complete attribute list:", agroup2._v_attrs._f_list("all"))
        self.assertEqual(agroup2._v_attrs['CLASS'], "GROUP2")
        self.assertEqual(agroup2._v_attrs['VERSION'], "1.3")

    def test10d_copyAttributes(self):
        """Checking copying attributes during leaf copies."""

        # With a Group object
        atable = self.root.atable
        atable._v_attrs['CLASS'] = "TABLE2"
        atable._v_attrs['VERSION'] = "1.3"
        # copy "/agroup" to "/agroup2"
        self.fileh.copy_node(atable, self.root, "atable2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+", node_cache_slots=self.node_cache_slots)
            self.root = self.fileh.root

        atable2 = self.root.atable2
        if common.verbose:
            print("Complete attribute list:", atable2._v_attrs._f_list("all"))
        self.assertEqual(atable2._v_attrs['CLASS'], "TABLE2")
        self.assertEqual(atable2._v_attrs['VERSION'], "1.3")

    def test11a_getitem(self):
        """Checking the __getitem__ interface."""

        attrs = self.group._v_attrs
        attrs.pq = "1"
        self.assertEqual(attrs['pq'], "1")

    def test11b_setitem(self):
        """Checking the __setitem__ interface."""

        attrs = self.group._v_attrs
        attrs['pq'] = "2"
        self.assertEqual(attrs['pq'], "2")

    def test11c_delitem(self):
        """Checking the __delitem__ interface."""

        attrs = self.group._v_attrs
        attrs.pq = "1"
        del attrs['pq']
        self.assertTrue('pq' not in attrs._f_list())

    def test11d_KeyError(self):
        """Checking that KeyError is raised in __getitem__/__delitem__."""

        attrs = self.group._v_attrs
        self.assertRaises(KeyError, attrs.__getitem__, 'pq')
        self.assertRaises(KeyError, attrs.__delitem__, 'pq')

    def test_2d_non_contiguous(self):
        """Checking setting 2D and non-contiguous NumPy attributes"""

        # Regression for gh-176 numpy.
        # In the views old implementation PyTAbles performa a copy of the
        # array:
        #
        #     value = numpy.array(value)
        #
        # in order to get a contiguous array.
        # Unfortunately array with swapped axis are copyed as they are so
        # thay are stored in to HDF5 attributes without being actually
        # contiguous and ths causes an error whn they are restored.

        data = numpy.array([[0, 1], [2, 3]])

        self.array.attrs['a'] = data
        self.array.attrs['b'] = data.T.copy()
        self.array.attrs['c'] = data.T

        assert_array_equal(self.array.attrs['a'], data)
        assert_array_equal(self.array.attrs['b'], data.T)
        assert_array_equal(self.array.attrs['c'], data.T)  # AssertionError!


class NotCloseCreate(CreateTestCase):
    close = False
    node_cache_slots = NODE_CACHE_SLOTS


class CloseCreate(CreateTestCase):
    close = True
    node_cache_slots = NODE_CACHE_SLOTS


class NoCacheNotCloseCreate(CreateTestCase):
    close = False
    node_cache_slots = 0


class NoCacheCloseCreate(CreateTestCase):
    close = True
    node_cache_slots = 0


class DictCacheNotCloseCreate(CreateTestCase):
    close = False
    node_cache_slots = -NODE_CACHE_SLOTS


class DictCacheCloseCreate(CreateTestCase):
    close = True
    node_cache_slots = -NODE_CACHE_SLOTS


class TypesTestCase(unittest.TestCase):

    def setUp(self):
        # Create an instance of HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.root = self.fileh.root

        # Create an array object
        self.array = self.fileh.create_array(self.root, 'anarray',
                                             [1], "Array title")
        # Create a group object
        self.group = self.fileh.create_group(self.root, 'agroup',
                                             "Group title")

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

#---------------------------------------

    def test00a_setBoolAttributes(self):
        """Checking setting Bool attributes (scalar, Python case)"""

        self.array.attrs.pq = True
        self.array.attrs.qr = False
        self.array.attrs.rs = True

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertEqual(self.root.anarray.attrs.pq, True)
        self.assertEqual(self.root.anarray.attrs.qr, False)
        self.assertEqual(self.root.anarray.attrs.rs, True)

    def test00b_setBoolAttributes(self):
        """Checking setting Bool attributes (scalar, NumPy case)"""

        self.array.attrs.pq = numpy.bool_(True)
        self.array.attrs.qr = numpy.bool_(False)
        self.array.attrs.rs = numpy.bool_(True)

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.root.anarray.attrs.pq, numpy.bool_))
        self.assertTrue(isinstance(self.root.anarray.attrs.qr, numpy.bool_))
        self.assertTrue(isinstance(self.root.anarray.attrs.rs, numpy.bool_))
        self.assertEqual(self.root.anarray.attrs.pq, True)
        self.assertEqual(self.root.anarray.attrs.qr, False)
        self.assertEqual(self.root.anarray.attrs.rs, True)

    def test00c_setBoolAttributes(self):
        """Checking setting Bool attributes (NumPy, 0-dim case)"""

        self.array.attrs.pq = numpy.array(True)
        self.array.attrs.qr = numpy.array(False)
        self.array.attrs.rs = numpy.array(True)

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertEqual(self.root.anarray.attrs.pq, True)
        self.assertEqual(self.root.anarray.attrs.qr, False)
        self.assertEqual(self.root.anarray.attrs.rs, True)

    def test00d_setBoolAttributes(self):
        """Checking setting Bool attributes (NumPy, multidim case)"""

        self.array.attrs.pq = numpy.array([True])
        self.array.attrs.qr = numpy.array([[False]])
        self.array.attrs.rs = numpy.array([[True, False], [True, False]])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.root.anarray.attrs.pq, numpy.array([True]))
        assert_array_equal(self.root.anarray.attrs.qr, numpy.array([[False]]))
        assert_array_equal(self.root.anarray.attrs.rs,
                           numpy.array([[True, False], [True, False]]))

    def test01a_setIntAttributes(self):
        """Checking setting Int attributes (scalar, Python case)"""

        self.array.attrs.pq = 1
        self.array.attrs.qr = 2
        self.array.attrs.rs = 3

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.root.anarray.attrs.pq, numpy.int_))
        self.assertTrue(isinstance(self.root.anarray.attrs.qr, numpy.int_))
        self.assertTrue(isinstance(self.root.anarray.attrs.rs, numpy.int_))
        self.assertEqual(self.root.anarray.attrs.pq, 1)
        self.assertEqual(self.root.anarray.attrs.qr, 2)
        self.assertEqual(self.root.anarray.attrs.rs, 3)

    def test01b_setIntAttributes(self):
        """Checking setting Int attributes (scalar, NumPy case)"""

        # 'UInt64' not supported on Win
        checktypes = ['Int8', 'Int16', 'Int32', 'Int64',
                      'UInt8', 'UInt16', 'UInt32']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype, numpy.array(1, dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            assert_array_equal(getattr(self.array.attrs, dtype),
                               numpy.array(1, dtype=dtype))

    def test01c_setIntAttributes(self):
        """Checking setting Int attributes (unidimensional NumPy case)"""

        # 'UInt64' not supported on Win
        checktypes = ['Int8', 'Int16', 'Int32', 'Int64',
                      'UInt8', 'UInt16', 'UInt32']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype, numpy.array([1, 2], dtype=dtype))

        # Check the results
        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            if common.verbose:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))
            assert_array_equal(getattr(self.array.attrs, dtype),
                               numpy.array([1, 2], dtype=dtype))

    def test01d_setIntAttributes(self):
        """Checking setting Int attributes (unidimensional, non-contiguous)"""

        # 'UInt64' not supported on Win
        checktypes = ['Int8', 'Int16', 'Int32', 'Int64',
                      'UInt8', 'UInt16', 'UInt32']

        for dtype in checktypes:
            arr = numpy.array([1, 2, 3, 4], dtype=dtype)[::2]
            setattr(self.array.attrs, dtype, arr)

        # Check the results
        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            arr = numpy.array([1, 2, 3, 4], dtype=dtype)[::2]
            if common.verbose:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))
            assert_array_equal(getattr(self.array.attrs, dtype), arr)

    def test01e_setIntAttributes(self):
        """Checking setting Int attributes (bidimensional NumPy case)"""

        # 'UInt64' not supported on Win
        checktypes = ['Int8', 'Int16', 'Int32', 'Int64',
                      'UInt8', 'UInt16', 'UInt32']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array([[1, 2], [2, 3]], dtype=dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        # Check the results
        for dtype in checktypes:
            if common.verbose:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))
            assert_array_equal(getattr(self.array.attrs, dtype),
                               numpy.array([[1, 2], [2, 3]], dtype=dtype))

    def test02a_setFloatAttributes(self):
        """Checking setting Float (double) attributes."""

        # Set some attrs
        self.array.attrs.pq = 1.0
        self.array.attrs.qr = 2.0
        self.array.attrs.rs = 3.0

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.root.anarray.attrs.pq, numpy.float_))
        self.assertTrue(isinstance(self.root.anarray.attrs.qr, numpy.float_))
        self.assertTrue(isinstance(self.root.anarray.attrs.rs, numpy.float_))
        self.assertTrue(self.root.anarray.attrs.pq, 1.0)
        self.assertTrue(self.root.anarray.attrs.qr, 2.0)
        self.assertTrue(self.root.anarray.attrs.rs, 3.0)

    def test02b_setFloatAttributes(self):
        """Checking setting Float attributes (scalar, NumPy case)"""

        checktypes = ['Float32', 'Float64']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array(1.1, dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            # assert getattr(self.array.attrs, dtype) == 1.1
            # In order to make Float32 tests pass. This is legal, not a trick.
            assert_almost_equal(getattr(self.array.attrs, dtype), 1.1)

    def test02c_setFloatAttributes(self):
        """Checking setting Float attributes (unidimensional NumPy case)"""

        checktypes = ['Float32', 'Float64']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array([1.1, 2.1], dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            assert_array_equal(getattr(self.array.attrs, dtype),
                               numpy.array([1.1, 2.1], dtype=dtype))

    def test02d_setFloatAttributes(self):
        """Checking setting Float attributes (unidimensional,
        non-contiguous)"""

        checktypes = ['Float32', 'Float64']

        for dtype in checktypes:
            arr = numpy.array([1.1, 2.1, 3.1, 4.1], dtype=dtype)[1::2]
            setattr(self.array.attrs, dtype, arr)

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            arr = numpy.array([1.1, 2.1, 3.1, 4.1], dtype=dtype)[1::2]
            assert_array_equal(getattr(self.array.attrs, dtype), arr)

    def test02e_setFloatAttributes(self):
        """Checking setting Int attributes (bidimensional NumPy case)"""

        checktypes = ['Float32', 'Float64']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array([[1.1, 2.1], [2.1, 3.1]], dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            assert_array_equal(
                getattr(self.array.attrs, dtype),
                numpy.array([[1.1, 2.1], [2.1, 3.1]], dtype=dtype))

    def test03_setObjectAttributes(self):
        """Checking setting Object attributes."""

        # Set some attrs
        self.array.attrs.pq = [1.0, 2]
        self.array.attrs.qr = (1, 2)
        self.array.attrs.rs = {"ddf": 32.1, "dsd": 1}

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertEqual(self.root.anarray.attrs.pq, [1.0, 2])
        self.assertEqual(self.root.anarray.attrs.qr, (1, 2))
        self.assertEqual(self.root.anarray.attrs.rs, {"ddf": 32.1, "dsd": 1})

    def test04a_setStringAttributes(self):
        """Checking setting string attributes (scalar case)"""

        self.array.attrs.pq = 'foo'
        self.array.attrs.qr = 'bar'
        self.array.attrs.rs = 'baz'

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.root.anarray.attrs.pq, numpy.str_))
        self.assertTrue(isinstance(self.root.anarray.attrs.qr, numpy.str_))
        self.assertTrue(isinstance(self.root.anarray.attrs.rs, numpy.str_))
        self.assertEqual(self.root.anarray.attrs.pq, 'foo')
        self.assertEqual(self.root.anarray.attrs.qr, 'bar')
        self.assertEqual(self.root.anarray.attrs.rs, 'baz')

    def test04b_setStringAttributes(self):
        """Checking setting string attributes (unidimensional 1-elem case)"""

        self.array.attrs.pq = numpy.array(['foo'])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.root.anarray.attrs.pq,
                           numpy.array(['foo']))

    def test04c_setStringAttributes(self):
        """Checking setting string attributes (empty unidimensional
        1-elem case)"""

        self.array.attrs.pq = numpy.array([''])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray
            if common.verbose:
                print("pq -->", self.array.attrs.pq)

        assert_array_equal(self.root.anarray.attrs.pq,
                           numpy.array(['']))

    def test04d_setStringAttributes(self):
        """Checking setting string attributes (unidimensional 2-elem case)"""

        self.array.attrs.pq = numpy.array(['foo', 'bar3'])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.root.anarray.attrs.pq,
                           numpy.array(['foo', 'bar3']))

    def test04e_setStringAttributes(self):
        """Checking setting string attributes (empty unidimensional
        2-elem case)"""

        self.array.attrs.pq = numpy.array(['', ''])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.root.anarray.attrs.pq,
                           numpy.array(['', '']))

    def test04f_setStringAttributes(self):
        """Checking setting string attributes (bidimensional 4-elem case)"""

        self.array.attrs.pq = numpy.array([['foo', 'foo2'],
                                           ['foo3', 'foo4']])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.root.anarray.attrs.pq,
                           numpy.array([['foo', 'foo2'],
                                        ['foo3', 'foo4']]))

    def test05a_setComplexAttributes(self):
        """Checking setting Complex (python) attributes."""

        # Set some attrs
        self.array.attrs.pq = 1.0 + 2j
        self.array.attrs.qr = 2.0 + 3j
        self.array.attrs.rs = 3.0 + 4j

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.root.anarray.attrs.pq, numpy.complex_))
        self.assertTrue(isinstance(self.root.anarray.attrs.qr, numpy.complex_))
        self.assertTrue(isinstance(self.root.anarray.attrs.rs, numpy.complex_))
        self.assertEqual(self.root.anarray.attrs.pq, 1.0 + 2j)
        self.assertEqual(self.root.anarray.attrs.qr, 2.0 + 3j)
        self.assertEqual(self.root.anarray.attrs.rs, 3.0 + 4j)

    def test05b_setComplexAttributes(self):
        """Checking setting Complex attributes (scalar, NumPy case)"""

        checktypes = ['complex64', 'complex128']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array(1.1 + 2j, dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            # assert getattr(self.array.attrs, dtype) == 1.1 + 2j
            # In order to make Complex32 tests pass.
            assert_almost_equal(getattr(self.array.attrs, dtype), 1.1 + 2j)

    def test05c_setComplexAttributes(self):
        """Checking setting Complex attributes (unidimensional NumPy case)"""

        checktypes = ['Complex32', 'Complex64']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array([1.1, 2.1], dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->", dtype,
                      getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            assert_array_equal(getattr(self.array.attrs, dtype),
                               numpy.array([1.1, 2.1], dtype=dtype))

    def test05d_setComplexAttributes(self):
        """Checking setting Int attributes (bidimensional NumPy case)"""

        checktypes = ['Complex32', 'Complex64']

        for dtype in checktypes:
            setattr(self.array.attrs, dtype,
                    numpy.array([[1.1, 2.1], [2.1, 3.1]], dtype=dtype))

        # Check the results
        if common.verbose:
            for dtype in checktypes:
                print("type, value-->",
                      dtype, getattr(self.array.attrs, dtype))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        for dtype in checktypes:
            assert_array_equal(
                getattr(self.array.attrs, dtype),
                numpy.array([[1.1, 2.1], [2.1, 3.1]], dtype=dtype))

    def test06a_setUnicodeAttributes(self):
        """Checking setting unicode attributes (scalar case)"""

        self.array.attrs.pq = u'para\u0140lel'
        self.array.attrs.qr = u''                 # check #213 or gh-64
        self.array.attrs.rs = u'baz'

        # Check the results
        if common.verbose:
            if sys.platform != 'win32':
                # It seems that Windows cannot print this
                print("pq -->", repr(self.array.attrs.pq))
                # XXX: try to use repr instead
                # print("pq -->", repr(self.array.attrs.pq))
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.array.attrs.pq, numpy.unicode_))
        self.assertTrue(isinstance(self.array.attrs.qr, numpy.unicode_))
        self.assertTrue(isinstance(self.array.attrs.rs, numpy.unicode_))
        self.assertEqual(self.array.attrs.pq, u'para\u0140lel')
        self.assertEqual(self.array.attrs.qr, u'')
        self.assertEqual(self.array.attrs.rs, u'baz')

    def test06b_setUnicodeAttributes(self):
        """Checking setting unicode attributes (unidimensional 1-elem case)"""

        self.array.attrs.pq = numpy.array([u'para\u0140lel'])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.array.attrs.pq,
                           numpy.array([u'para\u0140lel']))

    def test06c_setUnicodeAttributes(self):
        """Checking setting unicode attributes (empty unidimensional
        1-elem case)"""

        # The next raises a `TypeError` when unpickled. See:
        # http://projects.scipy.org/numpy/ticket/1037
        # self.array.attrs.pq = numpy.array([u''])
        self.array.attrs.pq = numpy.array([u''], dtype="U1")

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray
            if common.verbose:
                print("pq -->", repr(self.array.attrs.pq))

        assert_array_equal(self.array.attrs.pq,
                           numpy.array([u''], dtype="U1"))

    def test06d_setUnicodeAttributes(self):
        """Checking setting unicode attributes (unidimensional 2-elem case)"""

        self.array.attrs.pq = numpy.array([u'para\u0140lel', u'bar3'])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.array.attrs.pq,
                           numpy.array([u'para\u0140lel', u'bar3']))

    def test06e_setUnicodeAttributes(self):
        """Checking setting unicode attributes (empty unidimensional
        2-elem case)"""

        self.array.attrs.pq = numpy.array(['', ''], dtype="U1")

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.array.attrs.pq,
                           numpy.array(['', ''], dtype="U1"))

    def test06f_setUnicodeAttributes(self):
        """Checking setting unicode attributes (bidimensional 4-elem case)"""

        self.array.attrs.pq = numpy.array([[u'para\u0140lel', 'foo2'],
                                           ['foo3', u'para\u0140lel4']])

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        assert_array_equal(self.array.attrs.pq,
                           numpy.array([[u'para\u0140lel', 'foo2'],
                                        ['foo3', u'para\u0140lel4']]))

    def test07a_setRecArrayAttributes(self):
        """Checking setting RecArray (NumPy) attributes."""

        dt = numpy.dtype('i4,f8')
        # Set some attrs
        self.array.attrs.pq = numpy.zeros(2, dt)
        self.array.attrs.qr = numpy.ones((2, 2), dt)
        self.array.attrs.rs = numpy.array([(1, 2.)], dt)

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.array.attrs.pq, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.qr, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.rs, numpy.ndarray))
        assert_array_equal(self.array.attrs.pq, numpy.zeros(2, dt))
        assert_array_equal(self.array.attrs.qr, numpy.ones((2, 2), dt))
        assert_array_equal(self.array.attrs.rs, numpy.array([(1, 2.)], dt))

    def test07b_setRecArrayAttributes(self):
        """Checking setting nested RecArray (NumPy) attributes."""

        # Build a nested dtype
        dt = numpy.dtype([('f1', [('f1', 'i2'), ('f2', 'f8')])])
        # Set some attrs
        self.array.attrs.pq = numpy.zeros(2, dt)
        self.array.attrs.qr = numpy.ones((2, 2), dt)
        self.array.attrs.rs = numpy.array([((1, 2.),)], dt)

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.array.attrs.pq, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.qr, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.rs, numpy.ndarray))
        assert_array_equal(self.array.attrs.pq, numpy.zeros(2, dt))
        assert_array_equal(self.array.attrs.qr, numpy.ones((2, 2), dt))
        assert_array_equal(self.array.attrs.rs, numpy.array([((1, 2),)], dt))

    def test07c_setRecArrayAttributes(self):
        """Checking setting multidim nested RecArray (NumPy) attributes."""

        # Build a nested dtype
        dt = numpy.dtype([('f1', [('f1', 'i2', (2,)), ('f2', 'f8')])])
        # Set some attrs
        self.array.attrs.pq = numpy.zeros(2, dt)
        self.array.attrs.qr = numpy.ones((2, 2), dt)
        self.array.attrs.rs = numpy.array([(([1, 3], 2.),)], dt)

        # Check the results
        if common.verbose:
            print("pq -->", self.array.attrs.pq)
            print("qr -->", self.array.attrs.qr)
            print("rs -->", self.array.attrs.rs)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r+")
            self.root = self.fileh.root
            self.array = self.fileh.root.anarray

        self.assertTrue(isinstance(self.array.attrs.pq, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.qr, numpy.ndarray))
        self.assertTrue(isinstance(self.array.attrs.rs, numpy.ndarray))
        assert_array_equal(self.array.attrs.pq, numpy.zeros(2, dt))
        assert_array_equal(self.array.attrs.qr, numpy.ones((2, 2), dt))
        assert_array_equal(self.array.attrs.rs, numpy.array(
            [(([1, 3], 2),)], dt))


class NotCloseTypesTestCase(TypesTestCase):
    close = 0


class CloseTypesTestCase(TypesTestCase):
    close = 1


class NoSysAttrsTestCase(unittest.TestCase):

    def setUp(self):
        # Create an instance of HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(
            self.file, mode="w", pytables_sys_attrs=False)
        self.root = self.fileh.root

        # Create a table object
        self.table = self.fileh.create_table(self.root, 'atable',
                                             Record, "Table title")
        # Create an array object
        self.array = self.fileh.create_array(self.root, 'anarray',
                                             [1], "Array title")
        # Create a group object
        self.group = self.fileh.create_group(self.root, 'agroup',
                                             "Group title")

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_listAttributes(self):
        """Checking listing attributes (no system attrs version)."""

        # With a Group object
        self.group._v_attrs.pq = "1"
        self.group._v_attrs.qr = "2"
        self.group._v_attrs.rs = "3"
        if common.verbose:
            print("Attribute list:", self.group._v_attrs._f_list())

        # Now, try with a Table object
        self.table.attrs.a = "1"
        self.table.attrs.c = "2"
        self.table.attrs.b = "3"
        if common.verbose:
            print("Attribute list:", self.table.attrs._f_list())

        # Finally, try with an Array object
        self.array.attrs.k = "1"
        self.array.attrs.j = "2"
        self.array.attrs.i = "3"
        if common.verbose:
            print("Attribute list:", self.array.attrs._f_list())

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(
                self.file, mode="r+")
            self.root = self.fileh.root

        agroup = self.root.agroup
        self.assertEqual(agroup._v_attrs._f_list("user"), ["pq", "qr", "rs"])
        self.assertEqual(agroup._v_attrs._f_list("sys"), [])
        self.assertEqual(agroup._v_attrs._f_list("all"), ["pq", "qr", "rs"])

        atable = self.root.atable
        self.assertEqual(atable.attrs._f_list(), ["a", "b", "c"])
        self.assertEqual(atable.attrs._f_list("sys"), [])
        self.assertEqual(atable.attrs._f_list("all"), ["a", "b", "c"])

        anarray = self.root.anarray
        self.assertEqual(anarray.attrs._f_list(), ["i", "j", "k"])
        self.assertEqual(anarray.attrs._f_list("sys"), [])
        self.assertEqual(anarray.attrs._f_list("all"), ["i", "j", "k"])


class NoSysAttrsNotClose(NoSysAttrsTestCase):
    close = False


class NoSysAttrsClose(NoSysAttrsTestCase):
    close = True


class SegFaultPythonTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_segfault(self):
        """Checking workaround for Python unpickle problem (see #253)."""

        self.h5file.root._v_attrs.trouble1 = "0"
        self.assertEqual(self.h5file.root._v_attrs.trouble1, "0")
        self.h5file.root._v_attrs.trouble2 = "0."
        self.assertEqual(self.h5file.root._v_attrs.trouble2, "0.")
        # Problem happens after reopening
        self._reopen()
        self.assertEqual(self.h5file.root._v_attrs.trouble1, "0")
        self.assertEqual(self.h5file.root._v_attrs.trouble2, "0.")
        if common.verbose:
            print("Great! '0' and '0.' values can be safely retrieved.")


class VlenStrAttrTestCase(PyTablesTestCase):

    def test01_vlen_str_scalar(self):
        """Checking file with variable length string attributes."""

        filename = self._testFilename('vlstr_attr.h5')
        fileh = open_file(filename)
        attr = "vlen_str_scalar"
        self.assertEqual(fileh.get_node_attr("/", attr), attr.encode('ascii'))
        fileh.close()

    def test02_vlen_str_array(self):
        """Checking file with variable length string attributes (1d)."""

        filename = self._testFilename('vlstr_attr.h5')
        fileh = open_file(filename)
        attr = "vlen_str_array"
        v = fileh.get_node_attr('/', attr)
        self.assertEqual(v.ndim, 1)
        for idx, item in enumerate(v):
            value = "%s_%d" % (attr, idx)
            self.assertEqual(item, value.encode('ascii'))
        fileh.close()

    def test03_vlen_str_matrix(self):
        """Checking file with variable length string attributes (2d)."""

        filename = self._testFilename('vlstr_attr.h5')
        fileh = open_file(filename)
        attr = "vlen_str_matrix"
        m = fileh.get_node_attr('/', attr)
        self.assertEqual(m.ndim, 2)
        for row, rowdata in enumerate(m):
            for col, item in enumerate(rowdata):
                value = "%s_%d%d" % (attr, row, col)
                self.assertEqual(item, value.encode('ascii'))
        fileh.close()


class UnsupportedAttrTypeTestCase(PyTablesTestCase):

    def test00_unsupportedType(self):
        """Checking file with unsupported type."""

        filename = self._testFilename('attr-u16.h5')
        fileh = open_file(filename)
        self.failUnlessWarns(DataTypeWarning, repr, fileh)
        fileh.close()


# Test for specific system attributes in EArray
class SpecificAttrsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_earray(self):
        "Testing EArray specific attrs (create)."
        ea = self.h5file.create_earray('/', 'ea', Int32Atom(), (2, 0, 4))
        if common.verbose:
            print("EXTDIM-->", ea.attrs.EXTDIM)
        self.assertEqual(ea.attrs.EXTDIM, 1)

    def test01_earray(self):
        "Testing EArray specific attrs (open)."
        ea = self.h5file.create_earray('/', 'ea', Int32Atom(), (0, 1, 4))
        self._reopen('r')
        ea = self.h5file.root.ea
        if common.verbose:
            print("EXTDIM-->", ea.attrs.EXTDIM)
        self.assertEqual(ea.attrs.EXTDIM, 0)


#----------------------------------------------------------------------
def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(NotCloseCreate))
        theSuite.addTest(unittest.makeSuite(CloseCreate))
        theSuite.addTest(unittest.makeSuite(NoCacheNotCloseCreate))
        theSuite.addTest(unittest.makeSuite(NoCacheCloseCreate))
        theSuite.addTest(unittest.makeSuite(DictCacheNotCloseCreate))
        theSuite.addTest(unittest.makeSuite(DictCacheCloseCreate))
        theSuite.addTest(unittest.makeSuite(NotCloseTypesTestCase))
        theSuite.addTest(unittest.makeSuite(CloseTypesTestCase))
        theSuite.addTest(unittest.makeSuite(NoSysAttrsNotClose))
        theSuite.addTest(unittest.makeSuite(NoSysAttrsClose))
        theSuite.addTest(unittest.makeSuite(SegFaultPythonTestCase))
        theSuite.addTest(unittest.makeSuite(VlenStrAttrTestCase))
        theSuite.addTest(unittest.makeSuite(UnsupportedAttrTypeTestCase))
        theSuite.addTest(unittest.makeSuite(SpecificAttrsTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_backcompat
# -*- coding: utf-8 -*-

from __future__ import print_function
import os
import shutil
import tempfile
import warnings
import unittest

import numpy

from tables import *
from tables.exceptions import FlavorWarning
from tables.tests import common
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

# Check read Tables from pytables version 0.8


class BackCompatTablesTestCase(common.PyTablesTestCase):

    #----------------------------------------

    def test01_readTable(self):
        """Checking backward compatibility of old formats of tables."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        warnings.filterwarnings("ignore", category=UserWarning)
        self.fileh = open_file(self._testFilename(self.file), "r")
        warnings.filterwarnings("default", category=UserWarning)

        table = self.fileh.get_node("/tuple0")

        # Read the 100 records
        result = [rec['var2'] for rec in table]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", rec)
            print("Total selected records in table ==> ", len(result))

        self.assertEqual(len(result), 100)
        self.fileh.close()


class Table2_1LZO(BackCompatTablesTestCase):
    file = "Table2_1_lzo_nrv2e_shuffle.h5"  # pytables 0.8.x versions and after


class Tables_LZO1(BackCompatTablesTestCase):
    file = "Tables_lzo1.h5"  # files compressed with LZO1


class Tables_LZO1_shuffle(BackCompatTablesTestCase):
    file = "Tables_lzo1_shuffle.h5"  # files compressed with LZO1 and shuffle


class Tables_LZO2(BackCompatTablesTestCase):
    file = "Tables_lzo2.h5"  # files compressed with LZO2


class Tables_LZO2_shuffle(BackCompatTablesTestCase):
    file = "Tables_lzo2_shuffle.h5"  # files compressed with LZO2 and shuffle

# Check read attributes from PyTables >= 1.0 properly


class BackCompatAttrsTestCase(common.PyTablesTestCase):
    file = "zerodim-attrs-%s.h5"

    def test01_readAttr(self):
        """Checking backward compatibility of old formats for attributes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readAttr..." % self.__class__.__name__)

        # Read old formats
        filename = self._testFilename(self.file)
        self.fileh = open_file(filename % self.format, "r")
        a = self.fileh.get_node("/a")
        scalar = numpy.array(1, dtype="int32")
        vector = numpy.array([1], dtype="int32")
        if self.format == "1.3":
            self.assertTrue(allequal(a.attrs.arrdim1, vector))
            self.assertTrue(allequal(a.attrs.arrscalar, scalar))
            self.assertEqual(a.attrs.pythonscalar, 1)
        elif self.format == "1.4":
            self.assertTrue(allequal(a.attrs.arrdim1, vector))
            self.assertTrue(allequal(a.attrs.arrscalar, scalar))
            self.assertTrue(allequal(a.attrs.pythonscalar, scalar))

        self.fileh.close()


class Attrs_1_3(BackCompatAttrsTestCase):
    format = "1.3"    # pytables 1.0.x versions and earlier


class Attrs_1_4(BackCompatAttrsTestCase):
    format = "1.4"    # pytables 1.1.x versions and later


class VLArrayTestCase(common.PyTablesTestCase):

    def test01_backCompat(self):
        """Checking backward compatibility with old flavors of VLArray."""

        # Open a PYTABLES_FORMAT_VERSION=1.6 file
        filename = self._testFilename("flavored_vlarrays-format1.6.h5")
        fileh = open_file(filename, "r")
        # Check that we can read the contents without problems (nor warnings!)
        vlarray1 = fileh.root.vlarray1
        self.assertEqual(vlarray1.flavor, "numeric")
        vlarray2 = fileh.root.vlarray2
        self.assertEqual(vlarray2.flavor, "python")
        self.assertEqual(vlarray2[1], [b'5', b'6', b'77'])

        fileh.close()


# Make sure that 1.x files with TimeXX types continue to be readable
# and that its byteorder is correctly retrieved.
class TimeTestCase(common.PyTablesTestCase):

    def setUp(self):
        # Open a PYTABLES_FORMAT_VERSION=1.x file
        filename = self._testFilename("time-table-vlarray-1_x.h5")
        self.fileh = open_file(filename, "r")

    def tearDown(self):
        self.fileh.close()

    def test00_table(self):
        """Checking backward compatibility with old TimeXX types (tables)."""

        # Check that we can read the contents without problems (nor warnings!)
        table = self.fileh.root.table
        self.assertEqual(table.byteorder, "little")

    def test01_vlarray(self):
        """Checking backward compatibility with old TimeXX types (vlarrays)."""

        # Check that we can read the contents without problems (nor warnings!)
        vlarray4 = self.fileh.root.vlarray4
        self.assertEqual(vlarray4.byteorder, "little")
        vlarray8 = self.fileh.root.vlarray4
        self.assertEqual(vlarray8.byteorder, "little")


class OldFlavorsTestCase01(common.PyTablesTestCase):
    close = False

    # numeric
    def test01_open(self):
        """Checking opening of (X)Array (old 'numeric' flavor)"""

        # Open the HDF5 with old numeric flavor
        filename = self._testFilename("oldflavor_numeric.h5")
        fileh = open_file(filename)

        # Assert other properties in array
        self.assertEqual(fileh.root.array1.flavor, 'numeric')
        self.assertEqual(fileh.root.array2.flavor, 'python')
        self.assertEqual(fileh.root.carray1.flavor, 'numeric')
        self.assertEqual(fileh.root.carray2.flavor, 'python')
        self.assertEqual(fileh.root.vlarray1.flavor, 'numeric')
        self.assertEqual(fileh.root.vlarray2.flavor, 'python')

        # Close the file
        fileh.close()

    def test02_copy(self):
        """Checking (X)Array.copy() method ('numetic' flavor)"""

        srcfile = self._testFilename("oldflavor_numeric.h5")
        tmpfile = tempfile.mktemp(".h5")
        shutil.copy(srcfile, tmpfile)

        # Open the HDF5 with old numeric flavor
        fileh = open_file(tmpfile, "r+")

        # Copy to another location
        self.failUnlessWarns(FlavorWarning,
                             fileh.root.array1.copy, '/', 'array1copy')
        fileh.root.array2.copy('/', 'array2copy')
        fileh.root.carray1.copy('/', 'carray1copy')
        fileh.root.carray2.copy('/', 'carray2copy')
        fileh.root.vlarray1.copy('/', 'vlarray1copy')
        fileh.root.vlarray2.copy('/', 'vlarray2copy')

        if self.close:
            fileh.close()
            fileh = open_file(tmpfile)

        else:
            fileh.flush()

        # Assert other properties in array
        self.assertEqual(fileh.root.array1copy.flavor, 'numeric')
        self.assertEqual(fileh.root.array2copy.flavor, 'python')
        self.assertEqual(fileh.root.carray1copy.flavor, 'numeric')
        self.assertEqual(fileh.root.carray2copy.flavor, 'python')
        self.assertEqual(fileh.root.vlarray1copy.flavor, 'numeric')
        self.assertEqual(fileh.root.vlarray2copy.flavor, 'python')

        # Close the file
        fileh.close()
        os.remove(tmpfile)


class OldFlavorsTestCase02(common.PyTablesTestCase):
    close = True

#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    lzo_avail = which_lib_version("lzo") is not None
    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(VLArrayTestCase))
        theSuite.addTest(unittest.makeSuite(TimeTestCase))
        theSuite.addTest(unittest.makeSuite(OldFlavorsTestCase01))
        theSuite.addTest(unittest.makeSuite(OldFlavorsTestCase02))
        if lzo_avail:
            theSuite.addTest(unittest.makeSuite(Table2_1LZO))
            theSuite.addTest(unittest.makeSuite(Tables_LZO1))
            theSuite.addTest(unittest.makeSuite(Tables_LZO1_shuffle))
            theSuite.addTest(unittest.makeSuite(Tables_LZO2))
            theSuite.addTest(unittest.makeSuite(Tables_LZO2_shuffle))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_basics
# -*- coding: utf-8 -*-

from __future__ import print_function
import os
import sys
import Queue
import shutil
import tempfile
import unittest
import warnings
import threading
import subprocess

try:
    import multiprocessing as mp
    multiprocessing_imported = True
except ImportError:
    multiprocessing_imported = False

import numpy

import tables
import tables.flavor
from tables import *
from tables.flavor import all_flavors, array_of_flavor
from tables.tests import common
from tables.parameters import NODE_CACHE_SLOTS
from tables.description import descr_from_dtype, dtype_from_descr

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class OpenFileFailureTestCase(common.PyTablesTestCase):
    def setUp(self):
        import tables.file

        self.N = len(tables.file._open_files)
        self.open_files = tables.file._open_files

    def test01_open_file(self):
        """Checking opening of a non existing file."""

        filename = tempfile.mktemp(".h5")
        try:
            fileh = open_file(filename)
            fileh.close()
        except IOError:
            self.assertEqual(self.N, len(self.open_files))
        else:
            self.fail("IOError exception not raised")

    def test02_open_file(self):
        """Checking opening of an existing non HDF5 file."""

        # create a dummy file
        filename = tempfile.mktemp(".h5")
        open(filename, 'wb').close()

        # Try to open the dummy file
        try:
            try:
                fileh = tables.open_file(filename)
                fileh.close()
            except HDF5ExtError:
                self.assertEqual(self.N, len(self.open_files))
            else:
                self.fail("HDF5ExtError exception not raised")
        finally:
            os.remove(filename)

    def test03_open_file(self):
        """Checking opening of an existing file with invalid mode."""

        # See gh-318

        # create a dummy file
        filename = tempfile.mktemp(".h5")
        fileh = tables.open_file(filename, "w")
        fileh.close()

        # Try to open the dummy file
        self.assertRaises(ValueError, tables.open_file, filename, "ab")

        os.remove(filename)


class OpenFileTestCase(common.PyTablesTestCase):

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        fileh = open_file(self.file, mode="w", title="File title",
                          node_cache_slots=self.node_cache_slots)
        root = fileh.root
        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Array example")
        fileh.create_table(root, 'table', {'var1': IntCol()}, "Table example")
        root._v_attrs.testattr = 41

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")
        fileh.create_table(root, 'atable', {'var1': IntCol()}, "Table title")

        # Create a group object
        group = fileh.create_group(root, 'agroup',
                                   "Group title")
        group._v_attrs.testattr = 42

        # Create a some objects there
        array1 = fileh.create_array(group, 'anarray1',
                                    [1, 2, 3, 4, 5, 6, 7], "Array title 1")
        array1.attrs.testattr = 42
        fileh.create_array(group, 'anarray2', [2], "Array title 2")
        fileh.create_table(group, 'atable1', {
                           'var1': IntCol()}, "Table title 1")
        ra = numpy.rec.array([(1, 11, 'a')], formats='u1,f4,a1')
        fileh.create_table(group, 'atable2', ra, "Table title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        group3 = fileh.create_group(group, 'agroup3', "Group title 3")

        # Create a new group in the third level
        fileh.create_group(group3, 'agroup4', "Group title 4")

        # Create an array in the root with the same name as one in 'agroup'
        fileh.create_array(root, 'anarray1', [1, 2],
                           title="Array example")

        fileh.close()

    def tearDown(self):
        # Remove the temporary file
        os.remove(self.file)
        common.cleanup(self)

    def test00_newFile(self):
        """Checking creation of a new file."""

        # Create an HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(
            file, mode="w", node_cache_slots=self.node_cache_slots)
        fileh.create_array(fileh.root, 'array', [1, 2],
                           title="Array example")
        # Get the CLASS attribute of the arr object
        class_ = fileh.root.array.attrs.CLASS

        # Close and delete the file
        fileh.close()
        os.remove(file)

        self.assertEqual(class_.capitalize(), "Array")

    def test00_newFile_unicode_filename(self):
        temp_dir = tempfile.mkdtemp()
        file_path = unicode(os.path.join(temp_dir, 'test.h5'))
        with open_file(file_path, 'w') as fileh:
            self.assertTrue(fileh, File)
        shutil.rmtree(temp_dir)

    def test00_newFile_numpy_str_filename(self):
        temp_dir = tempfile.mkdtemp()
        file_path = numpy.str_(os.path.join(temp_dir, 'test.h5'))
        with open_file(file_path, 'w') as fileh:
            self.assertTrue(fileh, File)
        shutil.rmtree(temp_dir)

    def test00_newFile_numpy_unicode_filename(self):
        temp_dir = tempfile.mkdtemp()
        file_path = numpy.unicode_(os.path.join(temp_dir, 'test.h5'))
        with open_file(file_path, 'w') as fileh:
            self.assertTrue(fileh, File)
        shutil.rmtree(temp_dir)

    def test01_openFile(self):
        """Checking opening of an existing file."""

        # Open the old HDF5 file
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Get the CLASS attribute of the arr object
        title = fileh.root.array.get_attr("TITLE")

        self.assertEqual(title, "Array example")
        fileh.close()

    def test02_appendFile(self):
        """Checking appending objects to an existing file."""

        # Append a new array to the existing file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.create_array(fileh.root, 'array2', [3, 4],
                           title="Title example 2")
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Get the CLASS attribute of the arr object
        title = fileh.root.array2.get_attr("TITLE")

        self.assertEqual(title, "Title example 2")
        fileh.close()

    def test02b_appendFile2(self):
        """Checking appending objects to an existing file ("a" version)"""

        # Append a new array to the existing file
        fileh = open_file(
            self.file, mode="a", node_cache_slots=self.node_cache_slots)
        fileh.create_array(fileh.root, 'array2', [3, 4],
                           title="Title example 2")
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Get the CLASS attribute of the arr object
        title = fileh.root.array2.get_attr("TITLE")

        self.assertEqual(title, "Title example 2")
        fileh.close()

    # Begin to raise errors...

    def test03_appendErrorFile(self):
        """Checking appending objects to an existing file in "w" mode."""

        # Append a new array to the existing file but in write mode
        # so, the existing file should be deleted!
        fileh = open_file(
            self.file, mode="w", node_cache_slots=self.node_cache_slots)
        fileh.create_array(fileh.root, 'array2', [3, 4],
                           title="Title example 2")
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)

        try:
            # Try to get the 'array' object in the old existing file
            fileh.root.array
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test04a_openErrorFile(self):
        """Checking opening a non-existing file for reading"""

        try:
            open_file("nonexistent.h5", mode="r",
                      node_cache_slots=self.node_cache_slots)
        except IOError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IOError was catched!")
                print(value)
        else:
            self.fail("expected an IOError")

    def test04b_alternateRootFile(self):
        """Checking alternate root access to the object tree."""

        # Open the existent HDF5 file
        fileh = open_file(self.file, mode="r", root_uep="/agroup",
                          node_cache_slots=self.node_cache_slots)
        # Get the CLASS attribute of the arr object
        if common.verbose:
            print("\nFile tree dump:", fileh)
        title = fileh.root.anarray1.get_attr("TITLE")
        # Get the node again, as this can trigger errors in some situations
        anarray1 = fileh.root.anarray1
        self.assertTrue(anarray1 is not None)

        self.assertEqual(title, "Array title 1")
        fileh.close()

    # This test works well, but HDF5 emits a series of messages that
    # may loose the user. It is better to deactivate it.
    def notest04c_alternateRootFile(self):
        """Checking non-existent alternate root access to the object tree"""

        try:
            open_file(self.file, mode="r", root_uep="/nonexistent",
                      node_cache_slots=self.node_cache_slots)
        except RuntimeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next RuntimeError was catched!")
                print(value)
        else:
            self.fail("expected an IOError")

    def test05a_removeGroupRecursively(self):
        """Checking removing a group recursively."""

        # Delete a group with leafs
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        try:
            fileh.remove_node(fileh.root.agroup)
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected a NodeError")

        # This should work now
        fileh.remove_node(fileh.root, 'agroup', recursive=1)

        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Try to get the removed object
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child of the removed object
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test05b_removeGroupRecursively(self):
        """Checking removing a group recursively and access to it
        immediately."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_removeGroupRecursively..." %
                  self.__class__.__name__)

        # Delete a group with leafs
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        try:
            fileh.remove_node(fileh.root, 'agroup')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected a NodeError")

        # This should work now
        fileh.remove_node(fileh.root, 'agroup', recursive=1)

        # Try to get the removed object
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child of the removed object
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test06_removeNodeWithDel(self):
        """Checking removing a node using ``__delattr__()``"""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        try:
            # This should fail because there is no *Python attribute*
            # called ``agroup``.
            del fileh.root.agroup
        except AttributeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next AttributeError was catched!")
                print(value)
        else:
            self.fail("expected an AttributeError")

        fileh.close()

    def test06a_removeGroup(self):
        """Checking removing a lonely group from an existing file."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.remove_node(fileh.root, 'agroup2')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Try to get the removed object
        try:
            fileh.root.agroup2
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test06b_removeLeaf(self):
        """Checking removing Leaves from an existing file."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.remove_node(fileh.root, 'anarray')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Try to get the removed object
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test06c_removeLeaf(self):
        """Checking removing Leaves and access it immediately."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.remove_node(fileh.root, 'anarray')

        # Try to get the removed object
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test06d_removeLeaf(self):
        """Checking removing a non-existent node"""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # Try to get the removed object
        try:
            fileh.remove_node(fileh.root, 'nonexistent')
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test06e_removeTable(self):
        """Checking removing Tables from an existing file."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.remove_node(fileh.root, 'atable')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Try to get the removed object
        try:
            fileh.root.atable
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test07_renameLeaf(self):
        """Checking renaming a leave and access it after a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.anarray, 'anarray2')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Ensure that the new name exists
        array_ = fileh.root.anarray2
        self.assertEqual(array_.name, "anarray2")
        self.assertEqual(array_._v_pathname, "/anarray2")
        self.assertEqual(array_._v_depth, 1)
        # Try to get the previous object with the old name
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test07b_renameLeaf(self):
        """Checking renaming Leaves and accesing them immediately."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.anarray, 'anarray2')

        # Ensure that the new name exists
        array_ = fileh.root.anarray2
        self.assertEqual(array_.name, "anarray2")
        self.assertEqual(array_._v_pathname, "/anarray2")
        self.assertEqual(array_._v_depth, 1)
        # Try to get the previous object with the old name
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test07c_renameLeaf(self):
        """Checking renaming Leaves and modify attributes after that."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.anarray, 'anarray2')
        array_ = fileh.root.anarray2
        array_.attrs.TITLE = "hello"
        # Ensure that the new attribute has been written correctly
        self.assertEqual(array_.title, "hello")
        self.assertEqual(array_.attrs.TITLE, "hello")
        fileh.close()

    def test07d_renameLeaf(self):
        """Checking renaming a Group under a nested group."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.agroup.anarray2, 'anarray3')

        # Ensure that we can access n attributes in the new group
        node = fileh.root.agroup.anarray3
        self.assertEqual(node._v_title, "Array title 2")
        fileh.close()

    def test08_renameToExistingLeaf(self):
        """Checking renaming a node to an existing name."""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        # Try to get the previous object with the old name
        try:
            fileh.rename_node(fileh.root.anarray, 'array')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        # Now overwrite the destination node.
        anarray = fileh.root.anarray
        fileh.rename_node(anarray, 'array', overwrite=True)
        self.assertTrue('/anarray' not in fileh)
        self.assertTrue(fileh.root.array is anarray)
        fileh.close()

    def test08b_renameToNotValidNaturalName(self):
        """Checking renaming a node to a non-valid natural name"""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        warnings.filterwarnings("error", category=NaturalNameWarning)
        # Try to get the previous object with the old name
        try:
            fileh.rename_node(fileh.root.anarray, 'array 2')
        except NaturalNameWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NaturalNameWarning was catched!")
                print(value)
        else:
            self.fail("expected an NaturalNameWarning")
        # Reset the warning
        warnings.filterwarnings("default", category=NaturalNameWarning)
        fileh.close()

    def test09_renameGroup(self):
        """Checking renaming a Group and access it after a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.agroup, 'agroup3')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Ensure that the new name exists
        group = fileh.root.agroup3
        self.assertEqual(group._v_name, "agroup3")
        self.assertEqual(group._v_pathname, "/agroup3")
        # The children of this group also must be accessible through the
        # new name path
        group2 = fileh.get_node("/agroup3/agroup3")
        self.assertEqual(group2._v_name, "agroup3")
        self.assertEqual(group2._v_pathname, "/agroup3/agroup3")
        # Try to get the previous object with the old name
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child with the old pathname
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test09b_renameGroup(self):
        """Checking renaming a Group and access it immediately."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.agroup, 'agroup3')

        # Ensure that the new name exists
        group = fileh.root.agroup3
        self.assertEqual(group._v_name, "agroup3")
        self.assertEqual(group._v_pathname, "/agroup3")
        # The children of this group also must be accessible through the
        # new name path
        group2 = fileh.get_node("/agroup3/agroup3")
        self.assertEqual(group2._v_name, "agroup3")
        self.assertEqual(group2._v_pathname, "/agroup3/agroup3")
        # Try to get the previous object with the old name
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child with the old pathname
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test09c_renameGroup(self):
        """Checking renaming a Group and modify attributes afterwards."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.agroup, 'agroup3')

        # Ensure that we can modify attributes in the new group
        group = fileh.root.agroup3
        group._v_attrs.TITLE = "Hello"
        self.assertEqual(group._v_title, "Hello")
        self.assertEqual(group._v_attrs.TITLE, "Hello")
        fileh.close()

    def test09d_renameGroup(self):
        """Checking renaming a Group under a nested group."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        fileh.rename_node(fileh.root.agroup.agroup3, 'agroup4')

        # Ensure that we can access n attributes in the new group
        group = fileh.root.agroup.agroup4
        self.assertEqual(group._v_title, "Group title 3")
        fileh.close()

    def test09e_renameGroup(self):
        """Checking renaming a Group with nested groups in the LRU cache."""
        # This checks for ticket #126.

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        # Load intermediate groups and keep a nested one alive.
        g = fileh.root.agroup.agroup3.agroup4
        self.assertTrue(g is not None)
        fileh.rename_node('/', name='agroup', newname='agroup_')
        self.assertTrue('/agroup_/agroup4' not in fileh)  # see ticket #126
        self.assertTrue('/agroup' not in fileh)
        for newpath in ['/agroup_', '/agroup_/agroup3',
                        '/agroup_/agroup3/agroup4']:
            self.assertTrue(newpath in fileh)
            self.assertEqual(newpath, fileh.get_node(newpath)._v_pathname)
        fileh.close()

    def test10_moveLeaf(self):
        """Checking moving a leave and access it after a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.anarray, newgroup, 'anarray2')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Ensure that the new name exists
        array_ = fileh.root.newgroup.anarray2
        self.assertEqual(array_.name, "anarray2")
        self.assertEqual(array_._v_pathname, "/newgroup/anarray2")
        self.assertEqual(array_._v_depth, 2)
        # Try to get the previous object with the old name
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test10b_moveLeaf(self):
        """Checking moving a leave and access it without a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.anarray, newgroup, 'anarray2')

        # Ensure that the new name exists
        array_ = fileh.root.newgroup.anarray2
        self.assertEqual(array_.name, "anarray2")
        self.assertEqual(array_._v_pathname, "/newgroup/anarray2")
        self.assertEqual(array_._v_depth, 2)
        # Try to get the previous object with the old name
        try:
            fileh.root.anarray
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test10c_moveLeaf(self):
        """Checking moving Leaves and modify attributes after that."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.anarray, newgroup, 'anarray2')
        array_ = fileh.root.newgroup.anarray2
        array_.attrs.TITLE = "hello"
        # Ensure that the new attribute has been written correctly
        self.assertEqual(array_.title, "hello")
        self.assertEqual(array_.attrs.TITLE, "hello")
        fileh.close()

    def test10d_moveToExistingLeaf(self):
        """Checking moving a leaf to an existing name."""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        # Try to get the previous object with the old name
        try:
            fileh.move_node(fileh.root.anarray, fileh.root, 'array')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test10_2_moveTable(self):
        """Checking moving a table and access it after a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.atable, newgroup, 'atable2')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Ensure that the new name exists
        table_ = fileh.root.newgroup.atable2
        self.assertEqual(table_.name, "atable2")
        self.assertEqual(table_._v_pathname, "/newgroup/atable2")
        self.assertEqual(table_._v_depth, 2)
        # Try to get the previous object with the old name
        try:
            fileh.root.atable
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test10_2b_moveTable(self):
        """Checking moving a table and access it without a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.atable, newgroup, 'atable2')

        # Ensure that the new name exists
        table_ = fileh.root.newgroup.atable2
        self.assertEqual(table_.name, "atable2")
        self.assertEqual(table_._v_pathname, "/newgroup/atable2")
        self.assertEqual(table_._v_depth, 2)
        # Try to get the previous object with the old name
        try:
            fileh.root.atable
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test10_2b_bis_moveTable(self):
        """Checking moving a table and use cached row without a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        # Cache the Row attribute prior to the move
        row = fileh.root.atable.row
        fileh.move_node(fileh.root.atable, newgroup, 'atable2')

        # Ensure that the new name exists
        table_ = fileh.root.newgroup.atable2
        self.assertEqual(table_.name, "atable2")
        self.assertEqual(table_._v_pathname, "/newgroup/atable2")
        self.assertEqual(table_._v_depth, 2)
        # Ensure that cache Row attribute has been updated
        row = table_.row
        self.assertEqual(table_._v_pathname, row.table._v_pathname)
        nrows = table_.nrows
        # Add a new row just to make sure that this works
        row.append()
        table_.flush()
        self.assertEqual(table_.nrows, nrows + 1)
        fileh.close()

    def test10_2c_moveTable(self):
        """Checking moving tables and modify attributes after that."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group("/", "newgroup")
        fileh.move_node(fileh.root.atable, newgroup, 'atable2')
        table_ = fileh.root.newgroup.atable2
        table_.attrs.TITLE = "hello"
        # Ensure that the new attribute has been written correctly
        self.assertEqual(table_.title, "hello")
        self.assertEqual(table_.attrs.TITLE, "hello")
        fileh.close()

    def test10_2d_moveToExistingTable(self):
        """Checking moving a table to an existing name."""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        # Try to get the previous object with the old name
        try:
            fileh.move_node(fileh.root.atable, fileh.root, 'table')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test10_2e_moveToExistingTableOverwrite(self):
        """Checking moving a table to an existing name, overwriting it."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        srcNode = fileh.root.atable
        fileh.move_node(srcNode, fileh.root, 'table', overwrite=True)
        dstNode = fileh.root.table

        self.assertTrue(srcNode is dstNode)
        fileh.close()

    def test11_moveGroup(self):
        """Checking moving a Group and access it after a close/open."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group(fileh.root, 'newgroup')
        fileh.move_node(fileh.root.agroup, newgroup, 'agroup3')
        fileh.close()

        # Open this file in read-only mode
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Ensure that the new name exists
        group = fileh.root.newgroup.agroup3
        self.assertEqual(group._v_name, "agroup3")
        self.assertEqual(group._v_pathname, "/newgroup/agroup3")
        self.assertEqual(group._v_depth, 2)
        # The children of this group must also be accessible through the
        # new name path
        group2 = fileh.get_node("/newgroup/agroup3/agroup3")
        self.assertEqual(group2._v_name, "agroup3")
        self.assertEqual(group2._v_pathname, "/newgroup/agroup3/agroup3")
        self.assertEqual(group2._v_depth, 3)
        # Try to get the previous object with the old name
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child with the old pathname
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test11b_moveGroup(self):
        """Checking moving a Group and access it immediately."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group(fileh.root, 'newgroup')
        fileh.move_node(fileh.root.agroup, newgroup, 'agroup3')
        # Ensure that the new name exists
        group = fileh.root.newgroup.agroup3
        self.assertEqual(group._v_name, "agroup3")
        self.assertEqual(group._v_pathname, "/newgroup/agroup3")
        self.assertEqual(group._v_depth, 2)
        # The children of this group must also be accessible through the
        # new name path
        group2 = fileh.get_node("/newgroup/agroup3/agroup3")
        self.assertEqual(group2._v_name, "agroup3")
        self.assertEqual(group2._v_pathname, "/newgroup/agroup3/agroup3")
        self.assertEqual(group2._v_depth, 3)
        # Try to get the previous object with the old name
        try:
            fileh.root.agroup
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        # Try to get a child with the old pathname
        try:
            fileh.get_node("/agroup/agroup3")
        except LookupError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next LookupError was catched!")
                print(value)
        else:
            self.fail("expected an LookupError")
        fileh.close()

    def test11c_moveGroup(self):
        """Checking moving a Group and modify attributes afterwards."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        newgroup = fileh.create_group(fileh.root, 'newgroup')
        fileh.move_node(fileh.root.agroup, newgroup, 'agroup3')

        # Ensure that we can modify attributes in the new group
        group = fileh.root.newgroup.agroup3
        group._v_attrs.TITLE = "Hello"
        group._v_attrs.hola = "Hello"
        self.assertEqual(group._v_title, "Hello")
        self.assertEqual(group._v_attrs.TITLE, "Hello")
        self.assertEqual(group._v_attrs.hola, "Hello")
        fileh.close()

    def test11d_moveToExistingGroup(self):
        """Checking moving a group to an existing name."""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        # Try to get the previous object with the old name
        try:
            fileh.move_node(fileh.root.agroup, fileh.root, 'agroup2')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test11e_moveToExistingGroupOverwrite(self):
        """Checking moving a group to an existing name, overwriting it."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup2 -> agroup
        srcNode = fileh.root.agroup2
        fileh.move_node(srcNode, fileh.root, 'agroup', overwrite=True)
        dstNode = fileh.root.agroup

        self.assertTrue(srcNode is dstNode)
        fileh.close()

    def test12a_moveNodeOverItself(self):
        """Checking moving a node over itself."""

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # array -> array
        srcNode = fileh.root.array
        fileh.move_node(srcNode, fileh.root, 'array')
        dstNode = fileh.root.array

        self.assertTrue(srcNode is dstNode)
        fileh.close()

    def test12b_moveGroupIntoItself(self):
        """Checking moving a group into itself."""

        # Open this file
        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        try:
            # agroup2 -> agroup2/
            fileh.move_node(fileh.root.agroup2, fileh.root.agroup2)
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test13a_copyLeaf(self):
        "Copying a leaf."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # array => agroup2/
        new_node = fileh.copy_node(fileh.root.array, fileh.root.agroup2)
        dstNode = fileh.root.agroup2.array

        self.assertTrue(new_node is dstNode)
        fileh.close()

    def test13b_copyGroup(self):
        "Copying a group."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup2 => agroup/
        new_node = fileh.copy_node(fileh.root.agroup2, fileh.root.agroup)
        dstNode = fileh.root.agroup.agroup2

        self.assertTrue(new_node is dstNode)
        fileh.close()

    def test13c_copyGroupSelf(self):
        "Copying a group into itself."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup2 => agroup2/
        new_node = fileh.copy_node(fileh.root.agroup2, fileh.root.agroup2)
        dstNode = fileh.root.agroup2.agroup2

        self.assertTrue(new_node is dstNode)
        fileh.close()

    def test13d_copyGroupRecursive(self):
        "Recursively copying a group."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup => agroup2/
        new_node = fileh.copy_node(
            fileh.root.agroup, fileh.root.agroup2, recursive=True)
        dstNode = fileh.root.agroup2.agroup

        self.assertTrue(new_node is dstNode)
        dstChild1 = dstNode.anarray1
        self.assertTrue(dstChild1 is not None)
        dstChild2 = dstNode.anarray2
        self.assertTrue(dstChild2 is not None)
        dstChild3 = dstNode.agroup3
        self.assertTrue(dstChild3 is not None)
        fileh.close()

    def test13e_copyRootRecursive(self):
        "Recursively copying the root group into the root of another file."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(
            file2, mode="w", node_cache_slots=self.node_cache_slots)

        # fileh.root => fileh2.root
        new_node = fileh.copy_node(
            fileh.root, fileh2.root, recursive=True)
        dstNode = fileh2.root

        self.assertTrue(new_node is dstNode)
        self.assertTrue("/agroup" in fileh2)
        self.assertTrue("/agroup/anarray1" in fileh2)
        self.assertTrue("/agroup/agroup3" in fileh2)

        fileh.close()
        fileh2.close()
        os.remove(file2)

    def test13f_copyRootRecursive(self):
        "Recursively copying the root group into a group in another file."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(
            file2, mode="w", node_cache_slots=self.node_cache_slots)
        fileh2.create_group('/', 'agroup2')

        # fileh.root => fileh2.root.agroup2
        new_node = fileh.copy_node(
            fileh.root, fileh2.root.agroup2, recursive=True)
        dstNode = fileh2.root.agroup2

        self.assertTrue(new_node is dstNode)
        self.assertTrue("/agroup2/agroup" in fileh2)
        self.assertTrue("/agroup2/agroup/anarray1" in fileh2)
        self.assertTrue("/agroup2/agroup/agroup3" in fileh2)

        fileh.close()
        fileh2.close()
        os.remove(file2)

    def test13g_copyRootItself(self):
        "Recursively copying the root group into itself."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        agroup2 = fileh.root
        self.assertTrue(agroup2 is not None)

        # fileh.root => fileh.root
        self.assertRaises(IOError, fileh.copy_node,
                          fileh.root, fileh.root, recursive=True)
        fileh.close()

    def test14a_copyNodeExisting(self):
        "Copying over an existing node."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        try:
            # agroup2 => agroup
            fileh.copy_node(fileh.root.agroup2, newname='agroup')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test14b_copyNodeExistingOverwrite(self):
        "Copying over an existing node, overwriting it."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup2 => agroup
        new_node = fileh.copy_node(fileh.root.agroup2, newname='agroup',
                                   overwrite=True)
        dstNode = fileh.root.agroup

        self.assertTrue(new_node is dstNode)
        fileh.close()

    def test14b2_copyNodeExistingOverwrite(self):
        "Copying over an existing node in other file, overwriting it."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(
            file2, mode="w", node_cache_slots=self.node_cache_slots)

        # file1:/anarray1 => file2:/anarray1
        new_node = fileh.copy_node(fileh.root.agroup.anarray1,
                                   newparent=fileh2.root)
        # file1:/ => file2:/
        new_node = fileh.copy_node(fileh.root, fileh2.root,
                                   overwrite=True, recursive=True)
        dstNode = fileh2.root

        self.assertTrue(new_node is dstNode)
        fileh.close()
        fileh2.close()
        os.remove(file2)

    def test14c_copyNodeExistingSelf(self):
        "Copying over self."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        try:
            # agroup => agroup
            fileh.copy_node(fileh.root.agroup, newname='agroup')
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test14d_copyNodeExistingOverwriteSelf(self):
        "Copying over self, trying to overwrite."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        try:
            # agroup => agroup
            fileh.copy_node(
                fileh.root.agroup, newname='agroup', overwrite=True)
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test14e_copyGroupSelfRecursive(self):
        "Recursively copying a group into itself."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        try:
            # agroup => agroup/
            fileh.copy_node(
                fileh.root.agroup, fileh.root.agroup, recursive=True)
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NodeError was catched!")
                print(value)
        else:
            self.fail("expected an NodeError")
        fileh.close()

    def test15a_oneStepMove(self):
        "Moving and renaming a node in a single action."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # anarray1 -> agroup/array
        srcNode = fileh.root.anarray1
        fileh.move_node(srcNode, fileh.root.agroup, 'array')
        dstNode = fileh.root.agroup.array

        self.assertTrue(srcNode is dstNode)
        fileh.close()

    def test15b_oneStepCopy(self):
        "Copying and renaming a node in a single action."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # anarray1 => agroup/array
        new_node = fileh.copy_node(
            fileh.root.anarray1, fileh.root.agroup, 'array')
        dstNode = fileh.root.agroup.array

        self.assertTrue(new_node is dstNode)
        fileh.close()

    def test16a_fullCopy(self):
        "Copying full data and user attributes."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup => groupcopy
        srcNode = fileh.root.agroup
        new_node = fileh.copy_node(
            srcNode, newname='groupcopy', recursive=True)
        dstNode = fileh.root.groupcopy

        self.assertTrue(new_node is dstNode)
        self.assertEqual(srcNode._v_attrs.testattr, dstNode._v_attrs.testattr)
        self.assertEqual(
            srcNode.anarray1.attrs.testattr, dstNode.anarray1.attrs.testattr)
        self.assertEqual(srcNode.anarray1.read(), dstNode.anarray1.read())
        fileh.close()

    def test16b_partialCopy(self):
        "Copying partial data and no user attributes."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        # agroup => groupcopy
        srcNode = fileh.root.agroup
        new_node = fileh.copy_node(
            srcNode, newname='groupcopy',
            recursive=True, copyuserattrs=False,
            start=0, stop=5, step=2)
        dstNode = fileh.root.groupcopy

        self.assertTrue(new_node is dstNode)
        self.assertFalse(hasattr(dstNode._v_attrs, 'testattr'))
        self.assertFalse(hasattr(dstNode.anarray1.attrs, 'testattr'))
        self.assertEqual(srcNode.anarray1.read()[
                         0:5:2], dstNode.anarray1.read())
        fileh.close()

    def test16c_fullCopy(self):
        "Copying full data and user attributes (from file to file)."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)

        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(
            file2, mode="w", node_cache_slots=self.node_cache_slots)

        # file1:/ => file2:groupcopy
        srcNode = fileh.root
        new_node = fileh.copy_node(
            srcNode, fileh2.root, newname='groupcopy', recursive=True)
        dstNode = fileh2.root.groupcopy

        self.assertTrue(new_node is dstNode)
        self.assertEqual(srcNode._v_attrs.testattr, dstNode._v_attrs.testattr)
        self.assertEqual(
            srcNode.agroup.anarray1.attrs.testattr,
            dstNode.agroup.anarray1.attrs.testattr)
        self.assertEqual(srcNode.agroup.anarray1.read(),
                         dstNode.agroup.anarray1.read())

        fileh.close()
        fileh2.close()
        os.remove(file2)

    def test17a_CopyChunkshape(self):
        "Copying dataset with a chunkshape."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        srcTable = fileh.root.table
        newTable = fileh.copy_node(
            srcTable, newname='tablecopy', chunkshape=11)

        self.assertEqual(newTable.chunkshape, (11,))
        self.assertNotEqual(srcTable.chunkshape, newTable.chunkshape)
        fileh.close()

    def test17b_CopyChunkshape(self):
        "Copying dataset with a chunkshape with 'keep' value."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        srcTable = fileh.root.table
        newTable = fileh.copy_node(
            srcTable, newname='tablecopy', chunkshape='keep')

        self.assertEqual(srcTable.chunkshape, newTable.chunkshape)
        fileh.close()

    def test17c_CopyChunkshape(self):
        "Copying dataset with a chunkshape with 'auto' value."

        fileh = open_file(
            self.file, mode="r+", node_cache_slots=self.node_cache_slots)
        srcTable = fileh.root.table
        newTable = fileh.copy_node(
            srcTable, newname='tablecopy', chunkshape=11)
        newTable2 = fileh.copy_node(
            newTable, newname='tablecopy2', chunkshape='auto')

        self.assertEqual(srcTable.chunkshape, newTable2.chunkshape)
        fileh.close()

    def test18_closedRepr(self):
        "Representing a closed node as a string."
        fileh = open_file(
            self.file, node_cache_slots=self.node_cache_slots)
        for node in [fileh.root.agroup, fileh.root.anarray]:
            node._f_close()
            self.assertTrue('closed' in str(node))
            self.assertTrue('closed' in repr(node))
        fileh.close()

    def test19_fileno(self):
        """Checking that the 'fileno()' method works."""

        # Open the old HDF5 file
        fileh = open_file(
            self.file, mode="r", node_cache_slots=self.node_cache_slots)
        # Get the file descriptor for this file
        fd = fileh.fileno()
        if common.verbose:
            print("Value of fileno():", fd)
        self.assertTrue(fd >= 0)
        fileh.close()


class NodeCacheOpenFile(OpenFileTestCase):
    node_cache_slots = NODE_CACHE_SLOTS


class NoNodeCacheOpenFile(OpenFileTestCase):
    node_cache_slots = 0


class DictNodeCacheOpenFile(OpenFileTestCase):
    node_cache_slots = -NODE_CACHE_SLOTS


class CheckFileTestCase(common.PyTablesTestCase):

    def test00_isHDF5File(self):
        """Checking is_hdf5_file function (TRUE case)"""

        # Create a PyTables file (and by so, an HDF5 file)
        filename = tempfile.mktemp(".h5")
        fileh = open_file(filename, mode="w")
        fileh.create_array(fileh.root, 'array', [
                           1, 2], title="Title example")

        # For this method to run, it needs a closed file
        fileh.close()

        # When file has an HDF5 format, always returns 1
        if common.verbose:
            print("\nisHDF5File(%s) ==> %d" % (filename,
                                               is_hdf5_file(filename)))
        self.assertEqual(is_hdf5_file(filename), 1)

        # Then, delete the file
        os.remove(filename)

    def test01_isHDF5File(self):
        """Checking is_hdf5_file function (FALSE case)"""

        # Create a regular (text) file
        file = tempfile.mktemp(".h5")
        fileh = open(file, "w")
        fileh.write("Hello!")
        fileh.close()

        version = is_hdf5_file(file)
        # When file is not an HDF5 format, always returns 0 or
        # negative value
        self.assertTrue(version <= 0)

        # Then, delete the file
        os.remove(file)

    def test01x_isHDF5File_nonexistent(self):
        """Identifying a nonexistent HDF5 file."""
        self.assertRaises(IOError, is_hdf5_file, 'nonexistent')

    def test01x_isHDF5File_unreadable(self):
        """Identifying an unreadable HDF5 file."""

        if hasattr(os, 'getuid') and os.getuid() != 0:
            h5fname = tempfile.mktemp(suffix='.h5')
            open_file(h5fname, 'w').close()
            try:
                os.chmod(h5fname, 0)  # no permissions at all
                self.assertRaises(IOError, is_hdf5_file, h5fname)
            finally:
                os.remove(h5fname)

    def test02_isPyTablesFile(self):
        """Checking is_pytables_file function (TRUE case)"""

        # Create a PyTables file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        fileh.create_array(fileh.root, 'array', [
                           1, 2], title="Title example")

        # For this method to run, it needs a closed file
        fileh.close()

        version = is_pytables_file(file)
        # When file has a PyTables format, always returns "1.0" string or
        # greater
        if common.verbose:
            print()
            print("\nPyTables format version number ==> %s" % version)
        self.assertTrue(version >= "1.0")

        # Then, delete the file
        os.remove(file)

    def test03_isPyTablesFile(self):
        """Checking is_pytables_file function (FALSE case)"""

        # Create a regular (text) file
        file = tempfile.mktemp(".h5")
        fileh = open(file, "w")
        fileh.write("Hello!")
        fileh.close()

        version = is_pytables_file(file)
        # When file is not a PyTables format, always returns 0 or
        # negative value
        if common.verbose:
            print()
            print("\nPyTables format version number ==> %s" % version)
        self.assertTrue(version is None)

        # Then, delete the file
        os.remove(file)

    def test04_openGenericHDF5File(self):
        """Checking opening of a generic HDF5 file."""

        # Open an existing generic HDF5 file
        fileh = open_file(self._testFilename("ex-noattr.h5"), mode="r")

        # Check for some objects inside

        # A group
        columns = fileh.get_node("/columns", classname="Group")
        self.assertEqual(columns._v_name, "columns")

        # An Array
        array_ = fileh.get_node(columns, "TDC", classname="Array")
        self.assertEqual(array_._v_name, "TDC")

        # (The new LRU code defers the appearance of a warning to this point).

        # Here comes an Array of H5T_ARRAY type
        ui = fileh.get_node(columns, "pressure", classname="Array")
        self.assertEqual(ui._v_name, "pressure")
        if common.verbose:
            print("Array object with type H5T_ARRAY -->", repr(ui))
            print("Array contents -->", ui[:])

        # A Table
        table = fileh.get_node("/detector", "table", classname="Table")
        self.assertEqual(table._v_name, "table")

        fileh.close()

    def test04b_UnImplementedOnLoading(self):
        """Checking failure loading resulting in an ``UnImplemented`` node."""

        ############### Note for developers ###############################
        # This test fails if you have the line:                           #
        # ##return ChildClass(self, childname)  # uncomment for debugging #
        # uncommented in Group.py!                                        #
        ###################################################################

        h5file = open_file(self._testFilename('smpl_unsupptype.h5'))
        try:
            node = self.assertWarns(
                UserWarning, h5file.get_node, '/CompoundChunked')
            self.assertTrue(isinstance(node, UnImplemented))
        finally:
            h5file.close()

    def test04c_UnImplementedScalar(self):
        """Checking opening of HDF5 files containing scalar dataset of
        UnImlemented type."""

        h5file = open_file(self._testFilename("scalar.h5"))
        try:
            node = self.assertWarns(
                UserWarning, h5file.get_node, '/variable length string')
            self.assertTrue(isinstance(node, UnImplemented))
        finally:
            h5file.close()

    def test05_copyUnimplemented(self):
        """Checking that an UnImplemented object cannot be copied."""

        # Open an existing generic HDF5 file
        fileh = open_file(self._testFilename("smpl_unsupptype.h5"), mode="r")
        ui = self.assertWarns(
            UserWarning, fileh.get_node, '/CompoundChunked')
        self.assertEqual(ui._v_name, 'CompoundChunked')
        if common.verbose:
            print("UnImplement object -->", repr(ui))

        # Check that it cannot be copied to another file
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w")
        # Force the userwarning to issue an error
        warnings.filterwarnings("error", category=UserWarning)
        try:
            ui.copy(fileh2.root, "newui")
        except UserWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next UserWarning was catched:")
                print(value)
        else:
            self.fail("expected an UserWarning")

        # Reset the warnings
        # Be careful with that, because this enables all the warnings
        # on the rest of the tests!
        # warnings.resetwarnings()
        # better use:
        warnings.filterwarnings("default", category=UserWarning)

        # Delete the new (empty) file
        fileh2.close()
        os.remove(file2)

        fileh.close()

    # The next can be used to check the copy of Array objects with H5T_ARRAY
    # in the future
    def _test05_copyUnimplemented(self):
        """Checking that an UnImplemented object cannot be copied."""

        # Open an existing generic HDF5 file
        # We don't need to wrap this in a try clause because
        # it has already been tried and the warning will not happen again
        fileh = open_file(self._testFilename("ex-noattr.h5"), mode="r")
        # An unsupported object (the deprecated H5T_ARRAY type in
        # Array, from pytables 0.8 on)
        ui = fileh.get_node(fileh.root.columns, "pressure")
        self.assertEqual(ui._v_name, "pressure")
        if common.verbose:
            print("UnImplement object -->", repr(ui))

        # Check that it cannot be copied to another file
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w")
        # Force the userwarning to issue an error
        warnings.filterwarnings("error", category=UserWarning)
        try:
            ui.copy(fileh2.root, "newui")
        except UserWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next UserWarning was catched:")
                print(value)
        else:
            self.fail("expected an UserWarning")

        # Reset the warnings
        # Be careful with that, because this enables all the warnings
        # on the rest of the tests!
        # warnings.resetwarnings()
        # better use:
        warnings.filterwarnings("default", category=UserWarning)

        # Delete the new (empty) file
        fileh2.close()
        os.remove(file2)

        fileh.close()


class ThreadingTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def setUp(self):
        super(ThreadingTestCase, self).setUp()
        self.h5file.create_carray('/', 'test_array', tables.Int64Atom(),
                                  (200, 300))
        self.h5file.close()

    def test(self):
        filename = self.h5fname

        def run(filename, q):
            try:
                f = tables.open_file(filename, mode='r')
                arr = f.root.test_array[8:12, 18:22]
                assert arr.max() == arr.min() == 0
                f.close()
            except Exception as e:
                q.put(sys.exc_info())
            else:
                q.put('OK')

        threads = []
        q = Queue.Queue()
        for i in xrange(10):
            t = threading.Thread(target=run, args=(filename, q))
            t.start()
            threads.append(t)

        for i in xrange(10):
            self.assertEqual(q.get(), 'OK')

        for t in threads:
            t.join()


class PythonAttrsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test interactions of Python attributes and child nodes."""

    def test00_attrOverChild(self):
        """Setting a Python attribute over a child node."""

        root = self.h5file.root

        # Create ``/test`` and overshadow it with ``root.test``.
        child = self.h5file.create_array(root, 'test', [1])
        attr = 'foobar'
        self.assertWarns(NaturalNameWarning,
                         setattr, root, 'test', attr)

        self.assertTrue(root.test is attr)
        self.assertTrue(root._f_get_child('test') is child)

        # Now bring ``/test`` again to light.
        del root.test

        self.assertTrue(root.test is child)

        # Now there is no *attribute* named ``test``.
        self.assertRaises(AttributeError,
                          delattr, root, 'test')

    def test01_childUnderAttr(self):
        """Creating a child node under a Python attribute."""

        h5file = self.h5file
        root = h5file.root

        # Create ``root.test`` and an overshadowed ``/test``.
        attr = 'foobar'
        root.test = attr
        self.assertWarns(NaturalNameWarning,
                         h5file.create_array, root, 'test', [1])
        child = h5file.get_node('/test')

        self.assertTrue(root.test is attr)
        self.assertTrue(root._f_get_child('test') is child)

        # Now bring ``/test`` again to light.
        del root.test

        self.assertTrue(root.test is child)

        # Now there is no *attribute* named ``test``.
        self.assertRaises(AttributeError,
                          delattr, root, 'test')

    def test02_nodeAttrInLeaf(self):
        """Assigning a ``Node`` value as an attribute to a ``Leaf``."""

        h5file = self.h5file

        array1 = h5file.create_array('/', 'array1', [1])
        array2 = h5file.create_array('/', 'array2', [1])

        # This may make the garbage collector work a little.
        array1.array2 = array2
        array2.array1 = array1

        # Check the assignments.
        self.assertTrue(array1.array2 is array2)
        self.assertTrue(array2.array1 is array1)
        self.assertRaises(NoSuchNodeError,  # ``/array1`` is not a group
                          h5file.get_node, '/array1/array2')
        self.assertRaises(NoSuchNodeError,  # ``/array2`` is not a group
                          h5file.get_node, '/array2/array3')

    def test03_nodeAttrInGroup(self):
        """Assigning a ``Node`` value as an attribute to a ``Group``."""

        h5file = self.h5file
        root = h5file.root

        array = h5file.create_array('/', 'array', [1])

        # Assign the array to a pair of attributes,
        # one of them overshadowing the original.
        root.arrayAlias = array
        self.assertWarns(NaturalNameWarning,
                         setattr, root, 'array', array)

        # Check the assignments.
        self.assertTrue(root.arrayAlias is array)
        self.assertTrue(root.array is array)
        self.assertRaises(NoSuchNodeError, h5file.get_node, '/arrayAlias')
        self.assertTrue(h5file.get_node('/array') is array)

        # Remove the attribute overshadowing the child.
        del root.array
        # Now there is no *attribute* named ``array``.
        self.assertRaises(AttributeError,
                          delattr, root, 'array')


class StateTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test that ``File`` and ``Node`` operations check their state (open or
    closed, readable or writable) before proceeding."""

    def test00_fileCopyFileClosed(self):
        """Test copying a closed file."""

        h5cfname = tempfile.mktemp(suffix='.h5')
        self.h5file.close()

        try:
            self.assertRaises(ClosedFileError,
                              self.h5file.copy_file, h5cfname)
        finally:
            if os.path.exists(h5cfname):
                os.remove(h5fcname)
                self.fail("a (maybe incomplete) copy "
                          "of a closed file was created")

    def test01_fileCloseClosed(self):
        """Test closing an already closed file."""

        self.h5file.close()

        try:
            self.h5file.close()
        except ClosedFileError:
            self.fail("could not close an already closed file")

    def test02_fileFlushClosed(self):
        """Test flushing a closed file."""

        self.h5file.close()
        self.assertRaises(ClosedFileError, self.h5file.flush)

    def test03_fileFlushRO(self):
        """Flushing a read-only file."""

        self._reopen('r')

        try:
            self.h5file.flush()
        except FileModeError:
            self.fail("could not flush a read-only file")

    def test04_fileCreateNodeClosed(self):
        """Test creating a node in a closed file."""

        self.h5file.close()
        self.assertRaises(ClosedFileError,
                          self.h5file.create_group, '/', 'test')

    def test05_fileCreateNodeRO(self):
        """Test creating a node in a read-only file."""

        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.create_group, '/', 'test')

    def test06_fileRemoveNodeClosed(self):
        """Test removing a node from a closed file."""

        self.h5file.create_group('/', 'test')
        self.h5file.close()
        self.assertRaises(ClosedFileError,
                          self.h5file.remove_node, '/', 'test')

    def test07_fileRemoveNodeRO(self):
        """Test removing a node from a read-only file."""

        self.h5file.create_group('/', 'test')
        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.remove_node, '/', 'test')

    def test08_fileMoveNodeClosed(self):
        """Test moving a node in a closed file."""

        self.h5file.create_group('/', 'test1')
        self.h5file.create_group('/', 'test2')
        self.h5file.close()
        self.assertRaises(ClosedFileError,
                          self.h5file.move_node, '/test1', '/', 'test2')

    def test09_fileMoveNodeRO(self):
        """Test moving a node in a read-only file."""

        self.h5file.create_group('/', 'test1')
        self.h5file.create_group('/', 'test2')
        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.move_node, '/test1', '/', 'test2')

    def test10_fileCopyNodeClosed(self):
        """Test copying a node in a closed file."""

        self.h5file.create_group('/', 'test1')
        self.h5file.create_group('/', 'test2')
        self.h5file.close()
        self.assertRaises(ClosedFileError,
                          self.h5file.copy_node, '/test1', '/', 'test2')

    def test11_fileCopyNodeRO(self):
        """Test copying a node in a read-only file."""

        self.h5file.create_group('/', 'test1')
        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.copy_node, '/test1', '/', 'test2')

    def test13_fileGetNodeClosed(self):
        """Test getting a node from a closed file."""

        self.h5file.create_group('/', 'test')
        self.h5file.close()
        self.assertRaises(ClosedFileError, self.h5file.get_node, '/test')

    def test14_fileWalkNodesClosed(self):
        """Test walking a closed file."""

        self.h5file.create_group('/', 'test1')
        self.h5file.create_group('/', 'test2')
        self.h5file.close()
        self.assertRaises(ClosedFileError, self.h5file.walk_nodes().next)

    def test15_fileAttrClosed(self):
        """Test setting and deleting a node attribute in a closed file."""

        self.h5file.create_group('/', 'test')
        self.h5file.close()
        self.assertRaises(ClosedFileError,
                          self.h5file.set_node_attr, '/test', 'foo', 'bar')
        self.assertRaises(ClosedFileError,
                          self.h5file.del_node_attr, '/test', 'foo')

    def test16_fileAttrRO(self):
        """Test setting and deleting a node attribute in a read-only file."""

        self.h5file.create_group('/', 'test')
        self.h5file.set_node_attr('/test', 'foo', 'foo')
        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.set_node_attr, '/test', 'foo', 'bar')
        self.assertRaises(FileModeError,
                          self.h5file.del_node_attr, '/test', 'foo')

    def test17_fileUndoClosed(self):
        """Test undo operations in a closed file."""

        self.h5file.enable_undo()
        self.h5file.create_group('/', 'test2')
        self.h5file.close()
        self.assertRaises(ClosedFileError, self.h5file.is_undo_enabled)
        self.assertRaises(ClosedFileError, self.h5file.get_current_mark)
        self.assertRaises(ClosedFileError, self.h5file.undo)
        self.assertRaises(ClosedFileError, self.h5file.disable_undo)

    def test18_fileUndoRO(self):
        """Test undo operations in a read-only file."""

        self.h5file.enable_undo()
        self.h5file.create_group('/', 'test')
        self._reopen('r')
        self.assertEqual(self.h5file._undoEnabled, False)
        # self.assertRaises(FileModeError, self.h5file.undo)
        # self.assertRaises(FileModeError, self.h5file.disable_undo)

    def test19a_getNode(self):
        """Test getting a child of a closed node."""

        g1 = self.h5file.create_group('/', 'g1')
        g2 = self.h5file.create_group('/g1', 'g2')

        # Close this *object* so that it should not be used.
        g1._f_close()
        self.assertRaises(ClosedNodeError, g1._f_get_child, 'g2')

        # Getting a node by its closed object is not allowed.
        self.assertRaises(ClosedNodeError,
                          self.h5file.get_node, g1)

        # Going through that *node* should reopen it automatically.
        try:
            g2_ = self.h5file.get_node('/g1/g2')
        except ClosedNodeError:
            self.fail("closed parent group has not been reopened")

        # Already open nodes should be closed now, but not the new ones.
        self.assertTrue(g2._v_isopen is False,
                        "open child of closed group has not been closed")
        self.assertTrue(g2_._v_isopen is True,
                        "open child of closed group has not been closed")

        # And existing closed ones should remain closed, but not the new ones.
        g1_ = self.h5file.get_node('/g1')
        self.assertTrue(g1._v_isopen is False,
                        "already closed group is not closed anymore")
        self.assertTrue(g1_._v_isopen is True,
                        "newly opened group is still closed")

    def test19b_getNode(self):
        """Test getting a node that does not start with a slash ('/')."""

        # Create an array in the root
        self.h5file.create_array('/', 'array', [1, 2], title="Title example")

        # Get the array without specifying a leading slash
        self.assertRaises(NameError, self.h5file.get_node, "array")

    def test20_removeNode(self):
        """Test removing a closed node."""

        # This test is a little redundant once we know that ``File.get_node()``
        # will reload a closed node, but anyway...

        group = self.h5file.create_group('/', 'group')
        array = self.h5file.create_array('/group', 'array', [1])

        # The closed *object* can not be used.
        group._f_close()
        self.assertRaises(ClosedNodeError, group._f_remove)
        self.assertRaises(ClosedNodeError, self.h5file.remove_node, group)

        # Still, the *node* is reloaded when necessary.
        try:
            self.h5file.remove_node('/group', recursive=True)
        except ClosedNodeError:
            self.fail("closed node has not been reloaded")

        # Objects of descendent removed nodes
        # should have been automatically closed when removed.
        self.assertRaises(ClosedNodeError, array._f_remove)

        self.assertTrue('/group/array' not in self.h5file)  # just in case
        self.assertTrue('/group' not in self.h5file)  # just in case

    def test21_attrsOfNode(self):
        """Test manipulating the attributes of a closed node."""

        node = self.h5file.create_group('/', 'test')
        nodeAttrs = node._v_attrs

        nodeAttrs.test = attr = 'foo'

        node._f_close()
        self.assertRaises(ClosedNodeError, getattr, node, '_v_attrs')
        # The design of ``AttributeSet`` does not yet allow this test.
        ## self.assertRaises(ClosedNodeError, getattr, nodeAttrs, 'test')

        self.assertEqual(self.h5file.get_node_attr('/test', 'test'), attr)

    def test21b_attrsOfNode(self):
        """Test manipulating the attributes of a node in a read-only file."""

        self.h5file.create_group('/', 'test')
        self.h5file.set_node_attr('/test', 'test', 'foo')

        self._reopen('r')
        self.assertRaises(FileModeError,
                          self.h5file.set_node_attr, '/test', 'test', 'bar')

    def test22_fileClosesNode(self):
        """Test node closing because of file closing."""

        node = self.h5file.create_group('/', 'test')

        self.h5file.close()
        self.assertRaises(ClosedNodeError, getattr, node, '_v_attrs')

    def test23_reopenFile(self):
        """Testing reopening a file and closing it several times."""

        self.h5file.create_array('/', 'test', [1, 2, 3])
        self.h5file.close()

        file1 = open_file(self.h5fname, "r")
        self.assertEqual(file1.open_count, 1)
        if tables.file._FILE_OPEN_POLICY == 'strict':
            self.assertRaises(ValueError, tables.open_file, self.h5fname, "r")
            file1.close()
        else:
            file2 = open_file(self.h5fname, "r")
            self.assertEqual(file1.open_count, 1)
            self.assertEqual(file2.open_count, 1)
            if common.verbose:
                print("(file1) open_count:", file1.open_count)
                print("(file1) test[1]:", file1.root.test[1])
            self.assertEqual(file1.root.test[1], 2)
            file1.close()
            self.assertEqual(file2.open_count, 1)
            if common.verbose:
                print("(file2) open_count:", file2.open_count)
                print("(file2) test[1]:", file2.root.test[1])
            self.assertEqual(file2.root.test[1], 2)
            file2.close()


class FlavorTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test that setting, getting and changing the ``flavor`` attribute of a
    leaf works as expected."""

    array_data = numpy.arange(10)
    scalar_data = numpy.int32(10)

    def _reopen(self, mode='r'):
        super(FlavorTestCase, self)._reopen(mode)
        self.array = self.h5file.get_node('/array')
        self.scalar = self.h5file.get_node('/scalar')
        return True

    def setUp(self):
        super(FlavorTestCase, self).setUp()
        self.array = self.h5file.create_array('/', 'array', self.array_data)
        self.scalar = self.h5file.create_array('/', 'scalar', self.scalar_data)

    def tearDown(self):
        self.array = None
        super(FlavorTestCase, self).tearDown()

    def test00_invalid(self):
        """Setting an invalid flavor."""
        self.assertRaises(FlavorError, setattr, self.array, 'flavor', 'foo')

    def test01_readonly(self):
        """Setting a flavor in a read-only file."""
        self._reopen(mode='r')
        self.assertRaises(FileModeError,
                          setattr, self.array, 'flavor',
                          tables.flavor.internal_flavor)

    def test02_change(self):
        """Changing the flavor and reading data."""
        for flavor in all_flavors:
            self.array.flavor = flavor
            self.assertEqual(self.array.flavor, flavor)
            idata = array_of_flavor(self.array_data, flavor)
            odata = self.array[:]
            self.assertTrue(common.allequal(odata, idata, flavor))

    def test03_store(self):
        """Storing a changed flavor."""
        for flavor in all_flavors:
            self.array.flavor = flavor
            self.assertEqual(self.array.flavor, flavor)
            self._reopen(mode='r+')
            self.assertEqual(self.array.flavor, flavor)

    def test04_missing(self):
        """Reading a dataset of a missing flavor."""
        flavor = self.array.flavor  # default is internal
        self.array._v_attrs.FLAVOR = 'foobar'  # breaks flavor
        self._reopen(mode='r')
        idata = array_of_flavor(self.array_data, flavor)
        odata = self.assertWarns(FlavorWarning, self.array.read)
        self.assertTrue(common.allequal(odata, idata, flavor))

    def test05_delete(self):
        """Deleting the flavor of a dataset."""
        self.array.flavor = 'python'  # non-default
        self.assertEqual(self.array.flavor, 'python')
        self.assertEqual(self.array.attrs.FLAVOR, 'python')
        del self.array.flavor
        self.assertEqual(self.array.flavor, tables.flavor.internal_flavor)
        self.assertRaises(AttributeError, getattr, self.array.attrs, 'FLAVOR')

    def test06_copyDeleted(self):
        """Copying a node with a deleted flavor (see #100)."""
        snames = [node._v_name for node in [self.array, self.scalar]]
        dnames = ['%s_copy' % name for name in snames]
        for name in snames:
            node = self.h5file.get_node('/', name)
            del node.flavor
        # Check the copied flavors right after copying and after reopening.
        for fmode in ['r+', 'r']:
            self._reopen(fmode)
            for sname, dname in zip(snames, dnames):
                if fmode == 'r+':
                    snode = self.h5file.get_node('/', sname)
                    node = snode.copy('/', dname)
                elif fmode == 'r':
                    node = self.h5file.get_node('/', dname)
                self.assertEqual(node.flavor, tables.flavor.internal_flavor,
                                 "flavor of node ``%s`` is not internal: %r"
                                 % (node._v_pathname, node.flavor))

    def test07_restrict_flavors(self):
        # regression test for gh-163

        all_flavors = list(tables.flavor.all_flavors)
        alias_map = tables.flavor.alias_map.copy()
        converter_map = tables.flavor.converter_map.copy()
        identifier_map = tables.flavor.identifier_map.copy()
        description_map = tables.flavor.description_map.copy()

        try:
            tables.flavor.restrict_flavors(keep=[])
            self.assertTrue(len(tables.flavor.alias_map) < len(alias_map))
            self.assertTrue(len(
                tables.flavor.converter_map) < len(converter_map))
        finally:
            tables.flavor.all_flavors[:] = all_flavors[:]
            tables.flavor.alias_map.update(alias_map)
            tables.flavor.converter_map.update(converter_map)
            tables.flavor.identifier_map.update(identifier_map)
            tables.flavor.description_map.update(description_map)


class UnicodeFilename(common.PyTablesTestCase):
    unicode_prefix = u'para\u0140lel'

    def setUp(self):
        self.h5fname = tempfile.mktemp(prefix=self.unicode_prefix,
                                       suffix=".h5")
        self.h5file = tables.open_file(self.h5fname, "w")
        self.test = self.h5file.create_array('/', 'test', [1, 2])
        # So as to check the reading
        self.h5file.close()
        self.h5file = tables.open_file(self.h5fname, "r")

    def tearDown(self):
        # Remove the temporary file
        os.remove(self.h5fname)

    def test01(self):
        """Checking creating a filename with Unicode chars."""

        test = self.h5file.root.test
        if common.verbose:
            print("Filename:", self.h5fname)
            print("Array:", test[:])
            print("Should look like:", [1, 2])
        self.assertEqual(test[:], [1, 2], "Values does not match.")

    def test02(self):
        """Checking is_hdf5_file with a Unicode filename."""

        self.h5file.close()
        if common.verbose:
            print("Filename:", self.h5fname)
            print("is_hdf5_file?:", tables.is_hdf5_file(self.h5fname))
        self.assertTrue(tables.is_hdf5_file(self.h5fname))

    def test03(self):
        """Checking is_pytables_file with a Unicode filename."""

        self.h5file.close()
        if common.verbose:
            print("Filename:", self.h5fname)
            print("is_pytables_file?:", tables.is_pytables_file(self.h5fname))
        self.assertNotEqual(tables.is_pytables_file(self.h5fname), False)


class FilePropertyTestCase(common.PyTablesTestCase):
    def setUp(self):
        self.h5fname = tempfile.mktemp(".h5")
        self.h5file = None

    def tearDown(self):
        if self.h5file:
            self.h5file.close()

        if os.path.exists(self.h5fname):
            os.remove(self.h5fname)

    def test_get_filesize(self):
        data = numpy.zeros((2000, 2000))
        datasize = numpy.prod(data.shape) * data.dtype.itemsize

        self.h5file = open_file(self.h5fname, mode="w")
        self.h5file.create_array(self.h5file.root, 'array', data)
        h5_filesize = self.h5file.get_filesize()
        self.h5file.close()

        fs_filesize = os.stat(self.h5fname)[6]

        self.assertTrue(h5_filesize >= datasize)
        self.assertEqual(h5_filesize, fs_filesize)

    def test01_null_userblock_size(self):
        self.h5file = open_file(self.h5fname, mode="w")
        self.h5file.create_array(self.h5file.root, 'array', [1, 2])
        self.assertEqual(self.h5file.get_userblock_size(), 0)

    def test02_null_userblock_size(self):
        self.h5file = open_file(self.h5fname, mode="w")
        self.h5file.create_array(self.h5file.root, 'array', [1, 2])
        self.h5file.close()
        self.h5file = open_file(self.h5fname, mode="r")
        self.assertEqual(self.h5file.get_userblock_size(), 0)

    def test03_null_userblock_size(self):
        USER_BLOCK_SIZE = 0
        self.h5file = open_file(self.h5fname, mode="w",
                                user_block_size=USER_BLOCK_SIZE)
        self.h5file.create_array(self.h5file.root, 'array', [1, 2])
        self.assertEqual(self.h5file.get_userblock_size(), 0)

    def test01_userblock_size(self):
        USER_BLOCK_SIZE = 512
        self.h5file = open_file(self.h5fname, mode="w",
                                user_block_size=USER_BLOCK_SIZE)
        self.h5file.create_array(self.h5file.root, 'array', [1, 2])
        self.assertEqual(self.h5file.get_userblock_size(), USER_BLOCK_SIZE)

    def test02_userblock_size(self):
        USER_BLOCK_SIZE = 512
        self.h5file = open_file(self.h5fname, mode="w",
                                user_block_size=USER_BLOCK_SIZE)
        self.h5file.create_array(self.h5file.root, 'array', [1, 2])
        self.h5file.close()
        self.h5file = open_file(self.h5fname, mode="r")
        self.assertEqual(self.h5file.get_userblock_size(), USER_BLOCK_SIZE)

    def test_small_userblock_size(self):
        USER_BLOCK_SIZE = 12
        self.assertRaises(ValueError, open_file, self.h5fname, mode="w",
                          user_block_size=USER_BLOCK_SIZE)

    def test_invalid_userblock_size(self):
        USER_BLOCK_SIZE = 1025
        self.assertRaises(ValueError, open_file, self.h5fname, mode="w",
                          user_block_size=USER_BLOCK_SIZE)


# Test for reading a file that uses Blosc and created on a big-endian platform
class BloscBigEndian(common.PyTablesTestCase):

    def setUp(self):
        filename = self._testFilename("blosc_bigendian.h5")
        self.fileh = open_file(filename, "r")

    def tearDown(self):
        self.fileh.close()

    def test00_bigendian(self):
        """Checking compatibility with Blosc on big-endian machines."""

        # Check that we can read the contents without problems (nor warnings!)
        for dset_name in ('i1', 'i2', 'i4', 'i8'):
            a = numpy.arange(10, dtype=dset_name)
            dset = self.fileh.get_node('/'+dset_name)
            self.assertTrue(common.allequal(a, dset[:]),
                            "Error in big-endian data!")


# Case test for Blosc and subprocesses (via multiprocessing module)

# The worker function for the subprocess (needs to be here because Windows
# has problems pickling nested functions with the multiprocess module :-/)
def _worker(fn, qout=None):
    fp = tables.open_file(fn)
    if common.verbose:
        print("About to load: ", fn)
    rows = fp.root.table.where('(f0 < 10)')
    if common.verbose:
        print("Got the iterator, about to iterate")
    next(rows)
    if common.verbose:
        print("Succeeded in one iteration\n")
    fp.close()

    if qout is not None:
        qout.put("Done")


class BloscSubprocess(common.PyTablesTestCase):
    def test_multiprocess(self):
        # From: Yaroslav Halchenko <debian@onerussian.com>
        # Subject: Skip the unittest on kFreeBSD and Hurd -- locking seems to
        #         be N/A
        #
        #  on kfreebsd /dev/shm is N/A
        #  on Hurd -- inter-process semaphore locking is N/A
        import platform

        if platform.system().lower() in ('gnu', 'gnu/kfreebsd'):
            raise common.SkipTest("multiprocessing module is not supported "
                                  "on Hurd/kFreeBSD")

        # Create a relatively large table with Blosc level 9 (large blocks)
        fn = tempfile.mktemp(prefix="multiproc-blosc9-", suffix=".h5")
        size = int(3e5)
        sa = numpy.fromiter(((i, i**2, i//3)
                             for i in xrange(size)), 'i4,i8,f8')
        fp = open_file(fn, 'w')
        fp.create_table(fp.root, 'table', sa,
                        filters=Filters(complevel=9, complib="blosc"),
                        chunkshape=(size // 3,))
        fp.close()

        if common.verbose:
            print("**** Running from main process:")
        _worker(fn)

        if common.verbose:
            print("**** Running from subprocess:")

        try:
            qout = mp.Queue()
        except OSError:
            print("Permission denied due to /dev/shm settings")
        else:
            ps = mp.Process(target=_worker, args=(fn, qout,))
            ps.daemon = True
            ps.start()

            result = qout.get()
            if common.verbose:
                print(result)

        os.remove(fn)


class HDF5ErrorHandling(common.PyTablesTestCase):

    def setUp(self):
        self._old_policy = tables.HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY

    def tearDown(self):
        tables.HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY = self._old_policy

    def test_silence_messages(self):
        code = """
import tables
tables.silence_hdf5_messages(False)
tables.silence_hdf5_messages()
try:
    tables.open_file(r'%s')
except tables.HDF5ExtError, e:
    pass
"""

        fn = tempfile.mktemp(prefix="hdf5-error-handling-", suffix=".py")
        fp = open(fn, 'w')
        try:
            fp.write(code % fn)
            fp.close()

            p = subprocess.Popen([sys.executable, fn],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            (stdout, stderr) = p.communicate()

            self.assertFalse("HDF5-DIAG" in stderr.decode('ascii'))
        finally:
            os.remove(fn)

    def test_enable_messages(self):
        code = """
import tables
tables.silence_hdf5_messages()
tables.silence_hdf5_messages(False)
try:
    tables.open_file(r'%s')
except tables.HDF5ExtError as e:
    pass
"""

        fn = tempfile.mktemp(prefix="hdf5-error-handling-", suffix=".py")
        fp = open(fn, 'w')
        try:
            fp.write(code % fn)
            fp.close()

            p = subprocess.Popen([sys.executable, fn],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
            (stdout, stderr) = p.communicate()

            self.assertTrue("HDF5-DIAG" in stderr.decode('ascii'))
        finally:
            os.remove(fn)

    def _raise_exterror(self):
        filename = tempfile.mktemp(".h5")
        open(filename, 'wb').close()

        try:
            f = tables.open_file(filename)
            f.close()
        finally:
            os.remove(filename)

    def test_h5_backtrace_quiet(self):
        tables.HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY = True

        try:
            self._raise_exterror()
        except tables.HDF5ExtError as e:
            self.assertFalse(e.h5backtrace is None)
        else:
            self.fail("HDF5ExtError exception not raised")

    def test_h5_backtrace_verbose(self):
        tables.HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY = "VERBOSE"

        try:
            self._raise_exterror()
        except tables.HDF5ExtError as e:
            self.assertFalse(e.h5backtrace is None)
            msg = str(e)
            self.assertTrue(e.h5backtrace[-1][-1] in msg)
        else:
            self.fail("HDF5ExtError exception not raised")

    def test_h5_backtrace_ignore(self):
        tables.HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY = False

        try:
            self._raise_exterror()
        except tables.HDF5ExtError as e:
            self.assertTrue(e.h5backtrace is None)
        else:
            self.fail("HDF5ExtError exception not raised")


class TestDescription(common.PyTablesTestCase):
    def test_isdescription_inheritance(self):
        # Regression test for gh-65
        class TestDescParent(IsDescription):
            c = Int32Col()

        class TestDesc(TestDescParent):
            pass

        self.assertTrue('c' in TestDesc.columns)

    def test_descr_from_dtype(self):
        t = numpy.dtype([('col1', 'int16'), ('col2', float)])
        descr, byteorder = descr_from_dtype(t)

        self.assertTrue('col1' in descr._v_colobjects)
        self.assertTrue('col2' in descr._v_colobjects)
        self.assertEqual(len(descr._v_colobjects), 2)
        self.assertTrue(isinstance(descr._v_colobjects['col1'], Col))
        self.assertTrue(isinstance(descr._v_colobjects['col2'], Col))
        self.assertEqual(descr._v_colobjects['col1'].dtype, numpy.int16)
        self.assertEqual(descr._v_colobjects['col2'].dtype, float)

    def test_descr_from_dtype_rich_dtype(self):
        header = [(('timestamp', 't'), 'u4'),
                  (('unit (cluster) id', 'unit'), 'u2')]
        t = numpy.dtype(header)

        descr, byteorder = descr_from_dtype(t)
        self.assertEqual(len(descr._v_names), 2)
        self.assertEqual(sorted(descr._v_names), ['t', 'unit'])

    def test_dtype_from_descr_is_description(self):
        # See gh-152
        class TestDescParent(IsDescription):
            col1 = Int16Col()
            col2 = FloatCol()

        dtype = numpy.dtype([('col1', 'int16'), ('col2', float)])
        t = dtype_from_descr(TestDescParent)

        self.assertEqual(t, dtype)

    def test_dtype_from_descr_is_description_instance(self):
        # See gh-152
        class TestDescParent(IsDescription):
            col1 = Int16Col()
            col2 = FloatCol()

        dtype = numpy.dtype([('col1', 'int16'), ('col2', float)])
        t = dtype_from_descr(TestDescParent())

        self.assertEqual(t, dtype)

    def test_dtype_from_descr_description_instance(self):
        # See gh-152
        class TestDescParent(IsDescription):
            col1 = Int16Col()
            col2 = FloatCol()

        dtype = numpy.dtype([('col1', 'int16'), ('col2', float)])
        desctiption = Description(TestDescParent().columns)
        t = dtype_from_descr(desctiption)

        self.assertEqual(t, dtype)

    def test_dtype_from_descr_dict(self):
        # See gh-152
        dtype = numpy.dtype([('col1', 'int16'), ('col2', float)])
        t = dtype_from_descr({'col1': Int16Col(), 'col2': FloatCol()})

        self.assertEqual(t, dtype)

    def test_dtype_from_descr_invalid_type(self):
        # See gh-152
        self.assertRaises(ValueError, dtype_from_descr, [])

    def test_dtype_from_descr_byteorder(self):
        # See gh-152
        class TestDescParent(IsDescription):
            col1 = Int16Col()
            col2 = FloatCol()

        t = dtype_from_descr(TestDescParent, byteorder='>')

        self.assertEqual(t['col1'].byteorder, '>')
        self.assertEqual(t['col2'].byteorder, '>')

    def test_str_names(self):
        # see gh-42
        d = {'name': tables.Int16Col()}
        descr = Description(d)
        self.assertEqual(sorted(descr._v_names), sorted(d.keys()))
        self.assertTrue(isinstance(descr._v_dtype, numpy.dtype))
        self.assertTrue(sorted(descr._v_dtype.fields.keys()),
                        sorted(d.keys()))

    if sys.version_info[0] < 3:
        def test_unicode_names(self):
            # see gh-42
            # the name used is a valid ASCII identifier passed as unicode
            # string
            d = {unicode('name'): tables.Int16Col()}
            descr = Description(d)
            self.assertEqual(sorted(descr._v_names), sorted(d.keys()))
            self.assertTrue(isinstance(descr._v_dtype, numpy.dtype))
            keys = []
            for key in d.keys():
                if isinstance(key, unicode):
                    keys.append(key.encode())
                else:
                    keys.append(key)
            self.assertTrue(sorted(descr._v_dtype.fields.keys()), sorted(keys))


class TestAtom(common.PyTablesTestCase):
    def test_atom_attributes01(self):
        shape = (10, 10)
        a = Float64Atom(shape=shape)

        self.assertEqual(a.dflt, 0.)
        self.assertEqual(a.dtype, numpy.dtype((numpy.float64, shape)))
        self.assertEqual(a.itemsize, a.dtype.base.itemsize)
        self.assertEqual(a.kind, 'float')
        self.assertEqual(a.ndim, len(shape))
        # self.assertEqual(a.recarrtype, )
        self.assertEqual(a.shape, shape)
        self.assertEqual(a.size, a.itemsize * numpy.prod(shape))
        self.assertEqual(a.type, 'float64')

    def test_atom_copy01(self):
        shape = (10, 10)
        a = Float64Atom(shape=shape)
        aa = a.copy()
        self.assertEqual(aa.shape, shape)

    def test_atom_copy02(self):
        dflt = 2.0
        a = Float64Atom(dflt=dflt)
        aa = a.copy()
        self.assertEqual(aa.dflt, dflt)

    def test_atom_copy_override(self):
        shape = (10, 10)
        dflt = 2.0
        a = Float64Atom(shape=shape, dflt=dflt)
        aa = a.copy(dflt=-dflt)
        self.assertEqual(aa.shape, shape)
        self.assertNotEqual(aa.dflt, dflt)
        self.assertEqual(aa.dflt, -dflt)


class TestCol(common.PyTablesTestCase):
    def test_col_copy01(self):
        shape = (10, 10)
        c = Float64Col(shape=shape)
        cc = c.copy()
        self.assertEqual(cc.shape, shape)

    def test_col_copy02(self):
        dflt = 2.0
        c = Float64Col(dflt=dflt)
        cc = c.copy()
        self.assertEqual(cc.dflt, dflt)

    def test_col_copy_override(self):
        shape = (10, 10)
        dflt = 2.0
        pos = 3
        c = Float64Col(shape=shape, dflt=dflt, pos=pos)
        cc = c.copy(pos=2)
        self.assertEqual(cc.shape, shape)
        self.assertEqual(cc.dflt, dflt)
        self.assertNotEqual(cc._v_pos, pos)
        self.assertEqual(cc._v_pos, 2)


class TestSysattrCompatibility(common.PyTablesTestCase):

    def test_open_python2(self):
        filename = self._testFilename("python2.h5")
        fileh = open_file(filename, "r")
        self.assertTrue(fileh.isopen)
        fileh.close()

    def test_open_python3(self):
        filename = self._testFilename("python2.h5")
        fileh = open_file(filename, "r")
        self.assertTrue(fileh.isopen)
        fileh.close()


#----------------------------------------------------------------------

def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    blosc_avail = which_lib_version("blosc") is not None

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(OpenFileFailureTestCase))
        theSuite.addTest(unittest.makeSuite(NodeCacheOpenFile))
        theSuite.addTest(unittest.makeSuite(NoNodeCacheOpenFile))
        theSuite.addTest(unittest.makeSuite(DictNodeCacheOpenFile))
        theSuite.addTest(unittest.makeSuite(CheckFileTestCase))
        if tables.file._FILE_OPEN_POLICY != 'strict':
            theSuite.addTest(unittest.makeSuite(ThreadingTestCase))
        theSuite.addTest(unittest.makeSuite(PythonAttrsTestCase))
        theSuite.addTest(unittest.makeSuite(StateTestCase))
        theSuite.addTest(unittest.makeSuite(FlavorTestCase))
        theSuite.addTest(unittest.makeSuite(FilePropertyTestCase))
        if blosc_avail:
            theSuite.addTest(unittest.makeSuite(BloscBigEndian))
        if multiprocessing_imported:
            theSuite.addTest(unittest.makeSuite(BloscSubprocess))
        theSuite.addTest(unittest.makeSuite(HDF5ErrorHandling))
        theSuite.addTest(unittest.makeSuite(TestDescription))
        theSuite.addTest(unittest.makeSuite(TestAtom))
        theSuite.addTest(unittest.makeSuite(TestCol))
        theSuite.addTest(unittest.makeSuite(TestSysattrCompatibility))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

## Local Variables:
## mode: python
## End:

########NEW FILE########
__FILENAME__ = test_carray
# -*- coding: utf-8 -*-

from __future__ import print_function
import unittest
import os
import tempfile

import numpy

import tables
from tables import *
from tables.tests import common
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class BasicTestCase(unittest.TestCase):
    # Default values
    obj = None
    flavor = "numpy"
    type = 'int32'
    shape = (2, 2)
    start = 0
    stop = 10
    step = 1
    length = 1
    chunkshape = (5, 5)
    compress = 0
    complib = "zlib"  # Default compression library
    shuffle = 0
    fletcher32 = 0
    reopen = 1  # Tells whether the file has to be reopened on each test or not

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()
        if self.reopen:
            # Close the file
            self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        obj = self.obj
        if obj is None:
            if self.type == "string":
                atom = StringAtom(itemsize=self.length)
            else:
                atom = Atom.from_type(self.type)
        else:
            atom = None
        title = self.__class__.__name__
        filters = Filters(complevel=self.compress,
                          complib=self.complib,
                          shuffle=self.shuffle,
                          fletcher32=self.fletcher32)
        carray = self.fileh.create_carray(group, 'carray1',
                                          atom=atom, shape=self.shape,
                                          title=title, filters=filters,
                                          chunkshape=self.chunkshape, obj=obj)
        carray.flavor = self.flavor

        # Fill it with data
        self.rowshape = list(carray.shape)
        self.objsize = self.length * numpy.prod(carray.shape)

        if self.flavor == "numpy":
            if self.type == "string":
                object = numpy.ndarray(buffer=b"a"*self.objsize,
                                       shape=self.shape,
                                       dtype="S%s" % carray.atom.itemsize)
            else:
                object = numpy.arange(self.objsize, dtype=carray.atom.dtype)
                object.shape = carray.shape
        if common.verbose:
            print("Object to append -->", repr(object))

        carray[...] = object

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def _get_shape(self):
        if self.shape is not None:
            shape = self.shape
        else:
            shape = numpy.asarray(self.obj).shape

        return shape

    def test00_attributes(self):
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        obj = self.fileh.get_node("/carray1")

        shape = self._get_shape()

        self.assertEqual(obj.flavor, self.flavor)
        self.assertEqual(obj.shape, shape)
        self.assertEqual(obj.ndim, len(shape))
        self.assertEqual(obj.chunkshape, self.chunkshape)
        self.assertEqual(obj.nrows, shape[0])
        self.assertEqual(obj.atom.type, self.type)

    def test01_readCArray(self):
        """Checking read() of chunked layout arrays."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readCArray..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        carray = self.fileh.get_node("/carray1")

        # Choose a small value for buffer size
        carray.nrowsinbuf = 3
        if common.verbose:
            print("CArray descr:", repr(carray))
            print("shape of read array ==>", carray.shape)
            print("reopening?:", self.reopen)

        shape = self._get_shape()

        # Build the array to do comparisons
        if self.flavor == "numpy":
            if self.type == "string":
                object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                        shape=self.shape,
                                        dtype="S%s" % carray.atom.itemsize)
            else:
                object_ = numpy.arange(self.objsize, dtype=carray.atom.dtype)
                object_.shape = shape

        stop = self.stop
        # stop == None means read only the element designed by start
        # (in read() contexts)
        if self.stop is None:
            if self.start == -1:  # corner case
                stop = carray.nrows
            else:
                stop = self.start + 1
        # Protection against number of elements less than existing
        # if rowshape[self.extdim] < self.stop or self.stop == 0:
        if carray.nrows < stop:
            # self.stop == 0 means last row only in read()
            # and not in [::] slicing notation
            stop = int(carray.nrows)
        # do a copy() in order to ensure that len(object._data)
        # actually do a measure of its length
        object = object_[self.start:stop:self.step].copy()

        # Read all the array
        try:
            data = carray.read(self.start, stop, self.step)
        except IndexError:
            if self.flavor == "numpy":
                data = numpy.empty(shape=self.shape, dtype=self.type)
            else:
                data = numpy.empty(shape=self.shape, dtype=self.type)

        if common.verbose:
            if hasattr(object, "shape"):
                print("shape should look as:", object.shape)
            print("Object read ==>", repr(data))
            print("Should look like ==>", repr(object))

        if hasattr(data, "shape"):
            self.assertEqual(len(data.shape), len(shape))
        else:
            # Scalar case
            self.assertEqual(len(self.shape), 1)
        self.assertEqual(carray.chunkshape, self.chunkshape)
        self.assertTrue(allequal(data, object, self.flavor))

    def test01_readCArray_out_argument(self):
        """Checking read() of chunked layout arrays."""

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        carray = self.fileh.get_node("/carray1")

        shape = self._get_shape()

        # Choose a small value for buffer size
        carray.nrowsinbuf = 3
        # Build the array to do comparisons
        if self.flavor == "numpy":
            if self.type == "string":
                object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                        shape=self.shape,
                                        dtype="S%s" % carray.atom.itemsize)
            else:
                object_ = numpy.arange(self.objsize, dtype=carray.atom.dtype)
                object_.shape = shape

        stop = self.stop
        # stop == None means read only the element designed by start
        # (in read() contexts)
        if self.stop is None:
            if self.start == -1:  # corner case
                stop = carray.nrows
            else:
                stop = self.start + 1
        # Protection against number of elements less than existing
        # if rowshape[self.extdim] < self.stop or self.stop == 0:
        if carray.nrows < stop:
            # self.stop == 0 means last row only in read()
            # and not in [::] slicing notation
            stop = int(carray.nrows)
        # do a copy() in order to ensure that len(object._data)
        # actually do a measure of its length
        object = object_[self.start:stop:self.step].copy()

        # Read all the array
        try:
            data = numpy.empty(shape, dtype=carray.atom.dtype)
            data = data[self.start:stop:self.step].copy()
            carray.read(self.start, stop, self.step, out=data)
        except IndexError:
            if self.flavor == "numpy":
                data = numpy.empty(shape=shape, dtype=self.type)
            else:
                data = numpy.empty(shape=shape, dtype=self.type)

        if hasattr(data, "shape"):
            self.assertEqual(len(data.shape), len(shape))
        else:
            # Scalar case
            self.assertEqual(len(shape), 1)
        self.assertEqual(carray.chunkshape, self.chunkshape)
        self.assertTrue(allequal(data, object, self.flavor))

    def test02_getitemCArray(self):
        """Checking chunked layout array __getitem__ special method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_getitemCArray..." %
                  self.__class__.__name__)

        if not hasattr(self, "slices"):
            # If there is not a slices attribute, create it
            self.slices = (slice(self.start, self.stop, self.step),)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        carray = self.fileh.get_node("/carray1")

        if common.verbose:
            print("CArray descr:", repr(carray))
            print("shape of read array ==>", carray.shape)
            print("reopening?:", self.reopen)

        shape = self._get_shape()

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.shape,
                                    dtype="S%s" % carray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=carray.atom.dtype)
            object_.shape = shape

        # do a copy() in order to ensure that len(object._data)
        # actually do a measure of its length
        object = object_.__getitem__(self.slices).copy()

        # Read data from the array
        try:
            data = carray.__getitem__(self.slices)
        except IndexError:
            print("IndexError!")
            if self.flavor == "numpy":
                data = numpy.empty(shape=self.shape, dtype=self.type)
            else:
                data = numpy.empty(shape=self.shape, dtype=self.type)

        if common.verbose:
            print("Object read:\n", repr(data))  # , data.info()
            print("Should look like:\n", repr(object))  # , object.info()
            if hasattr(object, "shape"):
                print("Original object shape:", self.shape)
                print("Shape read:", data.shape)
                print("shape should look as:", object.shape)

        if not hasattr(data, "shape"):
            # Scalar case
            self.assertEqual(len(self.shape), 1)
        self.assertEqual(carray.chunkshape, self.chunkshape)
        self.assertTrue(allequal(data, object, self.flavor))

    def test03_setitemCArray(self):
        """Checking chunked layout array __setitem__ special method."""

        if self.__class__.__name__ == "Ellipsis6CArrayTestCase":
            # see test_earray.py BasicTestCase.test03_setitemEArray
            return
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_setitemCArray..." %
                  self.__class__.__name__)

        if not hasattr(self, "slices"):
            # If there is not a slices attribute, create it
            self.slices = (slice(self.start, self.stop, self.step),)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "a")
        carray = self.fileh.get_node("/carray1")

        if common.verbose:
            print("CArray descr:", repr(carray))
            print("shape of read array ==>", carray.shape)
            print("reopening?:", self.reopen)

        shape = self._get_shape()

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.shape,
                                    dtype="S%s" % carray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=carray.atom.dtype)
            object_.shape = shape

        # do a copy() in order to ensure that len(object._data)
        # actually do a measure of its length
        object = object_.__getitem__(self.slices).copy()

        if self.type == "string":
            if hasattr(self, "wslice"):
                object[self.wslize] = "xXx"
                carray[self.wslice] = "xXx"
            elif sum(object[self.slices].shape) != 0:
                object[:] = "xXx"
                if object.size > 0:
                    carray[self.slices] = object
        else:
            if hasattr(self, "wslice"):
                object[self.wslice] = object[self.wslice] * 2 + 3
                carray[self.wslice] = carray[self.wslice] * 2 + 3
            elif sum(object[self.slices].shape) != 0:
                object = object * 2 + 3
                if numpy.prod(object.shape) > 0:
                    carray[self.slices] = carray[self.slices] * 2 + 3
            # Cast again object to its original type
            object = numpy.array(object, dtype=carray.atom.dtype)
        # Read datafrom the array
        try:
            data = carray.__getitem__(self.slices)
        except IndexError:
            print("IndexError!")
            if self.flavor == "numpy":
                data = numpy.empty(shape=self.shape, dtype=self.type)
            else:
                data = numpy.empty(shape=self.shape, dtype=self.type)

        if common.verbose:
            print("Object read:\n", repr(data))  # , data.info()
            print("Should look like:\n", repr(object))  # , object.info()
            if hasattr(object, "shape"):
                print("Original object shape:", self.shape)
                print("Shape read:", data.shape)
                print("shape should look as:", object.shape)

        if not hasattr(data, "shape"):
            # Scalar case
            self.assertEqual(len(self.shape), 1)
        self.assertEqual(carray.chunkshape, self.chunkshape)
        self.assertTrue(allequal(data, object, self.flavor))


class BasicWriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2,)
    chunkshape = (5,)
    step = 1
    wslice = 1  # single element case


class BasicWrite2TestCase(BasicTestCase):
    type = 'int32'
    shape = (2,)
    chunkshape = (5,)
    step = 1
    wslice = slice(shape[0]-2, shape[0], 2)  # range of elements
    reopen = 0  # This case does not reopen files


class BasicWrite3TestCase(BasicTestCase):
    obj = [1, 2]
    type = numpy.asarray(obj).dtype.name
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 0  # This case does not reopen files


class BasicWrite4TestCase(BasicTestCase):
    obj = numpy.array([1, 2])
    type = obj.dtype.name
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 0  # This case does not reopen files


class BasicWrite5TestCase(BasicTestCase):
    obj = [[1, 2], [3, 4]]
    type = numpy.asarray(obj).dtype.name
    shape = None
    chunkshape = (5, 1)
    step = 1
    reopen = 0  # This case does not reopen files


class BasicWrite6TestCase(BasicTestCase):
    obj = [1, 2]
    type = numpy.asarray(obj).dtype.name
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 1  # This case does reopen files


class BasicWrite7TestCase(BasicTestCase):
    obj = numpy.array([1, 2])
    type = obj.dtype.name
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 1  # This case does reopen files


class BasicWrite8TestCase(BasicTestCase):
    obj = [[1, 2], [3, 4]]
    type = numpy.asarray(obj).dtype.name
    shape = None
    chunkshape = (5, 1)
    step = 1
    reopen = 1  # This case does reopen files


class EmptyCArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2)
    chunkshape = (5, 5)
    start = 0
    stop = 10
    step = 1


class EmptyCArray2TestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2)
    chunkshape = (5, 5)
    start = 0
    stop = 10
    step = 1
    reopen = 0  # This case does not reopen files


class SlicesCArrayTestCase(BasicTestCase):
    compress = 1
    complib = "lzo"
    type = 'int32'
    shape = (2, 2)
    chunkshape = (5, 5)
    slices = (slice(1, 2, 1), slice(1, 3, 1))


class EllipsisCArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2)
    chunkshape = (5, 5)
    # slices = (slice(1,2,1), Ellipsis)
    slices = (Ellipsis, slice(1, 2, 1))


class Slices2CArrayTestCase(BasicTestCase):
    compress = 1
    complib = "lzo"
    type = 'int32'
    shape = (2, 2, 4)
    chunkshape = (5, 5, 5)
    slices = (slice(1, 2, 1), slice(None, None, None), slice(1, 4, 2))


class Ellipsis2CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2, 4)
    chunkshape = (5, 5, 5)
    slices = (slice(1, 2, 1), Ellipsis, slice(1, 4, 2))


class Slices3CArrayTestCase(BasicTestCase):
    compress = 1      # To show the chunks id DEBUG is on
    complib = "lzo"
    type = 'int32'
    shape = (2, 3, 4, 2)
    chunkshape = (5, 5, 5, 5)
    slices = (slice(1, 2, 1), slice(
        0, None, None), slice(1, 4, 2))  # Don't work
    # slices = (slice(None, None, None), slice(0, None, None),
    #           slice(1,4,1))  # W
    # slices = (slice(None, None, None), slice(None, None, None),
    #           slice(1,4,2))  # N
    # slices = (slice(1,2,1), slice(None, None, None), slice(1,4,2))  # N
    # Disable the failing test temporarily with a working test case
    slices = (slice(1, 2, 1), slice(1, 4, None), slice(1, 4, 2))  # Y
    # slices = (slice(1,2,1), slice(0, 4, None), slice(1,4,1))  # Y
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(1, 4, 2))  # N
    # slices = (slice(1,2,1), slice(0, 4, None), slice(1,4,2),
    #           slice(0,100,1))  # N


class Slices4CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 2, 5, 6)
    chunkshape = (5, 5, 5, 5, 5, 5)
    slices = (slice(1, 2, 1), slice(0, None, None), slice(1, 4, 2),
              slice(0, 4, 2), slice(3, 5, 2), slice(2, 7, 1))


class Ellipsis3CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 2)
    chunkshape = (5, 5, 5, 5)
    slices = (Ellipsis, slice(0, 4, None), slice(1, 4, 2))
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(1, 4, 2), Ellipsis)


class Ellipsis4CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 5)
    chunkshape = (5, 5, 5, 5)
    slices = (Ellipsis, slice(0, 4, None), slice(1, 4, 2))
    slices = (slice(1, 2, 1), Ellipsis, slice(1, 4, 2))


class Ellipsis5CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 5)
    chunkshape = (5, 5, 5, 5)
    slices = (slice(1, 2, 1), slice(0, 4, None), Ellipsis)


class Ellipsis6CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 5)
    chunkshape = (5, 5, 5, 5)
    # The next slices gives problems with setting values (test03)
    # This is a problem on the test design, not the Array.__setitem__
    # code, though. See # see test_earray.py Ellipsis6EArrayTestCase
    slices = (slice(1, 2, 1), slice(0, 4, None), 2, Ellipsis)


class Ellipsis7CArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 5)
    chunkshape = (5, 5, 5, 5)
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(2, 3), Ellipsis)


class MD3WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2, 3)
    chunkshape = (4, 4, 4)
    step = 2


class MD5WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 2, 3, 4, 5)  # ok
    # shape = (1, 1, 2, 1)  # Minimum shape that shows problems with HDF5 1.6.1
    # shape = (2, 3, 2, 4, 5)  # Floating point exception (HDF5 1.6.1)
    # shape = (2, 3, 3, 2, 5, 6) # Segmentation fault (HDF5 1.6.1)
    chunkshape = (1, 1, 1, 1, 1)
    start = 1
    stop = 10
    step = 10


class MD6WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 3, 2, 5, 6)
    chunkshape = (1, 1, 1, 1, 5, 6)
    start = 1
    stop = 10
    step = 3


class MD6WriteTestCase__(BasicTestCase):
    type = 'int32'
    shape = (2, 2)
    chunkshape = (1, 1)
    start = 1
    stop = 3
    step = 1


class MD7WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 3, 4, 5, 2, 3)
    chunkshape = (10, 10, 10, 10, 10, 10, 10)
    start = 1
    stop = 10
    step = 2


class MD10WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (1, 2, 3, 4, 5, 5, 4, 3, 2, 2)
    chunkshape = (5, 5, 5, 5, 5, 5, 5, 5, 5, 5)
    start = -1
    stop = -1
    step = 10


class ZlibComprTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    start = 3
    # stop = 0   # means last row
    stop = None   # means last row from 0.8 on
    step = 10


class ZlibShuffleTestCase(BasicTestCase):
    shuffle = 1
    compress = 1
    complib = "zlib"
    # case start < stop , i.e. no rows read
    start = 3
    stop = 1
    step = 10


class BloscComprTestCase(BasicTestCase):
    compress = 1  # sss
    complib = "blosc"
    chunkshape = (10, 10)
    start = 3
    stop = 10
    step = 3


class BloscShuffleTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class BloscFletcherTestCase(BasicTestCase):
    # see gh-21
    shape = (200, 300)
    compress = 1
    shuffle = 1
    fletcher32 = 1
    complib = "blosc"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class BloscBloscLZTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc:blosclz"
    chunkshape = (200, 100)
    start = 2
    stop = 11
    step = 7


class BloscLZ4TestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc:lz4"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class BloscLZ4HCTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc:lz4hc"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class BloscSnappyTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc:snappy"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class BloscZlibTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "blosc:zlib"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class LZOComprTestCase(BasicTestCase):
    compress = 1  # sss
    complib = "lzo"
    chunkshape = (10, 10)
    start = 3
    stop = 10
    step = 3


class LZOShuffleTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "lzo"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 7


class Bzip2ComprTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    complib = "bzip2"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 8


class Bzip2ShuffleTestCase(BasicTestCase):
    shape = (20, 30)
    compress = 1
    shuffle = 1
    complib = "bzip2"
    chunkshape = (100, 100)
    start = 3
    stop = 10
    step = 6


class Fletcher32TestCase(BasicTestCase):
    shape = (60, 50)
    compress = 0
    fletcher32 = 1
    chunkshape = (50, 50)
    start = 4
    stop = 20
    step = 7


class AllFiltersTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    fletcher32 = 1
    complib = "zlib"
    chunkshape = (20, 20)  # sss
    start = 2
    stop = 99
    step = 6


class FloatTypeTestCase(BasicTestCase):
    type = 'float64'
    shape = (2, 2)
    chunkshape = (5, 5)
    start = 3
    stop = 10
    step = 20


class ComplexTypeTestCase(BasicTestCase):
    type = 'complex128'
    shape = (2, 2)
    chunkshape = (5, 5)
    start = 3
    stop = 10
    step = 20


class StringTestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (2, 2)
    # shape = (2,2,20)
    chunkshape = (5, 5)
    start = 3
    stop = 10
    step = 20
    slices = (slice(0, 1), slice(1, 2))


class String2TestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (2, 20)
    chunkshape = (5, 5)
    start = 1
    stop = 10
    step = 2


class StringComprTestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (20, 2, 10)
    # shape = (20,0,10,20)
    compr = 1
    # shuffle = 1  # this shouldn't do nothing on chars
    chunkshape = (50, 50, 2)
    start = -1
    stop = 100
    step = 20


class Int8TestCase(BasicTestCase):
    type = "int8"
    shape = (2, 2)
    compress = 1
    shuffle = 1
    chunkshape = (50, 50)
    start = -1
    stop = 100
    step = 20


class Int16TestCase(BasicTestCase):
    type = "int16"
    shape = (2, 2)
    compress = 1
    shuffle = 1
    chunkshape = (50, 50)
    start = 1
    stop = 100
    step = 1


class Int32TestCase(BasicTestCase):
    type = "int32"
    shape = (2, 2)
    compress = 1
    shuffle = 1
    chunkshape = (50, 50)
    start = -1
    stop = 100
    step = 20


class Float16TestCase(BasicTestCase):
    type = "float16"
    shape = (200,)
    compress = 1
    shuffle = 1
    chunkshape = (20,)
    start = -1
    stop = 100
    step = 20


class Float32TestCase(BasicTestCase):
    type = "float32"
    shape = (200,)
    compress = 1
    shuffle = 1
    chunkshape = (20,)
    start = -1
    stop = 100
    step = 20


class Float64TestCase(BasicTestCase):
    type = "float64"
    shape = (200,)
    compress = 1
    shuffle = 1
    chunkshape = (20,)
    start = -1
    stop = 100
    step = 20


class Float96TestCase(BasicTestCase):
    type = "float96"
    shape = (200,)
    compress = 1
    shuffle = 1
    chunkshape = (20,)
    start = -1
    stop = 100
    step = 20


class Float128TestCase(BasicTestCase):
    type = "float128"
    shape = (200,)
    compress = 1
    shuffle = 1
    chunkshape = (20,)
    start = -1
    stop = 100
    step = 20


class Complex64TestCase(BasicTestCase):
    type = "complex64"
    shape = (4,)
    compress = 1
    shuffle = 1
    chunkshape = (2,)
    start = -1
    stop = 100
    step = 20


class Complex128TestCase(BasicTestCase):
    type = "complex128"
    shape = (20,)
    compress = 1
    shuffle = 1
    chunkshape = (2,)
    start = -1
    stop = 100
    step = 20


class Complex192TestCase(BasicTestCase):
    type = "complex192"
    shape = (20,)
    compress = 1
    shuffle = 1
    chunkshape = (2,)
    start = -1
    stop = 100
    step = 20


class Complex256TestCase(BasicTestCase):
    type = "complex256"
    shape = (20,)
    compress = 1
    shuffle = 1
    chunkshape = (2,)
    start = -1
    stop = 100
    step = 20


class ComprTestCase(BasicTestCase):
    type = "float64"
    compress = 1
    shuffle = 1
    shape = (200,)
    compr = 1
    chunkshape = (21,)
    start = 51
    stop = 100
    step = 7


# this is a subset of the tests in test_array.py, mostly to verify that errors
# are handled in the same way
class ReadOutArgumentTests(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode='w')
        self.size = 1000
        self.filters = Filters(complevel=1, complib='blosc')

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def create_array(self):
        array = numpy.arange(self.size, dtype='i8')
        disk_array = self.fileh.create_carray('/', 'array', atom=Int64Atom(),
                                              shape=(self.size, ),
                                              filters=self.filters)
        disk_array[:] = array
        return array, disk_array

    def test_read_entire_array(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size, ), 'i8')
        disk_array.read(out=out_buffer)
        numpy.testing.assert_equal(out_buffer, array)

    def test_read_non_contiguous_buffer(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size, ), 'i8')
        out_buffer_slice = out_buffer[0:self.size:2]
        # once Python 2.6 support is dropped, this could change
        # to assertRaisesRegexp to check exception type and message at once
        self.assertRaises(ValueError, disk_array.read, 0, self.size, 2,
                          out_buffer_slice)
        try:
            disk_array.read(0, self.size, 2, out_buffer_slice)
        except ValueError as exc:
            self.assertEqual('output array not C contiguous', str(exc))

    def test_buffer_too_small(self):
        array, disk_array = self.create_array()
        out_buffer = numpy.empty((self.size // 2, ), 'i8')
        self.assertRaises(ValueError, disk_array.read, 0, self.size, 1,
                          out_buffer)
        try:
            disk_array.read(0, self.size, 1, out_buffer)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))


class SizeOnDiskInMemoryPropertyTestCase(unittest.TestCase):

    def setUp(self):
        self.array_size = (10000, 10)
        # set chunkshape so it divides evenly into array_size, to avoid
        # partially filled chunks
        self.chunkshape = (1000, 10)
        # approximate size (in bytes) of non-data portion of hdf5 file
        self.hdf_overhead = 6000
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")

    def tearDown(self):
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def create_array(self, complevel):
        filters = Filters(complevel=complevel, complib='blosc')
        self.array = self.fileh.create_carray('/', 'somearray',
                                              atom=Int16Atom(),
                                              shape=self.array_size,
                                              filters=filters,
                                              chunkshape=self.chunkshape)

    def test_no_data(self):
        complevel = 0
        self.create_array(complevel)
        self.assertEqual(self.array.size_on_disk, 0)
        self.assertEqual(self.array.size_in_memory, 10000 * 10 * 2)

    def test_data_no_compression(self):
        complevel = 0
        self.create_array(complevel)
        self.array[:] = 1
        self.assertEqual(self.array.size_on_disk, 10000 * 10 * 2)
        self.assertEqual(self.array.size_in_memory, 10000 * 10 * 2)

    def test_highly_compressible_data(self):
        complevel = 1
        self.create_array(complevel)
        self.array[:] = 1
        self.fileh.flush()
        file_size = os.stat(self.file).st_size
        self.assertTrue(
            abs(self.array.size_on_disk - file_size) <= self.hdf_overhead)
        self.assertTrue(self.array.size_on_disk < self.array.size_in_memory)
        self.assertEqual(self.array.size_in_memory, 10000 * 10 * 2)

    # XXX
    def test_random_data(self):
        complevel = 1
        self.create_array(complevel)
        self.array[:] = numpy.random.randint(0, 1e6, self.array_size)
        self.fileh.flush()
        file_size = os.stat(self.file).st_size
        self.assertTrue(
            abs(self.array.size_on_disk - file_size) <= self.hdf_overhead)

        # XXX: check. The test fails if blosc is not available
        if which_lib_version('blosc') is not None:
            self.assertAlmostEqual(self.array.size_on_disk, 10000 * 10 * 2)
        else:
            self.assertTrue(
                abs(self.array.size_on_disk - 10000 * 10 * 2) < 200)


class OffsetStrideTestCase(unittest.TestCase):
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01a_String(self):
        """Checking carray with offseted NumPy strings appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_String..." % self.__class__.__name__)

        shape = (3, 2, 2)
        # Create an string atom
        carray = self.fileh.create_carray(root, 'strings',
                                          atom=StringAtom(itemsize=3),
                                          shape=shape,
                                          title="Array of strings",
                                          chunkshape=(1, 2, 2))
        a = numpy.array([[["a", "b"], [
                        "123", "45"], ["45", "123"]]], dtype="S3")
        carray[0] = a[0, 1:]
        a = numpy.array([[["s", "a"], [
                        "ab", "f"], ["s", "abc"], ["abc", "f"]]])
        carray[1] = a[0, 2:]

        # Read all the data:
        data = carray.read()
        if common.verbose:
            print("Object read:", data)
            print("Nrows in", carray._v_pathname, ":", carray.nrows)
            print("Second row in carray ==>", data[1].tolist())

        self.assertEqual(carray.nrows, 3)
        self.assertEqual(data[0].tolist(), [[b"123", b"45"], [b"45", b"123"]])
        self.assertEqual(data[1].tolist(), [[b"s", b"abc"], [b"abc", b"f"]])
        self.assertEqual(len(data[0]), 2)
        self.assertEqual(len(data[1]), 2)

    def test01b_String(self):
        """Checking carray with strided NumPy strings appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_String..." % self.__class__.__name__)

        shape = (3, 2, 2)
        # Create an string atom
        carray = self.fileh.create_carray(root, 'strings',
                                          atom=StringAtom(itemsize=3),
                                          shape=shape,
                                          title="Array of strings",
                                          chunkshape=(1, 2, 2))
        a = numpy.array([[["a", "b"], [
                        "123", "45"], ["45", "123"]]], dtype="S3")
        carray[0] = a[0, ::2]
        a = numpy.array([[["s", "a"], [
                        "ab", "f"], ["s", "abc"], ["abc", "f"]]])
        carray[1] = a[0, ::2]

        # Read all the rows:
        data = carray.read()
        if common.verbose:
            print("Object read:", data)
            print("Nrows in", carray._v_pathname, ":", carray.nrows)
            print("Second row in carray ==>", data[1].tolist())

        self.assertEqual(carray.nrows, 3)
        self.assertEqual(data[0].tolist(), [[b"a", b"b"], [b"45", b"123"]])
        self.assertEqual(data[1].tolist(), [[b"s", b"a"], [b"s", b"abc"]])
        self.assertEqual(len(data[0]), 2)
        self.assertEqual(len(data[1]), 2)

    def test02a_int(self):
        """Checking carray with offseted NumPy ints appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02a_int..." % self.__class__.__name__)

        shape = (3, 3)
        # Create an string atom
        carray = self.fileh.create_carray(root, 'CAtom',
                                          atom=Int32Atom(), shape=shape,
                                          title="array of ints",
                                          chunkshape=(1, 3))
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (0, 0, 0)], dtype='int32')
        carray[0:2] = a[2:]  # Introduce an offset
        a = numpy.array([(1, 1, 1), (-1, 0, 0)], dtype='int32')
        carray[2:3] = a[1:]  # Introduce an offset

        # Read all the rows:
        data = carray.read()
        if common.verbose:
            print("Object read:", data)
            print("Nrows in", carray._v_pathname, ":", carray.nrows)
            print("Third row in carray ==>", data[2])

        self.assertEqual(carray.nrows, 3)
        self.assertTrue(allequal(data[
                        0], numpy.array([1, 1, 1], dtype='int32')))
        self.assertTrue(allequal(data[
                        1], numpy.array([0, 0, 0], dtype='int32')))
        self.assertTrue(allequal(data[
                        2], numpy.array([-1, 0, 0], dtype='int32')))

    def test02b_int(self):
        """Checking carray with strided NumPy ints appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_int..." % self.__class__.__name__)

        shape = (3, 3)
        # Create an string atom
        carray = self.fileh.create_carray(root, 'CAtom',
                                          atom=Int32Atom(), shape=shape,
                                          title="array of ints",
                                          chunkshape=(1, 3))
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='int32')
        carray[0:2] = a[::3]  # Create an offset
        a = numpy.array([(1, 1, 1), (-1, 0, 0)], dtype='int32')
        carray[2:3] = a[::2]  # Create an offset

        # Read all the rows:
        data = carray.read()
        if common.verbose:
            print("Object read:", data)
            print("Nrows in", carray._v_pathname, ":", carray.nrows)
            print("Third row in carray ==>", data[2])

        self.assertEqual(carray.nrows, 3)
        self.assertTrue(allequal(data[
                        0], numpy.array([0, 0, 0], dtype='int32')))
        self.assertTrue(allequal(data[
                        1], numpy.array([3, 3, 3], dtype='int32')))
        self.assertTrue(allequal(data[
                        2], numpy.array([1, 1, 1], dtype='int32')))


class CopyTestCase(unittest.TestCase):

    def test01a_copy(self):
        """Checking CArray.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[...] = numpy.array([[456, 2], [3, 457]], dtype='int16')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test01b_copy(self):
        """Checking CArray.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(5, 5))
        array1[...] = numpy.array([[456, 2], [3, 457]], dtype='int16')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # By default, the chunkshape should be the same
        self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test01c_copy(self):
        """Checking CArray.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (5, 5)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[:2, :2] = numpy.array([[456, 2], [3, 457]], dtype='int16')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_copy(self):
        """Checking CArray.copy() method (where specified)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (5, 5)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[:2, :2] = numpy.array([[456, 2], [3, 457]], dtype='int16')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        group1 = fileh.create_group("/", "group1")
        array2 = array1.copy(group1, 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.group1.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03a_copy(self):
        """Checking CArray.copy() method (python flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03c_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1.flavor = "python"
        array1[...] = [[456, 2], [3, 457]]

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertEqual(array1.read(), array2.read())
        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03b_copy(self):
        """Checking CArray.copy() method (string python flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03d_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        shape = (2, 2)
        atom = StringAtom(itemsize=4)
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1.flavor = "python"
        array1[...] = [["456", "2"], ["3", "457"]]

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("type value-->", type(array2[:][0][0]))
            print("value-->", array2[:])
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertEqual(array1.read(), array2.read())

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03c_copy(self):
        """Checking CArray.copy() method (chararray flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03e_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        shape = (2, 2)
        atom = StringAtom(itemsize=4)
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[...] = numpy.array([["456", "2"], ["3", "457"]], dtype="S4")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))
        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))
        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # self.assertEqual(array1.chunkshape, array2.chunkshape)

        # Close the file
        fileh.close()
        os.remove(file)

    def test04_copy(self):
        """Checking CArray.copy() method (checking title copying)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[...] = numpy.array([[456, 2], [3, 457]], dtype='int16')
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', title="title array2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        # Assert user attributes
        if common.verbose:
            print("title of destination array-->", array2.title)
        self.assertEqual(array2.title, "title array2")

        # Close the file
        fileh.close()
        os.remove(file)

    def test05_copy(self):
        """Checking CArray.copy() method (user attributes copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[...] = numpy.array([[456, 2], [3, 457]], dtype='int16')
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(array2.attrs.attr1, "attr1")
        self.assertEqual(array2.attrs.attr2, 2)

        # Close the file
        fileh.close()
        os.remove(file)

    def test05b_copy(self):
        """Checking CArray.copy() method (user attributes not copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        shape = (2, 2)
        atom = Int16Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        array1[...] = numpy.array([[456, 2], [3, 457]], dtype='int16')
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(hasattr(array2.attrs, "attr1"), 0)
        self.assertEqual(hasattr(array2.attrs, "attr2"), 0)

        # Close the file
        fileh.close()
        os.remove(file)


class CloseCopyTestCase(CopyTestCase):
    close = 1


class OpenCopyTestCase(CopyTestCase):
    close = 0


class CopyIndexTestCase(unittest.TestCase):
    nrowsinbuf = 2

    def test01_index(self):
        """Checking CArray.copy() method with indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_index..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (100, 2)
        atom = Int32Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        r = numpy.arange(200, dtype='int32')
        r.shape = shape
        array1[...] = r

        # Select a different buffer size:
        array1.nrowsinbuf = self.nrowsinbuf

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])

        # The next line is commented out because a copy should not
        # keep the same chunkshape anymore.
        # F. Alted 2006-11-27
        # assert array1.chunkshape == array2.chunkshape
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)

    def _test02_indexclosef(self):
        """Checking CArray.copy() method with indexes (close file version)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_indexclosef..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an CArray
        shape = (100, 2)
        atom = Int32Atom()
        array1 = fileh.create_carray(fileh.root, 'array1',
                                     atom=atom, shape=shape,
                                     title="title array1", chunkshape=(2, 2))
        r = numpy.arange(200, dtype='int32')
        r.shape = shape
        array1[...] = r

        # Select a different buffer size:
        array1.nrowsinbuf = self.nrowsinbuf

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        # Close and reopen the file
        fileh.close()
        fileh = open_file(file, mode="r")
        array1 = fileh.root.array1
        array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertEqual(array1.chunkshape, array2.chunkshape)
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)


class CopyIndex1TestCase(CopyIndexTestCase):
    nrowsinbuf = 1
    start = 0
    stop = 7
    step = 1


class CopyIndex2TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 0
    stop = -1
    step = 1


class CopyIndex3TestCase(CopyIndexTestCase):
    nrowsinbuf = 3
    start = 1
    stop = 7
    step = 1


class CopyIndex4TestCase(CopyIndexTestCase):
    nrowsinbuf = 4
    start = 0
    stop = 6
    step = 1


class CopyIndex5TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 3
    stop = 7
    step = 1


class CopyIndex6TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 3
    stop = 6
    step = 2


class CopyIndex7TestCase(CopyIndexTestCase):
    start = 0
    stop = 7
    step = 10


class CopyIndex8TestCase(CopyIndexTestCase):
    start = 6
    stop = -1  # Negative values means starting from the end
    step = 1


class CopyIndex9TestCase(CopyIndexTestCase):
    start = 3
    stop = 4
    step = 1


class CopyIndex10TestCase(CopyIndexTestCase):
    nrowsinbuf = 1
    start = 3
    stop = 4
    step = 2


class CopyIndex11TestCase(CopyIndexTestCase):
    start = -3
    stop = -1
    step = 2


class CopyIndex12TestCase(CopyIndexTestCase):
    start = -1   # Should point to the last element
    stop = None  # None should mean the last element (including it)
    step = 1

# The next test should be run only in **heavy** mode


class Rows64bitsTestCase(unittest.TestCase):
    narows = 1000 * 1000   # each array will have 1 million entries
    # narows = 1000        # for testing only
    nanumber = 1000 * 3    # That should account for more than 2**31-1

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        fileh = self.fileh = open_file(self.file, "a")
        # Create an CArray
        shape = (self.narows * self.nanumber,)
        array = fileh.create_carray(fileh.root, 'array',
                                    atom=Int8Atom(), shape=shape,
                                    filters=Filters(complib='lzo',
                                                    complevel=1))

        # Fill the array
        na = numpy.arange(self.narows, dtype='int8')
        #~ for i in xrange(self.nanumber):
            #~ s = slice(i * self.narows, (i + 1)*self.narows)
            #~ array[s] = na
        s = slice(0, self.narows)
        array[s] = na
        s = slice((self.nanumber-1)*self.narows, self.nanumber * self.narows)
        array[s] = na

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01_basiccheck(self):
        "Some basic checks for carrays exceeding 2**31 rows"

        fileh = self.fileh
        array = fileh.root.array

        if self.close:
            if common.verbose:
                # Check how many entries there are in the array
                print("Before closing")
                print("Entries:", array.nrows, type(array.nrows))
                print("Entries:", array.nrows / (1000 * 1000), "Millions")
                print("Shape:", array.shape)
            # Close the file
            fileh.close()
            # Re-open the file
            fileh = self.fileh = open_file(self.file)
            array = fileh.root.array
            if common.verbose:
                print("After re-open")

        # Check how many entries there are in the array
        if common.verbose:
            print("Entries:", array.nrows, type(array.nrows))
            print("Entries:", array.nrows / (1000 * 1000), "Millions")
            print("Shape:", array.shape)
            print("Last 10 elements-->", array[-10:])
            stop = self.narows % 256
            if stop > 127:
                stop -= 256
            start = stop - 10
            # print("start, stop-->", start, stop)
            print("Should look like:", numpy.arange(start, stop, dtype='int8'))

        nrows = self.narows * self.nanumber
        # check nrows
        self.assertEqual(array.nrows, nrows)
        # Check shape
        self.assertEqual(array.shape, (nrows,))
        # check the 10 first elements
        self.assertTrue(allequal(array[:10], numpy.arange(10, dtype='int8')))
        # check the 10 last elements
        stop = self.narows % 256
        if stop > 127:
            stop -= 256
        start = stop - 10
        self.assertTrue(allequal(array[-10:],
                                 numpy.arange(start, stop, dtype='int8')))


class Rows64bitsTestCase1(Rows64bitsTestCase):
    close = 0


class Rows64bitsTestCase2(Rows64bitsTestCase):
    close = 1


class BigArrayTestCase(common.TempFileMixin, common.PyTablesTestCase):
    shape = (3000000000,)  # more than 2**31-1

    def setUp(self):
        super(BigArrayTestCase, self).setUp()
        # This should be fast since disk space isn't actually allocated,
        # so this case is OK for non-heavy test runs.
        self.h5file.create_carray('/', 'array',
                                  atom=Int8Atom(), shape=self.shape)

    def test00_shape(self):
        """Check that the shape doesn't overflow."""
        # See ticket #147.
        self.assertEqual(self.h5file.root.array.shape, self.shape)
        try:
            self.assertEqual(len(self.h5file.root.array), self.shape[0])
        except OverflowError:
            # In python 2.4 calling "len(self.h5file.root.array)" raises
            # an OverflowError also on 64bit platforms::
            #   OverflowError: __len__() should return 0 <= outcome < 2**31
            import sys
            if sys.version_info[:2] > (2, 4):
                # This can't be avoided in 32-bit platforms.
                self.assertTrue(self.shape[0] > numpy.iinfo(int).max,
                                "Array length overflowed but ``int`` "
                                "is wide enough.")

    def test01_shape_reopen(self):
        """Check that the shape doesn't overflow after reopening."""
        self._reopen('r')
        self.test00_shape()


# Test for default values when creating arrays.
class DfltAtomTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_dflt(self):
        "Check that Atom.dflt is honored (string version)."

        # Create a CArray with default values
        self.h5file.create_carray('/', 'bar',
                                  atom=StringAtom(itemsize=5, dflt=b"abdef"),
                                  shape=(10, 10))

        if self.reopen:
            self._reopen()

        # Check the values
        values = self.h5file.root.bar[:]
        if common.verbose:
            print("Read values:", values)
        self.assertTrue(
            allequal(values, numpy.array(["abdef"]*100, "S5").reshape(10, 10)))

    def test01_dflt(self):
        "Check that Atom.dflt is honored (int version)."

        # Create a CArray with default values
        self.h5file.create_carray('/', 'bar',
                                  atom=IntAtom(dflt=1), shape=(10, 10))

        if self.reopen:
            self._reopen()

        # Check the values
        values = self.h5file.root.bar[:]
        if common.verbose:
            print("Read values:", values)
        self.assertTrue(allequal(values, numpy.ones((10, 10), "i4")))

    def test02_dflt(self):
        "Check that Atom.dflt is honored (float version)."

        # Create a CArray with default values
        self.h5file.create_carray('/', 'bar',
                                  atom=FloatAtom(dflt=1.134), shape=(10, 10))

        if self.reopen:
            self._reopen()

        # Check the values
        values = self.h5file.root.bar[:]
        if common.verbose:
            print("Read values:", values)
        self.assertTrue(allequal(values, numpy.ones((10, 10), "f8")*1.134))


class DfltAtomNoReopen(DfltAtomTestCase):
    reopen = False


class DfltAtomReopen(DfltAtomTestCase):
    reopen = True


# Test for representation of defaults in atoms. Ticket #212.
class AtomDefaultReprTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00a_zeros(self):
        "Testing default values.  Zeros (scalar)."
        N = ()
        atom = StringAtom(itemsize=3, shape=N, dflt=b"")
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", repr(ca[0]))
            print("Defaults-->", repr(ca.atom.dflt))
        self.assertTrue(allequal(ca[0], numpy.zeros(N, 'S3')))
        self.assertTrue(allequal(ca.atom.dflt, numpy.zeros(N, 'S3')))

    def test00b_zeros(self):
        "Testing default values.  Zeros (array)."
        N = 2
        atom = StringAtom(itemsize=3, shape=N, dflt=b"")
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", ca[0])
            print("Defaults-->", ca.atom.dflt)
        self.assertTrue(allequal(ca[0], numpy.zeros(N, 'S3')))
        self.assertTrue(allequal(ca.atom.dflt, numpy.zeros(N, 'S3')))

    def test01a_values(self):
        "Testing default values.  Ones."
        N = 2
        atom = Int32Atom(shape=N, dflt=1)
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", ca[0])
            print("Defaults-->", ca.atom.dflt)
        self.assertTrue(allequal(ca[0], numpy.ones(N, 'i4')))
        self.assertTrue(allequal(ca.atom.dflt, numpy.ones(N, 'i4')))

    def test01b_values(self):
        "Testing default values.  Generic value."
        N = 2
        generic = 112.32
        atom = Float32Atom(shape=N, dflt=generic)
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", ca[0])
            print("Defaults-->", ca.atom.dflt)
        self.assertTrue(allequal(ca[0], numpy.ones(N, 'f4')*generic))
        self.assertTrue(allequal(ca.atom.dflt, numpy.ones(N, 'f4')*generic))

    def test02a_None(self):
        "Testing default values.  None (scalar)."
        N = ()
        atom = Int32Atom(shape=N, dflt=None)
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", repr(ca[0]))
            print("Defaults-->", repr(ca.atom.dflt))
        self.assertTrue(allequal(ca.atom.dflt, numpy.zeros(N, 'i4')))

    def test02b_None(self):
        "Testing default values.  None (array)."
        N = 2
        atom = Int32Atom(shape=N, dflt=None)
        ca = self.h5file.create_carray('/', 'test', atom=atom, shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", ca[0])
            print("Defaults-->", ca.atom.dflt)
        self.assertTrue(allequal(ca.atom.dflt, numpy.zeros(N, 'i4')))


class AtomDefaultReprNoReopen(AtomDefaultReprTestCase):
    reopen = False


class AtomDefaultReprReopen(AtomDefaultReprTestCase):
    reopen = True


class TruncateTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test(self):
        """Test for unability to truncate Array objects."""
        array1 = self.h5file.create_carray('/', 'array1', IntAtom(), [2, 2])
        self.assertRaises(TypeError, array1.truncate, 0)


# Test for dealing with multidimensional atoms
class MDAtomTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test01a_assign(self):
        "Assign a row to a (unidimensional) CArray with a MD atom."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2, 2)), shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign one row
        ca[0] = [[1, 3], [4, 5]]
        self.assertEqual(ca.nrows, 1)
        if common.verbose:
            print("First row-->", ca[0])
        self.assertTrue(allequal(ca[0], numpy.array([[1, 3], [4, 5]], 'i4')))

    def test01b_assign(self):
        "Assign several rows to a (unidimensional) CArray with a MD atom."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2, 2)), shape=(3,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign three rows
        ca[:] = [[[1]], [[2]], [[3]]]   # Simple broadcast
        self.assertEqual(ca.nrows, 3)
        if common.verbose:
            print("Third row-->", ca[2])
        self.assertTrue(allequal(ca[2], numpy.array([[3, 3], [3, 3]], 'i4')))

    def test02a_assign(self):
        "Assign a row to a (multidimensional) CArray with a MD atom."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2,)), shape=(1, 3))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign one row
        ca[:] = [[[1, 3], [4, 5], [7, 9]]]
        self.assertEqual(ca.nrows, 1)
        if common.verbose:
            print("First row-->", ca[0])
        self.assertTrue(allequal(ca[0], numpy.array(
            [[1, 3], [4, 5], [7, 9]], 'i4')))

    def test02b_assign(self):
        "Assign several rows to a (multidimensional) CArray with a MD atom."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2,)), shape=(3, 3))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign three rows
        ca[:] = [[[1, -3], [4, -5], [-7, 9]],
                 [[-1, 3], [-4, 5], [7, -8]],
                 [[-2, 3], [-5, 5], [7, -9]]]
        self.assertEqual(ca.nrows, 3)
        if common.verbose:
            print("Third row-->", ca[2])
        self.assertTrue(
            allequal(ca[2], numpy.array([[-2, 3], [-5, 5], [7, -9]], 'i4')))

    def test03a_MDMDMD(self):
        "Complex assign of a MD array in a MD CArray with a MD atom."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2, 4)), shape=(3, 2, 3))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign values
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(2 * 3*2*4, dtype='i4').reshape((2, 3, 2, 4))
        ca[:] = [a * 1, a*2, a*3]
        self.assertEqual(ca.nrows, 3)
        if common.verbose:
            print("Third row-->", ca[2])
        self.assertTrue(allequal(ca[2], a * 3))

    def test03b_MDMDMD(self):
        "Complex assign of a MD array in a MD CArray with a MD atom (II)."
        # Create an CArray
        ca = self.h5file.create_carray(
            '/', 'test', atom=Int32Atom((2, 4)), shape=(2, 3, 3))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign values
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(2 * 3*3*2*4, dtype='i4').reshape((2, 3, 3, 2, 4))
        ca[:] = a
        self.assertEqual(ca.nrows, 2)
        if common.verbose:
            print("Third row-->", ca[:, 2, ...])
        self.assertTrue(allequal(ca[:, 2, ...], a[:, 2, ...]))

    def test03c_MDMDMD(self):
        "Complex assign of a MD array in a MD CArray with a MD atom (III)."
        # Create an CArray
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom((2, 4)), shape=(3, 1, 2))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Assign values
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(3 * 1*2*2*4, dtype='i4').reshape((3, 1, 2, 2, 4))
        ca[:] = a
        self.assertEqual(ca.nrows, 3)
        if common.verbose:
            print("Second row-->", ca[:, :, 1, ...])
        self.assertTrue(allequal(ca[:, :, 1, ...], a[:, :, 1, ...]))


class MDAtomNoReopen(MDAtomTestCase):
    reopen = False


class MDAtomReopen(MDAtomTestCase):
    reopen = True


# Test for building very large MD atoms without defaults.  Ticket #211.
class MDLargeAtomTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test01_create(self):
        "Create a CArray with a very large MD atom."
        N = 2**16      # 4x larger than maximum object header size (64 KB)
        ca = self.h5file.create_carray('/', 'test',
                                       atom=Int32Atom(shape=N), shape=(1,))
        if self.reopen:
            self._reopen('a')
            ca = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", ca[0])
        self.assertTrue(allequal(ca[0], numpy.zeros(N, 'i4')))


class MDLargeAtomNoReopen(MDLargeAtomTestCase):
    reopen = False


class MDLargeAtomReopen(MDLargeAtomTestCase):
    reopen = True


class AccessClosedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(AccessClosedTestCase, self).setUp()
        self.array = self.h5file.create_carray(self.h5file.root, 'array',
                                               atom=Int32Atom(),
                                               shape=(10, 10))
        self.array[...] = numpy.zeros((10, 10))

    def test_read(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.read)

    def test_getitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__getitem__, 0)

    def test_setitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__setitem__, 0, 0)


class TestCreateCArrayArgs(common.TempFileMixin, common.PyTablesTestCase):
    obj = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    where = '/'
    name = 'carray'
    atom = Atom.from_dtype(obj.dtype)
    shape = obj.shape
    title = 'title'
    filters = None
    chunkshape = (1, 2)
    byteorder = None
    createparents = False

    def test_positional_args_01(self):
        self.h5file.create_carray(self.where, self.name,
                                  self.atom, self.shape,
                                  self.title, self.filters, self.chunkshape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(numpy.zeros_like(self.obj), nparr))

    def test_positional_args_02(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          self.atom, self.shape,
                                          self.title,
                                          self.filters, self.chunkshape)
        ptarr[...] = self.obj
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_positional_args_obj(self):
        self.h5file.create_carray(self.where, self.name,
                                  None, None,
                                  self.title,
                                  self.filters, self.chunkshape,
                                  self.byteorder, self.createparents,
                                  self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj(self):
        self.h5file.create_carray(self.where, self.name, title=self.title,
                                  chunkshape=self.chunkshape,
                                  obj=self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_01(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          atom=self.atom, shape=self.shape)
        ptarr[...] = self.obj
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_02(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          atom=self.atom, shape=self.shape)
        #ptarr[...] = self.obj
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(numpy.zeros_like(self.obj), nparr))

    def test_kwargs_obj_atom(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          atom=self.atom)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_shape(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_shape(self):
        ptarr = self.h5file.create_carray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          atom=self.atom,
                                          shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_error(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_carray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom)

    def test_kwargs_obj_shape_error(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_carray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_01(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_carray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=self.shape)

    def test_kwargs_obj_atom_shape_error_02(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_carray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=self.atom,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_03(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_carray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=shape)


#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    # theSuite.addTest(unittest.makeSuite(BasicTestCase))
    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicWriteTestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite2TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite3TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite4TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite5TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite6TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite7TestCase))
        theSuite.addTest(unittest.makeSuite(BasicWrite8TestCase))
        theSuite.addTest(unittest.makeSuite(EmptyCArrayTestCase))
        theSuite.addTest(unittest.makeSuite(EmptyCArray2TestCase))
        theSuite.addTest(unittest.makeSuite(SlicesCArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Slices2CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(EllipsisCArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis2CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis3CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibComprTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(BloscComprTestCase))
        theSuite.addTest(unittest.makeSuite(BloscShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(BloscFletcherTestCase))
        theSuite.addTest(unittest.makeSuite(BloscBloscLZTestCase))
        if 'lz4' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscLZ4TestCase))
            theSuite.addTest(unittest.makeSuite(BloscLZ4HCTestCase))
        if 'snappy' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscSnappyTestCase))
        if 'zlib' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscZlibTestCase))
        theSuite.addTest(unittest.makeSuite(LZOComprTestCase))
        theSuite.addTest(unittest.makeSuite(LZOShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ComprTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(FloatTypeTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexTypeTestCase))
        theSuite.addTest(unittest.makeSuite(StringTestCase))
        theSuite.addTest(unittest.makeSuite(String2TestCase))
        theSuite.addTest(unittest.makeSuite(StringComprTestCase))
        theSuite.addTest(unittest.makeSuite(Int8TestCase))
        theSuite.addTest(unittest.makeSuite(Int16TestCase))
        theSuite.addTest(unittest.makeSuite(Int32TestCase))
        if 'Float16Atom' in globals():
            theSuite.addTest(unittest.makeSuite(Float16TestCase))
        theSuite.addTest(unittest.makeSuite(Float32TestCase))
        theSuite.addTest(unittest.makeSuite(Float64TestCase))
        if 'Float96Atom' in globals():
            theSuite.addTest(unittest.makeSuite(Float96TestCase))
        if 'Float128Atom' in globals():
            theSuite.addTest(unittest.makeSuite(Float128TestCase))
        theSuite.addTest(unittest.makeSuite(Complex64TestCase))
        theSuite.addTest(unittest.makeSuite(Complex128TestCase))
        if 'Complex192Atom' in globals():
            theSuite.addTest(unittest.makeSuite(Complex192TestCase))
        if 'Complex256Atom' in globals():
            theSuite.addTest(unittest.makeSuite(Complex256TestCase))
        theSuite.addTest(unittest.makeSuite(ComprTestCase))
        theSuite.addTest(unittest.makeSuite(OffsetStrideTestCase))
        theSuite.addTest(unittest.makeSuite(Fletcher32TestCase))
        theSuite.addTest(unittest.makeSuite(AllFiltersTestCase))
        theSuite.addTest(unittest.makeSuite(ReadOutArgumentTests))
        theSuite.addTest(unittest.makeSuite(
            SizeOnDiskInMemoryPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(CloseCopyTestCase))
        theSuite.addTest(unittest.makeSuite(OpenCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex1TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex2TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex3TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex4TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex5TestCase))
        theSuite.addTest(unittest.makeSuite(BigArrayTestCase))
        theSuite.addTest(unittest.makeSuite(DfltAtomNoReopen))
        theSuite.addTest(unittest.makeSuite(DfltAtomReopen))
        theSuite.addTest(unittest.makeSuite(AtomDefaultReprNoReopen))
        theSuite.addTest(unittest.makeSuite(AtomDefaultReprReopen))
        theSuite.addTest(unittest.makeSuite(TruncateTestCase))
        theSuite.addTest(unittest.makeSuite(MDAtomNoReopen))
        theSuite.addTest(unittest.makeSuite(MDAtomReopen))
        theSuite.addTest(unittest.makeSuite(MDLargeAtomNoReopen))
        theSuite.addTest(unittest.makeSuite(MDLargeAtomReopen))
        theSuite.addTest(unittest.makeSuite(AccessClosedTestCase))
        theSuite.addTest(unittest.makeSuite(TestCreateCArrayArgs))
    if common.heavy:
        theSuite.addTest(unittest.makeSuite(Slices3CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Slices4CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis4CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis5CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis6CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis7CArrayTestCase))
        theSuite.addTest(unittest.makeSuite(MD3WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD5WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD6WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD7WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD10WriteTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex6TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex7TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex8TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex9TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex10TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex11TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex12TestCase))
        theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase1))
        theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase2))

    return theSuite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = test_create
# -*- coding: utf-8 -*-

"""This test unit checks object creation funtions, like open_file,
create_table, create_array or create_group.

It also checks:

- name identifiers in tree objects
- title character limit for objects (255)
- limit in number in table fields (255)

"""

from __future__ import print_function
import os
import sys
import hashlib
import unittest
import tempfile
import warnings

import numpy

from tables import *
# important objects to test
from tables import Group, Leaf, Table, Array, hdf5_version
from tables.tests import common
from tables.parameters import MAX_COLUMNS
from tables.hdf5extension import HAVE_DIRECT_DRIVER, HAVE_WINDOWS_DRIVER
from tables.utils import quantize

import tables

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class Record(IsDescription):
    var1 = StringCol(itemsize=4)  # 4-character String
    var2 = IntCol()               # integer
    var3 = Int16Col()             # short integer
    var4 = FloatCol()             # double (double-precision)
    var5 = Float32Col()           # float  (single-precision)


class createTestCase(unittest.TestCase):
    file = "test.h5"
    title = "This is the table title"
    expectedrows = 100
    maxshort = 2 ** 15
    maxint = 2147483648   # (2 ** 31)
    compress = 0

    def setUp(self):
        # Create an instance of HDF5 Table
        self.fileh = open_file(self.file, mode="w")
        self.root = self.fileh.root

        # Create a table object
        self.table = self.fileh.create_table(self.root, 'atable',
                                             Record, "Table title")
        # Create an array object
        self.array = self.fileh.create_array(self.root, 'anarray',
                                             [1], "Array title")
        # Create a group object
        self.group = self.fileh.create_group(self.root, 'agroup',
                                             "Group title")

    def tearDown(self):

        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_isClass(self):
        """Testing table creation."""
        self.assertTrue(isinstance(self.table, Table))
        self.assertTrue(isinstance(self.array, Array))
        self.assertTrue(isinstance(self.array, Leaf))
        self.assertTrue(isinstance(self.group, Group))

    def test01_overwriteNode(self):
        """Checking protection against node overwriting."""

        try:
            self.array = self.fileh.create_array(self.root, 'anarray',
                                                 [1], "Array title")
        except NodeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NameError was catched!")
                print(value)
        else:
            self.fail("expected a NodeError")

    def test02_syntaxname(self):
        """Checking syntax in object tree names."""

        # Now, try to attach an array to the object tree with
        # a not allowed Python variable name
        warnings.filterwarnings("error", category=NaturalNameWarning)
        try:
            self.array = self.fileh.create_array(self.root, ' array',
                                                 [1], "Array title")
        except NaturalNameWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NaturalNameWarning was catched!")
                print(value)
        else:
            self.fail("expected a NaturalNameWarning")

        # another name error
        try:
            self.array = self.fileh.create_array(self.root, '$array',
                                                 [1], "Array title")
        except NaturalNameWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NaturalNameWarning was catched!")
                print(value)
        else:
            self.fail("expected a NaturalNameWarning")

        # Finally, test a reserved word
        try:
            self.array = self.fileh.create_array(self.root, 'for',
                                                 [1], "Array title")
        except NaturalNameWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NaturalNameWarning was catched!")
                print(value)
        else:
            self.fail("expected a NaturalNameWarning")
        # Reset the warning
        warnings.filterwarnings("default", category=NaturalNameWarning)

    def test03a_titleAttr(self):
        """Checking the self.title attr in nodes."""

        # Close the opened file to destroy the object tree
        self.fileh.close()
        # Open the file again to re-create the objects
        self.fileh = open_file(self.file, "r")

        # Now, test that self.title exists and is correct in all the nodes
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        self.assertEqual(self.fileh.root.atable.title, "Table title")
        self.assertEqual(self.fileh.root.anarray.title, "Array title")

    def test03b_titleLength(self):
        """Checking large title character length limit (1023)"""

        titlelength = 1023
        # Try to put a very long title on a group object
        group = self.fileh.create_group(self.root, 'group',
                                        "t" * titlelength)
        self.assertEqual(group._v_title, "t" * titlelength)
        self.assertEqual(group._f_getattr('TITLE'), "t" * titlelength)

        # Now, try with a table object
        table = self.fileh.create_table(self.root, 'table',
                                        Record, "t" * titlelength)
        self.assertEqual(table.title, "t" * titlelength)
        self.assertEqual(table.get_attr("TITLE"), "t" * titlelength)

        # Finally, try with an Array object
        arr = self.fileh.create_array(self.root, 'arr',
                                      [1], "t" * titlelength)
        self.assertEqual(arr.title, "t" * titlelength)
        self.assertEqual(arr.get_attr("TITLE"), "t" * titlelength)

    def test04_maxFields(self):
        "Checking a large number of fields in tables"

        # The number of fields for a table
        varnumber = MAX_COLUMNS

        varnames = []
        for i in range(varnumber):
            varnames.append('int%d' % i)

        # Build a dictionary with the types as values and varnames as keys
        recordDict = {}
        i = 0
        for varname in varnames:
            recordDict[varname] = Col.from_type("int32", dflt=1, pos=i)
            i += 1
        # Append this entry to indicate the alignment!
        recordDict['_v_align'] = "="
        table = self.fileh.create_table(self.root, 'table',
                                        recordDict, "MetaRecord instance")
        row = table.row
        listrows = []
        # Write 10 records
        for j in range(10):
            rowlist = []
            for i in range(len(table.colnames)):
                row[varnames[i]] = i * j
                rowlist.append(i * j)

            row.append()
            listrows.append(tuple(rowlist))

        # write data on disk
        table.flush()

        # Read all the data as a list
        listout = table.read().tolist()

        # Compare the input rowlist and output row list. They should
        # be equal.
        if common.verbose:
            print("Original row list:", listrows[-1])
            print("Retrieved row list:", listout[-1])
        self.assertEqual(listrows, listout)

    # The next limitation has been released. A warning is still there, though
    def test05_maxFieldsExceeded(self):
        "Checking an excess of the maximum number of fields in tables"

        # The number of fields for a table
        varnumber = MAX_COLUMNS + 1

        varnames = []
        for i in range(varnumber):
            varnames.append('int%d' % i)

        # Build a dictionary with the types as values and varnames as keys
        recordDict = {}
        i = 0
        for varname in varnames:
            recordDict[varname] = Col.from_type("int32", dflt=1)
            i += 1

        # Now, create a table with this record object
        # This way of creating node objects has been deprecated
        # table = Table(recordDict, "MetaRecord instance")

        # Attach the table to object tree
        warnings.filterwarnings("error", category=PerformanceWarning)
        # Here, a PerformanceWarning should be raised!
        try:
            self.fileh.create_table(self.root, 'table',
                                    recordDict, "MetaRecord instance")
        except PerformanceWarning:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next PerformanceWarning was catched!")
                print(value)
        else:
            self.fail("expected an PerformanceWarning")
        # Reset the warning
        warnings.filterwarnings("default", category=PerformanceWarning)

    # The next limitation has been released
    def _test06_maxColumnNameLengthExceeded(self):
        "Checking an excess (256) of the maximum length in column names"

        # Build a dictionary with the types as values and varnames as keys
        recordDict = {}
        recordDict["a" * 255] = IntCol(dflt=1)
        recordDict["b" * 256] = IntCol(dflt=1)  # Should trigger a ValueError

        # Now, create a table with this record object
        # This way of creating node objects has been deprecated
        table = Table(recordDict, "MetaRecord instance")
        self.assertTrue(table is not None)

        # Attach the table to object tree
        # Here, ValueError should be raised!
        try:
            self.fileh.create_table(self.root, 'table',
                                    recordDict, "MetaRecord instance")
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
        else:
            self.fail("expected an ValueError")

    def test06_noMaxColumnNameLength(self):
        "Checking unlimited length in column names"

        # Build a dictionary with the types as values and varnames as keys
        recordDict = {}
        recordDict["a" * 255] = IntCol(dflt=1, pos=0)
        recordDict["b" * 1024] = IntCol(dflt=1, pos=1)  # Should work well

        # Attach the table to object tree
        # Here, IndexError should be raised!
        table = self.fileh.create_table(self.root, 'table',
                                        recordDict, "MetaRecord instance")
        self.assertEqual(table.colnames[0], "a" * 255)
        self.assertEqual(table.colnames[1], "b" * 1024)


class Record2(IsDescription):
    var1 = StringCol(itemsize=4)  # 4-character String
    var2 = IntCol()               # integer
    var3 = Int16Col()             # short integer


class FiltersTreeTestCase(unittest.TestCase):
    title = "A title"
    nrows = 10

    def setUp(self):
        # Create a temporary file
        self.file = tempfile.mktemp(".h5")
        # Create an instance of HDF5 Table
        self.h5file = open_file(self.file, "w", filters=self.filters)
        self.populateFile()

    def populateFile(self):
        group = self.h5file.root
        # Create a tree with three levels of depth
        for j in range(5):
            # Create a table
            table = self.h5file.create_table(group, 'table1', Record2,
                                             title=self.title,
                                             filters=None)
            # Get the record object associated with the new table
            d = table.row
            # Fill the table
            for i in xrange(self.nrows):
                d['var1'] = '%04d' % (self.nrows - i)
                d['var2'] = i
                d['var3'] = i * 2
                d.append()      # This injects the Record values
            # Flush the buffer for this table
            table.flush()

            # Create a couple of arrays in each group
            var1List = [x['var1'] for x in table.iterrows()]
            var3List = [x['var3'] for x in table.iterrows()]

            self.h5file.create_array(group, 'array1', var1List, "col 1")
            self.h5file.create_array(group, 'array2', var3List, "col 3")

            # Create a couple of EArrays as well
            ea1 = self.h5file.create_earray(group, 'earray1',
                                            StringAtom(itemsize=4), (0,),
                                            "col 1")
            ea2 = self.h5file.create_earray(group, 'earray2',
                                            Int16Atom(), (0,), "col 3")
            # And fill them with some values
            ea1.append(var1List)
            ea2.append(var3List)

            # Finally a couple of VLArrays too
            vla1 = self.h5file.create_vlarray(group, 'vlarray1',
                                              StringAtom(itemsize=4), "col 1")
            vla2 = self.h5file.create_vlarray(group, 'vlarray2',
                                              Int16Atom(), "col 3")
            # And fill them with some values
            vla1.append(var1List)
            vla2.append(var3List)

            # Create a new group (descendant of group)
            if j == 1:  # The second level
                group2 = self.h5file.create_group(group, 'group' + str(j),
                                                  filters=self.gfilters)
            elif j == 2:  # third level
                group2 = self.h5file.create_group(group, 'group' + str(j))
            else:   # The rest of levels
                group2 = self.h5file.create_group(group, 'group' + str(j),
                                                  filters=self.filters)
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        # Close the file
        if self.h5file.isopen:
            self.h5file.close()

        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_checkFilters(self):
        "Checking inheritance of filters on trees (open file version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_checkFilters..." %
                  self.__class__.__name__)

        # First level check
        if common.verbose:
            print("Test filter:", repr(self.filters))
            print("Filters in file:", repr(self.h5file.filters))

        if self.filters is None:
            filters = Filters()
        else:
            filters = self.filters
        self.assertEqual(repr(filters), repr(self.h5file.filters))
        # The next nodes have to have the same filter properties as
        # self.filters
        nodelist = [
            '/table1', '/group0/earray1', '/group0/vlarray1', '/group0',
        ]
        for node in nodelist:
            object = self.h5file.get_node(node)
            if isinstance(object, Group):
                self.assertEqual(repr(filters), repr(object._v_filters))
            else:
                self.assertEqual(repr(filters), repr(object.filters))

        # Second and third level check
        group1 = self.h5file.root.group0.group1
        if self.gfilters is None:
            if self.filters is None:
                gfilters = Filters()
            else:
                gfilters = self.filters
        else:
            gfilters = self.gfilters
        if common.verbose:
            print("Test gfilter:", repr(gfilters))
            print("Filters in file:", repr(group1._v_filters))

        self.assertEqual(repr(gfilters), repr(group1._v_filters))
        # The next nodes have to have the same filter properties as
        # gfilters
        nodelist = ['/group0/group1', '/group0/group1/earray1',
                    '/group0/group1/vlarray1',
                    '/group0/group1/table1', '/group0/group1/group2/table1']
        for node in nodelist:
            object = self.h5file.get_node(node)
            if isinstance(object, Group):
                self.assertEqual(repr(gfilters), repr(object._v_filters))
            else:
                self.assertEqual(repr(gfilters), repr(object.filters))

        # Fourth and fifth level check
        if self.filters is None:
            # If None, the filters are inherited!
            if self.gfilters is None:
                filters = Filters()
            else:
                filters = self.gfilters
        else:
            filters = self.filters
        group3 = self.h5file.root.group0.group1.group2.group3
        if common.verbose:
            print("Test filter:", repr(filters))
            print("Filters in file:", repr(group3._v_filters))

        self.assertEqual(repr(filters), repr(group3._v_filters))
        # The next nodes have to have the same filter properties as
        # self.filter
        nodelist = ['/group0/group1/group2/group3',
                    '/group0/group1/group2/group3/earray1',
                    '/group0/group1/group2/group3/vlarray1',
                    '/group0/group1/group2/group3/table1',
                    '/group0/group1/group2/group3/group4']
        for node in nodelist:
            object = self.h5file.get_node(node)
            if isinstance(object, Group):
                self.assertEqual(repr(filters), repr(object._v_filters))
            else:
                self.assertEqual(repr(filters), repr(object.filters))

        # Checking the special case for Arrays in which the compression
        # should always be the empty Filter()
        # The next nodes have to have the same filter properties as
        # Filter()
        nodelist = ['/array1',
                    '/group0/array1',
                    '/group0/group1/array1',
                    '/group0/group1/group2/array1',
                    '/group0/group1/group2/group3/array1']
        for node in nodelist:
            object = self.h5file.get_node(node)
            self.assertEqual(repr(Filters()), repr(object.filters))

    def test01_checkFilters(self):
        "Checking inheritance of filters on trees (close file version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_checkFilters..." %
                  self.__class__.__name__)

        # Close the file
        self.h5file.close()
        # And open it again
        self.h5file = open_file(self.file, "r")

        # First level check
        if self.filters is None:
            filters = Filters()
        else:
            filters = self.filters
        if common.verbose:
            print("Test filter:", repr(filters))
            print("Filters in file:", repr(self.h5file.filters))

        self.assertEqual(repr(filters), repr(self.h5file.filters))
        # The next nodes have to have the same filter properties as
        # self.filters
        nodelist = [
            '/table1', '/group0/earray1', '/group0/vlarray1', '/group0',
        ]
        for node in nodelist:
            object_ = self.h5file.get_node(node)
            if isinstance(object_, Group):
                self.assertEqual(repr(filters), repr(object_._v_filters))
            else:
                self.assertEqual(repr(filters), repr(object_.filters))

        # Second and third level check
        group1 = self.h5file.root.group0.group1
        if self.gfilters is None:
            if self.filters is None:
                gfilters = Filters()
            else:
                gfilters = self.filters
        else:
            gfilters = self.gfilters
        if common.verbose:
            print("Test filter:", repr(gfilters))
            print("Filters in file:", repr(group1._v_filters))

        repr(gfilters) == repr(group1._v_filters)
        # The next nodes have to have the same filter properties as
        # gfilters
        nodelist = ['/group0/group1', '/group0/group1/earray1',
                    '/group0/group1/vlarray1',
                    '/group0/group1/table1', '/group0/group1/group2/table1']
        for node in nodelist:
            object_ = self.h5file.get_node(node)
            if isinstance(object_, Group):
                self.assertEqual(repr(gfilters), repr(object_._v_filters))
            else:
                self.assertEqual(repr(gfilters), repr(object_.filters))

        # Fourth and fifth level check
        if self.filters is None:
            if self.gfilters is None:
                filters = Filters()
            else:
                filters = self.gfilters
        else:
            filters = self.filters
        group3 = self.h5file.root.group0.group1.group2.group3
        if common.verbose:
            print("Test filter:", repr(filters))
            print("Filters in file:", repr(group3._v_filters))

        repr(filters) == repr(group3._v_filters)
        # The next nodes have to have the same filter properties as
        # self.filters
        nodelist = ['/group0/group1/group2/group3',
                    '/group0/group1/group2/group3/earray1',
                    '/group0/group1/group2/group3/vlarray1',
                    '/group0/group1/group2/group3/table1',
                    '/group0/group1/group2/group3/group4']
        for node in nodelist:
            object = self.h5file.get_node(node)
            if isinstance(object, Group):
                self.assertEqual(repr(filters), repr(object._v_filters))
            else:
                self.assertEqual(repr(filters), repr(object.filters))

        # Checking the special case for Arrays in which the compression
        # should always be the empty Filter()
        # The next nodes have to have the same filter properties as
        # Filter()
        nodelist = ['/array1',
                    '/group0/array1',
                    '/group0/group1/array1',
                    '/group0/group1/group2/array1',
                    '/group0/group1/group2/group3/array1']
        for node in nodelist:
            object = self.h5file.get_node(node)
            self.assertEqual(repr(Filters()), repr(object.filters))


class FiltersCase1(FiltersTreeTestCase):
    filters = Filters()
    gfilters = Filters(complevel=1)


class FiltersCase2(FiltersTreeTestCase):
    filters = Filters(complevel=1, complib="bzip2")
    gfilters = Filters(complevel=1)


class FiltersCase3(FiltersTreeTestCase):
    filters = Filters(shuffle=True, complib="zlib")
    gfilters = Filters(complevel=1, shuffle=False, complib="lzo")


class FiltersCase4(FiltersTreeTestCase):
    filters = Filters(shuffle=True)
    gfilters = Filters(complevel=1, shuffle=False)


class FiltersCase5(FiltersTreeTestCase):
    filters = Filters(fletcher32=True)
    gfilters = Filters(complevel=1, shuffle=False)


class FiltersCase6(FiltersTreeTestCase):
    filters = None
    gfilters = Filters(complevel=1, shuffle=False)


class FiltersCase7(FiltersTreeTestCase):
    filters = Filters(complevel=1)
    gfilters = None


class FiltersCase8(FiltersTreeTestCase):
    filters = None
    gfilters = None


class FiltersCase9(FiltersTreeTestCase):
    filters = Filters(shuffle=True, complib="zlib")
    gfilters = Filters(complevel=5, shuffle=True, complib="bzip2")


class FiltersCase10(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc")


class FiltersCaseBloscBloscLZ(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc:blosclz")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc:blosclz")


class FiltersCaseBloscLZ4(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc:lz4")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc:lz4")


class FiltersCaseBloscLZ4HC(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc:lz4hc")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc:lz4hc")


class FiltersCaseBloscSnappy(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc:snappy")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc:snappy")


class FiltersCaseBloscZlib(FiltersTreeTestCase):
    filters = Filters(shuffle=False, complevel=1, complib="blosc:zlib")
    gfilters = Filters(complevel=5, shuffle=True, complib="blosc:zlib")


class CopyGroupTestCase(unittest.TestCase):
    title = "A title"
    nrows = 10

    def setUp(self):
        # Create a temporary file
        self.file = tempfile.mktemp(".h5")
        self.file2 = tempfile.mktemp(".h5")
        # Create the source file
        self.h5file = open_file(self.file, "w")
        # Create the destination
        self.h5file2 = open_file(self.file2, "w")
        self.populateFile()

    def populateFile(self):
        group = self.h5file.root
        # Add some user attrs:
        group._v_attrs.attr1 = "an string for root group"
        group._v_attrs.attr2 = 124
        # Create a tree
        for j in range(5):
            for i in range(2):
                # Create a new group (brother of group)
                group2 = self.h5file.create_group(group, 'bgroup' + str(i),
                                                  filters=None)

                # Create a table
                table = self.h5file.create_table(group2, 'table1', Record2,
                                                 title=self.title,
                                                 filters=None)
                # Get the record object associated with the new table
                d = table.row
                # Fill the table
                for i in xrange(self.nrows):
                    d['var1'] = '%04d' % (self.nrows - i)
                    d['var2'] = i
                    d['var3'] = i * 2
                    d.append()      # This injects the Record values
                # Flush the buffer for this table
                table.flush()

                # Add some user attrs:
                table.attrs.attr1 = "an string"
                table.attrs.attr2 = 234

                # Create a couple of arrays in each group
                var1List = [x['var1'] for x in table.iterrows()]
                var3List = [x['var3'] for x in table.iterrows()]

                self.h5file.create_array(group2, 'array1', var1List, "col 1")
                self.h5file.create_array(group2, 'array2', var3List, "col 3")

                # Create a couple of EArrays as well
                ea1 = self.h5file.create_earray(group2, 'earray1',
                                                StringAtom(itemsize=4), (0,),
                                                "col 1")
                ea2 = self.h5file.create_earray(group2, 'earray2',
                                                Int16Atom(), (0,), "col 3")
                # Add some user attrs:
                ea1.attrs.attr1 = "an string for earray"
                ea2.attrs.attr2 = 123
                # And fill them with some values
                ea1.append(var1List)
                ea2.append(var3List)

            # Create a new group (descendant of group)
            group3 = self.h5file.create_group(group, 'group' + str(j),
                                              filters=None)
            # Iterate over this new group (group3)
            group = group3
            # Add some user attrs:
            group._v_attrs.attr1 = "an string for group"
            group._v_attrs.attr2 = 124

    def tearDown(self):
        # Close the file
        if self.h5file.isopen:
            self.h5file.close()
        if self.h5file2.isopen:
            self.h5file2.close()

        os.remove(self.file)
        os.remove(self.file2)
        common.cleanup(self)

    #----------------------------------------

    def test00_nonRecursive(self):
        "Checking non-recursive copy of a Group"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_nonRecursive..." %
                  self.__class__.__name__)

        # Copy a group non-recursively
        srcgroup = self.h5file.root.group0.group1
#         srcgroup._f_copy_children(self.h5file2.root,
#                                recursive=False,
#                                filters=self.filters)
        self.h5file.copy_children(srcgroup, self.h5file2.root,
                                  recursive=False, filters=self.filters)
        if self.close:
            # Close the destination file
            self.h5file2.close()
            # And open it again
            self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        dstgroup = self.h5file2.root
        nodelist1 = srcgroup._v_children.keys()
        nodelist2 = dstgroup._v_children.keys()
        # Sort the lists
        nodelist1.sort()
        nodelist2.sort()
        if common.verbose:
            print("The origin node list -->", nodelist1)
            print("The copied node list -->", nodelist2)
        self.assertEqual(srcgroup._v_nchildren, dstgroup._v_nchildren)
        self.assertEqual(nodelist1, nodelist2)

    def test01_nonRecursiveAttrs(self):
        "Checking non-recursive copy of a Group (attributes copied)"

        if common.verbose:
            print('\n', '-=' * 30)
            print(("Running %s.test01_nonRecursiveAttrs..." %
                   self.__class__.__name__))

        # Copy a group non-recursively with attrs
        srcgroup = self.h5file.root.group0.group1
        srcgroup._f_copy_children(self.h5file2.root,
                                  recursive=False,
                                  filters=self.filters,
                                  copyuserattrs=1)
        if self.close:
            # Close the destination file
            self.h5file2.close()
            # And open it again
            self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        dstgroup = self.h5file2.root
        for srcnode in srcgroup:
            dstnode = getattr(dstgroup, srcnode._v_name)
            if isinstance(srcnode, Group):
                srcattrs = srcnode._v_attrs
                srcattrskeys = srcattrs._f_list("all")
                dstattrs = dstnode._v_attrs
                dstattrskeys = dstattrs._f_list("all")
            else:
                srcattrs = srcnode.attrs
                srcattrskeys = srcattrs._f_list("all")
                dstattrs = dstnode.attrs
                dstattrskeys = dstattrs._f_list("all")
            # Filters may differ, do not take into account
            if self.filters is not None:
                dstattrskeys.remove('FILTERS')
            # These lists should already be ordered
            if common.verbose:
                print("srcattrskeys for node %s: %s" % (srcnode._v_name,
                                                        srcattrskeys))
                print("dstattrskeys for node %s: %s" % (dstnode._v_name,
                                                        dstattrskeys))
            self.assertEqual(srcattrskeys, dstattrskeys)
            if common.verbose:
                print("The attrs names has been copied correctly")

            # Now, for the contents of attributes
            for srcattrname in srcattrskeys:
                srcattrvalue = str(getattr(srcattrs, srcattrname))
                dstattrvalue = str(getattr(dstattrs, srcattrname))
                self.assertEqual(srcattrvalue, dstattrvalue)
            if self.filters is not None:
                self.assertEqual(dstattrs.FILTERS, self.filters)

            if common.verbose:
                print("The attrs contents has been copied correctly")

    def test02_Recursive(self):
        "Checking recursive copy of a Group"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_Recursive..." % self.__class__.__name__)

        # Create the destination node
        group = self.h5file2.root
        for groupname in self.dstnode.split("/"):
            if groupname:
                group = self.h5file2.create_group(group, groupname)
        dstgroup = self.h5file2.get_node(self.dstnode)

        # Copy a group non-recursively
        srcgroup = self.h5file.get_node(self.srcnode)
        self.h5file.copy_children(srcgroup, dstgroup,
                                  recursive=True,
                                  filters=self.filters)
        lenSrcGroup = len(srcgroup._v_pathname)
        if lenSrcGroup == 1:
            lenSrcGroup = 0  # Case where srcgroup == "/"
        if self.close:
            # Close the destination file
            self.h5file2.close()
            # And open it again
            self.h5file2 = open_file(self.file2, "r")
            dstgroup = self.h5file2.get_node(self.dstnode)

        # Check that the copy has been done correctly
        lenDstGroup = len(dstgroup._v_pathname)
        if lenDstGroup == 1:
            lenDstGroup = 0  # Case where dstgroup == "/"
        first = 1
        nodelist1 = []
        for node in srcgroup._f_walknodes():
            if first:
                # skip the first group
                first = 0
                continue
            nodelist1.append(node._v_pathname[lenSrcGroup:])

        first = 1
        nodelist2 = []
        for node in dstgroup._f_walknodes():
            if first:
                # skip the first group
                first = 0
                continue
            nodelist2.append(node._v_pathname[lenDstGroup:])

        if common.verbose:
            print("The origin node list -->", nodelist1)
            print("The copied node list -->", nodelist2)
        self.assertEqual(nodelist1, nodelist2)

    def test03_RecursiveFilters(self):
        "Checking recursive copy of a Group (cheking Filters)"

        if common.verbose:
            print('\n', '-=' * 30)
            print(("Running %s.test03_RecursiveFilters..." %
                   self.__class__.__name__))

        # Create the destination node
        group = self.h5file2.root
        for groupname in self.dstnode.split("/"):
            if groupname:
                group = self.h5file2.create_group(group, groupname)
        dstgroup = self.h5file2.get_node(self.dstnode)

        # Copy a group non-recursively
        srcgroup = self.h5file.get_node(self.srcnode)
        srcgroup._f_copy_children(dstgroup,
                                  recursive=True,
                                  filters=self.filters)
        lenSrcGroup = len(srcgroup._v_pathname)
        if lenSrcGroup == 1:
            lenSrcGroup = 0  # Case where srcgroup == "/"
        if self.close:
            # Close the destination file
            self.h5file2.close()
            # And open it again
            self.h5file2 = open_file(self.file2, "r")
            dstgroup = self.h5file2.get_node(self.dstnode)

        # Check that the copy has been done correctly
        lenDstGroup = len(dstgroup._v_pathname)
        if lenDstGroup == 1:
            lenDstGroup = 0  # Case where dstgroup == "/"
        first = 1
        nodelist1 = {}
        for node in srcgroup._f_walknodes():
            if first:
                # skip the first group
                first = 0
                continue
            nodelist1[node._v_name] = node._v_pathname[lenSrcGroup:]

        first = 1
        for node in dstgroup._f_walknodes():
            if first:
                # skip the first group
                first = 0
                continue
            if isinstance(node, Group):
                repr(node._v_filters) == repr(nodelist1[node._v_name])
            else:
                repr(node.filters) == repr(nodelist1[node._v_name])


class CopyGroupCase1(CopyGroupTestCase):
    close = 0
    filters = None
    srcnode = '/group0/group1'
    dstnode = '/'


class CopyGroupCase2(CopyGroupTestCase):
    close = 1
    filters = None
    srcnode = '/group0/group1'
    dstnode = '/'


class CopyGroupCase3(CopyGroupTestCase):
    close = 0
    filters = None
    srcnode = '/group0'
    dstnode = '/group2/group3'


class CopyGroupCase4(CopyGroupTestCase):
    close = 1
    filters = Filters(complevel=1)
    srcnode = '/group0'
    dstnode = '/group2/group3'


class CopyGroupCase5(CopyGroupTestCase):
    close = 0
    filters = Filters()
    srcnode = '/'
    dstnode = '/group2/group3'


class CopyGroupCase6(CopyGroupTestCase):
    close = 1
    filters = Filters(fletcher32=True)
    srcnode = '/group0'
    dstnode = '/group2/group3'


class CopyGroupCase7(CopyGroupTestCase):
    close = 0
    filters = Filters(complevel=1, shuffle=False)
    srcnode = '/'
    dstnode = '/'


class CopyGroupCase8(CopyGroupTestCase):
    close = 1
    filters = Filters(complevel=1, complib="lzo")
    srcnode = '/'
    dstnode = '/'


class CopyFileTestCase(unittest.TestCase):
    title = "A title"
    nrows = 10

    def setUp(self):
        # Create a temporary file
        self.file = tempfile.mktemp(".h5")
        self.file2 = tempfile.mktemp(".h5")
        # Create the source file
        self.h5file = open_file(self.file, "w")
        self.populateFile()

    def populateFile(self):
        group = self.h5file.root
        # Add some user attrs:
        group._v_attrs.attr1 = "an string for root group"
        group._v_attrs.attr2 = 124
        # Create a tree
        for j in range(5):
            for i in range(2):
                # Create a new group (brother of group)
                group2 = self.h5file.create_group(group, 'bgroup' + str(i),
                                                  filters=None)

                # Create a table
                table = self.h5file.create_table(group2, 'table1', Record2,
                                                 title=self.title,
                                                 filters=None)
                # Get the record object associated with the new table
                d = table.row
                # Fill the table
                for i in xrange(self.nrows):
                    d['var1'] = '%04d' % (self.nrows - i)
                    d['var2'] = i
                    d['var3'] = i * 2
                    d.append()      # This injects the Record values
                # Flush the buffer for this table
                table.flush()

                # Add some user attrs:
                table.attrs.attr1 = "an string"
                table.attrs.attr2 = 234

                # Create a couple of arrays in each group
                var1List = [x['var1'] for x in table.iterrows()]
                var3List = [x['var3'] for x in table.iterrows()]

                self.h5file.create_array(group2, 'array1', var1List, "col 1")
                self.h5file.create_array(group2, 'array2', var3List, "col 3")

                # Create a couple of EArrays as well
                ea1 = self.h5file.create_earray(group2, 'earray1',
                                                StringAtom(itemsize=4), (0,),
                                                "col 1")
                ea2 = self.h5file.create_earray(group2, 'earray2',
                                                Int16Atom(), (0,),
                                                "col 3")
                # Add some user attrs:
                ea1.attrs.attr1 = "an string for earray"
                ea2.attrs.attr2 = 123
                # And fill them with some values
                ea1.append(var1List)
                ea2.append(var3List)

            # Create a new group (descendant of group)
            group3 = self.h5file.create_group(group, 'group' + str(j),
                                              filters=None)
            # Iterate over this new group (group3)
            group = group3
            # Add some user attrs:
            group._v_attrs.attr1 = "an string for group"
            group._v_attrs.attr2 = 124

    def tearDown(self):
        # Close the file
        if self.h5file.isopen:
            self.h5file.close()
        if hasattr(self, 'h5file2') and self.h5file2.isopen:
            self.h5file2.close()

        os.remove(self.file)
        if hasattr(self, 'file2') and os.path.exists(self.file2):
            os.remove(self.file2)
        common.cleanup(self)

    #----------------------------------------

    def test00_overwrite(self):
        "Checking copy of a File (overwriting file)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_overwrite..." % self.__class__.__name__)

        # Create a temporary file
        file2h = open(self.file2, "w")
        file2h.close()
        # Copy the file to the destination
        self.h5file.copy_file(self.file2, title=self.title,
                              overwrite=1,
                              copyuserattrs=0,
                              filters=None)

        # Close the original file, if needed
        if self.close:
            self.h5file.close()
            # re-open it
            self.h5file = open_file(self.file, "r")

        # ...and open the destination file
        self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        srcgroup = self.h5file.root
        dstgroup = self.h5file2.root
        nodelist1 = srcgroup._v_children.keys()
        nodelist2 = dstgroup._v_children.keys()
        # Sort the lists
        nodelist1.sort()
        nodelist2.sort()
        if common.verbose:
            print("The origin node list -->", nodelist1)
            print("The copied node list -->", nodelist2)
        self.assertEqual(srcgroup._v_nchildren, dstgroup._v_nchildren)
        self.assertEqual(nodelist1, nodelist2)
        self.assertEqual(self.h5file2.title, self.title)

    def test00a_srcdstequal(self):
        "Checking copy of a File (srcfile == dstfile)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00a_srcdstequal..." %
                  self.__class__.__name__)

        # Copy the file to the destination
        self.assertRaises(IOError, self.h5file.copy_file, self.h5file.filename)

    def test00b_firstclass(self):
        "Checking copy of a File (first-class function)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00b_firstclass..." % self.__class__.__name__)

        # Close the temporary file
        self.h5file.close()

        # Copy the file to the destination
        copy_file(self.file, self.file2, title=self.title,
                  copyuserattrs=0, filters=None, overwrite=1)

        # ...and open the source and destination file
        self.h5file = open_file(self.file, "r")
        self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        srcgroup = self.h5file.root
        dstgroup = self.h5file2.root
        nodelist1 = srcgroup._v_children.keys()
        nodelist2 = dstgroup._v_children.keys()
        # Sort the lists
        nodelist1.sort()
        nodelist2.sort()
        if common.verbose:
            print("The origin node list -->", nodelist1)
            print("The copied node list -->", nodelist2)
        self.assertEqual(srcgroup._v_nchildren, dstgroup._v_nchildren)
        self.assertEqual(nodelist1, nodelist2)
        self.assertEqual(self.h5file2.title, self.title)

    def test01_copy(self):
        "Checking copy of a File (attributes not copied)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_copy..." % self.__class__.__name__)

        # Copy the file to the destination
        self.h5file.copy_file(self.file2, title=self.title,
                              copyuserattrs=0,
                              filters=self.filters)

        # Close the original file, if needed
        if self.close:
            self.h5file.close()
            # re-open it
            self.h5file = open_file(self.file, "r")

        # ...and open the destination file
        self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        srcgroup = self.h5file.root
        dstgroup = self.h5file2.root
        nodelist1 = srcgroup._v_children.keys()
        nodelist2 = dstgroup._v_children.keys()
        # Sort the lists
        nodelist1.sort()
        nodelist2.sort()
        if common.verbose:
            print("The origin node list -->", nodelist1)
            print("The copied node list -->", nodelist2)
        self.assertEqual(srcgroup._v_nchildren, dstgroup._v_nchildren)
        self.assertEqual(nodelist1, nodelist2)
        # print("_v_attrnames-->", self.h5file2.root._v_attrs._v_attrnames)
        # print("--> <%s,%s>" % (self.h5file2.title, self.title))
        self.assertEqual(self.h5file2.title, self.title)

        # Check that user attributes has not been copied
        for srcnode in srcgroup:
            dstnode = getattr(dstgroup, srcnode._v_name)
            srcattrs = srcnode._v_attrs
            srcattrskeys = srcattrs._f_list("sys")
            dstattrs = dstnode._v_attrs
            dstattrskeys = dstattrs._f_list("all")
            # Filters may differ, do not take into account
            if self.filters is not None:
                dstattrskeys.remove('FILTERS')
            # These lists should already be ordered
            if common.verbose:
                print("srcattrskeys for node %s: %s" % (srcnode._v_name,
                                                        srcattrskeys))
                print("dstattrskeys for node %s: %s" % (dstnode._v_name,
                                                        dstattrskeys))
            self.assertEqual(srcattrskeys, dstattrskeys)
            if common.verbose:
                print("The attrs names has been copied correctly")

            # Now, for the contents of attributes
            for srcattrname in srcattrskeys:
                srcattrvalue = str(getattr(srcattrs, srcattrname))
                dstattrvalue = str(getattr(dstattrs, srcattrname))
                self.assertEqual(srcattrvalue, dstattrvalue)
            if self.filters is not None:
                self.assertEqual(dstattrs.FILTERS, self.filters)

            if common.verbose:
                print("The attrs contents has been copied correctly")

    def test02_Attrs(self):
        "Checking copy of a File (attributes copied)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_Attrs..." % self.__class__.__name__)

        # Copy the file to the destination
        self.h5file.copy_file(self.file2, title=self.title,
                              copyuserattrs=1,
                              filters=self.filters)

        # Close the original file, if needed
        if self.close:
            self.h5file.close()
            # re-open it
            self.h5file = open_file(self.file, "r")

        # ...and open the destination file
        self.h5file2 = open_file(self.file2, "r")

        # Check that the copy has been done correctly
        srcgroup = self.h5file.root
        dstgroup = self.h5file2.root
        for srcnode in srcgroup:
            dstnode = getattr(dstgroup, srcnode._v_name)
            srcattrs = srcnode._v_attrs
            srcattrskeys = srcattrs._f_list("all")
            dstattrs = dstnode._v_attrs
            dstattrskeys = dstattrs._f_list("all")
            # These lists should already be ordered
            if common.verbose:
                print("srcattrskeys for node %s: %s" % (srcnode._v_name,
                                                        srcattrskeys))
                print("dstattrskeys for node %s: %s" % (dstnode._v_name,
                                                        dstattrskeys))
            # Filters may differ, do not take into account
            if self.filters is not None:
                dstattrskeys.remove('FILTERS')
            self.assertEqual(srcattrskeys, dstattrskeys)
            if common.verbose:
                print("The attrs names has been copied correctly")

            # Now, for the contents of attributes
            for srcattrname in srcattrskeys:
                srcattrvalue = str(getattr(srcattrs, srcattrname))
                dstattrvalue = str(getattr(dstattrs, srcattrname))
                self.assertEqual(srcattrvalue, dstattrvalue)
            if self.filters is not None:
                self.assertEqual(dstattrs.FILTERS, self.filters)

            if common.verbose:
                print("The attrs contents has been copied correctly")


class CopyFileCase1(CopyFileTestCase):
    close = 0
    title = "A new title"
    filters = None


class CopyFileCase2(CopyFileTestCase):
    close = 1
    title = "A new title"
    filters = None


class CopyFileCase3(CopyFileTestCase):
    close = 0
    title = "A new title"
    filters = Filters(complevel=1)


class CopyFileCase4(CopyFileTestCase):
    close = 1
    title = "A new title"
    filters = Filters(complevel=1)


class CopyFileCase5(CopyFileTestCase):
    close = 0
    title = "A new title"
    filters = Filters(fletcher32=True)


class CopyFileCase6(CopyFileTestCase):
    close = 1
    title = "A new title"
    filters = Filters(fletcher32=True)


class CopyFileCase7(CopyFileTestCase):
    close = 0
    title = "A new title"
    filters = Filters(complevel=1, complib="lzo")


class CopyFileCase8(CopyFileTestCase):
    close = 1
    title = "A new title"
    filters = Filters(complevel=1, complib="lzo")


class CopyFileCase10(unittest.TestCase):

    def test01_notoverwrite(self):
        "Checking copy of a File (checking not overwriting)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_notoverwrite..." %
                  self.__class__.__name__)

        # Create two empty files:
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, "w")
        fileh2.close()  # close the second one
        # Copy the first into the second
        try:
            fileh.copy_file(file2, overwrite=False)
        except IOError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IOError was catched!")
                print(value)
        else:
            self.fail("expected a IOError")

        # Delete files
        fileh.close()
        os.remove(file)
        os.remove(file2)


class GroupFiltersTestCase(common.TempFileMixin, common.PyTablesTestCase):
    filters = tables.Filters(complevel=4)  # something non-default

    def setUp(self):
        super(GroupFiltersTestCase, self).setUp()

        atom, shape = tables.IntAtom(), (1, 1)
        create_group = self.h5file.create_group
        create_carray = self.h5file.create_carray

        create_group('/', 'implicit_no')
        create_group('/implicit_no', 'implicit_no')
        create_carray('/implicit_no/implicit_no', 'implicit_no',
                      atom=atom, shape=shape)
        create_carray('/implicit_no/implicit_no', 'explicit_no',
                      atom=atom, shape=shape, filters=tables.Filters())
        create_carray('/implicit_no/implicit_no', 'explicit_yes',
                      atom=atom, shape=shape, filters=self.filters)

        create_group('/', 'explicit_yes', filters=self.filters)
        create_group('/explicit_yes', 'implicit_yes')
        create_carray('/explicit_yes/implicit_yes', 'implicit_yes',
                      atom=atom, shape=shape)
        create_carray('/explicit_yes/implicit_yes', 'explicit_yes',
                      atom=atom, shape=shape, filters=self.filters)
        create_carray('/explicit_yes/implicit_yes', 'explicit_no',
                      atom=atom, shape=shape, filters=tables.Filters())

    def _check_filters(self, h5file, filters=None):
        for node in h5file:
            # Get node filters.
            if hasattr(node, 'filters'):
                node_filters = node.filters
            else:
                node_filters = node._v_filters
            # Compare to given filters.
            if filters is not None:
                self.assertEqual(node_filters, filters)
                return
            # Guess filters to compare to by node name.
            if node._v_name.endswith('_no'):
                self.assertEqual(
                    node_filters, tables.Filters(),
                    "node ``%s`` should have no filters" % node._v_pathname)
            elif node._v_name.endswith('_yes'):
                self.assertEqual(
                    node_filters, self.filters,
                    "node ``%s`` should have filters" % node._v_pathname)

    def test00_propagate(self):
        """Filters propagating to children."""
        self._check_filters(self.h5file)

    def _test_copyFile(self, filters=None):
        copyfname = tempfile.mktemp(suffix='.h5')
        try:
            self.h5file.copy_file(copyfname, filters=filters)
            try:
                copyf = tables.open_file(copyfname)
                self._check_filters(copyf, filters=filters)
            finally:
                copyf.close()
        finally:
            os.remove(copyfname)

    def test01_copyFile(self):
        """Keeping filters when copying a file."""
        self._test_copyFile()

    def test02_copyFile_override(self):
        """Overriding filters when copying a file."""
        self._test_copyFile(self.filters)

    def _test_change(self, pathname, change_filters, new_filters):
        group = self.h5file.get_node(pathname)
        # Check expected current filters.
        old_filters = tables.Filters()
        if pathname.endswith('_yes'):
            old_filters = self.filters
        self.assertEqual(group._v_filters, old_filters)
        # Change filters.
        change_filters(group)
        self.assertEqual(group._v_filters, new_filters)
        # Get and check changed filters.
        if self._reopen():
            group = self.h5file.get_node(pathname)
        self.assertEqual(group._v_filters, new_filters)

    def test03_change(self):
        """Changing the filters of a group."""
        def set_filters(group):
            group._v_filters = self.filters
        self._test_change('/', set_filters, self.filters)

    def test04_delete(self):
        """Deleting the filters of a group."""
        def del_filters(group):
            del group._v_filters
        self._test_change('/explicit_yes', del_filters, tables.Filters())


class SetBloscMaxThreadsTestCase(common.TempFileMixin,
                                 common.PyTablesTestCase):
    filters = tables.Filters(complevel=4, complib="blosc")

    def test00(self):
        """Checking set_blosc_max_threads()"""
        nthreads_old = tables.set_blosc_max_threads(4)
        if common.verbose:
            print("Previous max threads:", nthreads_old)
            print("Should be:", self.h5file.params['MAX_BLOSC_THREADS'])
        self.assertEqual(nthreads_old, self.h5file.params['MAX_BLOSC_THREADS'])
        self.h5file.create_carray('/', 'some_array',
                                  atom=tables.Int32Atom(), shape=(3, 3),
                                  filters = self.filters)
        nthreads_old = tables.set_blosc_max_threads(1)
        if common.verbose:
            print("Previous max threads:", nthreads_old)
            print("Should be:", 4)
        self.assertEqual(nthreads_old, 4)

    def test01(self):
        """Checking set_blosc_max_threads() (re-open)"""
        nthreads_old = tables.set_blosc_max_threads(4)
        self.h5file.create_carray('/', 'some_array',
                                  atom=tables.Int32Atom(), shape=(3, 3),
                                  filters = self.filters)
        self._reopen()
        nthreads_old = tables.set_blosc_max_threads(4)
        if common.verbose:
            print("Previous max threads:", nthreads_old)
            print("Should be:", self.h5file.params['MAX_BLOSC_THREADS'])
        self.assertEqual(nthreads_old, self.h5file.params['MAX_BLOSC_THREADS'])


class FilterTestCase(common.PyTablesTestCase):
    def test_filter_pack_type(self):
        self.assertEqual(type(Filters()._pack()), numpy.int64)

    @staticmethod
    def _hexl(n):
        if sys.version_info[0] > 2:
            return hex(int(n))
        else:
            return hex(long(n)).rstrip('L')

    def test_filter_pack_01(self):
        filter_ = Filters()
        self.assertEqual(self._hexl(filter_._pack()), '0x0')

    def test_filter_pack_02(self):
        filter_ = Filters(1, shuffle=False)
        self.assertEqual(self._hexl(filter_._pack()), '0x101')

    def test_filter_pack_03(self):
        filter_ = Filters(9, 'zlib', shuffle=True, fletcher32=True)
        self.assertEqual(self._hexl(filter_._pack()), '0x30109')

    def test_filter_pack_04(self):
        filter_ = Filters(1, shuffle=False, least_significant_digit=5)
        self.assertEqual(self._hexl(filter_._pack()), '0x5040101')

    def test_filter_unpack_01(self):
        filter_ = Filters._unpack(numpy.int64(0x0))
        self.assertFalse(filter_.shuffle)
        self.assertFalse(filter_.fletcher32)
        self.assertEqual(filter_.least_significant_digit, None)
        self.assertEqual(filter_.complevel, 0)
        self.assertEqual(filter_.complib, None)

    def test_filter_unpack_02(self):
        filter_ = Filters._unpack(numpy.int64(0x101))
        self.assertFalse(filter_.shuffle)
        self.assertFalse(filter_.fletcher32)
        self.assertEqual(filter_.least_significant_digit, None)
        self.assertEqual(filter_.complevel, 1)
        self.assertEqual(filter_.complib, 'zlib')

    def test_filter_unpack_03(self):
        filter_ = Filters._unpack(numpy.int64(0x30109))
        self.assertTrue(filter_.shuffle)
        self.assertTrue(filter_.fletcher32)
        self.assertEqual(filter_.least_significant_digit, None)
        self.assertEqual(filter_.complevel, 9)
        self.assertEqual(filter_.complib, 'zlib')

    def test_filter_unpack_04(self):
        filter_ = Filters._unpack(numpy.int64(0x5040101))
        self.assertFalse(filter_.shuffle)
        self.assertFalse(filter_.fletcher32)
        self.assertEqual(filter_.least_significant_digit, 5)
        self.assertEqual(filter_.complevel, 1)
        self.assertEqual(filter_.complib, 'zlib')


class DefaultDriverTestCase(common.PyTablesTestCase):
    DRIVER = None
    DRIVER_PARAMS = {}

    def setUp(self):
        self.h5fname = tempfile.mktemp(suffix=".h5")
        self.h5file = tables.open_file(self.h5fname, mode="w",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        # Create an HDF5 file and contents
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr", 41)
        self.h5file.create_array(root, "array", [1, 2], title="array")
        self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                 title="table")

    def tearDown(self):
        if self.h5file:
            self.h5file.close()
        self.h5file = None
        if os.path.isfile(self.h5fname):
            os.remove(self.h5fname)

    def assertIsFile(self):
        self.assertTrue(os.path.isfile(self.h5fname))

    def test_newFile(self):
        self.assertTrue(isinstance(self.h5file, tables.File))
        self.assertIsFile()

    def test_readFile(self):
        self.h5file.close()
        self.h5file = None

        self.assertIsFile()

        # Open an existing HDF5 file
        self.h5file = tables.open_file(self.h5fname, mode="r",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        # check contents
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

    def test_openFileA(self):
        self.h5file.close()
        self.h5file = None

        self.assertIsFile()

        # Open an existing HDF5 file in append mode
        self.h5file = tables.open_file(self.h5fname, mode="a",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        # check contents
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        # write new data
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr2", 42)
        self.h5file.create_array(root, "array2", [1, 2], title="array2")
        self.h5file.create_table(root, "table2", {"var2": tables.FloatCol()},
                                 title="table2")
        self.h5file.close()

        # check contents
        self.h5file = tables.open_file(self.h5fname, mode="a",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)
        self.assertEqual(self.h5file.get_node_attr(root, "testattr2"), 42)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.array2, tables.Array))
        self.assertEqual(root.array2._v_title, "array2")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        self.assertTrue(isinstance(root.table2, tables.Table))
        self.assertEqual(root.table2._v_title, "table2")
        self.assertTrue("var2" in root.table2.colnames)
        self.assertEqual(root.table2.cols.var2.dtype, tables.FloatCol().dtype)

    def test_openFileRW(self):
        self.h5file.close()
        self.h5file = None

        self.assertIsFile()

        # Open an existing HDF5 file in append mode
        self.h5file = tables.open_file(self.h5fname, mode="r+",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        # check contents
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        # write new data
        self.h5file.set_node_attr(root, "testattr2", 42)
        self.h5file.create_array(root, "array2", [1, 2], title="array2")
        self.h5file.create_table(root, "table2", {"var2": tables.FloatCol()},
                                 title="table2")
        self.h5file.close()

        # check contents
        self.h5file = tables.open_file(self.h5fname, mode="r+",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)

        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)
        self.assertEqual(self.h5file.get_node_attr(root, "testattr2"), 42)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.array2, tables.Array))
        self.assertEqual(root.array2._v_title, "array2")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        self.assertTrue(isinstance(root.table2, tables.Table))
        self.assertEqual(root.table2._v_title, "table2")
        self.assertTrue("var2" in root.table2.colnames)
        self.assertEqual(root.table2.cols.var2.dtype, tables.FloatCol().dtype)


class Sec2DriverTestCase(DefaultDriverTestCase):
    DRIVER = "H5FD_SEC2"

    if hdf5_version >= "1.8.9":
        def test_get_file_image(self):
            image = self.h5file.get_file_image()
            self.assertTrue(len(image) > 0)
            if sys.version_info[0] < 3:
                self.assertEqual([ord(i) for i in image[
                                 :4]], [137, 72, 68, 70])
            else:
                self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])


class StdioDriverTestCase(DefaultDriverTestCase):
    DRIVER = "H5FD_STDIO"

    if hdf5_version >= "1.8.9":
        def test_get_file_image(self):
            image = self.h5file.get_file_image()
            self.assertTrue(len(image) > 0)
            if sys.version_info[0] < 3:
                self.assertEqual([ord(i) for i in image[
                                 :4]], [137, 72, 68, 70])
            else:
                self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])


class CoreDriverTestCase(DefaultDriverTestCase):
    DRIVER = "H5FD_CORE"

    if hdf5_version >= "1.8.9":
        def test_get_file_image(self):
            image = self.h5file.get_file_image()
            self.assertTrue(len(image) > 0)
            if sys.version_info[0] < 3:
                self.assertEqual([ord(i) for i in image[
                                 :4]], [137, 72, 68, 70])
            else:
                self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])


class CoreDriverNoBackingStoreTestCase(common.PyTablesTestCase):
    DRIVER = "H5FD_CORE"

    def setUp(self):
        self.h5fname = tempfile.mktemp(suffix=".h5")
        self.h5file = None

    def tearDown(self):
        if self.h5file:
            self.h5file.close()
        elif self.h5fname in tables.file._open_files:
            open_files = tables.file._open_files
            for h5file in open_files.get_handlers_by_name(self.h5fname):
                h5file.close()

        self.h5file = None
        if os.path.isfile(self.h5fname):
            os.remove(self.h5fname)

    def test_newFile(self):
        """Ensure that nothing is written to file."""

        self.assertFalse(os.path.isfile(self.h5fname))

        self.h5file = tables.open_file(self.h5fname, mode="w",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)

        # Create an HDF5 file and contents
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr", 41)
        self.h5file.create_array(root, "array", [1, 2], title="array")
        self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                 title="table")
        self.h5file.close()     # flush

        self.assertFalse(os.path.isfile(self.h5fname))

    def test_readNewFileW(self):
        self.assertFalse(os.path.isfile(self.h5fname))

        # Create an HDF5 file and contents
        self.h5file = tables.open_file(self.h5fname, mode="w",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr", 41)
        self.h5file.create_array(root, "array", [1, 2], title="array")
        self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                 title="table")

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        self.h5file.close()     # flush

        self.assertFalse(os.path.isfile(self.h5fname))

    def test_readNewFileA(self):
        self.assertFalse(os.path.isfile(self.h5fname))

        # Create an HDF5 file and contents
        self.h5file = tables.open_file(self.h5fname, mode="a",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr", 41)
        self.h5file.create_array(root, "array", [1, 2], title="array")
        self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                 title="table")

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        self.h5file.close()     # flush

        self.assertFalse(os.path.isfile(self.h5fname))

    def test_openNewFileRW(self):
        self.assertFalse(os.path.isfile(self.h5fname))
        self.assertRaises(HDF5ExtError,
                          tables.open_file, self.h5fname, mode="r+",
                          driver=self.DRIVER, driver_core_backing_store=False)

    def test_openNewFileR(self):
        self.assertFalse(os.path.isfile(self.h5fname))
        self.assertRaises(HDF5ExtError,
                          tables.open_file, self.h5fname, mode="r",
                          driver=self.DRIVER, driver_core_backing_store=False)

    def _create_file(self, filename):
        h5file = tables.open_file(filename, mode="w")

        root = h5file.root
        h5file.set_node_attr(root, "testattr", 41)
        h5file.create_array(root, "array", [1, 2], title="array")
        h5file.create_table(root, "table", {"var1": tables.IntCol()},
                            title="table")

        h5file.close()

    def test_readFile(self):
        self._create_file(self.h5fname)
        self.assertTrue(os.path.isfile(self.h5fname))

        # Open an existing HDF5 file
        self.h5file = tables.open_file(self.h5fname, mode="r",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

    def _get_digest(self, filename):
        md5 = hashlib.md5()
        fd = open(filename, 'rb')

        for data in fd:
            md5.update(data)

        fd.close()

        hexdigest = md5.hexdigest()

        return hexdigest

    def test_openFileA(self):
        self._create_file(self.h5fname)
        self.assertTrue(os.path.isfile(self.h5fname))

        # compute the file hash
        hexdigest = self._get_digest(self.h5fname)

        # Open an existing HDF5 file in append mode
        self.h5file = tables.open_file(self.h5fname, mode="a",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)

        # check contents
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        # write new data
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr2", 42)
        self.h5file.create_array(root, "array2", [1, 2], title="array2")
        self.h5file.create_table(root, "table2", {"var2": tables.FloatCol()},
                                 title="table2")
        self.h5file.close()

        # ensure that there is no change on the file on disk
        self.assertEqual(hexdigest, self._get_digest(self.h5fname))

    def test_openFileRW(self):
        self._create_file(self.h5fname)
        self.assertTrue(os.path.isfile(self.h5fname))

        # compute the file hash
        hexdigest = self._get_digest(self.h5fname)

        # Open an existing HDF5 file in append mode
        self.h5file = tables.open_file(self.h5fname, mode="r+",
                                       driver=self.DRIVER,
                                       driver_core_backing_store=False)

        # check contents
        root = self.h5file.root

        self.assertEqual(self.h5file.get_node_attr(root, "testattr"), 41)

        self.assertTrue(isinstance(root.array, tables.Array))
        self.assertEqual(root.array._v_title, "array")

        self.assertTrue(isinstance(root.table, tables.Table))
        self.assertEqual(root.table._v_title, "table")
        self.assertTrue("var1" in root.table.colnames)
        self.assertEqual(root.table.cols.var1.dtype, tables.IntCol().dtype)

        # write new data
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr2", 42)
        self.h5file.create_array(root, "array2", [1, 2], title="array2")
        self.h5file.create_table(root, "table2", {"var2": tables.FloatCol()},
                                 title="table2")
        self.h5file.close()

        # ensure that there is no change on the file on disk
        self.assertEqual(hexdigest, self._get_digest(self.h5fname))

    if hdf5_version >= "1.8.9":
        def test_get_file_image(self):
            self.h5file = tables.open_file(self.h5fname, mode="w",
                                           driver=self.DRIVER,
                                           driver_core_backing_store=False)
            root = self.h5file.root
            self.h5file.set_node_attr(root, "testattr", 41)
            self.h5file.create_array(root, "array", [1, 2], title="array")
            self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                     title="table")

            image = self.h5file.get_file_image()

            self.assertTrue(len(image) > 0)
            if sys.version_info[0] < 3:
                self.assertEqual([ord(i) for i in image[
                                 :4]], [137, 72, 68, 70])
            else:
                self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])


class SplitDriverTestCase(DefaultDriverTestCase):
    DRIVER = "H5FD_SPLIT"
    DRIVER_PARAMS = {
        "driver_split_meta_ext": "-xm.h5",
        "driver_split_raw_ext": "-xr.h5",
    }

    def setUp(self):
        self.h5fname = tempfile.mktemp()
        self.h5fnames = [self.h5fname + self.DRIVER_PARAMS[k] for k in
                         ("driver_split_meta_ext", "driver_split_raw_ext")]
        self.h5file = tables.open_file(self.h5fname, mode="w",
                                       driver=self.DRIVER,
                                       **self.DRIVER_PARAMS)
        root = self.h5file.root
        self.h5file.set_node_attr(root, "testattr", 41)
        self.h5file.create_array(root, "array", [1, 2], title="array")
        self.h5file.create_table(root, "table", {"var1": tables.IntCol()},
                                 title="table")

    def tearDown(self):
        if self.h5file:
            self.h5file.close()
        self.h5file = None
        for fname in self.h5fnames:
            if os.path.isfile(fname):
                os.remove(fname)

    def assertIsFile(self):
        for fname in self.h5fnames:
            self.assertTrue(os.path.isfile(fname))


class NotSpportedDriverTestCase(common.PyTablesTestCase):
    DRIVER = None
    DRIVER_PARAMS = {}
    EXCEPTION = ValueError

    def setUp(self):
        self.h5fname = tempfile.mktemp(suffix=".h5")

    def tearDown(self):
        open_files = tables.file._open_files
        if self.h5fname in open_files:
            for h5file in open_files.get_handlers_by_name(self.h5fname):
                h5file.close()
        if os.path.exists(self.h5fname):
            os.remove(self.h5fname)

    def test_newFile(self):
        self.assertRaises(self.EXCEPTION, tables.open_file, self.h5fname,
                          mode="w", driver=self.DRIVER, **self.DRIVER_PARAMS)
        self.assertFalse(os.path.isfile(self.h5fname))


if "H5FD_LOG" in tables.hdf5extension._supported_drivers:
    BaseLogDriverTestCase = DefaultDriverTestCase

else:
    BaseLogDriverTestCase = NotSpportedDriverTestCase


class LogDriverTestCase(BaseLogDriverTestCase):
    DRIVER = "H5FD_LOG"

    def setUp(self):
        # local binding
        self.DRIVER_PARAMS = {
            "driver_log_file": tempfile.mktemp(suffix=".log")
        }

        super(LogDriverTestCase, self).setUp()

    def tearDown(self):
        super(LogDriverTestCase, self).tearDown()
        if os.path.exists(self.DRIVER_PARAMS["driver_log_file"]):
            os.remove(self.DRIVER_PARAMS["driver_log_file"])


if HAVE_DIRECT_DRIVER:
    class DirectDriverTestCase(DefaultDriverTestCase):
        DRIVER = "H5FD_DIRECT"

else:
    class DirectDriverTestCase(NotSpportedDriverTestCase):
        DRIVER = "H5FD_DIRECT"
        EXCEPTION = RuntimeError


if HAVE_WINDOWS_DRIVER:
    class WindowsDriverTestCase(DefaultDriverTestCase):
        DRIVER = "H5FD_WINDOWS"

else:
    class WindowsDriverTestCase(NotSpportedDriverTestCase):
        DRIVER = "H5FD_WINDOWS"
        EXCEPTION = RuntimeError


class FamilyDriverTestCase(NotSpportedDriverTestCase):
    DRIVER = "H5FD_FAMILY"


class MultiDriverTestCase(NotSpportedDriverTestCase):
    DRIVER = "H5FD_MULTI"


class MpioDriverTestCase(NotSpportedDriverTestCase):
    DRIVER = "H5FD_MPIO"


class MpiPosixDriverTestCase(NotSpportedDriverTestCase):
    DRIVER = "H5FD_MPIPOSIX"


class StreamDriverTestCase(NotSpportedDriverTestCase):
    DRIVER = "H5FD_STREAM"


class InMemoryCoreDriverTestCase(common.PyTablesTestCase):
    DRIVER = "H5FD_CORE"

    def setUp(self):
        self.h5fname = tempfile.mktemp(".h5")
        self.h5file = None

    def tearDown(self):
        if self.h5file:
            self.h5file.close()
        self.h5file = None

        if os.path.isfile(self.h5fname):
            os.remove(self.h5fname)

    def _create_image(self, filename="in-memory", title="Title", mode='w'):
        fileh = open_file(filename, mode=mode, title=title,
                          driver=self.DRIVER, driver_core_backing_store=0)

        try:
            fileh.create_array(fileh.root, 'array', [1, 2], title="Array")
            fileh.create_table(fileh.root, 'table', {
                               'var1': IntCol()}, "Table")
            fileh.root._v_attrs.testattr = 41

            image = fileh.get_file_image()
        finally:
            fileh.close()

        return image

    def test_newFileW(self):
        image = self._create_image(self.h5fname, mode='w')
        self.assertTrue(len(image) > 0)
        if sys.version_info[0] < 3:
            self.assertEqual([ord(i) for i in image[:4]], [137, 72, 68, 70])
        else:
            self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])
        self.assertFalse(os.path.exists(self.h5fname))

    def test_newFileA(self):
        image = self._create_image(self.h5fname, mode='a')
        self.assertTrue(len(image) > 0)
        if sys.version_info[0] < 3:
            self.assertEqual([ord(i) for i in image[:4]], [137, 72, 68, 70])
        else:
            self.assertEqual([i for i in image[:4]], [137, 72, 68, 70])
        self.assertFalse(os.path.exists(self.h5fname))

    def test_openFileR(self):
        image = self._create_image(self.h5fname)
        self.assertFalse(os.path.exists(self.h5fname))

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="r",
                                driver=self.DRIVER,
                                driver_core_image=image,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

    def test_openFileRW(self):
        image = self._create_image(self.h5fname)
        self.assertFalse(os.path.exists(self.h5fname))

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="r+",
                                driver=self.DRIVER,
                                driver_core_image=image,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        self.h5file.create_array(self.h5file.root, 'array2', range(10000),
                                 title="Array2")
        self.h5file.root._v_attrs.testattr2 = 42

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

    def test_openFileRW_update(self):
        filename = tempfile.mktemp(".h5")
        image1 = self._create_image(filename)
        self.assertFalse(os.path.exists(self.h5fname))

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="r+",
                                driver=self.DRIVER,
                                driver_core_image=image1,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        data = range(2 * tables.parameters.DRIVER_CORE_INCREMENT)
        self.h5file.create_array(self.h5file.root, 'array2', data,
                                 title="Array2")
        self.h5file.root._v_attrs.testattr2 = 42

        image2 = self.h5file.get_file_image()

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

        self.assertNotEqual(len(image1), len(image2))
        self.assertNotEqual(image1, image2)

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="r",
                                driver=self.DRIVER,
                                driver_core_image=image2,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr2"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr2"), 42)
        self.assertTrue(hasattr(self.h5file.root, "array2"))
        self.assertEqual(self.h5file.get_node_attr(
            "/array2", "TITLE"), "Array2")
        self.assertEqual(self.h5file.root.array2.read(), data)

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

    def test_openFileA(self):
        image = self._create_image(self.h5fname)
        self.assertFalse(os.path.exists(self.h5fname))

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="a",
                                driver=self.DRIVER,
                                driver_core_image=image,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

    def test_openFileA_update(self):
        filename = tempfile.mktemp(".h5")
        image1 = self._create_image(filename)
        self.assertFalse(os.path.exists(self.h5fname))

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="a",
                                driver=self.DRIVER,
                                driver_core_image=image1,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        data = range(2 * tables.parameters.DRIVER_CORE_INCREMENT)
        self.h5file.create_array(self.h5file.root, 'array2', data,
                                 title="Array2")
        self.h5file.root._v_attrs.testattr2 = 42

        image2 = self.h5file.get_file_image()

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

        self.assertNotEqual(len(image1), len(image2))
        self.assertNotEqual(image1, image2)

        # Open an existing file
        self.h5file = open_file(self.h5fname, mode="r",
                                driver=self.DRIVER,
                                driver_core_image=image2,
                                driver_core_backing_store=0)

        # Get the CLASS attribute of the arr object
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "TITLE"))
        self.assertEqual(self.h5file.get_node_attr("/", "TITLE"), "Title")
        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr"), 41)
        self.assertTrue(hasattr(self.h5file.root, "array"))
        self.assertEqual(self.h5file.get_node_attr("/array", "TITLE"), "Array")
        self.assertTrue(hasattr(self.h5file.root, "table"))
        self.assertEqual(self.h5file.get_node_attr("/table", "TITLE"), "Table")
        self.assertEqual(self.h5file.root.array.read(), [1, 2])

        self.assertTrue(hasattr(self.h5file.root._v_attrs, "testattr2"))
        self.assertEqual(self.h5file.get_node_attr("/", "testattr2"), 42)
        self.assertTrue(hasattr(self.h5file.root, "array2"))
        self.assertEqual(self.h5file.get_node_attr(
            "/array2", "TITLE"), "Array2")
        self.assertEqual(self.h5file.root.array2.read(), data)

        self.h5file.close()

        self.assertFalse(os.path.exists(self.h5fname))

    def test_str(self):
        self.h5file = open_file(self.h5fname, mode="w", title="Title",
                                driver=self.DRIVER,
                                driver_core_backing_store=0)

        self.h5file.create_array(self.h5file.root, 'array', [1, 2],
                                 title="Array")
        self.h5file.create_table(self.h5file.root, 'table', {'var1': IntCol()},
                                 "Table")
        self.h5file.root._v_attrs.testattr = 41

        # ensure that the __str__ method works even if there is no phisical
        # file on disk (in which case the os.stat operation for date retrieval
        # fails)
        self.assertTrue(str(self.h5file) is not None)

        self.h5file.close()
        self.assertFalse(os.path.exists(self.h5fname))


class QuantizeTestCase(unittest.TestCase):
    mode = "w"
    title = "This is the table title"
    expectedrows = 10
    appendrows = 5

    def setUp(self):
        self.data = numpy.linspace(-5., 5., 41)
        self.randomdata = numpy.random.random_sample(1000000)
        self.randomints = numpy.random.random_integers(
            -1000000, 1000000, 1000000).astype('int64')
        # Create a temporary file
        self.file = tempfile.mktemp(".h5")
        # Create an instance of HDF5 Table
        self.h5file = open_file(self.file, self.mode, self.title)
        self.populateFile()
        self.h5file.close()
        self.quantizeddata_0 = numpy.asarray(
            [-5.] * 2 + [-4.] * 5 + [-3.] * 3 + [-2.] * 5 + [-1.] * 3 +
            [0.] * 5 + [1.] * 3 + [2.] * 5 + [3.] * 3 + [4.] * 5 + [5.] * 2)
        self.quantizeddata_m1 = numpy.asarray(
            [-8.] * 4 + [0.] * 33 + [8.] * 4)

    def populateFile(self):
        root = self.h5file.root
        filters = Filters(complevel=1, complib="blosc",
                          least_significant_digit=1)
        ints = self.h5file.create_carray(root, "integers", Int64Atom(),
                                         (1000000,), filters=filters)
        ints[:] = self.randomints
        floats = self.h5file.create_carray(root, "floats", Float32Atom(),
                                           (1000000,), filters=filters)
        floats[:] = self.randomdata
        data1 = self.h5file.create_carray(root, "data1", Float64Atom(),
                                          (41,), filters=filters)
        data1[:] = self.data
        filters = Filters(complevel=1, complib="blosc",
                          least_significant_digit=0)
        data0 = self.h5file.create_carray(root, "data0", Float64Atom(),
                                          (41,), filters=filters)
        data0[:] = self.data
        filters = Filters(complevel=1, complib="blosc",
                          least_significant_digit=2)
        data2 = self.h5file.create_carray(root, "data2", Float64Atom(),
                                          (41,), filters=filters)
        data2[:] = self.data
        filters = Filters(complevel=1, complib="blosc",
                          least_significant_digit=-1)
        datam1 = self.h5file.create_carray(root, "datam1", Float64Atom(),
                                           (41,), filters=filters)
        datam1[:] = self.data

    def tearDown(self):
        # Close the file
        if self.h5file.isopen:
            self.h5file.close()

        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_quantizeData(self):
        """Checking the quantize() function."""

        quantized_0 = quantize(self.data, 0)
        quantized_1 = quantize(self.data, 1)
        quantized_2 = quantize(self.data, 2)
        quantized_m1 = quantize(self.data, -1)
        numpy.testing.assert_array_equal(quantized_0, self.quantizeddata_0)
        numpy.testing.assert_array_equal(quantized_1, self.data)
        numpy.testing.assert_array_equal(quantized_2, self.data)
        numpy.testing.assert_array_equal(quantized_m1, self.quantizeddata_m1)

    def test01_quantizeDataMaxError(self):
        """Checking the maximum error introduced by the quantize() function."""

        quantized_0 = quantize(self.randomdata, 0)
        quantized_1 = quantize(self.randomdata, 1)
        quantized_2 = quantize(self.randomdata, 2)
        quantized_m1 = quantize(self.randomdata, -1)
        # assertLess is new in Python 2.7
        #self.assertLess(numpy.abs(quantized_0 - self.randomdata).max(), 0.5)
        #self.assertLess(numpy.abs(quantized_1 - self.randomdata).max(), 0.05)
        #self.assertLess(numpy.abs(quantized_2 - self.randomdata).max(), 0.005)
        #self.assertLess(numpy.abs(quantized_m1 - self.randomdata).max(), 1.)

        self.assertTrue(numpy.abs(quantized_0 - self.randomdata).max() < 0.5)
        self.assertTrue(numpy.abs(quantized_1 - self.randomdata).max() < 0.05)
        self.assertTrue(numpy.abs(quantized_2 - self.randomdata).max() < 0.005)
        self.assertTrue(numpy.abs(quantized_m1 - self.randomdata).max() < 1.)

    def test02_array(self):
        """Checking quantized data as written to disk."""

        self.h5file = open_file(self.file, "r")
        numpy.testing.assert_array_equal(self.h5file.root.data1[:], self.data)
        numpy.testing.assert_array_equal(self.h5file.root.data2[:], self.data)
        numpy.testing.assert_array_equal(self.h5file.root.data0[:],
                                         self.quantizeddata_0)
        numpy.testing.assert_array_equal(self.h5file.root.datam1[:],
                                         self.quantizeddata_m1)
        numpy.testing.assert_array_equal(self.h5file.root.integers[:],
                                         self.randomints)
        self.assertEqual(self.h5file.root.integers[:].dtype,
                         self.randomints.dtype)
        # assertLess is new in Python 2.7
        #self.assertLess(
        #    numpy.abs(self.h5file.root.floats[:] - self.randomdata).max(),
        #    0.05
        #)
        self.assertTrue(
            numpy.abs(self.h5file.root.floats[:] - self.randomdata).max() <
            0.05
        )


#----------------------------------------------------------------------

def suite():
    import doctest

    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1 # Uncomment this only for testing purposes!

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(FiltersCase1))
        theSuite.addTest(unittest.makeSuite(FiltersCase2))
        theSuite.addTest(unittest.makeSuite(FiltersCase10))
        theSuite.addTest(unittest.makeSuite(FiltersCaseBloscBloscLZ))
        if 'lz4' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(FiltersCaseBloscLZ4))
            theSuite.addTest(unittest.makeSuite(FiltersCaseBloscLZ4HC))
        if 'snappy' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(FiltersCaseBloscSnappy))
        if 'zlib' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(FiltersCaseBloscZlib))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase1))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase2))
        theSuite.addTest(unittest.makeSuite(CopyFileCase1))
        theSuite.addTest(unittest.makeSuite(CopyFileCase2))
        theSuite.addTest(unittest.makeSuite(GroupFiltersTestCase))
        theSuite.addTest(unittest.makeSuite(SetBloscMaxThreadsTestCase))
        theSuite.addTest(unittest.makeSuite(FilterTestCase))
        theSuite.addTest(doctest.DocTestSuite(tables.filters))

        theSuite.addTest(unittest.makeSuite(DefaultDriverTestCase))
        theSuite.addTest(unittest.makeSuite(Sec2DriverTestCase))
        theSuite.addTest(unittest.makeSuite(StdioDriverTestCase))
        theSuite.addTest(unittest.makeSuite(CoreDriverTestCase))
        theSuite.addTest(unittest.makeSuite(CoreDriverNoBackingStoreTestCase))
        theSuite.addTest(unittest.makeSuite(SplitDriverTestCase))

        theSuite.addTest(unittest.makeSuite(LogDriverTestCase))
        theSuite.addTest(unittest.makeSuite(DirectDriverTestCase))
        theSuite.addTest(unittest.makeSuite(WindowsDriverTestCase))

        theSuite.addTest(unittest.makeSuite(FamilyDriverTestCase))
        theSuite.addTest(unittest.makeSuite(MultiDriverTestCase))
        theSuite.addTest(unittest.makeSuite(MpioDriverTestCase))
        theSuite.addTest(unittest.makeSuite(MpiPosixDriverTestCase))
        theSuite.addTest(unittest.makeSuite(StreamDriverTestCase))

        if hdf5_version >= "1.8.9":
            theSuite.addTest(unittest.makeSuite(InMemoryCoreDriverTestCase))

        theSuite.addTest(unittest.makeSuite(QuantizeTestCase))

    if common.heavy:
        theSuite.addTest(unittest.makeSuite(createTestCase))
        theSuite.addTest(unittest.makeSuite(FiltersCase3))
        theSuite.addTest(unittest.makeSuite(FiltersCase4))
        theSuite.addTest(unittest.makeSuite(FiltersCase5))
        theSuite.addTest(unittest.makeSuite(FiltersCase6))
        theSuite.addTest(unittest.makeSuite(FiltersCase7))
        theSuite.addTest(unittest.makeSuite(FiltersCase8))
        theSuite.addTest(unittest.makeSuite(FiltersCase9))
        theSuite.addTest(unittest.makeSuite(CopyFileCase3))
        theSuite.addTest(unittest.makeSuite(CopyFileCase4))
        theSuite.addTest(unittest.makeSuite(CopyFileCase5))
        theSuite.addTest(unittest.makeSuite(CopyFileCase6))
        theSuite.addTest(unittest.makeSuite(CopyFileCase7))
        theSuite.addTest(unittest.makeSuite(CopyFileCase8))
        theSuite.addTest(unittest.makeSuite(CopyFileCase10))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase3))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase4))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase5))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase6))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase7))
        theSuite.addTest(unittest.makeSuite(CopyGroupCase8))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_do_undo
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile
import warnings

from tables import *
from tables.node import NotLoggedMixin
from tables.path import join_path

from tables.tests import common

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class BasicTestCase(unittest.TestCase):

    """Test for basic Undo/Redo operations."""

    _reopen = False
    """Whether to reopen the file at certain points."""

    def _doReopen(self):
        if self._reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, mode='r+')

    def setUp(self):
        # Create an HDF5 file
        # self.file = "/tmp/test.h5"
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_simple(self):
        """Checking simple do/undo."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_simple..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/otherarray" not in self.fileh)
        self.assertEqual(self.fileh._curaction, 0)
        self.assertEqual(self.fileh._curmark, 0)
        # Redo the operation
        self._doReopen()
        self.fileh.redo()
        if common.verbose:
            print("Object tree after redo:", self.fileh)
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray.title, "Another array")
        self.assertEqual(self.fileh._curaction, 1)
        self.assertEqual(self.fileh._curmark, 0)

    def test01_twice(self):
        """Checking do/undo (twice operations intertwined)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_twice..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operations
        self._doReopen()
        self.fileh.undo()
        self.assertTrue("/otherarray" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertEqual(self.fileh._curaction, 0)
        self.assertEqual(self.fileh._curmark, 0)
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray.title, "Another array")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh._curaction, 2)
        self.assertEqual(self.fileh._curmark, 0)

    def test02_twice2(self):
        """Checking twice ops and two marks."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_twice2..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        # Put a mark
        self._doReopen()
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        self.assertEqual(self.fileh._curaction, 3)
        self.assertEqual(self.fileh._curmark, 1)
        # Unwind just one mark
        self.fileh.undo()
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertEqual(self.fileh._curaction, 2)
        self.assertEqual(self.fileh._curmark, 1)
        # Unwind another mark
        self.fileh.undo()
        self.assertEqual(self.fileh._curaction, 0)
        self.assertEqual(self.fileh._curmark, 0)
        self.assertTrue("/otherarray" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        # Redo until the next mark
        self.fileh.redo()
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self._doReopen()
        self.assertEqual(self.fileh._curaction, 2)
        self.assertEqual(self.fileh._curmark, 1)
        # Redo until the end
        self.fileh.redo()
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray.title, "Another array")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh._curaction, 3)
        self.assertEqual(self.fileh._curmark, 1)

    def test03_6times3marks(self):
        """Checking with six ops and three marks."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_6times3marks..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Put a mark
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Put a mark
        self._doReopen()
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray5', [7, 8], "Another array 5")
        self.fileh.create_array('/', 'otherarray6', [8, 9], "Another array 6")
        # Unwind just one mark
        self.fileh.undo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Unwind another mark
        self.fileh.undo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Unwind all marks
        self.fileh.undo()
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Redo until the next mark
        self._doReopen()
        self.fileh.redo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Redo until the next mark
        self.fileh.redo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Redo until the end
        self.fileh.redo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        self.assertTrue("/otherarray5" in self.fileh)
        self.assertTrue("/otherarray6" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray4.read(), [6, 7])
        self.assertEqual(self.fileh.root.otherarray5.read(), [7, 8])
        self.assertEqual(self.fileh.root.otherarray6.read(), [8, 9])
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh.root.otherarray3.title, "Another array 3")
        self.assertEqual(self.fileh.root.otherarray4.title, "Another array 4")
        self.assertEqual(self.fileh.root.otherarray5.title, "Another array 5")
        self.assertEqual(self.fileh.root.otherarray6.title, "Another array 6")

    def test04_6times3marksro(self):
        """Checking with six operations, three marks and do/undo in random
        order."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_6times3marksro..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Put a mark
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Unwind the previous mark
        self.fileh.undo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Put a mark in the middle of stack
        if common.verbose:
            print("All nodes:", self.fileh.walk_nodes())
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray5', [7, 8], "Another array 5")
        self.fileh.create_array('/', 'otherarray6', [8, 9], "Another array 6")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" in self.fileh)
        self.assertTrue("/otherarray6" in self.fileh)
        # Unwind previous mark
        self.fileh.undo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" not in self.fileh)
        self.assertTrue("/otherarray6" not in self.fileh)
        # Redo until the last mark
        self.fileh.redo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" in self.fileh)
        self.assertTrue("/otherarray6" in self.fileh)
        # Redo until the next mark (non-existent, so no action)
        self._doReopen()
        self.fileh.redo()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        self.assertTrue("/otherarray5" in self.fileh)
        self.assertTrue("/otherarray6" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray5.read(), [7, 8])
        self.assertEqual(self.fileh.root.otherarray6.read(), [8, 9])
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh.root.otherarray5.title, "Another array 5")
        self.assertEqual(self.fileh.root.otherarray6.title, "Another array 6")

    def test05_destructive(self):
        """Checking with a destructive action during undo."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_destructive..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        # Put a mark
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        # Do the destructive operation
        self._doReopen()
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        # Check objects
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray3.title, "Another array 3")

    def test05b_destructive(self):
        """Checking with a destructive action during undo (II)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_destructive..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        # Put a mark
        self._doReopen()
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        # Do the destructive operation
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        # Put a mark
        self._doReopen()
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        self.assertTrue("/otherarray4" in self.fileh)
        # Now undo the past operation
        self.fileh.undo()
        # Check objects
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray3.title, "Another array 3")
        self.assertTrue("/otherarray4" not in self.fileh)

    def test05c_destructive(self):
        """Checking with a destructive action during undo (III)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05c_destructive..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        # Put a mark
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        # Do the destructive operation
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        # Put a mark
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        self.assertTrue("/otherarray4" in self.fileh)
        # Now unwind twice
        self.fileh.undo()
        self._doReopen()
        self.fileh.undo()
        # Check objects
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)

    def test05d_destructive(self):
        """Checking with a destructive action during undo (IV)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05d_destructive..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        # Put a mark
        self._doReopen()
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        # Do the destructive operation
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        # Put a mark
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        self.assertTrue("/otherarray4" in self.fileh)
        # Now, go to the first mark
        self._doReopen()
        self.fileh.undo(0)
        # Check objects
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)

    def test05e_destructive(self):
        """Checking with a destructive action during undo (V)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05e_destructive..." %
                  self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        # Put a mark
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        self._doReopen()
        # Do the destructive operation
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        # Now, unwind the actions
        self.fileh.undo(0)
        self._doReopen()
        # Check objects
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)

    def test05f_destructive(self):
        "Checking with a destructive creation of existing node during undo"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05f_destructive..." %
                  self.__class__.__name__)

        self.fileh.enable_undo()
        self.fileh.create_array('/', 'newarray', [1])
        self.fileh.undo()
        self._doReopen()
        self.assertTrue('/newarray' not in self.fileh)
        newarr = self.fileh.create_array('/', 'newarray', [1])
        self.fileh.undo()
        self.assertTrue('/newarray' not in self.fileh)
        self._doReopen()
        self.fileh.redo()
        self.assertTrue('/newarray' in self.fileh)
        if not self._reopen:
            self.assertTrue(self.fileh.root.newarray is newarr)

    def test06_totalunwind(self):
        """Checking do/undo (total unwind)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06_totalunwind..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operations
        self._doReopen()
        self.fileh.undo(0)
        self.assertTrue("/otherarray" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)

    def test07_totalrewind(self):
        """Checking do/undo (total rewind)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_totalunwind..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        self.fileh.mark()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operations
        self.fileh.undo(0)
        # Redo all the operations
        self._doReopen()
        self.fileh.redo(-1)
        # Check that objects has come back to life in a sane state
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray.title, "Another array")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")

    def test08_marknames(self):
        """Checking mark names."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_marknames..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self.fileh.mark("first")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        self.fileh.mark("second")
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self.fileh.mark("third")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Now go to mark "first"
        self.fileh.undo("first")
        self._doReopen()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to mark "third"
        self.fileh.redo("third")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Now go to mark "second"
        self.fileh.undo("second")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to the end
        self._doReopen()
        self.fileh.redo(-1)
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        # Check that objects has come back to life in a sane state
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray4.read(), [6, 7])

    def test08_initialmark(self):
        """Checking initial mark."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_initialmark..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        initmid = self.fileh.get_current_mark()
        # Create a new array
        self.fileh.create_array('/', 'otherarray', [3, 4], "Another array")
        self.fileh.mark()
        self._doReopen()
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        # Now undo the past operations
        self.fileh.undo(initmid)
        self.assertTrue("/otherarray" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        # Redo all the operations
        self.fileh.redo(-1)
        self._doReopen()
        # Check that objects has come back to life in a sane state
        self.assertTrue("/otherarray" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray.title, "Another array")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")

    def test09_marknames(self):
        """Checking mark names (wrong direction)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09_marknames..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self.fileh.mark("first")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        self.fileh.mark("second")
        self._doReopen()
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self.fileh.mark("third")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Now go to mark "first"
        self.fileh.undo("first")
        # Try to undo up to mark "third"
        try:
            self.fileh.undo("third")
        except UndoRedoError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next UndoRedoError was catched!")
                print(value)
        else:
            self.fail("expected an UndoRedoError")
        # Now go to mark "third"
        self.fileh.redo("third")
        self._doReopen()
        # Try to redo up to mark "second"
        try:
            self.fileh.redo("second")
        except UndoRedoError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next UndoRedoError was catched!")
                print(value)
        else:
            self.fail("expected an UndoRedoError")
        # Final checks
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)

    def test10_goto(self):
        """Checking mark names (goto)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10_goto..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self._doReopen()
        self.fileh.mark("first")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        self.fileh.mark("second")
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self._doReopen()
        self.fileh.mark("third")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Now go to mark "first"
        self.fileh.goto("first")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to mark "third"
        self.fileh.goto("third")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Now go to mark "second"
        self._doReopen()
        self.fileh.goto("second")
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to the end
        self.fileh.goto(-1)
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        # Check that objects has come back to life in a sane state
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray4.read(), [6, 7])

    def test10_gotoint(self):
        """Checking mark sequential ids (goto)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10_gotoint..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [3, 4], "Another array 1")
        self.fileh.mark("first")
        self.fileh.create_array('/', 'otherarray2', [4, 5], "Another array 2")
        self.fileh.mark("second")
        self._doReopen()
        self.fileh.create_array('/', 'otherarray3', [5, 6], "Another array 3")
        self.fileh.mark("third")
        self.fileh.create_array('/', 'otherarray4', [6, 7], "Another array 4")
        # Now go to mark "first"
        self.fileh.goto(1)
        self._doReopen()
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to beginning
        self.fileh.goto(0)
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to mark "third"
        self._doReopen()
        self.fileh.goto(3)
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Now go to mark "second"
        self.fileh.goto(2)
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        self.assertTrue("/otherarray4" not in self.fileh)
        # Go to the end
        self._doReopen()
        self.fileh.goto(-1)
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertTrue("/otherarray4" in self.fileh)
        # Check that objects has come back to life in a sane state
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.read(), [3, 4])
        self.assertEqual(self.fileh.root.otherarray2.read(), [4, 5])
        self.assertEqual(self.fileh.root.otherarray3.read(), [5, 6])
        self.assertEqual(self.fileh.root.otherarray4.read(), [6, 7])

    def test11_contiguous(self):
        "Creating contiguous marks"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11_contiguous..." % self.__class__.__name__)

        self.fileh.enable_undo()
        m1 = self.fileh.mark()
        m2 = self.fileh.mark()
        self.assertNotEqual(m1, m2)
        self._doReopen()
        self.fileh.undo(m1)
        self.assertEqual(self.fileh.get_current_mark(), m1)
        self.fileh.redo(m2)
        self.assertEqual(self.fileh.get_current_mark(), m2)
        self.fileh.goto(m1)
        self.assertEqual(self.fileh.get_current_mark(), m1)
        self.fileh.goto(m2)
        self.assertEqual(self.fileh.get_current_mark(), m2)
        self.fileh.goto(-1)
        self._doReopen()
        self.assertEqual(self.fileh.get_current_mark(), m2)
        self.fileh.goto(0)
        self.assertEqual(self.fileh.get_current_mark(), 0)

    def test12_keepMark(self):
        "Ensuring the mark is kept after an UNDO operation"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test12_keepMark..." % self.__class__.__name__)

        self.fileh.enable_undo()
        self.fileh.create_array('/', 'newarray1', [1])

        mid = self.fileh.mark()
        self.assertTrue(mid is not None)
        self._doReopen()
        self.fileh.undo()
        # We should have moved to the initial mark.
        self.assertEqual(self.fileh.get_current_mark(), 0)
        # So /newarray1 should not be there.
        self.assertTrue('/newarray1' not in self.fileh)

    def test13_severalEnableDisable(self):
        "Checking that successive enable/disable Undo works"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13_severalEnableDisable..." %
                  self.__class__.__name__)

        self.fileh.enable_undo()
        self.fileh.create_array('/', 'newarray1', [1])
        self.fileh.undo()
        self._doReopen()
        # We should have moved to 'mid' mark, not the initial mark.
        self.assertEqual(self.fileh.get_current_mark(), 0)
        # So /newarray1 should still be there.
        self.assertTrue('/newarray1' not in self.fileh)
        # Close this do/undo session
        self.fileh.disable_undo()
        # Do something
        self.fileh.create_array('/', 'newarray2', [1])

        # Enable again do/undo
        self.fileh.enable_undo()
        self.fileh.create_array('/', 'newarray3', [1])
        mid = self.fileh.mark()
        self.fileh.create_array('/', 'newarray4', [1])
        self.fileh.undo()

        # We should have moved to 'mid' mark, not the initial mark.
        self.assertEqual(self.fileh.get_current_mark(), mid)

        # So /newarray2 and /newarray3 should still be there.
        self.assertTrue('/newarray1' not in self.fileh)
        self.assertTrue('/newarray2' in self.fileh)
        self.assertTrue('/newarray3' in self.fileh)
        self.assertTrue('/newarray4' not in self.fileh)

        # Close this do/undo session
        self._doReopen()
        self.fileh.disable_undo()

        # Enable again do/undo
        self.fileh.enable_undo()
        self.fileh.create_array('/', 'newarray1', [1])
        self.fileh.create_array('/', 'newarray4', [1])

        # So /newarray2 and /newarray3 should still be there.
        self.assertTrue('/newarray1' in self.fileh)
        self.assertTrue('/newarray2' in self.fileh)
        self.assertTrue('/newarray3' in self.fileh)
        self.assertTrue('/newarray4' in self.fileh)
        self.fileh.undo()
        self._doReopen()
        self.assertTrue('/newarray1' not in self.fileh)
        self.assertTrue('/newarray2' in self.fileh)
        self.assertTrue('/newarray3' in self.fileh)
        self.assertTrue('/newarray4' not in self.fileh)
        # Close this do/undo session
        self.fileh.disable_undo()


class PersistenceTestCase(BasicTestCase):

    """Test for basic Undo/Redo operations with persistence."""

    _reopen = True


class createArrayTestCase(unittest.TestCase):
    "Test for create_array operations"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Checking one action."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [1, 2], "Another array 1")
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/otherarray1" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.otherarray1.read(), [1, 2])

    def test01(self):
        """Checking two actions."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [1, 2], "Another array 1")
        self.fileh.create_array('/', 'otherarray2', [2, 3], "Another array 2")
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh.root.otherarray1.read(), [1, 2])
        self.assertEqual(self.fileh.root.otherarray2.read(), [2, 3])

    def test02(self):
        """Checking three actions."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [1, 2], "Another array 1")
        self.fileh.create_array('/', 'otherarray2', [2, 3], "Another array 2")
        self.fileh.create_array('/', 'otherarray3', [3, 4], "Another array 3")
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/otherarray2" not in self.fileh)
        self.assertTrue("/otherarray3" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/otherarray2" in self.fileh)
        self.assertTrue("/otherarray3" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.otherarray2.title, "Another array 2")
        self.assertEqual(self.fileh.root.otherarray3.title, "Another array 3")
        self.assertEqual(self.fileh.root.otherarray1.read(), [1, 2])
        self.assertEqual(self.fileh.root.otherarray2.read(), [2, 3])
        self.assertEqual(self.fileh.root.otherarray3.read(), [3, 4])

    def test03(self):
        """Checking three actions in different depth levels."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.create_array('/', 'otherarray1', [1, 2], "Another array 1")
        self.fileh.create_array('/agroup', 'otherarray2', [
                                2, 3], "Another array 2")
        self.fileh.create_array('/agroup/agroup3', 'otherarray3', [
                                3, 4], "Another array 3")
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/otherarray1" not in self.fileh)
        self.assertTrue("/agroup/otherarray2" not in self.fileh)
        self.assertTrue("/agroup/agroup3/otherarray3" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/otherarray1" in self.fileh)
        self.assertTrue("/agroup/otherarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3/otherarray3" in self.fileh)
        self.assertEqual(self.fileh.root.otherarray1.title, "Another array 1")
        self.assertEqual(self.fileh.root.agroup.otherarray2.title,
                         "Another array 2")
        self.assertEqual(self.fileh.root.agroup.agroup3.otherarray3.title,
                         "Another array 3")
        self.assertEqual(self.fileh.root.otherarray1.read(), [1, 2])
        self.assertEqual(self.fileh.root.agroup.otherarray2.read(), [2, 3])
        self.assertEqual(self.fileh.root.agroup.agroup3.otherarray3.read(),
                         [3, 4])


class createGroupTestCase(unittest.TestCase):
    "Test for create_group operations"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Checking one action."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new group
        self.fileh.create_group('/', 'othergroup1', "Another group 1")
        # Now undo the past operation
        self.fileh.undo()
        # Check that othergroup1 does not exist in the object tree
        self.assertTrue("/othergroup1" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that othergroup1 has come back to life in a sane state
        self.assertTrue("/othergroup1" in self.fileh)
        self.assertEqual(self.fileh.root.othergroup1._v_title,
                         "Another group 1")

    def test01(self):
        """Checking two actions."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new group
        self.fileh.create_group('/', 'othergroup1', "Another group 1")
        self.fileh.create_group('/', 'othergroup2', "Another group 2")
        # Now undo the past operation
        self.fileh.undo()
        # Check that othergroup does not exist in the object tree
        self.assertTrue("/othergroup1" not in self.fileh)
        self.assertTrue("/othergroup2" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that othergroup* has come back to life in a sane state
        self.assertTrue("/othergroup1" in self.fileh)
        self.assertTrue("/othergroup2" in self.fileh)
        self.assertEqual(self.fileh.root.othergroup1._v_title,
                         "Another group 1")
        self.assertEqual(self.fileh.root.othergroup2._v_title,
                         "Another group 2")

    def test02(self):
        """Checking three actions."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new group
        self.fileh.create_group('/', 'othergroup1', "Another group 1")
        self.fileh.create_group('/', 'othergroup2', "Another group 2")
        self.fileh.create_group('/', 'othergroup3', "Another group 3")
        # Now undo the past operation
        self.fileh.undo()
        # Check that othergroup* does not exist in the object tree
        self.assertTrue("/othergroup1" not in self.fileh)
        self.assertTrue("/othergroup2" not in self.fileh)
        self.assertTrue("/othergroup3" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that othergroup* has come back to life in a sane state
        self.assertTrue("/othergroup1" in self.fileh)
        self.assertTrue("/othergroup2" in self.fileh)
        self.assertTrue("/othergroup3" in self.fileh)
        self.assertEqual(self.fileh.root.othergroup1._v_title,
                         "Another group 1")
        self.assertEqual(self.fileh.root.othergroup2._v_title,
                         "Another group 2")
        self.assertEqual(self.fileh.root.othergroup3._v_title,
                         "Another group 3")

    def test03(self):
        """Checking three actions in different depth levels."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new group
        self.fileh.create_group('/', 'othergroup1', "Another group 1")
        self.fileh.create_group(
            '/othergroup1', 'othergroup2', "Another group 2")
        self.fileh.create_group(
            '/othergroup1/othergroup2', 'othergroup3', "Another group 3")
        # Now undo the past operation
        self.fileh.undo()
        # Check that othergroup* does not exist in the object tree
        self.assertTrue("/othergroup1" not in self.fileh)
        self.assertTrue("/othergroup1/othergroup2" not in self.fileh)
        self.assertTrue(
            "/othergroup1/othergroup2/othergroup3" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that othergroup* has come back to life in a sane state
        self.assertTrue("/othergroup1" in self.fileh)
        self.assertTrue("/othergroup1/othergroup2" in self.fileh)
        self.assertTrue("/othergroup1/othergroup2/othergroup3" in self.fileh)
        self.assertEqual(self.fileh.root.othergroup1._v_title,
                         "Another group 1")
        self.assertEqual(self.fileh.root.othergroup1.othergroup2._v_title,
                         "Another group 2")
        self.assertEqual(
            self.fileh.root.othergroup1.othergroup2.othergroup3._v_title,
            "Another group 3")


minRowIndex = 10


def populateTable(where, name):
    "Create a table under where with name name"

    class Indexed(IsDescription):
        var1 = StringCol(itemsize=4, dflt=b"", pos=1)
        var2 = BoolCol(dflt=0, pos=2)
        var3 = IntCol(dflt=0, pos=3)
        var4 = FloatCol(dflt=0, pos=4)

    nrows = minRowIndex
    table = where._v_file.create_table(where, name, Indexed, "Indexed",
                                       None, nrows)
    for i in range(nrows):
        table.row['var1'] = str(i)
        # table.row['var2'] = i > 2
        table.row['var2'] = i % 2
        table.row['var3'] = i
        table.row['var4'] = float(nrows - i - 1)
        table.row.append()
    table.flush()

    # Index all entries:
    indexrows = table.cols.var1.create_index()
    indexrows = table.cols.var2.create_index()
    indexrows = table.cols.var3.create_index()
    # Do not index the var4 column
    # indexrows = table.cols.var4.create_index()
    if common.verbose:
        print("Number of written rows:", nrows)
        print("Number of indexed rows:", table.cols.var1.index.nelements)
        print("Number of indexed rows(2):", indexrows)


class renameNodeTestCase(unittest.TestCase):
    "Test for rename_node operations"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

        # Create a table in root
        populateTable(self.fileh.root, 'table')

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Checking rename_node (over Groups without children)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.rename_node('/agroup2', 'agroup3')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/agroup2" in self.fileh)
        self.assertTrue("/agroup3" not in self.fileh)
        self.assertEqual(self.fileh.root.agroup2._v_title, "Group title 2")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/agroup2" not in self.fileh)
        self.assertTrue("/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup3._v_title, "Group title 2")

    def test01(self):
        """Checking rename_node (over Groups with children)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.rename_node('/agroup', 'agroup3')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup3" not in self.fileh)
        # Check that children are reachable
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup3._v_title, "Group title")
        # Check that children are reachable
        self.assertTrue("/agroup3/anarray1" in self.fileh)
        self.assertTrue("/agroup3/anarray2" in self.fileh)
        self.assertTrue("/agroup3/agroup3" in self.fileh)

    def test01b(self):
        """Checking rename_node (over Groups with children 2)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.rename_node('/agroup', 'agroup3')
        self.fileh.rename_node('/agroup3', 'agroup4')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup4" not in self.fileh)
        # Check that children are reachable
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup4" in self.fileh)
        self.assertEqual(self.fileh.root.agroup4._v_title, "Group title")
        # Check that children are reachable
        self.assertTrue("/agroup4/anarray1" in self.fileh)
        self.assertTrue("/agroup4/anarray2" in self.fileh)
        self.assertTrue("/agroup4/agroup3" in self.fileh)

    def test02(self):
        """Checking rename_node (over Leaves)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.rename_node('/anarray', 'anarray2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/anarray" in self.fileh)
        self.assertTrue("/anarray2" not in self.fileh)
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/anarray" not in self.fileh)
        self.assertTrue("/anarray2" in self.fileh)
        self.assertEqual(self.fileh.root.anarray2.title, "Array title")

    def test03(self):
        """Checking rename_node (over Tables)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.rename_node('/table', 'table2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that table2 does not exist in the object tree
        self.assertTrue("/table" in self.fileh)
        table = self.fileh.root.table
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertTrue(table.cols.var4.index is None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertTrue("/table2" not in self.fileh)
        self.assertEqual(self.fileh.root.table.title, "Indexed")
        # Redo the operation
        self.fileh.redo()
        # Check that table2 has come back to life in a sane state
        self.assertTrue("/table" not in self.fileh)
        self.assertTrue("/table2" in self.fileh)
        self.assertEqual(self.fileh.root.table2.title, "Indexed")
        table = self.fileh.root.table2
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertTrue(table.cols.var4.index is None)


class moveNodeTestCase(unittest.TestCase):
    "Tests for move_node operations"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

        # Create a table in root
        populateTable(self.fileh.root, 'table')

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Checking move_node (over Leaf)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.move_node('/anarray', '/agroup/agroup3')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/anarray" in self.fileh)
        self.assertTrue("/agroup/agroup3/anarray" not in self.fileh)
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/anarray" not in self.fileh)
        self.assertTrue("/agroup/agroup3/anarray" in self.fileh)
        self.assertEqual(self.fileh.root.agroup.agroup3.anarray.title,
                         "Array title")

    def test01(self):
        """Checking move_node (over Groups with children)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.move_node('/agroup', '/agroup2', 'agroup3')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup2/agroup3" not in self.fileh)
        # Check that children are reachable
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup2/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup2.agroup3._v_title,
                         "Group title")
        # Check that children are reachable
        self.assertTrue("/agroup2/agroup3/anarray1" in self.fileh)
        self.assertTrue("/agroup2/agroup3/anarray2" in self.fileh)
        self.assertTrue("/agroup2/agroup3/agroup3" in self.fileh)

    def test01b(self):
        """Checking move_node (over Groups with children 2)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.move_node('/agroup', '/', 'agroup3')
        self.fileh.move_node('/agroup3', '/agroup2', 'agroup4')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does not exist in the object tree
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup2/agroup4" not in self.fileh)
        # Check that children are reachable
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup2/agroup4" in self.fileh)
        self.assertEqual(self.fileh.root.agroup2.agroup4._v_title,
                         "Group title")
        # Check that children are reachable
        self.assertTrue("/agroup2/agroup4/anarray1" in self.fileh)
        self.assertTrue("/agroup2/agroup4/anarray2" in self.fileh)
        self.assertTrue("/agroup2/agroup4/agroup3" in self.fileh)

    def test02(self):
        """Checking move_node (over Leaves)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.move_node('/anarray', '/agroup2', 'anarray2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that otherarray does not exist in the object tree
        self.assertTrue("/anarray" in self.fileh)
        self.assertTrue("/agroup2/anarray2" not in self.fileh)
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        # Redo the operation
        self.fileh.redo()
        # Check that otherarray has come back to life in a sane state
        self.assertTrue("/anarray" not in self.fileh)
        self.assertTrue("/agroup2/anarray2" in self.fileh)
        self.assertEqual(self.fileh.root.agroup2.anarray2.title, "Array title")

    def test03(self):
        """Checking move_node (over Tables)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.move_node('/table', '/agroup2', 'table2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that table2 does not exist in the object tree
        self.assertTrue("/table" in self.fileh)
        self.assertTrue("/agroup2/table2" not in self.fileh)
        table = self.fileh.root.table
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertTrue(table.cols.var4.index is None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertEqual(self.fileh.root.table.title, "Indexed")
        # Redo the operation
        self.fileh.redo()
        # Check that table2 has come back to life in a sane state
        self.assertTrue("/table" not in self.fileh)
        self.assertTrue("/agroup2/table2" in self.fileh)
        self.assertEqual(self.fileh.root.agroup2.table2.title, "Indexed")
        table = self.fileh.root.agroup2.table2
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertTrue(table.cols.var4.index is None)


class removeNodeTestCase(unittest.TestCase):
    "Test for remove_node operations"

    def setUp(self):
        # Create an HDF5 file
        # self.file = "/tmp/test.h5"
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

        # Create a table in root
        populateTable(self.fileh.root, 'table')

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Checking remove_node (over Leaf)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Delete an existing array
        self.fileh.remove_node('/anarray')
        # Now undo the past operation
        self.fileh.undo()
        # Check that it does exist in the object tree
        self.assertTrue("/anarray" in self.fileh)
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        # Redo the operation
        self.fileh.redo()
        # Check that array has gone again
        self.assertTrue("/anarray" not in self.fileh)

    def test00b(self):
        """Checking remove_node (over several Leaves)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00b..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Delete a couple of arrays
        self.fileh.remove_node('/anarray')
        self.fileh.remove_node('/agroup/anarray2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that arrays has come into life
        self.assertTrue("/anarray" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        self.assertEqual(
            self.fileh.root.agroup.anarray2.title, "Array title 2")
        # Redo the operation
        self.fileh.redo()
        # Check that arrays has disappeared again
        self.assertTrue("/anarray" not in self.fileh)
        self.assertTrue("/agroup/anarray2" not in self.fileh)

    def test00c(self):
        """Checking remove_node (over Tables)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00c..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Create a new array
        self.fileh.remove_node('/table')
        # Now undo the past operation
        self.fileh.undo()
        # Check that table2 does not exist in the object tree
        self.assertTrue("/table" in self.fileh)
        table = self.fileh.root.table
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertTrue(table.cols.var4.index is None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertEqual(self.fileh.root.table.title, "Indexed")
        # Redo the operation
        self.fileh.redo()
        # Check that table2 has come back to life in a sane state
        self.assertTrue("/table" not in self.fileh)

    def test01(self):
        """Checking remove_node (over Groups with children)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Delete a group recursively
        self.fileh.remove_node('/agroup', recursive=1)
        # Now undo the past operation
        self.fileh.undo()
        # Check that parent and children has come into life in a sane state
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that parent and children are not reachable
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup/anarray1" not in self.fileh)
        self.assertTrue("/agroup/anarray2" not in self.fileh)
        self.assertTrue("/agroup/agroup3" not in self.fileh)

    def test01b(self):
        """Checking remove_node (over Groups with children 2)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # Remove a couple of groups
        self.fileh.remove_node('/agroup', recursive=1)
        self.fileh.remove_node('/agroup2')
        # Now undo the past operation
        self.fileh.undo()
        # Check that they does exist in the object tree
        self.assertTrue("/agroup" in self.fileh)
        self.assertTrue("/agroup2" in self.fileh)
        # Check that children are reachable
        self.assertTrue("/agroup/anarray1" in self.fileh)
        self.assertTrue("/agroup/anarray2" in self.fileh)
        self.assertTrue("/agroup/agroup3" in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        # Redo the operation
        self.fileh.redo()
        # Check that groups does not exist again
        self.assertTrue("/agroup" not in self.fileh)
        self.assertTrue("/agroup2" not in self.fileh)
        # Check that children are not reachable
        self.assertTrue("/agroup/anarray1" not in self.fileh)
        self.assertTrue("/agroup/anarray2" not in self.fileh)
        self.assertTrue("/agroup/agroup3" not in self.fileh)


class copyNodeTestCase(unittest.TestCase):
    "Tests for copy_node and copy_children operations"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root

        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

        # Create a table in root
        populateTable(self.fileh.root, 'table')

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_copyLeaf(self):
        """Checking copy_node (over Leaves)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_copyLeaf..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # /anarray => /agroup/agroup3/
        new_node = self.fileh.copy_node('/anarray', '/agroup/agroup3')

        # Undo the copy.
        self.fileh.undo()
        # Check that the copied node does not exist in the object tree.
        self.assertTrue('/agroup/agroup3/anarray' not in self.fileh)

        # Redo the copy.
        self.fileh.redo()
        # Check that the copied node exists again in the object tree.
        self.assertTrue('/agroup/agroup3/anarray' in self.fileh)
        self.assertTrue(self.fileh.root.agroup.agroup3.anarray is new_node)

    def test00b_copyTable(self):
        """Checking copy_node (over Tables)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00b_copyTable..." % self.__class__.__name__)

        # open the do/undo
        self.fileh.enable_undo()
        # /table => /agroup/agroup3/
        warnings.filterwarnings("ignore", category=UserWarning)
        table = self.fileh.copy_node(
            '/table', '/agroup/agroup3', propindexes=True)
        warnings.filterwarnings("default", category=UserWarning)
        self.assertTrue("/agroup/agroup3/table" in self.fileh)

        table = self.fileh.root.agroup.agroup3.table
        self.assertEqual(table.title, "Indexed")
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertTrue(table.cols.var4.index is None)
        # Now undo the past operation
        self.fileh.undo()
        table = self.fileh.root.table
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertTrue(table.cols.var4.index is None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        # Check that the copied node does not exist in the object tree.
        self.assertTrue("/agroup/agroup3/table" not in self.fileh)
        # Redo the operation
        self.fileh.redo()
        # Check that table has come back to life in a sane state
        self.assertTrue("/table" in self.fileh)
        self.assertTrue("/agroup/agroup3/table" in self.fileh)
        table = self.fileh.root.agroup.agroup3.table
        self.assertEqual(table.title, "Indexed")
        self.assertTrue(table.cols.var1.index is not None)
        self.assertTrue(table.cols.var2.index is not None)
        self.assertTrue(table.cols.var3.index is not None)
        self.assertEqual(table.cols.var1.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var2.index.nelements, minRowIndex)
        self.assertEqual(table.cols.var3.index.nelements, minRowIndex)
        self.assertTrue(table.cols.var4.index is None)

    def test01_copyGroup(self):
        "Copying a group (recursively)."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_copyGroup..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # /agroup => /acopy
        new_node = self.fileh.copy_node(
            '/agroup', newname='acopy', recursive=True)

        # Undo the copy.
        self.fileh.undo()
        # Check that the copied node does not exist in the object tree.
        self.assertTrue('/acopy' not in self.fileh)
        self.assertTrue('/acopy/anarray1' not in self.fileh)
        self.assertTrue('/acopy/anarray2' not in self.fileh)
        self.assertTrue('/acopy/agroup3' not in self.fileh)

        # Redo the copy.
        self.fileh.redo()
        # Check that the copied node exists again in the object tree.
        self.assertTrue('/acopy' in self.fileh)
        self.assertTrue('/acopy/anarray1' in self.fileh)
        self.assertTrue('/acopy/anarray2' in self.fileh)
        self.assertTrue('/acopy/agroup3' in self.fileh)
        self.assertTrue(self.fileh.root.acopy is new_node)

    def test02_copyLeafOverwrite(self):
        "Copying a leaf, overwriting destination."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copyLeafOverwrite..." %
                  self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # /anarray => /agroup/agroup
        oldNode = self.fileh.root.agroup
        new_node = self.fileh.copy_node(
            '/anarray', newname='agroup', overwrite=True)

        # Undo the copy.
        self.fileh.undo()
        # Check that the copied node does not exist in the object tree.
        # Check that the overwritten node exists again in the object tree.
        self.assertTrue(self.fileh.root.agroup is oldNode)

        # Redo the copy.
        self.fileh.redo()
        # Check that the copied node exists again in the object tree.
        # Check that the overwritten node does not exist in the object tree.
        self.assertTrue(self.fileh.root.agroup is new_node)

    def test03_copyChildren(self):
        "Copying the children of a group."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_copyChildren..." %
                  self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # /agroup/* => /agroup/
        self.fileh.copy_children('/agroup', '/agroup2', recursive=True)

        # Undo the copy.
        self.fileh.undo()
        # Check that the copied nodes do not exist in the object tree.
        self.assertTrue('/agroup2/anarray1' not in self.fileh)
        self.assertTrue('/agroup2/anarray2' not in self.fileh)
        self.assertTrue('/agroup2/agroup3' not in self.fileh)

        # Redo the copy.
        self.fileh.redo()
        # Check that the copied nodes exist again in the object tree.
        self.assertTrue('/agroup2/anarray1' in self.fileh)
        self.assertTrue('/agroup2/anarray2' in self.fileh)
        self.assertTrue('/agroup2/agroup3' in self.fileh)


class ComplexTestCase(unittest.TestCase):
    "Tests for a mix of all operations"

    def setUp(self):
        # Create an HDF5 file
        # self.file = "/tmp/test.h5"
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w", title="File title")
        fileh = self.fileh
        root = fileh.root
        # Create an array
        fileh.create_array(root, 'array', [1, 2], title="Title example")

        # Create another array object
        fileh.create_array(root, 'anarray', [1], "Array title")

        # Create a group object
        group = fileh.create_group(root, 'agroup', "Group title")

        # Create a couple of objects there
        fileh.create_array(group, 'anarray1', [2], "Array title 1")
        fileh.create_array(group, 'anarray2', [2], "Array title 2")

        # Create a lonely group in first level
        fileh.create_group(root, 'agroup2', "Group title 2")

        # Create a new group in the second level
        fileh.create_group(group, 'agroup3', "Group title 3")

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00(self):
        """Mix of create_array, create_group, renameNone, move_node,
        remove_node, copy_node and copy_children."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # Create an array
        self.fileh.create_array(self.fileh.root, 'anarray3',
                                [1], "Array title 3")
        # Create a group
        self.fileh.create_group(self.fileh.root, 'agroup3', "Group title 3")
        # /anarray => /agroup/agroup3/
        new_node = self.fileh.copy_node('/anarray3', '/agroup/agroup3')
        new_node = self.fileh.copy_children('/agroup', '/agroup3', recursive=1)
        # rename anarray
        self.fileh.rename_node('/anarray', 'anarray4')
        # Move anarray
        new_node = self.fileh.copy_node('/anarray3', '/agroup')
        # Remove anarray4
        self.fileh.remove_node('/anarray4')
        # Undo the actions
        self.fileh.undo()
        self.assertTrue('/anarray4' not in self.fileh)
        self.assertTrue('/anarray3' not in self.fileh)
        self.assertTrue('/agroup/agroup3/anarray3' not in self.fileh)
        self.assertTrue('/agroup3' not in self.fileh)
        self.assertTrue('/anarray4' not in self.fileh)
        self.assertTrue('/anarray' in self.fileh)

        # Redo the actions
        self.fileh.redo()
        # Check that the copied node exists again in the object tree.
        self.assertTrue('/agroup/agroup3/anarray3' in self.fileh)
        self.assertTrue('/agroup/anarray3' in self.fileh)
        self.assertTrue('/agroup3/agroup3/anarray3' in self.fileh)
        self.assertTrue('/agroup3/anarray3' not in self.fileh)
        self.assertTrue(self.fileh.root.agroup.anarray3 is new_node)
        self.assertTrue('/anarray' not in self.fileh)
        self.assertTrue('/anarray4' not in self.fileh)

    def test01(self):
        "Test with multiple generations (Leaf case)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # remove /anarray
        self.fileh.remove_node('/anarray')
        # Create an array in the same place
        self.fileh.create_array(self.fileh.root, 'anarray',
                                [2], "Array title 2")
        # remove the array again
        self.fileh.remove_node('/anarray')
        # Create an array
        self.fileh.create_array(self.fileh.root, 'anarray',
                                [3], "Array title 3")
        # remove the array again
        self.fileh.remove_node('/anarray')
        # Create an array
        self.fileh.create_array(self.fileh.root, 'anarray',
                                [4], "Array title 4")
        # Undo the actions
        self.fileh.undo()
        # Check that /anarray is in the correct state before redoing
        self.assertEqual(self.fileh.root.anarray.title, "Array title")
        self.assertEqual(self.fileh.root.anarray[:], [1])
        # Redo the actions
        self.fileh.redo()
        self.assertEqual(self.fileh.root.anarray.title, "Array title 4")
        self.assertEqual(self.fileh.root.anarray[:], [4])

    def test02(self):
        "Test with multiple generations (Group case)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # remove /agroup
        self.fileh.remove_node('/agroup2')
        # Create a group in the same place
        self.fileh.create_group(self.fileh.root, 'agroup2', "Group title 22")
        # remove the group
        self.fileh.remove_node('/agroup2')
        # Create a group
        self.fileh.create_group(self.fileh.root, 'agroup2', "Group title 3")
        # remove the group
        self.fileh.remove_node('/agroup2')
        # Create a group
        self.fileh.create_group(self.fileh.root, 'agroup2', "Group title 4")
        # Create a child group
        self.fileh.create_group(self.fileh.root.agroup2, 'agroup5',
                                "Group title 5")
        # Undo the actions
        self.fileh.undo()
        # Check that /agroup is in the state before enabling do/undo
        self.assertEqual(self.fileh.root.agroup2._v_title, "Group title 2")
        self.assertTrue('/agroup2' in self.fileh)
        # Redo the actions
        self.fileh.redo()
        self.assertEqual(self.fileh.root.agroup2._v_title, "Group title 4")
        self.assertEqual(self.fileh.root.agroup2.agroup5._v_title,
                         "Group title 5")

    def test03(self):
        "Test with multiple generations (Group case, recursive remove)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # remove /agroup
        self.fileh.remove_node('/agroup', recursive=1)
        # Create a group in the same place
        self.fileh.create_group(self.fileh.root, 'agroup', "Group title 2")
        # remove the group
        self.fileh.remove_node('/agroup')
        # Create a group
        self.fileh.create_group(self.fileh.root, 'agroup', "Group title 3")
        # remove the group
        self.fileh.remove_node('/agroup')
        # Create a group
        self.fileh.create_group(self.fileh.root, 'agroup', "Group title 4")
        # Create a child group
        self.fileh.create_group(self.fileh.root.agroup, 'agroup5',
                                "Group title 5")
        # Undo the actions
        self.fileh.undo()
        # Check that /agroup is in the state before enabling do/undo
        self.assertTrue('/agroup' in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title")
        self.assertTrue('/agroup/anarray1' in self.fileh)
        self.assertTrue('/agroup/anarray2' in self.fileh)
        self.assertTrue('/agroup/agroup3' in self.fileh)
        self.assertTrue('/agroup/agroup5' not in self.fileh)
        # Redo the actions
        self.fileh.redo()
        self.assertTrue('/agroup' in self.fileh)
        self.assertEqual(self.fileh.root.agroup._v_title, "Group title 4")
        self.assertTrue('/agroup/agroup5' in self.fileh)
        self.assertEqual(
            self.fileh.root.agroup.agroup5._v_title, "Group title 5")

    def test03b(self):
        "Test with multiple generations (Group case, recursive remove, case 2)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b..." % self.__class__.__name__)

        # Enable undo/redo.
        self.fileh.enable_undo()
        # Create a new group with a child
        self.fileh.create_group(self.fileh.root, 'agroup3', "Group title 3")
        self.fileh.create_group(self.fileh.root.agroup3, 'agroup4',
                                "Group title 4")
        # remove /agroup3
        self.fileh.remove_node('/agroup3', recursive=1)
        # Create a group in the same place
        self.fileh.create_group(self.fileh.root, 'agroup3', "Group title 4")
        # Undo the actions
        self.fileh.undo()
        # Check that /agroup is in the state before enabling do/undo
        self.assertTrue('/agroup3' not in self.fileh)
        # Redo the actions
        self.fileh.redo()
        self.assertEqual(self.fileh.root.agroup3._v_title, "Group title 4")
        self.assertTrue('/agroup3' in self.fileh)
        self.assertTrue('/agroup/agroup4' not in self.fileh)


class AttributesTestCase(unittest.TestCase):
    "Tests for operation on attributes"

    def setUp(self):
        # Create an HDF5 file
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(
            self.file, mode="w", title="Attribute operations")

        # Create an array.
        array = self.fileh.create_array('/', 'array', [1, 2])

        # Set some attributes on it.
        attrs = array.attrs
        attrs.attr_1 = 10
        attrs.attr_2 = 20
        attrs.attr_3 = 30

    def tearDown(self):
        # Remove the temporary file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_setAttr(self):
        "Setting a nonexistent attribute."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_setAttr..." % self.__class__.__name__)

        array = self.fileh.root.array
        attrs = array.attrs

        self.fileh.enable_undo()
        setattr(attrs, 'attr_0', 0)
        self.assertTrue('attr_0' in attrs)
        self.assertEqual(attrs.attr_0, 0)
        self.fileh.undo()
        self.assertTrue('attr_0' not in attrs)
        self.fileh.redo()
        self.assertTrue('attr_0' in attrs)
        self.assertEqual(attrs.attr_0, 0)

    def test01_setAttrExisting(self):
        "Setting an existing attribute."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_setAttrExisting..." %
                  self.__class__.__name__)

        array = self.fileh.root.array
        attrs = array.attrs

        self.fileh.enable_undo()
        setattr(attrs, 'attr_1', 11)
        self.assertTrue('attr_1' in attrs)
        self.assertEqual(attrs.attr_1, 11)
        self.fileh.undo()
        self.assertTrue('attr_1' in attrs)
        self.assertEqual(attrs.attr_1, 10)
        self.fileh.redo()
        self.assertTrue('attr_1' in attrs)
        self.assertEqual(attrs.attr_1, 11)

    def test02_delAttr(self):
        "Removing an attribute."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_delAttr..." % self.__class__.__name__)

        array = self.fileh.root.array
        attrs = array.attrs

        self.fileh.enable_undo()
        delattr(attrs, 'attr_1')
        self.assertTrue('attr_1' not in attrs)
        self.fileh.undo()
        self.assertTrue('attr_1' in attrs)
        self.assertEqual(attrs.attr_1, 10)
        self.fileh.redo()
        self.assertTrue('attr_1' not in attrs)

    def test03_copyNodeAttrs(self):
        "Copying an attribute set."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_copyNodeAttrs..." %
                  self.__class__.__name__)

        rattrs = self.fileh.root._v_attrs
        rattrs.attr_0 = 0
        rattrs.attr_1 = 100

        array = self.fileh.root.array
        attrs = array.attrs

        self.fileh.enable_undo()
        attrs._f_copy(self.fileh.root)
        self.assertEqual(rattrs.attr_0, 0)
        self.assertEqual(rattrs.attr_1, 10)
        self.assertEqual(rattrs.attr_2, 20)
        self.assertEqual(rattrs.attr_3, 30)
        self.fileh.undo()
        self.assertEqual(rattrs.attr_0, 0)
        self.assertEqual(rattrs.attr_1, 100)
        self.assertTrue('attr_2' not in rattrs)
        self.assertTrue('attr_3' not in rattrs)
        self.fileh.redo()
        self.assertEqual(rattrs.attr_0, 0)
        self.assertEqual(rattrs.attr_1, 10)
        self.assertEqual(rattrs.attr_2, 20)
        self.assertEqual(rattrs.attr_3, 30)

    def test04_replaceNode(self):
        "Replacing a node with a rewritten attribute."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_replaceNode..." % self.__class__.__name__)

        array = self.fileh.root.array
        attrs = array.attrs

        self.fileh.enable_undo()
        attrs.attr_1 = 11
        self.fileh.remove_node('/array')
        arr = self.fileh.create_array('/', 'array', [1])
        arr.attrs.attr_1 = 12
        self.fileh.undo()
        self.assertTrue('attr_1' in self.fileh.root.array.attrs)
        self.assertEqual(self.fileh.root.array.attrs.attr_1, 10)
        self.fileh.redo()
        self.assertTrue('attr_1' in self.fileh.root.array.attrs)
        self.assertEqual(self.fileh.root.array.attrs.attr_1, 12)


class NotLoggedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test not logged nodes."""

    class NotLoggedArray(NotLoggedMixin, Array):
        pass

    def test00_hierarchy(self):
        """Performing hierarchy operations on a not logged node."""

        self.h5file.create_group('/', 'tgroup')
        self.h5file.enable_undo()

        # Node creation is not undone.
        arr = self.NotLoggedArray(self.h5file.root, 'test',
                                  [1], self._getMethodName())
        self.h5file.undo()
        self.assertTrue('/test' in self.h5file)

        # Node movement is not undone.
        arr.move('/tgroup')
        self.h5file.undo()
        self.assertTrue('/tgroup/test' in self.h5file)

        # Node removal is not undone.
        arr.remove()
        self.h5file.undo()
        self.assertTrue('/tgroup/test' not in self.h5file)

    def test01_attributes(self):
        """Performing attribute operations on a not logged node."""

        arr = self.NotLoggedArray(self.h5file.root, 'test',
                                  [1], self._getMethodName())
        self.h5file.enable_undo()

        # Attribute creation is not undone.
        arr._v_attrs.foo = 'bar'
        self.h5file.undo()
        self.assertEqual(arr._v_attrs.foo, 'bar')

        # Attribute change is not undone.
        arr._v_attrs.foo = 'baz'
        self.h5file.undo()
        self.assertEqual(arr._v_attrs.foo, 'baz')

        # Attribute removal is not undone.
        del arr._v_attrs.foo
        self.h5file.undo()
        self.assertRaises(AttributeError, getattr, arr._v_attrs, 'foo')


class CreateParentsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test the ``createparents`` flag."""

    def setUp(self):
        super(CreateParentsTestCase, self).setUp()
        g1 = self.h5file.create_group('/', 'g1')
        self.h5file.create_group(g1, 'g2')

    def existing(self, paths):
        """Return a set of the existing paths in `paths`."""
        return frozenset(path for path in paths if path in self.h5file)

    def basetest(self, doit, pre, post):
        pre()
        self.h5file.enable_undo()

        paths = ['/g1', '/g1/g2', '/g1/g2/g3', '/g1/g2/g3/g4']
        for newpath in paths:
            before = self.existing(paths)
            doit(newpath)
            after = self.existing(paths)
            self.assertTrue(after.issuperset(before))

            self.h5file.undo()
            post(newpath)
            after = self.existing(paths)
            self.assertEqual(after, before)

    def test00_create(self):
        """Test creating a node."""
        def pre():
            pass

        def doit(newpath):
            self.h5file.create_array(newpath, 'array', [1], createparents=True)
            self.assertTrue(join_path(newpath, 'array') in self.h5file)

        def post(newpath):
            self.assertTrue(join_path(newpath, 'array') not in self.h5file)
        self.basetest(doit, pre, post)

    def test01_move(self):
        """Test moving a node."""
        def pre():
            self.h5file.create_array('/', 'array', [1])

        def doit(newpath):
            self.h5file.move_node('/array', newpath, createparents=True)
            self.assertTrue('/array' not in self.h5file)
            self.assertTrue(join_path(newpath, 'array') in self.h5file)

        def post(newpath):
            self.assertTrue('/array' in self.h5file)
            self.assertTrue(join_path(newpath, 'array') not in self.h5file)
        self.basetest(doit, pre, post)

    def test02_copy(self):
        """Test copying a node."""
        def pre():
            self.h5file.create_array('/', 'array', [1])

        def doit(newpath):
            self.h5file.copy_node('/array', newpath, createparents=True)
            self.assertTrue(join_path(newpath, 'array') in self.h5file)

        def post(newpath):
            self.assertTrue(join_path(newpath, 'array') not in self.h5file)
        self.basetest(doit, pre, post)

    def test03_copyChildren(self):
        """Test copying the children of a group."""
        def pre():
            g = self.h5file.create_group('/', 'group')
            self.h5file.create_array(g, 'array1', [1])
            self.h5file.create_array(g, 'array2', [1])

        def doit(newpath):
            self.h5file.copy_children('/group', newpath, createparents=True)
            self.assertTrue(join_path(newpath, 'array1') in self.h5file)
            self.assertTrue(join_path(newpath, 'array2') in self.h5file)

        def post(newpath):
            self.assertTrue(join_path(newpath, 'array1') not in self.h5file)
            self.assertTrue(join_path(newpath, 'array2') not in self.h5file)
        self.basetest(doit, pre, post)


def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicTestCase))
        theSuite.addTest(unittest.makeSuite(PersistenceTestCase))
        theSuite.addTest(unittest.makeSuite(createArrayTestCase))
        theSuite.addTest(unittest.makeSuite(createGroupTestCase))
        theSuite.addTest(unittest.makeSuite(renameNodeTestCase))
        theSuite.addTest(unittest.makeSuite(moveNodeTestCase))
        theSuite.addTest(unittest.makeSuite(removeNodeTestCase))
        theSuite.addTest(unittest.makeSuite(copyNodeTestCase))
        theSuite.addTest(unittest.makeSuite(AttributesTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexTestCase))
        theSuite.addTest(unittest.makeSuite(NotLoggedTestCase))
        theSuite.addTest(unittest.makeSuite(CreateParentsTestCase))
    if common.heavy:
        pass

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

## Local Variables:
## mode: python
## End:

########NEW FILE########
__FILENAME__ = test_earray
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile

import numpy

from tables import *
from tables.utils import byteorders
from tables.tests import common
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class BasicTestCase(unittest.TestCase):
    # Default values
    obj = None
    flavor = "numpy"
    type = 'int32'
    dtype = 'int32'
    shape = (2, 0)
    start = 0
    stop = 10
    step = 1
    length = 1
    chunksize = 5
    nappends = 10
    compress = 0
    complib = "zlib"  # Default compression library
    shuffle = 0
    fletcher32 = 0
    reopen = 1  # Tells whether the file has to be reopened on each test or not

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()
        if self.reopen:
            # Close the file
            self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        obj = self.obj
        if obj is None:
            if self.type == "string":
                atom = StringAtom(itemsize=self.length)
            else:
                atom = Atom.from_type(self.type)
        else:
            atom = None
        title = self.__class__.__name__
        filters = Filters(complevel=self.compress,
                          complib=self.complib,
                          shuffle=self.shuffle,
                          fletcher32=self.fletcher32)
        earray = self.fileh.create_earray(group, 'earray1',
                                          atom=atom, shape=self.shape,
                                          title=title, filters=filters,
                                          expectedrows=1, obj=obj)
        earray.flavor = self.flavor

        # Fill it with rows
        self.rowshape = list(earray.shape)
        if obj is not None:
            self.rowshape[0] = 0
        self.objsize = self.length
        for i in self.rowshape:
            if i != 0:
                self.objsize *= i
        self.extdim = earray.extdim
        self.objsize *= self.chunksize
        self.rowshape[earray.extdim] = self.chunksize

        if self.type == "string":
            object = numpy.ndarray(buffer=b"a"*self.objsize,
                                   shape=self.rowshape,
                                   dtype="S%s" % earray.atom.itemsize)
        else:
            object = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object.shape = self.rowshape

        if common.verbose:
            if self.flavor == "numpy":
                print("Object to append -->", object)
            else:
                print("Object to append -->", repr(object))
        for i in range(self.nappends):
            if self.type == "string":
                earray.append(object)
            else:
                earray.append(object * i)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def _get_shape(self):
        if self.shape is not None:
            shape = self.shape
        else:
            shape = numpy.asarray(self.obj).shape

        return shape

    def test00_attributes(self):
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        obj = self.fileh.get_node("/earray1")

        shape = self._get_shape()
        shape = list(shape)
        shape[self.extdim] = self.chunksize * self.nappends
        if self.obj is not None:
            shape[self.extdim] += len(self.obj)
        shape = tuple(shape)

        self.assertEqual(obj.flavor, self.flavor)
        self.assertEqual(obj.shape, shape)
        self.assertEqual(obj.ndim, len(shape))
        self.assertEqual(obj.nrows, shape[self.extdim])
        self.assertEqual(obj.atom.type, self.type)

    def test01_iterEArray(self):
        """Checking enlargeable array iterator."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_iterEArray..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        earray.nrowsinbuf = 3
        if common.verbose:
            print("EArray descr:", repr(earray))
            print("shape of read array ==>", earray.shape)
            print("reopening?:", self.reopen)

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape
        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        shape = self._get_shape()

        # Read all the array
        for idx, row in enumerate(earray):
            if idx < initialrows:
                self.assertTrue(
                    allequal(row, numpy.asarray(self.obj[idx]), self.flavor))
                continue

            chunk = int((earray.nrow - initialrows) % self.chunksize)
            if chunk == 0:
                if self.type == "string":
                    object__ = object_
                else:
                    i = int(earray.nrow - initialrows)
                    object__ = object_ * (i // self.chunksize)

            object = object__[chunk]
            # The next adds much more verbosity
            if common.verbose and 0:
                print("number of row ==>", earray.nrow)
                if hasattr(object, "shape"):
                    print("shape should look as:", object.shape)
                print("row in earray ==>", repr(row))
                print("Should look like ==>", repr(object))

            self.assertEqual(initialrows + self.nappends * self.chunksize,
                             earray.nrows)
            self.assertTrue(allequal(row, object, self.flavor))
            if hasattr(row, "shape"):
                self.assertEqual(len(row.shape), len(shape) - 1)
            else:
                # Scalar case
                self.assertEqual(len(shape), 1)

            # Check filters:
            if self.compress != earray.filters.complevel and common.verbose:
                print("Error in compress. Class:", self.__class__.__name__)
                print("self, earray:", self.compress, earray.filters.complevel)
            self.assertEqual(earray.filters.complevel, self.compress)
            if self.compress > 0 and which_lib_version(self.complib):
                self.assertEqual(earray.filters.complib, self.complib)
            if self.shuffle != earray.filters.shuffle and common.verbose:
                print("Error in shuffle. Class:", self.__class__.__name__)
                print("self, earray:", self.shuffle, earray.filters.shuffle)
            self.assertEqual(self.shuffle, earray.filters.shuffle)
            if self.fletcher32 != earray.filters.fletcher32 and common.verbose:
                print("Error in fletcher32. Class:", self.__class__.__name__)
                print("self, earray:", self.fletcher32,
                      earray.filters.fletcher32)
            self.assertEqual(self.fletcher32, earray.filters.fletcher32)

    def test02_sssEArray(self):
        """Checking enlargeable array iterator with (start, stop, step)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_sssEArray..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        earray.nrowsinbuf = 3
        if common.verbose:
            print("EArray descr:", repr(earray))
            print("shape of read array ==>", earray.shape)
            print("reopening?:", self.reopen)

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape
        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        shape = self._get_shape()

        # Read all the array
        for idx, row in enumerate(earray.iterrows(start=self.start,
                                                  stop=self.stop,
                                                  step=self.step)):
            if idx < initialrows:
                self.assertTrue(
                    allequal(row, numpy.asarray(self.obj[idx]), self.flavor))
                continue

            if self.chunksize == 1:
                index = 0
            else:
                index = int((earray.nrow - initialrows) % self.chunksize)

            if self.type == "string":
                object__ = object_
            else:
                i = int(earray.nrow - initialrows)
                object__ = object_ * (i // self.chunksize)
            object = object__[index]

            # The next adds much more verbosity
            if common.verbose and 0:
                print("number of row ==>", earray.nrow)
                if hasattr(object, "shape"):
                    print("shape should look as:", object.shape)
                print("row in earray ==>", repr(row))
                print("Should look like ==>", repr(object))

            self.assertEqual(initialrows + self.nappends * self.chunksize,
                             earray.nrows)
            self.assertTrue(allequal(row, object, self.flavor))
            if hasattr(row, "shape"):
                self.assertEqual(len(row.shape), len(shape) - 1)
            else:
                # Scalar case
                self.assertEqual(len(shape), 1)

    def test03_readEArray(self):
        """Checking read() of enlargeable arrays."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_readEArray..." % self.__class__.__name__)

        # This conversion made just in case indices are numpy scalars
        if self.start is not None:
            self.start = long(self.start)
        if self.stop is not None:
            self.stop = long(self.stop)
        if self.step is not None:
            self.step = long(self.step)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        earray.nrowsinbuf = 3
        if common.verbose:
            print("EArray descr:", repr(earray))
            print("shape of read array ==>", earray.shape)
            print("reopening?:", self.reopen)

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape
        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        rowshape = self.rowshape
        rowshape[self.extdim] *= (self.nappends + initialrows)
        if self.type == "string":
            object__ = numpy.empty(
                shape=rowshape, dtype="S%s" % earray.atom.itemsize)
        else:
            object__ = numpy.empty(shape=rowshape, dtype=self.dtype)

        object__ = object__.swapaxes(0, self.extdim)

        if initialrows:
            object__[0:initialrows] = self.obj

        for i in range(self.nappends):
            j = initialrows + i * self.chunksize
            if self.type == "string":
                object__[j:j + self.chunksize] = object_
            else:
                object__[j:j + self.chunksize] = object_ * i

        stop = self.stop

        if self.nappends:
            # stop == None means read only the element designed by start
            # (in read() contexts)
            if self.stop is None:
                if self.start == -1:  # corner case
                    stop = earray.nrows
                else:
                    stop = self.start + 1
            # Protection against number of elements less than existing
            # if rowshape[self.extdim] < self.stop or self.stop == 0:
            if rowshape[self.extdim] < stop:
                # self.stop == 0 means last row only in read()
                # and not in [::] slicing notation
                stop = rowshape[self.extdim]
            # do a copy() in order to ensure that len(object._data)
            # actually do a measure of its length
            #object = object__[self.start:stop:self.step].copy()
            object = object__[self.start:self.stop:self.step].copy()
            # Swap the axes again to have normal ordering
            if self.flavor == "numpy":
                object = object.swapaxes(0, self.extdim)
        else:
            object = numpy.empty(shape=self.shape, dtype=self.dtype)

        # Read all the array
        try:
            row = earray.read(self.start, self.stop, self.step)
        except IndexError:
            row = numpy.empty(shape=self.shape, dtype=self.dtype)

        if common.verbose:
            if hasattr(object, "shape"):
                print("shape should look as:", object.shape)
            print("Object read ==>", repr(row))
            print("Should look like ==>", repr(object))

        self.assertEqual(initialrows + self.nappends * self.chunksize,
                         earray.nrows)
        self.assertTrue(allequal(row, object, self.flavor))

        shape = self._get_shape()
        if hasattr(row, "shape"):
            self.assertEqual(len(row.shape), len(shape))
            if self.flavor == "numpy":
                self.assertEqual(row.itemsize, earray.atom.itemsize)
        else:
            # Scalar case
            self.assertEqual(len(shape), 1)

    def test03_readEArray_out_argument(self):
        """Checking read() of enlargeable arrays."""

        # This conversion made just in case indices are numpy scalars
        if self.start is not None:
            self.start = long(self.start)
        if self.stop is not None:
            self.stop = long(self.stop)
        if self.step is not None:
            self.step = long(self.step)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        earray.nrowsinbuf = 3
        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape
        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        rowshape = self.rowshape
        rowshape[self.extdim] *= (self.nappends + initialrows)
        if self.type == "string":
            object__ = numpy.empty(
                shape=rowshape, dtype="S%s" % earray.atom.itemsize)
        else:
            object__ = numpy.empty(shape=rowshape, dtype=self.dtype)

        object__ = object__.swapaxes(0, self.extdim)

        if initialrows:
            object__[0:initialrows] = self.obj

        for i in range(self.nappends):
            j = initialrows + i * self.chunksize
            if self.type == "string":
                object__[j:j + self.chunksize] = object_
            else:
                object__[j:j + self.chunksize] = object_ * i

        stop = self.stop

        if self.nappends:
            # stop == None means read only the element designed by start
            # (in read() contexts)
            if self.stop is None:
                if self.start == -1:  # corner case
                    stop = earray.nrows
                else:
                    stop = self.start + 1
            # Protection against number of elements less than existing
            # if rowshape[self.extdim] < self.stop or self.stop == 0:
            if rowshape[self.extdim] < stop:
                # self.stop == 0 means last row only in read()
                # and not in [::] slicing notation
                stop = rowshape[self.extdim]
            # do a copy() in order to ensure that len(object._data)
            # actually do a measure of its length
            #object = object__[self.start:stop:self.step].copy()
            object = object__[self.start:self.stop:self.step].copy()
            # Swap the axes again to have normal ordering
            if self.flavor == "numpy":
                object = object.swapaxes(0, self.extdim)
        else:
            object = numpy.empty(shape=self.shape, dtype=self.dtype)

        # Read all the array
        try:
            row = numpy.empty(earray.shape, dtype=earray.atom.dtype)
            slice_obj = [slice(None)] * len(earray.shape)
            #slice_obj[earray.maindim] = slice(self.start, stop, self.step)
            slice_obj[earray.maindim] = slice(self.start, self.stop, self.step)
            row = row[slice_obj].copy()
            earray.read(self.start, self.stop, self.step, out=row)
        except IndexError:
            row = numpy.empty(shape=self.shape, dtype=self.dtype)

        if common.verbose:
            if hasattr(object, "shape"):
                print("shape should look as:", object.shape)
            print("Object read ==>", repr(row))
            print("Should look like ==>", repr(object))

        self.assertEqual(initialrows + self.nappends * self.chunksize,
                         earray.nrows)
        self.assertTrue(allequal(row, object, self.flavor))

        shape = self._get_shape()
        if hasattr(row, "shape"):
            self.assertEqual(len(row.shape), len(shape))
            if self.flavor == "numpy":
                self.assertEqual(row.itemsize, earray.atom.itemsize)
        else:
            # Scalar case
            self.assertEqual(len(shape), 1)

    def test04_getitemEArray(self):
        """Checking enlargeable array __getitem__ special method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_getitemEArray..." %
                  self.__class__.__name__)

        if not hasattr(self, "slices"):
            # If there is not a slices attribute, create it
            # This conversion made just in case indices are numpy scalars
            if self.start is not None:
                self.start = long(self.start)
            if self.stop is not None:
                self.stop = long(self.stop)
            if self.step is not None:
                self.step = long(self.step)
            self.slices = (slice(self.start, self.stop, self.step),)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "r")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        # earray.nrowsinbuf = 3   # this does not really changes the chunksize
        if common.verbose:
            print("EArray descr:", repr(earray))
            print("shape of read array ==>", earray.shape)
            print("reopening?:", self.reopen)

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape

        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        rowshape = self.rowshape
        rowshape[self.extdim] *= (self.nappends + initialrows)
        if self.type == "string":
            object__ = numpy.empty(
                shape=rowshape, dtype="S%s" % earray.atom.itemsize)
        else:
            object__ = numpy.empty(shape=rowshape, dtype=self.dtype)
            # Additional conversion for the numpy case
        object__ = object__.swapaxes(0, earray.extdim)

        if initialrows:
            object__[0:initialrows] = self.obj

        for i in range(self.nappends):
            j = initialrows + i * self.chunksize
            if self.type == "string":
                object__[j:j + self.chunksize] = object_
            else:
                object__[j:j + self.chunksize] = object_ * i

        if self.nappends:
            # Swap the axes again to have normal ordering
            if self.flavor == "numpy":
                object__ = object__.swapaxes(0, self.extdim)
            else:
                object__.swapaxes(0, self.extdim)
            # do a copy() in order to ensure that len(object._data)
            # actually do a measure of its length
            object = object__.__getitem__(self.slices).copy()
        else:
            object = numpy.empty(shape=self.shape, dtype=self.dtype)

        # Read all the array
        try:
            row = earray.__getitem__(self.slices)
        except IndexError:
            row = numpy.empty(shape=self.shape, dtype=self.dtype)

        if common.verbose:
            print("Object read:\n", repr(row))
            print("Should look like:\n", repr(object))
            if hasattr(object, "shape"):
                print("Original object shape:", self.shape)
                print("Shape read:", row.shape)
                print("shape should look as:", object.shape)

        self.assertEqual(initialrows + self.nappends * self.chunksize,
                         earray.nrows)
        self.assertTrue(allequal(row, object, self.flavor))
        if not hasattr(row, "shape"):
            # Scalar case
            self.assertEqual(len(self.shape), 1)

    def test05_setitemEArray(self):
        """Checking enlargeable array __setitem__ special method."""

        if self.__class__.__name__ == "Ellipsis6EArrayTestCase":
            # We have a problem with test design here, but I think
            # it is not worth the effort to solve it
            # F.Alted 2004-10-27
            return

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_setitemEArray..." %
                  self.__class__.__name__)

        if not hasattr(self, "slices"):
            # If there is not a slices attribute, create it
            # This conversion made just in case indices are numpy scalars
            if self.start is not None:
                self.start = long(self.start)
            if self.stop is not None:
                self.stop = long(self.stop)
            if self.step is not None:
                self.step = long(self.step)
            self.slices = (slice(self.start, self.stop, self.step),)

        # Create an instance of an HDF5 Table
        if self.reopen:
            self.fileh = open_file(self.file, "a")
        earray = self.fileh.get_node("/earray1")

        # Choose a small value for buffer size
        # earray.nrowsinbuf = 3   # this does not really changes the chunksize
        if common.verbose:
            print("EArray descr:", repr(earray))
            print("shape of read array ==>", earray.shape)
            print("reopening?:", self.reopen)

        # Build the array to do comparisons
        if self.type == "string":
            object_ = numpy.ndarray(buffer=b"a"*self.objsize,
                                    shape=self.rowshape,
                                    dtype="S%s" % earray.atom.itemsize)
        else:
            object_ = numpy.arange(self.objsize, dtype=earray.atom.dtype.base)
            object_.shape = self.rowshape

        object_ = object_.swapaxes(earray.extdim, 0)

        if self.obj is not None:
            initialrows = len(self.obj)
        else:
            initialrows = 0

        rowshape = self.rowshape
        rowshape[self.extdim] *= (self.nappends + initialrows)
        if self.type == "string":
            object__ = numpy.empty(
                shape=rowshape, dtype="S%s" % earray.atom.itemsize)
        else:
            object__ = numpy.empty(shape=rowshape, dtype=self.dtype)
            # Additional conversion for the numpy case
        object__ = object__.swapaxes(0, earray.extdim)

        for i in range(self.nappends):
            j = initialrows + i * self.chunksize
            if self.type == "string":
                object__[j:j + self.chunksize] = object_
            else:
                object__[j:j + self.chunksize] = object_ * i
                # Modify the earray
                # earray[j:j + self.chunksize] = object_ * i
                # earray[self.slices] = 1

        if initialrows:
            object__[0:initialrows] = self.obj

        if self.nappends:
            # Swap the axes again to have normal ordering
            if self.flavor == "numpy":
                object__ = object__.swapaxes(0, self.extdim)
            else:
                object__.swapaxes(0, self.extdim)
            # do a copy() in order to ensure that len(object._data)
            # actually do a measure of its length
            object = object__.__getitem__(self.slices).copy()
        else:
            object = numpy.empty(shape=self.shape, dtype=self.dtype)

        if self.flavor == "numpy":
            object = numpy.asarray(object)

        if self.type == "string":
            if hasattr(self, "wslice"):
                # These sentences should be equivalent
                # object[self.wslize] = object[self.wslice].pad("xXx")
                # earray[self.wslice] = earray[self.wslice].pad("xXx")
                object[self.wslize] = "xXx"
                earray[self.wslice] = "xXx"
            elif sum(object[self.slices].shape) != 0:
                # object[:] = object.pad("xXx")
                object[:] = "xXx"
                if object.size > 0:
                    earray[self.slices] = object
        else:
            if hasattr(self, "wslice"):
                object[self.wslice] = object[self.wslice] * 2 + 3
                earray[self.wslice] = earray[self.wslice] * 2 + 3
            elif sum(object[self.slices].shape) != 0:
                object = object * 2 + 3
                if numpy.prod(object.shape) > 0:
                    earray[self.slices] = earray[self.slices] * 2 + 3
        # Read all the array
        row = earray.__getitem__(self.slices)
        try:
            row = earray.__getitem__(self.slices)
        except IndexError:
            print("IndexError!")
            row = numpy.empty(shape=self.shape, dtype=self.dtype)

        if common.verbose:
            print("Object read:\n", repr(row))
            print("Should look like:\n", repr(object))
            if hasattr(object, "shape"):
                print("Original object shape:", self.shape)
                print("Shape read:", row.shape)
                print("shape should look as:", object.shape)

        self.assertEqual(initialrows + self.nappends * self.chunksize,
                         earray.nrows)
        self.assertTrue(allequal(row, object, self.flavor))
        if not hasattr(row, "shape"):
            # Scalar case
            self.assertEqual(len(self.shape), 1)


class BasicWriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (0,)
    chunksize = 5
    nappends = 10
    step = 1
    # wslice = slice(1,nappends,2)
    wslice = 1  # single element case


class Basic2WriteTestCase(BasicTestCase):
    type = 'int32'
    dtype = 'i4'
    shape = (0,)
    chunksize = 5
    nappends = 10
    step = 1
    wslice = slice(chunksize-2, nappends, 2)  # range of elements
    reopen = 0  # This case does not reopen files


class Basic3WriteTestCase(BasicTestCase):
    obj = [1, 2]
    type = numpy.asarray(obj).dtype.name
    dtype = numpy.asarray(obj).dtype.str
    shape = (0,)
    chunkshape = (5,)
    step = 1
    reopen = 0  # This case does not reopen files


class Basic4WriteTestCase(BasicTestCase):
    obj = numpy.array([1, 2])
    type = obj.dtype.name
    dtype = obj.dtype.str
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 0  # This case does not reopen files


class Basic5WriteTestCase(BasicTestCase):
    obj = [1, 2]
    type = numpy.asarray(obj).dtype.name
    dtype = numpy.asarray(obj).dtype.str
    shape = (0,)
    chunkshape = (5,)
    step = 1
    reopen = 1  # This case does reopen files


class Basic6WriteTestCase(BasicTestCase):
    obj = numpy.array([1, 2])
    type = obj.dtype.name
    dtype = obj.dtype.str
    shape = None
    chunkshape = (5,)
    step = 1
    reopen = 1  # This case does reopen files


class Basic7WriteTestCase(BasicTestCase):
    obj = [[1, 2], [3, 4]]
    type = numpy.asarray(obj).dtype.name
    dtype = numpy.asarray(obj).dtype.str
    shape = (0, 2)
    chunkshape = (5,)
    step = 1
    reopen = 0  # This case does not reopen files


class Basic8WriteTestCase(BasicTestCase):
    obj = [[1, 2], [3, 4]]
    type = numpy.asarray(obj).dtype.name
    dtype = numpy.asarray(obj).dtype.str
    shape = (0, 2)
    chunkshape = (5,)
    step = 1
    reopen = 1  # This case does reopen files


class EmptyEArrayTestCase(BasicTestCase):
    type = 'int32'
    dtype = numpy.dtype('int32')
    shape = (2, 0)
    chunksize = 5
    nappends = 0
    start = 0
    stop = 10
    step = 1


class NP_EmptyEArrayTestCase(BasicTestCase):
    type = 'int32'
    dtype = numpy.dtype('()int32')
    shape = (2, 0)
    chunksize = 5
    nappends = 0


class Empty2EArrayTestCase(BasicTestCase):
    type = 'int32'
    dtype = 'int32'
    shape = (2, 0)
    chunksize = 5
    nappends = 0
    start = 0
    stop = 10
    step = 1
    reopen = 0  # This case does not reopen files


class SlicesEArrayTestCase(BasicTestCase):
    compress = 1
    complib = "lzo"
    type = 'int32'
    shape = (2, 0)
    chunksize = 5
    nappends = 2
    slices = (slice(1, 2, 1), slice(1, 3, 1))


class Slices2EArrayTestCase(BasicTestCase):
    compress = 1
    complib = "blosc"
    type = 'int32'
    shape = (2, 0, 4)
    chunksize = 5
    nappends = 20
    slices = (slice(1, 2, 1), slice(None, None, None), slice(1, 4, 2))


class EllipsisEArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 0)
    chunksize = 5
    nappends = 2
    # slices = (slice(1,2,1), Ellipsis)
    slices = (Ellipsis, slice(1, 2, 1))


class Ellipsis2EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 0, 4)
    chunksize = 5
    nappends = 20
    slices = (slice(1, 2, 1), Ellipsis, slice(1, 4, 2))


class Slices3EArrayTestCase(BasicTestCase):
    compress = 1      # To show the chunks id DEBUG is on
    complib = "blosc"
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 20
    slices = (slice(1, 2, 1), slice(0, None, None),
              slice(1, 4, 2))  # Don't work
    # slices = (slice(None, None, None), slice(0, None, None),
    #           slice(1,4,1)) # W
    # slices = (slice(None, None, None), slice(None, None, None),
    #           slice(1,4,2)) # N
    # slices = (slice(1,2,1), slice(None, None, None), slice(1,4,2)) # N
    # Disable the failing test temporarily with a working test case
    slices = (slice(1, 2, 1), slice(1, 4, None), slice(1, 4, 2))  # Y
    # slices = (slice(1,2,1), slice(0, 4, None), slice(1,4,1)) # Y
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(1, 4, 2))  # N
    # slices = (slice(1,2,1), slice(0, 4, None), slice(1,4,2),
    #           slice(0,100,1))  # N


class Slices4EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0, 5, 6)
    chunksize = 5
    nappends = 20
    slices = (slice(1, 2, 1), slice(0, None, None), slice(1, 4, 2),
              slice(0, 4, 2), slice(3, 5, 2), slice(2, 7, 1))


class Ellipsis3EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 20
    slices = (Ellipsis, slice(0, 4, None), slice(1, 4, 2))
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(1, 4, 2), Ellipsis)


class Ellipsis4EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 20
    slices = (Ellipsis, slice(0, 4, None), slice(1, 4, 2))
    slices = (slice(1, 2, 1), Ellipsis, slice(1, 4, 2))


class Ellipsis5EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 20
    slices = (slice(1, 2, 1), slice(0, 4, None), Ellipsis)


class Ellipsis6EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 2
    # The next slices gives problems with setting values (test05)
    # This is a problem on the test design, not the Array.__setitem__
    # code, though.
    slices = (slice(1, 2, 1), slice(0, 4, None), 2, Ellipsis)


class Ellipsis7EArrayTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 4, 0)
    chunksize = 5
    nappends = 2
    slices = (slice(1, 2, 1), slice(0, 4, None), slice(2, 3), Ellipsis)


class MD3WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 0, 3)
    chunksize = 4
    step = 2


class MD5WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 0, 3, 4, 5)  # ok
    # shape = (1, 1, 0, 1)  # Minimum shape that shows problems with HDF5 1.6.1
    # shape = (2, 3, 0, 4, 5)  # Floating point exception (HDF5 1.6.1)
    # shape = (2, 3, 3, 0, 5, 6) # Segmentation fault (HDF5 1.6.1)
    chunksize = 1
    nappends = 1
    start = 1
    stop = 10
    step = 10


class MD6WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 3, 0, 5, 6)
    chunksize = 1
    nappends = 10
    start = 1
    stop = 10
    step = 3


class NP_MD6WriteTestCase(BasicTestCase):
    "Testing NumPy scalars as indexes"
    type = 'int32'
    shape = (2, 3, 3, 0, 5, 6)
    chunksize = 1
    nappends = 10


class MD6WriteTestCase__(BasicTestCase):
    type = 'int32'
    shape = (2, 0)
    chunksize = 1
    nappends = 3
    start = 1
    stop = 3
    step = 1


class MD7WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (2, 3, 3, 4, 5, 0, 3)
    chunksize = 10
    nappends = 1
    start = 1
    stop = 10
    step = 2


class MD10WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (1, 2, 3, 4, 5, 5, 4, 3, 2, 0)
    chunksize = 5
    nappends = 10
    start = -1
    stop = -1
    step = 10


class NP_MD10WriteTestCase(BasicTestCase):
    type = 'int32'
    shape = (1, 2, 3, 4, 5, 5, 4, 3, 2, 0)
    chunksize = 5
    nappends = 10


class ZlibComprTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    start = 3
    # stop = 0   # means last row
    stop = None   # means last row from 0.8 on
    step = 10


class ZlibShuffleTestCase(BasicTestCase):
    shuffle = 1
    compress = 1
    complib = "zlib"
    # case start < stop , i.e. no rows read
    start = 3
    stop = 1
    step = 10


class BloscComprTestCase(BasicTestCase):
    compress = 1  # sss
    complib = "blosc"
    chunksize = 10
    nappends = 100
    start = 3
    stop = 10
    step = 3


class BloscShuffleTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    complib = "blosc"
    chunksize = 100
    nappends = 10
    start = 3
    stop = 10
    step = 7


class LZOComprTestCase(BasicTestCase):
    compress = 1  # sss
    complib = "lzo"
    chunksize = 10
    nappends = 100
    start = 3
    stop = 10
    step = 3


class LZOShuffleTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    complib = "lzo"
    chunksize = 100
    nappends = 10
    start = 3
    stop = 10
    step = 7


class Bzip2ComprTestCase(BasicTestCase):
    compress = 1
    complib = "bzip2"
    chunksize = 100
    nappends = 10
    start = 3
    stop = 10
    step = 8


class Bzip2ShuffleTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    complib = "bzip2"
    chunksize = 100
    nappends = 10
    start = 3
    stop = 10
    step = 6


class Fletcher32TestCase(BasicTestCase):
    compress = 0
    fletcher32 = 1
    chunksize = 50
    nappends = 20
    start = 4
    stop = 20
    step = 7


class AllFiltersTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    fletcher32 = 1
    complib = "zlib"
    chunksize = 20  # sss
    nappends = 50
    start = 2
    stop = 99
    step = 6
#     chunksize = 3
#     nappends = 2
#     start = 1
#     stop = 10
#     step = 2


class FloatTypeTestCase(BasicTestCase):
    type = 'float64'
    dtype = 'float64'
    shape = (2, 0)
    chunksize = 5
    nappends = 10
    start = 3
    stop = 10
    step = 20


class ComplexTypeTestCase(BasicTestCase):
    type = 'complex128'
    dtype = 'complex128'
    shape = (2, 0)
    chunksize = 5
    nappends = 10
    start = 3
    stop = 10
    step = 20


class StringTestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (2, 0)
    # shape = (2,0,20)
    chunksize = 5
    nappends = 10
    start = 3
    stop = 10
    step = 20
    slices = (slice(0, 1), slice(1, 2))


class String2TestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (0,)
    # shape = (0, 20)
    chunksize = 5
    nappends = 10
    start = 1
    stop = 10
    step = 2


class StringComprTestCase(BasicTestCase):
    type = "string"
    length = 20
    shape = (20, 0, 10)
    # shape = (20,0,10,20)
    compr = 1
    # shuffle = 1  # this shouldn't do nothing on chars
    chunksize = 50
    nappends = 10
    start = -1
    stop = 100
    step = 20


class SizeOnDiskInMemoryPropertyTestCase(unittest.TestCase):

    def setUp(self):
        self.array_size = (0, 10)
        # set chunkshape so it divides evenly into array_size, to avoid
        # partially filled chunks
        self.chunkshape = (1000, 10)
        # approximate size (in bytes) of non-data portion of hdf5 file
        self.hdf_overhead = 6000
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")

    def tearDown(self):
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def create_array(self, complevel):
        filters = Filters(complevel=complevel, complib='blosc')
        self.array = self.fileh.create_earray('/', 'earray', atom=Int32Atom(),
                                              shape=self.array_size,
                                              filters=filters,
                                              chunkshape=self.chunkshape)

    def test_zero_length(self):
        complevel = 0
        self.create_array(complevel)
        self.assertEqual(self.array.size_on_disk, 0)
        self.assertEqual(self.array.size_in_memory, 0)

    # add 10 chunks of data in one append
    def test_no_compression_one_append(self):
        complevel = 0
        self.create_array(complevel)
        self.array.append([tuple(range(10))] * self.chunkshape[0] * 10)
        self.assertEqual(self.array.size_on_disk, 10 * 1000 * 10 * 4)
        self.assertEqual(self.array.size_in_memory, 10 * 1000 * 10 * 4)

    # add 10 chunks of data in two appends
    def test_no_compression_multiple_appends(self):
        complevel = 0
        self.create_array(complevel)
        self.array.append([tuple(range(10))] * self.chunkshape[0] * 5)
        self.array.append([tuple(range(10))] * self.chunkshape[0] * 5)
        self.assertEqual(self.array.size_on_disk, 10 * 1000 * 10 * 4)
        self.assertEqual(self.array.size_in_memory, 10 * 1000 * 10 * 4)

    def test_with_compression(self):
        complevel = 1
        self.create_array(complevel)
        self.array.append([tuple(range(10))] * self.chunkshape[0] * 10)
        file_size = os.stat(self.file).st_size
        self.assertTrue(
            abs(self.array.size_on_disk - file_size) <= self.hdf_overhead)
        self.assertEqual(self.array.size_in_memory, 10 * 1000 * 10 * 4)
        self.assertTrue(self.array.size_on_disk < self.array.size_in_memory)


class OffsetStrideTestCase(unittest.TestCase):
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01a_String(self):
        """Checking earray with offseted numpy strings appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_StringAtom..." % self.__class__.__name__)

        earray = self.fileh.create_earray(root, 'strings',
                                          atom=StringAtom(itemsize=3),
                                          shape=(0, 2, 2),
                                          title="Array of strings")
        a = numpy.array([[["a", "b"], [
                        "123", "45"], ["45", "123"]]], dtype="S3")
        earray.append(a[:, 1:])
        a = numpy.array([[["s", "a"], [
                        "ab", "f"], ["s", "abc"], ["abc", "f"]]])
        earray.append(a[:, 2:])

        # Read all the rows:
        row = earray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", earray._v_pathname, ":", earray.nrows)
            print("Second row in earray ==>", row[1].tolist())

        self.assertEqual(earray.nrows, 2)
        self.assertEqual(row[0].tolist(), [[b"123", b"45"], [b"45", b"123"]])
        self.assertEqual(row[1].tolist(), [[b"s", b"abc"], [b"abc", b"f"]])
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 2)

    def test01b_String(self):
        """Checking earray with strided numpy strings appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_StringAtom..." % self.__class__.__name__)

        earray = self.fileh.create_earray(root, 'strings',
                                          atom=StringAtom(itemsize=3),
                                          shape=(0, 2, 2),
                                          title="Array of strings")
        a = numpy.array([[["a", "b"], [
                        "123", "45"], ["45", "123"]]], dtype="S3")
        earray.append(a[:, ::2])
        a = numpy.array([[["s", "a"], [
                        "ab", "f"], ["s", "abc"], ["abc", "f"]]])
        earray.append(a[:, ::2])

        # Read all the rows:
        row = earray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", earray._v_pathname, ":", earray.nrows)
            print("Second row in earray ==>", row[1].tolist())

        self.assertEqual(earray.nrows, 2)
        self.assertEqual(row[0].tolist(), [[b"a", b"b"], [b"45", b"123"]])
        self.assertEqual(row[1].tolist(), [[b"s", b"a"], [b"s", b"abc"]])
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 2)

    def test02a_int(self):
        """Checking earray with offseted NumPy ints appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02a_int..." % self.__class__.__name__)

        # Create an string atom
        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Int32Atom(), shape=(0, 3),
                                          title="array of ints")
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (0, 0, 0)], dtype='int32')
        earray.append(a[2:])  # Create an offset
        a = numpy.array([(1, 1, 1), (-1, 0, 0)], dtype='int32')
        earray.append(a[1:])  # Create an offset

        # Read all the rows:
        row = earray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", earray._v_pathname, ":", earray.nrows)
            print("Third row in vlarray ==>", row[2])

        self.assertEqual(earray.nrows, 3)
        self.assertTrue(allequal(row[
                        0], numpy.array([1, 1, 1], dtype='int32')))
        self.assertTrue(allequal(row[
                        1], numpy.array([0, 0, 0], dtype='int32')))
        self.assertTrue(allequal(row[
                        2], numpy.array([-1, 0, 0], dtype='int32')))

    def test02b_int(self):
        """Checking earray with strided NumPy ints appends."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_int..." % self.__class__.__name__)

        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Int32Atom(), shape=(0, 3),
                                          title="array of ints")
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='int32')
        earray.append(a[::3])  # Create an offset
        a = numpy.array([(1, 1, 1), (-1, 0, 0)], dtype='int32')
        earray.append(a[::2])  # Create an offset

        # Read all the rows:
        row = earray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", earray._v_pathname, ":", earray.nrows)
            print("Third row in vlarray ==>", row[2])

        self.assertEqual(earray.nrows, 3)
        self.assertTrue(allequal(row[
                        0], numpy.array([0, 0, 0], dtype='int32')))
        self.assertTrue(allequal(row[
                        1], numpy.array([3, 3, 3], dtype='int32')))
        self.assertTrue(allequal(row[
                        2], numpy.array([1, 1, 1], dtype='int32')))

    def test03a_int(self):
        """Checking earray with byteswapped appends (ints)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03a_int..." % self.__class__.__name__)

        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Int32Atom(), shape=(0, 3),
                                          title="array of ints")
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Int32')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Native rows:", native)
            print("Byteorder native rows:", native.dtype.byteorder)
            print("Swapped rows:", swapped)
            print("Byteorder swapped rows:", swapped.dtype.byteorder)

        self.assertTrue(allequal(native, swapped))

    def test03b_float(self):
        """Checking earray with byteswapped appends (floats)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_float..." % self.__class__.__name__)

        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Float64Atom(), shape=(0, 3),
                                          title="array of floats")
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Float64')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Native rows:", native)
            print("Byteorder native rows:", native.dtype.byteorder)
            print("Swapped rows:", swapped)
            print("Byteorder swapped rows:", swapped.dtype.byteorder)

        self.assertTrue(allequal(native, swapped))

    def test04a_int(self):
        """Checking earray with byteswapped appends (2, ints)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04a_int..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Int32Atom(), shape=(0, 3),
                                          title="array of ints",
                                          byteorder=byteorder)
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Int32')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Byteorder native rows:", byteorders[native.dtype.byteorder])
            print("Byteorder earray on-disk:", earray.byteorder)

        self.assertEqual(byteorders[native.dtype.byteorder], sys.byteorder)
        self.assertEqual(earray.byteorder, byteorder)
        self.assertTrue(allequal(native, swapped))

    def test04b_int(self):
        """Checking earray with byteswapped appends (2, ints, reopen)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b_int..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Int32Atom(), shape=(0, 3),
                                          title="array of ints",
                                          byteorder=byteorder)
        self.fileh.close()
        self.fileh = open_file(self.file, "a")
        earray = self.fileh.get_node("/EAtom")
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Int32')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Byteorder native rows:", byteorders[native.dtype.byteorder])
            print("Byteorder earray on-disk:", earray.byteorder)

        self.assertEqual(byteorders[native.dtype.byteorder], sys.byteorder)
        self.assertEqual(earray.byteorder, byteorder)
        self.assertTrue(allequal(native, swapped))

    def test04c_float(self):
        """Checking earray with byteswapped appends (2, floats)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04c_float..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Float64Atom(), shape=(0, 3),
                                          title="array of floats",
                                          byteorder=byteorder)
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Float64')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Byteorder native rows:", byteorders[native.dtype.byteorder])
            print("Byteorder earray on-disk:", earray.byteorder)

        self.assertEqual(byteorders[native.dtype.byteorder], sys.byteorder)
        self.assertEqual(earray.byteorder, byteorder)
        self.assertTrue(allequal(native, swapped))

    def test04d_float(self):
        """Checking earray with byteswapped appends (2, floats, reopen)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04d_float..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        earray = self.fileh.create_earray(root, 'EAtom',
                                          atom=Float64Atom(), shape=(0, 3),
                                          title="array of floats",
                                          byteorder=byteorder)
        self.fileh.close()
        self.fileh = open_file(self.file, "a")
        earray = self.fileh.get_node("/EAtom")
        # Add a native ordered array
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (3, 3, 3)], dtype='Float64')
        earray.append(a)
        # Change the byteorder of the array
        a = a.byteswap()
        a = a.newbyteorder()
        # Add a byteswapped array
        earray.append(a)

        # Read all the rows:
        native = earray[:4, :]
        swapped = earray[4:, :]
        if common.verbose:
            print("Byteorder native rows:", byteorders[native.dtype.byteorder])
            print("Byteorder earray on-disk:", earray.byteorder)

        self.assertEqual(byteorders[native.dtype.byteorder], sys.byteorder)
        self.assertEqual(earray.byteorder, byteorder)
        self.assertTrue(allequal(native, swapped))


class CopyTestCase(unittest.TestCase):

    def test01_copy(self):
        """Checking EArray.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.atom.itemsize, array2.atom.itemsize)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_copy(self):
        """Checking EArray.copy() method (where specified)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        group1 = fileh.create_group("/", "group1")
        array2 = array1.copy(group1, 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.group1.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            # print("dirs-->", dir(array1), dir(array2))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.atom.itemsize, array2.atom.itemsize)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))

        # Close the file
        fileh.close()
        os.remove(file)

    def test03a_copy(self):
        """Checking EArray.copy() method (python flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.flavor = "python"
        array1.append(((456, 2), (3, 457)))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertEqual(array1.read(), array2.read())
        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.atom.itemsize, array2.atom.itemsize)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))

        # Close the file
        fileh.close()
        os.remove(file)

    def test03b_copy(self):
        """Checking EArray.copy() method (python string flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03d_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        atom = StringAtom(itemsize=3)
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.flavor = "python"
        array1.append([["456", "2"], ["3", "457"]])

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertEqual(array1.read(), array2.read())

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.atom.itemsize, array2.atom.itemsize)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))

        # Close the file
        fileh.close()
        os.remove(file)

    def test03c_copy(self):
        """Checking EArray.copy() method (String flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03e_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        atom = StringAtom(itemsize=4)
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.flavor = "numpy"
        array1.append(numpy.array([["456", "2"], ["3", "457"]], dtype="S4"))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all elements are equal
        self.assertTrue(allequal(array1.read(), array2.read()))
        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.extdim, array2.extdim)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here!
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(array1.atom.itemsize, array2.atom.itemsize)
        self.assertEqual(array1.title, array2.title)
        self.assertEqual(str(array1.atom), str(array2.atom))

        # Close the file
        fileh.close()
        os.remove(file)

    def test04_copy(self):
        """Checking EArray.copy() method (checking title copying)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', title="title array2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        # Assert user attributes
        if common.verbose:
            print("title of destination array-->", array2.title)
        self.assertEqual(array2.title, "title array2")

        # Close the file
        fileh.close()
        os.remove(file)

    def test05_copy(self):
        """Checking EArray.copy() method (user attributes copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(array2.attrs.attr1, "attr1")
        self.assertEqual(array2.attrs.attr2, 2)

        # Close the file
        fileh.close()
        os.remove(file)

    def test05b_copy(self):
        """Checking EArray.copy() method (user attributes not copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        atom = Int16Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(hasattr(array2.attrs, "attr1"), 0)
        self.assertEqual(hasattr(array2.attrs, "attr2"), 0)

        # Close the file
        fileh.close()
        os.remove(file)


class CloseCopyTestCase(CopyTestCase):
    close = 1


class OpenCopyTestCase(CopyTestCase):
    close = 0


class CopyIndexTestCase(unittest.TestCase):
    nrowsinbuf = 2

    def test01_index(self):
        """Checking EArray.copy() method with indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_index..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int32Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        r = numpy.arange(200, dtype='int32')
        r.shape = (100, 2)
        array1.append(r)

        # Select a different buffer size:
        array1.nrowsinbuf = self.nrowsinbuf

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_indexclosef(self):
        """Checking EArray.copy() method with indexes (close file version)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_indexclosef..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an EArray
        atom = Int32Atom()
        array1 = fileh.create_earray(fileh.root, 'array1',
                                     atom=atom, shape=(0, 2),
                                     title="title array1")
        r = numpy.arange(200, dtype='int32')
        r.shape = (100, 2)
        array1.append(r)

        # Select a different buffer size:
        array1.nrowsinbuf = self.nrowsinbuf

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        # Close and reopen the file
        fileh.close()
        fileh = open_file(file, mode="r")
        array1 = fileh.root.array1
        array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        self.assertTrue(allequal(r2, array2.read()))

        # Assert the number of rows in array
        if common.verbose:
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)


class CopyIndex1TestCase(CopyIndexTestCase):
    nrowsinbuf = 1
    start = 0
    stop = 7
    step = 1


class CopyIndex2TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 0
    stop = -1
    step = 1


class CopyIndex3TestCase(CopyIndexTestCase):
    nrowsinbuf = 3
    start = 1
    stop = 7
    step = 1


class CopyIndex4TestCase(CopyIndexTestCase):
    nrowsinbuf = 4
    start = 0
    stop = 6
    step = 1


class CopyIndex5TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 3
    stop = 7
    step = 1


class CopyIndex6TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    start = 3
    stop = 6
    step = 2


class CopyIndex7TestCase(CopyIndexTestCase):
    start = 0
    stop = 7
    step = 10


class CopyIndex8TestCase(CopyIndexTestCase):
    start = 6
    stop = -1  # Negative values means starting from the end
    step = 1


class CopyIndex9TestCase(CopyIndexTestCase):
    start = 3
    stop = 4
    step = 1


class CopyIndex10TestCase(CopyIndexTestCase):
    nrowsinbuf = 1
    start = 3
    stop = 4
    step = 2


class CopyIndex11TestCase(CopyIndexTestCase):
    start = -3
    stop = -1
    step = 2


class CopyIndex12TestCase(CopyIndexTestCase):
    start = -1   # Should point to the last element
    stop = None  # None should mean the last element (including it)
    step = 1


class TruncateTestCase(unittest.TestCase):

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create an EArray
        atom = Int16Atom(dflt=3)
        array1 = self.fileh.create_earray(self.fileh.root, 'array1',
                                          atom=atom, shape=(0, 2),
                                          title="title array1")
        # Add a couple of rows
        array1.append(numpy.array([[456, 2], [3, 457]], dtype='Int16'))

    def tearDown(self):
        # Close the file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_truncate(self):
        """Checking EArray.truncate() method (truncating to 0 rows)"""

        array1 = self.fileh.root.array1
        # Truncate to 0 elements
        array1.truncate(0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertTrue(allequal(
            array1[:], numpy.array([], dtype='Int16').reshape(0, 2)))

    def test01_truncate(self):
        """Checking EArray.truncate() method (truncating to 1 rows)"""

        array1 = self.fileh.root.array1
        # Truncate to 1 element
        array1.truncate(1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertTrue(allequal(
            array1.read(), numpy.array([[456, 2]], dtype='Int16')))

    def test02_truncate(self):
        """Checking EArray.truncate() method (truncating to == self.nrows)"""

        array1 = self.fileh.root.array1
        # Truncate to 2 elements
        array1.truncate(2)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertTrue(
            allequal(array1.read(),
                     numpy.array([[456, 2], [3, 457]], dtype='Int16')))

    def test03_truncate(self):
        """Checking EArray.truncate() method (truncating to > self.nrows)"""

        array1 = self.fileh.root.array1
        # Truncate to 4 elements
        array1.truncate(4)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertEqual(array1.nrows, 4)
        # Check the original values
        self.assertTrue(allequal(array1[:2], numpy.array([[456, 2], [3, 457]],
                                                         dtype='Int16')))
        # Check that the added rows have the default values
        self.assertTrue(allequal(array1[2:], numpy.array([[3, 3], [3, 3]],
                                                         dtype='Int16')))


class TruncateOpenTestCase(TruncateTestCase):
    close = 0


class TruncateCloseTestCase(TruncateTestCase):
    close = 1


# The next test should be run only in **common.heavy** mode
class Rows64bitsTestCase(unittest.TestCase):
    narows = 1000 * 1000   # each numpy object will have 1 million entries
    # narows = 1000   # for testing only
    nanumber = 1000 * 3    # That should account for more than 2**31-1

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        fileh = self.fileh = open_file(self.file, "a")
        # Create an EArray
        array = fileh.create_earray(fileh.root, 'array',
                                    atom=Int8Atom(), shape=(0,),
                                    filters=Filters(complib='lzo',
                                                    complevel=1),
                                    # Specifying expectedrows takes more
                                    # CPU, but less disk
                                    expectedrows=self.narows * self.nanumber)

        # Fill the array
        na = numpy.arange(self.narows, dtype='Int8')
        for i in range(self.nanumber):
            array.append(na)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01_basiccheck(self):
        "Some basic checks for earrays exceeding 2**31 rows"

        fileh = self.fileh
        array = fileh.root.array

        if self.close:
            if common.verbose:
                # Check how many entries there are in the array
                print("Before closing")
                print("Entries:", array.nrows, type(array.nrows))
                print("Entries:", array.nrows / (1000 * 1000), "Millions")
                print("Shape:", array.shape)
            # Close the file
            fileh.close()
            # Re-open the file
            fileh = self.fileh = open_file(self.file)
            array = fileh.root.array
            if common.verbose:
                print("After re-open")

        # Check how many entries there are in the array
        if common.verbose:
            print("Entries:", array.nrows, type(array.nrows))
            print("Entries:", array.nrows / (1000 * 1000), "Millions")
            print("Shape:", array.shape)
            print("Last 10 elements-->", array[-10:])
            stop = self.narows % 256
            if stop > 127:
                stop -= 256
            start = stop - 10
            print("Should look like-->", numpy.arange(start, stop,
                                                      dtype='Int8'))

        nrows = self.narows * self.nanumber
        # check nrows
        self.assertEqual(array.nrows, nrows)
        # Check shape
        self.assertEqual(array.shape, (nrows,))
        # check the 10 first elements
        self.assertTrue(allequal(array[:10], numpy.arange(10, dtype='Int8')))
        # check the 10 last elements
        stop = self.narows % 256
        if stop > 127:
            stop -= 256
        start = stop - 10
        self.assertTrue(allequal(array[-10:],
                                 numpy.arange(start, stop, dtype='Int8')))


class Rows64bitsTestCase1(Rows64bitsTestCase):
    close = 0


class Rows64bitsTestCase2(Rows64bitsTestCase):
    close = 1


# Test for appending zero-sized arrays
class ZeroSizedTestCase(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "a")
        # Create an EArray
        ea = self.fileh.create_earray('/', 'test',
                                      atom=Int32Atom(), shape=(3, 0))
        # Append a single row
        ea.append([[1], [2], [3]])

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01_canAppend(self):
        "Appending zero length array."

        fileh = self.fileh
        ea = fileh.root.test
        np = numpy.empty(shape=(3, 0), dtype='int32')
        ea.append(np)
        self.assertEqual(ea.nrows, 1, "The number of rows should be 1.")

    def test02_appendWithWrongShape(self):
        "Appending zero length array with wrong dimension."

        fileh = self.fileh
        ea = fileh.root.test
        np = numpy.empty(shape=(3, 0, 3), dtype='int32')
        self.assertRaises(ValueError, ea.append, np)


# Test for dealing with multidimensional atoms
class MDAtomTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test01a_append(self):
        "Append a row to a (unidimensional) EArray with a MD atom."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test',
                                       atom=Int32Atom((2, 2)), shape=(0,))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append one row
        ea.append([[[1, 3], [4, 5]]])
        self.assertEqual(ea.nrows, 1)
        if common.verbose:
            print("First row-->", ea[0])
        self.assertTrue(allequal(ea[0], numpy.array([[1, 3], [4, 5]], 'i4')))

    def test01b_append(self):
        "Append several rows to a (unidimensional) EArray with a MD atom."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test',
                                       atom=Int32Atom((2, 2)), shape=(0,))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append three rows
        ea.append([[[1]], [[2]], [[3]]])   # Simple broadcast
        self.assertEqual(ea.nrows, 3)
        if common.verbose:
            print("Third row-->", ea[2])
        self.assertTrue(allequal(ea[2], numpy.array([[3, 3], [3, 3]], 'i4')))

    def test02a_append(self):
        "Append a row to a (multidimensional) EArray with a MD atom."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test',
                                       atom=Int32Atom((2,)), shape=(0, 3))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append one row
        ea.append([[[1, 3], [4, 5], [7, 9]]])
        self.assertEqual(ea.nrows, 1)
        if common.verbose:
            print("First row-->", ea[0])
        self.assertTrue(allequal(ea[0], numpy.array(
            [[1, 3], [4, 5], [7, 9]], 'i4')))

    def test02b_append(self):
        "Append several rows to a (multidimensional) EArray with a MD atom."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test',
                                       atom=Int32Atom((2,)), shape=(0, 3))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append three rows
        ea.append([[[1, -3], [4, -5], [-7, 9]],
                   [[-1, 3], [-4, 5], [7, -8]],
                   [[-2, 3], [-5, 5], [7, -9]]])
        self.assertEqual(ea.nrows, 3)
        if common.verbose:
            print("Third row-->", ea[2])
        self.assertTrue(allequal(
            ea[2], numpy.array([[-2, 3], [-5, 5], [7, -9]], 'i4')))

    def test03a_MDMDMD(self):
        "Complex append of a MD array in a MD EArray with a MD atom."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test', atom=Int32Atom((2, 4)),
                                       shape=(0, 2, 3))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append three rows
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(2 * 3*2*4, dtype='i4').reshape((2, 3, 2, 4))
        ea.append([a * 1, a*2, a*3])
        self.assertEqual(ea.nrows, 3)
        if common.verbose:
            print("Third row-->", ea[2])
        self.assertTrue(allequal(ea[2], a * 3))

    def test03b_MDMDMD(self):
        "Complex append of a MD array in a MD EArray with a MD atom (II)."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test', atom=Int32Atom((2, 4)),
                                       shape=(2, 0, 3))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append three rows
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(2 * 3*2*4, dtype='i4').reshape((2, 1, 3, 2, 4))
        ea.append(a * 1)
        ea.append(a * 2)
        ea.append(a * 3)
        self.assertEqual(ea.nrows, 3)
        if common.verbose:
            print("Third row-->", ea[:, 2, ...])
        self.assertTrue(allequal(ea[:, 2, ...], a.reshape((2, 3, 2, 4))*3))

    def test03c_MDMDMD(self):
        "Complex append of a MD array in a MD EArray with a MD atom (III)."
        # Create an EArray
        ea = self.h5file.create_earray('/', 'test', atom=Int32Atom((2, 4)),
                                       shape=(2, 3, 0))
        if self.reopen:
            self._reopen('a')
            ea = self.h5file.root.test
        # Append three rows
        # The shape of the atom should be added at the end of the arrays
        a = numpy.arange(2 * 3*2*4, dtype='i4').reshape((2, 3, 1, 2, 4))
        ea.append(a * 1)
        ea.append(a * 2)
        ea.append(a * 3)
        self.assertEqual(ea.nrows, 3)
        if common.verbose:
            print("Third row-->", ea[:, :, 2, ...])
        self.assertTrue(allequal(ea[:, :, 2, ...], a.reshape((2, 3, 2, 4))*3))


class MDAtomNoReopen(MDAtomTestCase):
    reopen = False


class MDAtomReopen(MDAtomTestCase):
    reopen = True


class AccessClosedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(AccessClosedTestCase, self).setUp()
        self.array = self.h5file.create_earray(self.h5file.root, 'array',
                                               atom=Int32Atom(), shape=(0, 10))
        self.array.append(numpy.zeros((10, 10)))

    def test_read(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.read)

    def test_getitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__getitem__, 0)

    def test_setitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__setitem__, 0, 0)

    def test_append(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.append,
                          numpy.zeros((10, 10)))


class TestCreateEArrayArgs(common.TempFileMixin, common.PyTablesTestCase):
    obj = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    where = '/'
    name = 'earray'
    atom = Atom.from_dtype(obj.dtype)
    shape = (0,) + obj.shape[1:]
    title = 'title'
    filters = None
    expectedrows = 1000
    chunkshape = (1, 2)
    byteorder = None
    createparents = False

    def test_positional_args_01(self):
        self.h5file.create_earray(self.where, self.name,
                                  self.atom, self.shape,
                                  self.title, self.filters,
                                  self.expectedrows, self.chunkshape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)

    def test_positional_args_02(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          self.atom, self.shape,
                                          self.title,
                                          self.filters,
                                          self.expectedrows,
                                          self.chunkshape)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_positional_args_obj(self):
        self.h5file.create_earray(self.where, self.name,
                                  None, None,
                                  self.title,
                                  self.filters,
                                  self.expectedrows,
                                  self.chunkshape,
                                  self.byteorder,
                                  self.createparents,
                                  self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj(self):
        self.h5file.create_earray(self.where, self.name, title=self.title,
                                  chunkshape=self.chunkshape,
                                  obj=self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_01(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          atom=self.atom, shape=self.shape)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_shape_02(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          atom=self.atom, shape=self.shape)
        #ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.shape)
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)

    def test_kwargs_obj_atom(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          atom=self.atom)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_shape(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_shape(self):
        ptarr = self.h5file.create_earray(self.where, self.name,
                                          title=self.title,
                                          chunkshape=self.chunkshape,
                                          obj=self.obj,
                                          atom=self.atom,
                                          shape=self.shape)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, self.obj.shape[0])
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertEqual(ptarr.chunkshape, self.chunkshape)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_error(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_earray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom)

    def test_kwargs_obj_shape_error(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_earray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_01(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_earray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=self.shape)

    def test_kwargs_obj_atom_shape_error_02(self):
        #atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_earray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=self.atom,
                          shape=shape)

    def test_kwargs_obj_atom_shape_error_03(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_earray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom,
                          shape=shape)


#----------------------------------------------------------------------

def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    # theSuite.addTest(unittest.makeSuite(BasicWriteTestCase))
    # theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase1))
    # theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase2))
    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicWriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic2WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic3WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic4WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic5WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic6WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic7WriteTestCase))
        theSuite.addTest(unittest.makeSuite(Basic8WriteTestCase))
        theSuite.addTest(unittest.makeSuite(EmptyEArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Empty2EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(SlicesEArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Slices2EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(EllipsisEArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis2EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis3EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibComprTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(BloscComprTestCase))
        theSuite.addTest(unittest.makeSuite(BloscShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(LZOComprTestCase))
        theSuite.addTest(unittest.makeSuite(LZOShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ComprTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ShuffleTestCase))
        theSuite.addTest(unittest.makeSuite(FloatTypeTestCase))
        theSuite.addTest(unittest.makeSuite(ComplexTypeTestCase))
        theSuite.addTest(unittest.makeSuite(StringTestCase))
        theSuite.addTest(unittest.makeSuite(String2TestCase))
        theSuite.addTest(unittest.makeSuite(StringComprTestCase))
        theSuite.addTest(unittest.makeSuite(
            SizeOnDiskInMemoryPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(OffsetStrideTestCase))
        theSuite.addTest(unittest.makeSuite(Fletcher32TestCase))
        theSuite.addTest(unittest.makeSuite(AllFiltersTestCase))
        theSuite.addTest(unittest.makeSuite(CloseCopyTestCase))
        theSuite.addTest(unittest.makeSuite(OpenCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex1TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex2TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex3TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex4TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex5TestCase))
        theSuite.addTest(unittest.makeSuite(TruncateOpenTestCase))
        theSuite.addTest(unittest.makeSuite(TruncateCloseTestCase))
        theSuite.addTest(unittest.makeSuite(ZeroSizedTestCase))
        theSuite.addTest(unittest.makeSuite(MDAtomNoReopen))
        theSuite.addTest(unittest.makeSuite(MDAtomReopen))
        theSuite.addTest(unittest.makeSuite(AccessClosedTestCase))
        theSuite.addTest(unittest.makeSuite(TestCreateEArrayArgs))
    if common.heavy:
        theSuite.addTest(unittest.makeSuite(Slices3EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Slices4EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis4EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis5EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis6EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(Ellipsis7EArrayTestCase))
        theSuite.addTest(unittest.makeSuite(MD3WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD5WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD6WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD7WriteTestCase))
        theSuite.addTest(unittest.makeSuite(MD10WriteTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex6TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex7TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex8TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex9TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex10TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex11TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex12TestCase))
        theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase1))
        theSuite.addTest(unittest.makeSuite(Rows64bitsTestCase2))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = test_enum
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-05-09
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Test module for enumerated types under PyTables."""

import unittest
import operator
import itertools

import tables
from tables.tests import common


class CreateColTestCase(common.PyTablesTestCase):
    """Test creating enumerated column descriptions."""

    def _createCol(self, enum, dflt, base='uint32', shape=()):
        """Create and check an enumerated column description."""

        enumcol = tables.EnumCol(enum, dflt, base=base, shape=shape)
        sameEnum = tables.Enum(enum)
        self.assertEqual(enumcol.type, 'enum')
        self.assertEqual(enumcol.dtype.base.name, enumcol.base.type)
        # To avoid 'LongInt' vs 'Int' issues
        # self.assertEqual(enumcol.dflt, sameEnum[dflt])
        self.assertEqual(int(enumcol.dflt), int(sameEnum[dflt]))
        self.assertEqual(enumcol.dtype.shape, shape)
        self.assertEqual(enumcol.enum, sameEnum)

    def test00a_validFromEnum(self):
        """Describing an enumerated column from an enumeration."""
        colors = tables.Enum(['red', 'green', 'blue'])
        self._createCol(colors, 'red')

    def test00b_validFromDict(self):
        """Describing an enumerated column from a dictionary."""
        colors = {'red': 4, 'green': 2, 'blue': 1}
        self._createCol(colors, 'red')

    def test00c_validFromList(self):
        """Describing an enumerated column from a list."""
        colors = ['red', 'green', 'blue']
        self._createCol(colors, 'red')

    def test00d_invalidFromType(self):
        """Describing an enumerated column from an invalid object."""
        colors = 123
        self.assertRaises(TypeError, self._createCol, colors, 'red')

    def test01_invalidDflt(self):
        """Describing an enumerated column with an invalid default object."""
        colors = {'red': 4, 'green': 2, 'blue': 1}
        self.assertRaises(KeyError, self._createCol, colors, 'black')

    def test02a_validDtypeBroader(self):
        """Describing an enumerated column with a broader type."""
        colors = {'red': 4, 'green': 2, 'blue': 1}
        self._createCol(colors, 'red', 'int64')

    def test02b_invalidDtypeTooNarrow(self):
        """Describing an enumerated column with a too narrow type."""
        colors = ['e%d' % i for i in range(300)]
        self.assertRaises(TypeError, self._createCol, colors, 'e0', 'uint8')

    def test03a_validShapeMD(self):
        """Describing an enumerated column with multidimensional shape."""
        colors = ['red', 'green', 'blue']
        self._createCol(colors, 'red', shape=(2,))

    def test04a_validReprEnum(self):
        """Checking the string representation of an enumeration."""
        colors = tables.Enum(['red', 'green', 'blue'])
        enumcol = tables.EnumCol(colors, 'red', base='uint32', shape=())
        # needed due to "Hash randomization" (default on python 3.3)
        template = (
            "EnumCol(enum=Enum({%s}), dflt='red', base=UInt32Atom(shape=(), "
            "dflt=0), shape=(), pos=None)"
        )
        permitations = [
            template % ', '.join(items) for items in itertools.permutations(
                ("'blue': 2", "'green': 1", "'red': 0"))
        ]
        self.assertTrue(repr(enumcol) in permitations)

    def test99a_nonIntEnum(self):
        """Describing an enumerated column of floats (not implemented)."""
        colors = {'red': 1.0}
        self.assertRaises(NotImplementedError, self._createCol, colors, 'red',
                          base=tables.FloatAtom())

    def test99b_nonIntDtype(self):
        """Describing an enumerated column encoded as floats.

        (not implemented).

        """
        colors = ['red', 'green', 'blue']
        self.assertRaises(
            NotImplementedError, self._createCol, colors, 'red', 'float64')

    def test99b_nonScalarEnum(self):
        """Describing an enumerated column of non-scalars (not implemented)."""
        colors = {'red': (1, 2, 3)}
        self.assertRaises(NotImplementedError, self._createCol, colors, 'red',
                          base=tables.IntAtom(shape=3))


class CreateAtomTestCase(common.PyTablesTestCase):
    """Test creating enumerated atoms."""

    def _createAtom(self, enum, dflt, base='uint32', shape=()):
        """Create and check an enumerated atom."""

        enumatom = tables.EnumAtom(enum, dflt, base=base, shape=shape)
        sameEnum = tables.Enum(enum)
        self.assertEqual(enumatom.type, 'enum')
        self.assertEqual(enumatom.dtype.base.name, enumatom.base.type)
        self.assertEqual(enumatom.shape, shape)
        self.assertEqual(enumatom.enum, sameEnum)

    def test00a_validFromEnum(self):
        """Describing an enumerated atom from an enumeration."""
        colors = tables.Enum(['red', 'green', 'blue'])
        self._createAtom(colors, 'red')

    def test00b_validFromDict(self):
        """Describing an enumerated atom from a dictionary."""
        colors = {'red': 4, 'green': 2, 'blue': 1}
        self._createAtom(colors, 'red')

    def test00c_validFromList(self):
        """Describing an enumerated atom from a list."""
        colors = ['red', 'green', 'blue']
        self._createAtom(colors, 'red')

    def test00d_invalidFromType(self):
        """Describing an enumerated atom from an invalid object."""
        colors = 123
        self.assertRaises(TypeError, self._createAtom, colors, 'red')

    def test02a_validDtypeBroader(self):
        """Describing an enumerated atom with a broader type."""
        colors = {'red': 4, 'green': 2, 'blue': 1}
        self._createAtom(colors, 'red', base='int64')

    def test02b_invalidDtypeTooNarrow(self):
        """Describing an enumerated atom with a too narrow type."""
        colors = ['e%d' % i for i in range(300)]
        self.assertRaises(TypeError, self._createAtom, colors, 'red', 'uint8')

    def test03a_validShapeMD(self):
        """Describing an enumerated atom with multidimensional shape."""
        colors = ['red', 'green', 'blue']
        self._createAtom(colors, 'red', shape=(2,))

    def test99a_nonIntEnum(self):
        """Describing an enumerated atom of floats (not implemented)."""
        colors = {'red': 1.0}
        self.assertRaises(NotImplementedError, self._createAtom, colors, 'red',
                          base=tables.FloatAtom())

    def test99b_nonIntDtype(self):
        """Describing an enumerated atom encoded as a float.

        (not implemented).

        """
        colors = ['red', 'green', 'blue']
        self.assertRaises(
            NotImplementedError, self._createAtom, colors, 'red', 'float64')

    def test99b_nonScalarEnum(self):
        """Describing an enumerated atom of non-scalars (not implemented)."""
        colors = {'red': (1, 2, 3)}
        self.assertRaises(NotImplementedError, self._createAtom, colors, 'red',
                          base=tables.IntAtom(shape=3))


class EnumTableTestCase(common.TempFileMixin, common.PyTablesTestCase):
    """Test tables with enumerated columns."""

    enum = tables.Enum({'red': 4, 'green': 2, 'blue': 1, 'black': 0})
    defaultName = 'black'
    valueInEnum = enum.red
    valueOutOfEnum = 1234
    enumType = 'uint16'

    def _description(self, shape=()):
        class TestDescription(tables.IsDescription):
            rid = tables.IntCol(pos=0)
            rcolor = tables.EnumCol(
                self.enum, self.defaultName,
                base=self.enumType, shape=shape, pos=1)

        return TestDescription

    def test00a_reopen(self):
        """Reopening a file with tables using enumerated data."""

        self.h5file.create_table(
            '/', 'test', self._description(), title=self._getMethodName())

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum('rcolor'), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test00b_reopenMD(self):
        """
        Reopening a file with tables using enumerated multi-dimensional
        data.
        """

        self.h5file.create_table(
            '/', 'test', self._description((2,)), title=self._getMethodName())

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum('rcolor'), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test01_rowAppend(self):
        """Appending enumerated values using ``row.append()``."""

        tbl = self.h5file.create_table(
            '/', 'test', self._description(), title=self._getMethodName())

        appended = [
            (10, self.valueInEnum),
            (20, self.valueOutOfEnum)]

        row = tbl.row

        row['rid'] = appended[0][0]
        row['rcolor'] = appended[0][1]
        row.append()

        row['rid'] = appended[1][0]
        self.assertRaises(
            ValueError, operator.setitem, row, 'rcolor', appended[1][1])

        tbl.flush()
        tbl.flavor = 'python'
        read = tbl.read()
        common.verbosePrint(
            "* appended value: %s\n"
            "* read value: %s\n"
            % (appended[:-1], read))
        self.assertEqual(
            appended[:-1], read, "Written and read values differ.")

    def test02_append(self):
        """Appending enumerated values using ``table.append()``."""

        tbl = self.h5file.create_table(
            '/', 'test', self._description(), title=self._getMethodName())

        appended = [
            (10, self.valueInEnum),
            (20, self.valueOutOfEnum)]

        tbl.append(appended)
        tbl.flush()
        tbl.flavor = 'python'
        read = tbl.read()
        common.verbosePrint(
            "* appended value: %s\n"
            "* read value: %s\n"
            % (appended, read))
        self.assertEqual(appended, read, "Written and read values differ.")

    def test03_setitem(self):
        """Changing enumerated values using ``table.__setitem__()``."""

        tbl = self.h5file.create_table(
            '/', 'test', self._description(), title=self._getMethodName())

        appended = [
            (10, self.valueInEnum),
            (20, self.valueInEnum)]
        tbl.append(appended)

        written = [
            (10, self.valueInEnum),
            (20, self.valueOutOfEnum)]
        tbl[:] = written
        tbl.flavor = 'python'
        read = tbl.read()
        common.verbosePrint(
            "* written value: %s\n"
            "* read value: %s\n"
            % (written, read))
        self.assertEqual(written, read, "Written and read values differ.")

    def test04_multidim(self):
        """Appending multi-dimensional enumerated data."""

        tbl = self.h5file.create_table(
            '/', 'test', self._description((2,)), title=self._getMethodName())

        appended = [
            (10, (self.valueInEnum, self.valueOutOfEnum)),
            (20, (self.valueInEnum, self.valueOutOfEnum))]

        row = tbl.row
        row['rid'] = appended[0][0]
        self.assertRaises(
            ValueError, operator.setitem, row, 'rcolor', appended[0][1])

        tbl.append(appended)
        tbl.flush()
        tbl.flavor = 'python'
        read = tbl.read()
        for i in range(len(appended)):
            self.assertEqual(appended[i][0], read[i][0],
                             "Written and read values differ.")
            self.assertEqual(appended[i][1][0], read[i][1][0],
                             "Written and read values differ.")
            self.assertEqual(appended[i][1][1], read[i][1][1],
                             "Written and read values differ.")

    def test05_where(self):
        """Searching enumerated data."""

        tbl = self.h5file.create_table(
            '/', 'test', self._description(), title=self._getMethodName())

        appended = [
            (10, self.valueInEnum),
            (20, self.valueInEnum),
            (30, self.valueOutOfEnum)]
        tbl.append(appended)
        tbl.flush()

        searched = [
            (row['rid'], row['rcolor'])
            for row in tbl.where('rcolor == v', {'v': self.valueInEnum})]
        common.verbosePrint(
            "* ``valueInEnum``: %s\n"
            "* ``rcolor`` column: ``%s``\n"
            "* ``searched``: %s\n"
            "* Should look like: %s\n"
            % (self.valueInEnum, tbl.cols.rcolor, searched, appended[:-1]))
        self.assertEqual(
            searched, appended[:-1], "Search returned incorrect results.")


class EnumEArrayTestCase(common.TempFileMixin, common.PyTablesTestCase):
    """Test extendable arrays of enumerated values."""

    enum = tables.Enum({'red': 4, 'green': 2, 'blue': 1, 'black': 0})
    valueInEnum = enum.red
    valueOutOfEnum = 1234
    enumType = 'uint16'

    def _atom(self, shape=()):
        return tables.EnumAtom(
            self.enum, 'red', base=self.enumType, shape=shape)

    def test00a_reopen(self):
        """Reopening a file with extendable arrays using enumerated data."""

        self.h5file.create_earray(
            '/', 'test', self._atom(), shape=(0,),
            title=self._getMethodName())
        self.h5file.root.test.flavor = 'python'

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test00b_reopenMD(self):
        """
        Reopening a file with extendable arrays using enumerated
        multi-dimensional data.
        """

        self.h5file.create_earray(
            '/', 'test', self._atom(), shape=(0, 2),
            title=self._getMethodName())
        self.h5file.root.test.flavor = 'python'

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test_enum_default_persistence_red(self):
        dflt = 'red'
        atom = tables.EnumAtom(
            self.enum, dflt, base=self.enumType, shape=())

        self.h5file.create_earray('/', 'test', atom, shape=(0,),
                                  title=self._getMethodName())
        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

        self.assertEqual(
            self.h5file.root.test.atom.dflt, self.enum[dflt],
            "The default value of enumerated type was not restored correctly "
            "from disk.")

    def test_enum_default_persistence_green(self):
        dflt = 'green'
        atom = tables.EnumAtom(
            self.enum, dflt, base=self.enumType, shape=())

        self.h5file.create_earray('/', 'test', atom, shape=(0,),
                                  title=self._getMethodName())
        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

        self.assertEqual(
            self.h5file.root.test.atom.dflt, self.enum[dflt],
            "The default value of enumerated type was not restored correctly "
            "from disk.")

    def test_enum_default_persistence_blue(self):
        dflt = 'blue'
        atom = tables.EnumAtom(
            self.enum, dflt, base=self.enumType, shape=())

        self.h5file.create_earray('/', 'test', atom, shape=(0,),
                                  title=self._getMethodName())
        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

        self.assertEqual(
            self.h5file.root.test.atom.dflt, self.enum[dflt],
            "The default value of enumerated type was not restored correctly "
            "from disk.")

    def test_enum_default_persistence_black(self):
        dflt = 'black'
        atom = tables.EnumAtom(
            self.enum, dflt, base=self.enumType, shape=())

        self.h5file.create_earray('/', 'test', atom, shape=(0,),
                                  title=self._getMethodName())
        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

        self.assertEqual(
            self.h5file.root.test.atom.dflt, self.enum[dflt],
            "The default value of enumerated type was not restored correctly "
            "from disk.")

    def test01_append(self):
        """Appending scalar elements of enumerated values."""

        earr = self.h5file.create_earray(
            '/', 'test', self._atom(), shape=(0,),
            title=self._getMethodName())
        earr.flavor = 'python'

        appended = [self.valueInEnum, self.valueOutOfEnum]

        earr.append(appended)
        earr.flush()
        read = earr.read()
        self.assertEqual(appended, read, "Written and read values differ.")

    def test02_appendMD(self):
        """Appending multi-dimensional elements of enumerated values."""

        earr = self.h5file.create_earray(
            '/', 'test', self._atom(), shape=(0, 2),
            title=self._getMethodName())
        earr.flavor = 'python'

        appended = [
            [self.valueInEnum, self.valueOutOfEnum],
            [self.valueInEnum, self.valueOutOfEnum]]

        earr.append(appended)
        earr.flush()
        read = earr.read()
        self.assertEqual(appended, read, "Written and read values differ.")

    def test03_setitem(self):
        """Changing enumerated values using ``earray.__setitem__()``."""

        earr = self.h5file.create_earray(
            '/', 'test', self._atom(), shape=(0,),
            title=self._getMethodName())
        earr.flavor = 'python'

        appended = (self.valueInEnum, self.valueInEnum)
        earr.append(appended)

        written = [self.valueInEnum, self.valueOutOfEnum]
        earr[:] = written
        read = earr.read()
        self.assertEqual(written, read, "Written and read values differ.")


class EnumVLArrayTestCase(common.TempFileMixin, common.PyTablesTestCase):
    """Test variable-length arrays of enumerated values."""

    enum = tables.Enum({'red': 4, 'green': 2, 'blue': 1, 'black': 0})
    valueInEnum = enum.red
    valueOutOfEnum = 1234
    enumType = 'uint16'

    def _atom(self, shape=()):
        return tables.EnumAtom(
            self.enum, 'red', base=self.enumType, shape=shape)

    def test00a_reopen(self):
        """Reopening a file with variable-length arrays using
        enumerated data."""

        self.h5file.create_vlarray(
            '/', 'test', self._atom(),
            title=self._getMethodName())
        self.h5file.root.test.flavor = 'python'

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test00b_reopenMD(self):
        """
        Reopening a file with variable-length arrays using enumerated
        multi-dimensional data.
        """

        self.h5file.create_vlarray(
            '/', 'test', self._atom((2,)),
            title=self._getMethodName())
        self.h5file.root.test.flavor = 'python'

        self._reopen()

        self.assertEqual(
            self.h5file.root.test.get_enum(), self.enum,
            "Enumerated type was not restored correctly from disk.")

    def test01_append(self):
        """Appending scalar elements of enumerated values."""

        vlarr = self.h5file.create_vlarray(
            '/', 'test', self._atom(),
            title=self._getMethodName())
        vlarr.flavor = 'python'

        appended = [
            [self.valueInEnum, ],
            [self.valueInEnum, self.valueOutOfEnum]]

        vlarr.append(appended[0])
        vlarr.append(appended[1])
        vlarr.flush()
        read = vlarr.read()
        common.verbosePrint(
            "* appended value: %s\n"
            "* read value: %s\n"
            % (appended, read))
        self.assertEqual(appended, read, "Written and read values differ.")

    def test02_appendMD(self):
        """Appending multi-dimensional elements of enumerated values."""

        vlarr = self.h5file.create_vlarray(
            '/', 'test', self._atom((2,)),
            title=self._getMethodName())
        vlarr.flavor = 'python'

        appended = [
            [[self.valueInEnum, self.valueInEnum], ],
            [[self.valueInEnum, self.valueOutOfEnum],
             [self.valueInEnum, self.valueInEnum]]]

        vlarr.append(appended[0])
        vlarr.append(appended[1])
        vlarr.flush()
        read = vlarr.read()
        common.verbosePrint(
            "* appended value: %s\n"
            "* read value: %s\n"
            % (appended, read))
        self.assertEqual(appended, read, "Written and read values differ.")

    def test03_setitem(self):
        """Changing enumerated values using ``vlarray.__setitem__()``."""

        vlarr = self.h5file.create_vlarray(
            '/', 'test', self._atom(),
            title=self._getMethodName())
        vlarr.flavor = 'python'

        appended = (self.valueInEnum, self.valueInEnum)
        vlarr.append(appended)

        written = [self.valueInEnum, self.valueOutOfEnum]
        vlarr[0] = written
        read = vlarr.read()
        common.verbosePrint(
            "* written value: %s\n"
            "* read value: %s\n"
            % (written, read))
        self.assertEqual(written, read[0], "Written and read values differ.")


def suite():
    """Return a test suite consisting of all the test cases in the module."""

    # These two are for including Enum's doctests here.
    import doctest
    from tables.misc import enum
    theSuite = unittest.TestSuite()
    niter = 1

    # theSuite.addTest(unittest.makeSuite(EnumTableTestCase))
    for i in range(niter):
        theSuite.addTest(doctest.DocTestSuite(enum))
        theSuite.addTest(unittest.makeSuite(CreateColTestCase))
        theSuite.addTest(unittest.makeSuite(CreateAtomTestCase))
        theSuite.addTest(unittest.makeSuite(EnumTableTestCase))
        theSuite.addTest(unittest.makeSuite(EnumEArrayTestCase))
        theSuite.addTest(unittest.makeSuite(EnumVLArrayTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_expression
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2009-06-14
# Author: Francesc Alted - faltet@pytables.org
#
# $Id$
#
########################################################################

"""Test module for evaluating expressions under PyTables."""

from __future__ import print_function
import unittest

import numpy as np
import tables as tb
from tables.tests import common

# An example of record


class Record(tb.IsDescription):
    colInt32 = tb.Int32Col()
    colInt64 = tb.Int64Col()
    colFloat32 = tb.Float32Col()
    colFloat64 = tb.Float64Col()
    colComplex = tb.ComplexCol(itemsize=16)


# Helper functions
def get_sliced_vars(npvars, start, stop, step):
    npvars_ = {}
    for name, var in npvars.iteritems():
        if hasattr(var, "__len__"):
            npvars_[name] = var[start:stop:step]
        else:
            npvars_[name] = var
    return npvars_


def get_sliced_vars2(npvars, start, stop, step, shape, maindim):
    npvars_ = {}
    slices = [slice(None) for dim in shape]
    slices[maindim] = slice(start, stop, step)
    for name, var in npvars.iteritems():
        npvars_[name] = var.__getitem__(tuple(slices))
    return npvars_


# Basic tests
class ExprTestCase(common.TempFileMixin, common.PyTablesTestCase):

    # The shape for the variables in expressions
    shape = (10, 20)

    def setUp(self):
        super(ExprTestCase, self).setUp()
        # The expression
        self.expr = "2 * a*b + c"
        # Define the NumPy variables to be used in expression
        N = np.prod(self.shape)
        self.a = a = np.arange(0, N, dtype='int32').reshape(self.shape)
        self.b = b = np.arange(N, 2 * N, dtype='int64').reshape(self.shape)
        self.c = c = np.arange(2 * N, 3*N, dtype='int32').reshape(self.shape)
        self.r1 = r1 = np.empty(N, dtype='int64').reshape(self.shape)
        self.npvars = {"a": a, "b": b, "c": c, }
        # Define other variables, if needed
        root = self.h5file.root
        if self.kind == "Array":
            self.a = self.h5file.create_array(root, "a", a)
            self.b = self.h5file.create_array(root, "b", b)
            self.c = self.h5file.create_array(root, "c", c)
            self.r1 = self.h5file.create_array(root, "r1", r1)
        elif self.kind == "CArray":
            self.a = self.h5file.create_carray(
                root, "a", atom=tb.Atom.from_dtype(a.dtype), shape=self.shape)
            self.b = self.h5file.create_carray(
                root, "b", atom=tb.Atom.from_dtype(b.dtype), shape=self.shape)
            self.c = self.h5file.create_carray(
                root, "c", atom=tb.Atom.from_dtype(c.dtype), shape=self.shape)
            self.r1 = self.h5file.create_carray(
                root, "r1", atom=tb.Atom.from_dtype(r1.dtype),
                shape=self.shape)
            self.a[:] = a
            self.b[:] = b
            self.c[:] = c
        elif self.kind == "EArray":
            shape = list(self.shape)
            shape[0] = 0
            self.a = self.h5file.create_earray(
                root, "a", atom=tb.Atom.from_dtype(a.dtype), shape=shape)
            self.b = self.h5file.create_earray(
                root, "b", atom=tb.Atom.from_dtype(b.dtype), shape=shape)
            self.c = self.h5file.create_earray(
                root, "c", atom=tb.Atom.from_dtype(c.dtype), shape=shape)
            self.r1 = self.h5file.create_earray(
                root, "r1", atom=tb.Atom.from_dtype(r1.dtype), shape=shape)
            self.a.append(a)
            self.b.append(b)
            self.c.append(c)
            self.r1.append(r1)   # Fill with uninitialized values
        elif self.kind == "Column":
            ra = np.rec.fromarrays(
                [a, b, c, r1],
                dtype="%si4,%si8,%si4,%si8" % ((self.shape[1:],)*4))
            t = self.h5file.create_table(root, "t", ra)
            self.a = t.cols.f0
            self.b = t.cols.f1
            self.c = t.cols.f2
            self.d = t.cols.f3
        self.vars = {"a": self.a, "b": self.b, "c": self.c, }

    def test00_simple(self):
        """Checking that expression is correctly evaluated."""

        expr = tb.Expr(self.expr, self.vars)
        r1 = expr.eval()
        r2 = eval(self.expr, self.npvars)
        if common.verbose:
            print("Computed expression:", repr(r1))
            print("Should look like:", repr(r2))
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01_out(self):
        """Checking that expression is correctly evaluated (`out` param)"""

        expr = tb.Expr(self.expr, self.vars)
        expr.set_output(self.r1)
        r1 = expr.eval()
        if self.kind != "NumPy":
            r1 = r1[:]
        r2 = eval(self.expr, self.npvars)
        if common.verbose:
            print("Computed expression:", repr(r1))
            print("Should look like:", repr(r2))
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


class ExprNumPy(ExprTestCase):
    kind = "NumPy"


class ExprArray(ExprTestCase):
    kind = "Array"


class ExprCArray(ExprTestCase):
    kind = "CArray"


class ExprEArray(ExprTestCase):
    kind = "EArray"


class ExprColumn(ExprTestCase):
    kind = "Column"


# Test for mixed containers
class MixedContainersTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(MixedContainersTestCase, self).setUp()
        # The expression
        self.expr = "2 * a*b + c**2+d**2+e-f+g"
        # Create a directory in file for outputs
        root = self.h5file.root
        outs = self.h5file.create_group(root, "outs")
        # Define the NumPy variables to be used in expression
        N = np.prod(self.shape)
        # Initial values for variables
        a = np.arange(0, N, dtype='int32').reshape(self.shape)
        b = np.arange(N, 2 * N, dtype='int64').reshape(self.shape)
        c = np.arange(2 * N, 3*N, dtype='int32').reshape(self.shape)
        d = np.arange(3 * N, 4*N, dtype='int32').reshape(self.shape)
        e = np.arange(4 * N, 5*N, dtype='int32').reshape(self.shape)
        self.f = f = long(3)   # a regular python type
        self.g = g = np.int16(2)   # a NumPy scalar type
        # Original values
        self.npvars = {"a": a, "b": b, "c": c, "d": d, "e": e, "f": f, "g": g}
        rnda = b.copy()
        # ndarray input and output
        self.a = a
        self.rnda = rnda
        # Array input and output
        self.b = self.h5file.create_array(root, "b", b)
        self.rarr = self.b.copy(outs)
        # CArray input and output
        self.c = self.h5file.create_carray(
            root, "c", atom=tb.Atom.from_dtype(c.dtype), shape=self.shape)
        self.c[:] = c
        self.rcarr = self.c.copy(outs)
        # EArray input and output
        eshape = list(self.shape)
        eshape[0] = 0
        self.d = self.h5file.create_earray(
            root, "d", atom=tb.Atom.from_dtype(d.dtype), shape=eshape)
        self.d.append(d)
        self.rearr = self.d.copy(outs)
        # Column input and output
        rtype = {}
        colshape = self.shape[1:]
        for i, col in enumerate((a, b, c, d, e, rnda)):
            rtype['f%d' % i] = tb.Col.from_sctype(col.dtype.type, colshape)
        t = self.h5file.create_table(root, "t", rtype)
        nrows = self.shape[0]
        row = t.row
        for nrow in range(nrows):
            for i, col in enumerate((a, b, c, d, e, rnda)):
                row['f%d' % i] = col[nrow]
            row.append()
        t.flush()
        self.e = t.cols.f4
        self.rcol = t.cols.f5
        # Input vars
        self.vars = {"a": self.a, "b": self.b, "c": self.c, "d": self.d,
                     "e": self.e, "f": self.f, "g": self.g, }

    def test00a_simple(self):
        """Checking expressions with mixed objects."""

        expr = tb.Expr(self.expr, self.vars)
        r1 = expr.eval()
        r2 = eval(self.expr, self.npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)

        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test00b_simple_scalars(self):
        """Checking that scalars in expression evaluate correctly."""

        expr_str = "2 * f + g"
        expr = tb.Expr(expr_str, self.vars)
        r1 = expr.eval()
        r2 = eval(expr_str, self.npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(
            r1.shape == r2.shape and r1.dtype == r2.dtype and r1 == r2,
            "Evaluate is returning a wrong value.")

    def test01a_out(self):
        """Checking expressions with mixed objects (`out` param)"""

        expr = tb.Expr(self.expr, self.vars)
        for r1 in self.rnda, self.rarr, self.rcarr, self.rearr, self.rcol:
            if common.verbose:
                print("Checking output container:", type(r1))
            expr.set_output(r1)
            r1 = expr.eval()
            if not isinstance(r1, type(self.rnda)):
                r1 = r1[:]
            r2 = eval(self.expr, self.npvars)
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")

    def test01b_out_scalars(self):
        """Checking expressions with mixed objects (`out` param, scalars)"""

        if len(self.shape) > 1:
            # This test is only meant for undimensional outputs
            return
        expr_str = "2 * f + g"
        expr = tb.Expr(expr_str, self.vars)
        for r1 in self.rnda, self.rarr, self.rcarr, self.rearr, self.rcol:
            if common.verbose:
                print("Checking output container:", type(r1))
            expr.set_output(r1)
            r1 = expr.eval()
            r1 = r1[()]  # convert a 0-dim array into a scalar
            r2 = eval(expr_str, self.npvars)
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")

    def test02a_sss(self):
        """Checking mixed objects and start, stop, step (I)"""

        start, stop, step = (self.start, self.stop, 1)
        expr = tb.Expr(self.expr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = expr.eval()
        npvars = get_sliced_vars(self.npvars, start, stop, step)
        r2 = eval(self.expr, npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test02b_sss(self):
        """Checking mixed objects and start, stop, step (II)"""

        start, stop, step = (0, self.shape[0], self.step)
        expr = tb.Expr(self.expr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = expr.eval()
        npvars = get_sliced_vars(self.npvars, start, stop, step)
        r2 = eval(self.expr, npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test02c_sss(self):
        """Checking mixed objects and start, stop, step (III)"""

        start, stop, step = (self.start, self.stop, self.step)
        expr = tb.Expr(self.expr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = expr.eval()
        npvars = get_sliced_vars(self.npvars, start, stop, step)
        r2 = eval(self.expr, npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test03_sss(self):
        """Checking start, stop, step as numpy.int64."""

        start, stop, step = [np.int64(i) for i in
                                     (self.start, self.stop, self.step)]
        expr = tb.Expr(self.expr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = expr.eval()
        npvars = get_sliced_vars(self.npvars, start, stop, step)
        r2 = eval(self.expr, npvars)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


class MixedContainers0(MixedContainersTestCase):
    shape = (1,)
    start, stop, step = (0, 1, 1)


class MixedContainers1(MixedContainersTestCase):
    shape = (10,)
    start, stop, step = (3, 6, 2)


class MixedContainers2(MixedContainersTestCase):
    shape = (10, 5)
    start, stop, step = (2, 9, 3)


class MixedContainers3(MixedContainersTestCase):
    shape = (10, 3, 2)
    start, stop, step = (2, -1, 1)


# Test for unaligned objects
class UnalignedObject(common.PyTablesTestCase):

    def test00_simple(self):
        """Checking expressions with unaligned objects."""

        # Build unaligned arrays
        a0 = np.empty(10, dtype="int8")
        a1 = np.arange(10, dtype="int32")
        a2 = a1.copy()
        a3 = a2.copy()
        ra = np.rec.fromarrays([a0, a1, a2, a3])
        # The inputs
        a = ra['f1']
        b = ra['f2']
        self.assertEqual(a.flags.aligned, False)
        self.assertEqual(b.flags.aligned, False)
        # The expression
        sexpr = "2 * a + b"
        expr = tb.Expr(sexpr)
        r1 = expr.eval()
        r2 = eval(sexpr)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01_md(self):
        """Checking expressions with unaligned objects (MD version)"""

        # Build unaligned arrays
        a0 = np.empty((10, 4), dtype="int8")
        a1 = np.arange(10 * 4, dtype="int32").reshape(10, 4)
        a2 = a1.copy()
        a3 = a2.copy()
        ra = np.rec.fromarrays([a0, a1, a2, a3])
        # The inputs
        a = ra['f1']
        b = ra['f2']
        self.assertEqual(a.flags.aligned, False)
        self.assertEqual(b.flags.aligned, False)
        # The expression
        sexpr = "2 * a + b"
        expr = tb.Expr(sexpr)
        r1 = expr.eval()
        r2 = eval(sexpr)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


# Test for non-contiguous objects
class NonContiguousObject(common.PyTablesTestCase):

    def test00_simple(self):
        """Checking expressions with non-contiguous objects"""

        # Build non-contiguous arrays as inputs
        a = np.arange(10, dtype="int32")
        b = a[::2]
        a = b * 2
        self.assertEqual(b.flags.contiguous, False)
        self.assertEqual(b.flags.aligned, True)
        # The expression
        sexpr = "2 * a + b"
        expr = tb.Expr(sexpr)
        r1 = expr.eval()
        r2 = eval(sexpr)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01a_md(self):
        """Checking expressions with non-contiguous objects (MD version, I)"""

        # Build non-contiguous arrays
        a = np.arange(10 * 4, dtype="int32").reshape(10, 4)
        b = a[::2]
        a = b * 2
        self.assertEqual(b.flags.contiguous, False)
        self.assertEqual(b.flags.aligned, True)
        # The expression
        sexpr = "2 * a + b"
        expr = tb.Expr(sexpr)
        r1 = expr.eval()
        r2 = eval(sexpr)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01b_md(self):
        """Checking expressions with non-contiguous objects (MD version, II)"""

        # Build non-contiguous arrays
        a = np.arange(10 * 4, dtype="int32").reshape(10, 4)
        b = a[:, ::2]
        a = b * 2
        self.assertEqual(b.flags.contiguous, False)
        self.assertEqual(b.flags.aligned, True)
        # The expression
        sexpr = "2 * a + b"
        expr = tb.Expr(sexpr)
        r1 = expr.eval()
        r2 = eval(sexpr)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


# Test for errors
class ExprError(common.TempFileMixin, common.PyTablesTestCase):

    # The shape for the variables in expressions
    shape = (10,)

    def setUp(self):
        super(ExprError, self).setUp()
        # Define the NumPy variables to be used in expression
        N = np.prod(self.shape)
        self.a = np.arange(N, dtype='int32').reshape(self.shape)
        self.b = np.arange(N, dtype='int64').reshape(self.shape)
        self.c = np.arange(N, dtype='int32').reshape(self.shape)
        self.r1 = np.empty(N, dtype='int64').reshape(self.shape)

    def _test00_shape(self):
        """Checking that inconsistent shapes are detected."""

        self.b = self.b.reshape(self.shape+(1,))
        expr = "a * b + c"
        vars_ = {"a": self.a, "b": self.b, "c": self.c, }
        expr = tb.Expr(expr, vars_)
        self.assertRaises(ValueError, expr.eval)

    def test02_uint64(self):
        """Checking that uint64 arrays in expression are detected."""

        self.b = self.b.view('uint64')
        expr = "a * b + c"
        vars_ = {"a": self.a, "b": self.b, "c": self.c, }
        self.assertRaises(NotImplementedError, tb.Expr, expr, vars_)

    def test03_table(self):
        """Checking that tables in expression are detected."""

        class Rec(tb.IsDescription):
            col1 = tb.Int32Col()
            col2 = tb.Int64Col()

        t = self.h5file.create_table("/", "a", Rec)
        expr = "a * b + c"
        vars_ = {"a": t, "b": self.b, "c": self.c, }
        self.assertRaises(TypeError, tb.Expr, expr, vars_)

    def test04_nestedcols(self):
        """Checking that nested cols in expression are detected."""

        class Nested(tb.IsDescription):
            col1 = tb.Int32Col()

            class col2(tb.IsDescription):
                col3 = tb.Int64Col()

        t = self.h5file.create_table("/", "a", Nested)
        expr = "a * b + c"
        # The next non-nested column should work
        a = t.cols.col2.col3
        vars_ = {"a": a, "b": self.b, "c": self.c, }
        expr = tb.Expr(expr, vars_)
        r1 = expr.eval()
        self.assertTrue(r1 is not None)
        # But a nested column should not
        a = t.cols.col2
        vars_ = {"a": a, "b": self.b, "c": self.c, }
        self.assertRaises(TypeError, tb.Expr, expr, vars_)

    def test05_vlarray(self):
        """Checking that VLArrays in expression are detected."""

        vla = self.h5file.create_vlarray("/", "a", tb.Int32Col())
        expr = "a * b + c"
        vars_ = {"a": vla, "b": self.b, "c": self.c, }
        self.assertRaises(TypeError, tb.Expr, expr, vars_)


# Test for broadcasting arrays
class BroadcastTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking broadcast in expression."""

        shapes = (self.shape1, self.shape2, self.shape3)
        # Build arrays with different shapes as inputs
        a = np.arange(np.prod(shapes[0]), dtype="i4").reshape(shapes[0])
        b = np.arange(np.prod(shapes[1]), dtype="i4").reshape(shapes[1])
        c = np.arange(np.prod(shapes[2]), dtype="i4").reshape(shapes[2])
        root = self.h5file.root
        if a.shape[0] > 0:
            a1 = self.h5file.create_array(root, 'a1', a)
        else:
            a1 = self.h5file.create_earray(
                root, 'a1', atom=tb.Int32Col(), shape=a.shape)
        self.assertTrue(a1 is not None)
        b1 = self.h5file.create_array(root, 'b1', b)
        self.assertTrue(b1 is not None)
        c1 = self.h5file.create_array(root, 'c1', c)
        self.assertTrue(c1 is not None)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        r1 = expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shapes:", self.shape1, self.shape2, self.shape3)
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


class Broadcast0(BroadcastTestCase):
    shape1 = (0, 3, 4)
    shape2 = (3, 4)
    shape3 = (4,)


class Broadcast1(BroadcastTestCase):
    shape1 = (2, 3, 4)
    shape2 = (3, 4)
    shape3 = (4,)


class Broadcast2(BroadcastTestCase):
    shape1 = (3, 4,)
    shape2 = (3, 4)
    shape3 = (4,)


class Broadcast3(BroadcastTestCase):
    shape1 = (4,)
    shape2 = (3, 4)
    shape3 = (4,)


class Broadcast4(BroadcastTestCase):
    shape1 = (1,)
    shape2 = (3, 4)
    shape3 = (4,)


class Broadcast5(BroadcastTestCase):
    shape1 = (1,)
    shape2 = (3, 1)
    shape3 = (4,)


# Test for different length inputs
class DiffLengthTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking different length inputs in expression."""

        shapes = (list(self.shape1), list(self.shape2), list(self.shape3))
        # Build arrays with different shapes as inputs
        a = np.arange(np.prod(shapes[0]), dtype="i4").reshape(shapes[0])
        b = np.arange(np.prod(shapes[1]), dtype="i4").reshape(shapes[1])
        c = np.arange(np.prod(shapes[2]), dtype="i4").reshape(shapes[2])
        # The expression
        expr = tb.Expr("2 * a + b-c")
        r1 = expr.eval()
        # Compute the minimum length for shapes
        maxdim = max([len(shape) for shape in shapes])
        minlen = min([shape[0] for i, shape in enumerate(shapes)
                      if len(shape) == maxdim])
        for i, shape in enumerate(shapes):
            if len(shape) == maxdim:
                shape[0] = minlen
        # Build arrays with the new shapes as inputs
        a = np.arange(np.prod(shapes[0]), dtype="i4").reshape(shapes[0])
        self.assertTrue(a is not None)
        b = np.arange(np.prod(shapes[1]), dtype="i4").reshape(shapes[1])
        self.assertTrue(b is not None)
        c = np.arange(np.prod(shapes[2]), dtype="i4").reshape(shapes[2])
        self.assertTrue(c is not None)
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shapes:", self.shape1, self.shape2, self.shape3)
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


class DiffLength0(DiffLengthTestCase):
    shape1 = (0,)
    shape2 = (10,)
    shape3 = (20,)


class DiffLength1(DiffLengthTestCase):
    shape1 = (3,)
    shape2 = (10,)
    shape3 = (20,)


class DiffLength2(DiffLengthTestCase):
    shape1 = (3, 4)
    shape2 = (2, 3, 4)
    shape3 = (4, 3, 4)


class DiffLength3(DiffLengthTestCase):
    shape1 = (1, 3, 4)
    shape2 = (2, 3, 4)
    shape3 = (4, 3, 4)


class DiffLength4(DiffLengthTestCase):
    shape1 = (0, 3, 4)
    shape2 = (2, 3, 4)
    shape3 = (4, 3, 4)


# Test for different type inputs
class TypesTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_bool(self):
        """Checking booleans in expression."""

        # Build arrays with different shapes as inputs
        a = np.array([True, False, True])
        b = np.array([False, True, False])
        root = self.h5file.root
        a1 = self.h5file.create_array(root, 'a1', a)
        self.assertTrue(a1 is not None)
        b1 = self.h5file.create_array(root, 'b1', b)
        self.assertTrue(b1 is not None)
        expr = tb.Expr("a | b")
        r1 = expr.eval()
        r2 = eval("a | b")
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01_shortint(self):
        """Checking int8,uint8,int16,uint16 and int32 in expression."""

        for dtype in 'int8', 'uint8', 'int16', 'uint16', 'int32':
            if common.verbose:
                print("Checking type:", dtype)
            # Build arrays with different shapes as inputs
            a = np.array([1, 2, 3], dtype)
            b = np.array([3, 4, 5], dtype)
            root = self.h5file.root
            a1 = self.h5file.create_array(root, 'a1', a)
            b1 = self.h5file.create_array(root, 'b1', b)
            two = np.int32(2)
            expr = tb.Expr("two * a1-b1")
            r1 = expr.eval()
            a = np.array([1, 2, 3], 'int32')
            b = np.array([3, 4, 5], 'int32')
            r2 = eval("two * a-b")
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertEqual(r1.dtype, r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")
            # Remove created leaves
            a1.remove()
            b1.remove()

    def test02_longint(self):
        """Checking uint32 and int64 in expression."""

        for dtype in 'uint32', 'int64':
            if common.verbose:
                print("Checking type:", dtype)
            # Build arrays with different shapes as inputs
            a = np.array([1, 2, 3], dtype)
            b = np.array([3, 4, 5], dtype)
            root = self.h5file.root
            a1 = self.h5file.create_array(root, 'a1', a)
            b1 = self.h5file.create_array(root, 'b1', b)
            expr = tb.Expr("2 * a1-b1")
            r1 = expr.eval()
            a = np.array([1, 2, 3], 'int64')
            b = np.array([3, 4, 5], 'int64')
            r2 = eval("2 * a-b")
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertEqual(r1.dtype, r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")
            # Remove created leaves
            a1.remove()
            b1.remove()

    def test03_float(self):
        """Checking float32 and float64 in expression."""

        for dtype in 'float32', 'float64':
            if common.verbose:
                print("Checking type:", dtype)
            # Build arrays with different shapes as inputs
            a = np.array([1, 2, 3], dtype)
            b = np.array([3, 4, 5], dtype)
            root = self.h5file.root
            a1 = self.h5file.create_array(root, 'a1', a)
            b1 = self.h5file.create_array(root, 'b1', b)
            expr = tb.Expr("2 * a1-b1")
            r1 = expr.eval()
            a = np.array([1, 2, 3], dtype)
            b = np.array([3, 4, 5], dtype)
            r2 = eval("2 * a-b")
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertEqual(r1.dtype, r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")
            # Remove created leaves
            a1.remove()
            b1.remove()

    def test04_complex(self):
        """Checking complex64 and complex128 in expression."""

        for dtype in 'complex64', 'complex128':
            if common.verbose:
                print("Checking type:", dtype)
            # Build arrays with different shapes as inputs
            a = np.array([1, 2j, 3 + 2j], dtype)
            b = np.array([3, 4j, 5 + 1j], dtype)
            root = self.h5file.root
            a1 = self.h5file.create_array(root, 'a1', a)
            b1 = self.h5file.create_array(root, 'b1', b)
            expr = tb.Expr("2 * a1-b1")
            r1 = expr.eval()
            a = np.array([1, 2j, 3 + 2j], 'complex128')
            b = np.array([3, 4j, 5 + 1j], 'complex128')
            r2 = eval("2 * a-b")
            if common.verbose:
                print("Computed expression:", repr(r1), r1.dtype)
                print("Should look like:", repr(r2), r2.dtype)
            self.assertEqual(r1.dtype, r2.dtype)
            self.assertTrue(common.areArraysEqual(r1, r2),
                            "Evaluate is returning a wrong value.")
            # Remove created leaves
            a1.remove()
            b1.remove()

    def test05_string(self):
        """Checking strings in expression."""

        # Build arrays with different shapes as inputs
        a = np.array(['a', 'bd', 'cd'], 'S')
        b = np.array(['a', 'bdcd', 'ccdc'], 'S')
        root = self.h5file.root
        a1 = self.h5file.create_array(root, 'a1', a)
        self.assertTrue(a1 is not None)
        b1 = self.h5file.create_array(root, 'b1', b)
        self.assertTrue(b1 is not None)
        expr = tb.Expr("(a1 > b'a') | ( b1 > b'b')")
        r1 = expr.eval()
        r2 = eval("(a > b'a') | ( b > b'b')")
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


# Test for different functions
class FunctionsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking some math functions in expression."""

        # Build arrays with different shapes as inputs
        a = np.array([.1, .2, .3])
        b = np.array([.3, .4, .5])
        root = self.h5file.root
        a1 = self.h5file.create_array(root, 'a1', a)
        self.assertTrue(a1 is not None)
        b1 = self.h5file.create_array(root, 'b1', b)
        self.assertTrue(b1 is not None)
        # The expression
        expr = tb.Expr("sin(a1) * sqrt(b1)")
        r1 = expr.eval()
        r2 = np.sin(a) * np.sqrt(b)
        if common.verbose:
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")


# Test for EArrays with maindim != 0
class MaindimTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking other dimensions than 0 as main dimension."""

        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        root = self.h5file.root
        shape[self.maindim] = 0
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        r1 = expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01_out(self):
        """Checking other dimensions than 0 as main dimension (out)"""

        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        root = self.h5file.root
        shape[self.maindim] = 0
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        r1.append(c)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        expr.set_output(r1)
        expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")

    def test02_diff_in_maindims(self):
        """Checking different main dimensions in inputs."""

        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        root = self.h5file.root
        shape2 = shape[:]
        shape[self.maindim] = 0
        shape2[0] = 0
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        self.assertTrue(a1.maindim, self.maindim)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape2)
        self.assertEqual(b1.maindim, 0)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        r1.append(c)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        r1 = expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test03_diff_in_out_maindims(self):
        """Checking different maindims in inputs and output."""

        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        root = self.h5file.root
        shape2 = shape[:]
        shape[self.maindim] = 0
        shape2[0] = 0
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        self.assertTrue(a1.maindim, self.maindim)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape2)
        self.assertEqual(r1.maindim, 0)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        r1.append(c)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        expr.set_output(r1)
        expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")

    def test04_diff_in_out_maindims_lengths(self):
        """Checking different maindims and lengths in inputs and output."""

        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        root = self.h5file.root
        shape2 = shape[:]
        shape[self.maindim] = 0
        shape2[0] = 0
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        self.assertTrue(a1.maindim, self.maindim)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape2)
        self.assertEqual(r1.maindim, 0)
        a1.append(a)
        a1.append(a)
        b1.append(b)
        b1.append(b)
        c1.append(c)
        c1.append(c)
        r1.append(c)   # just once so that output is smaller
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        expr.set_output(r1)
        # This should raise an error
        self.assertRaises(ValueError, expr.eval)


class Maindim0(MaindimTestCase):
    maindim = 1
    shape = (1, 2)


class Maindim1(MaindimTestCase):
    maindim = 1
    shape = (2, 3)


class Maindim2(MaindimTestCase):
    maindim = 1
    shape = (2, 3, 4)


class Maindim3(MaindimTestCase):
    maindim = 2
    shape = (2, 3, 4)


# Test `append` mode flag in `set_output()`
class AppendModeTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test01_append(self):
        """Checking append mode in `set_output()`"""

        shape = [3, 2]
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        shape[1] = 0
        root = self.h5file.root
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        if not self.append:
            r1.append(c)
        # The expression
        expr = tb.Expr("2 * a1 + b1-c1")
        expr.set_output(r1, append_mode=self.append)
        expr.eval()
        r2 = eval("2 * a + b-c")
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")


class AppendModeTrue(AppendModeTestCase):
    append = True


class AppendModeFalse(AppendModeTestCase):
    append = False


# Test for `__iter__()` iterator
class iterTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(iterTestCase, self).setUp()
        shape = list(self.shape)
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        c = a.copy()
        self.npvars = {'a': a, 'b': b, 'c': c}
        shape[self.maindim] = 0
        root = self.h5file.root
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        c1 = self.h5file.create_earray(
            root, 'c1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        c1.append(c)
        self.vars = {'a': a1, 'b': b1, 'c': c1}
        # The expression
        self.sexpr = "2 * a + b-c"

    def test00_iter(self):
        """Checking the __iter__ iterator."""

        expr = tb.Expr(self.sexpr, self.vars)
        r1 = np.array([row for row in expr])
        r2 = eval(self.sexpr, self.npvars)
        if common.verbose:
            print("Tested shape, maindim:", self.shape, self.maindim)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")

    def test01a_sss(self):
        """Checking the __iter__ iterator (with ranges, I)"""

        start, stop, step = self.range_[0], None, None
        expr = tb.Expr(self.sexpr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = np.array([row for row in expr])
        npvars = get_sliced_vars2(
            self.npvars, start, stop, step, self.shape, self.maindim)
        r2 = eval(self.sexpr, npvars)
        if common.verbose:
            print("Tested shape, maindim:", self.shape, self.maindim)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")

    def test01b_sss(self):
        """Checking the __iter__ iterator (with ranges, II)"""

        start, stop, step = self.range_[0], self.range_[2], None
        expr = tb.Expr(self.sexpr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = np.array([row for row in expr])
        npvars = get_sliced_vars2(
            self.npvars, start, stop, step, self.shape, self.maindim)
        r2 = eval(self.sexpr, npvars)
        if common.verbose:
            print("Tested shape, maindim:", self.shape, self.maindim)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")

    def test01c_sss(self):
        """Checking the __iter__ iterator (with ranges, III)"""

        start, stop, step = self.range_
        expr = tb.Expr(self.sexpr, self.vars)
        expr.set_inputs_range(start, stop, step)
        r1 = np.array([row for row in expr])
        npvars = get_sliced_vars2(
            self.npvars, start, stop, step, self.shape, self.maindim)
        r2 = eval(self.sexpr, npvars)
        if common.verbose:
            print("Tested shape, maindim:", self.shape, self.maindim)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r2),
                        "Evaluate is returning a wrong value.")


class iter0(iterTestCase):
    maindim = 0
    shape = (0,)
    range_ = (1, 2, 1)


class iter1(iterTestCase):
    maindim = 0
    shape = (3,)
    range_ = (1, 2, 1)


class iter2(iterTestCase):
    maindim = 0
    shape = (3, 2)
    range_ = (0, 3, 2)


class iter3(iterTestCase):
    maindim = 1
    shape = (3, 2)
    range_ = (0, 3, 2)


class iter4(iterTestCase):
    maindim = 2
    shape = (3, 2, 1)
    range_ = (1, 3, 2)


class iter5(iterTestCase):
    maindim = 2
    shape = (1, 2, 5)
    range_ = (0, 4, 2)


# Test for set_output_range
class setOutputRangeTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking the range selection for output."""

        shape = list(self.shape)
        start, stop, step = self.range_
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        r = a.copy()
        root = self.h5file.root
        a1 = self.h5file.create_array(root, 'a1', a)
        self.assertTrue(a1 is not None)
        b1 = self.h5file.create_array(root, 'b1', b)
        self.assertTrue(b1 is not None)
        r1 = self.h5file.create_array(root, 'r1', r)
        # The expression
        expr = tb.Expr("a1-b1-1")
        expr.set_output(r1)
        expr.set_output_range(start, stop, step)
        expr.eval()
        r2 = eval("a-b-1")
        r[start:stop:step] = r2[:len(xrange(start, stop, step))]
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r), r.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r),
                        "Evaluate is returning a wrong value.")

    def test01_maindim(self):
        """Checking the range selection for output (maindim > 0)"""

        shape = list(self.shape)
        start, stop, step = self.range_
        # Build input arrays
        a = np.arange(np.prod(shape), dtype="i4").reshape(shape)
        b = a.copy()
        r = a.copy()
        shape[self.maindim] = 0
        root = self.h5file.root
        a1 = self.h5file.create_earray(
            root, 'a1', atom=tb.Int32Col(), shape=shape)
        b1 = self.h5file.create_earray(
            root, 'b1', atom=tb.Int32Col(), shape=shape)
        r1 = self.h5file.create_earray(
            root, 'r1', atom=tb.Int32Col(), shape=shape)
        a1.append(a)
        b1.append(b)
        r1.append(r)
        # The expression
        expr = tb.Expr("a1-b1-1")
        expr.set_output(r1)
        expr.set_output_range(start, stop, step)
        expr.eval()
        r2 = eval("a-b-1")
        lsl = tuple([slice(None)] * self.maindim)
        # print "lsl-->", lsl + (slice(start,stop,step),)
        l = len(xrange(start, stop, step))
        r.__setitem__(lsl + (slice(start, stop, step),),
                      r2.__getitem__(lsl + (slice(0, l),)))
        if common.verbose:
            print("Tested shape:", shape)
            print("Computed expression:", repr(r1[:]), r1.dtype)
            print("Should look like:", repr(r), r.dtype)
        self.assertTrue(common.areArraysEqual(r1[:], r),
                        "Evaluate is returning a wrong value.")


class setOutputRange0(setOutputRangeTestCase):
    maindim = 0
    shape = (10,)
    range_ = (0, 1, 2)


class setOutputRange1(setOutputRangeTestCase):
    maindim = 0
    shape = (10,)
    range_ = (0, 10, 2)


class setOutputRange2(setOutputRangeTestCase):
    maindim = 0
    shape = (10,)
    range_ = (1, 10, 2)


class setOutputRange3(setOutputRangeTestCase):
    maindim = 0
    shape = (10, 1)
    range_ = (1, 10, 3)


class setOutputRange4(setOutputRangeTestCase):
    maindim = 0
    shape = (10, 2)
    range_ = (1, 10, 3)


class setOutputRange5(setOutputRangeTestCase):
    maindim = 0
    shape = (5, 3, 1)
    range_ = (1, 5, 1)


class setOutputRange6(setOutputRangeTestCase):
    maindim = 1
    shape = (2, 5)
    range_ = (1, 3, 2)


class setOutputRange7(setOutputRangeTestCase):
    maindim = 1
    shape = (2, 5, 1)
    range_ = (1, 3, 2)


class setOutputRange8(setOutputRangeTestCase):
    maindim = 2
    shape = (1, 3, 5)
    range_ = (1, 5, 2)


class setOutputRange9(setOutputRangeTestCase):
    maindim = 3
    shape = (1, 3, 4, 5)
    range_ = (1, 5, 3)


# Test for very large inputs
class VeryLargeInputsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00_simple(self):
        """Checking very large inputs."""

        shape = self.shape
        # Use filters so as to not use too much space
        if tb.which_lib_version("blosc") is not None:
            filters = tb.Filters(complevel=1, complib='blosc', shuffle=False)
        elif tb.which_lib_version("lzo") is not None:
            filters = tb.Filters(complevel=1, complib='lzo', shuffle=False)
        else:
            filters = tb.Filters(complevel=1, shuffle=False)
        # Build input arrays
        root = self.h5file.root
        a = self.h5file.create_carray(root, 'a', atom=tb.Float64Atom(dflt=3),
                                      shape=shape, filters=filters)
        self.assertTrue(a is not None)
        b = self.h5file.create_carray(root, 'b', atom=tb.Float64Atom(dflt=2),
                                      shape=shape, filters=filters)
        self.assertTrue(b is not None)
        r1 = self.h5file.create_carray(root, 'r1', atom=tb.Float64Atom(dflt=3),
                                       shape=shape, filters=filters)
        # The expression
        expr = tb.Expr("a * b-6")   # Should give 0
        expr.set_output(r1)
        expr.eval()
        r1 = r1[-10:]  # Get the last ten rows
        r2 = np.zeros(10, dtype='float64')
        if common.verbose:
            print("Tested shape:", shape)
            print("Ten last rows:", repr(r1), r1.dtype)
            print("Should look like:", repr(r2), r2.dtype)
        self.assertTrue(common.areArraysEqual(r1, r2),
                        "Evaluate is returning a wrong value.")

    def test01_iter(self):
        """Checking very large inputs (__iter__ version)"""

        shape = self.shape
        if shape[0] >= 2**24:
            # The iterator is much more slower, so don't run it for
            # extremeley large arrays.
            if common.verbose:
                print("Skipping this *very* long test")
            return
        # Use filters so as to not use too much space
        if tb.which_lib_version("lzo") is not None:
            filters = tb.Filters(complevel=1, complib='lzo', shuffle=False)
        else:
            filters = tb.Filters(complevel=1, shuffle=False)
        # Build input arrays
        root = self.h5file.root
        a = self.h5file.create_carray(root, 'a', atom=tb.Int32Atom(dflt=1),
                                      shape=shape, filters=filters)
        self.assertTrue(a is not None)
        b = self.h5file.create_carray(root, 'b', atom=tb.Int32Atom(dflt=2),
                                      shape=shape, filters=filters)
        self.assertTrue(b is not None)
        r1 = self.h5file.create_carray(root, 'r1', atom=tb.Int32Atom(dflt=3),
                                       shape=shape, filters=filters)
        # The expression
        expr = tb.Expr("a-b + 1")
        r1 = sum(expr)     # Should give 0
        if common.verbose:
            print("Tested shape:", shape)
            print("Cummulated sum:", r1)
            print("Should look like:", 0)
        self.assertEqual(r1, 0, "Evaluate is returning a wrong value.")

# The next can go on regular tests, as it should be light enough


class VeryLargeInputs1(VeryLargeInputsTestCase):
    shape = (2**20,)    # larger than any internal I/O buffers

# The next is only meant for 'heavy' mode as it can take more than 1 minute
# on modern machines


class VeryLargeInputs2(VeryLargeInputsTestCase):
    shape = (2**32 + 1,)    # check that arrays > 32-bit are supported


#----------------------------------------------------------------------

def suite():
    """Return a test suite consisting of all the test cases in the module."""

    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(ExprNumPy))
        theSuite.addTest(unittest.makeSuite(ExprArray))
        theSuite.addTest(unittest.makeSuite(ExprCArray))
        theSuite.addTest(unittest.makeSuite(ExprEArray))
        theSuite.addTest(unittest.makeSuite(ExprColumn))
        theSuite.addTest(unittest.makeSuite(MixedContainers0))
        theSuite.addTest(unittest.makeSuite(MixedContainers1))
        theSuite.addTest(unittest.makeSuite(MixedContainers2))
        theSuite.addTest(unittest.makeSuite(MixedContainers3))
        theSuite.addTest(unittest.makeSuite(UnalignedObject))
        theSuite.addTest(unittest.makeSuite(NonContiguousObject))
        theSuite.addTest(unittest.makeSuite(ExprError))
        theSuite.addTest(unittest.makeSuite(Broadcast0))
        theSuite.addTest(unittest.makeSuite(Broadcast1))
        theSuite.addTest(unittest.makeSuite(Broadcast2))
        theSuite.addTest(unittest.makeSuite(Broadcast3))
        theSuite.addTest(unittest.makeSuite(Broadcast4))
        theSuite.addTest(unittest.makeSuite(Broadcast5))
        theSuite.addTest(unittest.makeSuite(DiffLength0))
        theSuite.addTest(unittest.makeSuite(DiffLength1))
        theSuite.addTest(unittest.makeSuite(DiffLength2))
        theSuite.addTest(unittest.makeSuite(DiffLength3))
        theSuite.addTest(unittest.makeSuite(DiffLength4))
        theSuite.addTest(unittest.makeSuite(TypesTestCase))
        theSuite.addTest(unittest.makeSuite(FunctionsTestCase))
        theSuite.addTest(unittest.makeSuite(Maindim0))
        theSuite.addTest(unittest.makeSuite(Maindim1))
        theSuite.addTest(unittest.makeSuite(Maindim2))
        theSuite.addTest(unittest.makeSuite(Maindim3))
        theSuite.addTest(unittest.makeSuite(AppendModeTrue))
        theSuite.addTest(unittest.makeSuite(AppendModeFalse))
        theSuite.addTest(unittest.makeSuite(iter0))
        theSuite.addTest(unittest.makeSuite(iter1))
        theSuite.addTest(unittest.makeSuite(iter2))
        theSuite.addTest(unittest.makeSuite(iter3))
        theSuite.addTest(unittest.makeSuite(iter4))
        theSuite.addTest(unittest.makeSuite(iter5))
        theSuite.addTest(unittest.makeSuite(setOutputRange0))
        theSuite.addTest(unittest.makeSuite(setOutputRange1))
        theSuite.addTest(unittest.makeSuite(setOutputRange2))
        theSuite.addTest(unittest.makeSuite(setOutputRange3))
        theSuite.addTest(unittest.makeSuite(setOutputRange4))
        theSuite.addTest(unittest.makeSuite(setOutputRange5))
        theSuite.addTest(unittest.makeSuite(setOutputRange6))
        theSuite.addTest(unittest.makeSuite(setOutputRange7))
        theSuite.addTest(unittest.makeSuite(setOutputRange8))
        theSuite.addTest(unittest.makeSuite(setOutputRange9))
        theSuite.addTest(unittest.makeSuite(VeryLargeInputs1))
        if common.heavy:
            theSuite.addTest(unittest.makeSuite(VeryLargeInputs2))
    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_garbage
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-09-20
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Test module for detecting uncollectable garbage in PyTables.

This test module *must* be loaded in the last place.  It just checks for
the existence of uncollectable garbage in ``gc.garbage`` after running
all the tests.

"""

from __future__ import print_function
import unittest
import gc

from tables.tests import common


class GarbageTestCase(common.PyTablesTestCase):

    """Test for uncollectable garbage."""

    def test00(self):
        """Checking for uncollectable garbage."""

        garbageLen = len(gc.garbage)
        if garbageLen == 0:
            return  # success

        if common.verbose:
            classCount = {}
            # Count uncollected objects for each class.
            for obj in gc.garbage:
                objClass = obj.__class__.__name__
                if objClass in classCount:
                    classCount[objClass] += 1
                else:
                    classCount[objClass] = 1
            incidence = ['``%s``: %d' % (cls, cnt)
                         for (cls, cnt) in classCount.iteritems()]
            print("Class incidence:", ', '.join(incidence))
        self.fail("Possible leak: %d uncollected objects." % garbageLen)


def suite():
    """Return a test suite consisting of all the test cases in the module."""

    theSuite = unittest.TestSuite()
    theSuite.addTest(unittest.makeSuite(GarbageTestCase))
    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_hdf5compat
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-09-29
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Test module for compatibility with plain HDF files."""

import unittest
import tempfile
import shutil
import os

import numpy

import tables
from tables.tests import common
from tables.tests.common import allequal


class HDF5CompatibilityTestCase(common.PyTablesTestCase):

    """Base class for HDF5 compatibility tests.

    Test cases deriving from this class must define an ``h5fname``
    attribute with the name of the file to be opened, and a ``_test()``
    method with checks on the opened file.

    """

    def setUp(self):
        self.h5file = None

    def tearDown(self):
        if self.h5file is not None:
            self.h5file.close()
        self.h5file = None

    def test(self):
        self.h5fname = self._testFilename(self.h5fname)
        self.h5file = tables.open_file(self.h5fname)
        self._test()


class EnumTestCase(HDF5CompatibilityTestCase):

    """Test for enumerated datatype.

    See ftp://ftp.hdfgroup.org/HDF5/current/src/unpacked/test/enum.c.

    """

    h5fname = 'smpl_enum.h5'

    def _test(self):
        self.assertTrue('/EnumTest' in self.h5file)

        arr = self.h5file.get_node('/EnumTest')
        self.assertTrue(isinstance(arr, tables.Array))

        enum = arr.get_enum()
        expectedEnum = tables.Enum(['RED', 'GREEN', 'BLUE', 'WHITE', 'BLACK'])
        self.assertEqual(enum, expectedEnum)

        data = list(arr.read())
        expectedData = [
            enum[name] for name in
            ['RED', 'GREEN', 'BLUE', 'WHITE', 'BLACK',
             'RED', 'GREEN', 'BLUE', 'WHITE', 'BLACK']]
        self.assertEqual(data, expectedData)


class NumericTestCase(HDF5CompatibilityTestCase):

    """Test for several numeric datatypes.

    See
    ftp://ftp.ncsa.uiuc.edu/HDF/files/hdf5/samples/[fiu]l?{8,16,32,64}{be,le}.c
    (they seem to be no longer available).

    """

    def _test(self):
        self.assertTrue('/TestArray' in self.h5file)

        arr = self.h5file.get_node('/TestArray')
        self.assertTrue(isinstance(arr, tables.Array))

        self.assertEqual(arr.atom.type, self.type)
        self.assertEqual(arr.byteorder, self.byteorder)
        self.assertEqual(arr.shape, (6, 5))

        data = arr.read()
        expectedData = numpy.array([
            [0, 1, 2, 3, 4],
            [1, 2, 3, 4, 5],
            [2, 3, 4, 5, 6],
            [3, 4, 5, 6, 7],
            [4, 5, 6, 7, 8],
            [5, 6, 7, 8, 9]], dtype=self.type)
        self.assertTrue(common.areArraysEqual(data, expectedData))


class F64BETestCase(NumericTestCase):
    h5fname = 'smpl_f64be.h5'
    type = 'float64'
    byteorder = 'big'


class F64LETestCase(NumericTestCase):
    h5fname = 'smpl_f64le.h5'
    type = 'float64'
    byteorder = 'little'


class I64BETestCase(NumericTestCase):
    h5fname = 'smpl_i64be.h5'
    type = 'int64'
    byteorder = 'big'


class I64LETestCase(NumericTestCase):
    h5fname = 'smpl_i64le.h5'
    type = 'int64'
    byteorder = 'little'


class I32BETestCase(NumericTestCase):
    h5fname = 'smpl_i32be.h5'
    type = 'int32'
    byteorder = 'big'


class I32LETestCase(NumericTestCase):
    h5fname = 'smpl_i32le.h5'
    type = 'int32'
    byteorder = 'little'


class ChunkedCompoundTestCase(HDF5CompatibilityTestCase):

    """Test for a more complex and chunked compound structure.

    This is generated by a chunked version of the example in
    ftp://ftp.ncsa.uiuc.edu/HDF/files/hdf5/samples/compound2.c.

    """

    h5fname = 'smpl_compound_chunked.h5'

    def _test(self):
        self.assertTrue('/CompoundChunked' in self.h5file)

        tbl = self.h5file.get_node('/CompoundChunked')
        self.assertTrue(isinstance(tbl, tables.Table))

        self.assertEqual(
            tbl.colnames,
            ['a_name', 'c_name', 'd_name', 'e_name', 'f_name', 'g_name'])

        self.assertEqual(tbl.coltypes['a_name'], 'int32')
        self.assertEqual(tbl.coldtypes['a_name'].shape, ())

        self.assertEqual(tbl.coltypes['c_name'], 'string')
        self.assertEqual(tbl.coldtypes['c_name'].shape, ())

        self.assertEqual(tbl.coltypes['d_name'], 'int16')
        self.assertEqual(tbl.coldtypes['d_name'].shape, (5, 10))

        self.assertEqual(tbl.coltypes['e_name'], 'float32')
        self.assertEqual(tbl.coldtypes['e_name'].shape, ())

        self.assertEqual(tbl.coltypes['f_name'], 'float64')
        self.assertEqual(tbl.coldtypes['f_name'].shape, (10,))

        self.assertEqual(tbl.coltypes['g_name'], 'uint8')
        self.assertEqual(tbl.coldtypes['g_name'].shape, ())

        for m in range(len(tbl)):
            row = tbl[m]
        # This version of the loop seems to fail because of ``iterrows()``.
        # for (m, row) in enumerate(tbl):
            self.assertEqual(row['a_name'], m)
            self.assertEqual(row['c_name'], b"Hello!")
            dRow = row['d_name']
            for n in range(5):
                for o in range(10):
                    self.assertEqual(dRow[n][o], m + n + o)
            self.assertAlmostEqual(row['e_name'], m * 0.96, places=6)
            fRow = row['f_name']
            for n in range(10):
                self.assertAlmostEqual(fRow[n], m * 1024.9637)
            self.assertEqual(row['g_name'], ord('m'))


class ContiguousCompoundTestCase(HDF5CompatibilityTestCase):

    """Test for support of native contiguous compound datasets.

    This example has been provided by Dav Clark.

    """

    h5fname = 'non-chunked-table.h5'

    def _test(self):
        self.assertTrue('/test_var/structure variable' in self.h5file)

        tbl = self.h5file.get_node('/test_var/structure variable')
        self.assertTrue(isinstance(tbl, tables.Table))

        self.assertEqual(
            tbl.colnames,
            ['a', 'b', 'c', 'd'])

        self.assertEqual(tbl.coltypes['a'], 'float64')
        self.assertEqual(tbl.coldtypes['a'].shape, ())

        self.assertEqual(tbl.coltypes['b'], 'float64')
        self.assertEqual(tbl.coldtypes['b'].shape, ())

        self.assertEqual(tbl.coltypes['c'], 'float64')
        self.assertEqual(tbl.coldtypes['c'].shape, (2,))

        self.assertEqual(tbl.coltypes['d'], 'string')
        self.assertEqual(tbl.coldtypes['d'].shape, ())

        for row in tbl.iterrows():
            self.assertEqual(row['a'], 3.0)
            self.assertEqual(row['b'], 4.0)
            self.assertTrue(allequal(row['c'], numpy.array([2.0, 3.0],
                                                           dtype="float64")))
            self.assertEqual(row['d'], b"d")

        self.h5file.close()


class ContiguousCompoundAppendTestCase(HDF5CompatibilityTestCase):

    """Test for appending data to native contiguous compound datasets."""

    h5fname = 'non-chunked-table.h5'

    def _test(self):
        self.assertTrue('/test_var/structure variable' in self.h5file)
        self.h5file.close()
        # Do a copy to a temporary to avoid modifying the original file
        h5fname_copy = tempfile.mktemp(".h5")
        shutil.copy(self.h5fname, h5fname_copy)
        # Reopen in 'a'ppend mode
        try:
            self.h5file = tables.open_file(h5fname_copy, 'a')
        except IOError:
            # Problems for opening (probably not permisions to write the file)
            return
        tbl = self.h5file.get_node('/test_var/structure variable')
        # Try to add rows to a non-chunked table (this should raise an error)
        self.assertRaises(tables.HDF5ExtError, tbl.append,
                          [(4.0, 5.0, [2.0, 3.0], 'd')])
        # Appending using the Row interface
        self.assertRaises(tables.HDF5ExtError, tbl.row.append)
        # Remove the file copy
        self.h5file.close()  # Close the handler first
        os.remove(h5fname_copy)


class ExtendibleTestCase(HDF5CompatibilityTestCase):

    """Test for extendible datasets.

    See the example programs in the Introduction to HDF5.

    """

    h5fname = 'smpl_SDSextendible.h5'

    def _test(self):
        self.assertTrue('/ExtendibleArray' in self.h5file)

        arr = self.h5file.get_node('/ExtendibleArray')
        self.assertTrue(isinstance(arr, tables.EArray))

        self.assertEqual(arr.byteorder, 'big')
        self.assertEqual(arr.atom.type, 'int32')
        self.assertEqual(arr.shape, (10, 5))
        self.assertEqual(arr.extdim, 0)
        self.assertEqual(len(arr), 10)

        data = arr.read()
        expectedData = numpy.array([
            [1, 1, 1, 3, 3],
            [1, 1, 1, 3, 3],
            [1, 1, 1, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0],
            [2, 0, 0, 0, 0]], dtype=arr.atom.type)

        self.assertTrue(common.areArraysEqual(data, expectedData))


class SzipTestCase(HDF5CompatibilityTestCase):
    """Test for native HDF5 files with datasets compressed with szip."""

    h5fname = 'test_szip.h5'

    def _test(self):
        self.assertTrue('/dset_szip' in self.h5file)

        arr = self.h5file.get_node('/dset_szip')
        filters = ("Filters(complib='szip', shuffle=False, fletcher32=False, "
                   "least_significant_digit=None)")
        self.assertEqual(repr(arr.filters), filters)


# this demonstrates github #203
class MatlabFileTestCase(common.PyTablesTestCase):

    def setUp(self):
        h5fname = 'matlab_file.mat'
        file_path = self._testFilename(h5fname)
        self.h5file = tables.open_file(file_path, 'r')

    def tearDown(self):
        self.h5file.close()

    def test_unicode(self):
        array = self.h5file.get_node(unicode('/'), unicode('a'))
        self.assertEqual(array.shape, (3, 1))

    # in Python 3 this will be the same as the test above
    def test_string(self):
        array = self.h5file.get_node('/', 'a')
        self.assertEqual(array.shape, (3, 1))

    def test_numpy_str(self):
        array = self.h5file.get_node(numpy.str_('/'), numpy.str_('a'))
        self.assertEqual(array.shape, (3, 1))


def suite():
    """Return a test suite consisting of all the test cases in the module."""

    theSuite = unittest.TestSuite()
    niter = 1

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(EnumTestCase))
        theSuite.addTest(unittest.makeSuite(F64BETestCase))
        theSuite.addTest(unittest.makeSuite(F64LETestCase))
        theSuite.addTest(unittest.makeSuite(I64BETestCase))
        theSuite.addTest(unittest.makeSuite(I64LETestCase))
        theSuite.addTest(unittest.makeSuite(I32BETestCase))
        theSuite.addTest(unittest.makeSuite(I32LETestCase))
        theSuite.addTest(unittest.makeSuite(ChunkedCompoundTestCase))
        theSuite.addTest(unittest.makeSuite(ContiguousCompoundTestCase))
        theSuite.addTest(unittest.makeSuite(ContiguousCompoundAppendTestCase))
        theSuite.addTest(unittest.makeSuite(ExtendibleTestCase))
        theSuite.addTest(unittest.makeSuite(SzipTestCase))
        theSuite.addTest(unittest.makeSuite(MatlabFileTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_indexes
# -*- coding: utf-8 -*-

from __future__ import print_function
import unittest
import os
import tempfile
import copy

from tables import *
from tables.index import Index, default_auto_index, default_index_filters
from tables.idxutils import calc_chunksize
from tables.tests.common import verbose, allequal, heavy, cleanup, \
    PyTablesTestCase, TempFileMixin
from tables.exceptions import OldIndexWarning

# To delete the internal attributes automagically
unittest.TestCase.tearDown = cleanup

import numpy


# Sensible parameters for indexing with small blocksizes
minRowIndex = 10
small_blocksizes = (96, 24, 6, 3)


class TDescr(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"", pos=1)
    var2 = BoolCol(dflt=0, pos=2)
    var3 = IntCol(dflt=0, pos=3)
    var4 = FloatCol(dflt=0, pos=4)


class BasicTestCase(PyTablesTestCase):
    compress = 0
    complib = "zlib"
    shuffle = 0
    fletcher32 = 0
    nrows = minRowIndex
    ss = small_blocksizes[2]

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()
        # Close the file
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        # Create a table
        title = "This is the IndexArray title"
        self.filters = Filters(complevel=self.compress,
                               complib=self.complib,
                               shuffle=self.shuffle,
                               fletcher32=self.fletcher32)
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        self.filters, self.nrows)
        for i in range(self.nrows):
            table.row['var1'] = str(i).encode('ascii')
            # table.row['var2'] = i > 2
            table.row['var2'] = i % 2
            table.row['var3'] = i
            table.row['var4'] = float(self.nrows - i - 1)
            table.row.append()
        table.flush()
        # Index all entries:
        for col in table.colinstances.itervalues():
            indexrows = col.create_index(_blocksizes=small_blocksizes)
        if verbose:
            print("Number of written rows:", self.nrows)
            print("Number of indexed rows:", indexrows)

        return

    def tearDown(self):
        self.fileh.close()
        # print "File %s not removed!" % self.file
        os.remove(self.file)
        cleanup(self)

    #----------------------------------------

    def test00_flushLastRow(self):
        """Checking flushing an Index incrementing only the last row."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_flushLastRow..." %
                  self.__class__.__name__)

        # Open the HDF5 file in append mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        # Add just 3 rows more
        for i in range(3):
            table.row['var1'] = str(i).encode('ascii')
            table.row.append()
        table.flush()  # redo the indexes
        idxcol = table.cols.var1.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)
            print("Elements in last row:", idxcol.indicesLR[-1])

        # Do a selection
        results = [p["var1"] for p in table.where('var1 == b"1"')]
        self.assertEqual(len(results), 2)
        self.assertEqual(results, [b'1']*2)

    def test00_update(self):
        """Checking automatic re-indexing after an update operation."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_update..." % self.__class__.__name__)

        # Open the HDF5 file in append mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        # Modify a couple of columns
        for i, row in enumerate(table.where("(var3>1) & (var3<5)")):
            row['var1'] = str(i)
            row['var3'] = i
            row.update()
        table.flush()  # redo the indexes
        idxcol1 = table.cols.var1.index
        idxcol3 = table.cols.var3.index
        if verbose:
            print("Dirtyness of var1 col:", idxcol1.dirty)
            print("Dirtyness of var3 col:", idxcol3.dirty)
        self.assertEqual(idxcol1.dirty, False)
        self.assertEqual(idxcol3.dirty, False)

        # Do a couple of selections
        results = [p["var1"] for p in table.where('var1 == b"1"')]
        self.assertEqual(len(results), 2)
        self.assertEqual(results, [b'1']*2)
        results = [p["var3"] for p in table.where('var3 == 0')]
        self.assertEqual(len(results), 2)
        self.assertEqual(results, [0]*2)

    def test01_readIndex(self):
        """Checking reading an Index (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-only mode
        self.fileh = open_file(self.file, mode="r")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        results = [p["var1"] for p in table.where('var1 == b"1"')]
        self.assertEqual(len(results), 1)
        self.assertEqual(results, [b'1'])

    def test02_readIndex(self):
        """Checking reading an Index (bool flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_readIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-only mode
        self.fileh = open_file(self.file, mode="r")
        table = self.fileh.root.table
        idxcol = table.cols.var2.index
        if verbose:
            print("Rows in table:", table.nrows)
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        results = [p["var2"] for p in table.where('var2 == True')]
        if verbose:
            print("Selected values:", results)
        self.assertEqual(len(results), self.nrows // 2)
        self.assertEqual(results, [True]*(self.nrows // 2))

    def test03_readIndex(self):
        """Checking reading an Index (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_readIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-only mode
        self.fileh = open_file(self.file, mode="r")
        table = self.fileh.root.table
        idxcol = table.cols.var3.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        results = [p["var3"] for p in table.where('(1<var3)&(var3<10)')]
        if verbose:
            print("Selected values:", results)
        self.assertEqual(len(results), min(10, table.nrows) - 2)
        self.assertEqual(results, range(2, min(10, table.nrows)))

    def test04_readIndex(self):
        """Checking reading an Index (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_readIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-only mode
        self.fileh = open_file(self.file, mode="r")
        table = self.fileh.root.table
        idxcol = table.cols.var4.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of rows in table:", table.nrows)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        results = [p["var4"] for p in table.where('var4 < 10')]
        # results = [p["var4"] for p in table.where('(1<var4)&(var4<10)')]
        if verbose:
            print("Selected values:", results)
        self.assertEqual(len(results), min(10, table.nrows))
        self.assertEqual(results, [float(i) for i in
                                   reversed(range(min(10, table.nrows)))])

    def test05_getWhereList(self):
        """Checking reading an Index with get_where_list (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_getWhereList..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var4.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        table.flavor = "python"
        rowList1 = table.get_where_list('var1 < b"10"')
        rowList2 = [p.nrow for p in table if p['var1'] < b"10"]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test06_getWhereList(self):
        """Checking reading an Index with get_where_list (bool flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06_getWhereList..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var2.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Rows in tables:", table.nrows)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        table.flavor = "numpy"
        rowList1 = table.get_where_list('var2 == False', sort=True)
        rowList2 = [p.nrow for p in table if p['var2'] is False]
        # Convert to a NumPy object
        rowList2 = numpy.array(rowList2, numpy.int64)
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertTrue(allequal(rowList1, rowList2))

    def test07_getWhereList(self):
        """Checking reading an Index with get_where_list (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_getWhereList..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var4.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        table.flavor = "python"
        rowList1 = table.get_where_list('var3 < 15', sort=True)
        rowList2 = [p.nrow for p in table if p["var3"] < 15]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test08_getWhereList(self):
        """Checking reading an Index with get_where_list (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_getWhereList..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var4.index
        if verbose:
            print("Max rows in buf:", table.nrowsinbuf)
            print("Number of elements per slice:", idxcol.slicesize)
            print("Chunk size:", idxcol.sorted.chunksize)

        # Do a selection
        table.flavor = "python"
        rowList1 = table.get_where_list('var4 < 10', sort=True)
        rowList2 = [p.nrow for p in table if p['var4'] < 10]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test09a_removeIndex(self):
        """Checking removing an index."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09a_removeIndex..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before deletion")
            print("var1 column:", table.cols.var1)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # delete the index
        table.cols.var1.remove_index()
        if verbose:
            print("After deletion")
            print("var1 column:", table.cols.var1)
        self.assertTrue(table.cols.var1.index is None)
        self.assertEqual(table.colindexed["var1"], 0)

        # re-create the index again
        indexrows = table.cols.var1.create_index(_blocksizes=small_blocksizes)
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        if verbose:
            print("After re-creation")
            print("var1 column:", table.cols.var1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)

    def test09b_removeIndex(self):
        """Checking removing an index (persistent version)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09b_removeIndex..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before deletion")
            print("var1 index column:", table.cols.var1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)
        # delete the index
        table.cols.var1.remove_index()

        # close and reopen the file
        self.fileh.close()
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index

        if verbose:
            print("After deletion")
            print("var1 column:", table.cols.var1)
        self.assertTrue(table.cols.var1.index is None)
        self.assertEqual(table.colindexed["var1"], 0)

        # re-create the index again
        indexrows = table.cols.var1.create_index(_blocksizes=small_blocksizes)
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        if verbose:
            print("After re-creation")
            print("var1 column:", table.cols.var1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)

    def test10a_moveIndex(self):
        """Checking moving a table with an index."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10a_moveIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before move")
            print("var1 column:", idxcol)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # Create a new group called "agroup"
        agroup = self.fileh.create_group("/", "agroup")

        # move the table to "agroup"
        table.move(agroup, "table2")
        if verbose:
            print("After move")
            print("var1 column:", idxcol)
        self.assertTrue(table.cols.var1.index is not None)
        self.assertEqual(table.colindexed["var1"], 1)

        # Some sanity checks
        table.flavor = "python"
        rowList1 = table.get_where_list('var1 < b"10"')
        rowList2 = [p.nrow for p in table if p['var1'] < b"10"]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test10b_moveIndex(self):
        """Checking moving a table with an index (persistent version)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10b_moveIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before move")
            print("var1 index column:", idxcol)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)
        # Create a new group called "agroup"
        agroup = self.fileh.create_group("/", "agroup")

        # move the table to "agroup"
        table.move(agroup, "table2")

        # close and reopen the file
        self.fileh.close()
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.agroup.table2
        idxcol = table.cols.var1.index

        if verbose:
            print("After move")
            print("var1 column:", idxcol)
        self.assertTrue(table.cols.var1.index is not None)
        self.assertEqual(table.colindexed["var1"], 1)

        # Some sanity checks
        table.flavor = "python"
        rowList1 = table.get_where_list('var1 < b"10"')
        rowList2 = [p.nrow for p in table if p['var1'] < b"10"]
        if verbose:
            print("Selected values:", rowList1, type(rowList1))
            print("Should look like:", rowList2, type(rowList2))
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test10c_moveIndex(self):
        """Checking moving a table with an index (small node cache)."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10c_moveIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a", node_cache_slots=10)
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before move")
            print("var1 column:", idxcol)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # Create a new group called "agroup"
        agroup = self.fileh.create_group("/", "agroup")

        # move the table to "agroup"
        table.move(agroup, "table2")
        if verbose:
            print("After move")
            print("var1 column:", idxcol)
        self.assertTrue(table.cols.var1.index is not None)
        self.assertEqual(table.colindexed["var1"], 1)

        # Some sanity checks
        table.flavor = "python"
        rowList1 = table.get_where_list('var1 < b"10"')
        rowList2 = [p.nrow for p in table if p['var1'] < b"10"]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test10d_moveIndex(self):
        """Checking moving a table with an index (no node cache)."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10d_moveIndex..." % self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a", node_cache_slots=0)
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before move")
            print("var1 column:", idxcol)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # Create a new group called "agroup"
        agroup = self.fileh.create_group("/", "agroup")

        # move the table to "agroup"
        table.move(agroup, "table2")
        if verbose:
            print("After move")
            print("var1 column:", idxcol)
        self.assertTrue(table.cols.var1.index is not None)
        self.assertEqual(table.colindexed["var1"], 1)

        # Some sanity checks
        table.flavor = "python"
        rowList1 = table.get_where_list('var1 < b"10"')
        rowList2 = [p.nrow for p in table if p['var1'] < b"10"]
        if verbose:
            print("Selected values:", rowList1)
            print("Should look like:", rowList2)
        self.assertEqual(len(rowList1), len(rowList2))
        self.assertEqual(rowList1, rowList2)

    def test11a_removeTableWithIndex(self):
        """Checking removing a table with indexes."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11a_removeTableWithIndex..." %
                  self.__class__.__name__)

        # Open the HDF5 file in read-write mode
        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before deletion")
            print("var1 column:", table.cols.var1)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # delete the table
        self.fileh.remove_node("/table")
        if verbose:
            print("After deletion")
        self.assertTrue("table" not in self.fileh.root)

        # re-create the table and the index again
        table = self.fileh.create_table("/", 'table', TDescr, "New table",
                                        self.filters, self.nrows)
        for i in range(self.nrows):
            table.row['var1'] = str(i)
            table.row['var2'] = i % 2
            table.row['var3'] = i
            table.row['var4'] = float(self.nrows - i - 1)
            table.row.append()
        table.flush()
        # Index all entries:
        for col in table.colinstances.itervalues():
            indexrows = col.create_index(_blocksizes=small_blocksizes)
            self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        if verbose:
            print("After re-creation")
            print("var1 column:", table.cols.var1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)

    def test11b_removeTableWithIndex(self):
        """Checking removing a table with indexes (persistent version 2)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11b_removeTableWithIndex..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table
        idxcol = table.cols.var1.index
        if verbose:
            print("Before deletion")
            print("var1 column:", table.cols.var1)
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)

        # delete the table
        self.fileh.remove_node("/table")
        if verbose:
            print("After deletion")
        self.assertTrue("table" not in self.fileh.root)

        # close and reopen the file
        self.fileh.close()
        self.fileh = open_file(self.file, mode="r+")

        # re-create the table and the index again
        table = self.fileh.create_table("/", 'table', TDescr, "New table",
                                        self.filters, self.nrows)
        for i in range(self.nrows):
            table.row['var1'] = str(i)
            table.row['var2'] = i % 2
            table.row['var3'] = i
            table.row['var4'] = float(self.nrows - i - 1)
            table.row.append()
        table.flush()
        # Index all entries:
        for col in table.colinstances.itervalues():
            indexrows = col.create_index(_blocksizes=small_blocksizes)
            self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        if verbose:
            print("After re-creation")
            print("var1 column:", table.cols.var1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(table.colindexed["var1"], 1)

    # Test provided by Andrew Straw
    def test11c_removeTableWithIndex(self):
        """Checking removing a table with indexes (persistent version 3)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11c_removeTableWithIndex..." %
                  self.__class__.__name__)

        class Distance(IsDescription):
            frame = Int32Col(pos=0)
            distance = FloatCol(pos=1)

        # Delete the old temporal file
        os.remove(self.file)

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode='w')
        table = self.fileh.create_table(
            self.fileh.root, 'distance_table', Distance)
        table.cols.frame.create_index(_blocksizes=small_blocksizes)
        r = table.row
        for i in range(10):
            r['frame'] = i
            r['distance'] = float(i**2)
            r.append()
        table.flush()
        self.fileh.close()

        self.fileh = open_file(self.file, mode='r+')
        self.fileh.remove_node(self.fileh.root.distance_table)


small_ss = small_blocksizes[2]


class BasicReadTestCase(BasicTestCase):
    compress = 0
    complib = "zlib"
    shuffle = 0
    fletcher32 = 0
    nrows = small_ss


class ZlibReadTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    shuffle = 0
    fletcher32 = 0
    nrows = small_ss


class BloscReadTestCase(BasicTestCase):
    compress = 1
    complib = "blosc"
    shuffle = 0
    fletcher32 = 0
    nrows = small_ss


class LZOReadTestCase(BasicTestCase):
    compress = 1
    complib = "lzo"
    shuffle = 0
    fletcher32 = 0
    nrows = small_ss


class Bzip2ReadTestCase(BasicTestCase):
    compress = 1
    complib = "bzip2"
    shuffle = 0
    fletcher32 = 0
    nrows = small_ss


class ShuffleReadTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    shuffle = 1
    fletcher32 = 0
    nrows = small_ss


class Fletcher32ReadTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    shuffle = 0
    fletcher32 = 1
    nrows = small_ss


class ShuffleFletcher32ReadTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"
    shuffle = 1
    fletcher32 = 1
    nrows = small_ss


class OneHalfTestCase(BasicTestCase):
    nrows = small_ss + small_ss//2


class UpperBoundTestCase(BasicTestCase):
    nrows = small_ss + 1


class LowerBoundTestCase(BasicTestCase):
    nrows = small_ss * 2-1


class DeepTableIndexTestCase(unittest.TestCase):
    nrows = minRowIndex

    def test01(self):
        "Checking the indexing of a table in a 2nd level hierarchy"
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.create_group(self.fileh.root, "agroup")
        # Create a table
        title = "This is the IndexArray title"
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        None, self.nrows)
        for i in range(self.nrows):
            # Fill rows with defaults
            table.row.append()
        table.flush()
        # Index some column
        indexrows = table.cols.var1.create_index()
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        # Some sanity checks
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(idxcol.nelements, self.nrows)

        self.fileh.close()
        os.remove(self.file)

    def test01b(self):
        "Checking the indexing of a table in 2nd level (persistent version)"
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.create_group(self.fileh.root, "agroup")
        # Create a table
        title = "This is the IndexArray title"
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        None, self.nrows)
        for i in range(self.nrows):
            # Fill rows with defaults
            table.row.append()
        table.flush()
        # Index some column
        indexrows = table.cols.var1.create_index()
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        # Close and re-open this file
        self.fileh.close()
        self.fileh = open_file(self.file, "a")
        table = self.fileh.root.agroup.table
        idxcol = table.cols.var1.index
        # Some sanity checks
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(idxcol.nelements, self.nrows)

        self.fileh.close()
        os.remove(self.file)

    def test02(self):
        "Checking the indexing of a table in a 4th level hierarchy"
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.create_group(self.fileh.root, "agroup")
        group = self.fileh.create_group(group, "agroup")
        group = self.fileh.create_group(group, "agroup")
        # Create a table
        title = "This is the IndexArray title"
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        None, self.nrows)
        for i in range(self.nrows):
            # Fill rows with defaults
            table.row.append()
        table.flush()
        # Index some column
        indexrows = table.cols.var1.create_index()
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        # Some sanity checks
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(idxcol.nelements, self.nrows)

        self.fileh.close()
        os.remove(self.file)

    def test02b(self):
        "Checking the indexing of a table in a 4th level (persistent version)"
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.create_group(self.fileh.root, "agroup")
        group = self.fileh.create_group(group, "agroup")
        group = self.fileh.create_group(group, "agroup")
        # Create a table
        title = "This is the IndexArray title"
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        None, self.nrows)
        for i in range(self.nrows):
            # Fill rows with defaults
            table.row.append()
        table.flush()
        # Index some column
        indexrows = table.cols.var1.create_index()
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        # Close and re-open this file
        self.fileh.close()
        self.fileh = open_file(self.file, "a")
        table = self.fileh.root.agroup.agroup.agroup.table
        idxcol = table.cols.var1.index
        # Some sanity checks
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(idxcol.nelements, self.nrows)

        self.fileh.close()
        os.remove(self.file)

    def test03(self):
        "Checking the indexing of a table in a 100th level hierarchy"
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.root
        for i in range(100):
            group = self.fileh.create_group(group, "agroup")
        # Create a table
        title = "This is the IndexArray title"
        table = self.fileh.create_table(group, 'table', TDescr, title,
                                        None, self.nrows)
        for i in range(self.nrows):
            # Fill rows with defaults
            table.row.append()
        table.flush()
        # Index some column
        indexrows = table.cols.var1.create_index()
        self.assertTrue(indexrows is not None)
        idxcol = table.cols.var1.index
        # Some sanity checks
        self.assertEqual(table.colindexed["var1"], 1)
        self.assertTrue(idxcol is not None)
        self.assertEqual(idxcol.nelements, self.nrows)

        self.fileh.close()
        os.remove(self.file)


class IndexProps(object):
    def __init__(self, auto=default_auto_index, filters=default_index_filters):
        self.auto = auto
        self.filters = filters

DefaultProps = IndexProps()
NoAutoProps = IndexProps(auto=False)
ChangeFiltersProps = IndexProps(
    filters=Filters(complevel=6, complib="zlib",
                    shuffle=False, fletcher32=False))


class AutomaticIndexingTestCase(unittest.TestCase):
    reopen = 1
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']
    small_blocksizes = (16, 8, 4, 2)

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        # Create a table
        title = "This is the IndexArray title"
        root = self.fileh.root
        # Make the chunkshape smaller or equal than small_blocksizes[-1]
        chunkshape = (2,)
        self.table = self.fileh.create_table(root, 'table', TDescr, title,
                                             None, self.nrows,
                                             chunkshape=chunkshape)
        self.table.autoindex = self.iprops.auto
        for colname in self.colsToIndex:
            self.table.colinstances[colname].create_index(
                _blocksizes=self.small_blocksizes)
        for i in range(self.nrows):
            # Fill rows with defaults
            self.table.row.append()
        self.table.flush()
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            self.table = self.fileh.root.table

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        cleanup(self)

    def test01_attrs(self):
        "Checking indexing attributes (part1)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_attrs..." % self.__class__.__name__)

        table = self.table
        if self.iprops is DefaultProps:
            self.assertEqual(table.indexed, 0)
        else:
            self.assertEqual(table.indexed, 1)
        if self.iprops is DefaultProps:
            self.assertEqual(table.colindexed["var1"], 0)
            self.assertTrue(table.cols.var1.index is None)
            self.assertEqual(table.colindexed["var2"], 0)
            self.assertTrue(table.cols.var2.index is None)
            self.assertEqual(table.colindexed["var3"], 0)
            self.assertTrue(table.cols.var3.index is None)
            self.assertEqual(table.colindexed["var4"], 0)
            self.assertTrue(table.cols.var4.index is None)
        else:
            # Check that the var1, var2 and var3 (and only these)
            # has been indexed
            self.assertEqual(table.colindexed["var1"], 1)
            self.assertTrue(table.cols.var1.index is not None)
            self.assertEqual(table.colindexed["var2"], 1)
            self.assertTrue(table.cols.var2.index is not None)
            self.assertEqual(table.colindexed["var3"], 1)
            self.assertTrue(table.cols.var3.index is not None)
            self.assertEqual(table.colindexed["var4"], 0)
            self.assertTrue(table.cols.var4.index is None)

    def test02_attrs(self):
        "Checking indexing attributes (part2)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_attrs..." % self.__class__.__name__)

        table = self.table
        # Check the policy parameters
        if verbose:
            if table.indexed:
                print("index props:", table.autoindex)
            else:
                print("Table is not indexed")
        # Check non-default values for index saving policy
        if self.iprops is NoAutoProps:
            self.assertFalse(table.autoindex)
        elif self.iprops is ChangeFiltersProps:
            self.assertTrue(table.autoindex)

        # Check Index() objects exists and are properly placed
        if self.iprops is DefaultProps:
            self.assertEqual(table.cols.var1.index, None)
            self.assertEqual(table.cols.var2.index, None)
            self.assertEqual(table.cols.var3.index, None)
            self.assertEqual(table.cols.var4.index, None)
        else:
            self.assertTrue(isinstance(table.cols.var1.index, Index))
            self.assertTrue(isinstance(table.cols.var2.index, Index))
            self.assertTrue(isinstance(table.cols.var3.index, Index))
            self.assertEqual(table.cols.var4.index, None)

    def test03_counters(self):
        "Checking indexing counters"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_counters..." % self.__class__.__name__)
        table = self.table
        # Check the counters for indexes
        if verbose:
            if table.indexed:
                print("indexedrows:", table._indexedrows)
                print("unsavedindexedrows:", table._unsaved_indexedrows)
                index = table.cols.var1.index
                print("table rows:", table.nrows)
                print("computed indexed rows:", index.nrows * index.slicesize)
            else:
                print("Table is not indexed")
        if self.iprops is not DefaultProps:
            index = table.cols.var1.index
            indexedrows = index.nelements
            self.assertEqual(table._indexedrows, indexedrows)
            indexedrows = index.nelements
            self.assertEqual(table._unsaved_indexedrows,
                             self.nrows - indexedrows)

    def test04_noauto(self):
        "Checking indexing counters (non-automatic mode)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_noauto..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Check the counters for indexes
        if verbose:
            if table.indexed:
                print("indexedrows:", table._indexedrows)
                print("unsavedindexedrows:", table._unsaved_indexedrows)
                index = table.cols.var1.index
                print("computed indexed rows:", index.nelements)
            else:
                print("Table is not indexed")

        # No unindexated rows should remain
        index = table.cols.var1.index
        if self.iprops is DefaultProps:
            self.assertTrue(index is None)
        else:
            indexedrows = index.nelements
            self.assertEqual(table._indexedrows, index.nelements)
            self.assertEqual(table._unsaved_indexedrows,
                             self.nrows - indexedrows)

        # Check non-default values for index saving policy
        if self.iprops is NoAutoProps:
            self.assertFalse(table.autoindex)
        elif self.iprops is ChangeFiltersProps:
            self.assertTrue(table.autoindex)

    def test05_icounters(self):
        "Checking indexing counters (remove_rows)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_icounters..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            unsavedindexedrows = table._unsaved_indexedrows
        # Now, remove some rows:
        table.remove_rows(2, 4)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check the counters for indexes
        if verbose:
            if table.indexed:
                print("indexedrows:", table._indexedrows)
                print("original indexedrows:", indexedrows)
                print("unsavedindexedrows:", table._unsaved_indexedrows)
                print("original unsavedindexedrows:", unsavedindexedrows)
                # index = table.cols.var1.index
                print("index dirty:", table.cols.var1.index.dirty)
            else:
                print("Table is not indexed")

        # Check the counters
        self.assertEqual(table.nrows, self.nrows - 2)
        if self.iprops is NoAutoProps:
            self.assertTrue(table.cols.var1.index.dirty)

        # Check non-default values for index saving policy
        if self.iprops is NoAutoProps:
            self.assertFalse(table.autoindex)
        elif self.iprops is ChangeFiltersProps:
            self.assertTrue(table.autoindex)

    def test06_dirty(self):
        "Checking dirty flags (remove_rows action)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06_dirty..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Now, remove some rows:
        table.remove_rows(3, 5)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check the dirty flag for indexes
        if verbose:
            print("auto flag:", table.autoindex)
            for colname in table.colnames:
                if table.cols._f_col(colname).index:
                    print("dirty flag col %s: %s" %
                          (colname, table.cols._f_col(colname).index.dirty))
        # Check the flags
        for colname in table.colnames:
            if table.cols._f_col(colname).index:
                if not table.autoindex:
                    self.assertEqual(table.cols._f_col(colname).index.dirty,
                                     True)
                else:
                    self.assertEqual(table.cols._f_col(colname).index.dirty,
                                     False)

    def test07_noauto(self):
        "Checking indexing counters (modify_rows, no-auto mode)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_noauto..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # No unindexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            unsavedindexedrows = table._unsaved_indexedrows
        # Now, modify just one row:
        table.modify_rows(3, None, 1, [["asa", 0, 3, 3.1]])
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check the counters for indexes
        if verbose:
            if table.indexed:
                print("indexedrows:", table._indexedrows)
                print("original indexedrows:", indexedrows)
                print("unsavedindexedrows:", table._unsaved_indexedrows)
                print("original unsavedindexedrows:", unsavedindexedrows)
                index = table.cols.var1.index
                print("computed indexed rows:", index.nelements)
            else:
                print("Table is not indexed")

        # Check the counters
        self.assertEqual(table.nrows, self.nrows)
        if self.iprops is NoAutoProps:
            self.assertTrue(table.cols.var1.index.dirty)

        # Check the dirty flag for indexes
        if verbose:
            for colname in table.colnames:
                if table.cols._f_col(colname).index:
                    print("dirty flag col %s: %s" %
                          (colname, table.cols._f_col(colname).index.dirty))
        for colname in table.colnames:
            if table.cols._f_col(colname).index:
                if not table.autoindex:
                    self.assertEqual(table.cols._f_col(colname).index.dirty,
                                     True)
                else:
                    self.assertEqual(table.cols._f_col(colname).index.dirty,
                                     False)

    def test07b_noauto(self):
        "Checking indexing queries (modify in iterator, no-auto mode)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07b_noauto..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Do a query that uses indexes
        res = [row.nrow for row in table.where('(var2 == True) & (var3 > 0)')]
        # Now, modify just one row:
        for row in table:
            if row.nrow == 3:
                row['var1'] = "asa"
                row['var2'] = True
                row['var3'] = 3
                row['var4'] = 3.1
                row.update()
        table.flush()
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table

        # Do a query that uses indexes
        resq = [row.nrow for row in table.where('(var2 == True) & (var3 > 0)')]
        res_ = res + [3]
        if verbose:
            print("AutoIndex?:", table.autoindex)
            print("Query results (original):", res)
            print("Query results (after modifying table):", resq)
            print("Should look like:", res_)
        self.assertEqual(res_, resq)

    def test07c_noauto(self):
        "Checking indexing queries (append, no-auto mode)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07c_noauto..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Do a query that uses indexes
        res = [row.nrow for row in table.where('(var2 == True) & (var3 > 0)')]
        # Now, append three rows
        table.append([("asa", True, 1, 3.1)])
        table.append([("asb", True, 2, 3.1)])
        table.append([("asc", True, 3, 3.1)])
        table.flush()
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table

        # Do a query that uses indexes
        resq = [row.nrow for row in table.where('(var2 == True) & (var3 > 0)')]
        res_ = res + [table.nrows-3, table.nrows-2, table.nrows-1]
        if verbose:
            print("AutoIndex?:", table.autoindex)
            print("Query results (original):", res)
            print("Query results (after modifying table):", resq)
            print("Should look like:", res_)
        self.assertEqual(res_, resq)

    def test08_dirty(self):
        "Checking dirty flags (modify_columns)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_dirty..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            self.assertTrue(indexedrows is not None)
            unsavedindexedrows = table._unsaved_indexedrows
            self.assertTrue(unsavedindexedrows is not None)
        # Now, modify a couple of rows:
        table.modify_columns(1, columns=[["asa", "asb"], [1., 2.]],
                             names=["var1", "var4"])
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table

        # Check the counters
        self.assertEqual(table.nrows, self.nrows)
        if self.iprops is NoAutoProps:
            self.assertTrue(table.cols.var1.index.dirty)

        # Check the dirty flag for indexes
        if verbose:
            for colname in table.colnames:
                if table.cols._f_col(colname).index:
                    print("dirty flag col %s: %s" %
                          (colname, table.cols._f_col(colname).index.dirty))
        for colname in table.colnames:
            if table.cols._f_col(colname).index:
                if not table.autoindex:
                    if colname in ["var1"]:
                        self.assertEqual(
                            table.cols._f_col(colname).index.dirty, True)
                    else:
                        self.assertEqual(
                            table.cols._f_col(colname).index.dirty, False)
                else:
                    self.assertEqual(table.cols._f_col(colname).index.dirty,
                                     False)

    def test09a_propIndex(self):
        "Checking propagate Index feature in Table.copy() (attrs)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09a_propIndex..." % self.__class__.__name__)
        table = self.table
        # Don't force a sync in indexes
        # table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            self.assertTrue(indexedrows is not None)
            unsavedindexedrows = table._unsaved_indexedrows
            self.assertTrue(unsavedindexedrows is not None)
        # Now, remove some rows to make columns dirty
        # table.remove_rows(3,5)
        # Copy a Table to another location
        table2 = table.copy("/", 'table2', propindexes=True)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
            table2 = self.fileh.root.table2

        index1 = table.cols.var1.index
        index2 = table2.cols.var1.index
        if verbose:
            print("Copied index:", index2)
            print("Original index:", index1)
            if index1:
                print("Elements in copied index:", index2.nelements)
                print("Elements in original index:", index1.nelements)
        # Check the counters
        self.assertEqual(table.nrows, table2.nrows)
        if table.indexed:
            self.assertTrue(table2.indexed)
        if self.iprops is DefaultProps:
            # No index: the index should not exist
            self.assertTrue(index1 is None)
            self.assertTrue(index2 is None)
        elif self.iprops is NoAutoProps:
            self.assertTrue(index2 is not None)

        # Check the dirty flag for indexes
        if verbose:
            for colname in table2.colnames:
                if table2.cols._f_col(colname).index:
                    print("dirty flag col %s: %s" %
                          (colname, table2.cols._f_col(colname).index.dirty))
        for colname in table2.colnames:
            if table2.cols._f_col(colname).index:
                self.assertEqual(table2.cols._f_col(colname).index.dirty,
                                 False)

    def test09b_propIndex(self):
        "Checking that propindexes=False works"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09b_propIndex..." % self.__class__.__name__)
        table = self.table
        # Don't force a sync in indexes
        # table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            self.assertTrue(indexedrows is not None)
            unsavedindexedrows = table._unsaved_indexedrows
            self.assertTrue(unsavedindexedrows is not None)
        # Now, remove some rows to make columns dirty
        # table.remove_rows(3,5)
        # Copy a Table to another location
        table2 = table.copy("/", 'table2', propindexes=False)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
            table2 = self.fileh.root.table2

        if verbose:
            print("autoindex?:", self.iprops.auto)
            print("Copied index indexed?:", table2.cols.var1.is_indexed)
            print("Original index indexed?:", table.cols.var1.is_indexed)
        if self.iprops is DefaultProps:
            # No index: the index should not exist
            self.assertFalse(table2.cols.var1.is_indexed)
            self.assertFalse(table.cols.var1.is_indexed)
        elif self.iprops is NoAutoProps:
            self.assertFalse(table2.cols.var1.is_indexed)
            self.assertTrue(table.cols.var1.is_indexed)

    def test10_propIndex(self):
        "Checking propagate Index feature in Table.copy() (values)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10_propIndex..." % self.__class__.__name__)
        table = self.table
        # Don't force a sync in indexes
        # table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            self.assertTrue(indexedrows is not None)
            unsavedindexedrows = table._unsaved_indexedrows
            self.assertTrue(unsavedindexedrows is not None)
        # Now, remove some rows to make columns dirty
        # table.remove_rows(3,5)
        # Copy a Table to another location
        table2 = table.copy("/", 'table2', propindexes=True)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
            table2 = self.fileh.root.table2

        index1 = table.cols.var3.index
        index2 = table2.cols.var3.index
        if verbose:
            print("Copied index:", index2)
            print("Original index:", index1)
            if index1:
                print("Elements in copied index:", index2.nelements)
                print("Elements in original index:", index1.nelements)

    def test11_propIndex(self):
        "Checking propagate Index feature in Table.copy() (dirty flags)"
        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11_propIndex..." % self.__class__.__name__)
        table = self.table
        # Force a sync in indexes
        table.flush_rows_to_index()
        # Non indexated rows should remain here
        if self.iprops is not DefaultProps:
            indexedrows = table._indexedrows
            self.assertTrue(indexedrows is not None)
            unsavedindexedrows = table._unsaved_indexedrows
            self.assertTrue(unsavedindexedrows is not None)
        # Now, modify an indexed column and an unindexed one
        # to make the "var1" dirty
        table.modify_columns(1, columns=[["asa", "asb"], [1., 2.]],
                             names=["var1", "var4"])
        # Copy a Table to another location
        table2 = table.copy("/", 'table2', propindexes=True)
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
            table2 = self.fileh.root.table2

        index1 = table.cols.var1.index
        index2 = table2.cols.var1.index
        if verbose:
            print("Copied index:", index2)
            print("Original index:", index1)
            if index1:
                print("Elements in copied index:", index2.nelements)
                print("Elements in original index:", index1.nelements)

        # Check the dirty flag for indexes
        if verbose:
            for colname in table2.colnames:
                if table2.cols._f_col(colname).index:
                    print("dirty flag col %s: %s" %
                          (colname, table2.cols._f_col(colname).index.dirty))
        for colname in table2.colnames:
            if table2.cols._f_col(colname).index:
                if table2.autoindex:
                    # All the destination columns should be non-dirty because
                    # the copy removes the dirty state and puts the
                    # index in a sane state
                    self.assertEqual(table2.cols._f_col(colname).index.dirty,
                                     False)


# minRowIndex = 10000  # just if one wants more indexed rows to be checked
class AI1TestCase(AutomaticIndexingTestCase):
    # nrows = 10002
    nrows = 102
    reopen = 0
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI2TestCase(AutomaticIndexingTestCase):
    # nrows = 10002
    nrows = 102
    reopen = 1
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI4bTestCase(AutomaticIndexingTestCase):
    # nrows = 10012
    nrows = 112
    reopen = 1
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI5TestCase(AutomaticIndexingTestCase):
    sbs, bs, ss, cs = calc_chunksize(minRowIndex, memlevel=1)
    nrows = ss * 11-1
    reopen = 0
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI6TestCase(AutomaticIndexingTestCase):
    sbs, bs, ss, cs = calc_chunksize(minRowIndex, memlevel=1)
    nrows = ss * 21 + 1
    reopen = 1
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI7TestCase(AutomaticIndexingTestCase):
    sbs, bs, ss, cs = calc_chunksize(minRowIndex, memlevel=1)
    nrows = ss * 12-1
    # nrows = ss * 1-1  # faster test
    reopen = 0
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI8TestCase(AutomaticIndexingTestCase):
    sbs, bs, ss, cs = calc_chunksize(minRowIndex, memlevel=1)
    nrows = ss * 15 + 100
    # nrows = ss * 1 + 100  # faster test
    reopen = 1
    iprops = NoAutoProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI9TestCase(AutomaticIndexingTestCase):
    sbs, bs, ss, cs = calc_chunksize(minRowIndex, memlevel=1)
    nrows = ss
    reopen = 0
    iprops = DefaultProps
    colsToIndex = []


class AI10TestCase(AutomaticIndexingTestCase):
    # nrows = 10002
    nrows = 102
    reopen = 1
    iprops = DefaultProps
    colsToIndex = []


class AI11TestCase(AutomaticIndexingTestCase):
    # nrows = 10002
    nrows = 102
    reopen = 0
    iprops = ChangeFiltersProps
    colsToIndex = ['var1', 'var2', 'var3']


class AI12TestCase(AutomaticIndexingTestCase):
    # nrows = 10002
    nrows = 102
    reopen = 0
    iprops = ChangeFiltersProps
    colsToIndex = ['var1', 'var2', 'var3']


class ManyNodesTestCase(PyTablesTestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w", node_cache_slots=64)

    def test00(self):
        """Indexing many nodes in one single session (based on bug #26)"""
        IdxRecord = {
            'f0': Int8Col(),
            'f1': Int8Col(),
            'f2': Int8Col(),
        }
        h5 = self.fileh
        for qn in range(5):
            for sn in range(5):
                qchr = 'chr' + str(qn)
                name = 'chr' + str(sn)
                path = "/at/%s/pt" % (qchr)
                table = h5.create_table(path, name, IdxRecord, createparents=1)
                table.cols.f0.create_index()
                table.cols.f1.create_index()
                table.cols.f2.create_index()
                table.row.append()
                table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        cleanup(self)


class IndexPropsChangeTestCase(TempFileMixin, PyTablesTestCase):
    """Test case for changing index properties in a table."""

    class MyDescription(IsDescription):
        icol = IntCol()
    oldIndexProps = IndexProps()
    newIndexProps = IndexProps(auto=False, filters=Filters(complevel=9))

    def setUp(self):
        super(IndexPropsChangeTestCase, self).setUp()
        table = self.h5file.create_table('/', 'test', self.MyDescription)
        table.autoindex = self.oldIndexProps.auto
        row = table.row
        for i in xrange(100):
            row['icol'] = i % 25
            row.append()
        table.flush()
        self.table = table

    def tearDown(self):
        super(IndexPropsChangeTestCase, self).tearDown()

    def test_attributes(self):
        """Storing index properties as table attributes."""
        for refprops in [self.oldIndexProps, self.newIndexProps]:
            self.assertEqual(self.table.autoindex, refprops.auto)
            self.table.autoindex = self.newIndexProps.auto

    def test_copyattrs(self):
        """Copying index properties attributes."""
        oldtable = self.table
        newtable = oldtable.copy('/', 'test2')
        self.assertEqual(oldtable.autoindex, newtable.autoindex)


class IndexFiltersTestCase(TempFileMixin, PyTablesTestCase):
    """Test case for setting index filters."""

    def setUp(self):
        super(IndexFiltersTestCase, self).setUp()
        description = {'icol': IntCol()}
        self.table = self.h5file.create_table('/', 'test', description)

    def test_createIndex(self):
        """Checking input parameters in new indexes."""
        # Different from default.
        argfilters = copy.copy(default_index_filters)
        argfilters.shuffle = not default_index_filters.shuffle

        # Different both from default and the previous one.
        idxfilters = copy.copy(default_index_filters)
        idxfilters.shuffle = not default_index_filters.shuffle
        idxfilters.fletcher32 = not default_index_filters.fletcher32

        icol = self.table.cols.icol

        # First create
        icol.create_index(kind='ultralight', optlevel=4)
        self.assertEqual(icol.index.kind, 'ultralight')
        self.assertEqual(icol.index.optlevel, 4)
        self.assertEqual(icol.index.filters, default_index_filters)
        icol.remove_index()

        # Second create
        icol.create_index(kind='medium', optlevel=3, filters=argfilters)
        self.assertEqual(icol.index.kind, 'medium')
        self.assertEqual(icol.index.optlevel, 3)
        self.assertEqual(icol.index.filters, argfilters)
        icol.remove_index()

    def test_reindex(self):
        """Checking input parameters in recomputed indexes."""
        icol = self.table.cols.icol
        icol.create_index(
            kind='full', optlevel=5, filters=Filters(complevel=3))
        kind = icol.index.kind
        optlevel = icol.index.optlevel
        filters = icol.index.filters
        icol.reindex()
        ni = icol.index
        if verbose:
            print("Old parameters: %s, %s, %s" % (kind, optlevel, filters))
            print("New parameters: %s, %s, %s" % (
                ni.kind, ni.optlevel, ni.filters))
        self.assertEqual(ni.kind, kind)
        self.assertEqual(ni.optlevel, optlevel)
        self.assertEqual(ni.filters, filters)


class OldIndexTestCase(PyTablesTestCase):

    def test1_x(self):
        """Check that files with 1.x indexes are recognized and warned."""
        fname = self._testFilename("idx-std-1.x.h5")
        f = open_file(fname)
        self.assertWarns(OldIndexWarning, f.get_node, "/table")
        f.close()


# Sensible parameters for indexing with small blocksizes
small_blocksizes = (512, 128, 32, 8)


class CompletelySortedIndexTestCase(TempFileMixin, PyTablesTestCase):
    """Test case for testing a complete sort in a table."""

    nrows = 100
    nrowsinbuf = 11

    class MyDescription(IsDescription):
        rcol = IntCol(pos=1)
        icol = IntCol(pos=2)

    def setUp(self):
        super(CompletelySortedIndexTestCase, self).setUp()
        table = self.h5file.create_table('/', 'table', self.MyDescription)
        row = table.row
        nrows = self.nrows
        for i in xrange(nrows):
            row['rcol'] = i
            row['icol'] = nrows - i
            row.append()
        table.flush()
        self.table = table
        self.icol = self.table.cols.icol
        # A full index with maximum optlevel should always be completely sorted
        self.icol.create_csindex(_blocksizes=small_blocksizes)

    def test00_isCompletelySortedIndex(self):
        """Testing the Column.is_csi property."""
        icol = self.icol
        self.assertEqual(icol.index.is_csi, True)
        icol.remove_index()
        # Other kinds than full, should never return a CSI
        icol.create_index(kind="medium", optlevel=9)
        self.assertEqual(icol.index.is_csi, False)
        icol.remove_index()
        # As the table is small, lesser optlevels should be able to
        # create a completely sorted index too.
        icol.create_index(kind="full", optlevel=6)
        self.assertEqual(icol.index.is_csi, True)
        # Checking a CSI in a sorted copy
        self.table.copy("/", 'table2', sortby='icol', checkCSI=True)
        self.assertEqual(icol.index.is_csi, True)

    def test01_readSorted1(self):
        """Testing the Index.read_sorted() method with no arguments."""
        icol = self.icol
        sortedcol = numpy.sort(icol[:])
        sortedcol2 = icol.index.read_sorted()
        if verbose:
            print("Original sorted column:", sortedcol)
            print("The values from the index:", sortedcol2)
        self.assertTrue(allequal(sortedcol, sortedcol2))

    def test01_readSorted2(self):
        """Testing the Index.read_sorted() method with arguments (I)."""
        icol = self.icol
        sortedcol = numpy.sort(icol[:])[30:55]
        sortedcol2 = icol.index.read_sorted(30, 55)
        if verbose:
            print("Original sorted column:", sortedcol)
            print("The values from the index:", sortedcol2)
        self.assertTrue(allequal(sortedcol, sortedcol2))

    def test01_readSorted3(self):
        """Testing the Index.read_sorted() method with arguments (II)."""
        icol = self.icol
        sortedcol = numpy.sort(icol[:])[33:97]
        sortedcol2 = icol.index.read_sorted(33, 97)
        if verbose:
            print("Original sorted column:", sortedcol)
            print("The values from the index:", sortedcol2)
        self.assertTrue(allequal(sortedcol, sortedcol2))

    def test02_readIndices1(self):
        """Testing the Index.read_indices() method with no arguments."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:]).astype('uint64')
        indicescol2 = icol.index.read_indices()
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test02_readIndices2(self):
        """Testing the Index.read_indices() method with arguments (I)."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[30:55].astype('uint64')
        indicescol2 = icol.index.read_indices(30, 55)
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test02_readIndices3(self):
        """Testing the Index.read_indices() method with arguments (II)."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[33:97].astype('uint64')
        indicescol2 = icol.index.read_indices(33, 97)
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test02_readIndices4(self):
        """Testing the Index.read_indices() method with arguments (III)."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[33:97:2].astype('uint64')
        indicescol2 = icol.index.read_indices(33, 97, 2)
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test02_readIndices5(self):
        """Testing the Index.read_indices() method with arguments (IV)."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[33:55:5].astype('uint64')
        indicescol2 = icol.index.read_indices(33, 55, 5)
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test02_readIndices6(self):
        """Testing the Index.read_indices() method with step only."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[::3].astype('uint64')
        indicescol2 = icol.index.read_indices(step=3)
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test03_getitem1(self):
        """Testing the Index.__getitem__() method with no arguments."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:]).astype('uint64')
        indicescol2 = icol.index[:]
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test03_getitem2(self):
        """Testing the Index.__getitem__() method with start."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[31].astype('uint64')
        indicescol2 = icol.index[31]
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test03_getitem3(self):
        """Testing the Index.__getitem__() method with start, stop."""
        icol = self.icol
        indicescol = numpy.argsort(icol[:])[2:16].astype('uint64')
        indicescol2 = icol.index[2:16]
        if verbose:
            print("Original indices column:", indicescol)
            print("The values from the index:", indicescol2)
        self.assertTrue(allequal(indicescol, indicescol2))

    def test04_itersorted1(self):
        """Testing the Table.itersorted() method with no arguments."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol')], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted2(self):
        """Testing the Table.itersorted() method with a start."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[15:]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', start=15)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted3(self):
        """Testing the Table.itersorted() method with a stop."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[:20]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', stop=20)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted4(self):
        """Testing the Table.itersorted() method with a start and stop."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[15:20]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', start=15, stop=20)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted5(self):
        """Testing the Table.itersorted() method with a start, stop and
        step."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[15:45:4]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', start=15, stop=45, step=4)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted6(self):
        """Testing the Table.itersorted() method with a start, stop and
        step."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[33:55:5]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', start=33, stop=55, step=5)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted7(self):
        """Testing the Table.itersorted() method with checkCSI=True."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', checkCSI=True)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted8(self):
        """Testing the Table.itersorted() method with a start, stop and
        negative step."""
        # see also gh-252
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[55:33:-5]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', start=55, stop=33, step=-5)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test04_itersorted9(self):
        """Testing the Table.itersorted() method with a negative step."""
        # see also gh-252
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[::-5]
        sortedtable2 = numpy.array(
            [row.fetch_all_fields() for row in table.itersorted(
             'icol', step=-5)], dtype=table._v_dtype)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from the iterator:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted1(self):
        """Testing the Table.read_sorted() method with no arguments."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table.read_sorted('icol')
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted2(self):
        """Testing the Table.read_sorted() method with a start."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[16:17]
        sortedtable2 = table.read_sorted('icol', start=16)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted3(self):
        """Testing the Table.read_sorted() method with a start and stop."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[16:33]
        sortedtable2 = table.read_sorted('icol', start=16, stop=33)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted4(self):
        """Testing the Table.read_sorted() method with a start, stop and
        step."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[33:55:5]
        sortedtable2 = table.read_sorted('icol', start=33, stop=55, step=5)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted5(self):
        """Testing the Table.read_sorted() method with only a step."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[::3]
        sortedtable2 = table.read_sorted('icol', step=3)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted6(self):
        """Testing the Table.read_sorted() method with negative step."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[::-1]
        sortedtable2 = table.read_sorted('icol', step=-1)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted7(self):
        """Testing the Table.read_sorted() method with negative step (II)."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')[::-2]
        sortedtable2 = table.read_sorted('icol', step=-2)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted8(self):
        """Testing the Table.read_sorted() method with negative step (III))."""
        table = self.table
        sstart = 100-24-1
        sstop = 100-54-1
        sortedtable = numpy.sort(table[:], order='icol')[sstart:sstop:-1]
        sortedtable2 = table.read_sorted('icol', start=24, stop=54, step=-1)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted9(self):
        """Testing the Table.read_sorted() method with negative step (IV))."""
        table = self.table
        sstart = 100-14-1
        sstop = 100-54-1
        sortedtable = numpy.sort(table[:], order='icol')[sstart:sstop:-3]
        sortedtable2 = table.read_sorted('icol', start=14, stop=54, step=-3)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted10(self):
        """Testing the Table.read_sorted() method with negative step (V))."""
        table = self.table
        sstart = 100-24-1
        sstop = 100-25-1
        sortedtable = numpy.sort(table[:], order='icol')[sstart:sstop:-2]
        sortedtable2 = table.read_sorted('icol', start=24, stop=25, step=-2)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05_readSorted11(self):
        """Testing the Table.read_sorted() method with start > stop."""
        table = self.table
        sstart = 100-137-1
        sstop = 100-25-1
        sortedtable = numpy.sort(table[:], order='icol')[sstart:sstop:-2]
        sortedtable2 = table.read_sorted('icol', start=137, stop=25, step=-2)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05a_readSorted12(self):
        """Testing the Table.read_sorted() method with checkCSI (I)."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table.read_sorted('icol', checkCSI=True)
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test05b_readSorted12(self):
        """Testing the Table.read_sorted() method with checkCSI (II)."""
        table = self.table
        self.assertRaises(ValueError,
                          table.read_sorted, "rcol", checkCSI=False)

    def test06_copy_sorted1(self):
        """Testing the Table.copy(sortby) method with no arguments."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted2(self):
        """Testing the Table.copy(sortby) method with step=-1."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol", step=-1)
        sortedtable = numpy.sort(table[:], order='icol')[::-1]
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted3(self):
        """Testing the Table.copy(sortby) method with only a start."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol", start=3)
        sortedtable = numpy.sort(table[:], order='icol')[3:4]
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted4(self):
        """Testing the Table.copy(sortby) method with start, stop."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol", start=3, stop=40)
        sortedtable = numpy.sort(table[:], order='icol')[3:40]
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted5(self):
        """Testing the Table.copy(sortby) method with start, stop, step."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol",
                            start=3, stop=33, step=5)
        sortedtable = numpy.sort(table[:], order='icol')[3:33:5]
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted6(self):
        """Testing the Table.copy(sortby) method after table re-opening."""
        self._reopen(mode='a')
        table = self.h5file.root.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table2[:]
        if verbose:
            print("Original sorted table:", sortedtable)
            print("The values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test06_copy_sorted7(self):
        """Testing the `checkCSI` parameter of Table.copy() (I)."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        self.assertRaises(ValueError,
                          table2.copy, "/", 'table3',
                          sortby="rcol", checkCSI=False)

    def test06_copy_sorted8(self):
        """Testing the `checkCSI` parameter of Table.copy() (II)."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        self.assertRaises(ValueError,
                          table2.copy, "/", 'table3',
                          sortby="rcol", checkCSI=True)

    def test07_isCSI_noelements(self):
        """Testing the representation of an index with no elements."""
        t2 = self.h5file.create_table('/', 't2', self.MyDescription)
        irows = t2.cols.rcol.create_csindex()
        if verbose:
            print("repr(t2)-->\n", repr(t2))
        self.assertEqual(irows, 0)
        self.assertEqual(t2.colindexes['rcol'].is_csi, False)


class ReadSortedIndexTestCase(TempFileMixin, PyTablesTestCase):
    """Test case for testing sorted reading in a "full" sorted column."""

    nrows = 100
    nrowsinbuf = 11

    class MyDescription(IsDescription):
        rcol = IntCol(pos=1)
        icol = IntCol(pos=2)

    def setUp(self):
        super(ReadSortedIndexTestCase, self).setUp()
        table = self.h5file.create_table('/', 'table', self.MyDescription)
        row = table.row
        nrows = self.nrows
        for i in xrange(nrows):
            row['rcol'] = i
            row['icol'] = nrows - i
            row.append()
        table.flush()
        self.table = table
        self.icol = self.table.cols.icol
        # A full index with maximum optlevel should always be completely sorted
        self.icol.create_index(optlevel=self.optlevel, kind="full",
                               _blocksizes=small_blocksizes)

    def test01_readSorted1(self):
        """Testing the Table.read_sorted() method with no arguments."""
        table = self.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table.read_sorted('icol')
        if verbose:
            print("Sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        # Compare with the sorted read table because we have no
        # guarantees that read_sorted returns a completely sorted table
        self.assertTrue(allequal(sortedtable,
                                 numpy.sort(sortedtable2, order="icol")))

    def test01_readSorted2(self):
        """Testing the Table.read_sorted() method with no arguments (re-open).
        """
        self._reopen()
        table = self.h5file.root.table
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = table.read_sorted('icol')
        if verbose:
            print("Sorted table:", sortedtable)
            print("The values from read_sorted:", sortedtable2)
        # Compare with the sorted read table because we have no
        # guarantees that read_sorted returns a completely sorted table
        self.assertTrue(allequal(sortedtable,
                                 numpy.sort(sortedtable2, order="icol")))

    def test02_copy_sorted1(self):
        """Testing the Table.copy(sortby) method."""
        table = self.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = numpy.sort(table2[:], order='icol')
        if verbose:
            print("Original table:", table2[:])
            print("The sorted values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))

    def test02_copy_sorted2(self):
        """Testing the Table.copy(sortby) method after table re-opening."""
        self._reopen(mode='a')
        table = self.h5file.root.table
        # Copy to another table
        table.nrowsinbuf = self.nrowsinbuf
        table2 = table.copy("/", 'table2', sortby="icol")
        sortedtable = numpy.sort(table[:], order='icol')
        sortedtable2 = numpy.sort(table2[:], order='icol')
        if verbose:
            print("Original table:", table2[:])
            print("The sorted values from copy:", sortedtable2)
        self.assertTrue(allequal(sortedtable, sortedtable2))


class ReadSortedIndex0(ReadSortedIndexTestCase):
    optlevel = 0


class ReadSortedIndex3(ReadSortedIndexTestCase):
    optlevel = 3


class ReadSortedIndex6(ReadSortedIndexTestCase):
    optlevel = 6


class ReadSortedIndex9(ReadSortedIndexTestCase):
    optlevel = 9


class Issue156TestBase(PyTablesTestCase):
    # field name in table according to which test_copysort() sorts the table
    sort_field = None

    def setUp(self):
        # create hdf5 file
        self.filename = tempfile.mktemp(".hdf5")
        self.file = open_file(self.filename, mode="w")

        # create nested table
        class Foo(IsDescription):
            frame = UInt16Col()

            class Bar(IsDescription):
                code = UInt16Col()

        table = self.file.create_table('/', 'foo', Foo,
                                       filters=Filters(3, 'zlib'),
                                       createparents=True)

        self.file.flush()

        # fill table with 10 random numbers
        for k in xrange(10):
            row = table.row
            row['frame'] = numpy.random.random_integers(0, 2**16-1)
            row['Bar/code'] = numpy.random.random_integers(0, 2**16-1)
            row.append()

        self.file.flush()

    def tearDown(self):
        self.file.close()
        os.remove(self.filename)

    def test_copysort(self):
        # copy table
        oldNode = self.file.get_node('/foo')

        # create completely sorted index on a main column
        oldNode.colinstances[self.sort_field].create_csindex()

        # this fails on ade2ba123efd267fd31
        # see gh-156
        new_node = oldNode.copy(newname='foo2', overwrite=True,
                                sortby=self.sort_field, checkCSI=True,
                                propindexes=True)

        # check column is sorted
        self.assertTrue(numpy.all(
            new_node.col(self.sort_field) ==
            sorted(oldNode.col(self.sort_field))))
        # check index is available
        self.assertTrue(self.sort_field in new_node.colindexes)
        # check CSI was propagated
        self.assertTrue(new_node.colindexes[self.sort_field].is_csi)


class Issue156TestCase01(Issue156TestBase):
    # sort by field from non nested entry
    sort_field = 'frame'


class Issue156TestCase02(Issue156TestBase):
    # sort by field from nested entry
    sort_field = 'Bar/code'


class Issue119Time32ColTestCase(PyTablesTestCase):
    """TimeCol not properly indexing."""

    col_typ = Time32Col
    values = [
        0.93240451618785880,
        0.76322375510776170,
        0.16695030056300875,
        0.91259117097807850,
        0.93977847053454630,
        0.51450406513503090,
        0.24452129962257563,
        0.85475938924825230,
        0.32512326762476930,
        0.75127635627046820,
    ]

    def setUp(self):
        # create hdf5 file
        self.filename = tempfile.mktemp(".hdf5")
        self.file = open_file(self.filename, mode="w")

        class Descr(IsDescription):
            when = self.col_typ(pos=1)
            value = Float32Col(pos=2)

        self.table = self.file.create_table('/', 'test', Descr)

        self.t = 1321031471.0  # 11/11/11 11:11:11
        data = [(self.t + i, item) for i, item in enumerate(self.values)]
        self.table.append(data)
        self.file.flush()

    def tearDown(self):
        self.file.close()
        os.remove(self.filename)

    def test_timecol_issue(self):
        tbl = self.table
        t = self.t

        wherestr = '(when >= %d) & (when < %d)' % (t, t + 5)

        no_index = tbl.read_where(wherestr)

        tbl.cols.when.create_index(_verbose=False)
        with_index = tbl.read_where(wherestr)

        self.assertTrue((no_index == with_index).all())


class Issue119Time64ColTestCase(Issue119Time32ColTestCase):
    col_typ = Time64Col


class TestIndexingNans(TempFileMixin, PyTablesTestCase):
    def test_issue_282(self):
        trMap = {'index': Int64Col(), 'values': FloatCol()}
        table = self.h5file.create_table('/', 'table', trMap)

        r = table.row
        for i in range(5):
            r['index'] = i
            r['values'] = numpy.nan if i == 0 else i
            r.append()
        table.flush()

        table.cols.values.create_index()

        # retrieve
        result = table.read_where('(values >= 0)')
        self.assertTrue(len(result) == 4)

    def test_issue_327(self):
        table = self.h5file.create_table('/', 'table', dict(
            index=Int64Col(),
            values=FloatCol(shape=()),
            values2=FloatCol(shape=()),
        ))

        r = table.row
        for i in range(5):
            r['index'] = i
            r['values'] = numpy.nan if i == 2 or i == 3 else i
            r['values2'] = i
            r.append()
        table.flush()

        table.cols.values.create_index()
        table.cols.values2.create_index()

        results2 = table.read_where('(values2 > 0)')
        self.assertTrue(len(results2) == 4)

        results = table.read_where('(values > 0)')
        self.assertTrue(len(results) == 2)

#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()

    niter = 1
    # heavy = 1  # Uncomment this only for testing purposes!

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicReadTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibReadTestCase))
        theSuite.addTest(unittest.makeSuite(BloscReadTestCase))
        theSuite.addTest(unittest.makeSuite(LZOReadTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ReadTestCase))
        theSuite.addTest(unittest.makeSuite(ShuffleReadTestCase))
        theSuite.addTest(unittest.makeSuite(Fletcher32ReadTestCase))
        theSuite.addTest(unittest.makeSuite(ShuffleFletcher32ReadTestCase))
        theSuite.addTest(unittest.makeSuite(OneHalfTestCase))
        theSuite.addTest(unittest.makeSuite(UpperBoundTestCase))
        theSuite.addTest(unittest.makeSuite(LowerBoundTestCase))
        theSuite.addTest(unittest.makeSuite(AI1TestCase))
        theSuite.addTest(unittest.makeSuite(AI2TestCase))
        theSuite.addTest(unittest.makeSuite(AI9TestCase))
        theSuite.addTest(unittest.makeSuite(DeepTableIndexTestCase))
        theSuite.addTest(unittest.makeSuite(IndexPropsChangeTestCase))
        theSuite.addTest(unittest.makeSuite(IndexFiltersTestCase))
        theSuite.addTest(unittest.makeSuite(OldIndexTestCase))
        theSuite.addTest(unittest.makeSuite(CompletelySortedIndexTestCase))
        theSuite.addTest(unittest.makeSuite(ManyNodesTestCase))
        theSuite.addTest(unittest.makeSuite(ReadSortedIndex0))
        theSuite.addTest(unittest.makeSuite(ReadSortedIndex3))
        theSuite.addTest(unittest.makeSuite(ReadSortedIndex6))
        theSuite.addTest(unittest.makeSuite(ReadSortedIndex9))
        theSuite.addTest(unittest.makeSuite(Issue156TestCase01))
        theSuite.addTest(unittest.makeSuite(Issue156TestCase02))
        theSuite.addTest(unittest.makeSuite(Issue119Time32ColTestCase))
        theSuite.addTest(unittest.makeSuite(Issue119Time64ColTestCase))
        theSuite.addTest(unittest.makeSuite(TestIndexingNans))
    if heavy:
        # These are too heavy for normal testing
        theSuite.addTest(unittest.makeSuite(AI4bTestCase))
        theSuite.addTest(unittest.makeSuite(AI5TestCase))
        theSuite.addTest(unittest.makeSuite(AI6TestCase))
        theSuite.addTest(unittest.makeSuite(AI7TestCase))
        theSuite.addTest(unittest.makeSuite(AI8TestCase))
        theSuite.addTest(unittest.makeSuite(AI10TestCase))
        theSuite.addTest(unittest.makeSuite(AI11TestCase))
        theSuite.addTest(unittest.makeSuite(AI12TestCase))

    return theSuite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_indexvalues
# -*- coding: utf-8 -*-

from __future__ import print_function
import os
import random
import unittest
import tempfile

import numpy

from tables import *
from tables.idxutils import calc_chunksize
from tables.tests import common
from tables.tests.common import verbose, heavy, cleanup

# To delete the internal attributes automagically
unittest.TestCase.tearDown = cleanup

# An alias for frozenset
fzset = frozenset

# To make the tests values reproductibles
random.seed(19)

# Sensible parameters for indexing with small blocksizes
small_blocksizes = (16, 8, 4, 2)  # The smaller set of parameters...
# The size for medium indexes
minRowIndex = 1000


class Small(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"")
    var2 = BoolCol(dflt=0)
    var3 = IntCol(dflt=0)
    var4 = FloatCol(dflt=0)


class SelectValuesTestCase(unittest.TestCase):
    compress = 1
    complib = "zlib"
    shuffle = 1
    fletcher32 = 0
    chunkshape = 10
    buffersize = 0
    random = 0
    values = None
    reopen = False

    def setUp(self):
        # Create an instance of an HDF5 Table
        if verbose:
            print("Checking index kind-->", self.kind)
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()

    def populateFile(self):
        # Set a seed for the random generator if needed.
        # This is useful when one need reproductible results.
        if self.random and hasattr(self, "seed"):
            random.seed(self.seed)
        group = self.rootgroup
        # Create an table
        title = "This is the IndexArray title"
        filters = Filters(complevel=self.compress,
                          complib=self.complib,
                          shuffle=self.shuffle,
                          fletcher32=self.fletcher32)
        table1 = self.fileh.create_table(group, 'table1', Small, title,
                                         filters, self.nrows,
                                         chunkshape=(self.chunkshape,))
        table2 = self.fileh.create_table(group, 'table2', Small, title,
                                         filters, self.nrows,
                                         chunkshape=(self.chunkshape,))
        count = 0
        for i in xrange(0, self.nrows, self.nrep):
            for j in range(self.nrep):
                if self.random:
                    k = random.randrange(self.nrows)
                elif self.values is not None:
                    lenvalues = len(self.values)
                    if i >= lenvalues:
                        i %= lenvalues
                    k = self.values[i]
                else:
                    k = i
                bk = str(k).encode('ascii')
                table1.row['var1'] = bk
                table2.row['var1'] = bk
                table1.row['var2'] = k % 2
                table2.row['var2'] = k % 2
                table1.row['var3'] = k
                table2.row['var3'] = k
                table1.row['var4'] = float(self.nrows - k - 1)
                table2.row['var4'] = float(self.nrows - k - 1)
                table1.row.append()
                table2.row.append()
                count += 1
        table1.flush()
        table2.flush()
        if self.buffersize:
            # Change the buffersize by default
            table1.nrowsinbuf = self.buffersize
        # Index all entries:
        for col in table1.colinstances.itervalues():
            indexrows = col.create_index(
                kind=self.kind, _blocksizes=self.blocksizes)
        if verbose:
            print("Number of written rows:", table1.nrows)
            print("Number of indexed rows:", indexrows)

        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")  # for flavor changes
            self.table1 = self.fileh.root.table1
            self.table2 = self.fileh.root.table1

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        cleanup(self)

    #----------------------------------------

    def test01a(self):
        """Checking selecting values from an Index (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        # First selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in
                    table1.where('(il<=t1var1)&(t1var1<=sl)')]
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Superior & inferior limits:", il, sl
#             print "Selection results (index):", results1
            print("Should look like:", results2)
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in
                    table1.where('(il<=t1var1)&(t1var1<sl)')]
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] < sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in
                    table1.where('(il<t1var1)&(t1var1<=sl)')]
        results2 = [p["var1"] for p in table2
                    if il < p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Forth selection
        t1var1 = table1.cols.var1
        self.assertTrue(t1var1 is not None)
        results1 = [p["var1"] for p in
                    table1.where('(il<t1var1)&(t1var1<sl)')]
        results2 = [p["var1"] for p in table2
                    if il < p["var1"] < sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test01b(self):
        """Checking selecting values from an Index (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        # First selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in table1.where('t1var1 < sl')]
        results2 = [p["var1"] for p in table2
                    if p["var1"] < sl]
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in table1.where('t1var1 <= sl')]
        results2 = [p["var1"] for p in table2
                    if p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        t1var1 = table1.cols.var1
        results1 = [p["var1"] for p in table1.where('t1var1 > sl')]
        results2 = [p["var1"] for p in table2
                    if p["var1"] > sl]
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        t1var1 = table1.cols.var1
        self.assertTrue(t1var1 is not None)
        results1 = [p["var1"] for p in table1.where('t1var1 >= sl')]
        results2 = [p["var1"] for p in table2
                    if p["var1"] >= sl]
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test02a(self):
        """Checking selecting values from an Index (bool flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Do some selections and check the results
        t1var2 = table1.cols.var2
        self.assertTrue(t1var2 is not None)
        results1 = [p["var2"] for p in table1.where('t1var2 == True')]
        results2 = [p["var2"] for p in table2 if p["var2"] is True]
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test02b(self):
        """Checking selecting values from an Index (bool flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Do some selections and check the results
        t1var2 = table1.cols.var2
        self.assertTrue(t1var2 is not None)
        results1 = [p["var2"] for p in table1.where('t1var2 == False')]
        results2 = [p["var2"] for p in table2 if p["var2"] is False]
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test03a(self):
        """Checking selecting values from an Index (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var3"] for p in table1.where('(il<=t1col)&(t1col<=sl)')]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        results1 = [p["var3"] for p in table1.where('(il<=t1col)&(t1col<sl)')]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var3"] for p in table1.where('(il<t1col)&(t1col<=sl)')]
        results2 = [p["var3"] for p in table2
                    if il < p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var3"] for p in table1.where('(il<t1col)&(t1col<sl)')]
        results2 = [p["var3"] for p in table2
                    if il < p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test03b(self):
        """Checking selecting values from an Index (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var3"] for p in table1.where('t1col < sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        results1 = [p["var3"] for p in table1.where('t1col <= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var3"] for p in table1.where('t1col > sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var3"] for p in table1.where('t1col >= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test03c(self):
        """Checking selecting values from an Index (long flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03c..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = long(self.il)
        sl = long(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var3"] for p in table1.where('t1col < sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        results1 = [p["var3"] for p in table1.where('t1col <= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var3"] for p in table1.where('t1col > sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var3"] for p in table1.where('t1col >= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test03d(self):
        """Checking selecting values from an Index (long and int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03d..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = int(self.il)
        sl = long(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var3"] for p in table1.where('t1col < sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        results1 = [p["var3"] for p in table1.where('t1col <= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var3"] for p in table1.where('t1col > sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var3"] for p in table1.where('t1col >= sl')]
        results2 = [p["var3"] for p in table2
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test04a(self):
        """Checking selecting values from an Index (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var4"] for p in table1.where('(il<=t1col)&(t1col<=sl)')]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())

        # Second selection
        results1 = [p["var4"] for p in table1.where('(il<=t1col)&(t1col<sl)')]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var4"] for p in table1.where('(il<t1col)&(t1col<=sl)')]
        results2 = [p["var4"] for p in table2
                    if il < p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var4"] for p in table1.where('(il<t1col)&(t1col<sl)')]
        results2 = [p["var4"] for p in table2
                    if il < p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test04b(self):
        """Checking selecting values from an Index (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var4"] for p in table1.where('t1col < sl')]
        results2 = [p["var4"] for p in table2
                    if p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        results1 = [p["var4"] for p in table1.where('t1col <= sl')]
        results2 = [p["var4"] for p in table2
                    if p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        results1 = [p["var4"] for p in table1.where('t1col > sl')]
        results2 = [p["var4"] for p in table2
                    if p["var4"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        results1 = [p["var4"] for p in table1.where('t1col >= sl')]
        results2 = [p["var4"] for p in table2
                    if p["var4"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test05a(self):
        """Checking get_where_list & itersequence (string, python flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        # First selection
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())

        # Second selection
        condition = '(il<=t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if il < p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = '(il<t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if il < p["var1"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test05b(self):
        """Checking get_where_list & itersequence (numpy string lims & python
        flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = numpy.string_(self.il)
        sl = numpy.string_(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var1

        # First selection
        condition = 't1col<sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if p["var1"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't1col<=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't1col>sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2
                    if p["var1"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var1'] for p in table1.itersequence(rowList1)]
        results2 = [p["var1"] for p in table2 if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test06a(self):
        """Checking get_where_list & itersequence (bool flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Do some selections and check the results
        t1var2 = table1.cols.var2
        condition = 't1var2==True'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1var2.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var2'] for p in table1.itersequence(rowList1)]
        results2 = [p["var2"] for p in table2 if p["var2"] is True]
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test06b(self):
        """Checking get_where_list & itersequence (numpy bool limits &
        flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Do some selections and check the results
        t1var2 = table1.cols.var2
        false = numpy.bool_(False)
        self.assertFalse(false)     # silence pyflakes
        condition = 't1var2==false'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1var2.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var2'] for p in table1.itersequence(rowList1)]
        results2 = [p["var2"] for p in table2 if p["var2"] is False]
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test07a(self):
        """Checking get_where_list & itersequence (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        # First selection
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())

        # Second selection
        condition = '(il<=t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if il < p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = '(il<t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if il < p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test07b(self):
        """Checking get_where_list & itersequence (numpy int limits &
        flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = numpy.int32(self.il)
        sl = numpy.uint16(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3

        # First selection
        condition = 't1col<sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if p["var3"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't1col<=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't1col>sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if p["var3"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var3'] for p in table1.itersequence(rowList1)]
        results2 = [p["var3"] for p in table2
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test08a(self):
        """Checking get_where_list & itersequence (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4
        # First selection
        condition = '(il<=t1col)&(t1col<=sl)'
        # results1 = [p["var4"] for p in table1.where(condition)]
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())

        # Second selection
        condition = '(il<=t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if il < p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = '(il<t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        table1.flavor = "python"
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if il < p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test08b(self):
        """Checking get_where_list & itersequence (numpy float limits &
        flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        # il = numpy.float32(self.il)
        sl = numpy.float64(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4

        # First selection
        condition = 't1col<sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if p["var4"] < sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't1col<=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't1col>sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if p["var4"] > sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        rowList1 = table1.get_where_list(condition)
        results1 = [p['var4'] for p in table1.itersequence(rowList1)]
        results2 = [p["var4"] for p in table2 if p["var4"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limit:", sl)
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test09a(self):
        """Checking non-indexed where() (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        table1._disable_indexing_in_queries()

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        self.assertTrue(t1col is not None)

        # First selection
        condition = 't1col<=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in table1.where(
            condition, start=2, stop=10)]
        results2 = [p["var1"] for p in table2.iterrows(2, 10)
                    if p["var1"] <= sl]
        if verbose:
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = '(il<t1col)&(t1col<sl)'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=30, step=2)]
        results2 = [p["var1"] for p in table2.iterrows(2, 30, 2)
                    if il < p["var1"] < sl]
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il>t1col)&(t1col>sl)'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=-5)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, -5)  # Negative indices
            if (il > p["var1"] > sl)
        ]
        if verbose:
            print("Limits:", il, sl)
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # This selection to be commented out
#         condition = 't1col>=sl'
#         self.assertTrue(not table1.will_query_use_indexing(condition))
#         results1 = [p['var1'] for p in table1.where(condition,start=2,
#                                                     stop=-1,step=1)]
#         results2 = [p["var1"] for p in table2.iterrows(2, -1, 1)
#                     if p["var1"] >= sl]
#         if verbose:
#             print "Limit:", sl
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
#             print "Length results:", len(results1)
#             print "Should be:", len(results2)
#         self.assertEqual(len(results1), len(results2))
#         self.assertEqual(results1, results2)

        # Fourth selection
        # results1 = [p['var1'] for p in
        # table1.where(condition,start=2,stop=-1,step=3)]
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=-1, step=3)]
        results2 = [p["var1"] for p in table2.iterrows(2, -1, 3)
                    if p["var1"] >= sl]
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Re-enable the indexing in queries basically to unnail the
        # condition cache and not raising the performance warning
        # about some indexes being dirty
        table1._enable_indexing_in_queries()

    def test09b(self):
        """Checking non-indexed where() (float flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        table1._disable_indexing_in_queries()

        # Convert the limits to the appropriate type
        il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4
        self.assertTrue(t1col is not None)

        # First selection
        condition = 't1col<sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var4'] for p in
                    table1.where(condition, start=2, stop=5)]
        results2 = [p["var4"] for p in table2.iterrows(2, 5)
                    if p["var4"] < sl]
        if verbose:
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = '(il<t1col)&(t1col<=sl)'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var4'] for p in
                    table1.where(condition, start=2, stop=-1, step=2)]
        results2 = [p["var4"] for p in table2.iterrows(2, -1, 2)
                    if il < p["var4"] <= sl]
        if verbose:
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [
            p['var4'] for p in table1.where(condition, start=2, stop=-5)
        ]
        results2 = [
            p["var4"] for p in table2.iterrows(2, -5)  # Negative indices
            if il <= p["var4"] <= sl
        ]
        if verbose:
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var4'] for p in
                    table1.where(condition, start=0, stop=-1, step=3)]
        results2 = [p["var4"] for p in table2.iterrows(0, -1, 3)
                    if p["var4"] >= sl]
        if verbose:
            print("Limit:", sl)
#             print "Selection results (in-kernel):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Re-enable the indexing in queries basically to unnail the
        # condition cache and not raising the performance warning
        # about some indexes being dirty
        table1._enable_indexing_in_queries()

    def test09c(self):
        "Check non-indexed where() w/ ranges, changing step (string flavor)"

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09c..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        table1._disable_indexing_in_queries()

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        self.assertTrue(t1col is not None)

        # First selection
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=-1, step=3)]
        results2 = [p["var1"] for p in table2.iterrows(2, -1, 3)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=5, stop=-1, step=10)]
        results2 = [p["var1"] for p in table2.iterrows(5, -1, 10)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=5, stop=-3, step=11)]
        results2 = [p["var1"] for p in table2.iterrows(5, -3, 11)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=-1, step=300)]
        results2 = [p["var1"] for p in table2.iterrows(2, -1, 300)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Re-enable the indexing in queries basically to unnail the
        # condition cache and not raising the performance warning
        # about some indexes being dirty
        table1._enable_indexing_in_queries()

    def test09d(self):
        "Checking non-indexed where() w/ ranges, changing step (int flavor)"

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09d..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        table1._disable_indexing_in_queries()

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t3col = table1.cols.var3
        self.assertTrue(t3col is not None)

        # First selection
        condition = 't3col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=2, stop=-1, step=3)]
        results2 = [p["var3"] for p in table2.iterrows(2, -1, 3)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't3col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=5, stop=-1, step=10)]
        results2 = [p["var3"] for p in table2.iterrows(5, -1, 10)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't3col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=5, stop=-3, step=11)]
        results2 = [p["var3"] for p in table2.iterrows(5, -3, 11)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't3col>=sl'
        self.assertTrue(not table1.will_query_use_indexing(condition))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=2, stop=-1, step=300)]
        results2 = [p["var3"] for p in table2.iterrows(2, -1, 300)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Re-enable the indexing in queries basically to unnail the
        # condition cache and not raising the performance warning
        # about some indexes being dirty
        table1._enable_indexing_in_queries()

    def test10a(self):
        """Checking indexed where() with ranges (string flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        # First selection
        condition = 't1col<=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=10)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 10) if p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat second selection (testing caches)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=2)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 2)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
            print("Selection results (indexed):", results1)
            print("Should look like:", results2)
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<t1col)&(t1col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=-5)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, -5)  # Negative indices
            if (il < p["var1"] < sl)
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=1, stop=-1,
                                            step=3)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(1, -1, 3)
            if p["var1"] >= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test10b(self):
        """Checking indexed where() with ranges (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t3col = table1.cols.var3
        # First selection
        condition = 't3col<=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [
            p['var3'] for p in table1.where(condition, start=2, stop=10)
        ]
        results2 = [
            p["var3"] for p in table2.iterrows(2, 10)
            if p["var3"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = '(il<=t3col)&(t3col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [
            p['var3'] for p in table1.where(condition, start=2, stop=30,
                                            step=2)
        ]
        results2 = [
            p["var3"] for p in table2.iterrows(2, 30, 2)
            if il <= p["var3"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = '(il<t3col)&(t3col<sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [
            p['var3'] for p in table1.where(condition, start=2, stop=-5)
        ]
        results2 = [
            p["var3"] for p in table2.iterrows(2, -5)  # Negative indices
            if (il < p["var3"] < sl)
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't3col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=1, stop=-1, step=3)]
        results2 = [p["var3"] for p in table2.iterrows(1, -1, 3)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test10c(self):
        """Checking indexed where() with ranges, changing step (string
        flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10c..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1

        # First selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=-1, step=3)]
        results2 = [p["var1"] for p in table2.iterrows(2, -1, 3)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=5, stop=-1, step=10)]
        results2 = [p["var1"] for p in table2.iterrows(5, -1, 10)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=5, stop=-3, step=11)]
        results2 = [p["var1"] for p in table2.iterrows(5, -3, 11)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't1col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=2, stop=-1, step=300)]
        results2 = [p["var1"] for p in table2.iterrows(2, -1, 300)
                    if p["var1"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test10d(self):
        """Checking indexed where() with ranges, changing step (int flavor)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10d..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t3col = table1.cols.var3

        # First selection
        condition = 't3col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=2, stop=-1, step=3)]
        results2 = [p["var3"] for p in table2.iterrows(2, -1, 3)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection
        condition = 't3col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=5, stop=-1, step=10)]
        results2 = [p["var3"] for p in table2.iterrows(5, -1, 10)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection
        condition = 't3col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=5, stop=-3, step=11)]
        results2 = [p["var3"] for p in table2.iterrows(5, -3, 11)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection
        condition = 't3col>=sl'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t3col.pathname]))
        results1 = [p['var3'] for p in
                    table1.where(condition, start=2, stop=-1, step=300)]
        results2 = [p["var3"] for p in table2.iterrows(2, -1, 300)
                    if p["var3"] >= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test11a(self):
        """Checking selecting values from an Index via read_coordinates()"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do a selection and check the result
        t1var1 = table1.cols.var1
        condition = '(il<=t1var1)&(t1var1<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1var1.pathname])
        )
        coords1 = table1.get_where_list(condition)
        table1.flavor = "python"
        results1 = table1.read_coordinates(coords1, field="var1")
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Superior & inferior limits:", il, sl
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test12a(self):
        """Checking selecting values after a Table.append() operation."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test12a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Append more rows in already created indexes
        count = 0
        for i in xrange(0, self.nrows//2, self.nrep):
            for j in range(self.nrep):
                if self.random:
                    k = random.randrange(self.nrows)
                elif self.values is not None:
                    lenvalues = len(self.values)
                    if i >= lenvalues:
                        i %= lenvalues
                    k = self.values[i]
                else:
                    k = i
                table1.row['var1'] = str(k)
                table2.row['var1'] = str(k)
                table1.row['var2'] = k % 2
                table2.row['var2'] = k % 2
                table1.row['var3'] = k
                table2.row['var3'] = k
                table1.row['var4'] = float(self.nrows - k - 1)
                table2.row['var4'] = float(self.nrows - k - 1)
                table1.row.append()
                table2.row.append()
                count += 1
        table1.flush()
        table2.flush()

        t1var1 = table1.cols.var1
        t1var2 = table1.cols.var2
        t1var3 = table1.cols.var3
        t1var4 = table1.cols.var4
        self.assertFalse(t1var1.index.dirty)
        self.assertFalse(t1var2.index.dirty)
        self.assertFalse(t1var3.index.dirty)
        self.assertFalse(t1var4.index.dirty)

        # Do some selections and check the results
        # First selection: string
        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        results1 = [p["var1"] for p in
                    table1.where('(il<=t1var1)&(t1var1<=sl)')]
        results2 = [p["var1"] for p in table2
                    if il <= p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Superior & inferior limits:", il, sl
#             print "Selection results (index):", results1
            print("Should look like:", results2)
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Second selection: bool
        results1 = [p["var2"] for p in table1.where('t1var2 == True')]
        results2 = [p["var2"] for p in table2 if p["var2"] is True]
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Third selection: int
        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        t1var3 = table1.cols.var3
        results1 = [p["var3"] for p in table1.where(
            '(il<=t1var3)&(t1var3<=sl)')]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Fourth selection: float
        # Convert the limits to the appropriate type
        il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        results1 = [p["var4"] for p in table1.where(
            '(il<=t1var4)&(t1var4<=sl)')]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())

    def test13a(self):
        """Checking repeated queries (checking caches)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13a..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection (testing caches)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=2)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 2)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13b(self):
        """Checking repeated queries, varying step (checking caches)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13b..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection (testing caches)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=2, stop=30,
                                            step=2)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(2, 30, 2)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13c(self):
        """Checking repeated queries, varying start, stop, step."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13c..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=0, stop=1, step=2)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 1, 2)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection (testing caches)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=0, stop=5, step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 5, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13d(self):
        """Checking repeated queries, varying start, stop, step (another
        twist)"""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13d..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname])
        )
        results1 = [
            p['var1'] for p in table1.where(condition, start=0, stop=1, step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 1, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection (testing caches)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=0, stop=1, step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 1, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#            print "Selection results (indexed):", results1
#            print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13e(self):
        """Checking repeated queries, with varying condition."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13e..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [
            p['var1'] for p in table1.where(condition, start=0, stop=10,
                                            step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 10, 1)
            if il <= p["var1"] <= sl
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection with a more complex condition
        t2col = table1.cols.var2
        condition = '(il<=t1col)&(t1col<=sl)&(t2col==True)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname, t2col.pathname]))
        results1 = [
            p['var1'] for p in
            table1.where(condition, start=0, stop=10, step=1)
        ]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 10, 1)
            if il <= p["var1"] <= sl and p["var2"] is True
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13f(self):
        """Checking repeated queries, with varying condition."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13f..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Remove indexes in var2 column
        table1.cols.var2.remove_index()
        table2.cols.var2.remove_index()

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        t2col = table1.cols.var2
        self.assertTrue(t2col is not None)
        condition = '(il<=t1col)&(t1col<=sl)&(t2col==True)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=0, stop=10, step=1)]
        results2 = [
            p["var1"] for p in table2.iterrows(0, 10, 1)
            if il <= p["var1"] <= sl and p["var2"] is True
        ]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection with a simpler condition
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=0, stop=10, step=1)]
        results2 = [p["var1"] for p in table2.iterrows(0, 10, 1)
                    if il <= p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat again with the original condition, but with a constant
        constant = True
        condition = '(il<=t1col)&(t1col<=sl)&(t2col==constant)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=0, stop=10, step=1)]
        results2 = [p["var1"] for p in table2.iterrows(0, 10, 1)
                    if il <= p["var1"] <= sl and p["var2"] == constant]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test13g(self):
        """Checking repeated queries, with different limits."""

        if verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13g..." % self.__class__.__name__)

        table1 = self.fileh.root.table1
        table2 = self.fileh.root.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        t1col = table1.cols.var1
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=0, stop=10, step=1)]
        results2 = [p["var1"] for p in table2.iterrows(0, 10, 1)
                    if il <= p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

        # Repeat the selection with different limits
        il, sl = (str(self.il + 1).encode(
            'ascii'), str(self.sl-2).encode('ascii'))
        t2col = table1.cols.var2
        self.assertTrue(t2col is not None)
        condition = '(il<=t1col)&(t1col<=sl)'
        self.assertTrue(
            table1.will_query_use_indexing(condition) ==
            fzset([t1col.pathname]))
        results1 = [p['var1'] for p in
                    table1.where(condition, start=0, stop=10, step=1)]
        results2 = [p["var1"] for p in table2.iterrows(0, 10, 1)
                    if il <= p["var1"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
            print("Limits:", il, sl)
#             print "Selection results (indexed):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)


class SV1aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    chunkshape = 1
    buffersize = 2
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV1bTestCase(SV1aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = blocksizes[2]//2**9
    buffersize = chunkshape * 5


class SV2aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    chunkshape = 2
    buffersize = 2
    ss = blocksizes[2]
    nrows = ss * 2-1
    reopen = 1
    nrep = 1
    il = 0
    sl = 2


class SV2bTestCase(SV2aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = blocksizes[2]//2**7
    buffersize = chunkshape * 20


class SV3aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    chunkshape = 2
    buffersize = 3
    ss = blocksizes[2]
    nrows = ss * 5-1
    reopen = 1
    nrep = 3
    il = 0
    sl = 3


class SV3bTestCase(SV3aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
#    chunkshape = 4
#    buffersize = 16
    chunkshape = 3
    buffersize = 9


class SV4aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    buffersize = 10
    ss = blocksizes[2]
    nrows = ss * 3
    reopen = 0
    nrep = 1
    # il = nrows-cs
    il = 0
    sl = nrows


class SV4bTestCase(SV4aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = 500
    buffersize = 1000


class SV5aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5
    reopen = 0
    nrep = 1
    il = 0
    sl = nrows


class SV5bTestCase(SV5aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)


class SV6aTestCase(SelectValuesTestCase):
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5 + 1
    reopen = 0
    cs = blocksizes[3]
    nrep = cs + 1
    il = -1
    sl = nrows


class SV6bTestCase(SV6aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)


class SV7aTestCase(SelectValuesTestCase):
    random = 1
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5 + 3
    reopen = 0
    cs = blocksizes[3]
    nrep = cs-1
    il = -10
    sl = nrows


class SV7bTestCase(SV7aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)


class SV8aTestCase(SelectValuesTestCase):
    random = 0
    chunkshape = 1
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5-3
    reopen = 0
    cs = blocksizes[3]
    nrep = cs-1
    il = 10
    sl = nrows-10


class SV8bTestCase(SV8aTestCase):
    random = 0
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)


class SV9aTestCase(SelectValuesTestCase):
    random = 1
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5 + 11
    reopen = 0
    cs = blocksizes[3]
    nrep = cs-1
    il = 10
    sl = nrows-10


class SV9bTestCase(SV9aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)


class SV10aTestCase(SelectValuesTestCase):
    random = 1
    blocksizes = small_blocksizes
    chunkshape = 1
    buffersize = 1
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV10bTestCase(SV10aTestCase):
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = 5
    buffersize = 6


class SV11aTestCase(SelectValuesTestCase):
    # This checks a special case that failed. It was discovered in a
    # random test above (SV10a). It is explicitely put here as a way
    # to always check that specific case.
    values = [1, 7, 6, 7, 0, 7, 4, 4, 9, 5]
    blocksizes = small_blocksizes
    chunkshape = 1
    buffersize = 1
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV11bTestCase(SelectValuesTestCase):
    # This checks a special case that failed. It was discovered in a
    # random test above (SV10a). It is explicitely put here as a way
    # to always check that specific case.
    values = [1, 7, 6, 7, 0, 7, 4, 4, 9, 5]
    chunkshape = 2
    buffersize = 2
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV12aTestCase(SelectValuesTestCase):
    # This checks a special case that failed. It was discovered in a
    # random test above (SV10b). It is explicitely put here as a way
    # to always check that specific case.
    # values = [0, 7, 0, 6, 5, 1, 6, 7, 0, 0]
    values = [4, 4, 1, 5, 2, 0, 1, 4, 3, 9]
    blocksizes = small_blocksizes
    chunkshape = 1
    buffersize = 1
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV12bTestCase(SelectValuesTestCase):
    # This checks a special case that failed. It was discovered in a
    # random test above (SV10b). It is explicitely put here as a way
    # to always check that specific case.
    # values = [0, 7, 0, 6, 5, 1, 6, 7, 0, 0]
    values = [4, 4, 1, 5, 2, 0, 1, 4, 3, 9]
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = 2
    buffersize = 2
    ss = blocksizes[2]
    nrows = ss
    reopen = 1
    nrep = ss
    il = 0
    sl = ss


class SV13aTestCase(SelectValuesTestCase):
    values = [0, 7, 0, 6, 5, 1, 6, 7, 0, 0]
    blocksizes = small_blocksizes
    chunkshape = 3
    buffersize = 5
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    nrep = ss
    il = 0
    sl = ss


class SV13bTestCase(SelectValuesTestCase):
    values = [0, 7, 0, 6, 5, 1, 6, 7, 0, 0]
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = 5
    buffersize = 10
    ss = blocksizes[2]
    nrows = ss
    reopen = 1
    nrep = ss
    il = 0
    sl = ss


class SV14aTestCase(SelectValuesTestCase):
    values = [1, 7, 6, 7, 0, 7, 4, 4, 9, 5]
    blocksizes = small_blocksizes
    chunkshape = 2
    buffersize = 5
    ss = blocksizes[2]
    nrows = ss
    reopen = 0
    cs = blocksizes[3]
    nrep = cs
    il = -5
    sl = 500


class SV14bTestCase(SelectValuesTestCase):
    values = [1, 7, 6, 7, 0, 7, 4, 4, 9, 5]
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    chunkshape = 9
    buffersize = 10
    ss = blocksizes[2]
    nrows = ss
    reopen = 1
    nrep = 9
    il = 0
    cs = blocksizes[3]
    sl = ss-cs + 1


class SV15aTestCase(SelectValuesTestCase):
    # Test that checks for case where there are not valid values in
    # the indexed part, but they exist in the non-indexed region.
    # At least, test01b takes account of that
    random = 1
    # Both values of seed below triggers a fail in indexing code
    # seed = 1885
    seed = 183
    blocksizes = small_blocksizes
    ss = blocksizes[2]
    nrows = ss * 5 + 1
    reopen = 0
    cs = blocksizes[3]
    nrep = cs-1
    il = -10
    sl = nrows


class SV15bTestCase(SelectValuesTestCase):
    # Test that checks for case where there are not valid values in
    # the indexed part, but they exist in the non-indexed region.
    # At least, test01b takes account of that
    random = 1
    # Both values of seed below triggers a fail in indexing code
    seed = 1885
    # seed = 183
    blocksizes = calc_chunksize(minRowIndex, memlevel=1)
    ss = blocksizes[2]
    nrows = ss * 5 + 1
    reopen = 1
    cs = blocksizes[3]
    nrep = cs-1
    il = -10
    sl = nrows


class LastRowReuseBuffers(common.PyTablesTestCase):
    # Test that checks for possible reuse of buffers coming
    # from last row in the sorted part of indexes
    nelem = 1221
    numpy.random.seed(1)
    random.seed(1)

    class Record(IsDescription):
        id1 = Int16Col()

    def test00_lrucache(self):
        filename = tempfile.mktemp(".h5")
        fp = open_file(filename, 'w', node_cache_slots=64)
        ta = fp.create_table('/', 'table', self.Record, filters=Filters(1))
        id1 = numpy.random.randint(0, 2**15, self.nelem)
        ta.append([id1])

        ta.cols.id1.create_index()

        for i in xrange(self.nelem):
            nrow = random.randint(0, self.nelem-1)
            value = id1[nrow]
            idx = ta.get_where_list('id1 == %s' % value)
            self.assertTrue(len(idx) > 0,
                            "idx--> %s %s %s %s" % (idx, i, nrow, value))
            self.assertTrue(
                nrow in idx,
                "nrow not found: %s != %s, %s" % (idx, nrow, value))

        fp.close()
        os.remove(filename)

    def test01_nocache(self):
        filename = tempfile.mktemp(".h5")
        fp = open_file(filename, 'w', node_cache_slots=0)
        ta = fp.create_table('/', 'table', self.Record, filters=Filters(1))
        id1 = numpy.random.randint(0, 2**15, self.nelem)
        ta.append([id1])

        ta.cols.id1.create_index()

        for i in xrange(self.nelem):
            nrow = random.randint(0, self.nelem-1)
            value = id1[nrow]
            idx = ta.get_where_list('id1 == %s' % value)
            self.assertTrue(len(idx) > 0,
                            "idx--> %s %s %s %s" % (idx, i, nrow, value))
            self.assertTrue(
                nrow in idx,
                "nrow not found: %s != %s, %s" % (idx, nrow, value))

        fp.close()
        os.remove(filename)

    def test02_dictcache(self):
        filename = tempfile.mktemp(".h5")
        fp = open_file(filename, 'w', node_cache_slots=-64)
        ta = fp.create_table('/', 'table', self.Record, filters=Filters(1))
        id1 = numpy.random.randint(0, 2**15, self.nelem)
        ta.append([id1])

        ta.cols.id1.create_index()

        for i in xrange(self.nelem):
            nrow = random.randint(0, self.nelem-1)
            value = id1[nrow]
            idx = ta.get_where_list('id1 == %s' % value)
            self.assertTrue(len(idx) > 0,
                            "idx--> %s %s %s %s" % (idx, i, nrow, value))
            self.assertTrue(
                nrow in idx,
                "nrow not found: %s != %s, %s" % (idx, nrow, value))

        fp.close()
        os.remove(filename)


normal_tests = (
    "SV1aTestCase", "SV2aTestCase", "SV3aTestCase",
    )

heavy_tests = (
    # The next are too hard to be in the 'normal' suite
    "SV1bTestCase", "SV2bTestCase", "SV3bTestCase",
    "SV4aTestCase", "SV5aTestCase", "SV6aTestCase",
    "SV7aTestCase", "SV8aTestCase", "SV9aTestCase",
    "SV10aTestCase", "SV11aTestCase", "SV12aTestCase",
    "SV13aTestCase", "SV14aTestCase", "SV15aTestCase",
    # This are properly heavy
    "SV4bTestCase", "SV5bTestCase", "SV6bTestCase",
    "SV7bTestCase", "SV8bTestCase", "SV9bTestCase",
    "SV10bTestCase", "SV11bTestCase", "SV12bTestCase",
    "SV13bTestCase", "SV14bTestCase", "SV15bTestCase",
    )


# Base classes for the different type indexes.
class UltraLightITableMixin:
    kind = "ultralight"


class LightITableMixin:
    kind = "light"


class MediumITableMixin:
    kind = "medium"


class FullITableMixin:
    kind = "full"

# Parameters for indexed queries.
ckinds = ['UltraLight', 'Light', 'Medium', 'Full']
testlevels = ['Normal', 'Heavy']

# Indexed queries: ``[ULMF]I[NH]SVXYTestCase``, where:
#
# 1. U is for 'UltraLight', L for 'Light', M for 'Medium', F for 'Full' indexes
# 2. N is for 'Normal', H for 'Heavy' tests


def iclassdata():
    for ckind in ckinds:
        for ctest in normal_tests + heavy_tests:
            classname = '%sI%s%s' % (ckind[0], testlevels[heavy][0], ctest)
            # Uncomment the next one and comment the past one if one
            # don't want to include the methods (testing purposes only)
            # cbasenames = ( '%sITableMixin' % ckind, "object")
            cbasenames = ('%sITableMixin' % ckind, ctest)
            classdict = dict(heavy=bool(ctest in heavy_tests))
            yield (classname, cbasenames, classdict)


# Create test classes.
for (cname, cbasenames, cdict) in iclassdata():
    cbases = tuple(eval(cbase) for cbase in cbasenames)
    class_ = type(cname, cbases, cdict)
    exec('%s = class_' % cname)


# -----------------------------

def suite():
    theSuite = unittest.TestSuite()

    niter = 1

    for n in range(niter):
        for cdata in iclassdata():
            class_ = eval(cdata[0])
            if not class_.heavy:
                suite_ = unittest.makeSuite(class_)
                theSuite.addTest(suite_)
            elif heavy:
                suite_ = unittest.makeSuite(class_)
                theSuite.addTest(suite_)
        theSuite.addTest(unittest.makeSuite(LastRowReuseBuffers))

    return theSuite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_index_backcompat
# -*- coding: utf-8 -*-

from __future__ import print_function
import unittest

from tables import *
from tables.tests import common
from tables.tests.common import verbose, cleanup


# Check indexes from PyTables version 2.0
class IndexesTestCase(common.PyTablesTestCase):

    def setUp(self):
        self.fileh = open_file(self._testFilename(self.file_), "r")
        self.table1 = self.fileh.root.table1
        self.table2 = self.fileh.root.table2
        self.il = 0
        self.sl = self.table1.cols.var1.index.slicesize

    def tearDown(self):
        self.fileh.close()
        cleanup(self)

    #----------------------------------------
    def test00_version(self):
        """Checking index version."""

        t1var1 = self.table1.cols.var1
        if "2_0" in self.file_:
            self.assertEqual(t1var1.index._v_version, "2.0")
        elif "2_1" in self.file_:
            self.assertEqual(t1var1.index._v_version, "2.1")

    def test01_string(self):
        """Checking string indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_string..." % self.__class__.__name__)

        table1 = self.table1
        table2 = self.table2

        # Convert the limits to the appropriate type
        il = str(self.il).encode('ascii')
        sl = str(self.sl).encode('ascii')

        # Do some selections and check the results
        # First selection
        t1var1 = table1.cols.var1
        self.assertTrue(t1var1 is not None)
        results1 = [p["var1"] for p in
                    table1.where('(il<=t1var1)&(t1var1<=sl)')]
        results2 = [p["var1"] for p in table2 if il <= p["var1"] <= sl]
        results1.sort()
        results2.sort()
        if verbose:
#             print "Superior & inferior limits:", il, sl
#             print "Selection results (index):", results1
            print("Should look like:", results2)
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test02_bool(self):
        """Checking bool indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_bool..." % self.__class__.__name__)

        table1 = self.table1
        table2 = self.table2

        # Do some selections and check the results
        t1var2 = table1.cols.var2
        self.assertTrue(t1var2 is not None)
        results1 = [p["var2"] for p in table1.where('t1var2 == True')]
        results2 = [p["var2"] for p in table2 if p["var2"] is True]
        if verbose:
            print("Selection results (index):", results1)
            print("Should look like:", results2)
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test03_int(self):
        """Checking int indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_int..." % self.__class__.__name__)

        table1 = self.table1
        table2 = self.table2

        # Convert the limits to the appropriate type
        il = int(self.il)
        sl = int(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var3
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var3"] for p in table1.where('(il<=t1col)&(t1col<=sl)')]
        results2 = [p["var3"] for p in table2
                    if il <= p["var3"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1, results2)

    def test04_float(self):
        """Checking float indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_float..." % self.__class__.__name__)

        table1 = self.table1
        table2 = self.table2

        # Convert the limits to the appropriate type
        il = float(self.il)
        sl = float(self.sl)

        # Do some selections and check the results
        t1col = table1.cols.var4
        self.assertTrue(t1col is not None)

        # First selection
        results1 = [p["var4"] for p in table1.where('(il<=t1col)&(t1col<=sl)')]
        results2 = [p["var4"] for p in table2
                    if il <= p["var4"] <= sl]
        # sort lists (indexing does not guarantee that rows are returned in
        # order)
        results1.sort()
        results2.sort()
        if verbose:
#             print "Selection results (index):", results1
#             print "Should look like:", results2
            print("Length results:", len(results1))
            print("Should be:", len(results2))
        self.assertEqual(len(results1), len(results2))
        self.assertEqual(results1.sort(), results2.sort())


# Check indexes from PyTables version 2.0
class Indexes2_0TestCase(IndexesTestCase):
    file_ = "indexes_2_0.h5"

# Check indexes from PyTables version 2.1


class Indexes2_1TestCase(IndexesTestCase):
    file_ = "indexes_2_1.h5"


#----------------------------------------------------------------------

def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(Indexes2_0TestCase))
        theSuite.addTest(unittest.makeSuite(Indexes2_1TestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_links
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2009-11-24
# Author: Francesc Alted - faltet@pytables.org
#
# $Id$
#
########################################################################

"""Test module for diferent kind of links under PyTables."""

from __future__ import print_function
import os
import unittest
import tempfile

import tables
from tables.tests import common


# Test for hard links
class HardLinkTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def _createFile(self):
        self.h5file.create_array('/', 'arr1', [1, 2])
        group1 = self.h5file.create_group('/', 'group1')
        arr2 = self.h5file.create_array(group1, 'arr2', [1, 2, 3])
        lgroup1 = self.h5file.create_hard_link('/', 'lgroup1', '/group1')
        self.assertTrue(lgroup1 is not None)
        larr1 = self.h5file.create_hard_link(group1, 'larr1', '/arr1')
        self.assertTrue(larr1 is not None)
        larr2 = self.h5file.create_hard_link('/', 'larr2', arr2)
        self.assertTrue(larr2 is not None)

    def test00_create(self):
        """Creating hard links."""
        self._createFile()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup1,
                                 hardlink=True)
        self._checkEqualityLeaf(self.h5file.root.arr1,
                                self.h5file.root.group1.larr1,
                                hardlink=True)
        self._checkEqualityLeaf(self.h5file.root.lgroup1.arr2,
                                self.h5file.root.larr2,
                                hardlink=True)

    def test01_open(self):
        """Opening a file with hard links."""

        self._createFile()
        self._reopen()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup1,
                                 hardlink=True)
        self._checkEqualityLeaf(self.h5file.root.arr1,
                                self.h5file.root.group1.larr1,
                                hardlink=True)
        self._checkEqualityLeaf(self.h5file.root.lgroup1.arr2,
                                self.h5file.root.larr2,
                                hardlink=True)

    def test02_removeLeaf(self):
        """Removing a hard link to a Leaf."""

        self._createFile()
        # First delete the initial link
        self.h5file.root.arr1.remove()
        self.assertTrue('/arr1' not in self.h5file)
        # The second link should still be there
        if common.verbose:
            print("Remaining link:", self.h5file.root.group1.larr1)
        self.assertTrue('/group1/larr1' in self.h5file)
        # Remove the second link
        self.h5file.root.group1.larr1.remove()
        self.assertTrue('/group1/larr1' not in self.h5file)

    def test03_removeGroup(self):
        """Removing a hard link to a Group."""

        self._createFile()
        if common.verbose:
            print("Original object tree:", self.h5file)
        # First delete the initial link
        self.h5file.root.group1._f_remove(force=True)
        self.assertTrue('/group1' not in self.h5file)
        # The second link should still be there
        if common.verbose:
            print("Remaining link:", self.h5file.root.lgroup1)
            print("Object tree:", self.h5file)
        self.assertTrue('/lgroup1' in self.h5file)
        # Remove the second link
        self.h5file.root.lgroup1._g_remove(recursive=True)
        self.assertTrue('/lgroup1' not in self.h5file)
        if common.verbose:
            print("Final object tree:", self.h5file)


# Test for soft links
class SoftLinkTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def _createFile(self):
        self.h5file.create_array('/', 'arr1', [1, 2])
        group1 = self.h5file.create_group('/', 'group1')
        arr2 = self.h5file.create_array(group1, 'arr2', [1, 2, 3])
        lgroup1 = self.h5file.create_soft_link('/', 'lgroup1', '/group1')
        self.assertTrue(lgroup1 is not None)
        larr1 = self.h5file.create_soft_link(group1, 'larr1', '/arr1')
        self.assertTrue(larr1 is not None)
        larr2 = self.h5file.create_soft_link('/', 'larr2', arr2)
        self.assertTrue(larr2 is not None)

    def test00_create(self):
        """Creating soft links."""
        self._createFile()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup1())
        self._checkEqualityLeaf(self.h5file.root.arr1,
                                self.h5file.root.group1.larr1())
        self._checkEqualityLeaf(self.h5file.root.lgroup1().arr2,
                                self.h5file.root.larr2())

    def test01_open(self):
        """Opening a file with soft links."""

        self._createFile()
        self._reopen()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup1())
        self._checkEqualityLeaf(self.h5file.root.arr1,
                                self.h5file.root.group1.larr1())
        self._checkEqualityLeaf(self.h5file.root.lgroup1().arr2,
                                self.h5file.root.larr2())

    def test02_remove(self):
        """Removing a soft link."""

        self._createFile()
        # First delete the referred link
        self.h5file.root.arr1.remove()
        self.assertTrue('/arr1' not in self.h5file)
        # The soft link should still be there (but dangling)
        if common.verbose:
            print("Dangling link:", self.h5file.root.group1.larr1)
        self.assertTrue('/group1/larr1' in self.h5file)
        # Remove the soft link itself
        self.h5file.root.group1.larr1.remove()
        self.assertTrue('/group1/larr1' not in self.h5file)

    def test03_copy(self):
        """Copying a soft link."""

        self._createFile()
        # Copy the link into another location
        root = self.h5file.root
        lgroup1 = root.lgroup1
        lgroup2 = lgroup1.copy('/', 'lgroup2')
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self.assertTrue('lgroup2' in root._v_children)
        self.assertTrue('lgroup2' in root._v_links)
        if common.verbose:
            print("Copied link:", lgroup2)
        # Remove the first link
        lgroup1.remove()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test03_overwrite(self):
        """Overwrite a soft link."""

        self._createFile()
        # Copy the link into another location
        root = self.h5file.root
        lgroup1 = root.lgroup1
        lgroup2 = lgroup1.copy('/', 'lgroup2')
        lgroup2 = lgroup1.copy('/', 'lgroup2', overwrite=True)
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self.assertTrue('lgroup2' in root._v_children)
        self.assertTrue('lgroup2' in root._v_links)
        if common.verbose:
            print("Copied link:", lgroup2)
        # Remove the first link
        lgroup1.remove()
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test04_move(self):
        """Moving a soft link."""

        self._createFile()
        # Move the link into another location
        lgroup1 = self.h5file.root.lgroup1
        group2 = self.h5file.create_group('/', 'group2')
        lgroup1.move(group2, 'lgroup2')
        lgroup2 = self.h5file.root.group2.lgroup2
        if common.verbose:
            print("Moved link:", lgroup2)
        self.assertTrue('/lgroup1' not in self.h5file)
        self.assertTrue('/group2/lgroup2' in self.h5file)
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.group2.lgroup2())

    def test05_rename(self):
        """Renaming a soft link."""

        self._createFile()
        # Rename the link
        lgroup1 = self.h5file.root.lgroup1
        lgroup1.rename('lgroup2')
        lgroup2 = self.h5file.root.lgroup2
        if common.verbose:
            print("Moved link:", lgroup2)
        self.assertTrue('/lgroup1' not in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self._checkEqualityGroup(self.h5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test06a_relative_path(self):
        """Using soft links with relative paths."""

        self._createFile()
        # Create new group
        self.h5file.create_group('/group1', 'group3')
        # ... and relative link
        lgroup3 = self.h5file.create_soft_link(
            '/group1', 'lgroup3', 'group3')
        if common.verbose:
            print("Relative path link:", lgroup3)
        self.assertTrue('/group1/lgroup3' in self.h5file)
        self._checkEqualityGroup(self.h5file.root.group1.group3,
                                 self.h5file.root.group1.lgroup3())

    def test06b_relative_path(self):
        """Using soft links with relative paths (./ version)"""

        self._createFile()
        # Create new group
        self.h5file.create_group('/group1', 'group3')
        # ... and relative link
        lgroup3 = self.h5file.create_soft_link(
            '/group1', 'lgroup3', './group3')
        if common.verbose:
            print("Relative path link:", lgroup3)
        self.assertTrue('/group1/lgroup3' in self.h5file)
        self._checkEqualityGroup(self.h5file.root.group1.group3,
                                 self.h5file.root.group1.lgroup3())

    def test07_walkNodes(self):
        """Checking `walk_nodes` with `classname` option."""

        self._createFile()
        links = [node._v_pathname for node in
                 self.h5file.walk_nodes('/', classname="Link")]
        if common.verbose:
            print("detected links (classname='Link'):", links)
        self.assertEqual(links, ['/larr2', '/lgroup1', '/group1/larr1'])
        links = [node._v_pathname for node in
                 self.h5file.walk_nodes('/', classname="SoftLink")]
        if common.verbose:
            print("detected links (classname='SoftLink'):", links)
        self.assertEqual(links, ['/larr2', '/lgroup1', '/group1/larr1'])

    def test08__v_links(self):
        """Checking `Group._v_links`."""

        self._createFile()
        links = [node for node in self.h5file.root._v_links]
        if common.verbose:
            print("detected links (under root):", links)
        self.assertEqual(len(links), 2)
        links = [node for node in self.h5file.root.group1._v_links]
        if common.verbose:
            print("detected links (under /group1):", links)
        self.assertEqual(links, ['larr1'])

    def test09_link_to_link(self):
        """Checking linked links."""
        self._createFile()
        # Create a link to another existing link
        lgroup2 = self.h5file.create_soft_link(
            '/', 'lgroup2', '/lgroup1')
        # Dereference it once:
        self.assertTrue(lgroup2() is self.h5file.get_node('/lgroup1'))
        if common.verbose:
            print("First dereference is correct:", lgroup2())
        # Dereference it twice:
        self.assertTrue(lgroup2()() is self.h5file.get_node('/group1'))
        if common.verbose:
            print("Second dereference is correct:", lgroup2()())

    def test10_copy_link_to_file(self):
        """Checking copying a link to another file."""
        self._createFile()
        fname = tempfile.mktemp(".h5")
        h5f = tables.open_file(fname, "a")
        h5f.create_array('/', 'arr1', [1, 2])
        h5f.create_group('/', 'group1')
        lgroup1 = self.h5file.root.lgroup1
        lgroup1_ = lgroup1.copy(h5f.root, 'lgroup1')
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup1' in h5f)
        self.assertTrue(lgroup1_ in h5f)
        if common.verbose:
            print("Copied link:", lgroup1_, 'in:', lgroup1_._v_file.filename)
        h5f.close()
        os.remove(fname)


# Test for external links
class ExternalLinkTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def tearDown(self):
        """Remove ``extfname``."""
        self.exth5file.close()
        super(ExternalLinkTestCase, self).tearDown()

        #open_files = tables.file._open_files
        #if self.extfname in open_files:
        #    #assert False
        #    for handler in open_files.get_handlers_by_name(self.extfname):
        #        handler.close()

        os.remove(self.extfname)   # comment this for debugging purposes only

    def _createFile(self):
        self.h5file.create_array('/', 'arr1', [1, 2])
        group1 = self.h5file.create_group('/', 'group1')
        self.h5file.create_array(group1, 'arr2', [1, 2, 3])
        # The external file
        self.extfname = tempfile.mktemp(".h5")
        self.exth5file = tables.open_file(self.extfname, "w")
        extarr1 = self.exth5file.create_array('/', 'arr1', [1, 2])
        self.assertTrue(extarr1 is not None)
        extgroup1 = self.exth5file.create_group('/', 'group1')
        extarr2 = self.exth5file.create_array(extgroup1, 'arr2', [1, 2, 3])
        # Create external links
        lgroup1 = self.h5file.create_external_link(
            '/', 'lgroup1', '%s:/group1' % self.extfname)
        self.assertTrue(lgroup1 is not None)
        larr1 = self.h5file.create_external_link(
            group1, 'larr1', '%s:/arr1' % self.extfname)
        self.assertTrue(larr1 is not None)
        larr2 = self.h5file.create_external_link('/', 'larr2', extarr2)
        self.assertTrue(larr2 is not None)
        # Re-open the external file in 'r'ead-only mode
        self.exth5file.close()
        self.exth5file = tables.open_file(self.extfname, "r")

    def test00_create(self):
        """Creating soft links."""
        self._createFile()
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.lgroup1())
        self._checkEqualityLeaf(self.exth5file.root.arr1,
                                self.h5file.root.group1.larr1())
        self._checkEqualityLeaf(self.h5file.root.lgroup1().arr2,
                                self.h5file.root.larr2())

    def test01_open(self):
        """Opening a file with soft links."""

        self._createFile()
        self._reopen()
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.lgroup1())
        self._checkEqualityLeaf(self.exth5file.root.arr1,
                                self.h5file.root.group1.larr1())
        self._checkEqualityLeaf(self.h5file.root.lgroup1().arr2,
                                self.h5file.root.larr2())

    def test02_remove(self):
        """Removing an external link."""

        self._createFile()
        # Re-open the external file in 'a'ppend mode
        self.exth5file.close()
        self.exth5file = tables.open_file(self.extfname, "a")
        # First delete the referred link
        self.exth5file.root.arr1.remove()
        self.assertTrue('/arr1' not in self.exth5file)
        # The external link should still be there (but dangling)
        if common.verbose:
            print("Dangling link:", self.h5file.root.group1.larr1)
        self.assertTrue('/group1/larr1' in self.h5file)
        # Remove the external link itself
        self.h5file.root.group1.larr1.remove()
        self.assertTrue('/group1/larr1' not in self.h5file)

    def test03_copy(self):
        """Copying an external link."""

        self._createFile()
        # Copy the link into another location
        root = self.h5file.root
        lgroup1 = root.lgroup1
        lgroup2 = lgroup1.copy('/', 'lgroup2')
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self.assertTrue('lgroup2' in root._v_children)
        self.assertTrue('lgroup2' in root._v_links)
        if common.verbose:
            print("Copied link:", lgroup2)
        # Remove the first link
        lgroup1.remove()
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test03_overwrite(self):
        """Overwrite an external link."""

        self._createFile()
        # Copy the link into another location
        root = self.h5file.root
        lgroup1 = root.lgroup1
        lgroup2 = lgroup1.copy('/', 'lgroup2')
        lgroup2 = lgroup1.copy('/', 'lgroup2', overwrite=True)
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self.assertTrue('lgroup2' in root._v_children)
        self.assertTrue('lgroup2' in root._v_links)
        if common.verbose:
            print("Copied link:", lgroup2)
        # Remove the first link
        lgroup1.remove()
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test04_move(self):
        """Moving an external link."""

        self._createFile()
        # Move the link into another location
        lgroup1 = self.h5file.root.lgroup1
        group2 = self.h5file.create_group('/', 'group2')
        lgroup1.move(group2, 'lgroup2')
        lgroup2 = self.h5file.root.group2.lgroup2
        if common.verbose:
            print("Moved link:", lgroup2)
        self.assertTrue('/lgroup1' not in self.h5file)
        self.assertTrue('/group2/lgroup2' in self.h5file)
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.group2.lgroup2())

    def test05_rename(self):
        """Renaming an external link."""

        self._createFile()
        # Rename the link
        lgroup1 = self.h5file.root.lgroup1
        lgroup1.rename('lgroup2')
        lgroup2 = self.h5file.root.lgroup2
        if common.verbose:
            print("Moved link:", lgroup2)
        self.assertTrue('/lgroup1' not in self.h5file)
        self.assertTrue('/lgroup2' in self.h5file)
        self._checkEqualityGroup(self.exth5file.root.group1,
                                 self.h5file.root.lgroup2())

    def test07_walkNodes(self):
        """Checking `walk_nodes` with `classname` option."""

        self._createFile()
        # Create a new soft link
        self.h5file.create_soft_link('/group1', 'lgroup3', './group3')
        links = [node._v_pathname for node in
                 self.h5file.walk_nodes('/', classname="Link")]
        if common.verbose:
            print("detected links (classname='Link'):", links)
        self.assertEqual(links, ['/larr2', '/lgroup1',
                                 '/group1/larr1', '/group1/lgroup3'])
        links = [node._v_pathname for node in
                 self.h5file.walk_nodes('/', classname="ExternalLink")]
        if common.verbose:
            print("detected links (classname='ExternalLink'):", links)
        self.assertEqual(links, ['/larr2', '/lgroup1', '/group1/larr1'])

    def test08__v_links(self):
        """Checking `Group._v_links`."""

        self._createFile()
        links = [node for node in self.h5file.root._v_links]
        if common.verbose:
            print("detected links (under root):", links)
        self.assertEqual(len(links), 2)
        links = [node for node in self.h5file.root.group1._v_links]
        if common.verbose:
            print("detected links (under /group1):", links)
        self.assertEqual(links, ['larr1'])

    def test09_umount(self):
        """Checking `umount()` method."""
        self._createFile()
        link = self.h5file.root.lgroup1
        self.assertTrue(link.extfile is None)
        # Dereference a external node (and hence, 'mount' a file)
        enode = link()
        self.assertTrue(enode is not None)
        self.assertTrue(link.extfile is not None)
        # Umount the link
        link.umount()
        self.assertTrue(link.extfile is None)

    def test10_copy_link_to_file(self):
        """Checking copying a link to another file."""
        self._createFile()
        fname = tempfile.mktemp(".h5")
        h5f = tables.open_file(fname, "a")
        h5f.create_array('/', 'arr1', [1, 2])
        h5f.create_group('/', 'group1')
        lgroup1 = self.h5file.root.lgroup1
        lgroup1_ = lgroup1.copy(h5f.root, 'lgroup1')
        self.assertTrue('/lgroup1' in self.h5file)
        self.assertTrue('/lgroup1' in h5f)
        self.assertTrue(lgroup1_ in h5f)
        if common.verbose:
            print("Copied link:", lgroup1_, 'in:', lgroup1_._v_file.filename)
        h5f.close()
        os.remove(fname)

#----------------------------------------------------------------------


def suite():
    """Return a test suite consisting of all the test cases in the module."""

    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(HardLinkTestCase))
        theSuite.addTest(unittest.makeSuite(SoftLinkTestCase))
        if tables.file._FILE_OPEN_POLICY != 'strict':
            theSuite.addTest(unittest.makeSuite(ExternalLinkTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_lists
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile

from tables import *
from tables.tests import common

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


def WriteRead(filename, testTuple):
    if common.verbose:
        print('\n', '-=' * 30)
        print("Running test for object %s" % type(testTuple))

    # Create an instance of HDF5 Table
    fileh = open_file(filename, mode="w")
    root = fileh.root
    try:
        # Create the array under root and name 'somearray'
        a = testTuple
        fileh.create_array(root, 'somearray', a, "Some array")
    finally:
        # Close the file
        fileh.close()

    # Re-open the file in read-only mode
    fileh = open_file(filename, mode="r")
    root = fileh.root

    # Read the saved array
    try:
        b = root.somearray.read()
        # Compare them. They should be equal.
        if not a == b and common.verbose:
            print("Write and read lists/tuples differ!")
            print("Object written:", a)
            print("Object read:", b)

        # Check strictly the array equality
        assert a == b
    finally:
        # Close the file
        fileh.close()


class BasicTestCase(unittest.TestCase):
    def test00_char(self):
        "Data integrity during recovery (character types)"

        a = self.charList
        fname = tempfile.mktemp(".h5")
        try:
            WriteRead(fname, a)
        finally:
            os.remove(fname)

    def test01_types(self):
        "Data integrity during recovery (numerical types)"

        a = self.numericalList
        fname = tempfile.mktemp(".h5")
        try:
            WriteRead(fname, a)
        finally:
            os.remove(fname)


class Basic0DOneTestCase(BasicTestCase):
    # Scalar case
    title = "Rank-0 case 1"
    numericalList = 3
    charList = b"3"


class Basic0DTwoTestCase(BasicTestCase):
    # Scalar case
    title = "Rank-0 case 2"
    numericalList = 33.34
    charList = b"33"*500

# This does not work anymore because I've splitted the chunked arrays to happen
# mainly in EArray objects
# class Basic1DZeroTestCase(BasicTestCase):
#     title = "Rank-1 case 0"
#     numericalList = []
#     charList = []


class Basic1DOneTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 1"
    numericalList = [3]
    charList = [b"a"]


class Basic1DTwoTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 2"
    numericalList = [3.2, 4.2]
    charList = [b"aaa"]


class Basic2DTestCase(BasicTestCase):
    # 2D case
    title = "Rank-2 case 1"
    numericalList = [[1, 2]]*5
    charList = [[b"qq", b"zz"]]*5


class Basic10DTestCase(BasicTestCase):
    # 10D case
    title = "Rank-10 case 1"
    numericalList = [[[[[[[[[[1, 2], [3, 4]]]]]]]]]]*5
    # Dimensions greather than 6 in strings gives some warnings
    charList = [[[[[[[[[[b"a", b"b"], [b"qq", b"zz"]]]]]]]]]]*5


class ExceptionTestCase(unittest.TestCase):
    def test00_char(self):
        "Non suppported lists objects (character objects)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running test for %s" % (self.title))
        a = self.charList
        try:
            fname = tempfile.mktemp(".h5")
            try:
                WriteRead(fname, a)
            finally:
                os.remove(fname)
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next error was catched!")
                print(type, ":", value)
        else:
            self.fail("expected a ValueError")

    def test01_types(self):
        "Non supported lists object (numerical types)"

        a = self.numericalList
        try:
            fname = tempfile.mktemp(".h5")
            try:
                WriteRead(fname, a)
            finally:
                os.remove(fname)
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next was catched!")
                print(value)
        else:
            self.fail("expected an ValueError")


class Basic1DFourTestCase(ExceptionTestCase):
    title = "Rank-1 case 4 (non-regular list)"
    numericalList = [3, [4, 5.2]]
    charList = [b"aaa", [b"bbb", b"ccc"]]


class GetItemTestCase(unittest.TestCase):
    def test00_single(self):
        "Single element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0])
            print("Read first element:", arr[0])
        self.assertEqual(a[0], arr[0])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test01_single(self):
        "Single element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original first element:", a[0])
            print("Read first element:", arr[0])
        self.assertEqual(a[0], arr[0])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test02_range(self):
        "Range element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertEqual(a[1:4], arr[1:4])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test03_range(self):
        "Range element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4])
            print("Read elements:", arr[1:4])
        self.assertEqual(a[1:4], arr[1:4])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test04_range(self):
        "Range element access, strided (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertEqual(a[1:4:2], arr[1:4:2])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test05_range(self):
        "Range element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original elements:", a[1:4:2])
            print("Read elements:", arr[1:4:2])
        self.assertEqual(a[1:4:2], arr[1:4:2])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test06_negativeIndex(self):
        "Negative Index element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original last element:", a[-1])
            print("Read last element:", arr[-1])
        self.assertEqual(a[-1], arr[-1])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test07_negativeIndex(self):
        "Negative Index element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original before last element:", a[-2])
            print("Read before last element:", arr[-2])
        self.assertEqual(a[-2], arr[-2])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test08_negativeRange(self):
        "Negative range element access (character types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertEqual(a[-4:-1], arr[-4:-1])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test09_negativeRange(self):
        "Negative range element access (numerical types)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if common.verbose:
            print("Original last elements:", a[-4:-1])
            print("Read last elements:", arr[-4:-1])
        self.assertEqual(a[-4:-1], arr[-4:-1])

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)


class GI1ListTestCase(GetItemTestCase):
    title = "Rank-1 case 1 (lists)"
    numericalList = [3]
    numericalListME = [3, 2, 1, 0, 4, 5, 6]
    charList = [b"3"]
    charListME = [b"321", b"221", b"121", b"021", b"421", b"521", b"621"]


class GI2ListTestCase(GetItemTestCase):
    # A more complex example
    title = "Rank-1,2 case 2 (lists)"
    numericalList = [3, 4]
    numericalListME = [[3, 2, 1, 0, 4, 5, 6],
                       [2, 1, 0, 4, 5, 6, 7],
                       [4, 3, 2, 1, 0, 4, 5],
                       [3, 2, 1, 0, 4, 5, 6],
                       [3, 2, 1, 0, 4, 5, 6]]

    charList = [b"a", b"b"]
    charListME = [
        [b"321", b"221", b"121", b"021", b"421", b"521", b"621"],
        [b"21", b"21", b"11", b"02", b"42", b"21", b"61"],
        [b"31", b"21", b"12", b"21", b"41", b"51", b"621"],
        [b"321", b"221", b"121", b"021", b"421", b"521", b"621"],
        [b"3241", b"2321", b"13216", b"0621", b"4421", b"5421", b"a621"],
        [b"a321", b"s221", b"d121", b"g021", b"b421", b"5vvv21", b"6zxzxs21"],
    ]


class GeneratorTestCase(unittest.TestCase):
    def test00a_single(self):
        "Testing generator access to Arrays, single elements (char)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test00b_me(self):
        "Testing generator access to Arrays, multiple elements (char)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.charListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if isinstance(a[0], tuple):
            ga = [list(i) for i in a]
        else:
            ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test01a_single(self):
        "Testing generator access to Arrays, single elements (numeric)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalList
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test01b_me(self):
        "Testing generator access to Arrays, multiple elements (numeric)"

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Create the array under root and name 'somearray'
        a = self.numericalListME
        arr = fileh.create_array(fileh.root, 'somearray', a, "Some array")

        # Get and compare an element
        if isinstance(a[0], tuple):
            ga = [list(i) for i in a]
        else:
            ga = [i for i in a]
        garr = [i for i in arr]
        if common.verbose:
            print("Result of original iterator:", ga)
            print("Result of read generator:", garr)
        self.assertEqual(ga, garr)

        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)


class GE1ListTestCase(GeneratorTestCase):
    # Scalar case
    title = "Rank-1 case 1 (lists)"
    numericalList = [3]
    numericalListME = [3, 2, 1, 0, 4, 5, 6]
    charList = [b"3"]
    charListME = [b"321", b"221", b"121", b"021", b"421", b"521", b"621"]


class GE2ListTestCase(GeneratorTestCase):
    # Scalar case
    title = "Rank-1,2 case 2 (lists)"
    numericalList = [3, 4]
    numericalListME = [[3, 2, 1, 0, 4, 5, 6],
                       [2, 1, 0, 4, 5, 6, 7],
                       [4, 3, 2, 1, 0, 4, 5],
                       [3, 2, 1, 0, 4, 5, 6],
                       [3, 2, 1, 0, 4, 5, 6]]

    charList = [b"a", b"b"]
    charListME = [
        [b"321", b"221", b"121", b"021", b"421", b"521", b"621"],
        [b"21", b"21", b"11", b"02", b"42", b"21", b"61"],
        [b"31", b"21", b"12", b"21", b"41", b"51", b"621"],
        [b"321", b"221", b"121", b"021", b"421", b"521", b"621"],
        [b"3241", b"2321", b"13216", b"0621", b"4421", b"5421", b"a621"],
        [b"a321", b"s221", b"d121", b"g021", b"b421", b"5vvv21", b"6zxzxs21"],
    ]


def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(Basic0DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic0DTwoTestCase))
        # theSuite.addTest(unittest.makeSuite(Basic1DZeroTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DTwoTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DFourTestCase))
        theSuite.addTest(unittest.makeSuite(Basic2DTestCase))
        theSuite.addTest(unittest.makeSuite(Basic10DTestCase))
        theSuite.addTest(unittest.makeSuite(GI1ListTestCase))
        theSuite.addTest(unittest.makeSuite(GI2ListTestCase))
        theSuite.addTest(unittest.makeSuite(GE1ListTestCase))
        theSuite.addTest(unittest.makeSuite(GE2ListTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_nestedtypes
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2005-05-18
# Author: Francesc Alted - faltet@pytables.org
# Author: Ivan Vilata - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Test module for nested types under PyTables."""

from __future__ import print_function
import sys
import unittest
import itertools

import numpy

import tables as t
from tables.utils import SizeType
from tables.tests import common
from tables.description import Description

minRowIndex = 10


# This is the structure of the table used for testing (DON'T PANIC!):
#
# +-+---------------------------------+-----+----------+-+-+
# |x|Info                             |color|info      |y|z|
# | +-----+--+----------------+----+--+     +----+-----+ | |
# | |value|y2|Info2           |name|z2|     |Name|Value| | |
# | |     |  +----+-----+--+--+    |  |     |    |     | | |
# | |     |  |name|value|y3|z3|    |  |     |    |     | | |
# +-+-----+--+----+-----+--+--+----+--+-----+----+-----+-+-+
#
# Please note that some fields are explicitly ordered while others are
# ordered alphabetically by name.
# The declaration of the nested table:
class Info(t.IsDescription):
    _v_pos = 3
    Name = t.StringCol(itemsize=2)
    Value = t.ComplexCol(itemsize=16)


class TestTDescr(t.IsDescription):

    """A description that has several nested columns."""

    x = t.Int32Col(dflt=0, shape=2, pos=0)  # 0
    y = t.Float64Col(dflt=1, shape=(2, 2))
    z = t.UInt8Col(dflt=1)
    color = t.StringCol(itemsize=2, dflt=b" ", pos=2)
    info = Info()

    class Info(t.IsDescription):  # 1
        _v_pos = 1
        name = t.StringCol(itemsize=2)
        value = t.ComplexCol(itemsize=16, pos=0)  # 0
        y2 = t.Float64Col(dflt=1, pos=1)  # 1
        z2 = t.UInt8Col(dflt=1)

        class Info2(t.IsDescription):
            y3 = t.Time64Col(dflt=1, shape=2)
            z3 = t.EnumCol({'r': 4, 'g': 2, 'b': 1}, 'r', 'int32', shape=2)
            name = t.StringCol(itemsize=2)
            value = t.ComplexCol(itemsize=16, shape=2)

# The corresponding nested array description:
testADescr = [
    ('x', '(2,)Int32'),
    ('Info', [
        ('value', 'Complex64'),
        ('y2', 'Float64'),
        ('Info2', [
            ('name', 'a2'),
            ('value', '(2,)Complex64'),
            ('y3', '(2,)Float64'),
            ('z3', '(2,)Int32')]),
        ('name', 'a2'),
        ('z2', 'UInt8')]),
    ('color', 'a2'),
    ('info', [
        ('Name', 'a2'),
        ('Value', 'Complex64')]),
    ('y', '(2,2)Float64'),
    ('z', 'UInt8')]

# The corresponding nested array description (brief version):
testADescr2 = [
    ('x', '(2,)i4'),
    ('Info', [
        ('value', '()c16'),
        ('y2', '()f8'),
        ('Info2', [
            ('name', '()S2'),
            ('value', '(2,)c16'),
            ('y3', '(2,)f8'),
            ('z3', '(2,)i4')]),
        ('name', '()S2'),
        ('z2', '()u1')]),
    ('color', '()S2'),
    ('info', [
        ('Name', '()S2'),
        ('Value', '()c16')]),
    ('y', '(2, 2)f8'),
    ('z', '()u1')]

# A nested array for testing:
testABuffer = [
    # x     Info    color info      y       z
    #       value y2 Info2      name z2         Name Value
    #                name   value    y3       z3
    ((3, 2), (6j, 6., ('nn', (6j, 4j), (6., 4.), (1, 2)),
     'NN', 8), 'cc', ('NN', 6j), ((6., 4.), (6., 4.)), 8),
    ((4, 3), (7j, 7., ('oo', (7j, 5j), (7., 5.), (2, 1)),
     'OO', 9), 'dd', ('OO', 7j), ((7., 5.), (7., 5.)), 9),
]
testAData = numpy.array(testABuffer, dtype=testADescr)
# The name of the column to be searched:
testCondCol = 'Info/z2'
# The name of a nested column (it can not be searched):
testNestedCol = 'Info'
# The condition to be applied on the column (all but the last row match it):
testCondition = '(2 < col) & (col < 9)'


def areDescriptionsEqual(desc1, desc2):
    """Are both `desc1` and `desc2` equivalent descriptions?

    The arguments may be description objects (``IsDescription``,
    ``Description``) or dictionaries.

    """

    if isinstance(desc1, t.Col):
        # This is a rough comparison but it suffices here.
        return (desc1.type == desc2.type
                and desc2.dtype == desc2.dtype
                and desc1._v_pos == desc2._v_pos
                # and desc1.dflt == desc2.dflt)
                and common.areArraysEqual(desc1.dflt, desc2.dflt))

    if hasattr(desc1, '_v_colobjects'):  # quacks like a Description
        cols1 = desc1._v_colobjects
    elif hasattr(desc1, 'columns'):  # quacks like an IsDescription
        cols1 = desc1.columns
    else:  # hope it quacks like a dictionary
        cols1 = desc1

    if hasattr(desc2, '_v_colobjects'):  # quacks like a Description
        cols2 = desc2._v_colobjects
    elif hasattr(desc2, 'columns'):  # quacks like an IsDescription
        cols2 = desc2.columns
    else:  # hope it quacks like a dictionary
        cols2 = desc2

    if len(cols1) != len(cols2):
        return False

    for (colName, colobj1) in cols1.iteritems():
        colobj2 = cols2[colName]
        if colName == '_v_pos':
            # The comparison may not be quite exhaustive!
            return colobj1 == colobj2
        if not areDescriptionsEqual(colobj1, colobj2):
            return False

    return True


# Test creating nested column descriptions
class DescriptionTestCase(common.PyTablesTestCase):

    _TestTDescr = TestTDescr
    _testADescr = testADescr
    _testADescr2 = testADescr2
    _testAData = testAData

    def test00_instance(self):
        """Creating an instance of a nested description."""

        self.assertTrue(
            areDescriptionsEqual(self._TestTDescr, self._TestTDescr()),
            "Table description does not match the given one.")

    def test01_instance(self):
        """Checking attrs of an instance of a nested description."""

        descr = Description(self._TestTDescr().columns)
        if common.verbose:
            print("Generated description:", descr._v_nested_descr)
            print("Should look like:", self._testADescr2)
        self.assertEqual(self._testADescr2, descr._v_nested_descr,
                         "Description._v_nested_descr does not match.")


# Test creating a nested table and opening it
class CreateTestCase(common.TempFileMixin, common.PyTablesTestCase):

    _TestTDescr = TestTDescr
    _testABuffer = testABuffer
    _testAData = testAData

    def _checkColumns(self, cols, desc):
        """Check that `cols` has all the accessors for `self._TestTDescr`."""

        # ``_desc`` is a leaf column and ``cols`` a ``Column``.
        if isinstance(desc, t.Col):
            return isinstance(cols, t.Column)

        # ``_desc`` is a description object and ``cols`` a ``Cols``.
        descColumns = desc._v_colobjects
        for colName in descColumns:
            if colName not in cols._v_colnames:
                return False
            if not self._checkColumns(cols._f_col(colName),
                                      descColumns[colName]):
                return False

        return True

    def _checkDescription(self, table):
        """Check that description of `table` matches `self._TestTDescr`."""

        # Compare descriptions.
        self.assertTrue(
            areDescriptionsEqual(self._TestTDescr, table.description),
            "Table description does not match the given one.")
        # Check access to columns.
        self._checkColumns(table.cols, table.description)

    def _checkColinstances(self, table):
        """Check that ``colinstances`` and ``cols`` of `table` match."""
        for colpathname in table.description._v_pathnames:
            self.assertTrue(table.colinstances[colpathname]
                            is table.cols._f_col(colpathname))

    def test00_create(self):
        """Creating a nested table."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        self._checkDescription(tbl)
        self._checkColinstances(tbl)

    def test01_open(self):
        """Opening a nested table."""

        self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        self._reopen()
        tbl = self.h5file.root.test
        self._checkDescription(tbl)
        self._checkColinstances(tbl)

    def test02_NestedRecArrayCompat(self):
        """Creating a compatible nested record array``."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        self.assertTrue(common.areArraysEqual(nrarr, self._testAData),
                        "Can not create a compatible structured array.")

    def test03_NRA(self):
        """Creating a table from a nested record array object."""

        tbl = self.h5file.create_table(
            '/', 'test', self._testAData, title=self._getMethodName())
        tbl.flush()
        readAData = tbl.read()
        if common.verbose:
            print("Read data:", readAData)
            print("Should look like:", self._testAData)
        self.assertTrue(common.areArraysEqual(self._testAData, readAData),
                        "Written and read values differ.")

    def test04_NRA2(self):
        """Creating a table from a generated nested record array object."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        readAData = tbl.read()

        tbl2 = self.h5file.create_table(
            '/', 'test2', readAData, title=self._getMethodName())
        readAData2 = tbl2.read()

        self.assertTrue(common.areArraysEqual(self._testAData, readAData2),
                        "Written and read values differ.")


# Test writing data in a nested table
class WriteTestCase(common.TempFileMixin, common.PyTablesTestCase):

    _TestTDescr = TestTDescr
    _testAData = testAData
    _testCondition = testCondition
    _testCondCol = testCondCol
    _testNestedCol = testNestedCol

    def _testCondVars(self, table):
        """Get condition variables for the given `table`."""
        return {'col': table.cols._f_col(self._testCondCol)}

    def _testNestedCondVars(self, table):
        """Get condition variables for the given `table`."""
        return {'col': table.cols._f_col(self._testNestedCol)}

    def _appendRow(self, row, index):
        """
        Append the `index`-th row in `self._testAData` to `row`.

        Values are set field-by-field (be it nested or not).
        """

        record = self._testAData[index]
        for fieldName in self._testAData.dtype.names:
            row[fieldName] = record[fieldName]
        row.append()

    def test00_append(self):
        """Appending a set of rows."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        readAData = tbl.read()
        self.assertTrue(common.areArraysEqual(self._testAData, readAData),
                        "Written and read values differ.")

    def test01_row(self):
        """Appending individual rows."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        row = tbl.row
        # Add the first row
        self._appendRow(row, 0)
        # Add the rest of the rows field by field.
        for i in range(1, len(self._testAData)):
            self._appendRow(row, i)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        readAData = tbl.read()
        self.assertTrue(common.areArraysEqual(self._testAData, readAData),
                        "Written and read values differ.")

    def test02_where(self):
        """Searching nested data."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        searchedCoords = tbl.get_where_list(
            self._testCondition, self._testCondVars(tbl))

        # All but the last row match the condition.
        searchedCoords.sort()
        self.assertEqual(searchedCoords.tolist(),
                         range(len(self._testAData) - 1),
                         "Search returned incorrect results.")

    def test02b_whereAppend(self):
        """Searching nested data and appending it to another table."""

        tbl1 = self.h5file.create_table(
            '/', 'test1', self._TestTDescr, title=self._getMethodName())
        tbl1.append(self._testAData)
        tbl1.flush()

        tbl2 = self.h5file.create_table(
            '/', 'test2', self._TestTDescr, title=self._getMethodName())
        tbl1.append_where(
            tbl2, self._testCondition, self._testCondVars(tbl1))

        if self.reopen:
            self._reopen()
            tbl1 = self.h5file.root.test1
            tbl2 = self.h5file.root.test2

        searchedCoords = tbl2.get_where_list(
            self._testCondition, self._testCondVars(tbl2))

        # All but the last row match the condition.
        searchedCoords.sort()
        self.assertEqual(searchedCoords.tolist(),
                         range(len(self._testAData) - 1),
                         "Search returned incorrect results.")

    def test03_colscond(self):
        """Searching on a column with nested columns."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        self.assertRaises(
            TypeError, tbl.get_where_list,
            self._testCondition, self._testNestedCondVars(tbl))

    def test04_modifyColumn(self):
        """Modifying one single nested column (modify_column)."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        nColumn = self._testNestedCol
        # Get the nested column data and swap the first and last rows.
        raTable = self._testAData.copy()
        raColumn = raTable[nColumn]
        # The next will not work until NestedRecords supports copies
        (raColumn[0], raColumn[-1]) = (raColumn[-1], raColumn[0])

        # Write the resulting column and re-read the whole table.
        tbl.modify_column(colname=nColumn, column=raColumn)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        raReadTable = tbl.read()
        if common.verbose:
            print("Table read:", raReadTable)
            print("Should look like:", raTable)

        # Compare it to the written one.
        self.assertTrue(common.areArraysEqual(raTable, raReadTable),
                        "Written and read values differ.")

    def test05a_modifyColumns(self):
        """Modifying one nested column (modify_columns)."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        nColumn = self._testNestedCol
        # Get the nested column data and swap the first and last rows.
        raTable = self._testAData.copy()
        raColumn = raTable[nColumn]
        (raColumn[0], raColumn[-1]) = (raColumn[-1].copy(), raColumn[0].copy())
        newdtype = numpy.dtype([(nColumn, raTable.dtype.fields[nColumn][0])])
        self.assertTrue(newdtype is not None)

        # Write the resulting column and re-read the whole table.
        tbl.modify_columns(names=[nColumn], columns=raColumn)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        raReadTable = tbl.read()
        if common.verbose:
            print("Table read:", raReadTable)
            print("Should look like:", raTable)

        # Compare it to the written one.
        self.assertTrue(common.areArraysEqual(raTable, raReadTable),
                        "Written and read values differ.")

    def test05b_modifyColumns(self):
        """Modifying two nested columns (modify_columns)."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        # Get the nested column data and swap the first and last rows.
        colnames = ['x', 'color']  # Get the first two columns
        raCols = numpy.rec.fromarrays([
            self._testAData['x'].copy(),
            self._testAData['color'].copy()],
            dtype=[('x', '(2,)i4'), ('color', '1a2')])
            # descr=tbl.description._v_nested_descr[0:2])
            # or...
            # names=tbl.description._v_nested_names[0:2],
            # formats=tbl.description._v_nested_formats[0:2])
        (raCols[0], raCols[-1]) = (raCols[-1].copy(), raCols[0].copy())

        # Write the resulting columns
        tbl.modify_columns(names=colnames, columns=raCols)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        # Re-read the appropriate columns
        raCols2 = numpy.rec.fromarrays([tbl.cols._f_col('x'),
                                        tbl.cols._f_col('color')],
                                       dtype=raCols.dtype)
        if common.verbose:
            print("Table read:", raCols2)
            print("Should look like:", raCols)

        # Compare it to the written one.
        self.assertTrue(common.areArraysEqual(raCols, raCols2),
                        "Written and read values differ.")

    def test06_modifyRows(self):
        "Checking modifying several rows at once (using nested rec array)"

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        # Get the nested record and swap the first and last rows.
        raTable = self._testAData.copy()
        (raTable[0], raTable[-1]) = (raTable[-1].copy(), raTable[0].copy())

        # Write the resulting nested record and re-read the whole table.
        tbl.modify_rows(start=0, stop=2, rows=raTable)
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        raReadTable = tbl.read()
        if common.verbose:
            print("Table read:", raReadTable)
            print("Should look like:", raTable)

        # Compare it to the written one.
        self.assertTrue(common.areArraysEqual(raTable, raReadTable),
                        "Written and read values differ.")

    def test07_index(self):
        """Checking indexes of nested columns."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName(),
            expectedrows=minRowIndex * 2)
        for i in range(minRowIndex):
            tbl.append(self._testAData)
        tbl.flush()
        coltoindex = tbl.cols._f_col(self._testCondCol)
        indexrows = coltoindex.create_index()
        self.assertTrue(indexrows is not None)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test
            coltoindex = tbl.cols._f_col(self._testCondCol)

        if common.verbose:
            print("Number of written rows:", tbl.nrows)
            print("Number of indexed rows:", coltoindex.index.nelements)

        # Check indexing flags:
        self.assertEqual(tbl.indexed, True, "Table not indexed")
        self.assertNotEqual(coltoindex.index, None, "Column not indexed")
        self.assertTrue(tbl.colindexed[
                        self._testCondCol], "Column not indexed")
        # Do a look-up for values
        searchedCoords = tbl.get_where_list(
            self._testCondition, self._testCondVars(tbl))
        searchedCoords.sort()

        expectedCoords = numpy.arange(0, minRowIndex * 2, 2, SizeType)
        if common.verbose:
            print("Searched coords:", searchedCoords)
            print("Expected coords:", expectedCoords)
        # All even rows match the condition.
        self.assertEqual(searchedCoords.tolist(), expectedCoords.tolist(),
                         "Search returned incorrect results.")

    def test08_setNestedField(self):
        "Checking modifying a nested field via natural naming."
        # See ticket #93 (http://www.pytables.org/trac/ticket/93).

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)
        tbl.flush()

        oldvalue = tbl.cols.Info.z2[0]
        tbl.cols.Info.z2[0] = oldvalue + 1
        tbl.flush()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        newvalue = tbl.cols.Info.z2[0]
        self.assertEqual(newvalue, oldvalue + 1)


class WriteNoReopen(WriteTestCase):
    reopen = 0


class WriteReopen(WriteTestCase):
    reopen = 1


class ReadTestCase(common.TempFileMixin, common.PyTablesTestCase):

    _TestTDescr = TestTDescr
    _testABuffer = testABuffer
    _testAData = testAData
    _testNestedCol = testNestedCol

    def test00a_repr(self):
        """Checking representation of a nested Table."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title="test00")
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        if common.verbose:
            print("str(tbl)-->", str(tbl))
            print("repr(tbl)-->", repr(tbl))

        self.assertEqual(str(tbl), "/test (Table(2,)) 'test00'")
        tblrepr = repr(tbl)
        # Remove the platform-dependent information (i.e. byteorder)
        tblrepr = "\n".join(tblrepr.split("\n")[:-2]) + "\n"
        if sys.version_info[0] < 3:
            template = """/test (Table(2,)) 'test00'
  description := {
  "x": Int32Col(shape=(2,), dflt=0, pos=0),
  "Info": {
    "value": ComplexCol(itemsize=16, shape=(), dflt=0j, pos=0),
    "y2": Float64Col(shape=(), dflt=1.0, pos=1),
    "Info2": {
      "name": StringCol(itemsize=2, shape=(), dflt='', pos=0),
      "value": ComplexCol(itemsize=16, shape=(2,), dflt=0j, pos=1),
      "y3": Time64Col(shape=(2,), dflt=1.0, pos=2),
      "z3": EnumCol(enum=Enum({%(value)s}), dflt='r', base=Int32Atom(shape=(), dflt=0), shape=(2,), pos=3)},
    "name": StringCol(itemsize=2, shape=(), dflt='', pos=3),
    "z2": UInt8Col(shape=(), dflt=1, pos=4)},
  "color": StringCol(itemsize=2, shape=(), dflt=' ', pos=2),
  "info": {
    "Name": StringCol(itemsize=2, shape=(), dflt='', pos=0),
    "Value": ComplexCol(itemsize=16, shape=(), dflt=0j, pos=1)},
  "y": Float64Col(shape=(2, 2), dflt=1.0, pos=4),
  "z": UInt8Col(shape=(), dflt=1, pos=5)}
"""
        else:
            template = """/test (Table(2,)) 'test00'
  description := {
  "x": Int32Col(shape=(2,), dflt=0, pos=0),
  "Info": {
    "value": ComplexCol(itemsize=16, shape=(), dflt=0j, pos=0),
    "y2": Float64Col(shape=(), dflt=1.0, pos=1),
    "Info2": {
      "name": StringCol(itemsize=2, shape=(), dflt=b'', pos=0),
      "value": ComplexCol(itemsize=16, shape=(2,), dflt=0j, pos=1),
      "y3": Time64Col(shape=(2,), dflt=1.0, pos=2),
      "z3": EnumCol(enum=Enum({%(value)s}), dflt='%(default)s', base=Int32Atom(shape=(), dflt=0), shape=(2,), pos=3)},
    "name": StringCol(itemsize=2, shape=(), dflt=b'', pos=3),
    "z2": UInt8Col(shape=(), dflt=1, pos=4)},
  "color": StringCol(itemsize=2, shape=(), dflt=b' ', pos=2),
  "info": {
    "Name": StringCol(itemsize=2, shape=(), dflt=b'', pos=0),
    "Value": ComplexCol(itemsize=16, shape=(), dflt=0j, pos=1)},
  "y": Float64Col(shape=(2, 2), dflt=1.0, pos=4),
  "z": UInt8Col(shape=(), dflt=1, pos=5)}
"""

        # The problem here is that the order in which items are stored in a
        # dict can't be assumed to be stable.
        # From python 3.3 on it is actually no more stable since the
        # "Hash randomization" feature is enable by default.
        #
        # For this reason we generate a representation string for each of the
        # prmutations of the Enum items.
        #
        # Also the default value of enum types is not preserved in HDF5.
        # It is assumed that the default value is the first one in the array
        # of Enum names and hence it is also affected by the issue related to
        # the "Hash randomization" feature.
        #
        # Also in this case it is genereted a representation string for each
        # of the possible default values.
        enums = [
            ', '.join(items) for items in itertools.permutations(
                ("'r': 4", "'b': 1", "'g': 2"))
        ]
        defaults = ('r', 'b', 'g')
        values = [
            template % {'value': v, 'default': d}
            for v, d in itertools.product(enums, defaults)
        ]
        self.assertTrue(tblrepr in values)

    def test00b_repr(self):
        """Checking representation of a root Column."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title="test00")
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        if common.verbose:
            print("str(tbl.cols.y)-->'%s'" % str(tbl.cols.y))
            print("repr(tbl.cols.y)-->'%s'" % repr(tbl.cols.y))

        self.assertEqual(str(tbl.cols.y),
                         "/test.cols.y (Column(2, 2, 2), float64, idx=None)")
        self.assertEqual(repr(tbl.cols.y),
                         "/test.cols.y (Column(2, 2, 2), float64, idx=None)")

    def test00c_repr(self):
        """Checking representation of a nested Column."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title="test00")
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        if common.verbose:
            print("str(tbl.cols.Info.z2)-->'%s'" % str(tbl.cols.Info.z2))
            print("repr(tbl.cols.Info.z2)-->'%s'" % repr(tbl.cols.Info.z2))

        self.assertEqual(str(tbl.cols.Info.z2),
                         "/test.cols.Info.z2 (Column(2,), uint8, idx=None)")
        self.assertEqual(repr(tbl.cols.Info.z2),
                         "/test.cols.Info.z2 (Column(2,), uint8, idx=None)")

    def test01_read(self):
        """Checking Table.read with subgroups with a range index with step."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.rec.array(testABuffer,
                                dtype=tbl.description._v_nested_descr)
        tblcols = tbl.read(start=0, step=2, field='Info')
        nrarrcols = nrarr['Info'][0::2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test01_read_out_arg(self):
        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.rec.array(testABuffer,
                                dtype=tbl.description._v_nested_descr)
        # When reading an entire nested column, the output array must contain
        # all fields in the table.  The output buffer will contain the contents
        # of all fields.  The selected column alone will be returned from the
        # method call.
        all_cols = numpy.empty(1, tbl.dtype)
        tblcols = tbl.read(start=0, step=2, field='Info', out=all_cols)
        nrarrcols = nrarr['Info'][0::2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")
        self.assertTrue(common.areArraysEqual(nrarr[0::2], all_cols),
                        "Output buffer does not match full table.")

    def test02_read(self):
        """Checking Table.read with a nested Column."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        tblcols = tbl.read(start=0, step=2, field='Info/value')
        nrarr = numpy.rec.array(testABuffer,
                                dtype=tbl.description._v_nested_descr)
        nrarrcols = nrarr['Info']['value'][0::2]
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test02_read_out_arg(self):
        """Checking Table.read with a nested Column."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        tblcols = numpy.empty(1, dtype='c16')
        tbl.read(start=0, step=2, field='Info/value', out=tblcols)
        nrarr = numpy.rec.array(testABuffer,
                                dtype=tbl.description._v_nested_descr)
        nrarrcols = nrarr['Info']['value'][0::2]
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")


class ReadNoReopen(ReadTestCase):
    reopen = 0


class ReadReopen(ReadTestCase):
    reopen = 1


# Checking the Table.Cols accessor
class ColsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    _TestTDescr = TestTDescr
    _testABuffer = testABuffer
    _testAData = testAData
    _testNestedCol = testNestedCol

    def test00a_repr(self):
        """Checking string representation of Cols."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title="test00")

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        if common.verbose:
            print("str(tbl.cols)-->", str(tbl.cols))
            print("repr(tbl.cols)-->", repr(tbl.cols))

        self.assertEqual(str(tbl.cols), "/test.cols (Cols), 6 columns")
        try:
            self.assertEqual(repr(tbl.cols),
                             """/test.cols (Cols), 6 columns
  x (Column(0, 2), ('int32',(2,)))
  Info (Cols(), Description)
  color (Column(0,), |S2)
  info (Cols(), Description)
  y (Column(0, 2, 2), ('float64',(2, 2)))
  z (Column(0,), uint8)
"""
                             )
        except AssertionError:
            self.assertEqual(repr(tbl.cols),
                             """/test.cols (Cols), 6 columns
  x (Column(0, 2), ('%s', (2,)))
  Info (Cols(), Description)
  color (Column(0,), |S2)
  info (Cols(), Description)
  y (Column(0, 2, 2), ('%s', (2, 2)))
  z (Column(0,), uint8)
""" % (numpy.int32(0).dtype.str, numpy.float64(0).dtype.str))

    def test00b_repr(self):
        """Checking string representation of nested Cols."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        if common.verbose:
            print("str(tbl.cols.Info)-->", str(tbl.cols.Info))
            print("repr(tbl.cols.Info)-->", repr(tbl.cols.Info))

        self.assertEqual(str(
            tbl.cols.Info), "/test.cols.Info (Cols), 5 columns")
        self.assertEqual(repr(tbl.cols.Info),
                         """/test.cols.Info (Cols), 5 columns
  value (Column(0,), complex128)
  y2 (Column(0,), float64)
  Info2 (Cols(), Description)
  name (Column(0,), |S2)
  z2 (Column(0,), uint8)
""")

    def test01a_f_col(self):
        """Checking cols._f_col() with a subgroup."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        tblcol = tbl.cols._f_col(self._testNestedCol)
        if common.verbose:
            print("Column group name:", tblcol._v_desc._v_pathname)
        self.assertEqual(tblcol._v_desc._v_pathname, self._testNestedCol,
                         "Column group name doesn't match.")

    def test01b_f_col(self):
        """Checking cols._f_col() with a column."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        tblcol = tbl.cols._f_col(self._testNestedCol + "/name")
        if common.verbose:
            print("Column name:", tblcol.name)
        self.assertEqual(tblcol.name, "name", "Column name doesn't match.")

    def test01c_f_col(self):
        """Checking cols._f_col() with a nested subgroup."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        tblcol = tbl.cols._f_col(self._testNestedCol + "/Info2")
        if common.verbose:
            print("Column group name:", tblcol._v_desc._v_pathname)
        self.assertEqual(tblcol._v_desc._v_pathname,
                         self._testNestedCol + "/Info2",
                         "Column group name doesn't match.")

    def test02a__len__(self):
        """Checking cols.__len__() in root level."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        length = len(tbl.cols)
        if common.verbose:
            print("Column group length:", length)
        self.assertEqual(length, len(tbl.colnames),
                         "Column group length doesn't match.")

    def test02b__len__(self):
        """Checking cols.__len__() in subgroup level."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        length = len(tbl.cols.Info)
        if common.verbose:
            print("Column group length:", length)
        self.assertEqual(length, len(tbl.cols.Info._v_colnames),
                         "Column group length doesn't match.")

    def test03a__getitem__(self):
        """Checking cols.__getitem__() with a single index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols[1]
        nrarrcols = nrarr[1]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test03b__getitem__(self):
        """Checking cols.__getitem__() with a range index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols[0:2]
        nrarrcols = nrarr[0:2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test03c__getitem__(self):
        """Checking cols.__getitem__() with a range index with step."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols[0::2]
        nrarrcols = nrarr[0::2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test04a__getitem__(self):
        """Checking cols.__getitem__() with subgroups with a single index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info')[1]
        nrarrcols = nrarr['Info'][1]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test04b__getitem__(self):
        """Checking cols.__getitem__() with subgroups with a range index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info')[0:2]
        nrarrcols = nrarr['Info'][0:2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test04c__getitem__(self):
        """Checking cols.__getitem__() with subgroups with a range index with
        step."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info')[0::2]
        nrarrcols = nrarr['Info'][0::2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test05a__getitem__(self):
        """Checking cols.__getitem__() with a column with a single index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info/value')[1]
        nrarrcols = nrarr['Info']['value'][1]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertEqual(nrarrcols, tblcols,
                         "Original array are retrieved doesn't match.")

    def test05b__getitem__(self):
        """Checking cols.__getitem__() with a column with a range index."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info/value')[0:2]
        nrarrcols = nrarr['Info']['value'][0:2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test05c__getitem__(self):
        """Checking cols.__getitem__() with a column with a range index with
        step."""

        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        tblcols = tbl.cols._f_col('Info/value')[0::2]
        nrarrcols = nrarr['Info']['value'][0::2]
        if common.verbose:
            print("Read cols:", tblcols)
            print("Should look like:", nrarrcols)
        self.assertTrue(common.areArraysEqual(nrarrcols, tblcols),
                        "Original array are retrieved doesn't match.")

    def test_01a__iter__(self):
        tbl = self.h5file.create_table(
            '/', 'test', self._TestTDescr, title=self._getMethodName())
        tbl.append(self._testAData)

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        nrarr = numpy.array(testABuffer, dtype=tbl.description._v_nested_descr)
        row_num = 0
        for item in tbl.cols.Info.value:
            self.assertEqual(item, nrarr['Info']['value'][row_num])
            row_num += 1
        self.assertEqual(row_num, len(nrarr))


class ColsNoReopen(ColsTestCase):
    reopen = 0


class ColsReopen(ColsTestCase):
    reopen = 1


class Nested(t.IsDescription):
    uid = t.IntCol(pos=1)
    value = t.FloatCol(pos=2)


class A_Candidate(t.IsDescription):
    nested1 = Nested()
    nested2 = Nested()


class B_Candidate(t.IsDescription):
    nested1 = Nested
    nested2 = Nested


class C_Candidate(t.IsDescription):
    nested1 = Nested()
    nested2 = Nested

Dnested = {'uid': t.IntCol(pos=1),
           'value': t.FloatCol(pos=2),
           }

D_Candidate = {"nested1": Dnested,
               "nested2": Dnested,
               }

E_Candidate = {"nested1": Nested,
               "nested2": Dnested,
               }

F_Candidate = {"nested1": Nested(),
               "nested2": Dnested,
               }

# Checking several nested columns declared in the same way


class SameNestedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    correct_names = ['',  # The root of columns
                     'nested1', 'nested1/uid', 'nested1/value',
                     'nested2', 'nested2/uid', 'nested2/value']

    def test01a(self):
        """Checking same nested columns (instance flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', A_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test01b(self):
        """Checking same nested columns (class flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', B_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test01c(self):
        """Checking same nested columns (mixed instance/class flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', C_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test01d(self):
        """Checking same nested columns (dictionary flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', D_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test01e(self):
        """Checking same nested columns (mixed dictionary/class flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', E_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test01f(self):
        """Checking same nested columns (mixed dictionary/instance flavor)."""

        tbl = self.h5file.create_table(
            '/', 'test', F_Candidate, title=self._getMethodName())

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test

        names = [col._v_pathname for col in tbl.description._f_walk(
            type="All")]
        if common.verbose:
            print("Pathnames of columns:", names)
            print("Should look like:", self.correct_names)
        self.assertEqual(names, self.correct_names,
                         "Column nested names doesn't match.")

    def test02a(self):
        """Indexing two simple columns under the same nested column."""

        desc = {
            'nested': {
                'i1': t.Int32Col(),
                'i2': t.Int32Col()
            }
        }

        i1 = 'nested/i1'
        i2 = 'nested/i2'
        tbl = self.h5file.create_table(
            '/', 'test', desc, title=self._getMethodName())

        row = tbl.row
        for i in xrange(1000):
            row[i1] = i
            row[i2] = i * 2
            row.append()
        tbl.flush()

        cols = {'i1': tbl.cols.nested.i1,
                'i2': tbl.cols.nested.i2, }
        cols['i1'].create_index()
        cols['i2'].create_index()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test
            # Redefine the cols dictionary
            cols = {'i1': tbl.cols.nested.i1,
                    'i2': tbl.cols.nested.i2, }

        i1res = [row[i1] for row in tbl.where('i1 < 10', cols)]
        i2res = [row[i2] for row in tbl.where('i2 < 10', cols)]

        if common.verbose:
            print("Retrieved values (i1):", i1res)
            print("Should look like:", range(10))
            print("Retrieved values (i2):", i2res)
            print("Should look like:", range(0, 10, 2))

        self.assertEqual(i1res, range(10),
                         "Select for nested column (i1) doesn't match.")
        self.assertEqual(i2res, range(0, 10, 2),
                         "Select for nested column (i2) doesn't match.")

    def test02b(self):
        """Indexing two simple columns under the same (very) nested column."""

        desc = {
            'nested1': {
                'nested2': {
                    'nested3': {
                        'i1': t.Int32Col(),
                        'i2': t.Int32Col()
                    }
                }
            }
        }

        i1 = 'nested1/nested2/nested3/i1'
        i2 = 'nested1/nested2/nested3/i2'

        tbl = self.h5file.create_table(
            '/', 'test', desc, title=self._getMethodName())

        row = tbl.row
        for i in xrange(1000):
            row[i1] = i
            row[i2] = i * 2
            row.append()
        tbl.flush()

        cols = {'i1': tbl.cols.nested1.nested2.nested3.i1,
                'i2': tbl.cols.nested1.nested2.nested3.i2, }
        cols['i1'].create_index()
        cols['i2'].create_index()

        if self.reopen:
            self._reopen()
            tbl = self.h5file.root.test
            # Redefine the cols dictionary
            cols = {'i1': tbl.cols.nested1.nested2.nested3.i1,
                    'i2': tbl.cols.nested1.nested2.nested3.i2, }

        i1res = [row[i1] for row in tbl.where('i1 < 10', cols)]
        i2res = [row[i2] for row in tbl.where('i2 < 10', cols)]

        if common.verbose:
            print("Retrieved values (i1):", i1res)
            print("Should look like:", range(10))
            print("Retrieved values (i2):", i2res)
            print("Should look like:", range(0, 10, 2))

        self.assertEqual(i1res, range(10),
                         "Select for nested column (i1) doesn't match.")
        self.assertEqual(i2res, range(0, 10, 2),
                         "Select for nested column (i2) doesn't match.")


class SameNestedNoReopen(SameNestedTestCase):
    reopen = 0


class SameNestedReopen(SameNestedTestCase):
    reopen = 1


class NestedTypesWithGaps(common.PyTablesTestCase):

    correct_descr = \
        """{
  "float": Float32Col(shape=(), dflt=0.0, pos=0),
  "compound": {
    "char": Int8Col(shape=(), dflt=0, pos=0),
    "double": Float64Col(shape=(), dflt=0.0, pos=1)}}"""

    def test01(self):
        """Opening a table with nested types with gaps."""

        h5file = t.open_file(self._testFilename('nested-type-with-gaps.h5'))
        tbl = h5file.get_node('/nestedtype')
        type_descr = repr(tbl.description)
        if common.verbose:
            print("Type size with no gaps:", tbl.description._v_itemsize)
            print("And should be: 13")
            print("Representation of the nested type:\n", type_descr)
            print("And should be:\n", self.correct_descr)

        self.assertEqual(tbl.description._v_itemsize, 13)
        self.assertEqual(type_descr, self.correct_descr)

        if common.verbose:
            print("Great!  Nested types with gaps recognized correctly.")

        h5file.close()


#----------------------------------------------------------------------
def suite():
    """Return a test suite consisting of all the test cases in the module."""

    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    # theSuite.addTest(unittest.makeSuite(DescriptionTestCase))
    # theSuite.addTest(unittest.makeSuite(WriteReopen))
    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(DescriptionTestCase))
        theSuite.addTest(unittest.makeSuite(CreateTestCase))
        theSuite.addTest(unittest.makeSuite(WriteNoReopen))
        theSuite.addTest(unittest.makeSuite(WriteReopen))
        theSuite.addTest(unittest.makeSuite(ColsNoReopen))
        theSuite.addTest(unittest.makeSuite(ColsReopen))
        theSuite.addTest(unittest.makeSuite(ReadNoReopen))
        theSuite.addTest(unittest.makeSuite(ReadReopen))
        theSuite.addTest(unittest.makeSuite(SameNestedNoReopen))
        theSuite.addTest(unittest.makeSuite(SameNestedReopen))
        theSuite.addTest(unittest.makeSuite(NestedTypesWithGaps))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = test_numpy
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile
import numpy

from numpy import *

from tables import *
from tables.tests import common
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

typecodes = ['b', 'h', 'i', 'l', 'q', 'f', 'd']
# UInt64 checking disabled on win platforms
# because this type is not supported
if sys.platform != 'win32':
    typecodes += ['B', 'H', 'I', 'L', 'Q', 'F', 'D']
else:
    typecodes += ['B', 'H', 'I', 'L', 'F', 'D']
typecodes += ['b1']   # boolean

if 'Float16Atom' in globals():
    typecodes.append('e')
if 'Float96Atom' in globals() or 'Float128Atom' in globals():
    typecodes.append('g')
if 'Complex192Atom' in globals() or 'Conplex256Atom' in globals():
    typecodes.append('G')

byteorder = {'little': '<', 'big': '>'}[sys.byteorder]


class BasicTestCase(unittest.TestCase):
    """Basic test for all the supported typecodes present in NumPy.

    All of them are included on PyTables.

    """
    endiancheck = 0

    def WriteRead(self, testArray):
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running test for array with typecode '%s'" %
                  testArray.dtype.char, end=' ')
            print("for class check:", self.title)

        # Create an instance of HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.root = self.fileh.root
        # Create the array under root and name 'somearray'
        a = testArray
        self.fileh.create_array(self.root, 'somearray', a, "Some array")

        # Close the file
        self.fileh.close()

        # Re-open the file in read-only mode
        self.fileh = open_file(self.file, mode="r")
        self.root = self.fileh.root

        # Read the saved array
        b = self.root.somearray.read()
        # For cases that read returns a python type instead of a numpy type
        if not hasattr(b, "shape"):
            b = array(b, dtype=a.dtype.str)

        # Compare them. They should be equal.
        # if not allequal(a,b, "numpy") and common.verbose:
        if common.verbose:
            print("Array written:", a)
            print("Array written shape:", a.shape)
            print("Array written itemsize:", a.itemsize)
            print("Array written type:", a.dtype.char)
            print("Array read:", b)
            print("Array read shape:", b.shape)
            print("Array read itemsize:", b.itemsize)
            print("Array read type:", b.dtype.char)

        type_ = self.root.somearray.atom.type
        # Check strictly the array equality
        self.assertEqual(type(a), type(b))
        self.assertEqual(a.shape, b.shape)
        self.assertEqual(a.shape, self.root.somearray.shape)
        self.assertEqual(a.dtype, b.dtype)
        if a.dtype.char[0] == "S":
            self.assertEqual(type_, "string")
        else:
            self.assertEqual(a.dtype.base.name, type_)

        self.assertTrue(allequal(a, b, "numpy"))
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        return

    def test00_char(self):
        "Data integrity during recovery (character objects)"

        a = array(self.tupleChar, 'S'+str(len(self.tupleChar)))
        self.WriteRead(a)
        return

    def test01_char_nc(self):
        "Data integrity during recovery (non-contiguous character objects)"

        a = array(self.tupleChar, 'S'+str(len(self.tupleChar)))
        if a.shape == ():
            b = a               # We cannot use the indexing notation
        else:
            b = a[::2]
            # Ensure that this numpy string is non-contiguous
            if a.shape[0] > 2:
                self.assertEqual(b.flags['CONTIGUOUS'], False)
        self.WriteRead(b)
        return

    def test02_types(self):
        "Data integrity during recovery (numerical types)"

        for typecode in typecodes:
            if self.tupleInt.shape:
                a = self.tupleInt.astype(typecode)
            else:
                # shape is the empty tuple ()
                a = array(self.tupleInt, dtype=typecode)
            self.WriteRead(a)

        return

    def test03_types_nc(self):
        "Data integrity during recovery (non-contiguous numerical types)"

        for typecode in typecodes:
            if self.tupleInt.shape:
                a = self.tupleInt.astype(typecode)
            else:
                # shape is the empty tuple ()
                a = array(self.tupleInt, dtype=typecode)
            # This should not be tested for the rank-0 case
            if len(a.shape) == 0:
                return
            b = a[::2]
            # Ensure that this array is non-contiguous (for non-trivial case)
            if a.shape[0] > 2:
                self.assertEqual(b.flags['CONTIGUOUS'], False)
            self.WriteRead(b)

        return


class Basic0DOneTestCase(BasicTestCase):
    # Rank-0 case
    title = "Rank-0 case 1"
    tupleInt = array(3)
    tupleChar = "4"


class Basic0DTwoTestCase(BasicTestCase):
    # Rank-0 case
    title = "Rank-0 case 2"
    tupleInt = array(33)
    tupleChar = "44"


class Basic1DOneTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 1"
    tupleInt = array((3,))
    tupleChar = ("a",)


class Basic1DTwoTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 2"
    tupleInt = array((0, 4))
    tupleChar = ("aaa",)


class Basic1DThreeTestCase(BasicTestCase):
    # 1D case
    title = "Rank-1 case 3"
    tupleInt = array((3, 4, 5))
    tupleChar = ("aaaa", "bbb",)


class Basic2DTestCase(BasicTestCase):
    # 2D case
    title = "Rank-2 case 1"
    # tupleInt = reshape(array(arange((4)**2)), (4,)*2)
    tupleInt = ones((4,)*2)
    tupleChar = [["aaa", "ddddd"], ["d", "ss"], ["s", "tt"]]


class Basic10DTestCase(BasicTestCase):
    # 10D case
    title = "Rank-10 case 1"
    # tupleInt = reshape(array(arange((2)**10)), (2,)*10)
    tupleInt = ones((2,)*10)
    # tupleChar = reshape(array([1],dtype="S1"),(1,)*10)
    # The next tuple consumes far more time, so this
    # test should be run in common.heavy mode.
    tupleChar = array(tupleInt, dtype="S1")


# class Basic32DTestCase(BasicTestCase):
#     # 32D case (maximum)
#     tupleInt = reshape(array((22,)), (1,)*32)
#     # Strings seems to be very slow with somewhat large dimensions
#     # This should not be run unless the numarray people address this problem
#     # F. Alted 2006-01-04
#     tupleChar = array(tupleInt, dtype="S1")


class GroupsArrayTestCase(unittest.TestCase):
    """This test class checks combinations of arrays with groups.

    It also uses arrays ranks which ranges until 10.

    """

    def test00_iterativeGroups(self):
        """Checking combinations of arrays with groups
        It also uses arrays ranks which ranges until 10.
        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_iterativeGroups..." %
                  self.__class__.__name__)

        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        # Get the root group
        group = fileh.root

        i = 1
        for typecode in typecodes:
            # Create an array of typecode, with incrementally bigger ranges
            a = ones((2,) * i, typecode)
            # Save it on the HDF5 file
            dsetname = 'array_' + typecode
            if common.verbose:
                print("Creating dataset:", group._g_join(dsetname))
            fileh.create_array(group, dsetname, a, "Large array")
            # Create a new group
            group = fileh.create_group(group, 'group' + str(i))
            # increment the range for next iteration
            i += 1

        # Close the file
        fileh.close()

        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        # Get the root group
        group = fileh.root

        # Get the metadata on the previosly saved arrays
        for i in range(1, len(typecodes)):
            # Create an array for later comparison
            a = ones((2,) * i, typecodes[i - 1])
            # Get the dset object hanging from group
            dset = getattr(group, 'array_' + typecodes[i-1])
            # Get the actual array
            b = dset.read()
            if not allequal(a, b, "numpy") and common.verbose:
                print("Array a original. Shape: ==>", a.shape)
                print("Array a original. Data: ==>", a)
                print("Info from dataset:", dset._v_pathname)
                print("  shape ==>", dset.shape, end=' ')
                print("  dtype ==> %s" % dset.dtype)
                print("Array b read from file. Shape: ==>", b.shape, end=' ')
                print(". Type ==> %s" % b.dtype.char)

            self.assertEqual(a.shape, b.shape)
            if dtype('l').itemsize == 4:
                if (a.dtype.char == "i" or a.dtype.char == "l"):
                    # Special expection. We have no way to distinguish between
                    # "l" and "i" typecode, and we can consider them the same
                    # to all practical effects
                    self.assertTrue(b.dtype.char == "l" or b.dtype.char == "i")
                elif (a.dtype.char == "I" or a.dtype.char == "L"):
                    # Special expection. We have no way to distinguish between
                    # "L" and "I" typecode, and we can consider them the same
                    # to all practical effects
                    self.assertTrue(b.dtype.char == "L" or b.dtype.char == "I")
                else:
                    self.assertTrue(allequal(a, b, "numpy"))
            elif dtype('l').itemsize == 8:
                if (a.dtype.char == "q" or a.dtype.char == "l"):
                    # Special expection. We have no way to distinguish between
                    # "q" and "l" typecode in 64-bit platforms, and we can
                    # consider them the same to all practical effects
                    self.assertTrue(b.dtype.char == "l" or b.dtype.char == "q")
                elif (a.dtype.char == "Q" or a.dtype.char == "L"):
                    # Special expection. We have no way to distinguish between
                    # "Q" and "L" typecode in 64-bit platforms, and we can
                    # consider them the same to all practical effects
                    self.assertTrue(b.dtype.char == "L" or b.dtype.char == "Q")
                else:
                    self.assertTrue(allequal(a, b, "numpy"))

            # Iterate over the next group
            group = getattr(group, 'group' + str(i))

        # Close the file
        fileh.close()

        # Then, delete the file
        os.remove(file)

    def test01_largeRankArrays(self):
        """Checking creation of large rank arrays (0 < rank <= 32)
        It also uses arrays ranks which ranges until maxrank.
        """

        # maximum level of recursivity (deepest group level) achieved:
        # maxrank = 32 (for a effective maximum rank of 32)
        # This limit is due to a limit in the HDF5 library.
        minrank = 1
        maxrank = 32

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_largeRankArrays..." %
                  self.__class__.__name__)
            print("Maximum rank for tested arrays:", maxrank)
        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, mode="w")
        group = fileh.root
        if common.verbose:
            print("Rank array writing progress: ", end=' ')
        for rank in range(minrank, maxrank + 1):
            # Create an array of integers, with incrementally bigger ranges
            a = ones((1,) * rank, 'i')
            if common.verbose:
                print("%3d," % (rank), end=' ')
            fileh.create_array(group, "array", a, "Rank: %s" % rank)
            group = fileh.create_group(group, 'group' + str(rank))
        # Flush the buffers
        fileh.flush()
        # Close the file
        fileh.close()

        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        group = fileh.root
        if common.verbose:
            print()
            print("Rank array reading progress: ")
        # Get the metadata on the previosly saved arrays
        for rank in range(minrank, maxrank + 1):
            # Create an array for later comparison
            a = ones((1,) * rank, 'i')
            # Get the actual array
            b = group.array.read()
            if common.verbose:
                print("%3d," % (rank), end=' ')
            if not a.tolist() == b.tolist() and common.verbose:
                print("Info from dataset:", dset._v_pathname)
                print("  Shape: ==>", dset.shape, end=' ')
                print("  typecode ==> %c" % dset.typecode)
                print("Array b read from file. Shape: ==>", b.shape, end=' ')
                print(". Type ==> %c" % b.dtype.char)
            self.assertEqual(a.shape, b.shape)
            if a.dtype.char == "i":
                # Special expection. We have no way to distinguish between
                # "l" and "i" typecode, and we can consider them the same
                # to all practical effects
                self.assertTrue(b.dtype.char == "l" or b.dtype.char == "i")
            else:
                self.assertEqual(a.dtype.char, b.dtype.char)

            self.assertEqual(a, b)

            # Iterate over the next group
            group = fileh.get_node(group, 'group' + str(rank))

        if common.verbose:
            print()  # This flush the stdout buffer
        # Close the file
        fileh.close()
        # Delete the file
        os.remove(file)


# Test Record class
class Record(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"abcd", pos=0)
    var2 = StringCol(itemsize=1, dflt=b"a", pos=1)
    var3 = BoolCol(dflt=1)
    var4 = Int8Col(dflt=1)
    var5 = UInt8Col(dflt=1)
    var6 = Int16Col(dflt=1)
    var7 = UInt16Col(dflt=1)
    var8 = Int32Col(dflt=1)
    var9 = UInt32Col(dflt=1)
    var10 = Int64Col(dflt=1)
    var11 = Float32Col(dflt=1.0)
    var12 = Float64Col(dflt=1.0)
    var13 = ComplexCol(itemsize=8, dflt=(1.+0.j))
    var14 = ComplexCol(itemsize=16, dflt=(1.+0.j))
    if 'Float16Col' in globals():
        var15 = Float16Col(dflt=1.0)
    if 'Float96Col' in globals():
        var16 = Float96Col(dflt=1.0)
    if 'Float128Col' in globals():
        var17 = Float128Col(dflt=1.0)
    if 'Complex196Col' in globals():
        var18 = ComplexCol(itemsize=24, dflt=(1.+0.j))
    if 'Complex256Col' in globals():
        var19 = ComplexCol(itemsize=32, dflt=(1.+0.j))


class TableReadTestCase(common.PyTablesTestCase):
    nrows = 100

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        fileh = open_file(self.file, "w")
        table = fileh.create_table(fileh.root, 'table', Record)
        for i in range(self.nrows):
            table.row.append()  # Fill 100 rows with default values
        fileh.close()
        self.fileh = open_file(self.file, "a")  # allow flavor changes

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01_readTableChar(self):
        """Checking column conversion into NumPy in read().

        Char flavor

        """

        table = self.fileh.root.table
        table.flavor = "numpy"
        for colname in table.colnames:
            numcol = table.read(field=colname)
            typecol = table.coltypes[colname]
            itemsizecol = table.description._v_dtypes[colname].base.itemsize
            nctypecode = numcol.dtype.char
            if typecol == "string":
                if itemsizecol > 1:
                    orignumcol = array(['abcd']*self.nrows, dtype='S4')
                else:
                    orignumcol = array(['a']*self.nrows, dtype='S1')
                if common.verbose:
                    print("Typecode of NumPy column read:", nctypecode)
                    print("Should look like:", 'c')
                    print("Itemsize of column:", itemsizecol)
                    print("Shape of NumPy column read:", numcol.shape)
                    print("Should look like:", orignumcol.shape)
                    print("First 3 elements of read col:", numcol[:3])
                # Check that both NumPy objects are equal
                self.assertTrue(allequal(numcol, orignumcol, "numpy"))

    def test01_readTableNum(self):
        """Checking column conversion into NumPy in read().

        NumPy flavor

        """

        table = self.fileh.root.table
        table.flavor = "numpy"
        for colname in table.colnames:
            numcol = table.read(field=colname)
            typecol = table.coltypes[colname]
            nctypecode = typeNA[numcol.dtype.char[0]]
            if typecol != "string":
                if common.verbose:
                    print("Typecode of NumPy column read:", nctypecode)
                    print("Should look like:", typecol)
                orignumcol = ones(shape=self.nrows, dtype=numcol.dtype.char)
                # Check that both NumPy objects are equal
                self.assertTrue(allequal(numcol, orignumcol, "numpy"))

    def test02_readCoordsChar(self):
        """Column conversion into NumPy in readCoords().

        Chars

        """

        table = self.fileh.root.table
        table.flavor = "numpy"
        coords = [1, 2, 3]
        self.nrows = len(coords)
        for colname in table.colnames:
            numcol = table.read_coordinates(coords, field=colname)
            typecol = table.coltypes[colname]
            itemsizecol = table.description._v_dtypes[colname].base.itemsize
            nctypecode = numcol.dtype.char
            if typecol == "string":
                if itemsizecol > 1:
                    orignumcol = array(['abcd']*self.nrows, dtype='S4')
                else:
                    orignumcol = array(['a']*self.nrows, dtype='S1')
                if common.verbose:
                    print("Typecode of NumPy column read:", nctypecode)
                    print("Should look like:", 'c')
                    print("Itemsize of column:", itemsizecol)
                    print("Shape of NumPy column read:", numcol.shape)
                    print("Should look like:", orignumcol.shape)
                    print("First 3 elements of read col:", numcol[:3])
                # Check that both NumPy objects are equal
                self.assertTrue(allequal(numcol, orignumcol, "numpy"))

    def test02_readCoordsNum(self):
        """Column conversion into NumPy in read_coordinates().

        NumPy.

        """

        table = self.fileh.root.table
        table.flavor = "numpy"
        coords = [1, 2, 3]
        self.nrows = len(coords)
        for colname in table.colnames:
            numcol = table.read_coordinates(coords, field=colname)
            typecol = table.coltypes[colname]
            type_ = numcol.dtype.type
            if typecol != "string":
                if typecol == "int64":
                    return
                if common.verbose:
                    print("Type of read NumPy column:", type_)
                    print("Should look like:", typecol)
                orignumcol = ones(shape=self.nrows, dtype=numcol.dtype.char)
                # Check that both NumPy objects are equal
                self.assertTrue(allequal(numcol, orignumcol, "numpy"))

    def test03_getIndexNumPy(self):
        """Getting table rows specifyied as NumPy scalar integers."""

        table = self.fileh.root.table
        coords = numpy.array([1, 2, 3], dtype='int8')
        for colname in table.colnames:
            numcol = [table[coord][colname] for coord in coords]
            typecol = table.coltypes[colname]
            if typecol != "string":
                if typecol == "int64":
                    return
                numcol = numpy.array(numcol, typecol)
                if common.verbose:
                    type_ = numcol.dtype.type
                    print("Type of read NumPy column:", type_)
                    print("Should look like:", typecol)
                orignumcol = ones(shape=len(numcol), dtype=numcol.dtype.char)
                # Check that both NumPy objects are equal
                self.assertTrue(allequal(numcol, orignumcol, "numpy"))

    def test04_setIndexNumPy(self):
        """Setting table rows specifyied as NumPy integers."""

        self.fileh.close()
        self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        table.flavor = "numpy"
        coords = numpy.array([1, 2, 3], dtype='int8')
        # Modify row 1
        # From PyTables 2.0 on, assignments to records can be done
        # only as tuples (see http://projects.scipy.org/scipy/numpy/ticket/315)
        # table[coords[0]] = ["aasa","x"]+[232]*12

        n = len(Record.columns) - 2

        table[coords[0]] = tuple(["aasa", "x"]+[232]*n)     # XXX
        # record = list(table[coords[0]])
        record = table.read(coords[0], coords[0] + 1)
        if common.verbose:
            print("""Original row:
['aasa', 'x', True, -24, 232, 232, 232, 232, 232L, 232, 232.0, 232.0, (232 + 0j), (232+0j), 232.0, (232+0j)]
""")
            print("Read row:\n", record)
        self.assertEqual(record['var1'], b'aasa')
        self.assertEqual(record['var2'], b'x')
        self.assertEqual(record['var3'], True)
        self.assertEqual(record['var4'], -24)
        self.assertEqual(record['var7'], 232)


# The declaration of the nested table:
class Info(IsDescription):
    _v_pos = 3
    Name = StringCol(itemsize=2)
    Value = ComplexCol(itemsize=16)


class TestTDescr(IsDescription):

    """A description that has several nested columns."""

    x = Int32Col(dflt=0, shape=2, pos=0)  # 0
    y = FloatCol(dflt=1, shape=(2, 2))
    z = UInt8Col(dflt=1)
    z3 = EnumCol({'r': 4, 'g': 2, 'b': 1}, 'r', 'int32', shape=2)
    color = StringCol(itemsize=4, dflt=b"ab", pos=2)
    info = Info()

    class Info(IsDescription):  # 1
        _v_pos = 1
        name = StringCol(itemsize=2)
        value = ComplexCol(itemsize=16, pos=0)  # 0
        y2 = FloatCol(pos=1)  # 1
        z2 = UInt8Col()

        class Info2(IsDescription):
            y3 = Time64Col(shape=2)
            name = StringCol(itemsize=2)
            value = ComplexCol(itemsize=16, shape=2)


class TableNativeFlavorTestCase(common.PyTablesTestCase):
    nrows = 100

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        fileh = open_file(self.file, "w")
        table = fileh.create_table(fileh.root, 'table', TestTDescr,
                                   expectedrows=self.nrows)
        table.flavor = "numpy"
        for i in range(self.nrows):
            table.row.append()  # Fill 100 rows with default values
        table.flush()
        self.fileh = fileh

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01a_basicTableRead(self):
        """Checking the return of a NumPy in read()."""

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        data = table[:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the value of some columns
        # A flat column
        col = table.cols.x[:3]
        self.assertTrue(isinstance(col, ndarray))
        npcol = zeros((3, 2), dtype="int32")
        self.assertTrue(allequal(col, npcol, "numpy"))
        # A nested column
        col = table.cols.Info[:3]
        self.assertTrue(isinstance(col, ndarray))
        dtype = [('value', 'c16'),
                 ('y2', 'f8'),
                 ('Info2',
                  [('name', 'S2'),
                   ('value', 'c16', (2,)),
                   ('y3', 'f8', (2,))]),
                 ('name', 'S2'),
                 ('z2', 'u1')]
        npcol = zeros((3,), dtype=dtype)
        self.assertEqual(col.dtype.descr, npcol.dtype.descr)
        if common.verbose:
            print("col-->", col)
            print("npcol-->", npcol)
        # A copy() is needed in case the buffer can be in different segments
        self.assertEqual(bytes(col.copy().data), bytes(npcol.data))

    def test01b_basicTableRead(self):
        """Checking the return of a NumPy in read() (strided version)."""

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        data = table[::3]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the value of some columns
        # A flat column
        col = table.cols.x[:9:3]
        self.assertTrue(isinstance(col, ndarray))
        npcol = zeros((3, 2), dtype="int32")
        self.assertTrue(allequal(col, npcol, "numpy"))
        # A nested column
        col = table.cols.Info[:9:3]
        self.assertTrue(isinstance(col, ndarray))
        dtype = [('value', '%sc16' % byteorder),
                 ('y2', '%sf8' % byteorder),
                 ('Info2',
                  [('name', '|S2'),
                   ('value', '%sc16' % byteorder, (2,)),
                   ('y3', '%sf8' % byteorder, (2,))]),
                 ('name', '|S2'),
                 ('z2', '|u1')]
        npcol = zeros((3,), dtype=dtype)
        self.assertEqual(col.dtype.descr, npcol.dtype.descr)
        if common.verbose:
            print("col-->", col)
            print("npcol-->", npcol)
        # A copy() is needed in case the buffer can be in different segments
        self.assertEqual(bytes(col.copy().data), bytes(npcol.data))

    def test02_getWhereList(self):
        """Checking the return of NumPy in get_where_list method."""

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        data = table.get_where_list('z == 1')
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 100)
        # Finally, check that the contents are ok
        self.assertTrue(allequal(data, arange(100, dtype="i8"), "numpy"))

    def test03a_readWhere(self):
        """Checking the return of NumPy in read_where method (strings)."""

        table = self.fileh.root.table
        table.cols.color.create_index()
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.read_where('color == b"ab"')
        if common.verbose:
            print("Type of read:", type(data))
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), self.nrows)

    def test03b_readWhere(self):
        """Checking the return of NumPy in read_where method (numeric)."""

        table = self.fileh.root.table
        table.cols.z.create_index()
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.read_where('z == 0')
        if common.verbose:
            print("Type of read:", type(data))
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 0)

    def test04a_createTable(self):
        """Checking the Table creation from a numpy recarray."""

        dtype = [('value', '%sc16' % byteorder),
                 ('y2', '%sf8' % byteorder),
                 ('Info2',
                  [('name', '|S2'),
                   ('value', '%sc16' % byteorder, (2,)),
                   ('y3', '%sf8' % byteorder, (2,))]),
                 ('name', '|S2'),
                 ('z2', '|u1')]
        npdata = zeros((3,), dtype=dtype)
        table = self.fileh.create_table(self.fileh.root, 'table2', npdata)
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table2
        data = table[:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npdata.dtype.descr)
        if common.verbose:
            print("npdata-->", npdata)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(bytes(data.copy().data), bytes(npdata.data))

    def test04b_appendTable(self):
        """Checking appending a numpy recarray."""

        table = self.fileh.root.table
        npdata = table[3:6]
        table.append(npdata)
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table[-3:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("Last 3 elements of read:", data[-3:])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npdata.dtype.descr)
        if common.verbose:
            print("npdata-->", npdata)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(bytes(data.copy().data), bytes(npdata.data))

    def test05a_assignColumn(self):
        """Checking assigning to a column."""

        table = self.fileh.root.table
        table.cols.z[:] = zeros((100,), dtype='u1')
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.cols.z[:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 100)
        # Finally, check that the contents are ok
        self.assertTrue(allequal(data, zeros((100,), dtype="u1"), "numpy"))

    def test05b_modifyingColumns(self):
        """Checking modifying several columns at once."""

        table = self.fileh.root.table
        xcol = ones((3, 2), 'int32')
        ycol = zeros((3, 2, 2), 'float64')
        zcol = zeros((3,), 'uint8')
        table.modify_columns(3, 6, 1, [xcol, ycol, zcol], ['x', 'y', 'z'])
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.cols.y[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(data.copy().data, ycol.data)

    def test05c_modifyingColumns(self):
        """Checking modifying several columns using a single numpy buffer."""

        table = self.fileh.root.table
        dtype = [('x', 'i4', (2,)), ('y', 'f8', (2, 2)), ('z', 'u1')]
        nparray = zeros((3,), dtype=dtype)
        table.modify_columns(3, 6, 1, nparray, ['x', 'y', 'z'])
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        ycol = zeros((3, 2, 2), 'float64')
        data = table.cols.y[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(data.copy().data, ycol.data)

    def test06a_assignNestedColumn(self):
        """Checking assigning a nested column (using modify_column)."""

        table = self.fileh.root.table
        dtype = [('value', '%sc16' % byteorder),
                 ('y2', '%sf8' % byteorder),
                 ('Info2',
                  [('name', '|S2'),
                   ('value', '%sc16' % byteorder, (2,)),
                   ('y3', '%sf8' % byteorder, (2,))]),
                 ('name', '|S2'),
                 ('z2', '|u1')]
        npdata = zeros((3,), dtype=dtype)
        data = table.cols.Info[3:6]
        table.modify_column(3, 6, 1, column=npdata, colname='Info')
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.cols.Info[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npdata.dtype.descr)
        if common.verbose:
            print("npdata-->", npdata)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(bytes(data.copy().data), bytes(npdata.data))

    def test06b_assignNestedColumn(self):
        """Checking assigning a nested column (using the .cols accessor)."""

        table = self.fileh.root.table
        dtype = [('value', '%sc16' % byteorder),
                 ('y2', '%sf8' % byteorder),
                 ('Info2',
                  [('name', '|S2'),
                   ('value', '%sc16' % byteorder, (2,)),
                   ('y3', '%sf8' % byteorder, (2,))]),
                 ('name', '|S2'),
                 ('z2', '|u1')]
        npdata = zeros((3,), dtype=dtype)
#         self.assertRaises(NotImplementedError,
#                           table.cols.Info.__setitem__, slice(3,6,1),  npdata)
        table.cols.Info[3:6] = npdata
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        data = table.cols.Info[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npdata.dtype.descr)
        if common.verbose:
            print("npdata-->", npdata)
            print("data-->", data)
        # A copy() is needed in case the buffer would be in different segments
        self.assertEqual(bytes(data.copy().data), bytes(npdata.data))

    def test07a_modifyingRows(self):
        """Checking modifying several rows at once (using modify_rows)."""

        table = self.fileh.root.table
        # Read a chunk of the table
        chunk = table[0:3]
        # Modify it somewhat
        chunk['y'][:] = -1
        table.modify_rows(3, 6, 1, rows=chunk)
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        ycol = zeros((3, 2, 2), 'float64')-1
        data = table.cols.y[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        self.assertTrue(allequal(ycol, data, "numpy"))

    def test07b_modifyingRows(self):
        """Checking modifying several rows at once (using cols accessor)."""

        table = self.fileh.root.table
        # Read a chunk of the table
        chunk = table[0:3]
        # Modify it somewhat
        chunk['y'][:] = -1
        table.cols[3:6] = chunk
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check that some column has been actually modified
        ycol = zeros((3, 2, 2), 'float64')-1
        data = table.cols.y[3:6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        self.assertTrue(allequal(ycol, data, "numpy"))

    def test08a_modifyingRows(self):
        """Checking modifying just one row at once (using modify_rows)."""

        table = self.fileh.root.table
        # Read a chunk of the table
        chunk = table[3:4]
        # Modify it somewhat
        chunk['y'][:] = -1
        table.modify_rows(6, 7, 1, chunk)
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check that some column has been actually modified
        ycol = zeros((2, 2), 'float64')-1
        data = table.cols.y[6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        self.assertTrue(allequal(ycol, data, "numpy"))

    def test08b_modifyingRows(self):
        """Checking modifying just one row at once (using cols accessor)."""

        table = self.fileh.root.table
        # Read a chunk of the table
        chunk = table[3:4]
        # Modify it somewhat
        chunk['y'][:] = -1
        table.cols[6] = chunk
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            table = self.fileh.root.table
        # Check that some column has been actually modified
        ycol = zeros((2, 2), 'float64')-1
        data = table.cols.y[6]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
            print("Length of the data read:", len(data))
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, ycol.dtype.descr)
        if common.verbose:
            print("ycol-->", ycol)
            print("data-->", data)
        self.assertTrue(allequal(ycol, data, "numpy"))

    def test09a_getStrings(self):
        """Checking the return of string columns with spaces."""

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        rdata = table.get_where_list('color == b"ab"')
        data = table.read_coordinates(rdata)
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 100)
        # Finally, check that the contents are ok
        for idata in data['color']:
            self.assertEqual(idata, array("ab", dtype="|S4"))

    def test09b_getStrings(self):
        """Checking the return of string columns with spaces.

        (modify)

        """

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        for i in range(50):
            table.cols.color[i] = "a  "
        table.flush()
        data = table[:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 100)
        # Finally, check that the contents are ok
        for i in range(100):
            idata = data['color'][i]
            if i >= 50:
                self.assertEqual(idata, array("ab", dtype="|S4"))
            else:
                self.assertEqual(idata, array("a  ", dtype="|S4"))

    def test09c_getStrings(self):
        """Checking the return of string columns with spaces.

        (append)

        """

        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        table = self.fileh.root.table
        row = table.row
        for i in range(50):
            row["color"] = "a  "   # note the trailing spaces
            row.append()
        table.flush()
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
        data = self.fileh.root.table[:]
        if common.verbose:
            print("Type of read:", type(data))
            print("Description of the record:", data.dtype.descr)
            print("First 3 elements of read:", data[:3])
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check that all columns have been selected
        self.assertEqual(len(data), 150)
        # Finally, check that the contents are ok
        # Finally, check that the contents are ok
        for i in range(150):
            idata = data['color'][i]
            if i < 100:
                self.assertEqual(idata, array("ab", dtype="|S4"))
            else:
                self.assertEqual(idata, array("a  ", dtype="|S4"))


class TableNativeFlavorOpenTestCase(TableNativeFlavorTestCase):
    close = 0


class TableNativeFlavorCloseTestCase(TableNativeFlavorTestCase):
    close = 1


class AttributesTestCase(common.PyTablesTestCase):

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.fileh.create_group(self.fileh.root, 'group')

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01_writeAttribute(self):
        """Checking the creation of a numpy attribute."""
        group = self.fileh.root.group
        g_attrs = group._v_attrs
        g_attrs.numpy1 = zeros((1, 1), dtype='int16')
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            group = self.fileh.root.group
            g_attrs = group._v_attrs
        # Check that we can retrieve a numpy object
        data = g_attrs.numpy1
        npcomp = zeros((1, 1), dtype='int16')
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npcomp.dtype.descr)
        if common.verbose:
            print("npcomp-->", npcomp)
            print("data-->", data)
        self.assertTrue(allequal(npcomp, data, "numpy"))

    def test02_updateAttribute(self):
        """Checking the modification of a numpy attribute."""

        group = self.fileh.root.group
        g_attrs = group._v_attrs
        g_attrs.numpy1 = zeros((1, 2), dtype='int16')
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            group = self.fileh.root.group
            g_attrs = group._v_attrs
        # Update this attribute
        g_attrs.numpy1 = ones((1, 2), dtype='int16')
        # Check that we can retrieve a numpy object
        data = g_attrs.numpy1
        npcomp = ones((1, 2), dtype='int16')
        # Check that both NumPy objects are equal
        self.assertTrue(isinstance(data, ndarray))
        # Check the type
        self.assertEqual(data.dtype.descr, npcomp.dtype.descr)
        if common.verbose:
            print("npcomp-->", npcomp)
            print("data-->", data)
        self.assertTrue(allequal(npcomp, data, "numpy"))


class AttributesOpenTestCase(AttributesTestCase):
    close = 0


class AttributesCloseTestCase(AttributesTestCase):
    close = 1


class StrlenTestCase(common.PyTablesTestCase):

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        group = self.fileh.create_group(self.fileh.root, 'group')
        tablelayout = {'Text': StringCol(itemsize=1000), }
        self.table = self.fileh.create_table(group, 'table', tablelayout)
        self.table.flavor = 'numpy'
        row = self.table.row
        row['Text'] = 'Hello Francesc!'     # XXX: check unicode --> bytes
        row.append()
        row['Text'] = 'Hola Francesc!'      # XXX: check unicode --> bytes
        row.append()
        self.table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01(self):
        """Checking the lengths of strings (read field)."""
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            self.table = self.fileh.root.group.table
        # Get both strings
        str1 = self.table.col('Text')[0]
        str2 = self.table.col('Text')[1]
        if common.verbose:
            print("string1-->", str1)
            print("string2-->", str2)
        # Check that both NumPy objects are equal
        self.assertEqual(len(str1), len(b'Hello Francesc!'))
        self.assertEqual(len(str2), len(b'Hola Francesc!'))
        self.assertEqual(str1, b'Hello Francesc!')
        self.assertEqual(str2, b'Hola Francesc!')

    def test02(self):
        """Checking the lengths of strings (read recarray)."""
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            self.table = self.fileh.root.group.table
        # Get both strings
        str1 = self.table[:]['Text'][0]
        str2 = self.table[:]['Text'][1]
        # Check that both NumPy objects are equal
        self.assertEqual(len(str1), len(b'Hello Francesc!'))
        self.assertEqual(len(str2), len(b'Hola Francesc!'))
        self.assertEqual(str1, b'Hello Francesc!')
        self.assertEqual(str2, b'Hola Francesc!')

    def test03(self):
        """Checking the lengths of strings (read recarray, row by row)."""
        if self.close:
            self.fileh.close()
            self.fileh = open_file(self.file, "a")
            self.table = self.fileh.root.group.table
        # Get both strings
        str1 = self.table[0]['Text']
        str2 = self.table[1]['Text']
        # Check that both NumPy objects are equal
        self.assertEqual(len(str1), len(b'Hello Francesc!'))
        self.assertEqual(len(str2), len(b'Hola Francesc!'))
        self.assertEqual(str1, b'Hello Francesc!')
        self.assertEqual(str2, b'Hola Francesc!')


class StrlenOpenTestCase(StrlenTestCase):
    close = 0


class StrlenCloseTestCase(StrlenTestCase):
    close = 1


#--------------------------------------------------------

def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    # theSuite.addTest(unittest.makeSuite(StrlenOpenTestCase))
    # theSuite.addTest(unittest.makeSuite(Basic0DOneTestCase))
    # theSuite.addTest(unittest.makeSuite(GroupsArrayTestCase))
    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(Basic0DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic0DTwoTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DOneTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DTwoTestCase))
        theSuite.addTest(unittest.makeSuite(Basic1DThreeTestCase))
        theSuite.addTest(unittest.makeSuite(Basic2DTestCase))
        theSuite.addTest(unittest.makeSuite(GroupsArrayTestCase))
        theSuite.addTest(unittest.makeSuite(TableReadTestCase))
        theSuite.addTest(unittest.makeSuite(TableNativeFlavorOpenTestCase))
        theSuite.addTest(unittest.makeSuite(TableNativeFlavorCloseTestCase))
        theSuite.addTest(unittest.makeSuite(AttributesOpenTestCase))
        theSuite.addTest(unittest.makeSuite(AttributesCloseTestCase))
        theSuite.addTest(unittest.makeSuite(StrlenOpenTestCase))
        theSuite.addTest(unittest.makeSuite(StrlenCloseTestCase))
        if common.heavy:
            theSuite.addTest(unittest.makeSuite(Basic10DTestCase))
            # The 32 dimensions case takes forever to run!!
            # theSuite.addTest(unittest.makeSuite(Basic32DTestCase))
    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_queries
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: 2006-10-19
# Author: Ivan Vilata i Balaguer - ivan@selidor.net
#
# $Id$
#
########################################################################

"""Test module for queries on datasets."""

import re
import sys
import types
import unittest

import numpy

import tables
from tables.utils import SizeType
from tables.tests import common
from tables.tests.common import verbosePrint as vprint


# Data parameters
# ---------------
row_period = 50
"""Maximum number of unique rows before they start cycling."""
md_shape = (2, 2)
"""Shape of multidimensional fields."""

_maxnvalue = row_period + numpy.prod(md_shape, dtype=SizeType) - 1
_strlen = int(numpy.log10(_maxnvalue-1)) + 1

str_format = '%%0%dd' % _strlen
"""Format of string values."""

small_blocksizes = (300, 60, 20, 5)
# small_blocksizes = (512, 128, 32, 4)   # for manual testing only
"""Sensible parameters for indexing with small blocksizes."""


# Type information
# ----------------
type_info = {
    'bool': (numpy.bool_, bool),
    'int8': (numpy.int8, int),
    'uint8': (numpy.uint8, int),
    'int16': (numpy.int16, int),
    'uint16': (numpy.uint16, int),
    'int32': (numpy.int32, int),
    'uint32': (numpy.uint32, long),
    'int64': (numpy.int64, long),
    'uint64': (numpy.uint64, long),
    'float32': (numpy.float32, float),
    'float64': (numpy.float64, float),
    'complex64': (numpy.complex64, complex),
    'complex128': (numpy.complex128, complex),
    'time32': (numpy.int32, int),
    'time64': (numpy.float64, float),
    'enum': (numpy.uint8, int),  # just for these tests
    'string': ('S%s' % _strlen, numpy.string_),  # just for these tests
}
"""NumPy and Numexpr type for each PyTables type that will be tested."""

if hasattr(numpy, 'float16'):
    type_info['float16'] = (numpy.float16, float)
# if hasattr(numpy, 'float96'):
#    type_info['float96'] = (numpy.float96, float)
# if hasattr(numpy, 'float128'):
#    type_info['float128'] = (numpy.float128, float)
# if hasattr(numpy, 'complex192'):
#    type_info['complex192'] = (numpy.complex192, complex)
# if hasattr(numpy, 'complex256'):
#    type_info['complex256'] = (numpy.complex256, complex)

sctype_from_type = dict((type_, info[0])
                        for (type_, info) in type_info.iteritems())
"""Maps PyTables types to NumPy scalar types."""
nxtype_from_type = dict((type_, info[1])
                        for (type_, info) in type_info.iteritems())
"""Maps PyTables types to Numexpr types."""

heavy_types = frozenset(['uint8', 'int16', 'uint16', 'float32', 'complex64'])
"""PyTables types to be tested only in heavy mode."""

enum = tables.Enum(dict(('n%d' % i, i) for i in range(_maxnvalue)))
"""Enumerated type to be used in tests."""


# Table description
# -----------------
def append_columns(classdict, shape=()):
    """Append a ``Col`` of each PyTables data type to the `classdict`.

    A column of a certain TYPE gets called ``c_TYPE``.  The number of
    added columns is returned.

    """
    heavy = common.heavy
    for (itype, type_) in enumerate(sorted(type_info.iterkeys())):
        if not heavy and type_ in heavy_types:
            continue  # skip heavy type in non-heavy mode
        colpos = itype + 1
        colname = 'c_%s' % type_
        if type_ == 'enum':
            base = tables.Atom.from_sctype(sctype_from_type[type_])
            col = tables.EnumCol(enum, enum(0), base, shape=shape, pos=colpos)
        else:
            sctype = sctype_from_type[type_]
            dtype = numpy.dtype((sctype, shape))
            col = tables.Col.from_dtype(dtype, pos=colpos)
        classdict[colname] = col
    ncols = colpos
    return ncols


def nested_description(classname, pos, shape=()):
    """Return a nested column description with all PyTables data types.

    A column of a certain TYPE gets called ``c_TYPE``.  The nested
    column will be placed in the position indicated by `pos`.

    """
    classdict = {}
    append_columns(classdict, shape=shape)
    classdict['_v_pos'] = pos
    return type(classname, (tables.IsDescription,), classdict)


def table_description(classname, nclassname, shape=()):
    """Return a table description for testing queries.

    The description consists of all PyTables data types, both in the
    top level and in the ``c_nested`` nested column.  A column of a
    certain TYPE gets called ``c_TYPE``.  An extra integer column
    ``c_extra`` is also provided.  If a `shape` is given, it will be
    used for all columns.  Finally, an extra indexed column
    ``c_idxextra`` is added as well in order to provide some basic
    tests for multi-index queries.

    """
    classdict = {}
    colpos = append_columns(classdict, shape)

    ndescr = nested_description(nclassname, colpos, shape=shape)
    classdict['c_nested'] = ndescr
    colpos += 1

    extracol = tables.IntCol(shape=shape, pos=colpos)
    classdict['c_extra'] = extracol
    colpos += 1

    idxextracol = tables.IntCol(shape=shape, pos=colpos)
    classdict['c_idxextra'] = idxextracol
    colpos += 1

    return type(classname, (tables.IsDescription,), classdict)

TableDescription = table_description(
    'TableDescription', 'NestedDescription')
"""Unidimensional table description for testing queries."""

MDTableDescription = table_description(
    'MDTableDescription', 'MDNestedDescription', shape=md_shape)
"""Multidimensional table description for testing queries."""


# Table data
# ----------
table_data = {}
"""Cached table data for a given shape and number of rows."""
# Data is cached because computing it row by row is quite slow.  Hop!


def fill_table(table, shape, nrows):
    """Fill the given `table` with `nrows` rows of data.

    Values in the i-th row (where 0 <= i < `row_period`) for a
    multidimensional field with M elements span from i to i + M-1.  For
    subsequent rows, values repeat cyclically.

    The same goes for the ``c_extra`` column, but values range from
    -`row_period`/2 to +`row_period`/2.

    """
    # Reuse already computed data if possible.
    tdata = table_data.get((shape, nrows))
    if tdata is not None:
        table.append(tdata)
        table.flush()
        return

    heavy = common.heavy
    size = int(numpy.prod(shape, dtype=SizeType))

    row, value = table.row, 0
    for nrow in xrange(nrows):
        data = numpy.arange(value, value + size).reshape(shape)
        for (type_, sctype) in sctype_from_type.iteritems():
            if not heavy and type_ in heavy_types:
                continue  # skip heavy type in non-heavy mode
            colname = 'c_%s' % type_
            ncolname = 'c_nested/%s' % colname
            if type_ == 'bool':
                coldata = data > (row_period // 2)
            elif type_ == 'string':
                sdata = [str_format % x for x in range(value, value + size)]
                coldata = numpy.array(sdata, dtype=sctype).reshape(shape)
            else:
                coldata = numpy.asarray(data, dtype=sctype)
            row[ncolname] = row[colname] = coldata
            row['c_extra'] = data - (row_period // 2)
            row['c_idxextra'] = data - (row_period // 2)
        row.append()
        value += 1
        if value == row_period:
            value = 0
    table.flush()

    # Make computed data reusable.
    tdata = table.read()
    table_data[(shape, nrows)] = tdata


# Base test cases
# ---------------
class BaseTableQueryTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Base test case for querying tables.

    Sub-classes must define the following attributes:

    ``tableDescription``
        The description of the table to be created.
    ``shape``
        The shape of data fields in the table.
    ``nrows``
        The number of data rows to be generated for the table.

    Sub-classes may redefine the following attributes:

    ``indexed``
        Whether columns shall be indexed, if possible.  Default is not
        to index them.
    ``optlevel``
        The level of optimisation of column indexes.  Default is 0.

    """

    indexed = False
    optlevel = 0

    colNotIndexable_re = re.compile(r"\bcan not be indexed\b")
    condNotBoolean_re = re.compile(r"\bdoes not have a boolean type\b")

    def create_indexes(self, colname, ncolname, extracolname):
        if not self.indexed:
            return
        try:
            kind = self.kind
            vprint("* Indexing ``%s`` columns. Type: %s." % (colname, kind))
            for acolname in [colname, ncolname, extracolname]:
                acolumn = self.table.colinstances[acolname]
                acolumn.create_index(
                    kind=self.kind, optlevel=self.optlevel,
                    _blocksizes=small_blocksizes, _testmode=True)

        except TypeError as te:
            if self.colNotIndexable_re.search(str(te)):
                raise common.SkipTest(
                    "Columns of this type can not be indexed.")
            raise
        except NotImplementedError:
            raise common.SkipTest(
                "Indexing columns of this type is not supported yet.")

    def setUp(self):
        super(BaseTableQueryTestCase, self).setUp()
        self.table = table = self.h5file.create_table(
            '/', 'test', self.tableDescription, expectedrows=self.nrows)
        fill_table(table, self.shape, self.nrows)


class ScalarTableMixin:
    tableDescription = TableDescription
    shape = ()


class MDTableMixin:
    tableDescription = MDTableDescription
    shape = md_shape


# Test cases on query data
# ------------------------
operators = [
    None, '~',
    '<', '<=', '==', '!=', '>=', '>',
    ('<', '<='), ('>', '>=')]
"""Comparison operators to check with different types."""
heavy_operators = frozenset(['~', '<=', '>=', '>', ('>', '>=')])
"""Comparison operators to be tested only in heavy mode."""
left_bound = row_period // 4
"""Operand of left side operator in comparisons with operator pairs."""
right_bound = row_period * 3 // 4
"""Operand of right side operator in comparisons with operator pairs."""
extra_conditions = [
    '',                     # uses one index
    '& ((c_extra + 1) < 0)',  # uses one index
    '| (c_idxextra > 0)',   # uses two indexes
    '| ((c_idxextra > 0) | ((c_extra + 1) > 0))',  # can't use indexes
]
"""Extra conditions to append to comparison conditions."""


class TableDataTestCase(BaseTableQueryTestCase):
    """Base test case for querying table data.

    Automatically created test method names have the format
    ``test_XNNNN``, where ``NNNN`` is the zero-padded test number and
    ``X`` indicates whether the test belongs to the light (``l``) or
    heavy (``h``) set.

    """
    _testfmt_light = 'test_l%04d'
    _testfmt_heavy = 'test_h%04d'


def create_test_method(type_, op, extracond):
    sctype = sctype_from_type[type_]

    # Compute the value of bounds.
    condvars = {'bound': right_bound,
                'lbound': left_bound,
                'rbound': right_bound}
    for (bname, bvalue) in condvars.iteritems():
        if type_ == 'string':
            bvalue = str_format % bvalue
        bvalue = nxtype_from_type[type_](bvalue)
        condvars[bname] = bvalue

    # Compute the name of columns.
    colname = 'c_%s' % type_
    ncolname = 'c_nested/%s' % colname

    # Compute the query condition.
    if not op:  # as is
        cond = colname
    elif op == '~':  # unary
        cond = '~(%s)' % colname
    elif op == '<':  # binary variable-constant
        cond = '%s %s %s' % (colname, op, repr(condvars['bound']))
    elif isinstance(op, tuple):  # double binary variable-constant
        cond = ('(lbound %s %s) & (%s %s rbound)'
                % (op[0], colname, colname, op[1]))
    else:  # binary variable-variable
        cond = '%s %s bound' % (colname, op)
    if extracond:
        cond = '(%s) %s' % (cond, extracond)

    def test_method(self):
        vprint("* Condition is ``%s``." % cond)
        # Replace bitwise operators with their logical counterparts.
        pycond = cond
        for (ptop, pyop) in [('&', 'and'), ('|', 'or'), ('~', 'not')]:
            pycond = pycond.replace(ptop, pyop)
        pycond = compile(pycond, '<string>', 'eval')

        table = self.table
        self.create_indexes(colname, ncolname, 'c_idxextra')

        table_slice = dict(start=1, stop=table.nrows - 5, step=3)
        rownos, fvalues = None, None
        # Test that both simple and nested columns work as expected.
        # Knowing how the table is filled, results must be the same.
        for acolname in [colname, ncolname]:
            # First the reference Python version.
            pyrownos, pyfvalues, pyvars = [], [], condvars.copy()
            for row in table.iterrows(**table_slice):
                pyvars[colname] = row[acolname]
                pyvars['c_extra'] = row['c_extra']
                pyvars['c_idxextra'] = row['c_idxextra']
                try:
                    isvalidrow = eval(pycond, {}, pyvars)
                except TypeError:
                    raise common.SkipTest(
                        "The Python type does not support the operation.")
                if isvalidrow:
                    pyrownos.append(row.nrow)
                    pyfvalues.append(row[acolname])
            pyrownos = numpy.array(pyrownos)  # row numbers already sorted
            pyfvalues = numpy.array(pyfvalues, dtype=sctype)
            pyfvalues.sort()
            vprint("* %d rows selected by Python from ``%s``."
                   % (len(pyrownos), acolname))
            if rownos is None:
                rownos = pyrownos  # initialise reference results
                fvalues = pyfvalues
            else:
                self.assertTrue(numpy.all(pyrownos == rownos))  # check
                self.assertTrue(numpy.all(pyfvalues == fvalues))

            # Then the in-kernel or indexed version.
            ptvars = condvars.copy()
            ptvars[colname] = table.colinstances[acolname]
            ptvars['c_extra'] = table.colinstances['c_extra']
            ptvars['c_idxextra'] = table.colinstances['c_idxextra']
            try:
                isidxq = table.will_query_use_indexing(cond, ptvars)
                # Query twice to trigger possible query result caching.
                ptrownos = [table.get_where_list(cond, condvars, sort=True,
                                                 **table_slice)
                            for _ in range(2)]
                ptfvalues = [
                    table.read_where(cond, condvars, field=acolname,
                                     **table_slice)
                    for _ in range(2)
                ]
            except TypeError as te:
                if self.condNotBoolean_re.search(str(te)):
                    raise common.SkipTest("The condition is not boolean.")
                raise
            except NotImplementedError:
                raise common.SkipTest(
                    "The PyTables type does not support the operation.")
            for ptfvals in ptfvalues:  # row numbers already sorted
                ptfvals.sort()
            vprint("* %d rows selected by PyTables from ``%s``"
                   % (len(ptrownos[0]), acolname), nonl=True)
            vprint("(indexing: %s)." % ["no", "yes"][bool(isidxq)])
            self.assertTrue(numpy.all(ptrownos[0] == rownos))
            self.assertTrue(numpy.all(ptfvalues[0] == fvalues))
            # The following test possible caching of query results.
            self.assertTrue(numpy.all(ptrownos[0] == ptrownos[1]))
            self.assertTrue(numpy.all(ptfvalues[0] == ptfvalues[1]))

    test_method.__doc__ = "Testing ``%s``." % cond
    return test_method

# Create individual tests.  You may restrict which tests are generated
# by replacing the sequences in the ``for`` statements.  For instance:
testn = 0
for type_ in type_info:  # for type_ in ['string']:
    for op in operators:  # for op in ['!=']:
        # Decide to which set the test belongs.
        heavy = type_ in heavy_types or op in heavy_operators
        if heavy:
            testfmt = TableDataTestCase._testfmt_heavy
            numfmt = ' [#H%d]'
        else:
            testfmt = TableDataTestCase._testfmt_light
            numfmt = ' [#L%d]'
        for extracond in extra_conditions:  # for extracond in ['']:
            tmethod = create_test_method(type_, op, extracond)
            # The test number is appended to the docstring to help
            # identify failing methods in non-verbose mode.
            tmethod.__name__ = testfmt % testn
            # tmethod.__doc__ += numfmt % testn
            tmethod.__doc__ += testfmt % testn
            ptmethod = common.pyTablesTest(tmethod)
            if sys.version_info[0] < 3:
                imethod = types.MethodType(ptmethod, None, TableDataTestCase)
            else:
                imethod = ptmethod
            setattr(TableDataTestCase, tmethod.__name__, imethod)
            testn += 1


# Base classes for non-indexed queries.
NX_BLOCK_SIZE1 = 128  # from ``interpreter.c`` in Numexpr
NX_BLOCK_SIZE2 = 8  # from ``interpreter.c`` in Numexpr


class SmallNITableMixin:
    nrows = row_period * 2
    assert NX_BLOCK_SIZE2 < nrows < NX_BLOCK_SIZE1
    assert nrows % NX_BLOCK_SIZE2 != 0  # to have some residual rows


class BigNITableMixin:
    nrows = row_period * 3
    assert nrows > NX_BLOCK_SIZE1 + NX_BLOCK_SIZE2
    assert nrows % NX_BLOCK_SIZE1 != 0
    assert nrows % NX_BLOCK_SIZE2 != 0  # to have some residual rows

# Parameters for non-indexed queries.
table_sizes = ['Small', 'Big']
heavy_table_sizes = frozenset(['Big'])
table_ndims = ['Scalar']  # to enable multidimensional testing, include 'MD'

# Non-indexed queries: ``[SB][SM]TDTestCase``, where:
#
# 1. S is for small and B is for big size table.
#    Sizes are listed in `table_sizes`.
# 2. S is for scalar and M for multidimensional columns.
#    Dimensionalities are listed in `table_ndims`.


def niclassdata():
    for size in table_sizes:
        heavy = size in heavy_table_sizes
        for ndim in table_ndims:
            classname = '%s%sTDTestCase' % (size[0], ndim[0])
            cbasenames = ('%sNITableMixin' % size, '%sTableMixin' % ndim,
                          'TableDataTestCase')
            classdict = dict(heavy=heavy)
            yield (classname, cbasenames, classdict)


# Base classes for the different type index.
class UltraLightITableMixin:
    kind = "ultralight"


class LightITableMixin:
    kind = "light"


class MediumITableMixin:
    kind = "medium"


class FullITableMixin:
    kind = "full"

# Base classes for indexed queries.


class SmallSTableMixin:
    nrows = 50


class MediumSTableMixin:
    nrows = 100


class BigSTableMixin:
    nrows = 500

# Parameters for indexed queries.
ckinds = ['UltraLight', 'Light', 'Medium', 'Full']
itable_sizes = ['Small', 'Medium', 'Big']
heavy_itable_sizes = frozenset(['Medium', 'Big'])
itable_optvalues = [0, 1, 3, 7, 9]
heavy_itable_optvalues = frozenset([0, 1, 7, 9])

# Indexed queries: ``[SMB]I[ulmf]O[01379]TDTestCase``, where:
#
# 1. S is for small, M for medium and B for big size table.
#    Sizes are listed in `itable_sizes`.
# 2. U is for 'ultraLight', L for 'light', M for 'medium', F for 'Full' indexes
#    Index types are listed in `ckinds`.
# 3. 0 to 9 is the desired index optimization level.
#    Optimizations are listed in `itable_optvalues`.


def iclassdata():
    for ckind in ckinds:
        for size in itable_sizes:
            for optlevel in itable_optvalues:
                heavy = (optlevel in heavy_itable_optvalues
                         or size in heavy_itable_sizes)
                classname = '%sI%sO%dTDTestCase' % (
                    size[0], ckind[0], optlevel)
                cbasenames = ('%sSTableMixin' % size,
                              '%sITableMixin' % ckind,
                              'ScalarTableMixin',
                              'TableDataTestCase')
                classdict = dict(heavy=heavy, optlevel=optlevel, indexed=True)
                yield (classname, cbasenames, classdict)


# Create test classes.
for cdatafunc in [niclassdata, iclassdata]:
    for (cname, cbasenames, cdict) in cdatafunc():
        cbases = tuple(eval(cbase) for cbase in cbasenames)
        class_ = type(cname, cbases, cdict)
        exec('%s = class_' % cname)


# Test cases on query usage
# -------------------------
class BaseTableUsageTestCase(BaseTableQueryTestCase):
    nrows = row_period

_gvar = None
"""Use this when a global variable is needed."""


class ScalarTableUsageTestCase(ScalarTableMixin, BaseTableUsageTestCase):

    """Test case for query usage on scalar tables.

    This also tests for most usage errors and situations.

    """

    def test_empty_condition(self):
        """Using an empty condition."""
        self.assertRaises(SyntaxError, self.table.where, '')

    def test_syntax_error(self):
        """Using a condition with a syntax error."""
        self.assertRaises(SyntaxError, self.table.where, 'foo bar')

    def test_unsupported_object(self):
        """Using a condition with an unsupported object."""
        self.assertRaises(TypeError, self.table.where, '[]')
        self.assertRaises(TypeError, self.table.where, 'obj', {'obj': {}})
        self.assertRaises(TypeError, self.table.where, 'c_bool < []')

    def test_unsupported_syntax(self):
        """Using a condition with unsupported syntax."""
        self.assertRaises(TypeError, self.table.where, 'c_bool[0]')
        self.assertRaises(TypeError, self.table.where, 'c_bool()')
        self.assertRaises(NameError, self.table.where, 'c_bool.__init__')

    def test_no_column(self):
        """Using a condition with no participating columns."""
        self.assertRaises(ValueError, self.table.where, 'True')

    def test_foreign_column(self):
        """Using a condition with a column from other table."""
        table2 = self.h5file.create_table('/', 'other', self.tableDescription)
        self.assertRaises(ValueError, self.table.where,
                          'c_int32_a + c_int32_b > 0',
                          {'c_int32_a': self.table.cols.c_int32,
                           'c_int32_b': table2.cols.c_int32})

    def test_unsupported_op(self):
        """Using a condition with unsupported operations on types."""
        NIE = NotImplementedError
        self.assertRaises(NIE, self.table.where, 'c_complex128 > 0j')
        if sys.version_info[0] < 3:
            self.assertRaises(NIE, self.table.where, 'c_string + "a" > "abc"')
        else:
            self.assertRaises(NIE, self.table.where,
                              'c_string + b"a" > b"abc"')

    def test_not_boolean(self):
        """Using a non-boolean condition."""
        self.assertRaises(TypeError, self.table.where, 'c_int32')

    def test_nested_col(self):
        """Using a condition with nested columns."""
        self.assertRaises(TypeError, self.table.where, 'c_nested')

    def test_implicit_col(self):
        """Using implicit column names in conditions."""
        # If implicit columns didn't work, a ``NameError`` would be raised.
        self.assertRaises(TypeError, self.table.where, 'c_int32')
        # If overriding didn't work, no exception would be raised.
        self.assertRaises(TypeError, self.table.where,
                          'c_bool', {'c_bool': self.table.cols.c_int32})
        # External variables do not override implicit columns.

        def where_with_locals():
            c_int32 = self.table.cols.c_bool  # this wouldn't cause an error
            self.assertTrue(c_int32 is not None)
            self.table.where('c_int32')
        self.assertRaises(TypeError, where_with_locals)

    def test_condition_vars(self):
        """Using condition variables in conditions."""

        # If condition variables didn't work, a ``NameError`` would be raised.
        self.assertRaises(NotImplementedError, self.table.where,
                          'c_string > bound', {'bound': 0})

        def where_with_locals():
            bound = 'foo'  # this wouldn't cause an error
            self.table.where('c_string > bound', {'bound': 0})
        self.assertRaises(NotImplementedError, where_with_locals)

        def where_with_globals():
            global _gvar
            _gvar = 'foo'  # this wouldn't cause an error
            try:
                self.table.where('c_string > _gvar', {'_gvar': 0})
            finally:
                del _gvar  # to keep global namespace clean
        self.assertRaises(NotImplementedError, where_with_globals)

    def test_scopes(self):
        """Looking up different scopes for variables."""

        # Make sure the variable is not implicit.
        self.assertRaises(NameError, self.table.where, 'col')

        # First scope: dictionary of condition variables.
        self.assertRaises(TypeError, self.table.where,
                          'col', {'col': self.table.cols.c_int32})

        # Second scope: local variables.
        def where_whith_locals():
            col = self.table.cols.c_int32
            self.assertTrue(col is not None)
            self.table.where('col')
        self.assertRaises(TypeError, where_whith_locals)

        # Third scope: global variables.
        def where_with_globals():
            global _gvar
            _gvar = self.table.cols.c_int32
            try:
                self.table.where('_gvar')
            finally:
                del _gvar  # to keep global namespace clean
        self.assertRaises(TypeError, where_with_globals)


class MDTableUsageTestCase(MDTableMixin, BaseTableUsageTestCase):

    """Test case for query usage on multidimensional tables."""

    def test(self):
        """Using a condition on a multidimensional table."""
        # Easy: queries on multidimensional tables are not implemented yet!
        self.assertRaises(NotImplementedError, self.table.where, 'c_bool')


class IndexedTableUsage(ScalarTableMixin, BaseTableUsageTestCase):

    """Test case for query usage on indexed tables.

    Indexing could be used in more cases, but it is expected to kick in
    at least in the cases tested here.

    """
    nrows = 50
    indexed = True

    def setUp(self):
        super(IndexedTableUsage, self).setUp()
        self.table.cols.c_bool.create_index(_blocksizes=small_blocksizes)
        self.table.cols.c_int32.create_index(_blocksizes=small_blocksizes)
        self.will_query_use_indexing = self.table.will_query_use_indexing
        self.compileCondition = self.table._compile_condition
        self.requiredExprVars = self.table._required_expr_vars
        usable_idxs = set()
        for expr in self.idx_expr:
            idxvar = expr[0]
            if idxvar not in usable_idxs:
                usable_idxs.add(idxvar)
        self.usable_idxs = frozenset(usable_idxs)

    def test(self):
        for condition in self.conditions:
            c_usable_idxs = self.will_query_use_indexing(condition, {})
            self.assertEqual(c_usable_idxs, self.usable_idxs,
                             "\nQuery with condition: ``%s``\n"
                             "Computed usable indexes are: ``%s``\n"
                             "and should be: ``%s``" %
                            (condition, c_usable_idxs, self.usable_idxs))
            condvars = self.requiredExprVars(condition, None)
            compiled = self.compileCondition(condition, condvars)
            c_idx_expr = compiled.index_expressions
            self.assertEqual(c_idx_expr, self.idx_expr,
                             "\nWrong index expression in condition:\n``%s``\n"
                             "Compiled index expression is:\n``%s``\n"
                             "and should be:\n``%s``" %
                            (condition, c_idx_expr, self.idx_expr))
            c_str_expr = compiled.string_expression
            self.assertEqual(c_str_expr, self.str_expr,
                             "\nWrong index operations in condition:\n``%s``\n"
                             "Computed index operations are:\n``%s``\n"
                             "and should be:\n``%s``" %
                            (condition, c_str_expr, self.str_expr))
            vprint("* Query with condition ``%s`` will use "
                   "variables ``%s`` for indexing."
                   % (condition, compiled.index_variables))


class IndexedTableUsage1(IndexedTableUsage):
    conditions = [
        '(c_int32 > 0)',
        '(c_int32 > 0) & (c_extra > 0)',
        '(c_int32 > 0) & ((~c_bool) | (c_extra > 0))',
        '(c_int32 > 0) & ((c_extra < 3) & (c_extra > 0))',
    ]
    idx_expr = [('c_int32', ('gt',), (0,))]
    str_expr = 'e0'


class IndexedTableUsage2(IndexedTableUsage):
    conditions = [
        '(c_int32 > 0) & (c_int32 < 5)',
        '(c_int32 > 0) & (c_int32 < 5) & (c_extra > 0)',
        '(c_int32 > 0) & (c_int32 < 5) & ((c_bool == True) | (c_extra > 0))',
        '(c_int32 > 0) & (c_int32 < 5) & ((c_extra > 0) | (c_bool == True))',
    ]
    idx_expr = [('c_int32', ('gt', 'lt'), (0, 5))]
    str_expr = 'e0'


class IndexedTableUsage3(IndexedTableUsage):
    conditions = [
        '(c_bool == True)',
        '(c_bool == True) & (c_extra > 0)',
        '(c_extra > 0) & (c_bool == True)',
        '((c_extra > 0) & (c_extra < 4)) & (c_bool == True)',
        '(c_bool == True) & ((c_extra > 0) & (c_extra < 4))',
    ]
    idx_expr = [('c_bool', ('eq',), (True,))]
    str_expr = 'e0'


class IndexedTableUsage4(IndexedTableUsage):
    conditions = [
        '((c_int32 > 0) & (c_bool == True)) & (c_extra > 0)',
        '((c_int32 > 0) & (c_bool == True)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('gt',), (0,)),
                ('c_bool', ('eq',), (True,)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage5(IndexedTableUsage):
    conditions = [
        '(c_int32 >= 1) & (c_int32 < 2) & (c_bool == True)',
        '(c_int32 >= 1) & (c_int32 < 2) & (c_bool == True)' +
        ' & (c_extra > 0)',
    ]
    idx_expr = [('c_int32', ('ge', 'lt'), (1, 2)),
                ('c_bool', ('eq',), (True,)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage6(IndexedTableUsage):
    conditions = [
        '(c_int32 >= 1) & (c_int32 < 2) & (c_int32 > 0) & (c_int32 < 5)',
        '(c_int32 >= 1) & (c_int32 < 2) & (c_int32 > 0) & (c_int32 < 5)' +
        ' & (c_extra > 0)',
    ]
    idx_expr = [('c_int32', ('ge', 'lt'), (1, 2)),
                ('c_int32', ('gt',), (0,)),
                ('c_int32', ('lt',), (5,)),
                ]
    str_expr = '((e0 & e1) & e2)'


class IndexedTableUsage7(IndexedTableUsage):
    conditions = [
        '(c_int32 >= 1) & (c_int32 < 2) & ((c_int32 > 0) & (c_int32 < 5))',
        '((c_int32 >= 1) & (c_int32 < 2)) & ((c_int32 > 0) & (c_int32 < 5))',
        '((c_int32 >= 1) & (c_int32 < 2)) & ((c_int32 > 0) & (c_int32 < 5))' +
        ' & (c_extra > 0)',
    ]
    idx_expr = [('c_int32', ('ge', 'lt'), (1, 2)),
                ('c_int32', ('gt', 'lt'), (0, 5)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage8(IndexedTableUsage):
    conditions = [
        '(c_extra > 0) & ((c_int32 > 0) & (c_int32 < 5))',
    ]
    idx_expr = [('c_int32', ('gt', 'lt'), (0, 5)),
                ]
    str_expr = 'e0'


class IndexedTableUsage9(IndexedTableUsage):
    conditions = [
        '(c_extra > 0) & (c_int32 > 0) & (c_int32 < 5)',
        '((c_extra > 0) & (c_int32 > 0)) & (c_int32 < 5)',
        '(c_extra > 0) & (c_int32 > 0) & (c_int32 < 5) & (c_extra > 3)',
    ]
    idx_expr = [('c_int32', ('gt',), (0,)),
                ('c_int32', ('lt',), (5,))]
    str_expr = '(e0 & e1)'


class IndexedTableUsage10(IndexedTableUsage):
    conditions = [
        '(c_int32 < 5) & (c_extra > 0) & (c_bool == True)',
        '(c_int32 < 5) & (c_extra > 2) & c_bool',
        '(c_int32 < 5) & (c_bool == True) & (c_extra > 0) & (c_extra < 4)',
        '(c_int32 < 5) & (c_extra > 0) & (c_bool == True) & (c_extra < 4)',
    ]
    idx_expr = [('c_int32', ('lt',), (5,)),
                ('c_bool', ('eq',), (True,))]
    str_expr = '(e0 & e1)'


class IndexedTableUsage11(IndexedTableUsage):
    """Complex operations are not eligible for indexing."""
    conditions = [
        'sin(c_int32) > 0',
        '(c_int32 * 2.4) > 0',
        '(c_int32 + c_int32) > 0',
        'c_int32**2 > 0',
    ]
    idx_expr = []
    str_expr = ''


class IndexedTableUsage12(IndexedTableUsage):
    conditions = [
        '~c_bool',
        '~(c_bool)',
        '~c_bool & (c_extra > 0)',
        '~(c_bool) & (c_extra > 0)',
    ]
    idx_expr = [('c_bool', ('eq',), (False,))]
    str_expr = 'e0'


class IndexedTableUsage13(IndexedTableUsage):
    conditions = [
        '~(c_bool == True)',
        '~((c_bool == True))',
        '~(c_bool == True) & (c_extra > 0)',
        '~((c_bool == True)) & (c_extra > 0)',
    ]
    idx_expr = [('c_bool', ('eq',), (False,))]
    str_expr = 'e0'


class IndexedTableUsage14(IndexedTableUsage):
    conditions = [
        '~(c_int32 > 0)',
        '~((c_int32 > 0)) & (c_extra > 0)',
        '~(c_int32 > 0) & ((~c_bool) | (c_extra > 0))',
        '~(c_int32 > 0) & ((c_extra < 3) & (c_extra > 0))',
    ]
    idx_expr = [('c_int32', ('le',), (0,))]
    str_expr = 'e0'


class IndexedTableUsage15(IndexedTableUsage):
    conditions = [
        '(~(c_int32 > 0) | ~c_bool)',
        '(~(c_int32 > 0) | ~(c_bool)) & (c_extra > 0)',
        '(~(c_int32 > 0) | ~(c_bool == True)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('le',), (0,)),
                ('c_bool', ('eq',), (False,)),
                ]
    str_expr = '(e0 | e1)'


class IndexedTableUsage16(IndexedTableUsage):
    conditions = [
        '(~(c_int32 > 0) & ~(c_int32 < 2))',
        '(~(c_int32 > 0) & ~(c_int32 < 2)) & (c_extra > 0)',
        '(~(c_int32 > 0) & ~(c_int32 < 2)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('le',), (0,)),
                ('c_int32', ('ge',), (2,)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage17(IndexedTableUsage):
    conditions = [
        '(~(c_int32 > 0) & ~(c_int32 < 2))',
        '(~(c_int32 > 0) & ~(c_int32 < 2)) & (c_extra > 0)',
        '(~(c_int32 > 0) & ~(c_int32 < 2)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('le',), (0,)),
                ('c_int32', ('ge',), (2,)),
                ]
    str_expr = '(e0 & e1)'

# Negations of complex conditions are not supported yet


class IndexedTableUsage18(IndexedTableUsage):
    conditions = [
        '~((c_int32 > 0) & (c_bool))',
        '~((c_int32 > 0) & (c_bool)) & (c_extra > 0)',
        '~((c_int32 > 0) & (c_bool)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = []
    str_expr = ''


class IndexedTableUsage19(IndexedTableUsage):
    conditions = [
        '~((c_int32 > 0) & (c_bool)) & ((c_bool == False)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_bool', ('eq',), (False,)),
                ]
    str_expr = 'e0'


class IndexedTableUsage20(IndexedTableUsage):
    conditions = [
        '((c_int32 > 0) & ~(c_bool))',
        '((c_int32 > 0) & ~(c_bool)) & (c_extra > 0)',
        '((c_int32 > 0) & ~(c_bool == True)) & ((c_extra > 0) & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('gt',), (0,)),
                ('c_bool', ('eq',), (False,)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage21(IndexedTableUsage):
    conditions = [
        '(~(c_int32 > 0) & (c_bool))',
        '(~(c_int32 > 0) & (c_bool)) & (c_extra > 0)',
        '(~(c_int32 > 0) & (c_bool == True)) & ((c_extra > 0)' +
        ' & (c_extra < 4))',
    ]
    idx_expr = [('c_int32', ('le',), (0,)),
                ('c_bool', ('eq',), (True,)),
                ]
    str_expr = '(e0 & e1)'


class IndexedTableUsage22(IndexedTableUsage):
    conditions = [
        '~((c_int32 >= 1) & (c_int32 < 2)) & ~(c_bool == True)',
        '~(c_bool == True) & (c_extra > 0)',
        '~((c_int32 >= 1) & (c_int32 < 2)) & (~(c_bool == True)' +
        ' & (c_extra > 0))',
    ]
    idx_expr = [('c_bool', ('eq',), (False,)),
                ]
    str_expr = 'e0'


class IndexedTableUsage23(IndexedTableUsage):
    conditions = [
        'c_int32 != 1',
        'c_bool != False',
        '~(c_int32 != 1)',
        '~(c_bool != False)',
        '(c_int32 != 1) & (c_extra != 2)',
    ]
    idx_expr = []
    str_expr = ''


class IndexedTableUsage24(IndexedTableUsage):
    conditions = [
        'c_bool',
        'c_bool == True',
        'True == c_bool',
        '~(~c_bool)',
        '~~c_bool',
        '~~~~c_bool',
        '~(~c_bool) & (c_extra != 2)',
    ]
    idx_expr = [('c_bool', ('eq',), (True,)),
                ]
    str_expr = 'e0'


class IndexedTableUsage25(IndexedTableUsage):
    conditions = [
        '~c_bool',
        'c_bool == False',
        'False == c_bool',
        '~(c_bool)',
        '~((c_bool))',
        '~~~c_bool',
        '~~(~c_bool) & (c_extra != 2)',
    ]
    idx_expr = [
        ('c_bool', ('eq',), (False,)),
    ]
    str_expr = 'e0'


class IndexedTableUsage26(IndexedTableUsage):
    conditions = [
        'c_bool != True',
        'True != c_bool',
        'c_bool != False',
        'False != c_bool',
        ]
    idx_expr = []
    str_expr = ''


class IndexedTableUsage27(IndexedTableUsage):
    conditions = [
        '(c_int32 == 3) | c_bool | (c_int32 == 5)',
        '(((c_int32 == 3) | (c_bool == True)) | (c_int32 == 5))' +
        ' & (c_extra > 0)',
        ]
    idx_expr = [
        ('c_int32', ('eq',), (3,)),
        ('c_bool', ('eq',), (True,)),
        ('c_int32', ('eq',), (5,)),
    ]
    str_expr = '((e0 | e1) | e2)'


class IndexedTableUsage28(IndexedTableUsage):
    conditions = [
        '((c_int32 == 3) | c_bool) & (c_int32 == 5)',
        '(((c_int32 == 3) | (c_bool == True)) & (c_int32 == 5))' +
        ' & (c_extra > 0)',
        ]
    idx_expr = [
        ('c_int32', ('eq',), (3,)),
        ('c_bool', ('eq',), (True,)),
        ('c_int32', ('eq',), (5,)),
    ]
    str_expr = '((e0 | e1) & e2)'


class IndexedTableUsage29(IndexedTableUsage):
    conditions = [
        '(c_int32 == 3) | ((c_int32 == 4) & (c_int32 == 5))',
        '((c_int32 == 3) | ((c_int32 == 4) & (c_int32 == 5)))' +
        ' & (c_extra > 0)',
        ]
    idx_expr = [
        ('c_int32', ('eq',), (4,)),
        ('c_int32', ('eq',), (5,)),
        ('c_int32', ('eq',), (3,)),
    ]
    str_expr = '((e0 & e1) | e2)'


class IndexedTableUsage30(IndexedTableUsage):
    conditions = [
        '((c_int32 == 3) | (c_int32 == 4)) & (c_int32 == 5)',
        '((c_int32 == 3) | (c_int32 == 4)) & (c_int32 == 5)' +
        ' & (c_extra > 0)',
        ]
    idx_expr = [
        ('c_int32', ('eq',), (3,)),
        ('c_int32', ('eq',), (4,)),
        ('c_int32', ('eq',), (5,)),
    ]
    str_expr = '((e0 | e1) & e2)'


class IndexedTableUsage31(IndexedTableUsage):
    conditions = [
        '(c_extra > 0) & ((c_extra < 4) & (c_bool == True))',
        '(c_extra > 0) & ((c_bool == True) & (c_extra < 5))',
        '((c_int32 > 0) | (c_extra > 0)) & (c_bool == True)',
        ]
    idx_expr = [
        ('c_bool', ('eq',), (True,)),
    ]
    str_expr = 'e0'


class IndexedTableUsage32(IndexedTableUsage):
    conditions = [
        '(c_int32 < 5) & (c_extra > 0) & (c_bool == True) | (c_extra < 4)',
        ]
    idx_expr = []
    str_expr = ''


# Main part
# ---------
def suite():
    """Return a test suite consisting of all the test cases in the module."""

    testSuite = unittest.TestSuite()

    cdatafuncs = [niclassdata]  # non-indexing data tests
    cdatafuncs.append(iclassdata)  # indexing data tests

    heavy = common.heavy
    # Choose which tests to run in classes with autogenerated tests.
    if heavy:
        autoprefix = 'test'  # all tests
    else:
        autoprefix = 'test_l'  # only light tests

    niter = 1
    for i in range(niter):
        # Tests on query data.
        for cdatafunc in cdatafuncs:
            for cdata in cdatafunc():
                class_ = eval(cdata[0])
                if heavy or not class_.heavy:
                    suite_ = unittest.makeSuite(class_, prefix=autoprefix)
                    testSuite.addTest(suite_)
        # Tests on query usage.
        testSuite.addTest(unittest.makeSuite(ScalarTableUsageTestCase))
        testSuite.addTest(unittest.makeSuite(MDTableUsageTestCase))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage1))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage2))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage3))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage4))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage5))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage6))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage7))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage8))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage9))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage10))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage11))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage12))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage13))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage14))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage15))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage16))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage17))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage18))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage19))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage20))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage21))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage22))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage23))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage24))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage25))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage26))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage27))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage28))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage29))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage30))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage31))
        testSuite.addTest(unittest.makeSuite(IndexedTableUsage32))

    return testSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_tables
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile
import warnings

import numpy as np
from numpy import rec as records
from numpy import testing as npt

import tables
from tables import *
from tables.utils import SizeType, byteorders
from tables.tests import common
from tables.tests.common import allequal, areArraysEqual
from tables.description import descr_from_dtype

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

# Test Record class


class Record(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"abcd", pos=0)  # 4-character String
    var2 = IntCol(dflt=1, pos=1)                   # integer
    var3 = Int16Col(dflt=2, pos=2)                 # short integer
    var4 = Float64Col(dflt=3.1, pos=3)             # double (double-precision)
    var5 = Float32Col(dflt=4.2, pos=4)             # float  (single-precision)
    var6 = UInt16Col(dflt=5, pos=5)                # unsigned short integer
    var7 = StringCol(itemsize=1, dflt=b"e", pos=6)  # 1-character String
    var8 = BoolCol(dflt=True, pos=7)               # boolean
    var9 = ComplexCol(itemsize=8, dflt=(
        0.+1.j), pos=8)  # Complex single precision
    var10 = ComplexCol(itemsize=16, dflt=(
        1.-0.j), pos=9)  # Complex double precision
    if 'Float16Col' in globals():
        var11 = Float16Col(dflt=6.4)               # float  (half-precision)
    if 'Float96Col' in globals():
        var12 = Float96Col(
            dflt=6.4)               # float  (extended precision)
    if 'Float128Col' in globals():
        var13 = Float128Col(
            dflt=6.4)              # float  (extended precision)
    if 'Complex192Col' in globals():
        var14 = ComplexCol(itemsize=24, dflt=(
            1.-0.j))  # Complex double (extended precision)
    if 'Complex256Col' in globals():
        var15 = ComplexCol(itemsize=32, dflt=(
            1.-0.j))  # Complex double (extended precision)

#  Dictionary definition
RecordDescriptionDict = {
    'var1': StringCol(itemsize=4, dflt=b"abcd", pos=0),  # 4-character String
    'var2': IntCol(dflt=1, pos=1),              # integer
    'var3': Int16Col(dflt=2, pos=2),            # short integer
    'var4': Float64Col(dflt=3.1, pos=3),        # double (double-precision)
    'var5': Float32Col(dflt=4.2, pos=4),        # float  (single-precision)
    'var6': UInt16Col(dflt=5, pos=5),           # unsigned short integer
    'var7': StringCol(itemsize=1, dflt=b"e", pos=6),  # 1-character String
    'var8': BoolCol(dflt=True, pos=7),          # boolean
    'var9': ComplexCol(itemsize=8, dflt=(0.+1.j), pos=8),
                                                # Complex single precision
    'var10': ComplexCol(itemsize=16, dflt=(1.-0.j), pos=9),
                                                # Complex double precision
}

if 'Float16Col' in globals():
    RecordDescriptionDict['var11'] = Float16Col(
        dflt=6.4)    # float  (half-precision)
if 'Float96Col' in globals():
    RecordDescriptionDict['var12'] = Float96Col(
        dflt=6.4)    # float  (extended precision)
if 'Float128Col' in globals():
    RecordDescriptionDict['var13'] = Float128Col(
        dflt=6.4)   # float  (extended precision)
if 'Complex192Col' in globals():
    RecordDescriptionDict['var14'] = ComplexCol(itemsize=24, dflt=(
        1.-0.j))  # Complex double (extended precision)
if 'Complex256Col' in globals():
    RecordDescriptionDict['var15'] = ComplexCol(itemsize=32, dflt=(
        1.-0.j))  # Complex double (extended precision)


# Old fashion of defining tables (for testing backward compatibility)
class OldRecord(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"abcd", pos=0)
    var2 = Col.from_type("int32", (), 1, pos=1)
    var3 = Col.from_type("int16", (), 2, pos=2)
    var4 = Col.from_type("float64", (), 3.1, pos=3)
    var5 = Col.from_type("float32", (), 4.2, pos=4)
    var6 = Col.from_type("uint16", (), 5, pos=5)
    var7 = StringCol(itemsize=1, dflt=b"e", pos=6)
    var8 = Col.from_type("bool", shape=(), dflt=1, pos=7)
    var9 = ComplexCol(itemsize=8, shape=(), dflt=(0.+1.j), pos=8)
    var10 = ComplexCol(itemsize=16, shape=(), dflt=(1.-0.j), pos = 9)
    if 'Float16Col' in globals():
        var11 = Col.from_type("float16", (), 6.4)
    if 'Float96Col' in globals():
        var12 = Col.from_type("float96", (), 6.4)
    if 'Float128Col' in globals():
        var13 = Col.from_type("float128", (), 6.4)
    if 'Complex192Col' in globals():
        var14 = ComplexCol(itemsize=24, shape=(), dflt=(1.-0.j))
    if 'Complex256Col' in globals():
        var15 = ComplexCol(itemsize=32, shape=(), dflt=(1.-0.j))


class BasicTestCase(common.PyTablesTestCase):
    # file  = "test.h5"
    mode = "w"
    title = "This is the table title"
    expectedrows = 100
    appendrows = 20
    compress = 0
    shuffle = 0
    fletcher32 = 0
    complib = "zlib"  # Default compression library
    record = Record
    recarrayinit = 0
    maxshort = 1 << 15

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def initRecArray(self):
        record = self.recordtemplate
        row = record[0]
        buflist = []
        # Fill the recarray
        for i in xrange(self.expectedrows):
            tmplist = []
            var1 = '%04d' % (self.expectedrows - i)
            tmplist.append(var1)
            var2 = i
            tmplist.append(var2)
            var3 = i % self.maxshort
            tmplist.append(var3)
            if isinstance(row['var4'], np.ndarray):
                tmplist.append([float(i), float(i * i)])
            else:
                tmplist.append(float(i))
            if isinstance(row['var5'], np.ndarray):
                tmplist.append(np.array((float(i),)*4))
            else:
                tmplist.append(float(i))
            # var6 will be like var3 but byteswaped
            tmplist.append(((var3 >> 8) & 0xff) + ((var3 << 8) & 0xff00))
            var7 = var1[-1]
            tmplist.append(var7)
            if isinstance(row['var8'], np.ndarray):
                tmplist.append([0, 10])  # should be equivalent to [0,1]
            else:
                tmplist.append(10)  # should be equivalent to 1
            if isinstance(row['var9'], np.ndarray):
                tmplist.append([0.+float(i)*1j, float(i)+0.j])
            else:
                tmplist.append(float(i)+0j)
            if isinstance(row['var10'], np.ndarray):
                tmplist.append([float(i)+0j, 1 + float(i)*1j])
            else:
                tmplist.append(1 + float(i)*1j)
            if 'Float16Col' in globals():
                if isinstance(row['var11'], np.ndarray):
                    tmplist.append(np.array((float(i),)*4))
                else:
                    tmplist.append(float(i))
            if 'Float96Col' in globals():
                if isinstance(row['var12'], np.ndarray):
                    tmplist.append(np.array((float(i),)*4))
                else:
                    tmplist.append(float(i))
            if 'Float128Col' in globals():
                if isinstance(row['var13'], np.ndarray):
                    tmplist.append(np.array((float(i),)*4))
                else:
                    tmplist.append(float(i))
            if 'Complex192Col' in globals():
                if isinstance(row['var14'], np.ndarray):
                    tmplist.append([float(i)+0j, 1 + float(i)*1j])
                else:
                    tmplist.append(1 + float(i)*1j)
            if 'Complex256Col' in globals():
                if isinstance(row['var15'], np.ndarray):
                    tmplist.append([float(i)+0j, 1 + float(i)*1j])
                else:
                    tmplist.append(1 + float(i)*1j)

            buflist.append(tmplist)

        self.record = records.array(buflist, dtype=record.dtype,
                                    shape=self.expectedrows)

    def populateFile(self):
        group = self.rootgroup
        if self.recarrayinit:
            # Initialize an starting buffer, if any
            self.initRecArray()
        for j in range(3):
            # Create a table
            filterprops = Filters(complevel=self.compress,
                                  shuffle=self.shuffle,
                                  fletcher32=self.fletcher32,
                                  complib=self.complib)
            if j < 2:
                byteorder = sys.byteorder
            else:
                # table2 will be byteswapped
                byteorder = {"little": "big", "big": "little"}[sys.byteorder]
            table = self.fileh.create_table(group, 'table'+str(j), self.record,
                                            title=self.title,
                                            filters=filterprops,
                                            expectedrows=self.expectedrows,
                                            byteorder=byteorder)
            if not self.recarrayinit:
                # Get the row object associated with the new table
                row = table.row
                # Fill the table
                for i in xrange(self.expectedrows):
                    s = '%04d' % (self.expectedrows - i)
                    row['var1'] = s.encode('ascii')
                    row['var7'] = s[-1].encode('ascii')
                    # row['var7'] = ('%04d' % (self.expectedrows - i))[-1]
                    row['var2'] = i
                    row['var3'] = i % self.maxshort
                    if isinstance(row['var4'], np.ndarray):
                        row['var4'] = [float(i), float(i * i)]
                    else:
                        row['var4'] = float(i)
                    if isinstance(row['var8'], np.ndarray):
                        row['var8'] = [0, 1]
                    else:
                        row['var8'] = 1
                    if isinstance(row['var9'], np.ndarray):
                        row['var9'] = [0.+float(i)*1j, float(i)+0.j]
                    else:
                        row['var9'] = float(i)+0.j
                    if isinstance(row['var10'], np.ndarray):
                        row['var10'] = [float(i)+0.j, 1.+float(i)*1j]
                    else:
                        row['var10'] = 1.+float(i)*1j
                    if isinstance(row['var5'], np.ndarray):
                        row['var5'] = np.array((float(i),)*4)
                    else:
                        row['var5'] = float(i)
                    if 'Float16Col' in globals():
                        if isinstance(row['var11'], np.ndarray):
                            row['var11'] = np.array((float(i),)*4)
                        else:
                            row['var11'] = float(i)
                    if 'Float96Col' in globals():
                        if isinstance(row['var12'], np.ndarray):
                            row['var12'] = np.array((float(i),)*4)
                        else:
                            row['var12'] = float(i)
                    if 'Float128Col' in globals():
                        if isinstance(row['var13'], np.ndarray):
                            row['var13'] = np.array((float(i),)*4)
                        else:
                            row['var13'] = float(i)
                    if 'Complex192Col' in globals():
                        if isinstance(row['var14'], np.ndarray):
                            row['var14'] = [float(i)+0j, 1 + float(i)*1j]
                        else:
                            row['var14'] = 1 + float(i)*1j
                    if 'Complex256Col' in globals():
                        if isinstance(row['var15'], np.ndarray):
                            row['var15'] = [float(i)+0j, 1 + float(i)*1j]
                        else:
                            row['var15'] = 1 + float(i)*1j

                    # var6 will be like var3 but byteswaped
                    row['var6'] = (((row['var3'] >> 8) & 0xff) +
                                   ((row['var3'] << 8) & 0xff00))
                    # print("Saving -->", row)
                    row.append()

            # Flush the buffer for this table
            table.flush()
            # Create a new group (descendant of group)
            group2 = self.fileh.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_description(self):
        """Checking table description and descriptive fields."""

        self.fileh = open_file(self.file)

        tbl = self.fileh.get_node('/table0')
        desc = tbl.description

        if isinstance(self.record, dict):
            columns = self.record
        elif isinstance(self.record, np.ndarray):
            descr, _ = descr_from_dtype(self.record.dtype)
            columns = descr._v_colobjects
        elif isinstance(self.record, np.dtype):
            descr, _ = descr_from_dtype(self.record)
            columns = descr._v_colobjects
        else:
            # This is an ordinary description.
            columns = self.record.columns

        # Check table and description attributes at the same time.
        # These checks are only valid for non-nested tables.

        # Column names.
        fix_n_column = 10
        expectedNames = ['var%d' % n for n in range(1, fix_n_column + 1)]
        types = ("float16", "float96", "float128", "complex192", "complex256")
        for n, typename in enumerate(types, fix_n_column + 1):
            name = typename.capitalize() + 'Col'
            if name in globals():
                expectedNames.append('var%d' % n)

        self.assertEqual(expectedNames, list(tbl.colnames))
        self.assertEqual(expectedNames, list(desc._v_names))

        # Column instances.
        for colname in expectedNames:
            self.assertTrue(tbl.colinstances[colname]
                            is tbl.cols._f_col(colname))

        # Column types.
        expectedTypes = [columns[colname].dtype
                         for colname in expectedNames]
        self.assertEqual(expectedTypes,
                         [tbl.coldtypes[v] for v in expectedNames])
        self.assertEqual(expectedTypes,
                         [desc._v_dtypes[v] for v in expectedNames])

        # Column string types.
        expectedTypes = [columns[colname].type
                         for colname in expectedNames]
        self.assertEqual(expectedTypes,
                         [tbl.coltypes[v] for v in expectedNames])
        self.assertEqual(expectedTypes,
                         [desc._v_types[v] for v in expectedNames])

        # Column defaults.
        for v in expectedNames:
            if common.verbose:
                print("dflt-->", columns[v].dflt, type(columns[v].dflt))
                print("coldflts-->", tbl.coldflts[v], type(tbl.coldflts[v]))
                print("desc.dflts-->", desc._v_dflts[v],
                      type(desc._v_dflts[v]))
            self.assertTrue(areArraysEqual(tbl.coldflts[v], columns[v].dflt))
            self.assertTrue(areArraysEqual(desc._v_dflts[v], columns[v].dflt))

        # Column path names.
        self.assertEqual(expectedNames, list(desc._v_pathnames))

        # Column objects.
        for colName in expectedNames:
            expectedCol = columns[colName]
            col = desc._v_colobjects[colName]

            self.assertEqual(expectedCol.dtype, col.dtype)
            self.assertEqual(expectedCol.type, col.type)

    def test01_readTable(self):
        """Checking table read."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [rec['var2'] for rec in table.iterrows() if rec['var2'] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", rec)
            print("Total selected records in table ==> ", len(result))
        nrows = self.expectedrows - 1
        rec = list(table.iterrows())[-1]
        self.assertEqual((rec['var1'], rec['var2'], rec['var7']),
                         (b"0001", nrows, b"1"))
        if isinstance(rec['var5'], np.ndarray):
            self.assertTrue(allequal(rec['var5'],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(rec['var5'], float(nrows))
        if isinstance(rec['var9'], np.ndarray):
            self.assertTrue(
                allequal(rec['var9'],
                         np.array([0.+float(nrows)*1.j, float(nrows)+0.j],
                                  np.complex64)))
        else:
            self.assertEqual((rec['var9']), float(nrows)+0.j)
        self.assertEqual(len(result), 20)

    def test01a_fetch_all_fields(self):
        """Checking table read (using Row.fetch_all_fields)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_fetch_all_fields..." %
                  self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [rec.fetch_all_fields() for rec in table.iterrows()
                  if rec['var2'] < 20]
        rec = result[-1]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", rec)
            print("Total selected records in table ==> ", len(result))
        nrows = 20 - 1
        strnrows = "%04d" % (self.expectedrows - nrows)
        strnrows = strnrows.encode('ascii')
        self.assertEqual((rec['var1'], rec['var2'], rec['var7']),
                         (strnrows, nrows, b"1"))
        if isinstance(rec['var5'], np.ndarray):
            self.assertTrue(allequal(rec['var5'],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(rec['var5'], float(nrows))
        if isinstance(rec['var9'], np.ndarray):
            self.assertTrue(
                allequal(rec['var9'],
                         np.array([0.+float(nrows)*1.j, float(nrows)+0.j],
                                  np.complex64)))
        else:
            self.assertEqual(rec['var9'], float(nrows)+0.j)
        self.assertEqual(len(result), 20)

    def test01a_integer(self):
        """Checking table read (using Row[integer])"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_integer..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [rec[1] for rec in table.iterrows()
                  if rec['var2'] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Total selected records in table ==> ", len(result))
            print("All results ==>", result)
        self.assertEqual(len(result), 20)
        self.assertEqual(result, range(20))

    def test01a_extslice(self):
        """Checking table read (using Row[::2])"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_extslice..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [rec[::2] for rec in table.iterrows()
                  if rec['var2'] < 20]
        rec = result[-1]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", rec)
            print("Total selected records in table ==> ", len(result))
        nrows = 20 - 1
        strnrows = "%04d" % (self.expectedrows - nrows)
        strnrows = strnrows.encode('ascii')
        self.assertEqual(rec[:2], (strnrows, 19))
        self.assertEqual(rec[3], b'1')
        if isinstance(rec[2], np.ndarray):
            self.assertTrue(allequal(rec[2],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(rec[2], nrows)
        if isinstance(rec[4], np.ndarray):
            self.assertTrue(
                allequal(rec[4],
                         np.array([0.+float(nrows)*1.j, float(nrows)+0.j],
                                  np.complex64)))
        else:
            self.assertEqual(rec[4], float(nrows)+0.j)
        self.assertEqual(len(result), 20)

    def test01a_nofield(self):
        """Checking table read (using Row['no-field'])"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_nofield..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Check that a KeyError is raised
        # self.assertRaises only work with functions
        # self.assertRaises(KeyError, [rec['no-field'] for rec in table])
        try:
            result = [rec['no-field'] for rec in table]
        except KeyError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next KeyError was catched!")
                print(value)
        else:
            print(result)
            self.fail("expected a KeyError")

    def test01a_badtypefield(self):
        """Checking table read (using Row[{}])"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_badtypefield..." %
                  self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Check that a TypeError is raised
        # self.assertRaises only work with functions
        # self.assertRaises(TypeError, [rec[{}] for rec in table])
        try:
            result = [rec[{}] for rec in table]
        except TypeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next TypeError was catched!")
                print(value)
        else:
            print(result)
            self.fail("expected a TypeError")

    def test01b_readTable(self):
        """Checking table read and cuts (multidimensional columns case)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [rec['var5'] for rec in table.iterrows()
                  if rec['var2'] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", rec)
            print("rec['var5'] ==>", rec['var5'], end=' ')
            print("nrows ==>", table.nrows)
            print("Total selected records in table ==> ", len(result))
        nrows = table.nrows
        rec = list(table.iterrows())[-1]
        if isinstance(rec['var5'], np.ndarray):
            npt.assert_array_equal(result[0],
                                   np.array((float(0),)*4, np.float32))
            npt.assert_array_equal(result[1],
                                   np.array((float(1),)*4, np.float32))
            npt.assert_array_equal(result[2],
                                   np.array((float(2),)*4, np.float32))
            npt.assert_array_equal(result[3],
                                   np.array((float(3),)*4, np.float32))
            npt.assert_array_equal(result[10],
                                   np.array((float(10),)*4, np.float32))
            npt.assert_array_equal(rec['var5'],
                                   np.array((float(nrows-1),)*4, np.float32))
        else:
            self.assertEqual(rec['var5'], float(nrows - 1))
        # Read the records and select those with "var2" file less than 20
        result = [rec['var10'] for rec in table.iterrows()
                  if rec['var2'] < 20]
        if isinstance(rec['var10'], np.ndarray):
            npt.assert_array_equal(
                result[0],
                np.array([float(0)+0.j, 1.+float(0)*1j], np.complex128))
            npt.assert_array_equal(
                result[1],
                np.array([float(1)+0.j, 1.+float(1)*1j], np.complex128))
            npt.assert_array_equal(
                result[2],
                np.array([float(2)+0.j, 1.+float(2)*1j], np.complex128))
            npt.assert_array_equal(
                result[3],
                np.array([float(3)+0.j, 1.+float(3)*1j], np.complex128))
            npt.assert_array_equal(
                result[10],
                np.array([float(10)+0.j, 1.+float(10)*1j], np.complex128))
            npt.assert_array_equal(
                rec['var10'],
                np.array([float(nrows-1)+0.j, 1.+float(nrows-1)*1j],
                         np.complex128))
        else:
            self.assertEqual(rec['var10'], 1.+float(nrows-1)*1j)
        self.assertEqual(len(result), 20)

    def test01c_readTable(self):
        """Checking nested iterators (reading)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Read the records and select those with "var2" file less than 20
        result = []
        for rec in table.iterrows(stop=2):
            for rec2 in table.iterrows(stop=2):
                if rec2['var2'] < 20:
                    result.append([rec['var2'], rec2['var2']])
        if common.verbose:
            print("result ==>", result)

        self.assertEqual(result, [[0, 0], [0, 1], [1, 0], [1, 1]])

    def test01d_readTable(self):
        """Checking nested iterators (reading, mixed conditions)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01d_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Read the records and select those with "var2" file less than 20
        result = []
        for rec in table.iterrows(stop=2):
            for rec2 in table.where('var2 < 20', stop=2):
                result.append([rec['var2'], rec2['var2']])
        if common.verbose:
            print("result ==>", result)

        self.assertEqual(result, [[0, 0], [0, 1], [1, 0], [1, 1]])

    def test01e_readTable(self):
        """Checking nested iterators (reading, both conditions)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01e_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Read the records and select those with "var2" file less than 20
        result = []
        for rec in table.where('var3 < 2'):
            for rec2 in table.where('var2 < 3'):
                result.append([rec['var2'], rec2['var3']])
        if common.verbose:
            print("result ==>", result)

        self.assertEqual(result,
                         [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]])

    def test01f_readTable(self):
        """Checking nested iterators (reading, break in the loop)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01f_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Read the records and select those with "var2" file less than 20
        result = []
        for rec in table.where('var3 < 2'):
            for rec2 in table.where('var2 < 4'):
                if rec2['var2'] >= 3:
                    break
                result.append([rec['var2'], rec2['var3']])
        if common.verbose:
            print("result ==>", result)

        self.assertEqual(result,
                         [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]])

    def test01g_readTable(self):
        """Checking iterator with an evanescent table."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01g_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")

        # Read from an evanescent table
        result = [rec['var2'] for rec in self.fileh.get_node("/table0")
                  if rec['var2'] < 20]

        self.assertEqual(len(result), 20)

    def test02_AppendRows(self):
        """Checking whether appending record rows works or not."""

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_AppendRows..." % self.__class__.__name__)

        # Get a table
        table = self.fileh.get_node("/group0/table1")
        # Get their row object
        row = table.row
        if common.verbose:
            print("Nrows in old", table._v_pathname, ":", table.nrows)
            print("Record Format ==>", table.description._v_nested_formats)
            print("Record Size ==>", table.rowsize)
        # Append some rows
        for i in xrange(self.appendrows):
            s = '%04d' % (self.appendrows - i)
            row['var1'] = s.encode('ascii')
            row['var7'] = s[-1].encode('ascii')
            row['var2'] = i
            row['var3'] = i % self.maxshort
            if isinstance(row['var4'], np.ndarray):
                row['var4'] = [float(i), float(i * i)]
            else:
                row['var4'] = float(i)
            if isinstance(row['var8'], np.ndarray):
                row['var8'] = [0, 1]
            else:
                row['var8'] = 1
            if isinstance(row['var9'], np.ndarray):
                row['var9'] = [0.+float(i)*1j, float(i)+0.j]
            else:
                row['var9'] = float(i)+0.j
            if isinstance(row['var10'], np.ndarray):
                row['var10'] = [float(i)+0.j, 1.+float(i)*1j]
            else:
                row['var10'] = 1.+float(i)*1j
            if isinstance(row['var5'], np.ndarray):
                row['var5'] = np.array((float(i),)*4)
            else:
                row['var5'] = float(i)
            if 'Float16Col' in globals():
                if isinstance(row['var11'], np.ndarray):
                    row['var11'] = np.array((float(i),)*4)
                else:
                    row['var11'] = float(i)
            if 'Float96Col' in globals():
                if isinstance(row['var12'], np.ndarray):
                    row['var12'] = np.array((float(i),)*4)
                else:
                    row['var12'] = float(i)
            if 'Float128Col' in globals():
                if isinstance(row['var13'], np.ndarray):
                    row['var13'] = np.array((float(i),)*4)
                else:
                    row['var13'] = float(i)
            if 'Complex192Col' in globals():
                if isinstance(row['var14'], np.ndarray):
                    row['var14'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var14'] = 1 + float(i)*1j
            if 'Complex256Col' in globals():
                if isinstance(row['var15'], np.ndarray):
                    row['var15'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var15'] = 1 + float(i)*1j

            row.append()

        # Flush the buffer for this table and read it
        table.flush()
        result = [row['var2'] for row in table.iterrows() if row['var2'] < 20]

        nrows = self.appendrows - 1
        row = list(table.iterrows())[-1]
        self.assertEqual((row['var1'], row['var2'], row['var7']),
                         (b"0001", nrows, b"1"))
        if isinstance(row['var5'], np.ndarray):
            self.assertTrue(allequal(row['var5'],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(row['var5'], float(nrows))
        if self.appendrows <= 20:
            add = self.appendrows
        else:
            add = 20
        self.assertEqual(len(result), 20 + add)  # because we appended new rows

    # This test has been commented out because appending records without
    # flushing them explicitely is being warned from now on.
    # F. Alted 2006-08-03
    def _test02a_AppendRows(self):
        """Checking appending records without flushing explicitely."""

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02a_AppendRows..." % self.__class__.__name__)

        group = self.rootgroup
        for i in range(3):
            # Get a table
            table = self.fileh.get_node(group, 'table'+str(i))
            # Get the next group
            group = self.fileh.get_node(group, 'group'+str(i))
            # Get their row object
            row = table.row
            if common.verbose:
                print("Nrows in old", table._v_pathname, ":", table.nrows)
                print("Record Format ==>", table.description._v_nested_formats)
                print("Record Size ==>", table.rowsize)
            # Append some rows
            for i in xrange(self.appendrows):
                row['var1'] = '%04d' % (self.appendrows - i)
                row['var7'] = row['var1'][-1]
                row['var2'] = i
                row['var3'] = i % self.maxshort
                if isinstance(row['var4'], np.ndarray):
                    row['var4'] = [float(i), float(i * i)]
                else:
                    row['var4'] = float(i)
                if isinstance(row['var8'], np.ndarray):
                    row['var8'] = [0, 1]
                else:
                    row['var8'] = 1
                if isinstance(row['var9'], np.ndarray):
                    row['var9'] = [0.+float(i)*1j, float(i)+0.j]
                else:
                    row['var9'] = float(i)+0.j
                if isinstance(row['var10'], np.ndarray):
                    row['var10'] = [float(i)+0.j, 1.+float(i)*1j]
                else:
                    row['var10'] = 1.+float(i)*1j
                if isinstance(row['var5'], np.ndarray):
                    row['var5'] = np.array((float(i),)*4)
                else:
                    row['var5'] = float(i)
                if 'Float16Col' in globals():
                    if isinstance(row['var11'], np.ndarray):
                        row['var11'] = np.array((float(i),)*4)
                    else:
                        row['var11'] = float(i)
                if 'Float96Col' in globals():
                    if isinstance(row['var12'], np.ndarray):
                        row['var12'] = np.array((float(i),)*4)
                    else:
                        row['var12'] = float(i)
                if 'Float128Col' in globals():
                    if isinstance(row['var13'], np.ndarray):
                        row['var13'] = np.array((float(i),)*4)
                    else:
                        row['var13'] = float(i)
                if 'Complex192Col' in globals():
                    if isinstance(row['var14'], np.ndarray):
                        row['var14'] = [float(i)+0j, 1 + float(i)*1j]
                    else:
                        row['var14'] = 1 + float(i)*1j
                if 'Complex256Col' in globals():
                    if isinstance(row['var15'], np.ndarray):
                        row['var15'] = [float(i)+0j, 1 + float(i)*1j]
                    else:
                        row['var15'] = 1 + float(i)*1j

                row.append()
            table.flush()

        # Close the file and re-open it.
        self.fileh.close()

        self.fileh = open_file(self.file, mode="a")
        table = self.fileh.root.table0
        # Flush the buffer for this table and read it
        result = [row['var2'] for row in table.iterrows()
                  if row['var2'] < 20]

        nrows = self.appendrows - 1
        self.assertEqual((row['var1'], row['var2'], row['var7']),
                         ("0001", nrows, "1"))
        if isinstance(row['var5'], np.ndarray):
            self.assertTrue(allequal(row['var5'],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(row['var5'], float(nrows))
        if self.appendrows <= 20:
            add = self.appendrows
        else:
            add = 20
        self.assertEqual(len(result), 20 + add)  # because we appended new rows

    def test02b_AppendRows(self):
        """Checking whether appending *and* reading rows works or not"""

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_AppendRows..." % self.__class__.__name__)

        # Get a table
        table = self.fileh.get_node("/group0/table1")
        if common.verbose:
            print("Nrows in old", table._v_pathname, ":", table.nrows)
            print("Record Format ==>", table.description._v_nested_formats)
            print("Record Size ==>", table.rowsize)
        # Set a small number of buffer to make this test faster
        table.nrowsinbuf = 3
        # Get their row object
        row = table.row
        # Append some rows (3 * table.nrowsinbuf is enough for
        # checking purposes)
        for i in xrange(3 * table.nrowsinbuf):
            s = '%04d' % (self.appendrows - i)
            row['var1'] = s.encode('ascii')
            row['var7'] = s[-1].encode('ascii')
            # row['var7'] = table.cols['var1'][i][-1]
            row['var2'] = i
            row['var3'] = i % self.maxshort
            if isinstance(row['var4'], np.ndarray):
                row['var4'] = [float(i), float(i * i)]
            else:
                row['var4'] = float(i)
            if isinstance(row['var8'], np.ndarray):
                row['var8'] = [0, 1]
            else:
                row['var8'] = 1
            if isinstance(row['var9'], np.ndarray):
                row['var9'] = [0.+float(i)*1j, float(i)+0.j]
            else:
                row['var9'] = float(i)+0.j
            if isinstance(row['var10'], np.ndarray):
                row['var10'] = [float(i)+0.j, 1.+float(i)*1j]
            else:
                row['var10'] = 1.+float(i)*1j
            if isinstance(row['var5'], np.ndarray):
                row['var5'] = np.array((float(i),)*4)
            else:
                row['var5'] = float(i)
            if 'Float16Col' in globals():
                if isinstance(row['var11'], np.ndarray):
                    row['var11'] = np.array((float(i),)*4)
                else:
                    row['var11'] = float(i)
            if 'Float96Col' in globals():
                if isinstance(row['var12'], np.ndarray):
                    row['var12'] = np.array((float(i),)*4)
                else:
                    row['var12'] = float(i)
            if 'Float128Col' in globals():
                if isinstance(row['var13'], np.ndarray):
                    row['var13'] = np.array((float(i),)*4)
                else:
                    row['var13'] = float(i)
            if 'Complex192Col' in globals():
                if isinstance(row['var14'], np.ndarray):
                    row['var14'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var14'] = 1 + float(i)*1j
            if 'Complex256Col' in globals():
                if isinstance(row['var15'], np.ndarray):
                    row['var15'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var15'] = 1 + float(i)*1j

            row.append()
            # the next call can mislead the counters
            result = [row2['var2'] for row2 in table]
            # warning! the next will result into wrong results
            # result = [ row['var2'] for row in table ]
            # This is because the iterator for writing and for reading
            # cannot be shared!

        # Do not flush the buffer for this table and try to read it
        # We are forced now to flush tables after append operations
        # because of unsolved issues with the LRU cache that are too
        # difficult to track.
        # F. Alted 2006-08-03
        table.flush()
        result = [row['var2'] for row in table.iterrows() if row['var2'] < 20]
        if common.verbose:
            print("Result length ==>", len(result))
            print("Result contents ==>", result)
        self.assertEqual(len(result), 20 + 3 * table.nrowsinbuf)
        self.assertEqual(result, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
                                  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
                                  0, 1, 2, 3, 4, 5, 6, 7, 8])
        # Check consistency of I/O buffers when doing mixed I/O operations
        # That is, the next should work in these operations
        # row['var1'] = '%04d' % (self.appendrows - i)
        # row['var7'] = row['var1'][-1]
        result7 = [row['var7'] for row in table.iterrows() if row['var2'] < 20]
        if common.verbose:
            print("Result7 length ==>", len(result7))
            print("Result7 contents ==>", result7)
        self.assertEqual(
            result7,
            [b'0', b'9', b'8', b'7', b'6', b'5', b'4', b'3', b'2', b'1',
             b'0', b'9', b'8', b'7', b'6', b'5', b'4', b'3', b'2', b'1',
             b'0', b'9', b'8', b'7', b'6', b'5', b'4', b'3', b'2'])

    # This test is commented out as it should not work anymore due to
    # the new policy of not doing a flush in the middle of a __del__
    # operation. F. Alted 2006-08-24
    def _test02c_AppendRows(self):
        """Checking appending with evanescent table objects."""

        # This test is kind of magic, but it is a good sanity check anyway.

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02c_AppendRows..." % self.__class__.__name__)

        # Get a table
        table = self.fileh.get_node("/group0/table1")
        if common.verbose:
            print("Nrows in old", table._v_pathname, ":", table.nrows)
            print("Record Format ==>", table.description._v_nested_formats)
            print("Record Size ==>", table.rowsize)
        # Set a small number of buffer to make this test faster
        table.nrowsinbuf = 3
        # Get their row object
        self.row = table.row
        # delete the table reference
        del table
        # Append some rows
        for i in xrange(22):
            self.row['var2'] = 100 + i
            self.row.append()
        # del self.row # force the table object to be destroyed (and the
                       # user warned!)
        # convert a warning in an error
        warnings.filterwarnings('error', category=PerformanceWarning)
        self.assertRaises(PerformanceWarning, self.__dict__.pop, 'row')
#         try:
#             # force the table object to be destroyed
#             self.__dict__.pop('row')
#         except PerformanceWarning:
#             if common.verbose:
#                 (type, value, traceback) = sys.exc_info()
#                 print "\nGreat!, the next PerformanceWarning was catched:"
#                 print value
#             # Ignore the warning and actually flush the table
#             warnings.filterwarnings("ignore", category=PerformanceWarning)
#             table = self.fileh.get_node("/group0/table1")
#             table.flush()
#         else:
#             self.fail("expected a PeformanceWarning")
        # reset the warning
        warnings.filterwarnings('default', category=PerformanceWarning)
        result = [row['var2'] for row in table.iterrows()
                  if 100 <= row['var2'] < 122]
        if common.verbose:
            print("Result length ==>", len(result))
            print("Result contents ==>", result)
        self.assertEqual(len(result), 22)
        self.assertEqual(
            result,
            [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,
             111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121])

    def test02d_AppendRows(self):
        """Checking appending using the same Row object after flushing."""

        # This test is kind of magic, but it is a good sanity check anyway.

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02d_AppendRows..." % self.__class__.__name__)

        # Get a table
        table = self.fileh.get_node("/group0/table1")
        if common.verbose:
            print("Nrows in old", table._v_pathname, ":", table.nrows)
            print("Record Format ==>", table.description._v_nested_formats)
            print("Record Size ==>", table.rowsize)
        # Set a small number of buffer to make this test faster
        table.nrowsinbuf = 3
        # Get their row object
        row = table.row
        # Append some rows
        for i in xrange(10):
            row['var2'] = 100 + i
            row.append()
        # Force a flush
        table.flush()
        # Add new rows
        for i in xrange(9):
            row['var2'] = 110 + i
            row.append()
        table.flush()  # XXX al eliminar...
        result = [row['var2'] for row in table.iterrows()
                  if 100 <= row['var2'] < 120]
        if common.verbose:
            print("Result length ==>", len(result))
            print("Result contents ==>", result)
        if table.nrows > 119:
            # Case for big tables
            self.assertEqual(len(result), 39)
            self.assertEqual(result,
                             [100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
                              110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
                              100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
                              110, 111, 112, 113, 114, 115, 116, 117, 118])
        else:
            self.assertEqual(len(result), 19)
            self.assertEqual(result,
                             [100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
                              110, 111, 112, 113, 114, 115, 116, 117, 118])

    def test02e_AppendRows(self):
        """Checking appending using the Row of an unreferenced table."""
        # See ticket #94 (http://www.pytables.org/trac/ticket/94).

        # Reopen the file in append mode.
        self.fileh = open_file(self.file, mode='a')

        # Get the row handler which will outlive the reference to the table.
        table = self.fileh.get_node('/group0/table1')
        oldnrows = table.nrows
        row = table.row

        # Few appends are made to avoid flushing the buffers in ``row``.

        # First case: append to an alive (referenced) table.
        row.append()
        table.flush()
        newnrows = table.nrows
        self.assertEqual(newnrows, oldnrows + 1,
                         "Append to alive table failed.")

        if self.fileh._node_manager.cache.nslots == 0:
            # Skip this test from here on because the second case
            # won't work when thereis not a node cache.
            return

        # Second case: append to a dead (unreferenced) table.
        del table
        row.append()
        table = self.fileh.get_node('/group0/table1')
        table.flush()
        newnrows = table.nrows
        self.assertEqual(newnrows, oldnrows + 2,
                         "Append to dead table failed.")

    # CAVEAT: The next test only works for tables with rows < 2**15
    def test03_endianess(self):
        """Checking if table is endianess aware."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_endianess..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/group0/group1/table2")

        # Read the records and select the ones with "var3" column less than 20
        result = [rec['var2'] for rec in table.iterrows() if rec['var3'] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("On-disk byteorder ==>", table.byteorder)
            print("Last record in table ==>", rec)
            print("Selected records ==>", result)
            print("Total selected records in table ==>", len(result))
        nrows = self.expectedrows - 1
        self.assertEqual(table.byteorder,
                         {"little": "big", "big": "little"}[sys.byteorder])
        rec = list(table.iterrows())[-1]
        self.assertEqual((rec['var1'], rec['var3']), (b"0001", nrows))
        self.assertEqual(len(result), 20)

    def test04_delete(self):
        """Checking whether a single row can be deleted."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_delete..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        table = self.fileh.get_node("/table0")

        # Read the records and select the ones with "var2" column less than 20
        result = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result[-1])
            print("Total selected records in table ==>", len(result))

        nrows = table.nrows
        table.nrowsinbuf = 3  # small value of the buffer
        # Delete the twenty-th row
        table.remove_rows(19, 20)

        # Re-read the records
        result2 = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result2[-1])
            print("Total selected records in table ==>", len(result2))

        self.assertEqual(table.nrows, nrows - 1)
        self.assertEqual(table.shape, (nrows - 1,))
        # Check that the new list is smaller than the original one
        self.assertEqual(len(result), len(result2) + 1)
        self.assertEqual(result[:-1], result2)

    def test04a_delete(self):
        """Checking whether a single row can be deleted."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_delete..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        table = self.fileh.get_node("/table0")

        # Read the records and select the ones with "var2" column less than 20
        result = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result[-1])
            print("Total selected records in table ==>", len(result))

        nrows = table.nrows
        table.nrowsinbuf = 3  # small value of the buffer
        # Delete the twenty-th row
        table.remove_row(19)

        # Re-read the records
        result2 = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result2[-1])
            print("Total selected records in table ==>", len(result2))

        self.assertEqual(table.nrows, nrows - 1)
        self.assertEqual(table.shape, (nrows - 1,))
        # Check that the new list is smaller than the original one
        self.assertEqual(len(result), len(result2) + 1)
        self.assertEqual(result[:-1], result2)

    def test04b_delete(self):
        """Checking whether a range of rows can be deleted."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b_delete..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        table = self.fileh.get_node("/table0")

        # Read the records and select the ones with "var2" column less than 20
        result = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result[-1])
            print("Total selected records in table ==>", len(result))

        nrows = table.nrows
        table.nrowsinbuf = 4  # small value of the buffer
        # Delete the last ten rows
        table.remove_rows(10, 20)

        # Re-read the records
        result2 = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result2[-1])
            print("Total selected records in table ==>", len(result2))

        self.assertEqual(table.nrows, nrows - 10)
        self.assertEqual(table.shape, (nrows - 10,))
        # Check that the new list is smaller than the original one
        self.assertEqual(len(result), len(result2) + 10)
        self.assertEqual(result[:10], result2)

    def test04c_delete(self):
        """Checking whether removing a bad range of rows is detected."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04c_delete..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        table = self.fileh.get_node("/table0")

        # Read the records and select the ones with "var2" column less than 20
        result = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        nrows = table.nrows
        table.nrowsinbuf = 5  # small value of the buffer
        # Delete a too large range of rows
        table.remove_rows(10, nrows + 100)

        # Re-read the records
        result2 = [r['var2'] for r in table.iterrows() if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result2[-1])
            print("Total selected records in table ==>", len(result2))

        self.assertEqual(table.nrows, 10)
        self.assertEqual(table.shape, (10,))
        # Check that the new list is smaller than the original one
        self.assertEqual(len(result), len(result2) + 10)
        self.assertEqual(result[:10], result2)

    def test04d_delete(self):
        """Checking whether removing rows several times at once is working."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04d_delete..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        table = self.fileh.get_node("/table0")

        # Read the records and select the ones with "var2" column less than 20
        result = [r['var2'] for r in table if r['var2'] < 20]

        nrows = table.nrows
        nrowsinbuf = table.nrowsinbuf
        table.nrowsinbuf = 6  # small value of the buffer
        # Delete some rows
        table.remove_rows(10, 15)
        # It's necessary to restore the value of buffer to use the row object
        # afterwards...
        table.nrowsinbuf = nrowsinbuf

        # Append some rows
        row = table.row
        for i in xrange(10, 15):
            row['var1'] = '%04d' % (self.appendrows - i)
            # This line gives problems on Windows. Why?
            # row['var7'] = row['var1'][-1]
            row['var2'] = i
            row['var3'] = i % self.maxshort
            if isinstance(row['var4'], np.ndarray):
                row['var4'] = [float(i), float(i * i)]
            else:
                row['var4'] = float(i)
            if isinstance(row['var8'], np.ndarray):
                row['var8'] = [0, 1]
            else:
                row['var8'] = 1
            if isinstance(row['var9'], np.ndarray):
                row['var9'] = [0.+float(i)*1j, float(i)+0.j]
            else:
                row['var9'] = float(i)+0.j
            if isinstance(row['var10'], np.ndarray):
                row['var10'] = [float(i)+0.j, 1.+float(i)*1j]
            else:
                row['var10'] = 1.+float(i)*1j
            if isinstance(row['var5'], np.ndarray):
                row['var5'] = np.array((float(i),)*4)
            else:
                row['var5'] = float(i)
            if 'Float16Col' in globals():
                if isinstance(row['var11'], np.ndarray):
                    row['var11'] = np.array((float(i),)*4)
                else:
                    row['var11'] = float(i)
            if 'Float96Col' in globals():
                if isinstance(row['var12'], np.ndarray):
                    row['var12'] = np.array((float(i),)*4)
                else:
                    row['var12'] = float(i)
            if 'Float128Col' in globals():
                if isinstance(row['var13'], np.ndarray):
                    row['var13'] = np.array((float(i),)*4)
                else:
                    row['var13'] = float(i)
            if 'Complex192Col' in globals():
                if isinstance(row['var14'], np.ndarray):
                    row['var14'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var14'] = 1 + float(i)*1j
            if 'Complex256Col' in globals():
                if isinstance(row['var15'], np.ndarray):
                    row['var15'] = [float(i)+0j, 1 + float(i)*1j]
                else:
                    row['var15'] = 1 + float(i)*1j

            row.append()
        # Flush the buffer for this table
        table.flush()

        # Delete 5 rows more
        table.remove_rows(5, 10)

        # Re-read the records
        result2 = [r['var2'] for r in table if r['var2'] < 20]

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last selected value ==>", result2[-1])
            print("Total selected records in table ==>", len(result2))

        self.assertEqual(table.nrows, nrows - 5)
        self.assertEqual(table.shape, (nrows - 5,))
        # Check that the new list is smaller than the original one
        self.assertEqual(len(result), len(result2) + 5)
        # The last values has to be equal
        self.assertEqual(result[10:15], result2[10:15])

    def test05_filtersTable(self):
        """Checking tablefilters."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_filtersTable..." %
                  self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Check filters:
        if self.compress != table.filters.complevel and common.verbose:
            print("Error in compress. Class:", self.__class__.__name__)
            print("self, table:", self.compress, table.filters.complevel)
        self.assertEqual(table.filters.complevel, self.compress)
        if self.compress > 0 and which_lib_version(self.complib):
            self.assertEqual(table.filters.complib, self.complib)
        if self.shuffle != table.filters.shuffle and common.verbose:
            print("Error in shuffle. Class:", self.__class__.__name__)
            print("self, table:", self.shuffle, table.filters.shuffle)
        self.assertEqual(self.shuffle, table.filters.shuffle)
        if self.fletcher32 != table.filters.fletcher32 and common.verbose:
            print("Error in fletcher32. Class:", self.__class__.__name__)
            print("self, table:", self.fletcher32, table.filters.fletcher32)
        self.assertEqual(self.fletcher32, table.filters.fletcher32)

    def test06_attributes(self):
        self.fileh = open_file(self.file)
        obj = self.fileh.get_node('/table0')

        self.assertEqual(obj.flavor, 'numpy')
        self.assertEqual(obj.shape, (self.expectedrows,))
        self.assertEqual(obj.ndim, 1)
        self.assertEqual(obj.nrows, self.expectedrows)


class BasicWriteTestCase(BasicTestCase):
    title = "BasicWrite"


class OldRecordBasicWriteTestCase(BasicTestCase):
    title = "OldRecordBasicWrite"
    record = OldRecord


class DictWriteTestCase(BasicTestCase):
    # This checks also unidimensional arrays as columns
    title = "DictWrite"
    record = RecordDescriptionDict
    nrows = 21
    nrowsinbuf = 3  # Choose a small value for the buffer size
    start = 0
    stop = 10
    step = 3


if sys.version_info < (3,):
    class DictWriteTestCase2(DictWriteTestCase):
        record = RecordDescriptionDict.copy()
        record[unicode('var1')] = record.pop('var1')


# Pure NumPy dtype
class NumPyDTWriteTestCase(BasicTestCase):
    title = "NumPyDTWriteTestCase"
    formats = "a4,i4,i2,2f8,f4,i2,a1,b1,c8,c16".split(',')
    names = 'var1,var2,var3,var4,var5,var6,var7,var8,var9,var10'.split(',')

    if 'Float16Col' in globals():
        formats.append('f2')
        names.append('var11')
    if 'Float96Col' in globals():
        formats.append('f12')
        names.append('var12')
    if 'Float128Col' in globals():
        formats.append('f16')
        names.append('var13')
    if 'Complex192Col' in globals():
        formats.append('c24')
        names.append('var14')
    if 'Complex256Col' in globals():
        formats.append('c32')
        names.append('var15')

    record = np.dtype(','.join(formats))
    record.names = names


class RecArrayOneWriteTestCase(BasicTestCase):
    title = "RecArrayOneWrite"
    formats = "a4,i4,i2,2f8,f4,i2,a1,b1,c8,c16".split(',')
    names = 'var1,var2,var3,var4,var5,var6,var7,var8,var9,var10'.split(',')

    if 'Float16Col' in globals():
        formats.append('f2')
        names.append('var11')
    if 'Float96Col' in globals():
        formats.append('f12')
        names.append('var12')
    if 'Float128Col' in globals():
        formats.append('f16')
        names.append('var13')
    if 'Complex192Col' in globals():
        formats.append('c24')
        names.append('var14')
    if 'Complex256Col' in globals():
        formats.append('c32')
        names.append('var15')

    record = records.array(None, shape=0, formats=','.join(formats),
                           names=names)


class RecArrayTwoWriteTestCase(BasicTestCase):
    title = "RecArrayTwoWrite"
    expectedrows = 100
    recarrayinit = 1
    formats = "a4,i4,i2,2f8,f4,i2,a1,b1,c8,c16".split(',')
    names = 'var1,var2,var3,var4,var5,var6,var7,var8,var9,var10'.split(',')

    if 'Float16Col' in globals():
        formats.append('f2')
        names.append('var11')
    if 'Float96Col' in globals():
        formats.append('f12')
        names.append('var12')
    if 'Float128Col' in globals():
        formats.append('f16')
        names.append('var13')
    if 'Complex192Col' in globals():
        formats.append('c24')
        names.append('var14')
    if 'Complex256Col' in globals():
        formats.append('c32')
        names.append('var15')

    recordtemplate = records.array(None, shape=1, formats=','.join(formats),
                                   names=names)


class RecArrayThreeWriteTestCase(BasicTestCase):
    title = "RecArrayThreeWrite"
    expectedrows = 100
    recarrayinit = 1
    formats = "a4,i4,i2,2f8,f4,i2,a1,b1,c8,c16".split(',')
    names = 'var1,var2,var3,var4,var5,var6,var7,var8,var9,var10'.split(',')

    if 'Float16Col' in globals():
        formats.append('f2')
        names.append('var11')
    if 'Float96Col' in globals():
        formats.append('f12')
        names.append('var12')
    if 'Float128Col' in globals():
        formats.append('f16')
        names.append('var13')
    if 'Complex192Col' in globals():
        formats.append('c24')
        names.append('var14')
    if 'Complex256Col' in globals():
        formats.append('c32')
        names.append('var15')

    recordtemplate = records.array(None, shape=1, formats=','.join(formats),
                                   names=names)


class CompressBloscTablesTestCase(BasicTestCase):
    title = "CompressBloscTables"
    compress = 6
    complib = "blosc"


class CompressBloscShuffleTablesTestCase(BasicTestCase):
    title = "CompressBloscTables"
    compress = 1
    shuffle = 1
    complib = "blosc"


class CompressBloscBloscLZTablesTestCase(BasicTestCase):
    title = "CompressBloscLZTables"
    compress = 1
    shuffle = 1
    complib = "blosc:blosclz"


class CompressBloscLZ4TablesTestCase(BasicTestCase):
    title = "CompressLZ4Tables"
    compress = 1
    shuffle = 1
    complib = "blosc:lz4"


class CompressBloscLZ4HCTablesTestCase(BasicTestCase):
    title = "CompressLZ4HCTables"
    compress = 1
    shuffle = 1
    complib = "blosc:lz4hc"


class CompressBloscSnappyTablesTestCase(BasicTestCase):
    title = "CompressSnappyTables"
    compress = 1
    shuffle = 1
    complib = "blosc:snappy"


class CompressBloscZlibTablesTestCase(BasicTestCase):
    title = "CompressZlibTables"
    compress = 1
    shuffle = 1
    complib = "blosc:zlib"


class CompressLZOTablesTestCase(BasicTestCase):
    title = "CompressLZOTables"
    compress = 1
    complib = "lzo"


class CompressLZOShuffleTablesTestCase(BasicTestCase):
    title = "CompressLZOTables"
    compress = 1
    shuffle = 1
    complib = "lzo"


class CompressBzip2TablesTestCase(BasicTestCase):
    title = "CompressBzip2Tables"
    compress = 1
    complib = "bzip2"


class CompressBzip2ShuffleTablesTestCase(BasicTestCase):
    title = "CompressBzip2Tables"
    compress = 1
    shuffle = 1
    complib = "bzip2"


class CompressZLIBTablesTestCase(BasicTestCase):
    title = "CompressOneTables"
    compress = 1
    complib = "zlib"


class CompressZLIBShuffleTablesTestCase(BasicTestCase):
    title = "CompressOneTables"
    compress = 1
    shuffle = 1
    complib = "zlib"


class Fletcher32TablesTestCase(BasicTestCase):
    title = "Fletcher32Tables"
    fletcher32 = 1
    shuffle = 0
    complib = "zlib"


class AllFiltersTablesTestCase(BasicTestCase):
    title = "AllFiltersTables"
    compress = 1
    fletcher32 = 1
    shuffle = 1
    complib = "zlib"


class CompressTwoTablesTestCase(BasicTestCase):
    title = "CompressTwoTables"
    compress = 1
    # This checks also unidimensional arrays as columns
    record = RecordDescriptionDict


class BigTablesTestCase(BasicTestCase):
    title = "BigTables"
    # 10000 rows takes much more time than we can afford for tests
    # reducing to 1000 would be more than enough
    # F. Alted 2004-01-19
    # Will be executed only in common.heavy mode
    expectedrows = 10000
    appendrows = 100


class SizeOnDiskInMemoryPropertyTestCase(unittest.TestCase):
    def setUp(self):
        # set chunkshape so it divides evenly into array_size, to avoid
        # partially filled chunks
        self.chunkshape = (1000, )
        self.dtype = np.format_parser(['i4'] * 10, [], []).dtype
        # approximate size (in bytes) of non-data portion of hdf5 file
        self.hdf_overhead = 6000
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")

    def tearDown(self):
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def create_table(self, complevel):
        filters = Filters(complevel=complevel, complib='blosc')
        self.table = self.fileh.create_table('/', 'sometable', self.dtype,
                                             filters=filters,
                                             chunkshape=self.chunkshape)

    def test_zero_length(self):
        complevel = 0
        self.create_table(complevel)
        self.assertEqual(self.table.size_on_disk, 0)
        self.assertEqual(self.table.size_in_memory, 0)

    # add 10 chunks of data in one append
    def test_no_compression_one_append(self):
        complevel = 0
        self.create_table(complevel)
        self.table.append([tuple(range(10))] * self.chunkshape[0] * 10)
        self.assertEqual(self.table.size_on_disk, 10 * 1000 * 10 * 4)
        self.assertEqual(self.table.size_in_memory, 10 * 1000 * 10 * 4)

    # add 10 chunks of data in two appends
    def test_no_compression_multiple_appends(self):
        complevel = 0
        self.create_table(complevel)
        self.table.append([tuple(range(10))] * self.chunkshape[0] * 5)
        self.table.append([tuple(range(10))] * self.chunkshape[0] * 5)
        self.assertEqual(self.table.size_on_disk, 10 * 1000 * 10 * 4)
        self.assertEqual(self.table.size_in_memory, 10 * 1000 * 10 * 4)

    def test_with_compression(self):
        complevel = 1
        self.create_table(complevel)
        self.table.append([tuple(range(10))] * self.chunkshape[0] * 10)
        file_size = os.stat(self.file).st_size
        self.assertTrue(
            abs(self.table.size_on_disk - file_size) <= self.hdf_overhead)
        self.assertEqual(self.table.size_in_memory, 10 * 1000 * 10 * 4)
        self.assertTrue(self.table.size_on_disk < self.table.size_in_memory)


class NonNestedTableReadTestCase(unittest.TestCase):
    def setUp(self):
        self.dtype = np.format_parser(['i4'] * 10, [], []).dtype
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.table = self.fileh.create_table('/', 'table', self.dtype)
        self.shape = (100, )
        self.populate_file()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def populate_file(self):
        self.array = np.zeros(self.shape, self.dtype)
        for row_num, row in enumerate(self.array):
            start = row_num * len(self.array.dtype.names)
            for value, col in enumerate(self.array.dtype.names, start):
                row[col] = value
        self.table.append(self.array)
        self.assertEqual(len(self.table), len(self.array))

    def test_read_all(self):
        output = self.table.read()
        npt.assert_array_equal(output, self.array)

    def test_read_slice1(self):
        output = self.table.read(0, 51)
        npt.assert_array_equal(output, self.array[0:51])

    def test_read_all_rows_specified_field(self):
        output = self.table.read(field='f1')
        npt.assert_array_equal(output, self.array['f1'])

    def test_read_slice1_specified_field(self):
        output = self.table.read(1, 64, field='f1')
        npt.assert_array_equal(output, self.array['f1'][1:64])

    def test_out_arg_with_non_numpy_flavor(self):
        output = np.empty(self.shape, self.dtype)
        self.table.flavor = 'python'
        self.assertRaises(TypeError, lambda: self.table.read(out=output))
        try:
            self.table.read(out=output)
        except TypeError as exc:
            self.assertTrue("Optional 'out' argument may only be" in str(exc))

    def test_read_all_out_arg(self):
        output = np.empty(self.shape, self.dtype)
        self.table.read(out=output)
        npt.assert_array_equal(output, self.array)

    def test_read_slice1_out_arg(self):
        output = np.empty((51, ), self.dtype)
        self.table.read(0, 51, out=output)
        npt.assert_array_equal(output, self.array[0:51])

    def test_read_all_rows_specified_field_out_arg(self):
        output = np.empty(self.shape, 'i4')
        self.table.read(field='f1', out=output)
        npt.assert_array_equal(output, self.array['f1'])

    def test_read_slice1_specified_field_out_arg(self):
        output = np.empty((63, ), 'i4')
        self.table.read(1, 64, field='f1', out=output)
        npt.assert_array_equal(output, self.array['f1'][1:64])

    def test_read_all_out_arg_sliced(self):
        output = np.empty((200, ), self.dtype)
        output['f0'] = np.random.randint(0, 10000, (200, ))
        output_orig = output.copy()
        self.table.read(out=output[0:100])
        npt.assert_array_equal(output[0:100], self.array)
        npt.assert_array_equal(output[100:], output_orig[100:])

    def test_all_fields_non_contiguous_slice_contiguous_buffer(self):
        output = np.empty((50, ), self.dtype)
        self.table.read(0, 100, 2, out=output)
        npt.assert_array_equal(output, self.array[0:100:2])

    def test_specified_field_non_contiguous_slice_contiguous_buffer(self):
        output = np.empty((50, ), 'i4')
        self.table.read(0, 100, 2, field='f3', out=output)
        npt.assert_array_equal(output, self.array['f3'][0:100:2])

    def test_all_fields_non_contiguous_buffer(self):
        output = np.empty((100, ), self.dtype)
        output_slice = output[0:100:2]
        self.assertRaises(ValueError, self.table.read, 0, 100, 2, None,
                          output_slice)
        # once Python 2.6 support is dropped, this could change
        # to assertRaisesRegexp to check exception type and message at once
        try:
            self.table.read(0, 100, 2, field=None, out=output_slice)
        except ValueError as exc:
            self.assertEqual('output array not C contiguous', str(exc))

    def test_specified_field_non_contiguous_buffer(self):
        output = np.empty((100, ), 'i4')
        output_slice = output[0:100:2]
        self.assertRaises(ValueError, self.table.read, 0, 100, 2, 'f3',
                          output_slice)
        try:
            self.table.read(0, 100, 2, field='f3', out=output_slice)
        except ValueError as exc:
            self.assertEqual('output array not C contiguous', str(exc))

    def test_all_fields_buffer_too_small(self):
        output = np.empty((99, ), self.dtype)
        self.assertRaises(ValueError, lambda: self.table.read(out=output))
        try:
            self.table.read(out=output)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))

    def test_specified_field_buffer_too_small(self):
        output = np.empty((99, ), 'i4')
        func = lambda: self.table.read(field='f5', out=output)
        self.assertRaises(ValueError, func)
        try:
            self.table.read(field='f5', out=output)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))

    def test_all_fields_buffer_too_large(self):
        output = np.empty((101, ), self.dtype)
        self.assertRaises(ValueError, lambda: self.table.read(out=output))
        try:
            self.table.read(out=output)
        except ValueError as exc:
            self.assertTrue('output array size invalid, got' in str(exc))


class TableReadByteorderTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.system_byteorder = sys.byteorder
        self.other_byteorder = {
            'little': 'big', 'big': 'little'}[sys.byteorder]
        self.reverse_byteorders = {'little': '<', 'big': '>'}

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def create_table(self, byteorder):
        table_dtype_code = self.reverse_byteorders[byteorder] + 'i4'
        table_dtype = np.format_parser([table_dtype_code, 'a1'], [], []).dtype
        self.table = self.fileh.create_table('/', 'table', table_dtype,
                                             byteorder=byteorder)
        input_dtype = np.format_parser(['i4', 'a1'], [], []).dtype
        self.input_array = np.zeros((10, ), input_dtype)
        self.input_array['f0'] = np.arange(10)
        self.input_array['f1'] = b'a'
        self.table.append(self.input_array)

    def test_table_system_byteorder_no_out_argument(self):
        self.create_table(self.system_byteorder)
        output = self.table.read()
        self.assertEqual(byteorders[output['f0'].dtype.byteorder],
                         self.system_byteorder)
        npt.assert_array_equal(output['f0'], np.arange(10))

    def test_table_other_byteorder_no_out_argument(self):
        self.create_table(self.other_byteorder)
        output = self.table.read()
        self.assertEqual(byteorders[output['f0'].dtype.byteorder],
                         self.system_byteorder)
        npt.assert_array_equal(output['f0'], np.arange(10))

    def test_table_system_byteorder_out_argument_system_byteorder(self):
        self.create_table(self.system_byteorder)
        out_dtype_code = self.reverse_byteorders[self.system_byteorder] + 'i4'
        out_dtype = np.format_parser([out_dtype_code, 'a1'], [], []).dtype
        output = np.empty((10, ), out_dtype)
        self.table.read(out=output)
        self.assertEqual(byteorders[output['f0'].dtype.byteorder],
                         self.system_byteorder)
        npt.assert_array_equal(output['f0'], np.arange(10))

    def test_table_other_byteorder_out_argument_system_byteorder(self):
        self.create_table(self.other_byteorder)
        out_dtype_code = self.reverse_byteorders[self.system_byteorder] + 'i4'
        out_dtype = np.format_parser([out_dtype_code, 'a1'], [], []).dtype
        output = np.empty((10, ), out_dtype)
        self.table.read(out=output)
        self.assertEqual(byteorders[output['f0'].dtype.byteorder],
                         self.system_byteorder)
        npt.assert_array_equal(output['f0'], np.arange(10))

    def test_table_system_byteorder_out_argument_other_byteorder(self):
        self.create_table(self.system_byteorder)
        out_dtype_code = self.reverse_byteorders[self.other_byteorder] + 'i4'
        out_dtype = np.format_parser([out_dtype_code, 'a1'], [], []).dtype
        output = np.empty((10, ), out_dtype)
        self.assertRaises(ValueError, lambda: self.table.read(out=output))
        try:
            self.table.read(out=output)
        except ValueError as exc:
            self.assertTrue("array must be in system's byteorder" in str(exc))

    def test_table_other_byteorder_out_argument_other_byteorder(self):
        self.create_table(self.other_byteorder)
        out_dtype_code = self.reverse_byteorders[self.other_byteorder] + 'i4'
        out_dtype = np.format_parser([out_dtype_code, 'a1'], [], []).dtype
        output = np.empty((10, ), out_dtype)
        self.assertRaises(ValueError, lambda: self.table.read(out=output))
        try:
            self.table.read(out=output)
        except ValueError as exc:
            self.assertTrue("array must be in system's byteorder" in str(exc))


class BasicRangeTestCase(unittest.TestCase):
    # file  = "test.h5"
    mode = "w"
    title = "This is the table title"
    record = Record
    maxshort = 1 << 15
    expectedrows = 100
    compress = 0
    shuffle = 1
    # Default values
    nrows = 20
    nrowsinbuf = 3  # Choose a small value for the buffer size
    start = 1
    stop = nrows
    checkrecarray = 0
    checkgetCol = 0

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        for j in range(3):
            # Create a table
            filterprops = Filters(complevel=self.compress,
                                  shuffle=self.shuffle)
            table = self.fileh.create_table(group, 'table'+str(j), self.record,
                                            title=self.title,
                                            filters=filterprops,
                                            expectedrows=self.expectedrows)
            # Get the row object associated with the new table
            row = table.row

            # Fill the table
            for i in xrange(self.expectedrows):
                row['var1'] = '%04d' % (self.expectedrows - i)
                row['var7'] = row['var1'][-1]
                row['var2'] = i
                row['var3'] = i % self.maxshort
                if isinstance(row['var4'], np.ndarray):
                    row['var4'] = [float(i), float(i * i)]
                else:
                    row['var4'] = float(i)
                if isinstance(row['var5'], np.ndarray):
                    row['var5'] = np.array((float(i),)*4)
                else:
                    row['var5'] = float(i)
                # var6 will be like var3 but byteswaped
                row['var6'] = ((row['var3'] >> 8) & 0xff) + \
                              ((row['var3'] << 8) & 0xff00)
                row.append()

            # Flush the buffer for this table
            table.flush()
            # Create a new group (descendant of group)
            group2 = self.fileh.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        if self.fileh.isopen:
            self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def check_range(self):
        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        table.nrowsinbuf = self.nrowsinbuf
        r = slice(self.start, self.stop, self.step)
        resrange = r.indices(table.nrows)
        reslength = len(range(*resrange))
        #print "self.checkrecarray = ", self.checkrecarray
        #print "self.checkgetCol = ", self.checkgetCol
        if self.checkrecarray:
            recarray = table.read(self.start, self.stop, self.step)
            result = []
            for nrec in xrange(len(recarray)):
                if recarray['var2'][nrec] < self.nrows and 0 < self.step:
                    result.append(recarray['var2'][nrec])
                elif recarray['var2'][nrec] > self.nrows and 0 > self.step:
                    result.append(recarray['var2'][nrec])
        elif self.checkgetCol:
            column = table.read(self.start, self.stop, self.step, 'var2')
            result = []
            for nrec in xrange(len(column)):
                if column[nrec] < self.nrows and 0 < self.step:
                    result.append(column[nrec])
                elif column[nrec] > self.nrows and 0 > self.step:
                    result.append(column[nrec])
        else:
            if 0 < self.step:
                result = [
                    rec['var2'] for rec in table.iterrows(self.start,
                                                          self.stop,
                                                          self.step)
                    if rec['var2'] < self.nrows
                ]
            elif 0 > self.step:
                result = [
                    rec['var2'] for rec in table.iterrows(self.start,
                                                          self.stop,
                                                          self.step)
                    if rec['var2'] > self.nrows
                ]

        if self.start < 0:
            startr = self.expectedrows + self.start
        else:
            startr = self.start

        if self.stop is None:
            if self.checkrecarray or self.checkgetCol:
                # data read using the read method
                stopr = startr + 1
            else:
                # data read using the iterrows method
                stopr = self.nrows
        elif self.stop < 0:
            stopr = self.expectedrows + self.stop
        else:
            stopr = self.stop

        if self.nrows < stopr:
            stopr = self.nrows

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            if reslength:
                if self.checkrecarray:
                    print("Last record *read* in recarray ==>", recarray[-1])
                elif self.checkgetCol:
                    print("Last value *read* in getCol ==>", column[-1])
                else:
                    print("Last record *read* in table range ==>", rec)
            print("Total number of selected records ==>", len(result))
            print("Selected records:\n", result)
            print("Selected records should look like:\n",
                  range(startr, stopr, self.step))
            print("start, stop, step ==>", self.start, self.stop, self.step)
            print("startr, stopr, step ==>", startr, stopr, self.step)

        self.assertEqual(result, range(startr, stopr, self.step))
        if not (self.checkrecarray or self.checkgetCol):
            if startr < stopr and 0 < self.step:
                rec = [r for r in table.iterrows(self.start, self.stop,
                                                 self.step)
                       if r['var2'] < self.nrows][-1]
                if self.nrows < self.expectedrows:
                    self.assertEqual(
                        rec['var2'],
                        range(self.start, self.stop, self.step)[-1])
                else:
                    self.assertEqual(
                        rec['var2'],
                        range(startr, stopr, self.step)[-1])
            elif startr > stopr and 0 > self.step:
                rec = [r['var2'] for r in table.iterrows(self.start, self.stop,
                                                         self.step)
                       if r['var2'] > self.nrows][0]
                if self.nrows < self.expectedrows:
                    self.assertEqual(
                        rec,
                        range(self.start, self.stop or -1, self.step)[0])
                else:
                    self.assertEqual(
                        rec,
                        range(startr, stopr or -1, self.step)[0])

        # Close the file
        self.fileh.close()

    def test01_range(self):
        """Checking ranges in table iterators (case1)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 2 * step
        self.nrows = 21
        self.nrowsinbuf = 3
        self.start = 0
        self.stop = self.expectedrows
        self.step = 2

        self.check_range()

    def test01a_range(self):
        """Checking ranges in table iterators (case1)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 2 * step
        self.nrows = 21
        self.nrowsinbuf = 3
        self.start = self.expectedrows - 1
        self.stop = None
        self.step = -2

        self.check_range()

    def test02_range(self):
        """Checking ranges in table iterators (case2)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 10 * step
        self.nrows = 21
        self.nrowsinbuf = 31
        self.start = 11
        self.stop = self.expectedrows
        self.step = 3

        self.check_range()

    def test03_range(self):
        """Checking ranges in table iterators (case3)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 1.1 * step
        self.nrows = self.expectedrows
        self.nrowsinbuf = 11  # Choose a small value for the buffer size
        self.start = 0
        self.stop = self.expectedrows
        self.step = 10

        self.check_range()

    def test04_range(self):
        """Checking ranges in table iterators (case4)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_range..." % self.__class__.__name__)

        # Case where step == nrowsinbuf
        self.nrows = self.expectedrows
        self.nrowsinbuf = 11  # Choose a small value for the buffer size
        self.start = 1
        self.stop = self.expectedrows
        self.step = 11

        self.check_range()

    def test05_range(self):
        """Checking ranges in table iterators (case5)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_range..." % self.__class__.__name__)

        # Case where step > 1.1 * nrowsinbuf
        self.nrows = 21
        self.nrowsinbuf = 10  # Choose a small value for the buffer size
        self.start = 1
        self.stop = self.expectedrows
        self.step = 11

        self.check_range()

    def test06_range(self):
        """Checking ranges in table iterators (case6)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06_range..." % self.__class__.__name__)

        # Case where step > 3 * nrowsinbuf
        self.nrows = 3
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = 2
        self.stop = self.expectedrows
        self.step = 10

        self.check_range()

    def test07_range(self):
        """Checking ranges in table iterators (case7)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_range..." % self.__class__.__name__)

        # Case where start == stop
        self.nrows = 2
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = self.nrows
        self.stop = self.nrows
        self.step = 10

        self.check_range()

    def test08_range(self):
        """Checking ranges in table iterators (case8)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_range..." % self.__class__.__name__)

        # Case where start > stop
        self.nrows = 2
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = self.nrows + 1
        self.stop = self.nrows
        self.step = 1

        self.check_range()

    def test09_range(self):
        """Checking ranges in table iterators (case9)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09_range..." % self.__class__.__name__)

        # Case where stop = None (last row)
        self.nrows = 100
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = 1
        self.stop = 2
        self.step = 1

        self.check_range()

    def test10_range(self):
        """Checking ranges in table iterators (case10)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10_range..." % self.__class__.__name__)

        # Case where start < 0 and stop = None (last row)
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -6
        self.startr = self.expectedrows + self.start
        self.stop = -5
        self.stopr = self.expectedrows
        self.step = 2

        self.check_range()

    def test10a_range(self):
        """Checking ranges in table iterators (case10a)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10a_range..." % self.__class__.__name__)

        # Case where start < 0 and stop = 0
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -6
        self.startr = self.expectedrows + self.start
        self.stop = 0
        self.stopr = self.expectedrows
        self.step = 2

        self.check_range()

    def test11_range(self):
        """Checking ranges in table iterators (case11)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11_range..." % self.__class__.__name__)

        # Case where start < 0 and stop < 0
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -6
        self.startr = self.expectedrows + self.start
        self.stop = -2
        self.stopr = self.expectedrows + self.stop
        self.step = 1

        self.check_range()

    def test12_range(self):
        """Checking ranges in table iterators (case12)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test12_range..." % self.__class__.__name__)

        # Case where start < 0 and stop < 0 and start > stop
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -1
        self.startr = self.expectedrows + self.start
        self.stop = -2
        self.stopr = self.expectedrows + self.stop
        self.step = 1

        self.check_range()

    def test13_range(self):
        """Checking ranges in table iterators (case13)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13_range..." % self.__class__.__name__)

        # Case where step < 0
        self.step = -11
        try:
            self.check_range()
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
            self.fileh.close()
        #else:
        #    print rec
        #    self.fail("expected a ValueError")

        # Case where step == 0
        self.step = 0
        try:
            self.check_range()
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
            self.fileh.close()
        #else:
        #    print rec
        #    self.fail("expected a ValueError")


class IterRangeTestCase(BasicRangeTestCase):
    pass


class RecArrayRangeTestCase(BasicRangeTestCase):
    checkrecarray = 1


class getColRangeTestCase(BasicRangeTestCase):
    checkgetCol = 1

    def test01_nonexistentField(self):
        """Checking non-existing Field in getCol method """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_nonexistentField..." %
                  self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        self.root = self.fileh.root
        table = self.fileh.get_node("/table0")

        try:
            # column = table.read(field='non-existent-column')
            table.col('non-existent-column')
        except KeyError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next KeyError was catched!")
                print(value)
            self.fileh.close()
        else:
            print(rec)
            self.fail("expected a KeyError")


class getItemTestCase(unittest.TestCase):
    mode = "w"
    title = "This is the table title"
    record = Record
    maxshort = 1 << 15
    expectedrows = 100
    compress = 0
    shuffle = 1
    # Default values
    nrows = 20
    nrowsinbuf = 3  # Choose a small value for the buffer size
    start = 1
    stop = nrows
    checkrecarray = 0
    checkgetCol = 0

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        for j in range(3):
            # Create a table
            filterprops = Filters(complevel=self.compress,
                                  shuffle=self.shuffle)
            table = self.fileh.create_table(group, 'table'+str(j), self.record,
                                            title=self.title,
                                            filters=filterprops,
                                            expectedrows=self.expectedrows)
            # Get the row object associated with the new table
            row = table.row

            # Fill the table
            for i in xrange(self.expectedrows):
                row['var1'] = '%04d' % (self.expectedrows - i)
                row['var7'] = row['var1'][-1]
                row['var2'] = i
                row['var3'] = i % self.maxshort
                if isinstance(row['var4'], np.ndarray):
                    row['var4'] = [float(i), float(i * i)]
                else:
                    row['var4'] = float(i)
                if isinstance(row['var5'], np.ndarray):
                    row['var5'] = np.array((float(i),)*4)
                else:
                    row['var5'] = float(i)
                # var6 will be like var3 but byteswaped
                row['var6'] = ((row['var3'] >> 8) & 0xff) + \
                              ((row['var3'] << 8) & 0xff00)
                row.append()

            # Flush the buffer for this table
            table.flush()
            # Create a new group (descendant of group)
            group2 = self.fileh.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        if self.fileh.isopen:
            self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01a_singleItem(self):
        """Checking __getitem__ method with single parameter (int)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[2]
        self.assertEqual(result["var2"], 2)
        result = table[25]
        self.assertEqual(result["var2"], 25)
        result = table[self.expectedrows-1]
        self.assertEqual(result["var2"], self.expectedrows - 1)

    def test01b_singleItem(self):
        """Checking __getitem__ method with single parameter (neg.

        int)

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[-5]
        self.assertEqual(result["var2"], self.expectedrows - 5)
        result = table[-1]
        self.assertEqual(result["var2"], self.expectedrows - 1)
        result = table[-self.expectedrows]
        self.assertEqual(result["var2"], 0)

    def test01c_singleItem(self):
        """Checking __getitem__ method with single parameter (long)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[2]
        self.assertEqual(result["var2"], 2)
        result = table[25]
        self.assertEqual(result["var2"], 25)
        result = table[self.expectedrows-1]
        self.assertEqual(result["var2"], self.expectedrows - 1)

    def test01d_singleItem(self):
        """Checking __getitem__ method with single parameter (neg.

        long)

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01d_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[-5]
        self.assertEqual(result["var2"], self.expectedrows - 5)
        result = table[-1]
        self.assertEqual(result["var2"], self.expectedrows - 1)
        result = table[-self.expectedrows]
        self.assertEqual(result["var2"], 0)

    def test01e_singleItem(self):
        """Checking __getitem__ method with single parameter (rank-0 ints)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01e_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[np.array(2)]
        self.assertEqual(result["var2"], 2)
        result = table[np.array(25)]
        self.assertEqual(result["var2"], 25)
        result = table[np.array(self.expectedrows-1)]
        self.assertEqual(result["var2"], self.expectedrows - 1)

    def test02_twoItems(self):
        """Checking __getitem__ method with start, stop parameters."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_twoItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[2:6]
        self.assertEqual(result["var2"].tolist(), range(2, 6))
        result = table[2:-6]
        self.assertEqual(result["var2"].tolist(), range(
            2, self.expectedrows-6))
        result = table[2:]
        self.assertEqual(result["var2"].tolist(), range(2, self.expectedrows))
        result = table[-2:]
        self.assertEqual(result["var2"].tolist(),
                         range(self.expectedrows-2, self.expectedrows))

    def test03_threeItems(self):
        """Checking __getitem__ method with start, stop, step parameters."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_threeItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        result = table[2:6:3]
        self.assertEqual(result["var2"].tolist(), range(2, 6, 3))
        result = table[2::3]
        self.assertEqual(result["var2"].tolist(), range(
            2, self.expectedrows, 3))
        result = table[:6:2]
        self.assertEqual(result["var2"].tolist(), range(0, 6, 2))
        result = table[::]
        self.assertEqual(result["var2"].tolist(), range(
            0, self.expectedrows, 1))

    def test04_negativeStep(self):
        """Checking __getitem__ method with negative step parameter."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_negativeStep..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        try:
            table[2:3:-3]
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
        else:
            self.fail("expected a ValueError")

    def test06a_singleItemCol(self):
        """Checking __getitem__ method in Col with single parameter."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06a_singleItemCol..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        colvar2 = table.cols.var2
        self.assertEqual(colvar2[2], 2)
        self.assertEqual(colvar2[25], 25)
        self.assertEqual(colvar2[self.expectedrows-1], self.expectedrows - 1)

    def test06b_singleItemCol(self):
        """Checking __getitem__ method in Col with single parameter.

        (negative)

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06b_singleItem..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        colvar2 = table.cols.var2
        self.assertEqual(colvar2[-5], self.expectedrows - 5)
        self.assertEqual(colvar2[-1], self.expectedrows - 1)
        self.assertEqual(colvar2[-self.expectedrows], 0)

    def test07_twoItemsCol(self):
        """Checking __getitem__ method in Col with start, stop parameters."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_twoItemCol..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        colvar2 = table.cols.var2
        self.assertEqual(colvar2[2:6].tolist(), range(2, 6))
        self.assertEqual(colvar2[2:-6].tolist(), range(2, self.expectedrows-6))
        self.assertEqual(colvar2[2:].tolist(), range(2, self.expectedrows))
        self.assertEqual(colvar2[-2:].tolist(),
                         range(self.expectedrows-2, self.expectedrows))

    def test08_threeItemsCol(self):
        """Checking __getitem__ method in Col with start, stop, step
        parameters."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_threeItemCol..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        colvar2 = table.cols.var2
        self.assertEqual(colvar2[2:6:3].tolist(), range(2, 6, 3))
        self.assertEqual(colvar2[2::3].tolist(), range(
            2, self.expectedrows, 3))
        self.assertEqual(colvar2[:6:2].tolist(), range(0, 6, 2))
        self.assertEqual(colvar2[::].tolist(), range(0, self.expectedrows, 1))

    def test09_negativeStep(self):
        """Checking __getitem__ method in Col with negative step parameter."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09_negativeStep..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        colvar2 = table.cols.var2
        try:
            colvar2[2:3:-3]
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
        else:
            self.fail("expected a ValueError")

    def test10_list_integers(self):
        """Checking accessing Table with a list of integers."""

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        idx = list(range(10, 70, 11))

        result = table[idx]
        self.assertEqual(result["var2"].tolist(), idx)

        result = table.read_coordinates(idx)
        self.assertEqual(result["var2"].tolist(), idx)

    def test11_list_booleans(self):
        """Checking accessing Table with a list of boolean values."""

        self.fileh = open_file(self.file, "r")
        table = self.fileh.root.table0
        idx = list(range(10, 70, 11))

        selection = [n in idx for n in range(self.expectedrows)]

        result = table[selection]
        self.assertEqual(result["var2"].tolist(), idx)

        result = table.read_coordinates(selection)
        self.assertEqual(result["var2"].tolist(), idx)


class Rec(IsDescription):
    col1 = IntCol(pos=1)
    col2 = StringCol(itemsize=3, pos=2)
    col3 = FloatCol(pos=3)


class setItem(common.PyTablesTestCase):
    def tearDown(self):
        self.fileh.close()
        # del self.fileh, self.rootgroup
        os.remove(self.file)
        common.cleanup(self)

    def test01(self):
        "Checking modifying one table row with __setitem__"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing row
        table[2] = (456, 'db2', 1.2)
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [456, b'db2', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test01b(self):
        "Checking modifying one table row with __setitem__ (long index)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing row
        table[2] = (456, 'db2', 1.2)
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [456, b'db2', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test02(self):
        "Modifying one row, with a step (__setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, b'db1', 1.2]], formats="i4,a3,f8")
        table[1:3:2] = rows
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test03(self):
        "Checking modifying several rows at once (__setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, b'db1', 1.2], [5, b'de1', 1.3]],
                             formats="i4,a3,f8")
        # table.modify_rows(start=1, rows=rows)
        table[1:3] = rows
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [5, b'de1', 1.3], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test04(self):
        "Modifying several rows at once, with a step (__setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, b'db1', 1.2], [6, b'de2', 1.3]],
                             formats="i4,a3,f8")
        # table[1:4:2] = rows
        table[1::2] = rows
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [457, b'db1', 1.2], [6, b'de2', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test05(self):
        "Checking modifying one column (single element, __setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3]],
                          formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1] = -1
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [-1, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06a(self):
        "Checking modifying one column (several elements, __setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1:4] = [2, 3, 4]
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06b(self):
        "Checking modifying one column (iterator, __setitem__)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        try:
            for row in table.iterrows():
                row['col1'] = row.nrow + 1
                row.append()
            table.flush()
        except NotImplementedError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NotImplementedError was catched!")
                print(value)
        else:
            self.fail("expected a NotImplementedError")

#         # Create the modified recarray
#         r1=records.array([[1,b'dbe',1.2],[2,b'ded',1.3],
#                           [3,b'db1',1.2],[4,b'de1',1.3]],
#                          formats="i4,a3,f8",
#                          names = "col1,col2,col3")
#         # Read the modified table
#         if self.reopen:
#             self.fileh.close()
#             self.fileh = open_file(self.file, "r")
#             table = self.fileh.root.recarray
#             table.nrowsinbuf = self.buffersize  # set buffer value
#         r2 = table.read()
#         if common.verbose:
#             print "Original table-->", repr(r2)
#             print "Should look like-->", repr(r1)
#         self.assertEqual(r1.tostring(), r2.tostring())
#         self.assertEqual(table.nrows, 4)

    def test07(self):
        "Modifying one column (several elements, __setitem__, step)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          1, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify just one existing column
        table.cols.col1[1:4:2] = [2, 3]
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [3, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test08(self):
        "Modifying one column (one element, __setitem__, step)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1:4:3] = [2]
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test09(self):
        "Modifying beyond the table extend (__setitem__, step)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Try to modify beyond the extend
        # This will silently exclude the non-fitting rows
        rows = records.array([[457, b'db1', 1.2], [6, b'de2', 1.3]],
                             formats="i4,a3,f8")
        table[1::2] = rows
        # How it should look like
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [457, b'db1', 1.2], [6, b'de2', 1.3]],
                           formats="i4,a3,f8")

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)


class setItem1(setItem):
    reopen = 0
    buffersize = 1


class setItem2(setItem):
    reopen = 1
    buffersize = 2


class setItem3(setItem):
    reopen = 0
    buffersize = 1000


class setItem4(setItem):
    reopen = 1
    buffersize = 1000


class updateRow(common.PyTablesTestCase):
    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01(self):
        "Checking modifying one table row with Row.update"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing row
        for row in table.iterrows(2, 3):
            (row['col1'], row['col2'], row['col3']) = [456, 'db2', 1.2]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [456, b'db2', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test02(self):
        "Modifying one row, with a step (Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, 3, 2):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 3:
                (row['col1'], row['col2'], row['col3']) = [6, 'de2', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test03(self):
        "Checking modifying several rows at once (Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, 3):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 2:
                (row['col1'], row['col2'], row['col3']) = [5, 'de1', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [5, b'de1', 1.3], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test04(self):
        "Modifying several rows at once, with a step (Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, stop=4, step=2):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 3:
                (row['col1'], row['col2'], row['col3']) = [6, 'de2', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [457, b'db1', 1.2], [6, b'de2', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test05(self):
        "Checking modifying one column (single element, Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3]],
                          formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        for row in table.iterrows(1, 2):
            row['col1'] = -1
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [-1, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06(self):
        "Checking modifying one column (several elements, Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        for row in table.iterrows(1, 4):
            row['col1'] = row.nrow + 1
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test07(self):
        "Modifying values from a selection"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          1, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify just rows with col1 < 456
        for row in table.where('col1 < 456'):
            row['col1'] = 2
            row['col2'] = 'ada'
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ada', 1.3],
                            [457, b'db1', 1.2], [2, b'ada', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test08(self):
        "Modifying a large table (Row.update)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify all the rows
        for row in table:
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i
            r1['col2'][i] = 'b'+str(i)
            r1['col3'][i] = 0.0
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test08b(self):
        "Setting values on a large table without calling Row.update"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify all the rows (actually don't)
        for row in table:
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            # row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i-1
            r1['col2'][i] = 'a'+str(i-1)
            r1['col3'][i] = -1.0
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test09(self):
        "Modifying selected values on a large table"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify selected rows
        for row in table.where('col1 > nrows-3'):
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i-1
            r1['col2'][i] = 'a'+str(i-1)
            r1['col3'][i] = -1.0
        # modify just the last line
        r1['col1'][i] = i
        r1['col2'][i] = 'b'+str(i)
        r1['col3'][i] = 0.0

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test09b(self):
        "Modifying selected values on a large table (alternate values)"

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create a new table:
        table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        table.nrowsinbuf = self.buffersize  # set buffer value

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify selected rows
        for row in table.iterrows(step=10):
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        for i in xrange(nrows):
            if i % 10 > 0:
                r1['col1'][i] = i-1
                r1['col2'][i] = 'a'+str(i-1)
                r1['col3'][i] = -1.0
            else:
                r1['col1'][i] = i
                r1['col2'][i] = 'b'+str(i)
                r1['col3'][i] = 0.0

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)


class updateRow1(updateRow):
    reopen = 0
    buffersize = 1


class updateRow2(updateRow):
    reopen = 1
    buffersize = 2


class updateRow3(updateRow):
    reopen = 0
    buffersize = 1000


class updateRow4(updateRow):
    reopen = 1
    buffersize = 1000


class RecArrayIO(unittest.TestCase):
    def test00(self):
        "Checking saving a regular recarray"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')

        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r)

        # Read it again
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
        r2 = fileh.root.recarray.read()
        self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test01(self):
        "Checking saving a recarray with an offset in its buffer"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')

        # Get an offsetted bytearray
        r1 = r[1:]

        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)

        # Read it again
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test02(self):
        "Checking saving a large recarray with an offset in its buffer"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array(b'a'*200000, 'f4,3i4,a5,i2', 3000)

        # Get an offsetted bytearray
        r1 = r[2000:]

        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)

        # Read it again
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test03(self):
        "Checking saving a strided recarray with an offset in its buffer"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array(b'a'*200000, 'f4,3i4,a5,i2', 3000)

        # Get an strided recarray
        r2 = r[::2]

        # Get an offsetted bytearray
        r1 = r2[1200:]

        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)

        # Read it again
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())
        fileh.close()
        os.remove(file)

    def test04(self):
        "Checking appending several rows at once"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        class Rec(IsDescription):
            col1 = IntCol(pos=1)
            col2 = StringCol(itemsize=3, pos=2)
            col3 = FloatCol(pos=3)

        # Save it in a table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Create the complete table
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the original table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = fileh.root.recarray.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test05(self):
        "Checking appending several rows at once (close file version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Save it in a table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        fileh.close()
        fileh = open_file(file, "r")
        table = fileh.root.recarray
        # Create the complete table
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the original table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = fileh.root.recarray.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test06a(self):
        "Checking modifying one table row (list version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([(456, b'dbe', 1.2), (
            2, b'ded', 1.3)], formats="i4,a3,f8")
        table.append(r)
        table.append([(457, b'db1', 1.2), (5, b'de1', 1.3)])
        # Modify just one existing rows
        table.modify_rows(start=1, rows=[(456, 'db1', 1.2)])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [456, b'db1', 1.2],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test06b(self):
        "Checking modifying one table row (recarray version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify just one existing rows
        table.modify_rows(start=2, rows=records.array([[456, 'db2', 1.2]],
                                                      formats="i4,a3,f8"))
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [456, b'db2', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test07a(self):
        "Checking modifying several rows at once (list version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify two existing rows
        table.modify_rows(start=1, rows=[(457, 'db1', 1.2), (5, 'de1', 1.3)])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [5, b'de1', 1.3], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test07b(self):
        "Checking modifying several rows at once (recarray version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify two existing rows
        rows = records.array([[457, b'db1', 1.2], [5, b'de1', 1.3]],
                             formats="i4,a3,f8")
        table.modify_rows(start=1, rows=rows)
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [457, b'db1', 1.2],
                            [5, b'de1', 1.3], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test07c(self):
        "Checking modifying several rows with a mismatching value"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07c..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify two existing rows
        rows = records.array([[457, b'db1', 1.2], [5, b'de1', 1.3]],
                             formats="i4,a3,f8")
        self.assertRaises(ValueError, table.modify_rows,
                          start=1, stop=2, rows=rows)
        fileh.close()
        os.remove(file)

    def test08a(self):
        "Checking modifying one column (single column version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        table.modify_columns(start=1, columns=[[2, 3, 4]], names=["col1"])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08a2(self):
        "Checking modifying one column (single column version, modify_column)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08a2..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        table.modify_column(start=1, column=[2, 3, 4], colname="col1")
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08b(self):
        "Checking modifying one column (single column version, recarray)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        columns = records.fromarrays(np.array([[2, 3, 4]]), formats="i4")
        table.modify_columns(start=1, columns=columns, names=["col1"])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08b2(self):
        """Checking modifying one column (single column version, recarray,
        modify_column)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08b2..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        columns = records.fromarrays(np.array([[2, 3, 4]]), formats="i4")
        table.modify_column(start=1, column=columns, colname="col1")
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'ded', 1.3],
                            [3, b'db1', 1.2], [4, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08c(self):
        "Checking modifying one column (single column version, single element)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08c..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify just one existing column
        # columns = records.fromarrays(np.array([[4]]), formats="i4")
        # table.modify_columns(start=1, columns=columns, names=["col1"])
        table.modify_columns(start=1, columns=[[4]], names=["col1"])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [4, b'ded', 1.3],
                            [457, b'db1', 1.2], [5, b'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test09a(self):
        "Checking modifying table columns (multiple column version)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify a couple of columns
        columns = [["aaa", "bbb", "ccc"], [1.2, .1, .3]]
        table.modify_columns(start=1, columns=columns, names=["col2", "col3"])
        # Create the modified recarray
        r1 = records.array([[456, b'dbe', 1.2], [2, b'aaa', 1.2],
                            [457, b'bbb', .1], [5, b'ccc', .3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")

        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test09b(self):
        "Checking modifying table columns (multiple columns, recarray)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify a couple of columns
        columns = records.array([["aaa", 1.2], ["bbb", .1], ["ccc", .3]],
                                formats="a3,f8")
        table.modify_columns(start=1, columns=columns, names=["col2", "col3"])
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'aaa', 1.2],
                            [457, 'bbb', .1], [5, 'ccc', .3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test09c(self):
        "Checking modifying table columns (single column, step)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09c..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])
        # Modify a couple of columns
        columns = records.array([["aaa", 1.2], ["bbb", .1]],
                                formats="a3,f8")
        table.modify_columns(start=1, step=2, columns=columns,
                             names=["col2", "col3"])
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'aaa', 1.2],
                            [457, 'db1', 1.2], [5, 'bbb', .1]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test09d(self):
        "Checking modifying table columns (multiple columns, step)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09d..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        # Modify a couple of columns
        columns = records.array([["aaa", 1.3], ["bbb", .1]],
                                formats="a3,f8")
        table.modify_columns(start=0, step=2, columns=columns,
                             names=["col2", "col3"])
        # Create the modified recarray
        r1 = records.array([[456, 'aaa', 1.3], [2, 'ded', 1.3],
                            [457, 'bbb', .1], [5, 'de1', 1.3]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test10a(self):
        "Checking modifying rows using coordinates (readCoords/modifyCoords)."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        columns = table.read_coordinates([0, 3])

        # Modify both rows
        columns['col1'][:] = [55, 56]
        columns['col3'][:] = [1.9, 1.8]

        # Modify the table in the same coordinates
        table.modify_coordinates([0, 3], columns)

        # Create the modified recarray
        r1 = records.array([[55, b'dbe', 1.9], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [56, b'de1', 1.8]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test10b(self):
        "Checking modifying rows using coordinates (getitem/setitem)."

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # append new rows
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'ded', 1.3]], formats="i4,a3,f8")
        table.append(r)
        table.append([[457, b'db1', 1.2], [5, b'de1', 1.3]])

        columns = table[[0, 3]]

        # Modify both rows
        columns['col1'][:] = [55, 56]
        columns['col3'][:] = [1.9, 1.8]

        # Modify the table in the same coordinates
        table[[0, 3]] = columns

        # Create the modified recarray
        r1 = records.array([[55, b'dbe', 1.9], [2, b'ded', 1.3],
                            [457, b'db1', 1.2], [56, b'de1', 1.8]],
                           formats="i4,a3,f8",
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            fileh.close()
            fileh = open_file(file, "r")
            table = fileh.root.recarray
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)


class RecArrayIO1(RecArrayIO):
    reopen = 0


class RecArrayIO2(RecArrayIO):
    reopen = 1


class CopyTestCase(unittest.TestCase):
    def assertEqualColinstances(self, table1, table2):
        """Assert that column instance maps of both tables are equal."""
        cinst1, cinst2 = table1.colinstances, table2.colinstances
        self.assertEqual(len(cinst1), len(cinst2))
        for (cpathname, col1) in cinst1.iteritems():
            self.assertTrue(cpathname in cinst2)
            col2 = cinst2[cpathname]
            self.assertTrue(isinstance(col1, type(col2)))
            if isinstance(col1, Column):
                self.assertEqual(col1.name, col2.name)
                self.assertEqual(col1.pathname, col2.pathname)
                self.assertEqual(col1.dtype, col2.dtype)
                self.assertEqual(col1.type, col2.type)
            elif isinstance(col1, Cols):
                self.assertEqual(col1._v_colnames, col2._v_colnames)
                self.assertEqual(col1._v_colpathnames, col2._v_colpathnames)

    def test01_copy(self):
        """Checking Table.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table
        table2 = table1.copy('/', 'table2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1
            table2 = fileh.root.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            # print "dirs-->", dir(table1), dir(table2)
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            # row1 is a Row instance, while table2[] is a
            # RecArray.Record instance
            # print "reprs-->", repr(row1), repr(table2.read(nrow))
            for colname in table1.colnames:
                # Both ways to compare works well
                # self.assertEqual(row1[colname], table2[nrow][colname))
                self.assertEqual(row1[colname],
                                 table2.read(nrow, field=colname)[0])

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # This could be not the same when re-opening the file
        # self.assertEqual(table1.description._v_ColObjects,
        #                  table2.description._v_ColObjects)
        # Leaf attributes
        self.assertEqual(table1.title, table2.title)
        self.assertEqual(table1.filters.complevel, table2.filters.complevel)
        self.assertEqual(table1.filters.complib, table2.filters.complib)
        self.assertEqual(table1.filters.shuffle, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_copy(self):
        """Checking Table.copy() method (where specified)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table in another group
        group1 = fileh.create_group("/", "group1")
        table2 = table1.copy(group1, 'table2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            table1 = fileh.root.table1
            table2 = fileh.root.group1.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            for colname in table1.colnames:
                # Both ways to compare works well
                # self.assertEqual(row1[colname], table2[nrow][colname))
                self.assertEqual(row1[colname],
                                 table2.read(nrow, field=colname)[0])

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # Leaf attributes
        self.assertEqual(table1.title, table2.title)
        self.assertEqual(table1.filters.complevel, table2.filters.complevel)
        self.assertEqual(table1.filters.complib, table2.filters.complib)
        self.assertEqual(table1.filters.shuffle, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03_copy(self):
        """Checking Table.copy() method (table larger than buffer)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray exceeding buffers capability
        # This works, but takes too much CPU for a test
        # It is better to reduce the buffer size (table1.nrowsinbuf)
#         r=records.array(b'aaaabbbbccccddddeeeeffffgggg'*20000,
#                         formats='2i2,i4, (2,3)u2, (1,)f4, f8',shape=700)
        r = records.array(b'aaaabbbbccccddddeeeeffffgggg' * 200,
                          formats='2i2,i4, (2,3)u2, (1,)f4, f8', shape=7)
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table in another group and other title
        group1 = fileh.create_group("/", "group1")
        table1.nrowsinbuf = 2  # small value of buffer
        table2 = table1.copy(group1, 'table2', title="title table2")
        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            table1 = fileh.root.table1
            table2 = fileh.root.group1.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            for colname in table1.colnames:
                # self.assertTrue(allequal(row1[colname],
                # table2[nrow][colname]))
                self.assertTrue(allequal(row1[colname],
                                         table2.read(nrow, field=colname)[0]))

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # Leaf attributes
        self.assertEqual("title table2", table2.title)
        self.assertEqual(table1.filters.complevel, table2.filters.complevel)
        self.assertEqual(table1.filters.complib, table2.filters.complib)
        self.assertEqual(table1.filters.shuffle, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)

        # Close the file
        fileh.close()
        os.remove(file)

    def test04_copy(self):
        """Checking Table.copy() method (different compress level)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table in another group
        group1 = fileh.create_group("/", "group1")
        table2 = table1.copy(group1, 'table2',
                             filters=Filters(complevel=6))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            table1 = fileh.root.table1
            table2 = fileh.root.group1.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            for colname in table1.colnames:
                # Both ways to compare works well
                # self.assertEqual(row1[colname], table2[nrow][colname))
                self.assertEqual(row1[colname],
                                 table2.read(nrow, field=colname)[0])

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # Leaf attributes
        self.assertEqual(table1.title, table2.title)
        self.assertEqual(6, table2.filters.complevel)
        self.assertEqual(1, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)

        # Close the file
        fileh.close()
        os.remove(file)

    def test05_copy(self):
        """Checking Table.copy() method (user attributes copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")
        # Add some user attributes
        table1.attrs.attr1 = "attr1"
        table1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table in another group
        group1 = fileh.create_group("/", "group1")
        table2 = table1.copy(group1, 'table2',
                             copyuserattrs=1,
                             filters=Filters(complevel=6))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            table1 = fileh.root.table1
            table2 = fileh.root.group1.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            for colname in table1.colnames:
                # self.assertEqual(row1[colname], table2[nrow][colname))
                self.assertEqual(row1[colname],
                                 table2.read(nrow, field=colname)[0])

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # Leaf attributes
        self.assertEqual(table1.title, table2.title)
        self.assertEqual(6, table2.filters.complevel)
        self.assertEqual(1, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)
        # User attributes
        self.assertEqual(table2.attrs.attr1, "attr1")
        self.assertEqual(table2.attrs.attr2, 2)

        # Close the file
        fileh.close()
        os.remove(file)

    def test05b_copy(self):
        """Checking Table.copy() method (user attributes not copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[456, b'dbe', 1.2], [
                          2, b'de', 1.3]], names='col1,col2,col3')
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")
        # Add some user attributes
        table1.attrs.attr1 = "attr1"
        table1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table in another group
        group1 = fileh.create_group("/", "group1")
        table2 = table1.copy(group1, 'table2',
                             copyuserattrs=0,
                             filters=Filters(complevel=6))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            table1 = fileh.root.table1
            table2 = fileh.root.group1.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        for row1 in table1:
            nrow = row1.nrow   # current row
            for colname in table1.colnames:
                # self.assertEqual(row1[colname], table2[nrow][colname))
                self.assertEqual(row1[colname],
                                 table2.read(nrow, field=colname)[0])

        # Assert other properties in table
        self.assertEqual(table1.nrows, table2.nrows)
        self.assertEqual(table1.shape, table2.shape)
        self.assertEqual(table1.colnames, table2.colnames)
        self.assertEqual(table1.coldtypes, table2.coldtypes)
        self.assertEqualColinstances(table1, table2)
        self.assertEqual(repr(table1.description), repr(table2.description))

        # Leaf attributes
        self.assertEqual(table1.title, table2.title)
        self.assertEqual(6, table2.filters.complevel)
        self.assertEqual(1, table2.filters.shuffle)
        self.assertEqual(table1.filters.fletcher32, table2.filters.fletcher32)
        # User attributes
#       self.assertEqual(table2.attrs.attr1, None)
#       self.assertEqual(table2.attrs.attr2, None)
        self.assertEqual(hasattr(table2.attrs, "attr1"), 0)
        self.assertEqual(hasattr(table2.attrs, "attr2"), 0)

        # Close the file
        fileh.close()
        os.remove(file)


class CloseCopyTestCase(CopyTestCase):
    close = 1


class OpenCopyTestCase(CopyTestCase):
    close = 0


class CopyIndexTestCase(unittest.TestCase):
    def test01_index(self):
        """Checking Table.copy() method with indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_index..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray exceeding buffers capability
        r = records.array(b'aaaabbbbccccddddeeeeffffgggg' * 200,
                          formats='2i2, (1,)i4, (2,3)u2, (1,)f4, (1,)f8',
                          shape=10)
                          # The line below exposes a bug in numpy
                          # formats='2i2, i4, (2,3)u2, f4, f8',shape=10)
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table
        table1.nrowsinbuf = self.nrowsinbuf
        table2 = table1.copy("/", 'table2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)
        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        for nrow in range(r2.shape[0]):
            for colname in table1.colnames:
                self.assertTrue(allequal(r2[nrow][colname],
                                         table2[nrow][colname]))

        # Assert the number of rows in table
        if common.verbose:
            print("nrows in table2-->", table2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], table2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_indexclosef(self):
        """Checking Table.copy() method with indexes (close file version)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_indexclosef..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray exceeding buffers capability
        r = records.array(b'aaaabbbbccccddddeeeeffffgggg' * 200,
                          formats='2i2, i4, (2,3)u2, f4, f8', shape=10)
        # Save it in a table:
        table1 = fileh.create_table(fileh.root, 'table1', r, "title table1")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            table1 = fileh.root.table1

        # Copy to another table
        table1.nrowsinbuf = self.nrowsinbuf
        table2 = table1.copy("/", 'table2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)

        fileh.close()
        fileh = open_file(file, mode="r")
        table1 = fileh.root.table1
        table2 = fileh.root.table2

        if common.verbose:
            print("table1-->", table1.read())
            print("table2-->", table2.read())
            print("attrs table1-->", repr(table1.attrs))
            print("attrs table2-->", repr(table2.attrs))

        # Check that all the elements are equal
        r2 = r[self.start:self.stop:self.step]
        for nrow in range(r2.shape[0]):
            for colname in table1.colnames:
                self.assertTrue(allequal(r2[nrow][colname],
                                         table2[nrow][colname]))

        # Assert the number of rows in table
        if common.verbose:
            print("nrows in table2-->", table2.nrows)
            print("and it should be-->", r2.shape[0])
        self.assertEqual(r2.shape[0], table2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)


class CopyIndex1TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 1
    start = 0
    stop = 7
    step = 1


class CopyIndex2TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 0
    start = 0
    stop = -1
    step = 1


class CopyIndex3TestCase(CopyIndexTestCase):
    nrowsinbuf = 3
    close = 1
    start = 1
    stop = 7
    step = 1


class CopyIndex4TestCase(CopyIndexTestCase):
    nrowsinbuf = 4
    close = 0
    start = 0
    stop = 6
    step = 1


class CopyIndex5TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 1
    start = 3
    stop = 7
    step = 1


class CopyIndex6TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 0
    start = 3
    stop = 6
    step = 2


class CopyIndex7TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 1
    start = 0
    stop = 7
    step = 10


class CopyIndex8TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 0
    start = 6
    stop = 3
    step = 1


class CopyIndex9TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 1
    start = 3
    stop = 4
    step = 1


class CopyIndex10TestCase(CopyIndexTestCase):
    nrowsinbuf = 1
    close = 0
    start = 3
    stop = 4
    step = 2


class CopyIndex11TestCase(CopyIndexTestCase):
    nrowsinbuf = 2
    close = 1
    start = -3
    stop = -1
    step = 2


class CopyIndex12TestCase(CopyIndexTestCase):
    nrowsinbuf = 3
    close = 0
    start = -1   # Should point to the last element
    stop = None  # None should mean the last element (including it)
    step = 1


class LargeRowSize(unittest.TestCase):
    def test00(self):
        "Checking saving a Table with a moderately large rowsize"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[np.arange(100)]*2])

        # Save it in a table:
        fileh.create_table(fileh.root, 'largerow', r)

        # Read it again
        r2 = fileh.root.largerow.read()

        self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test01(self):
        "Checking saving a Table with an extremely large rowsize"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        r = records.array([[np.arange(40000)]*4])   # 640 KB

        # Save it in a table:
#         try:
#             fileh.create_table(fileh.root, 'largerow', r)
#         except ValueError:
#             if common.verbose:
#                 (type, value, traceback) = sys.exc_info()
#               print "\nGreat!, the next ValueError was catched!"
#                 print value
#         else:
#             self.fail("expected a ValueError")
        # From PyTables 1.3 on, we allow row sizes equal or larger than 640 KB
        fileh.create_table(fileh.root, 'largerow', r)

        # Read it again
        r2 = fileh.root.largerow.read()
        self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)


class DefaultValues(unittest.TestCase):
    record = Record

    def test00(self):
        "Checking saving a Table with default values (using the same Row)"
        file = tempfile.mktemp(".h5")
        # file = "/tmp/test.h5"
        fileh = open_file(file, "w")

        # Create a table
        table = fileh.create_table(fileh.root, 'table', self.record)

        table.nrowsinbuf = 46   # minimum amount that reproduces a problem
        # Take a number of records a bit greater
        nrows = int(table.nrowsinbuf * 1.1)
        row = table.row
        # Fill the table with nrows records
        for i in xrange(nrows):
            if i == 3:
                row['var2'] = 2
            if i == 4:
                row['var3'] = 3
            # This injects the row values.
            row.append()

        # We need to flush the buffers in table in order to get an
        # accurate number of records on it.
        table.flush()

        # Create a recarray with the same default values
        values = [b"abcd", 1, 2, 3.1, 4.2, 5, "e", 1, 1j, 1 + 0j]
        formats = 'a4,i4,i2,f8,f4,u2,a1,b1,c8,c16'.split(',')

        if 'Float16Col' in globals():
            values.append(6.4)
            formats.append('f2')
        if 'Float96Col' in globals():
            values.append(6.4)
            formats.append('f12')
        if 'Float128Col' in globals():
            values.append(6.4)
            formats.append('f16')
        if 'Complex192Col' in globals():
            values.append(1.-0.j)
            formats.append('c24')
        if 'Complex256Col' in globals():
            values.append(1.-0.j)
            formats.append('c32')

        r = records.array([values]*nrows, formats=','.join(formats))

        # Assign the value exceptions
        r["f1"][3] = 2
        r["f2"][4] = 3

        # Read the table in another recarray
        # r2 = table.read()
        r2 = table[::]  # Equivalent to table.read()

        # This generates too much output. Activate only when
        # self.nrowsinbuf is very small (<10)
        if common.verbose:
            print("First 10 table values:")
            for row in table.iterrows(0, 10):
                print(row)
            print("The first 5 read recarray values:")
            print(r2[:5])
            print("Records should look like:")
            print(r[:5])

        for name1, name2 in zip(r.dtype.names, r2.dtype.names):
            self.assertTrue(allequal(r[name1], r2[name2]))

        # The following can give false errors when columns with extended
        # precision data type are present in the record.
        # It is probably due to some difference in the value of bits used
        # for patting (longdoubles use just 80 bits but are stored in 96 or
        # 128 bits in numpy arrays)
        # self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test01(self):
        "Checking saving a Table with default values (using different Row)"
        file = tempfile.mktemp(".h5")
        # file = "/tmp/test.h5"
        fileh = open_file(file, "w")

        # Create a table
        table = fileh.create_table(fileh.root, 'table', self.record)

        table.nrowsinbuf = 46   # minimum amount that reproduces a problem
        # Take a number of records a bit greater
        nrows = int(table.nrowsinbuf * 1.1)
        # Fill the table with nrows records
        for i in xrange(nrows):
            if i == 3:
                table.row['var2'] = 2
            if i == 4:
                table.row['var3'] = 3
            # This injects the row values.
            table.row.append()

        # We need to flush the buffers in table in order to get an
        # accurate number of records on it.
        table.flush()

        # Create a recarray with the same default values
        values = [b"abcd", 1, 2, 3.1, 4.2, 5, "e", 1, 1j, 1 + 0j]
        formats = 'a4,i4,i2,f8,f4,u2,a1,b1,c8,c16'.split(',')

        if 'Float16Col' in globals():
            values.append(6.4)
            formats.append('f2')
        if 'Float96Col' in globals():
            values.append(6.4)
            formats.append('f12')
        if 'Float128Col' in globals():
            values.append(6.4)
            formats.append('f16')
        if 'Complex192Col' in globals():
            values.append(1.-0.j)
            formats.append('c24')
        if 'Complex256Col' in globals():
            values.append(1.-0.j)
            formats.append('c32')

        r = records.array([values]*nrows, formats=','.join(formats))

        # Assign the value exceptions
        r["f1"][3] = 2
        r["f2"][4] = 3

        # Read the table in another recarray
        # r2 = table.read()
        r2 = table[::]  # Equivalent to table.read()

        # This generates too much output. Activate only when
        # self.nrowsinbuf is very small (<10)
        if common.verbose:
            print("First 10 table values:")
            for row in table.iterrows(0, 10):
                print(row)
            print("The first 5 read recarray values:")
            print(r2[:5])
            print("Records should look like:")
            print(r[:5])

        for name1, name2 in zip(r.dtype.names, r2.dtype.names):
            self.assertTrue(allequal(r[name1], r2[name2]))

        # The following can give false errors when columns with extended
        # precision data type are present in the record.
        # It is probably due to some difference in the value of bits used
        # for patting (longdoubles use just 80 bits but are stored in 96 or
        # 128 bits in numpy arrays)
        # self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)


class OldRecordDefaultValues(DefaultValues):
    title = "OldRecordDefaultValues"
    record = OldRecord


class Record2(IsDescription):
    var1 = StringCol(itemsize=4, dflt=b"abcd")  # 4-character String
    var2 = IntCol(dflt=1)                       # integer
    var3 = Int16Col(dflt=2)                     # short integer
    var4 = Float64Col(dflt=3.1)                 # double (double-precision)


class LengthTestCase(unittest.TestCase):
    record = Record
    nrows = 20

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()

    def populateFile(self):
        # Create a table
        table = self.fileh.create_table(self.fileh.root, 'table',
                                        self.record, title="__length__ test")
        # Get the row object associated with the new table
        row = table.row

        # Fill the table
        for i in xrange(self.nrows):
            row.append()

        # Flush the buffer for this table
        table.flush()
        self.table = table

    def tearDown(self):
        if self.fileh.isopen:
            self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01_lengthrows(self):
        """Checking __length__ in Table."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_lengthrows..." % self.__class__.__name__)

        # Number of rows
        len(self.table) == self.nrows

    def test02_lengthcols(self):
        """Checking __length__ in Cols."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_lengthcols..." % self.__class__.__name__)

        # Number of columns
        if self.record is Record:
            len(self.table.cols) == 8
        elif self.record is Record2:
            len(self.table.cols) == 4

    def test03_lengthcol(self):
        """Checking __length__ in Column."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_lengthcol..." % self.__class__.__name__)

        # Number of rows for all columns column
        for colname in self.table.colnames:
            len(getattr(self.table.cols, colname)) == self.nrows


class Length1TestCase(LengthTestCase):
    record = Record
    nrows = 20


class Length2TestCase(LengthTestCase):
    record = Record2
    nrows = 100


class WhereAppendTestCase(common.TempFileMixin, common.PyTablesTestCase):
    """Tests `Table.append_where()` method."""

    class SrcTblDesc(IsDescription):
        id = IntCol()
        v1 = FloatCol()
        v2 = StringCol(itemsize=8)

    def setUp(self):
        super(WhereAppendTestCase, self).setUp()

        tbl = self.h5file.create_table('/', 'test', self.SrcTblDesc)
        row = tbl.row

        row['id'] = 1
        row['v1'] = 1.5
        row['v2'] = 'a' * 8
        row.append()

        row['id'] = 2
        row['v1'] = 2.5
        row['v2'] = 'b' * 6
        row.append()

        tbl.flush()

    def test00_same(self):
        """Query with same storage."""

        DstTblDesc = self.SrcTblDesc

        tbl1 = self.h5file.root.test
        tbl2 = self.h5file.create_table('/', 'test2', DstTblDesc)

        tbl1.append_where(tbl2, 'id > 1')

        # Rows resulting from the query are those in the new table.
        it2 = iter(tbl2)
        for r1 in tbl1.where('id > 1'):
            r2 = next(it2)
            self.assertTrue(r1['id'] == r2['id'] and r1['v1'] == r2['v1']
                            and r1['v2'] == r2['v2'])

        # There are no more rows.
        self.assertRaises(StopIteration, it2.next)

    def test01_compatible(self):
        """Query with compatible storage."""

        class DstTblDesc(IsDescription):
            id = FloatCol()  # float, not int
            v1 = FloatCol()
            v2 = StringCol(itemsize=16)  # a longer column
            v3 = FloatCol()  # extra column

        tbl1 = self.h5file.root.test
        tbl2 = self.h5file.create_table('/', 'test2', DstTblDesc)

        tbl1.append_where(tbl2, 'id > 1')

        # Rows resulting from the query are those in the new table.
        it2 = iter(tbl2)
        for r1 in tbl1.where('id > 1'):
            r2 = next(it2)
            self.assertTrue(r1['id'] == r2['id'] and r1['v1'] == r2['v1']
                            and r1['v2'] == r2['v2'])

        # There are no more rows.
        self.assertRaises(StopIteration, it2.next)

    def test02_lessPrecise(self):
        """Query with less precise storage."""

        class DstTblDesc(IsDescription):
            id = IntCol()
            v1 = IntCol()  # int, not float
            v2 = StringCol(itemsize=8)

        tbl1 = self.h5file.root.test
        tbl2 = self.h5file.create_table('/', 'test2', DstTblDesc)

        tbl1.append_where(tbl2, 'id > 1')

        # Rows resulting from the query are those in the new table.
        it2 = iter(tbl2)
        for r1 in tbl1.where('id > 1'):
            r2 = next(it2)
            self.assertTrue(r1['id'] == r2['id'] and int(r1['v1']) == r2['v1']
                            and r1['v2'] == r2['v2'])

        # There are no more rows.
        self.assertRaises(StopIteration, it2.next)

    def test03_incompatible(self):
        """Query with incompatible storage."""

        class DstTblDesc(IsDescription):
            id = StringCol(itemsize=4)  # string, not int
            v1 = FloatCol()
            v2 = StringCol(itemsize=8)

        tbl1 = self.h5file.root.test
        tbl2 = self.h5file.create_table('/', 'test2', DstTblDesc)

        self.assertRaises(NotImplementedError,
                          tbl1.append_where, tbl2, 'v1 == b"1"')

    def test04_noColumn(self):
        """Query with storage lacking columns."""

        class DstTblDesc(IsDescription):
            # no ``id`` field
            v1 = FloatCol()
            v2 = StringCol(itemsize=8)

        tbl1 = self.h5file.root.test
        tbl2 = self.h5file.create_table('/', 'test2', DstTblDesc)

        self.assertRaises(KeyError, tbl1.append_where, tbl2, 'id > 1')

    def test05_otherFile(self):
        """Appending to a table in another file."""

        h5fname2 = tempfile.mktemp(suffix='.h5')
        h5file2 = open_file(h5fname2, 'w')

        try:
            tbl1 = self.h5file.root.test
            tbl2 = h5file2.create_table('/', 'test', self.SrcTblDesc)

            # RW to RW.
            tbl1.append_where(tbl2, 'id > 1')

            # RW to RO.
            h5file2.close()
            h5file2 = open_file(h5fname2, 'r')
            tbl2 = h5file2.root.test
            self.assertRaises(FileModeError,
                              tbl1.append_where, tbl2, 'id > 1')

            # RO to RO.
            self._reopen('r')
            tbl1 = self.h5file.root.test
            self.assertRaises(FileModeError,
                              tbl1.append_where, tbl2, 'id > 1')

            # RO to RW.
            h5file2.close()
            h5file2 = open_file(h5fname2, 'a')
            tbl2 = h5file2.root.test
            tbl1.append_where(tbl2, 'id > 1')
        finally:
            h5file2.close()
            os.remove(h5fname2)


class DerivedTableTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='DeriveFromTable')

        self.fileh.create_table('/', 'original', Record)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def test00(self):
        """Deriving a table from the description of another."""

        tbl1 = self.fileh.root.original
        tbl2 = self.fileh.create_table('/', 'derived', tbl1.description)

        self.assertEqual(tbl1.description, tbl2.description)


class ChunkshapeTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='Chunkshape test')
        self.fileh.create_table('/', 'table', Record, chunkshape=13)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def test00(self):
        """Test setting the chunkshape in a table (no reopen)."""

        tbl = self.fileh.root.table
        if common.verbose:
            print("chunkshape-->", tbl.chunkshape)
        self.assertEqual(tbl.chunkshape, (13,))

    def test01(self):
        """Test setting the chunkshape in a table (reopen)."""

        self.fileh.close()
        self.fileh = open_file(self.file, 'r')
        tbl = self.fileh.root.table
        if common.verbose:
            print("chunkshape-->", tbl.chunkshape)
        self.assertEqual(tbl.chunkshape, (13,))


# Test for appending zero-sized recarrays
class ZeroSizedTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "a")
        # Create a Table
        t = self.fileh.create_table('/', 'table',
                                    {'c1': Int32Col(), 'c2': Float64Col()})
        # Append a single row
        t.append([(1, 2.2)])

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01_canAppend(self):
        "Appending zero length recarray."

        t = self.fileh.root.table
        a = np.empty(shape=(0,), dtype='i4,f8')
        t.append(a)
        self.assertEqual(t.nrows, 1, "The number of rows should be 1.")


# Case for testing ticket #103, i.e. selections in columns which are
# aligned but that its data length is not an exact multiple of the
# length of the record.  This exposes the problem only in 32-bit
# machines, because in 64-bit machine, 'c2' is unaligned.  However,
# this should check most platforms where, while not unaligned,
# len(datatype) > boundary_alignment is fullfilled.
class IrregularStrideTestCase(unittest.TestCase):
    def setUp(self):
        class IRecord(IsDescription):
            c1 = Int32Col(pos=1)
            c2 = Float64Col(pos=2)

        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='Chunkshape test')
        table = self.fileh.create_table('/', 'table', IRecord)
        for i in range(10):
            table.row['c1'] = i
            table.row['c2'] = i
            table.row.append()
        table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def test00(self):
        """Selecting rows in a table with irregular stride (but aligned)."""

        table = self.fileh.root.table
        coords1 = table.get_where_list('c1<5')
        coords2 = table.get_where_list('c2<5')
        if common.verbose:
            print("\nSelected coords1-->", coords1)
            print("Selected coords2-->", coords2)
        self.assertTrue(allequal(coords1, np.arange(5, dtype=SizeType)))
        self.assertTrue(allequal(coords2, np.arange(5, dtype=SizeType)))


class Issue262TestCase(unittest.TestCase):
    def setUp(self):
        class IRecord(IsDescription):
            c1 = Int32Col(pos=1)
            c2 = Float64Col(pos=2)

        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='Chunkshape test')
        table = self.fileh.create_table('/', 'table', IRecord)
        table.nrowsinbuf = 3

        for i in range(20):
            table.row['c1'] = i
            table.row['c2'] = i
            table.row.append()

            table.row['c1'] = i % 29
            table.row['c2'] = 300 - i
            table.row.append()

            table.row['c1'] = 300 - i
            table.row['c2'] = 100 + i % 30
            table.row.append()

        table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def test_gh260(self):
        """Regression test for gh-260"""

        table = self.fileh.root.table
        coords1 = table.get_where_list('(c1>5)&(c2<30)', start=0, step=2)
        coords2 = table.get_where_list('(c1>5)&(c2<30)', start=1, step=2)
        data = table.read()
        data = data[np.where((data['c1'] > 5) & (data['c2'] < 30))]

        if common.verbose:
            print()
            print("Selected coords1-->", coords1)
            print("Selected coords2-->", coords2)
            print("Selected data-->", data)
        self.assertEqual(len(coords1) + len(coords2), len(data))

    def test_gh262_01(self):
        """Regression test for gh-262 (start=0, step=1)"""

        table = self.fileh.root.table
        data = table.get_where_list('(c1>5)&(~(c1>5))', start=0, step=1)

        if common.verbose:
            print()
            print("data -->", data)
        self.assertEqual(len(data), 0)

    def test_gh262_02(self):
        """Regression test for gh-262 (start=1, step=1)"""

        table = self.fileh.root.table
        data = table.get_where_list('(c1>5)&(~(c1>5))', start=1, step=1)

        if common.verbose:
            print()
            print("data -->", data)
        self.assertEqual(len(data), 0)

    def test_gh262_03(self):
        """Regression test for gh-262 (start=0, step=2)"""

        table = self.fileh.root.table
        data = table.get_where_list('(c1>5)&(~(c1>5))', start=0, step=2)

        if common.verbose:
            print()
            print("data -->", data)
        self.assertEqual(len(data), 0)

    def test_gh262_04(self):
        """Regression test for gh-262 (start=1, step=2)"""

        table = self.fileh.root.table
        data = table.get_where_list('(c1>5)&(~(c1>5))', start=1, step=2)

        if common.verbose:
            print()
            print("data -->", data)
        self.assertEqual(len(data), 0)


class TruncateTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='Chunkshape test')
        table = self.fileh.create_table('/', 'table', self.IRecord)
        # Fill just a couple of rows
        for i in range(2):
            table.row['c1'] = i
            table.row['c2'] = i
            table.row.append()
        table.flush()
        # The defaults
        self.dflts = table.coldflts

    def tearDown(self):
        # Close the file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_truncate(self):
        """Checking Table.truncate() method (truncating to 0 rows)"""

        table = self.fileh.root.table
        # Truncate to 0 elements
        table.truncate(0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            table = self.fileh.root.table

        if common.verbose:
            print("table-->", table.read())

        self.assertEqual(table.nrows, 0)
        for row in table:
            self.assertEqual(row['c1'], row.nrow)

    def test01_truncate(self):
        """Checking Table.truncate() method (truncating to 1 rows)"""

        table = self.fileh.root.table
        # Truncate to 1 element
        table.truncate(1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            table = self.fileh.root.table

        if common.verbose:
            print("table-->", table.read())

        self.assertEqual(table.nrows, 1)
        for row in table:
            self.assertEqual(row['c1'], row.nrow)

    def test02_truncate(self):
        """Checking Table.truncate() method (truncating to == self.nrows)"""

        table = self.fileh.root.table
        # Truncate to 2 elements
        table.truncate(2)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            table = self.fileh.root.table

        if common.verbose:
            print("table-->", table.read())

        self.assertEqual(table.nrows, 2)
        for row in table:
            self.assertEqual(row['c1'], row.nrow)

    def test03_truncate(self):
        """Checking Table.truncate() method (truncating to > self.nrows)"""

        table = self.fileh.root.table
        # Truncate to 4 elements
        table.truncate(4)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            table = self.fileh.root.table

        if common.verbose:
            print("table-->", table.read())

        self.assertEqual(table.nrows, 4)
        # Check the original values
        for row in table.iterrows(start=0, stop=2):
            self.assertEqual(row['c1'], row.nrow)
        # Check that the added rows have the default values
        for row in table.iterrows(start=2, stop=4):
            self.assertEqual(row['c1'], self.dflts['c1'])
            self.assertEqual(row['c2'], self.dflts['c2'])


class TruncateOpen1(TruncateTestCase):
    class IRecord(IsDescription):
        c1 = Int32Col(pos=1)
        c2 = FloatCol(pos=2)
    close = 0


class TruncateOpen2(TruncateTestCase):
    class IRecord(IsDescription):
        c1 = Int32Col(pos=1, dflt=3)
        c2 = FloatCol(pos=2, dflt=-3.1)
    close = 0


class TruncateClose1(TruncateTestCase):
    class IRecord(IsDescription):
        c1 = Int32Col(pos=1)
        c2 = FloatCol(pos=2)
    close = 1


class TruncateClose2(TruncateTestCase):
    class IRecord(IsDescription):
        c1 = Int32Col(pos=1, dflt=4)
        c2 = FloatCol(pos=2, dflt=3.1)
    close = 1


class PointSelectionTestCase(common.PyTablesTestCase):
    def setUp(self):
        N = 100

        # Limits for selections
        self.limits = [
            (0, 1),  # just one element
            (20, -10),  # no elements
            (-10, 4),  # several elements
            (0, 10),   # several elements (again)
        ]

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = fileh = open_file(self.file, "w")
        # Create a sample tables
        self.data = data = np.arange(N)
        self.recarr = recarr = np.empty(N, dtype="i4,f4")
        recarr["f0"][:] = data
        recarr["f1"][:] = data
        self.table = fileh.create_table(fileh.root, 'table', recarr)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01a_read(self):
        """Test for point-selections (read, boolean keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = (data >= value1) & (data < value2)
            if common.verbose:
                print("Selection to test:", key)
            a = recarr[key]
            b = table[key]
            if common.verbose:
                print("NumPy selection:", a)
                print("PyTables selection:", b)
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables selections does not match.")

    def test01b_read(self):
        """Test for point-selections (read, tuples of integers keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))
            if common.verbose:
                print("Selection to test:", key, type(key))
            a = recarr[key]
            b = table[key]
#             if common.verbose:
#                 print "NumPy selection:", a
#                 print "PyTables selection:", b
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables selections does not match.")

    def test01c_read(self):
        """Test for point-selections (read, tuples of floats keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))
            if common.verbose:
                print("Selection to test:", key)
            recarr[key]
            fkey = np.array(key, "f4")
            self.assertRaises(TypeError, table.__getitem__, fkey)

    def test01d_read(self):
        """Test for point-selections (read, numpy keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))[0]
            if common.verbose:
                print("Selection to test:", key, type(key))
            a = recarr[key]
            b = table[key]
#             if common.verbose:
#                 print "NumPy selection:", a
#                 print "PyTables selection:", b
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables selections does not match.")

    def test01e_read(self):
        """Test for point-selections (read, list keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))[0].tolist()
            if common.verbose:
                print("Selection to test:", key, type(key))
            a = recarr[key]
            b = table[key]
#             if common.verbose:
#                 print "NumPy selection:", a
#                 print "PyTables selection:", b
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables selections does not match.")

    def test02a_write(self):
        """Test for point-selections (write, boolean keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))
            if common.verbose:
                print("Selection to test:", key)
            s = recarr[key]
            # Modify the s recarray
            s["f0"][:] = data[:len(s)]*2
            s["f1"][:] = data[:len(s)]*3
            # Modify recarr and table
            recarr[key] = s
            table[key] = s
            a = recarr[:]
            b = table[:]
#             if common.verbose:
#                 print "NumPy modified array:", a
#                 print "PyTables modifyied array:", b
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables modifications does not match.")

    def test02b_write(self):
        """Test for point-selections (write, integer keys)."""
        data = self.data
        recarr = self.recarr
        table = self.table
        for value1, value2 in self.limits:
            key = np.where((data >= value1) & (data < value2))
            if common.verbose:
                print("Selection to test:", key)
            s = recarr[key]
            # Modify the s recarray
            s["f0"][:] = data[:len(s)]*2
            s["f1"][:] = data[:len(s)]*3
            # Modify recarr and table
            recarr[key] = s
            table[key] = s
            a = recarr[:]
            b = table[:]
#             if common.verbose:
#                 print "NumPy modified array:", a
#                 print "PyTables modifyied array:", b
            npt.assert_array_equal(
                a, b, "NumPy array and PyTables modifications does not match.")


# Test for building very large MD columns without defaults
class MDLargeColTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test01_create(self):
        "Create a Table with a very large MD column.  Ticket #211."
        N = 2**18      # 4x larger than maximum object header size (64 KB)
        cols = {'col1': Int8Col(shape=N, dflt=0)}
        tbl = self.h5file.create_table('/', 'test', cols)
        tbl.row.append()   # add a single row
        tbl.flush()
        if self.reopen:
            self._reopen('a')
            tbl = self.h5file.root.test
        # Check the value
        if common.verbose:
            print("First row-->", tbl[0]['col1'])
        npt.assert_array_equal(tbl[0]['col1'], np.zeros(N, 'i1'))


class MDLargeColNoReopen(MDLargeColTestCase):
    reopen = False


class MDLargeColReopen(MDLargeColTestCase):
    reopen = True


# Test with itertools.groupby that iterates on exhausted Row iterator
# See ticket #264.
class ExhaustedIter(common.PyTablesTestCase):
    def setUp(self):
        """Create small database."""
        class Observations(IsDescription):
            market_id = IntCol(pos=0)
            scenario_id = IntCol(pos=1)
            value = Float32Col(pos=3)

        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, 'w')
        table = self.fileh.create_table('/', 'observations', Observations,
                                        chunkshape=32)

        # fill the database
        observations = np.arange(225)
        row = table.row
        for market_id in xrange(5):
            for scenario_id in xrange(3):
                for obs in observations:
                    row['market_id'] = market_id
                    row['scenario_id'] = scenario_id
                    row['value'] = obs
                    row.append()
        table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def average(self, values):
        return sum(values, 0.0) / len(values)

    def f_scenario(self, row):
        return row['scenario_id']

    def test00_groupby(self):
        """Checking iterating an exhausted iterator (ticket #264)"""
        from itertools import groupby
        rows = self.fileh.root.observations.where('(market_id == 3)')
        scenario_means = []
        for scenario_id, rows_grouped in groupby(rows, self.f_scenario):
            vals = [row['value'] for row in rows_grouped]
            scenario_means.append(self.average(vals))
        if common.verbose:
            print('Means -->', scenario_means)
        self.assertEqual(scenario_means, [112.0, 112.0, 112.0])

    def test01_groupby(self):
        """Checking iterating an exhausted iterator (ticket #264).

        Reopen.

        """
        from itertools import groupby
        self.fileh.close()
        self.fileh = open_file(self.file, 'r')
        rows = self.fileh.root.observations.where('(market_id == 3)')
        scenario_means = []
        for scenario_id, rows_grouped in groupby(rows, self.f_scenario):
            vals = [row['value'] for row in rows_grouped]
            scenario_means.append(self.average(vals))
        if common.verbose:
            print('Means -->', scenario_means)
        self.assertEqual(scenario_means, [112.0, 112.0, 112.0])


class SpecialColnamesTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test00_check_names(self):
        f = self.h5file
        a = np.array([(1, 2, 3)], dtype=[(
            "a", int), ("_b", int), ("__c", int)])
        t = f.create_table(f.root, "test", a)
        self.assertEqual(len(t.colnames), 3, "Number of columns incorrect")
        if common.verbose:
            print("colnames -->", t.colnames)
        for name, name2 in zip(t.colnames, ("a", "_b", "__c")):
            self.assertEqual(name, name2)


class RowContainsTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def test00_row_contains(self):
        f = self.h5file
        a = np.array([(1, 2, 3)], dtype="i1,i2,i4")
        t = f.create_table(f.root, "test", a)
        row = [r for r in t.iterrows()][0]
        if common.verbose:
            print("row -->", row[:])
        for item in (1, 2, 3):
            self.assertTrue(item in row)
        self.assertTrue(4 not in row)


class AccessClosedTestCase(common.TempFileMixin, common.PyTablesTestCase):
    def setUp(self):
        super(AccessClosedTestCase, self).setUp()
        self.table = self.h5file.create_table(
            self.h5file.root, 'table', Record)

        row = self.table.row
        for i in range(10):
            row['var1'] = '%04d' % i
            row['var2'] = i
            row['var3'] = i % 3
            row.append()
        self.table.flush()

    def test_read(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.read)

    def test_getitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.__getitem__, 0)

    def test_setitem(self):
        data = self.table[0]
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.__setitem__, 0, data)

    def test_append(self):
        data = self.table[0]
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.append, data)

    def test_readWhere(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.read_where, 'var2 > 3')

    def test_whereAppend(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.append_where, self.table,
                          'var2 > 3')

    def test_getWhereList(self):
        self.h5file.close()
        self.assertRaises(
            ClosedNodeError, self.table.get_where_list, 'var2 > 3')

    def test_readSorted(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.read_sorted, 'var2')

    def test_readCoordinates(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.table.read_coordinates, [2, 5])


class ColumnIterationTestCase(unittest.TestCase):
    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")
        self.buffer_size = self.fileh.params['IO_BUFFER_SIZE']

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def create_non_nested_table(self, nrows, dtype):
        array = np.empty((nrows, ), dtype)
        for name in dtype.names:
            array[name] = np.random.randint(0, 10000, nrows)
        table = self.fileh.create_table('/', 'table', dtype)
        table.append(array)
        return array, table

    def iterate(self, array, table):
        row_num = 0
        for item in table.cols.f0:
            self.assertEqual(item, array['f0'][row_num])
            row_num += 1
        self.assertEqual(row_num, len(array))

    def test_less_than_io_buffer(self):
        dtype = np.format_parser(['i8'] * 3, [], []).dtype
        rows_in_buffer = self.buffer_size // dtype[0].itemsize
        array, table = self.create_non_nested_table(rows_in_buffer // 2, dtype)
        self.iterate(array, table)

    def test_more_than_io_buffer(self):
        dtype = np.format_parser(['i8'] * 3, [], []).dtype
        rows_in_buffer = self.buffer_size // dtype[0].itemsize
        array, table = self.create_non_nested_table(rows_in_buffer * 3, dtype)
        self.iterate(array, table)

    def test_partially_filled_buffer(self):
        dtype = np.format_parser(['i8'] * 3, [], []).dtype
        rows_in_buffer = self.buffer_size // dtype[0].itemsize
        array, table = self.create_non_nested_table(rows_in_buffer * 2 + 2,
                                                    dtype)
        self.iterate(array, table)

    def test_zero_length_table(self):
        dtype = np.format_parser(['i8'] * 3, [], []).dtype
        array, table = self.create_non_nested_table(0, dtype)
        self.assertEqual(len(table), 0)
        self.iterate(array, table)


class TestCreateTableArgs(common.TempFileMixin, common.PyTablesTestCase):
    obj = np.array(
        [('aaaa', 1, 2.1), ('bbbb', 2, 3.2)],
        dtype=[('name', 'S4'), ('icol', np.int32), ('fcol', np.float32)])
    where = '/'
    name = 'table'
    description, _ = descr_from_dtype(obj.dtype)
    title = 'title'
    filters = None
    expectedrows = 10000
    chunkshape = None
    byteorder = None
    createparents = False

    def test_positional_args_01(self):
        self.h5file.create_table(self.where, self.name,
                                 self.description,
                                 self.title, self.filters,
                                 self.expectedrows)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (0,))
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)

    def test_positional_args_02(self):
        ptarr = self.h5file.create_table(self.where, self.name,
                                         self.description,
                                         self.title,
                                         self.filters,
                                         self.expectedrows)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (len(self.obj),))
        self.assertEqual(ptarr.nrows, len(self.obj))
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)
        self.assertEqual(nparr.dtype, self.obj.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_positional_args_obj(self):
        self.h5file.create_table(self.where, self.name,
                                 None,
                                 self.title,
                                 self.filters,
                                 self.expectedrows,
                                 self.chunkshape,
                                 self.byteorder,
                                 self.createparents,
                                 self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (len(self.obj),))
        self.assertEqual(ptarr.nrows, len(self.obj))
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj(self):
        self.h5file.create_table(self.where, self.name, title=self.title,
                                 obj=self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (len(self.obj),))
        self.assertEqual(ptarr.nrows, len(self.obj))
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_description_01(self):
        ptarr = self.h5file.create_table(self.where, self.name,
                                         title=self.title,
                                         description=self.description)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (len(self.obj),))
        self.assertEqual(ptarr.nrows, len(self.obj))
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_description_02(self):
        ptarr = self.h5file.create_table(self.where, self.name,
                                         title=self.title,
                                         description=self.description)
        #ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (0,))
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)

    def test_kwargs_obj_description(self):
        ptarr = self.h5file.create_table(self.where, self.name,
                                         title=self.title,
                                         obj=self.obj,
                                         description=self.description)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (len(self.obj),))
        self.assertEqual(ptarr.nrows, len(self.obj))
        self.assertEqual(tuple(ptarr.colnames), self.obj.dtype.names)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_description_error_01(self):
        self.assertRaises(TypeError,
                          self.h5file.create_table,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          description=Record)

    def test_kwargs_obj_description_error_02(self):
        self.assertRaises(TypeError,
                          self.h5file.create_table,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          description=Record())

    def test_kwargs_obj_description_error_03(self):
        self.assertRaises(TypeError,
                          self.h5file.create_table,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          description=RecordDescriptionDict)


#----------------------------------------------------------------------


def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # uncomment this only for testing purposes

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicWriteTestCase))
        theSuite.addTest(unittest.makeSuite(OldRecordBasicWriteTestCase))
        theSuite.addTest(unittest.makeSuite(DictWriteTestCase))
        if sys.version_info[0] < 3:
            theSuite.addTest(unittest.makeSuite(DictWriteTestCase2))
        theSuite.addTest(unittest.makeSuite(NumPyDTWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayOneWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayTwoWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayThreeWriteTestCase))
        theSuite.addTest(unittest.makeSuite(CompressBloscTablesTestCase))
        theSuite.addTest(unittest.makeSuite(
            CompressBloscShuffleTablesTestCase))
        theSuite.addTest(unittest.makeSuite(
            CompressBloscBloscLZTablesTestCase))
        if 'lz4' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(
                CompressBloscLZ4TablesTestCase))
            theSuite.addTest(unittest.makeSuite(
                CompressBloscLZ4HCTablesTestCase))
        if 'snappy' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(
                CompressBloscSnappyTablesTestCase))
        if 'zlib' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(
                CompressBloscZlibTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressLZOTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressLZOShuffleTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressZLIBTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressZLIBShuffleTablesTestCase))
        theSuite.addTest(unittest.makeSuite(Fletcher32TablesTestCase))
        theSuite.addTest(unittest.makeSuite(AllFiltersTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressTwoTablesTestCase))
        theSuite.addTest(unittest.makeSuite(
            SizeOnDiskInMemoryPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(NonNestedTableReadTestCase))
        theSuite.addTest(unittest.makeSuite(TableReadByteorderTestCase))
        theSuite.addTest(unittest.makeSuite(IterRangeTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayRangeTestCase))
        theSuite.addTest(unittest.makeSuite(getColRangeTestCase))
        theSuite.addTest(unittest.makeSuite(getItemTestCase))
        theSuite.addTest(unittest.makeSuite(setItem1))
        theSuite.addTest(unittest.makeSuite(setItem2))
        theSuite.addTest(unittest.makeSuite(setItem3))
        theSuite.addTest(unittest.makeSuite(setItem4))
        theSuite.addTest(unittest.makeSuite(updateRow1))
        theSuite.addTest(unittest.makeSuite(updateRow2))
        theSuite.addTest(unittest.makeSuite(updateRow3))
        theSuite.addTest(unittest.makeSuite(updateRow4))
        theSuite.addTest(unittest.makeSuite(RecArrayIO1))
        theSuite.addTest(unittest.makeSuite(RecArrayIO2))
        theSuite.addTest(unittest.makeSuite(OpenCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CloseCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex1TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex2TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex3TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex4TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex5TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex6TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex7TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex8TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex9TestCase))
        theSuite.addTest(unittest.makeSuite(DefaultValues))
        theSuite.addTest(unittest.makeSuite(OldRecordDefaultValues))
        theSuite.addTest(unittest.makeSuite(Length1TestCase))
        theSuite.addTest(unittest.makeSuite(Length2TestCase))
        theSuite.addTest(unittest.makeSuite(WhereAppendTestCase))
        theSuite.addTest(unittest.makeSuite(DerivedTableTestCase))
        theSuite.addTest(unittest.makeSuite(ChunkshapeTestCase))
        theSuite.addTest(unittest.makeSuite(ZeroSizedTestCase))
        theSuite.addTest(unittest.makeSuite(IrregularStrideTestCase))
        theSuite.addTest(unittest.makeSuite(Issue262TestCase))
        theSuite.addTest(unittest.makeSuite(TruncateOpen1))
        theSuite.addTest(unittest.makeSuite(TruncateOpen2))
        theSuite.addTest(unittest.makeSuite(TruncateClose1))
        theSuite.addTest(unittest.makeSuite(TruncateClose2))
        theSuite.addTest(unittest.makeSuite(PointSelectionTestCase))
        theSuite.addTest(unittest.makeSuite(MDLargeColNoReopen))
        theSuite.addTest(unittest.makeSuite(MDLargeColReopen))
        theSuite.addTest(unittest.makeSuite(ExhaustedIter))
        theSuite.addTest(unittest.makeSuite(SpecialColnamesTestCase))
        theSuite.addTest(unittest.makeSuite(RowContainsTestCase))
        theSuite.addTest(unittest.makeSuite(AccessClosedTestCase))
        theSuite.addTest(unittest.makeSuite(ColumnIterationTestCase))
        theSuite.addTest(unittest.makeSuite(TestCreateTableArgs))

    if common.heavy:
        theSuite.addTest(unittest.makeSuite(CompressBzip2TablesTestCase))
        theSuite.addTest(unittest.makeSuite(
            CompressBzip2ShuffleTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex10TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex11TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex12TestCase))
        theSuite.addTest(unittest.makeSuite(LargeRowSize))
        theSuite.addTest(unittest.makeSuite(BigTablesTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_tablesMD
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile

import numpy as np
from numpy import rec as records

from tables import *
from tables.tests import common
from tables.tests.common import allequal
from tables.description import descr_from_dtype

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

# It is important that columns are ordered according to their names
# to ease the comparison with structured arrays.

# Test Record class


class Record(IsDescription):
    var0 = StringCol(itemsize=4, dflt=b"", shape=2)  # 4-character string array
    var1 = StringCol(itemsize=4, dflt=[b"abcd", b"efgh"], shape=(2, 2))
    var1_ = IntCol(dflt=((1, 1),), shape=2)           # integer array
    var2 = IntCol(dflt=((1, 1), (1, 1)), shape=(2, 2))  # integer array
    var3 = Int16Col(dflt=2)                         # short integer
    var4 = FloatCol(dflt=3.1)                       # double (double-precision)
    var5 = Float32Col(dflt=4.2)                     # float  (single-precision)
    var6 = UInt16Col(dflt=5)                        # unsigned short integer
    var7 = StringCol(itemsize=1, dflt=b"e")          # 1-character String

#  Dictionary definition
RecordDescriptionDict = {
    'var0': StringCol(itemsize=4, dflt=b"", shape=2),  # 4-character string
                                                       # array
    'var1': StringCol(itemsize=4, dflt=[b"abcd", b"efgh"], shape=(2, 2)),
    #'var0': StringCol(itemsize=4, shape=2),       # 4-character String
    #'var1': StringCol(itemsize=4, shape=(2,2)),   # 4-character String
    'var1_': IntCol(shape=2),                      # integer array
    'var2': IntCol(shape=(2, 2)),                  # integer array
    'var3': Int16Col(),                           # short integer
    'var4': FloatCol(),                           # double (double-precision)
    'var5': Float32Col(),                         # float  (single-precision)
    'var6': Int16Col(),                           # unsigned short integer
    'var7': StringCol(itemsize=1),                # 1-character String
}

# Record class with numpy dtypes (mixed shapes is checked here)


class RecordDT(IsDescription):
    var0 = Col.from_dtype(np.dtype("2S4"), dflt=b"")  # shape in dtype
    var1 = Col.from_dtype(np.dtype(("S4", (
        2, 2))), dflt=[b"abcd", b"efgh"])  # shape is a mix
    var1_ = Col.from_dtype(np.dtype("2i4"), dflt=((1, 1),))  # shape in dtype
    var2 = Col.from_sctype("i4", shape=(
        2, 2), dflt=((1, 1), (1, 1)))  # shape is a mix
    var3 = Col.from_dtype(np.dtype("i2"), dflt=2)
    var4 = Col.from_dtype(np.dtype("2f8"), dflt=3.1)
    var5 = Col.from_dtype(np.dtype("f4"), dflt=4.2)
    var6 = Col.from_dtype(np.dtype("()u2"), dflt=5)
    var7 = Col.from_dtype(np.dtype("1S1"), dflt=b"e")   # no shape


class BasicTestCase(common.PyTablesTestCase):
    # file  = "test.h5"
    mode = "w"
    title = "This is the table title"
    expectedrows = 100
    appendrows = 20
    compress = 0
    complib = "zlib"  # Default compression library
    record = Record
    recarrayinit = 0
    maxshort = 1 << 15

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def initRecArray(self):
        record = self.recordtemplate
        row = record[0]
        buflist = []
        # Fill the recarray
        for i in xrange(self.expectedrows + 1):
            tmplist = []
            # Both forms (list or chararray) works
            var0 = ['%04d' % (self.expectedrows - i)] * 2
            tmplist.append(var0)
            var1 = [['%04d' % (self.expectedrows - i)] * 2] * 2
            tmplist.append(var1)
            var1_ = (i, 1)
            tmplist.append(var1_)
            var2 = ((i, 1), (1, 1))           # *-*
            tmplist.append(var2)
            var3 = i % self.maxshort
            tmplist.append(var3)
            if isinstance(row['var4'], np.ndarray):
                tmplist.append([float(i), float(i * i)])
            else:
                tmplist.append(float(i))
            if isinstance(row['var5'], np.ndarray):
                tmplist.append(np.array((float(i),)*4))
            else:
                tmplist.append(float(i))
            # var6 will be like var3 but byteswaped
            tmplist.append(((var3 >> 8) & 0xff) + ((var3 << 8) & 0xff00))
            var7 = var1[0][0][-1]
            tmplist.append(var7)
            buflist.append(tmplist)

        self.record = np.rec.array(buflist, dtype=record.dtype,
                                   shape=self.expectedrows)
        return

    def populateFile(self):
        group = self.rootgroup
        if self.recarrayinit:
            # Initialize an starting buffer, if any
            self.initRecArray()
        for j in range(3):
            # Create a table
            filters = Filters(complevel=self.compress,
                              complib=self.complib)
            if j < 2:
                byteorder = sys.byteorder
            else:
                # table2 will be byteswapped
                byteorder = {"little": "big", "big": "little"}[sys.byteorder]
            table = self.fileh.create_table(group, 'table'+str(j), self.record,
                                            title=self.title,
                                            filters=filters,
                                            expectedrows=self.expectedrows,
                                            byteorder=byteorder)
            if not self.recarrayinit:
                # Get the row object associated with the new table
                row = table.row

                # Fill the table
                for i in xrange(self.expectedrows):
                    s = '%04d' % (self.expectedrows - i)
                    row['var0'] = s.encode('ascii')
                    row['var1'] = s.encode('ascii')
                    row['var7'] = s[-1].encode('ascii')
                    row['var1_'] = (i, 1)
                    row['var2'] = ((i, 1), (1, 1))  # *-*
                    row['var3'] = i % self.maxshort
                    if isinstance(row['var4'], np.ndarray):
                        row['var4'] = [float(i), float(i * i)]
                    else:
                        row['var4'] = float(i)
                    if isinstance(row['var5'], np.ndarray):
                        row['var5'] = np.array((float(i),)*4)
                    else:
                        row['var5'] = float(i)
                    # var6 will be like var3 but byteswaped
                    row['var6'] = (((row['var3'] >> 8) & 0xff) +
                                   ((row['var3'] << 8) & 0xff00))
                    row.append()

            # Flush the buffer for this table
            table.flush()
            # Create a new group (descendant of group)
            group2 = self.fileh.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_description(self):
        """Checking table description and descriptive fields."""

        self.fileh = open_file(self.file)

        tbl = self.fileh.get_node('/table0')
        desc = tbl.description

        if isinstance(self.record, dict):
            columns = self.record
        elif isinstance(self.record, np.ndarray):
            descr, _ = descr_from_dtype(self.record.dtype)
            columns = descr._v_colobjects
        elif isinstance(self.record, np.dtype):
            descr, _ = descr_from_dtype(self.record)
            columns = descr._v_colobjects
        else:
            # This is an ordinary description.
            columns = self.record.columns

        # Check table and description attributes at the same time.
        # These checks are only valid for non-nested tables.

        # Column names.
        expectedNames = ['var0', 'var1', 'var1_', 'var2', 'var3', 'var4',
                         'var5', 'var6', 'var7']
        self.assertEqual(expectedNames, list(tbl.colnames))
        self.assertEqual(expectedNames, list(desc._v_names))

        # Column types.
        expectedTypes = [columns[colname].dtype
                         for colname in expectedNames]
        self.assertEqual(expectedTypes,
                         [tbl.coldtypes[v] for v in expectedNames])
        self.assertEqual(expectedTypes,
                         [desc._v_dtypes[v] for v in expectedNames])

        # Column string types.
        expectedTypes = [columns[colname].type
                         for colname in expectedNames]
        self.assertEqual(expectedTypes,
                         [tbl.coltypes[v] for v in expectedNames])
        self.assertEqual(expectedTypes,
                         [desc._v_types[v] for v in expectedNames])

        # Column defaults.
        for v in expectedNames:
            if common.verbose:
                print("dflt-->", columns[v].dflt)
                print("coldflts-->", tbl.coldflts[v])
                print("desc.dflts-->", desc._v_dflts[v])
            self.assertTrue(common.areArraysEqual(tbl.coldflts[v],
                                                  columns[v].dflt))
            self.assertTrue(common.areArraysEqual(desc._v_dflts[v],
                                                  columns[v].dflt))

        # Column path names.
        self.assertEqual(expectedNames, list(desc._v_pathnames))

        # Column objects.
        for colName in expectedNames:
            expectedCol = columns[colName]
            col = desc._v_colobjects[colName]
            self.assertEqual(expectedCol.dtype, col.dtype)
            self.assertEqual(expectedCol.type, col.type)

    def test01_readTable(self):
        """Checking table read and cuts."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [r['var2'][0][0] for r in table.iterrows()
                  if r['var2'][0][0] < 20]

        if common.verbose:
            print("Table:", repr(table))
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", r)
            print("Total selected records in table ==> ", len(result))
        nrows = self.expectedrows - 1
        r = [r for r in table.iterrows() if r['var2'][0][0] < 20][-1]
        self.assertEqual((
            r['var0'][0],
            r['var1'][0][0],
            r['var1_'][0],
            r['var2'][0][0],
            r['var7']
        ), (b"0001", b"0001", nrows, nrows, b"1"))
        if isinstance(r['var5'], np.ndarray):
            self.assertTrue(allequal(r['var5'],
                                     np.array((nrows,)*4, np.float32)))
        else:
            self.assertEqual(r['var5'], float(nrows))
        self.assertEqual(len(result), 20)

    def test01b_readTable(self):
        """Checking table read and cuts (multidimensional columns case)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        # Choose a small value for buffer size
        table.nrowsinbuf = 3
        # Read the records and select those with "var2" file less than 20
        result = [r['var5'] for r in table.iterrows() if r['var2'][0][0] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("Last record in table ==>", r)
            print("Total selected records in table ==> ", len(result))
        nrows = table.nrows
        r = [r for r in table.iterrows() if r['var2'][0][0] < 20][-1]
        if isinstance(r['var5'], np.ndarray):
            self.assertTrue(allequal(result[0],
                                     np.array((float(0),)*4, np.float32)))
            self.assertTrue(allequal(result[1],
                                     np.array((float(1),)*4, np.float32)))
            self.assertTrue(allequal(result[2],
                                     np.array((float(2),)*4, np.float32)))
            self.assertTrue(allequal(result[3],
                                     np.array((float(3),)*4, np.float32)))
            self.assertTrue(allequal(result[10],
                                     np.array((float(10),)*4, np.float32)))
            self.assertTrue(allequal(r['var5'],
                                     np.array((float(nrows-1),)*4,
                                              np.float32)))
        else:
            self.assertEqual(r['var5'], float(nrows-1))
        self.assertEqual(len(result), 20)

        # Read the records and select those with "var2" file less than 20
        result = [r['var1'] for r in table.iterrows() if r['var2'][0][0] < 20]
        r = [r for r in table.iterrows() if r['var2'][0][0] < 20][-1]

        if r['var1'].dtype.char == "S":
            a = np.array([['%04d' % (self.expectedrows - 0)]*2]*2, 'S')
            self.assertTrue(allequal(result[0], a))
            a = np.array([['%04d' % (self.expectedrows - 1)]*2]*2, 'S')
            self.assertTrue(allequal(result[1], a))
            a = np.array([['%04d' % (self.expectedrows - 2)]*2]*2, 'S')
            self.assertTrue(allequal(result[2], a))
            a = np.array([['%04d' % (self.expectedrows - 3)]*2]*2, 'S')
            self.assertTrue(allequal(result[3], a))
            a = np.array([['%04d' % (self.expectedrows - 10)]*2]*2, 'S')
            self.assertTrue(allequal(result[10], a))
            a = np.array([['%04d' % (1)]*2]*2, 'S')
            self.assertTrue(allequal(r['var1'], a))
        else:
            self.assertEqual(r['var1'], "0001")
        self.assertEqual(len(result), 20)

    def test01c_readTable(self):
        """Checking shape of multidimensional columns."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_readTable..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        if common.verbose:
            print("var2 col shape:", table.cols.var2.shape)
            print("Should be:", table.cols.var2[:].shape)
        self.assertEqual(table.cols.var2.shape, table.cols.var2[:].shape)

    def test02_AppendRows(self):
        """Checking whether appending record rows works or not."""

        # Now, open it, but in "append" mode
        self.fileh = open_file(self.file, mode="a")
        self.rootgroup = self.fileh.root
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_AppendRows..." % self.__class__.__name__)

        # Get a table
        table = self.fileh.get_node("/group0/table1")
        # Get their row object
        row = table.row
        if common.verbose:
            print("Nrows in old", table._v_pathname, ":", table.nrows)
            print("Record Format ==>", table.description._v_nested_formats)
            print("Record Size ==>", table.rowsize)
        # Append some rows
        for i in xrange(self.appendrows):
            s = '%04d' % (self.appendrows - i)
            row['var0'] = s.encode('ascii')
            row['var1'] = s.encode('ascii')
            row['var7'] = s[-1].encode('ascii')
            row['var1_'] = (i, 1)
            row['var2'] = ((i, 1), (1, 1))   # *-*
            row['var3'] = i % self.maxshort
            if isinstance(row['var4'], np.ndarray):
                row['var4'] = [float(i), float(i * i)]
            else:
                row['var4'] = float(i)
            if isinstance(row['var5'], np.ndarray):
                row['var5'] = np.array((float(i),)*4)
            else:
                row['var5'] = float(i)
            row.append()

        # Flush the buffer for this table and read it
        table.flush()
        result = [row['var2'][0][0] for row in table.iterrows()
                  if row['var2'][0][0] < 20]
        row = [r for r in table.iterrows() if r['var2'][0][0] < 20][-1]

        nrows = self.appendrows - 1
        self.assertEqual((
            row['var0'][0],
            row['var1'][0][0],
            row['var1_'][0],
            row['var2'][0][0],
            row['var7']),
            (b"0001", b"0001", nrows, nrows, b"1"))
        if isinstance(row['var5'], np.ndarray):
            self.assertTrue(allequal(row['var5'],
                                     np.array((float(nrows),)*4, np.float32)))
        else:
            self.assertEqual(row['var5'], float(nrows))
        if self.appendrows <= 20:
            add = self.appendrows
        else:
            add = 20
        self.assertEqual(len(result), 20 + add)  # because we appended new rows
        # del table

    # CAVEAT: The next test only works for tables with rows < 2**15
    def test03_endianess(self):
        """Checking if table is endianess aware."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_endianess..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/group0/group1/table2")

        # Read the records and select the ones with "var3" column less than 20
        result = [r['var2'] for r in table.iterrows() if r['var3'] < 20]
        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            print("On-disk byteorder ==>", table.byteorder)
            print("Last record in table ==>", r)
            print("Total selected records in table ==>", len(result))
        nrows = self.expectedrows - 1
        r = list(table.iterrows())[-1]
        self.assertEqual((r['var1'][0][0], r['var3']), (b"0001", nrows))
        self.assertEqual(len(result), 20)


class BasicWriteTestCase(BasicTestCase):
    title = "BasicWrite"
    pass


class DictWriteTestCase(BasicTestCase):
    # This checks also unidimensional arrays as columns
    title = "DictWrite"
    record = RecordDescriptionDict
    nrows = 21
    nrowsinbuf = 3  # Choose a small value for the buffer size
    start = 0
    stop = 10
    step = 3


class RecordDTWriteTestCase(BasicTestCase):
    title = "RecordDTWriteTestCase"
    record = RecordDT

# Pure NumPy dtype


class NumPyDTWriteTestCase(BasicTestCase):
    title = "NumPyDTWriteTestCase"
    record = np.dtype("(2,)S4,(2,2)S4,(2,)i4,(2,2)i4,i2,2f8,f4,i2,S1")
    record.names = 'var0,var1,var1_,var2,var3,var4,var5,var6,var7'.split(',')


class RecArrayOneWriteTestCase(BasicTestCase):
    title = "RecArrayOneWrite"
    record = np.rec.array(
        None,
        formats="(2,)S4,(2,2)S4,(2,)i4,(2,2)i4,i2,2f8,f4,i2,S1",
        names='var0,var1,var1_,var2,var3,var4,var5,var6,var7',
        shape=0)


class RecArrayTwoWriteTestCase(BasicTestCase):
    title = "RecArrayTwoWrite"
    expectedrows = 100
    recarrayinit = 1
    recordtemplate = np.rec.array(
        None,
        formats="(2,)a4,(2,2)a4,(2,)i4,(2,2)i4,i2,f8,f4,i2,a1",
        names='var0,var1,var1_,var2,var3,var4,var5,var6,var7',
        shape=1)


class RecArrayThreeWriteTestCase(BasicTestCase):
    title = "RecArrayThreeWrite"
    expectedrows = 100
    recarrayinit = 1
    recordtemplate = np.rec.array(
        None,
        formats="(2,)a4,(2,2)a4,(2,)i4,(2,2)i4,i2,2f8,4f4,i2,a1",
        names='var0,var1,var1_,var2,var3,var4,var5,var6,var7',
        shape=1)


class CompressBloscTablesTestCase(BasicTestCase):
    title = "CompressBloscTables"
    compress = 1
    complib = "blosc"


class CompressLZOTablesTestCase(BasicTestCase):
    title = "CompressLZOTables"
    compress = 1
    complib = "lzo"


class CompressBzip2TablesTestCase(BasicTestCase):
    title = "CompressBzip2Tables"
    compress = 1
    complib = "bzip2"


class CompressZLIBTablesTestCase(BasicTestCase):
    title = "CompressOneTables"
    compress = 1
    complib = "zlib"


class CompressTwoTablesTestCase(BasicTestCase):
    title = "CompressTwoTables"
    compress = 1
    # This checks also unidimensional arrays as columns
    record = RecordDescriptionDict


class BigTablesTestCase(BasicTestCase):
    title = "BigTables"
    # 10000 rows takes much more time than we can afford for tests
    # reducing to 1000 would be more than enough
    # F. Alted 2004-01-19
#     expectedrows = 10000
#     appendrows = 1000
    expectedrows = 1000
    appendrows = 100


class BasicRangeTestCase(unittest.TestCase):
    # file  = "test.h5"
    mode = "w"
    title = "This is the table title"
    record = Record
    maxshort = 1 << 15
    expectedrows = 100
    compress = 0
    # Default values
    nrows = 20
    nrowsinbuf = 3  # Choose a small value for the buffer size
    start = 1
    stop = nrows
    checkrecarray = 0
    checkgetCol = 0

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        for j in range(3):
            # Create a table
            table = self.fileh.create_table(group, 'table'+str(j), self.record,
                                            title=self.title,
                                            filters=Filters(self.compress),
                                            expectedrows=self.expectedrows)
            # Get the row object associated with the new table
            row = table.row

            # Fill the table
            for i in xrange(self.expectedrows):
                row['var1'] = '%04d' % (self.expectedrows - i)
                row['var7'] = row['var1'][0][0][-1]
                row['var2'] = i
                row['var3'] = i % self.maxshort
                if isinstance(row['var4'], np.ndarray):
                    row['var4'] = [float(i), float(i * i)]
                else:
                    row['var4'] = float(i)
                if isinstance(row['var5'], np.ndarray):
                    row['var5'] = np.array((float(i),)*4)
                else:
                    row['var5'] = float(i)
                # var6 will be like var3 but byteswaped
                row['var6'] = (((row['var3'] >> 8) & 0xff) +
                               ((row['var3'] << 8) & 0xff00))
                row.append()

            # Flush the buffer for this table
            table.flush()
            # Create a new group (descendant of group)
            group2 = self.fileh.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        if self.fileh.isopen:
            self.fileh.close()
        # del self.fileh, self.rootgroup
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def check_range(self):
        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        table = self.fileh.get_node("/table0")

        table.nrowsinbuf = self.nrowsinbuf
        r = slice(self.start, self.stop, self.step)
        resrange = r.indices(table.nrows)
        reslength = len(range(*resrange))
        if self.checkrecarray:
            recarray = table.read(self.start, self.stop, self.step)
            result = []
            for nrec in range(len(recarray)):
                if recarray['var2'][nrec][0][0] < self.nrows and 0 < self.step:
                    result.append(recarray['var2'][nrec][0][0])
                elif (recarray['var2'][nrec][0][0] > self.nrows and
                        0 > self.step):
                    result.append(recarray['var2'][nrec][0][0])
        elif self.checkgetCol:
            column = table.read(self.start, self.stop, self.step, 'var2')
            result = []
            for nrec in range(len(column)):
                if column[nrec][0][0] < self.nrows and 0 < self.step:  # *-*
                    result.append(column[nrec][0][0])  # *-*
                elif column[nrec][0][0] > self.nrows and 0 > self.step:  # *-*
                    result.append(column[nrec][0][0])  # *-*
        else:
            if 0 < self.step:
                result = [
                    r['var2'][0][0] for r in table.iterrows(self.start,
                                                            self.stop,
                                                            self.step)
                    if r['var2'][0][0] < self.nrows
                ]
            elif 0 > self.step:
                result = [
                    r['var2'][0][0] for r in table.iterrows(self.start,
                                                            self.stop,
                                                            self.step)
                    if r['var2'][0][0] > self.nrows
                ]

        if self.start < 0:
            startr = self.expectedrows + self.start
        else:
            startr = self.start

        if self.stop is None:
            if self.checkrecarray or self.checkgetCol:
                # data read using the read method
                stopr = startr + 1
            else:
                # data read using the  iterrows method
                stopr = self.nrows
        elif self.stop < 0:
            stopr = self.expectedrows + self.stop
        else:
            stopr = self.stop

        if self.nrows < stopr:
            stopr = self.nrows

        if common.verbose:
            print("Nrows in", table._v_pathname, ":", table.nrows)
            if reslength:
                if self.checkrecarray:
                    print("Last record *read* in recarray ==>", recarray[-1])
                elif self.checkgetCol:
                    print("Last value *read* in getCol ==>", column[-1])
                else:
                    print("Last record *read* in table range ==>", r)
            print("Total number of selected records ==>", len(result))
            print("Selected records:\n", result)
            print("Selected records should look like:\n",
                  range(startr, stopr, self.step))
            print("start, stop, step ==>", startr, stopr, self.step)

        self.assertEqual(result, range(startr, stopr, self.step))
        if not (self.checkrecarray or self.checkgetCol):
            if startr < stopr and 0 < self.step:
                r = [r['var2'] for r in table.iterrows(self.start, self.stop,
                                                       self.step)
                     if r['var2'][0][0] < self.nrows][-1]
                if self.nrows > self.expectedrows:
                    self.assertEqual(
                        r[0][0],
                        range(self.start, self.stop, self.step)[-1])
                else:
                    self.assertEqual(r[0][0],
                                     range(startr, stopr, self.step)[-1])
            elif startr > stopr and 0 > self.step:
                r = [r['var2'] for r in table.iterrows(self.start, self.stop,
                                                       self.step)
                     if r['var2'][0][0] > self.nrows][0]
                if self.nrows < self.expectedrows:
                    self.assertEqual(
                        r[0][0],
                        range(self.start, self.stop or -1, self.step)[0])
                else:
                    self.assertEqual(
                        r[0][0],
                        range(startr, stopr or -1, self.step)[0])

        # Close the file
        self.fileh.close()

    def test01_range(self):
        """Checking ranges in table iterators (case1)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 2 * step
        self.nrows = 21
        self.nrowsinbuf = 3
        self.start = 0
        self.stop = self.expectedrows
        self.step = 2

        self.check_range()

    def test01a_range(self):
        """Checking ranges in table iterators (case1)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 2 * step
        self.nrows = 21
        self.nrowsinbuf = 3
        self.start = self.expectedrows - 1
        self.stop = None
        self.step = -2

        self.check_range()

    def test02_range(self):
        """Checking ranges in table iterators (case2)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 10 * step
        self.nrows = 21
        self.nrowsinbuf = 31
        self.start = 11
        self.stop = self.expectedrows
        self.step = 3

        self.check_range()

    def test03_range(self):
        """Checking ranges in table iterators (case3)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_range..." % self.__class__.__name__)

        # Case where step < nrowsinbuf < 1.1 * step
        self.nrows = self.expectedrows
        self.nrowsinbuf = 11  # Choose a small value for the buffer size
        self.start = 0
        self.stop = self.expectedrows
        self.step = 10

        self.check_range()

    def test04_range(self):
        """Checking ranges in table iterators (case4)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_range..." % self.__class__.__name__)

        # Case where step == nrowsinbuf
        self.nrows = self.expectedrows
        self.nrowsinbuf = 11  # Choose a small value for the buffer size
        self.start = 1
        self.stop = self.expectedrows
        self.step = 11

        self.check_range()

    def test05_range(self):
        """Checking ranges in table iterators (case5)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_range..." % self.__class__.__name__)

        # Case where step > 1.1 * nrowsinbuf
        self.nrows = 21
        self.nrowsinbuf = 10  # Choose a small value for the buffer size
        self.start = 1
        self.stop = self.expectedrows
        self.step = 11

        self.check_range()

    def test06_range(self):
        """Checking ranges in table iterators (case6)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06_range..." % self.__class__.__name__)

        # Case where step > 3 * nrowsinbuf
        self.nrows = 3
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = 2
        self.stop = self.expectedrows
        self.step = 10

        self.check_range()

    def test07_range(self):
        """Checking ranges in table iterators (case7)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_range..." % self.__class__.__name__)

        # Case where start == stop
        self.nrows = 2
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = self.nrows
        self.stop = self.nrows
        self.step = 10

        self.check_range()

    def test08_range(self):
        """Checking ranges in table iterators (case8)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08_range..." % self.__class__.__name__)

        # Case where start > stop
        self.nrows = 2
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = self.nrows + 1
        self.stop = self.nrows
        self.step = 1

        self.check_range()

    def test09_range(self):
        """Checking ranges in table iterators (case9)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test09_range..." % self.__class__.__name__)

        # Case where stop = None
        self.nrows = 100
        self.nrowsinbuf = 3  # Choose a small value for the buffer size
        self.start = 1
        self.stop = 2
        self.step = 1

        self.check_range()

    def test10_range(self):
        """Checking ranges in table iterators (case10)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test10_range..." % self.__class__.__name__)

        # Case where start < 0 and stop = 0
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -6
        self.startr = self.expectedrows + self.start
        self.stop = 0
        self.stopr = self.expectedrows + self.stop
        self.step = 2

        self.check_range()

    def test11_range(self):
        """Checking ranges in table iterators (case11)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test11_range..." % self.__class__.__name__)

        # Case where start < 0 and stop < 0
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -6
        self.startr = self.expectedrows + self.start
        self.stop = -2
        self.stopr = self.expectedrows + self.stop
        self.step = 1

        self.check_range()

    def test12_range(self):
        """Checking ranges in table iterators (case12)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test12_range..." % self.__class__.__name__)

        # Case where start < 0 and stop < 0 and start > stop
        self.nrows = self.expectedrows
        self.nrowsinbuf = 5  # Choose a small value for the buffer size
        self.start = -1
        self.startr = self.expectedrows + self.start
        self.stop = -2
        self.stopr = self.expectedrows + self.stop
        self.step = 1

        self.check_range()

    def test13_range(self):
        """Checking ranges in table iterators (case13)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test13_range..." % self.__class__.__name__)

        # Case where step < 0
        self.step = -11
        try:
            self.check_range()
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
            self.fileh.close()
        #else:
        #    self.fail("expected a ValueError")

        # Case where step == 0
        self.step = 0
        try:
            self.check_range()
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
            self.fileh.close()
        #else:
        #    self.fail("expected a ValueError")


class IterRangeTestCase(BasicRangeTestCase):
    pass


class RecArrayRangeTestCase(BasicRangeTestCase):
    checkrecarray = 1


class getColRangeTestCase(BasicRangeTestCase):
    checkgetCol = 1

    def test01_nonexistentField(self):
        """Checking non-existing Field in getCol method """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_nonexistentField..." %
                  self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        self.root = self.fileh.root
        table = self.fileh.get_node("/table0")

        try:
            table.read(field='non-existent-column')
        except KeyError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next KeyError was catched!")
        else:
            self.fail("expected a KeyError")


class Rec(IsDescription):
    col1 = IntCol(pos=1, shape=(2,))
    col2 = StringCol(itemsize=3, pos=2, shape=(3,))
    col3 = FloatCol(pos=3, shape=(3, 2))


class RecArrayIO(unittest.TestCase):

    def test00(self):
        "Checking saving a normal recarray"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        intlist1 = [[456, 23]*3]*2
        intlist2 = np.array([[2, 2]*3]*2, dtype=int)
        arrlist1 = [['dbe']*2]*3
        arrlist2 = [['de']*2]*3
        floatlist1 = [[1.2, 2.3]*3]*4
        floatlist2 = np.array([[4.5, 2.4]*3]*4)
        b = [[intlist1, arrlist1, floatlist1], [
            intlist2, arrlist2, floatlist2]]
        r = np.rec.array(b, formats='(2,6)i4,(3,2)a3,(4,6)f8',
                         names='col1,col2,col3')

        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r)

        # Read it again
        r2 = fileh.root.recarray.read()

        self.assertEqual(r.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test01(self):
        "Checking saving a recarray with an offset in its buffer"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        intlist1 = [[456, 23]*3]*2
        intlist2 = np.array([[2, 2]*3]*2, dtype=int)
        arrlist1 = [['dbe']*2]*3
        arrlist2 = [['de']*2]*3
        floatlist1 = [[1.2, 2.3]*3]*4
        floatlist2 = np.array([[4.5, 2.4]*3]*4)
        b = [[intlist1, arrlist1, floatlist1], [
            intlist2, arrlist2, floatlist2]]
        r = np.rec.array(b, formats='(2,6)i4,(3,2)a3,(4,6)f8',
                         names='col1,col2,col3')

        # Get a view of the recarray
        r1 = r[1:]
        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)
        # Read it again
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test02(self):
        "Checking saving a slice of a large recarray"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        intlist1 = [[[23, 24, 35]*6]*6]
        intlist2 = np.array([[[2, 3, 4]*6]*6], dtype=int)
        arrlist1 = [['dbe']*2]*3
        arrlist2 = [['de']*2]*3
        floatlist1 = [[1.2, 2.3]*3]*4
        floatlist2 = np.array([[4.5, 2.4]*3]*4)
        b = [[intlist1, arrlist1, floatlist1], [
            intlist2, arrlist2, floatlist2]]
        r = np.rec.array(b * 300,  formats='(1,6,18)i4,(3,2)a3,(4,6)f8',
                         names='col1,col2,col3')

        # Get an slice of recarray
        r1 = r[290:292]
        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)
        # Read it again
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test03(self):
        "Checking saving a slice of an strided recarray"
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a recarray
        intlist1 = [[[23, 24, 35]*6]*6]
        intlist2 = np.array([[[2, 3, 4]*6]*6], dtype=int)
        arrlist1 = [['dbe']*2]*3
        arrlist2 = [['de']*2]*3
        floatlist1 = [[1.2, 2.3]*3]*4
        floatlist2 = np.array([[4.5, 2.4]*3]*4)
        b = [[intlist1, arrlist1, floatlist1], [
            intlist2, arrlist2, floatlist2]]
        r = np.rec.array(b * 300, formats='(1,6,18)i4,(3,2)a3,(4,6)f8',
                         names='col1,col2,col3', shape=300)

        # Get an strided recarray
        r2 = r[::2]
        # Get a slice
        r1 = r2[148:]
        # Save it in a table:
        fileh.create_table(fileh.root, 'recarray', r1)
        # Read it again
        r2 = fileh.root.recarray.read()

        self.assertEqual(r1.tostring(), r2.tostring())

        fileh.close()
        os.remove(file)

    def test08a(self):
        "Checking modifying one column (single column version, list)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08a..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # Append new rows
        s0, s1, s2, s3 = ['dbe']*3, ['ded']*3, ['db1']*3, ['de1']*3
        f0, f1, f2, f3 = [[1.2]*2]*3, [[1.3]*2]*3, [[1.4]*2]*3, [[1.5]*2]*3
        r = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1]],
                          formats="(2,)i4,(3,)a3,(3,2)f8")
        table.append(r)
        table.append([[[457, 458], s2, f2], [[5, 6], s3, f3]])

        # Modify just one existing column
        table.cols.col1[1:] = [[[2, 3], [3, 4], [4, 5]]]
        # Create the modified recarray
        r1 = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1],
                            [[3, 4], s2, f2], [[4, 5], s3, f3]],
                           formats="(2,)i4,(3,)a3,(3,2)f8",
                           names="col1,col2,col3")
        # Read the modified table
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08b(self):
        "Checking modifying one column (single column version, recarray)"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08b..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # Append new rows
        s0, s1, s2, s3 = ['dbe']*3, ['ded']*3, ['db1']*3, ['de1']*3
        f0, f1, f2, f3 = [[1.2]*2]*3, [[1.3]*2]*3, [[1.4]*2]*3, [[1.5]*2]*3
        r = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1]],
                          formats="(2,)i4,(3,)a3,(3,2)f8")
        table.append(r)
        table.append([[[457, 458], s2, f2], [[5, 6], s3, f3]])

        # Modify just one existing column
        columns = records.fromarrays(
            np.array([[[2, 3], [3, 4], [4, 5]]]), formats="i4")
        table.modify_columns(start=1, columns=columns, names=["col1"])
        # Create the modified recarray
        r1 = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1],
                            [[3, 4], s2, f2], [[4, 5], s3, f3]],
                           formats="(2,)i4,(3,)a3,(3,2)f8",
                           names="col1,col2,col3")
        # Read the modified table
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)

    def test08b2(self):
        """Checking modifying one column (single column version, recarray,
        modify_column)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test08b2..." % self.__class__.__name__)

        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create a new table:
        table = fileh.create_table(fileh.root, 'recarray', Rec)

        # Append new rows
        s0, s1, s2, s3 = ['dbe']*3, ['ded']*3, ['db1']*3, ['de1']*3
        f0, f1, f2, f3 = [[1.2]*2]*3, [[1.3]*2]*3, [[1.4]*2]*3, [[1.5]*2]*3
        r = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1]],
                          formats="(2,)i4,(3,)a3,(3,2)f8")
        table.append(r)
        table.append([[[457, 458], s2, f2], [[5, 6], s3, f3]])

        # Modify just one existing column
        columns = records.fromarrays(
            np.array([[[2, 3], [3, 4], [4, 5]]]), formats="i4")
        table.modify_column(start=1, column=columns, colname="col1")
        # Create the modified recarray
        r1 = records.array([[[456, 457], s0, f0], [[2, 3], s1, f1],
                            [[3, 4], s2, f2], [[4, 5], s3, f3]],
                           formats="(2,)i4,(3,)a3,(3,2)f8",
                           names="col1,col2,col3")
        # Read the modified table
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

        fileh.close()
        os.remove(file)


class DefaultValues(unittest.TestCase):

    def test00(self):
        "Checking saving a Table MD with default values"
        file = tempfile.mktemp(".h5")
        # file = "/tmp/test.h5"
        fileh = open_file(file, "w")

        # Create a table
        table = fileh.create_table(fileh.root, 'table', Record)

        # Take a number of records a bit large
        # nrows = int(table.nrowsinbuf * 1.1)
        nrows = 5  # for test
        # Fill the table with nrows records
        for i in xrange(nrows):
            if i == 3 or i == 4:
                table.row['var2'] = ((2, 2), (2, 2))  # *-*
            # This injects the row values.
            table.row.append()

        # We need to flush the buffers in table in order to get an
        # accurate number of records on it.
        table.flush()

        # Create a recarray with the same default values
        buffer = [[
            ["\x00"]*2,  # just "" does not initialize the buffer properly
            [["abcd", "efgh"]]*2,
            (1, 1),
            ((1, 1), (1, 1)),
            2, 3.1, 4.2, 5, "e"]]
        r = np.rec.array(
            buffer * nrows,
            formats='(2,)a4,(2,2)a4,(2,)i4,(2,2)i4,i2,f8,f4,u2,a1',
            names=['var0', 'var1', 'var1_', 'var2', 'var3', 'var4', 'var5',
                   'var6', 'var7'])  # *-*

        # Assign the value exceptions
        r["var2"][3] = ((2, 2), (2, 2))  # *-*
        r["var2"][4] = ((2, 2), (2, 2))  # *-*

        # Read the table in another recarray
        r2 = table.read()

        # This generates too much output. Activate only when
        # self.nrowsinbuf is very small (<10)
        if common.verbose and 1:
            print("Table values:")
            print(r2)
            print("Record values:")
            print(r)

        # Both checks do work, however, tostring() seems more stringent.
        self.assertEqual(r.tostring(), r2.tostring())
        # self.assertTrue(common.areArraysEqual(r,r2))

        fileh.close()
        os.remove(file)


class RecordT(IsDescription):
    var0 = IntCol(dflt=1, shape=())  # native int
    var1 = IntCol(dflt=[1], shape=(1,))  # 1-D int (one element)
    var2_s = IntCol(dflt=[1, 1], shape=2)  # 1-D int (two elements)
    var2 = IntCol(dflt=[1, 1], shape=(2,))  # 1-D int (two elements)
    var3 = IntCol(dflt=[[0, 0], [1, 1]], shape=(2, 2))  # 2-D int


class ShapeTestCase(unittest.TestCase):

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.populateFile()

    def populateFile(self):
        table = self.fileh.create_table(self.fileh.root, 'table', RecordT)
        row = table.row
        # Fill the table with some rows with default values
        for i in xrange(1):
            row.append()

        # Flush the buffer for this table
        table.flush()

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00(self):
        "Checking scalar shapes"

        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file)
        table = self.fileh.root.table

        if common.verbose:
            print("The values look like:", table.cols.var0[:])
            print("They should look like:", [1])

        # The real check
        self.assertEqual(table.cols.var0[:].tolist(), [1])

    def test01(self):
        "Checking undimensional (one element) shapes"

        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file)
        table = self.fileh.root.table

        if common.verbose:
            print("The values look like:", table.cols.var1[:])
            print("They should look like:", [[1]])

        # The real check
        self.assertEqual(table.cols.var1[:].tolist(), [[1]])

    def test02(self):
        "Checking undimensional (two elements) shapes"

        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file)
        table = self.fileh.root.table

        if common.verbose:
            print("The values look like:", table.cols.var2[:])
            print("They should look like:", [[1, 1]])

        # The real check
        self.assertEqual(table.cols.var2[:].tolist(), [[1, 1]])
        self.assertEqual(table.cols.var2_s[:].tolist(), [[1, 1]])

    def test03(self):
        "Checking bidimensional shapes"

        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file)
        table = self.fileh.root.table

        if common.verbose:
            print("The values look like:", table.cols.var3[:])
            print("They should look like:", [[[0, 0], [1, 1]]])

        # The real check
        self.assertEqual(table.cols.var3[:].tolist(), [[[0, 0], [1, 1]]])


class ShapeTestCase1(ShapeTestCase):
    reopen = 0


class ShapeTestCase2(ShapeTestCase):
    reopen = 1


class setItem(common.PyTablesTestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        # Create a new table:
        self.table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        self.table.nrowsinbuf = self.buffersize  # set buffer value

    def tearDown(self):
        self.fileh.close()
        # del self.fileh, self.rootgroup
        os.remove(self.file)
        common.cleanup(self)

    def test01(self):
        "Checking modifying one table row with __setitem__"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing row
        table[2] = (456, 'db2', 1.2)
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [456, 'db2', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test01b(self):
        "Checking modifying one table row with __setitem__ (long index)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing row
        table[2] = (456, 'db2', 1.2)
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [456, 'db2', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test02(self):
        "Modifying one row, with a step (__setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, 'db1', 1.2]],
                             formats=formats)
        table[1:3:2] = rows
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [457, 'db1', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test03(self):
        "Checking modifying several rows at once (__setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, 'db1', 1.2], [5, 'de1', 1.3]],
                             formats=formats)
        # table.modify_rows(start=1, rows=rows)
        table[1:3] = rows
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [5, 'de1', 1.3], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test04(self):
        "Modifying several rows at once, with a step (__setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        rows = records.array([[457, 'db1', 1.2], [6, 'de2', 1.3]],
                             formats=formats)
        # table[1:4:2] = rows
        table[1::2] = rows
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [457, 'db1', 1.2], [6, 'de2', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test05(self):
        "Checking modifying one column (single element, __setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1] = -1
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [-1, 'ded', 1.3],
                            [457, 'db1', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06a(self):
        "Checking modifying one column (several elements, __setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1:4] = [(2, 2), (3, 3), (4, 4)]
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [3, 'db1', 1.2], [4, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06b(self):
        "Checking modifying one column (iterator, __setitem__)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        try:
            for row in table.iterrows():
                row['col1'] = row.nrow + 1
                row.append()
            table.flush()
        except NotImplementedError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next NotImplementedError was catched!")
                print(value)
        else:
            self.fail("expected a NotImplementedError")

    def test07(self):
        "Modifying one column (several elements, __setitem__, step)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          1, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])
        # Modify just one existing column
        table.cols.col1[1:4:2] = [(2, 2), (3, 3)]
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [457, 'db1', 1.2], [3, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test08(self):
        "Modifying one column (one element, __setitem__, step)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        table.cols.col1[1:4:3] = [(2, 2)]
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [457, 'db1', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test09(self):
        "Modifying beyond the table extend (__setitem__, step)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Try to modify beyond the extend
        # This will silently exclude the non-fitting rows
        rows = records.array([[457, 'db1', 1.2], [6, 'de2', 1.3]],
                             formats=formats)
        table[1::2] = rows
        # How it should look like
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [457, 'db1', 1.2], [6, 'de2', 1.3]],
                           formats=formats)

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)


class setItem1(setItem):
    reopen = 0
    buffersize = 1


class setItem2(setItem):
    reopen = 1
    buffersize = 2


class setItem3(setItem):
    reopen = 0
    buffersize = 1000


class setItem4(setItem):
    reopen = 1
    buffersize = 1000


class updateRow(common.PyTablesTestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        # Create a new table:
        self.table = self.fileh.create_table(self.fileh.root, 'recarray', Rec)
        self.table.nrowsinbuf = self.buffersize  # set buffer value

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01(self):
        "Checking modifying one table row with Row.update"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3]],
                          formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing row
        for row in table.iterrows(2, 3):
            (row['col1'], row['col2'], row['col3']) = [456, 'db2', 1.2]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [456, 'db2', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test02(self):
        "Modifying one row, with a step (Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, 3, 2):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 3:
                (row['col1'], row['col2'], row['col3']) = [6, 'de2', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [457, 'db1', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test03(self):
        "Checking modifying several rows at once (Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, 3):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 2:
                (row['col1'], row['col2'], row['col3']) = [5, 'de1', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [5, 'de1', 1.3], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test04(self):
        "Modifying several rows at once, with a step (Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify two existing rows
        for row in table.iterrows(1, stop=4, step=2):
            if row.nrow == 1:
                (row['col1'], row['col2'], row['col3']) = [457, 'db1', 1.2]
            elif row.nrow == 3:
                (row['col1'], row['col2'], row['col3']) = [6, 'de2', 1.3]
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [457, 'db1', 1.2],
                            [457, 'db1', 1.2], [6, 'de2', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertTrue(table.nrows, 4)

    def test05(self):
        "Checking modifying one column (single element, Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        for row in table.iterrows(1, 2):
            row['col1'] = -1
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [-1, 'ded', 1.3],
                            [457, 'db1', 1.2], [5, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test06(self):
        "Checking modifying one column (several elements, Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          2, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])

        # Modify just one existing column
        for row in table.iterrows(1, 4):
            row['col1'] = row.nrow + 1
            row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ded', 1.3],
                            [3, 'db1', 1.2], [4, 'de1', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test07(self):
        "Modifying values from a selection"

        table = self.table
        formats = table.description._v_nested_formats

        # append new rows
        r = records.array([[456, 'dbe', 1.2], [
                          1, 'ded', 1.3]], formats=formats)
        table.append(r)
        table.append([[457, 'db1', 1.2], [5, 'de1', 1.3]])
        # Modify just rows with col1 < 456
        for row in table.iterrows():
            if row['col1'][0] < 456:
                row['col1'] = 2
                row['col2'] = 'ada'
                row.update()
        # Create the modified recarray
        r1 = records.array([[456, 'dbe', 1.2], [2, 'ada', 1.3],
                            [457, 'db1', 1.2], [2, 'ada', 1.3]],
                           formats=formats,
                           names="col1,col2,col3")
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, 4)

    def test08(self):
        "Modifying a large table (Row.update)"

        table = self.table
        formats = table.description._v_nested_formats

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify all the rows
        for row in table.iterrows():
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats=formats,
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i
            r1['col2'][i] = 'b'+str(i)
            r1['col3'][i] = 0.0
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test08b(self):
        "Setting values on a large table without calling Row.update"

        table = self.table
        formats = table.description._v_nested_formats

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify all the rows (actually don't)
        for row in table.iterrows():
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            # row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats=formats,
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i-1
            r1['col2'][i] = 'a'+str(i-1)
            r1['col3'][i] = -1.0
        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test09(self):
        "Modifying selected values on a large table"

        table = self.table
        formats = table.description._v_nested_formats

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify selected rows
        for row in table.iterrows():
            if row['col1'][0] > nrows-3:
                row['col1'] = row.nrow
                row['col2'] = 'b'+str(row.nrow)
                row['col3'] = 0.0
                row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats=formats,
                           names="col1,col2,col3")
        for i in xrange(nrows):
            r1['col1'][i] = i-1
            r1['col2'][i] = 'a'+str(i-1)
            r1['col3'][i] = -1.0
        # modify just the last line
        r1['col1'][i] = i
        r1['col2'][i] = 'b'+str(i)
        r1['col3'][i] = 0.0

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)

    def test09b(self):
        "Modifying selected values on a large table (alternate values)"

        table = self.table
        formats = table.description._v_nested_formats

        nrows = 100
        # append new rows
        row = table.row
        for i in xrange(nrows):
            row['col1'] = i-1
            row['col2'] = 'a'+str(i-1)
            row['col3'] = -1.0
            row.append()
        table.flush()

        # Modify selected rows
        for row in table.iterrows(step=10):
            row['col1'] = row.nrow
            row['col2'] = 'b'+str(row.nrow)
            row['col3'] = 0.0
            row.update()

        # Create the modified recarray
        r1 = records.array(None, shape=nrows,
                           formats=formats,
                           names="col1,col2,col3")
        for i in xrange(nrows):
            if i % 10 > 0:
                r1['col1'][i] = i-1
                r1['col2'][i] = 'a'+str(i-1)
                r1['col3'][i] = -1.0
            else:
                r1['col1'][i] = i
                r1['col2'][i] = 'b'+str(i)
                r1['col3'][i] = 0.0

        # Read the modified table
        if self.reopen:
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            table = self.fileh.root.recarray
            table.nrowsinbuf = self.buffersize  # set buffer value
        r2 = table.read()
        if common.verbose:
            print("Original table-->", repr(r2))
            print("Should look like-->", repr(r1))
        self.assertEqual(r1.tostring(), r2.tostring())
        self.assertEqual(table.nrows, nrows)


class updateRow1(updateRow):
    reopen = 0
    buffersize = 1


class updateRow2(updateRow):
    reopen = 1
    buffersize = 2


class updateRow3(updateRow):
    reopen = 0
    buffersize = 1000


class updateRow4(updateRow):
    reopen = 1
    buffersize = 1000


#----------------------------------------------------------------------
def suite():
    theSuite = unittest.TestSuite()
    niter = 1
    # common.heavy = 1  # Uncomment this only for testing purposes

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicWriteTestCase))
        theSuite.addTest(unittest.makeSuite(DictWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecordDTWriteTestCase))
        theSuite.addTest(unittest.makeSuite(NumPyDTWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayOneWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayTwoWriteTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayThreeWriteTestCase))
        theSuite.addTest(unittest.makeSuite(CompressZLIBTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressTwoTablesTestCase))
        theSuite.addTest(unittest.makeSuite(IterRangeTestCase))
        theSuite.addTest(unittest.makeSuite(RecArrayRangeTestCase))
        theSuite.addTest(unittest.makeSuite(getColRangeTestCase))
        theSuite.addTest(unittest.makeSuite(DefaultValues))
        theSuite.addTest(unittest.makeSuite(RecArrayIO))
        theSuite.addTest(unittest.makeSuite(ShapeTestCase1))
        theSuite.addTest(unittest.makeSuite(ShapeTestCase2))
        theSuite.addTest(unittest.makeSuite(setItem1))
        theSuite.addTest(unittest.makeSuite(setItem2))
        theSuite.addTest(unittest.makeSuite(setItem3))
        theSuite.addTest(unittest.makeSuite(setItem4))
        theSuite.addTest(unittest.makeSuite(updateRow1))
        theSuite.addTest(unittest.makeSuite(updateRow2))
        theSuite.addTest(unittest.makeSuite(updateRow3))
        theSuite.addTest(unittest.makeSuite(updateRow4))
        theSuite.addTest(unittest.makeSuite(CompressBloscTablesTestCase))
        theSuite.addTest(unittest.makeSuite(CompressLZOTablesTestCase))
    if common.heavy:
        theSuite.addTest(unittest.makeSuite(CompressBzip2TablesTestCase))
        theSuite.addTest(unittest.makeSuite(BigTablesTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_timetype
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: December 15, 2004
# Author:  Ivan Vilata i Balaguer - reverse:net.selidor@ivan
#
# $Id$
#
########################################################################

"""Unit test for the Time datatypes."""

from __future__ import print_function
import unittest
import tempfile
import os

import numpy

import tables
from tables.tests import common
from tables.tests.common import allequal

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class LeafCreationTestCase(common.PyTablesTestCase):
    "Tests creating Tables, VLArrays an EArrays with Time data."

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
          * 'h5file', the writable, empty, temporary HDF5 file
        """

        self.h5fname = tempfile.mktemp(suffix='.h5')
        self.h5file = tables.open_file(
            self.h5fname, 'w', title="Test for creating a time leaves")

    def tearDown(self):
        """tearDown() -> None

        Closes 'h5file'; removes 'h5fname'.
        """

        self.h5file.close()
        self.h5file = None
        os.remove(self.h5fname)

    def test00_UnidimLeaves(self):
        "Creating new nodes with unidimensional time elements."

        # Table creation.
        class MyTimeRow(tables.IsDescription):
            intcol = tables.IntCol()
            t32col = tables.Time32Col()
            t64col = tables.Time64Col()
        self.h5file.create_table('/', 'table', MyTimeRow)

        # VLArray creation.
        self.h5file.create_vlarray('/', 'vlarray4', tables.Time32Atom())
        self.h5file.create_vlarray('/', 'vlarray8', tables.Time64Atom())

        # EArray creation.
        self.h5file.create_earray('/', 'earray4',
                                  tables.Time32Atom(), shape=(0,))
        self.h5file.create_earray('/', 'earray8',
                                  tables.Time64Atom(), shape=(0,))

    def test01_MultidimLeaves(self):
        "Creating new nodes with multidimensional time elements."

        # Table creation.
        class MyTimeRow(tables.IsDescription):
            intcol = tables.IntCol(shape=(2, 1))
            t32col = tables.Time32Col(shape=(2, 1))
            t64col = tables.Time64Col(shape=(2, 1))
        self.h5file.create_table('/', 'table', MyTimeRow)

        # VLArray creation.
        self.h5file.create_vlarray(
            '/', 'vlarray4', tables.Time32Atom(shape=(2, 1)))
        self.h5file.create_vlarray(
            '/', 'vlarray8', tables.Time64Atom(shape=(2, 1)))

        # EArray creation.
        self.h5file.create_earray(
            '/', 'earray4', tables.Time32Atom(), shape=(0, 2, 1))
        self.h5file.create_earray(
            '/', 'earray8', tables.Time64Atom(), shape=(0, 2, 1))


class OpenTestCase(common.PyTablesTestCase):
    "Tests opening a file with Time nodes."

    # The description used in the test Table.
    class MyTimeRow(tables.IsDescription):
        t32col = tables.Time32Col(shape=(2, 1))
        t64col = tables.Time64Col(shape=(2, 1))

    # The atoms used in the test VLArrays.
    myTime32Atom = tables.Time32Atom(shape=(2, 1))
    myTime64Atom = tables.Time64Atom(shape=(2, 1))

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file with '/table',
            '/vlarray4' and '/vlarray8' nodes.
        """

        self.h5fname = tempfile.mktemp(suffix='.h5')

        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for creating time leaves")

        # Create test Table.
        h5file.create_table('/', 'table', self.MyTimeRow)

        # Create test VLArrays.
        h5file.create_vlarray('/', 'vlarray4', self.myTime32Atom)
        h5file.create_vlarray('/', 'vlarray8', self.myTime64Atom)

        h5file.close()

    def tearDown(self):
        """tearDown() -> None

        Removes 'h5fname'.
        """

        os.remove(self.h5fname)

    def test00_OpenFile(self):
        "Opening a file with Time nodes."

        h5file = tables.open_file(self.h5fname)

        # Test the Table node.
        tbl = h5file.root.table
        self.assertEqual(
            tbl.coldtypes['t32col'],
            self.MyTimeRow.columns['t32col'].dtype,
            "Column dtypes do not match.")
        self.assertEqual(
            tbl.coldtypes['t64col'],
            self.MyTimeRow.columns['t64col'].dtype,
            "Column dtypes do not match.")

        # Test the VLArray nodes.
        vla4 = h5file.root.vlarray4
        self.assertEqual(
            vla4.atom.dtype, self.myTime32Atom.dtype,
            "Atom types do not match.")
        self.assertEqual(
            vla4.atom.shape, self.myTime32Atom.shape,
            "Atom shapes do not match.")

        vla8 = h5file.root.vlarray8
        self.assertEqual(
            vla8.atom.dtype, self.myTime64Atom.dtype,
            "Atom types do not match.")
        self.assertEqual(
            vla8.atom.shape, self.myTime64Atom.shape,
            "Atom shapes do not match.")

        h5file.close()

    def test01_OpenFileStype(self):
        "Opening a file with Time nodes, comparing Atom.stype."

        h5file = tables.open_file(self.h5fname)

        # Test the Table node.
        tbl = h5file.root.table
        self.assertEqual(
            tbl.coltypes['t32col'],
            self.MyTimeRow.columns['t32col'].type,
            "Column types do not match.")
        self.assertEqual(
            tbl.coltypes['t64col'],
            self.MyTimeRow.columns['t64col'].type,
            "Column types do not match.")

        # Test the VLArray nodes.
        vla4 = h5file.root.vlarray4
        self.assertEqual(
            vla4.atom.type, self.myTime32Atom.type,
            "Atom types do not match.")

        vla8 = h5file.root.vlarray8
        self.assertEqual(
            vla8.atom.type, self.myTime64Atom.type,
            "Atom types do not match.")

        h5file.close()


class CompareTestCase(common.PyTablesTestCase):
    "Tests whether stored and retrieved time data is kept the same."

    # The description used in the test Table.
    class MyTimeRow(tables.IsDescription):
        t32col = tables.Time32Col(pos=0)
        t64col = tables.Time64Col(shape=(2,), pos = 1)

    # The atoms used in the test VLArrays.
    myTime32Atom = tables.Time32Atom(shape=(2,))
    myTime64Atom = tables.Time64Atom(shape=(2,))

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
        """

        self.h5fname = tempfile.mktemp(suffix='.h5')

    def tearDown(self):
        """tearDown() -> None

        Removes 'h5fname'.
        """

        os.remove(self.h5fname)

    def test00_Compare32VLArray(self):
        "Comparing written 32-bit time data with read data in a VLArray."

        wtime = numpy.array((1234567890,) * 2, numpy.int32)

        # Create test VLArray with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time32 VL arrays")
        vla = h5file.create_vlarray('/', 'test', self.myTime32Atom)
        vla.append(wtime)
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        rtime = h5file.root.test.read()[0][0]
        h5file.close()
        self.assertTrue(allequal(rtime, wtime),
                        "Stored and retrieved values do not match.")

    def test01_Compare64VLArray(self):
        "Comparing written 64-bit time data with read data in a VLArray."

        wtime = numpy.array((1234567890.123456,) * 2, numpy.float64)

        # Create test VLArray with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time64 VL arrays")
        vla = h5file.create_vlarray('/', 'test', self.myTime64Atom)
        vla.append(wtime)
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        rtime = h5file.root.test.read()[0][0]
        h5file.close()
        self.assertTrue(allequal(rtime, wtime),
                        "Stored and retrieved values do not match.")

    def test01b_Compare64VLArray(self):
        "Comparing several written and read 64-bit time values in a VLArray."

        # Create test VLArray with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time64 VL arrays")
        vla = h5file.create_vlarray('/', 'test', self.myTime64Atom)

        # Size of the test.
        nrows = vla.nrowsinbuf + 34  # Add some more rows than buffer.
        # Only for home checks; the value above should check better
        # the I/O with multiple buffers.
        # nrows = 10

        for i in xrange(nrows):
            j = i * 2
            vla.append((j + 0.012, j + 1 + 0.012))
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        arr = h5file.root.test.read()
        h5file.close()

        arr = numpy.array(arr)
        orig_val = numpy.arange(0, nrows * 2, dtype=numpy.int32) + 0.012
        orig_val.shape = (nrows, 1, 2)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", arr)
        self.assertTrue(allequal(arr, orig_val),
                        "Stored and retrieved values do not match.")

    def test02_CompareTable(self):
        "Comparing written time data with read data in a Table."

        wtime = 1234567890.123456

        # Create test Table with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time tables")
        tbl = h5file.create_table('/', 'test', self.MyTimeRow)
        row = tbl.row
        row['t32col'] = int(wtime)
        row['t64col'] = (wtime, wtime)
        row.append()
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        recarr = h5file.root.test.read(0)
        h5file.close()

        self.assertEqual(recarr['t32col'][0], int(wtime),
                         "Stored and retrieved values do not match.")

        comp = (recarr['t64col'][0] == numpy.array((wtime, wtime)))
        self.assertTrue(numpy.alltrue(comp),
                        "Stored and retrieved values do not match.")

    def test02b_CompareTable(self):
        "Comparing several written and read time values in a Table."

        # Create test Table with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time tables")
        tbl = h5file.create_table('/', 'test', self.MyTimeRow)

        # Size of the test.
        nrows = tbl.nrowsinbuf + 34  # Add some more rows than buffer.
        # Only for home checks; the value above should check better
        # the I/O with multiple buffers.
        # nrows = 10

        row = tbl.row
        for i in xrange(nrows):
            row['t32col'] = i
            j = i * 2
            row['t64col'] = (j + 0.012, j+1+0.012)
            row.append()
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        recarr = h5file.root.test.read()
        h5file.close()

        # Time32 column.
        orig_val = numpy.arange(nrows, dtype=numpy.int32)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", recarr['t32col'][:])
        self.assertTrue(numpy.alltrue(recarr['t32col'][:] == orig_val),
                        "Stored and retrieved values do not match.")

        # Time64 column.
        orig_val = numpy.arange(0, nrows * 2, dtype=numpy.int32) + 0.012
        orig_val.shape = (nrows, 2)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", recarr['t64col'][:])
        self.assertTrue(allequal(recarr['t64col'][:], orig_val, numpy.float64),
                        "Stored and retrieved values do not match.")

    def test03_Compare64EArray(self):
        "Comparing written 64-bit time data with read data in an EArray."

        wtime = 1234567890.123456

        # Create test EArray with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time64 EArrays")
        ea = h5file.create_earray(
            '/', 'test', tables.Time64Atom(), shape=(0,))
        ea.append((wtime,))
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        rtime = h5file.root.test[0]
        h5file.close()
        self.assertTrue(allequal(rtime, wtime),
                        "Stored and retrieved values do not match.")

    def test03b_Compare64EArray(self):
        "Comparing several written and read 64-bit time values in an EArray."

        # Create test EArray with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time64 E arrays")
        ea = h5file.create_earray(
            '/', 'test', tables.Time64Atom(), shape=(0, 2))

        # Size of the test.
        nrows = ea.nrowsinbuf + 34  # Add some more rows than buffer.
        # Only for home checks; the value above should check better
        # the I/O with multiple buffers.
        # nrows = 10

        for i in xrange(nrows):
            j = i * 2
            ea.append(((j + 0.012, j + 1 + 0.012),))
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        arr = h5file.root.test.read()
        h5file.close()

        orig_val = numpy.arange(0, nrows * 2, dtype=numpy.int32) + 0.012
        orig_val.shape = (nrows, 2)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", arr)
        self.assertTrue(allequal(arr, orig_val),
                        "Stored and retrieved values do not match.")


class UnalignedTestCase(common.PyTablesTestCase):
    "Tests writing and reading unaligned time values in a table."

    # The description used in the test Table.
    # Time fields are unaligned because of 'i8col'.
    class MyTimeRow(tables.IsDescription):
        i8col = tables.Int8Col(pos=0)
        t32col = tables.Time32Col(pos=1)
        t64col = tables.Time64Col(shape=(2,), pos = 2)

    def setUp(self):
        """setUp() -> None

        This method sets the following instance attributes:
          * 'h5fname', the name of the temporary HDF5 file
        """

        self.h5fname = tempfile.mktemp(suffix='.h5')

    def tearDown(self):
        """tearDown() -> None

        Removes 'h5fname'.
        """

        os.remove(self.h5fname)

    def test00_CompareTable(self):
        "Comparing written unaligned time data with read data in a Table."

        # Create test Table with data.
        h5file = tables.open_file(
            self.h5fname, 'w', title="Test for comparing Time tables")
        tbl = h5file.create_table('/', 'test', self.MyTimeRow)

        # Size of the test.
        nrows = tbl.nrowsinbuf + 34  # Add some more rows than buffer.
        # Only for home checks; the value above should check better
        # the I/O with multiple buffers.
        # nrows = 10

        row = tbl.row
        for i in xrange(nrows):
            row['i8col'] = i
            row['t32col'] = i
            j = i * 2
            row['t64col'] = (j + 0.012, j+1+0.012)
            row.append()
        h5file.close()

        # Check the written data.
        h5file = tables.open_file(self.h5fname)
        recarr = h5file.root.test.read()
        h5file.close()

        # Int8 column.
        orig_val = numpy.arange(nrows, dtype=numpy.int8)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", recarr['i8col'][:])
        self.assertTrue(numpy.alltrue(recarr['i8col'][:] == orig_val),
                        "Stored and retrieved values do not match.")

        # Time32 column.
        orig_val = numpy.arange(nrows, dtype=numpy.int32)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", recarr['t32col'][:])
        self.assertTrue(numpy.alltrue(recarr['t32col'][:] == orig_val),
                        "Stored and retrieved values do not match.")

        # Time64 column.
        orig_val = numpy.arange(0, nrows * 2, dtype=numpy.int32) + 0.012
        orig_val.shape = (nrows, 2)
        if common.verbose:
            print("Original values:", orig_val)
            print("Retrieved values:", recarr['t64col'][:])
        self.assertTrue(allequal(recarr['t64col'][:], orig_val, numpy.float64),
                        "Stored and retrieved values do not match.")


class BigEndianTestCase(common.PyTablesTestCase):
    "Tests for reading big-endian time values in arrays and nested tables."

    def setUp(self):
        filename = self._testFilename('times-nested-be.h5')
        self.h5f = tables.open_file(filename, 'r')

    def tearDown(self):
        self.h5f.close()

    def test00a_Read32Array(self):
        "Checking Time32 type in arrays."

        # Check the written data.
        earr = self.h5f.root.earr32[:]

        # Generate the expected Time32 array.
        start = 1178896298
        nrows = 10
        orig_val = numpy.arange(start, start + nrows, dtype=numpy.int32)

        if common.verbose:
            print("Retrieved values:", earr)
            print("Should look like:", orig_val)
        self.assertTrue(numpy.alltrue(earr == orig_val),
                        "Retrieved values do not match the expected values.")

    def test00b_Read64Array(self):
        "Checking Time64 type in arrays."

        # Check the written data.
        earr = self.h5f.root.earr64[:]

        # Generate the expected Time64 array.
        start = 1178896298.832258
        nrows = 10
        orig_val = numpy.arange(start, start + nrows, dtype=numpy.float64)

        if common.verbose:
            print("Retrieved values:", earr)
            print("Should look like:", orig_val)
        self.assertTrue(numpy.allclose(earr, orig_val, rtol=1.e-15),
                        "Retrieved values do not match the expected values.")

    def test01a_ReadPlainColumn(self):
        "Checking Time32 type in plain columns."

        # Check the written data.
        tbl = self.h5f.root.tbl
        t32 = tbl.cols.t32[:]

        # Generate the expected Time32 array.
        start = 1178896298
        nrows = 10
        orig_val = numpy.arange(start, start + nrows, dtype=numpy.int32)

        if common.verbose:
            print("Retrieved values:", t32)
            print("Should look like:", orig_val)
        self.assertTrue(numpy.alltrue(t32 == orig_val),
                        "Retrieved values do not match the expected values.")

    def test01b_ReadNestedColumn(self):
        "Checking Time64 type in nested columns."

        # Check the written data.
        tbl = self.h5f.root.tbl
        t64 = tbl.cols.nested.t64[:]

        # Generate the expected Time64 array.
        start = 1178896298.832258
        nrows = 10
        orig_val = numpy.arange(start, start + nrows, dtype=numpy.float64)

        if common.verbose:
            print("Retrieved values:", t64)
            print("Should look like:", orig_val)
        self.assertTrue(numpy.allclose(t64, orig_val, rtol=1.e-15),
                        "Retrieved values do not match the expected values.")

    def test02_ReadNestedColumnTwice(self):
        "Checking Time64 type in nested columns (read twice)."

        # Check the written data.
        tbl = self.h5f.root.tbl
        dummy = tbl.cols.nested.t64[:]
        self.assertTrue(dummy is not None)
        t64 = tbl.cols.nested.t64[:]

        # Generate the expected Time64 array.
        start = 1178896298.832258
        nrows = 10
        orig_val = numpy.arange(start, start + nrows, dtype=numpy.float64)

        if common.verbose:
            print("Retrieved values:", t64)
            print("Should look like:", orig_val)
        self.assertTrue(numpy.allclose(t64, orig_val, rtol=1.e-15),
                        "Retrieved values do not match the expected values.")


#----------------------------------------------------------------------

def suite():
    """suite() -> test suite

    Returns a test suite consisting of all the test cases in the module.
    """

    theSuite = unittest.TestSuite()

    theSuite.addTest(unittest.makeSuite(LeafCreationTestCase))
    theSuite.addTest(unittest.makeSuite(OpenTestCase))
    theSuite.addTest(unittest.makeSuite(CompareTestCase))
    theSuite.addTest(unittest.makeSuite(UnalignedTestCase))
    theSuite.addTest(unittest.makeSuite(BigEndianTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')



## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = test_tree
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import warnings
import unittest
import os
import tempfile

from tables import *
# Next imports are only necessary for this test suite
from tables import Group, Leaf, Table, Array

from tables.tests import common

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

# Test Record class


class Record(IsDescription):
    var1 = StringCol(itemsize=4)  # 4-character String
    var2 = IntCol()              # integer
    var3 = Int16Col()            # short integer
    var4 = FloatCol()            # double (double-precision)
    var5 = Float32Col()          # float  (single-precision)


class TreeTestCase(unittest.TestCase):
    mode = "w"
    title = "This is the table title"
    expectedrows = 10
    appendrows = 5

    def setUp(self):
        # Create a temporary file
        self.file = tempfile.mktemp(".h5")
        # Create an instance of HDF5 Table
        self.h5file = open_file(self.file, self.mode, self.title)
        self.populateFile()
        self.h5file.close()

    def populateFile(self):
        group = self.h5file.root
        maxshort = 1 << 15
        # maxint   = 2147483647   # (2 ** 31 - 1)
        for j in range(3):
            # Create a table
            table = self.h5file.create_table(group, 'table'+str(j), Record,
                                             title=self.title,
                                             filters=None,
                                             expectedrows=self.expectedrows)
            # Get the record object associated with the new table
            d = table.row
            # Fill the table
            for i in xrange(self.expectedrows):
                d['var1'] = '%04d' % (self.expectedrows - i)
                d['var2'] = i
                d['var3'] = i % maxshort
                d['var4'] = float(i)
                d['var5'] = float(i)
                d.append()      # This injects the Record values
            # Flush the buffer for this table
            table.flush()

            # Create a couple of arrays in each group
            var1List = [x['var1'] for x in table.iterrows()]
            var4List = [x['var4'] for x in table.iterrows()]

            self.h5file.create_array(group, 'var1', var1List, "1")
            self.h5file.create_array(group, 'var4', var4List, "4")

            # Create a new group (descendant of group)
            group2 = self.h5file.create_group(group, 'group'+str(j))
            # Iterate over this new group (group2)
            group = group2

    def tearDown(self):
        # Close the file
        if self.h5file.isopen:
            self.h5file.close()

        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_getNode(self):
        "Checking the File.get_node() with string node names"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_getNode..." % self.__class__.__name__)

        self.h5file = open_file(self.file, "r")
        nodelist = ['/', '/table0', '/group0/var1', '/group0/group1/var4']
        nodenames = []
        for node in nodelist:
            object = self.h5file.get_node(node)
            nodenames.append(object._v_pathname)

        self.assertEqual(nodenames, nodelist)
        if common.verbose:
            print("get_node(pathname) test passed")
        nodegroups = [
            '/', '/group0', '/group0/group1', '/group0/group1/group2']
        nodenames = ['var1', 'var4']
        nodepaths = []
        for group in nodegroups:
            for name in nodenames:
                try:
                    object = self.h5file.get_node(group, name)
                except LookupError:
                    pass
                else:
                    nodepaths.append(object._v_pathname)

        self.assertEqual(nodepaths,
                         ['/var1', '/var4',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("get_node(groupname, name) test passed")
        nodelist = ['/', '/group0', '/group0/group1', '/group0/group1/group2',
                    '/table0']
        nodenames = []
        groupobjects = []
        # warnings.filterwarnings("error", category=UserWarning)
        for node in nodelist:
            try:
                object = self.h5file.get_node(node, classname='Group')
            except LookupError:
                if common.verbose:
                    (type, value, traceback) = sys.exc_info()
                    print("\nGreat!, the next LookupError was catched!")
                    print(value)
            else:
                nodenames.append(object._v_pathname)
                groupobjects.append(object)

        self.assertEqual(nodenames,
                         ['/', '/group0', '/group0/group1',
                          '/group0/group1/group2'])
        if common.verbose:
            print("get_node(groupname, classname='Group') test passed")

        # Reset the warning
        # warnings.filterwarnings("default", category=UserWarning)

        nodenames = ['var1', 'var4']
        nodearrays = []
        for group in groupobjects:
            for name in nodenames:
                try:
                    object = self.h5file.get_node(group, name, 'Array')
                except:
                    pass
                else:
                    nodearrays.append(object._v_pathname)

        self.assertEqual(nodearrays,
                         ['/var1', '/var4',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/var1', '/group0/group1/var4'])
        if common.verbose:
            print("get_node(groupobject, name, classname='Array') test passed")

    def test01_getNodeClass(self):
        "Checking the File.get_node() with instances"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_getNodeClass..." %
                  self.__class__.__name__)

        self.h5file = open_file(self.file, "r")
        # This tree ways of get_node usage should return a table instance
        table = self.h5file.get_node("/group0/table1")
        self.assertTrue(isinstance(table, Table))
        table = self.h5file.get_node("/group0", "table1")
        self.assertTrue(isinstance(table, Table))
        table = self.h5file.get_node(self.h5file.root.group0, "table1")
        self.assertTrue(isinstance(table, Table))

        # This should return an array instance
        arr = self.h5file.get_node("/group0/var1")
        self.assertTrue(isinstance(arr, Array))
        self.assertTrue(isinstance(arr, Leaf))

        # And this a Group
        group = self.h5file.get_node("/group0", "group1", "Group")
        self.assertTrue(isinstance(group, Group))

    def test02_listNodes(self):
        "Checking the File.list_nodes() method"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_listNodes..." % self.__class__.__name__)

        # Made the warnings to raise an error
        # warnings.filterwarnings("error", category=UserWarning)
        self.h5file = open_file(self.file, "r")

        self.assertRaises(TypeError,
                          self.h5file.list_nodes, '/', 'NoSuchClass')

        nodelist = ['/', '/group0', '/group0/table1', '/group0/group1/group2',
                    '/var1']
        nodenames = []
        objects = []
        for node in nodelist:
            try:
                objectlist = self.h5file.list_nodes(node)
            except:
                pass
            else:
                objects.extend(objectlist)
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0', '/table0', '/var1', '/var4',
                          '/group0/group1', '/group0/table1',
                          '/group0/var1', '/group0/var4'])
        if common.verbose:
            print("list_nodes(pathname) test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = self.h5file.list_nodes(node)
            except:
                pass
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/group1', '/group0/table1',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/group2', '/group0/group1/table2',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("list_nodes(groupobject) test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = self.h5file.list_nodes(node, 'Leaf')
            except TypeError:
                if common.verbose:
                    (type, value, traceback) = sys.exc_info()
                    print("\nGreat!, the next TypeError was catched!")
                    print(value)
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/table1',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/table2',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("list_nodes(groupobject, classname = 'Leaf') test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = self.h5file.list_nodes(node, 'Table')
            except TypeError:
                if common.verbose:
                    (type, value, traceback) = sys.exc_info()
                    print("\nGreat!, the next TypeError was catched!")
                    print(value)
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/table1', '/group0/group1/table2'])

        if common.verbose:
            print("list_nodes(groupobject, classname = 'Table') test passed")

        # Reset the warning
        # warnings.filterwarnings("default", category=UserWarning)

    def test02b_iterNodes(self):
        "Checking the File.iter_nodes() method"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_iterNodes..." % self.__class__.__name__)

        self.h5file = open_file(self.file, "r")

        self.assertRaises(TypeError,
                          self.h5file.list_nodes, '/', 'NoSuchClass')

        nodelist = ['/', '/group0', '/group0/table1', '/group0/group1/group2',
                    '/var1']
        nodenames = []
        objects = []
        for node in nodelist:
            try:
                objectlist = [o for o in self.h5file.iter_nodes(node)]
            except:
                pass
            else:
                objects.extend(objectlist)
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0', '/table0', '/var1', '/var4',
                          '/group0/group1', '/group0/table1',
                          '/group0/var1', '/group0/var4'])
        if common.verbose:
            print("iter_nodes(pathname) test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = [o for o in self.h5file.iter_nodes(node)]
            except:
                pass
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/group1', '/group0/table1',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/group2', '/group0/group1/table2',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("iter_nodes(groupobject) test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = [o for o in self.h5file.iter_nodes(node, 'Leaf')]
            except TypeError:
                if common.verbose:
                    (type, value, traceback) = sys.exc_info()
                    print("\nGreat!, the next TypeError was catched!")
                    print(value)
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/table1',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/table2',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("iter_nodes(groupobject, classname = 'Leaf') test passed")

        nodenames = []
        for node in objects:
            try:
                objectlist = [o for o in self.h5file.iter_nodes(node, 'Table')]
            except TypeError:
                if common.verbose:
                    (type, value, traceback) = sys.exc_info()
                    print("\nGreat!, the next TypeError was catched!")
                    print(value)
            else:
                for object in objectlist:
                    nodenames.append(object._v_pathname)

        self.assertEqual(nodenames,
                         ['/group0/table1', '/group0/group1/table2'])

        if common.verbose:
            print("iter_nodes(groupobject, classname = 'Table') test passed")

        # Reset the warning
        # warnings.filterwarnings("default", category=UserWarning)

    def test03_TraverseTree(self):
        "Checking the File.walk_groups() method"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_TraverseTree..." %
                  self.__class__.__name__)

        self.h5file = open_file(self.file, "r")
        groups = []
        tables = []
        arrays = []
        for group in self.h5file.walk_groups():
            groups.append(group._v_pathname)
            for table in self.h5file.list_nodes(group, 'Table'):
                tables.append(table._v_pathname)
            for arr in self.h5file.list_nodes(group, 'Array'):
                arrays.append(arr._v_pathname)

        self.assertEqual(groups,
                         ["/", "/group0", "/group0/group1",
                          "/group0/group1/group2"])

        self.assertEqual(
            tables,
            ["/table0", "/group0/table1", "/group0/group1/table2"])

        self.assertEqual(arrays,
                         ['/var1', '/var4',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/var1', '/group0/group1/var4'])
        if common.verbose:
            print("walk_groups() test passed")

        groups = []
        tables = []
        arrays = []
        for group in self.h5file.walk_groups("/group0/group1"):
            groups.append(group._v_pathname)
            for table in self.h5file.list_nodes(group, 'Table'):
                tables.append(table._v_pathname)
            for arr in self.h5file.list_nodes(group, 'Array'):
                arrays.append(arr._v_pathname)

        self.assertEqual(groups,
                         ["/group0/group1", "/group0/group1/group2"])

        self.assertEqual(tables, ["/group0/group1/table2"])

        self.assertEqual(arrays, [
                         '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("walk_groups(pathname) test passed")

    def test04_walkNodes(self):
        "Checking File.walk_nodes"

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_walkNodes..." % self.__class__.__name__)

        self.h5file = open_file(self.file, "r")

        self.assertRaises(TypeError,
                          self.h5file.walk_nodes('/', 'NoSuchClass').next)

        groups = []
        tables = []
        tables2 = []
        arrays = []
        for group in self.h5file.walk_nodes(classname="Group"):
            groups.append(group._v_pathname)
            for table in group._f_iter_nodes(classname='Table'):
                tables.append(table._v_pathname)
        # Test the recursivity
        for table in self.h5file.root._f_walknodes('Table'):
            tables2.append(table._v_pathname)

        for arr in self.h5file.walk_nodes(classname='Array'):
            arrays.append(arr._v_pathname)

        self.assertEqual(groups,
                         ["/", "/group0", "/group0/group1",
                          "/group0/group1/group2"])
        self.assertEqual(tables,
                         ["/table0", "/group0/table1",
                          "/group0/group1/table2"])
        self.assertEqual(tables2,
                         ["/table0", "/group0/table1",
                          "/group0/group1/table2"])
        self.assertEqual(arrays,
                         ['/var1', '/var4',
                          '/group0/var1', '/group0/var4',
                          '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("File.__iter__() and Group.__iter__ test passed")

        groups = []
        tables = []
        arrays = []
        for group in self.h5file.walk_nodes("/group0/group1",
                                            classname="Group"):
            groups.append(group._v_pathname)
            for table in group._f_walknodes('Table'):
                tables.append(table._v_pathname)
            for arr in self.h5file.walk_nodes(group, 'Array'):
                arrays.append(arr._v_pathname)

        self.assertEqual(groups,
                         ["/group0/group1", "/group0/group1/group2"])

        self.assertEqual(tables, ["/group0/group1/table2"])

        self.assertEqual(arrays, [
                         '/group0/group1/var1', '/group0/group1/var4'])

        if common.verbose:
            print("walk_nodes(pathname, classname) test passed")


class DeepTreeTestCase(unittest.TestCase):
    """Checks for deep hierarchy levels in PyTables trees."""

    def setUp(self):
        # Here we put a more conservative limit to deal with more platforms
        # With maxdepth = 64 this test would take less than 40 MB
        # of main memory to run, which is quite reasonable nowadays.
        # With maxdepth = 1024 this test will take around 300 MB.
        if common.heavy:
            self.maxdepth = 256  # Takes around 60 MB of memory!
        else:
            self.maxdepth = 64  # This should be safe for most machines
        if common.verbose:
            print("Maximum depth tested :", self.maxdepth)

        # Open a new empty HDF5 file
        self.file = tempfile.mktemp(".h5")
        fileh = open_file(self.file, mode="w")
        group = fileh.root
        if common.verbose:
            print("Depth writing progress: ", end=' ')
        # Iterate until maxdepth
        for depth in range(self.maxdepth):
            # Save it on the HDF5 file
            if common.verbose:
                print("%3d," % (depth), end=' ')
            # Create a couple of arrays here
            fileh.create_array(group, 'array', [1, 1], "depth: %d" % depth)
            fileh.create_array(group, 'array2', [1, 1], "depth: %d" % depth)
            # And also a group
            fileh.create_group(group, 'group2_' + str(depth))
            # Finally, iterate over a new group
            group = fileh.create_group(group, 'group' + str(depth))
        # Close the file
        fileh.close()

    def tearDown(self):
        os.remove(self.file)
        common.cleanup(self)

    def _check_tree(self, file):
        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        group = fileh.root
        if common.verbose:
            print("\nDepth reading progress: ", end=' ')
        # Get the metadata on the previosly saved arrays
        for depth in range(self.maxdepth):
            if common.verbose:
                print("%3d," % (depth), end=' ')
            # Check the contents
            self.assertEqual(group.array[:], [1, 1])
            self.assertTrue("array2" in group)
            self.assertTrue("group2_"+str(depth) in group)
            # Iterate over the next group
            group = fileh.get_node(group, 'group' + str(depth))
        if common.verbose:
            print()  # This flush the stdout buffer
        fileh.close()

    def test00_deepTree(self):
        "Creation of a large depth object tree."
        self._check_tree(self.file)

    def test01a_copyDeepTree(self):
        "Copy of a large depth object tree."
        fileh = open_file(self.file, mode="r")
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w")
        if common.verbose:
            print("\nCopying deep tree...")
        fileh.copy_node(fileh.root, fileh2.root, recursive=True)
        fileh.close()
        fileh2.close()
        self._check_tree(file2)
        os.remove(file2)

    def test01b_copyDeepTree(self):
        "Copy of a large depth object tree with small node cache."
        fileh = open_file(self.file, mode="r", node_cache_slots=10)
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w", node_cache_slots=10)
        if common.verbose:
            print("\nCopying deep tree...")
        fileh.copy_node(fileh.root, fileh2.root, recursive=True)
        fileh.close()
        fileh2.close()
        self._check_tree(file2)
        os.remove(file2)

    def test01c_copyDeepTree(self):
        "Copy of a large depth object tree with no node cache."
        fileh = open_file(self.file, mode="r", node_cache_slots=0)
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w", node_cache_slots=0)
        if common.verbose:
            print("\nCopying deep tree...")
        fileh.copy_node(fileh.root, fileh2.root, recursive=True)
        fileh.close()
        fileh2.close()
        self._check_tree(file2)
        os.remove(file2)

    def test01d_copyDeepTree(self):
        "Copy of a large depth object tree with static node cache."
        # Do not execute this in heavy mode
        if common.heavy:
            return
        fileh = open_file(self.file, mode="r", node_cache_slots=-256)
        file2 = tempfile.mktemp(".h5")
        fileh2 = open_file(file2, mode="w", node_cache_slots=-256)
        if common.verbose:
            print("\nCopying deep tree...")
        fileh.copy_node(fileh.root, fileh2.root, recursive=True)
        fileh.close()
        fileh2.close()
        self._check_tree(file2)
        os.remove(file2)


class WideTreeTestCase(unittest.TestCase):
    """Checks for maximum number of children for a Group."""

    def test00_Leafs(self):
        """Checking creation of large number of leafs (1024) per group.

        Variable 'maxchildren' controls this check. PyTables support up
        to 4096 children per group, but this would take too much memory
        (up to 64 MB) for testing purposes (may be we can add a test for
        big platforms). A 1024 children run takes up to 30 MB. A 512
        children test takes around 25 MB.

        """

        import time
        if common.heavy:
            maxchildren = 4096
        else:
            maxchildren = 256
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_wideTree..." %
                  self.__class__.__name__)
            print("Maximum number of children tested :", maxchildren)
        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        # file = "test_widetree.h5"

        a = [1, 1]
        fileh = open_file(file, mode="w")
        if common.verbose:
            print("Children writing progress: ", end=' ')
        for child in range(maxchildren):
            if common.verbose:
                print("%3d," % (child), end=' ')
            fileh.create_array(fileh.root, 'array' + str(child),
                               a, "child: %d" % child)
        if common.verbose:
            print()
        # Close the file
        fileh.close()

        t1 = time.time()
        a = [1, 1]
        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        if common.verbose:
            print("\nTime spent opening a file with %d arrays: %s s" %
                  (maxchildren, time.time()-t1))
            print("\nChildren reading progress: ", end=' ')
        # Get the metadata on the previosly saved arrays
        for child in range(maxchildren):
            if common.verbose:
                print("%3d," % (child), end=' ')
            # Create an array for later comparison
            # Get the actual array
            array_ = getattr(fileh.root, 'array' + str(child))
            b = array_.read()
            # Arrays a and b must be equal
            self.assertEqual(a, b)
        if common.verbose:
            print()  # This flush the stdout buffer
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)

    def test01_wideTree(self):
        """Checking creation of large number of groups (1024) per group.

        Variable 'maxchildren' controls this check. PyTables support up
        to 4096 children per group, but this would take too much memory
        (up to 64 MB) for testing purposes (may be we can add a test for
        big platforms). A 1024 children run takes up to 30 MB. A 512
        children test takes around 25 MB.

        """

        import time
        if common.heavy:
            # for big platforms!
            maxchildren = 4096
        else:
            # for standard platforms
            maxchildren = 256
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_wideTree..." %
                  self.__class__.__name__)
            print("Maximum number of children tested :", maxchildren)
        # Open a new empty HDF5 file
        file = tempfile.mktemp(".h5")
        # file = "test_widetree.h5"

        fileh = open_file(file, mode="w")
        if common.verbose:
            print("Children writing progress: ", end=' ')
        for child in range(maxchildren):
            if common.verbose:
                print("%3d," % (child), end=' ')
            fileh.create_group(fileh.root, 'group' + str(child),
                               "child: %d" % child)
        if common.verbose:
            print()
        # Close the file
        fileh.close()

        t1 = time.time()
        # Open the previous HDF5 file in read-only mode
        fileh = open_file(file, mode="r")
        if common.verbose:
            print("\nTime spent opening a file with %d groups: %s s" %
                  (maxchildren, time.time()-t1))
            print("\nChildren reading progress: ", end=' ')
        # Get the metadata on the previosly saved arrays
        for child in range(maxchildren):
            if common.verbose:
                print("%3d," % (child), end=' ')
            # Get the actual group
            group = getattr(fileh.root, 'group' + str(child))
            # Arrays a and b must be equal
            self.assertEqual(group._v_title, "child: %d" % child)
        if common.verbose:
            print()  # This flush the stdout buffer
        # Close the file
        fileh.close()
        # Then, delete the file
        os.remove(file)


class HiddenTreeTestCase(unittest.TestCase):

    """Check for hidden groups, leaves and hierarchies."""

    def setUp(self):
        self.h5fname = tempfile.mktemp('.h5')
        self.h5file = open_file(
            self.h5fname, 'w', title="Test for hidden nodes")

        self.visible = []  # list of visible object paths
        self.hidden = []  # list of hidden object paths

        # Create some visible nodes: a, g, g/a1, g/a2, g/g, g/g/a.
        h5f = self.h5file
        h5f.create_array('/', 'a', [0])
        g = h5f.create_group('/', 'g')
        h5f.create_array(g, 'a1', [0])
        h5f.create_array(g, 'a2', [0])
        g_g = h5f.create_group(g, 'g')
        h5f.create_array(g_g, 'a', [0])

        self.visible.extend(['/a', '/g', '/g/a1', '/g/a2', '/g/g', '/g/g/a'])

        # Create some hidden nodes: _p_a, _p_g, _p_g/a, _p_g/_p_a, g/_p_a.
        h5f.create_array('/', '_p_a', [0])
        hg = h5f.create_group('/', '_p_g')
        h5f.create_array(hg, 'a', [0])
        h5f.create_array(hg, '_p_a', [0])
        h5f.create_array(g, '_p_a', [0])

        self.hidden.extend(
            ['/_p_a', '/_p_g', '/_p_g/a', '/_p_g/_p_a', '/g/_p_a'])

    def tearDown(self):
        self.h5file.close()
        self.h5file = None
        os.remove(self.h5fname)

    # The test behind commented out because the .objects dictionary
    # has been removed (as well as .leaves and .groups)
    def _test00_objects(self):
        """Absence of hidden nodes in `File.objects`."""

        objects = self.h5file.objects

        warnings.filterwarnings('ignore', category=DeprecationWarning)

        for vpath in self.visible:
            self.assertTrue(
                vpath in objects,
                "Missing visible node ``%s`` from ``File.objects``." % vpath)
        for hpath in self.hidden:
            self.assertTrue(
                hpath not in objects,
                "Found hidden node ``%s`` in ``File.objects``." % hpath)

        warnings.filterwarnings('default', category=DeprecationWarning)

    # The test behind commented out because the .objects dictionary
    # has been removed (as well as .leaves and .groups)
    def _test00b_objects(self):
        """Object dictionaries conformance with ``walk_nodes()``."""

        def dictCheck(dictName, classname):
            file_ = self.h5file

            objects = getattr(file_, dictName)
            walkPaths = [node._v_pathname
                         for node in file_.walk_nodes('/', classname)]
            dictPaths = [path for path in objects]
            walkPaths.sort()
            dictPaths.sort()
            self.assertEqual(
                walkPaths, dictPaths,
                "nodes in ``%s`` do not match those from ``walk_nodes()``"
                % dictName)
            self.assertEqual(
                len(walkPaths), len(objects),
                "length of ``%s`` differs from that of ``walk_nodes()``"
                % dictName)

        warnings.filterwarnings('ignore', category=DeprecationWarning)

        dictCheck('objects', None)
        dictCheck('groups', 'Group')
        dictCheck('leaves', 'Leaf')

        warnings.filterwarnings('default', category=DeprecationWarning)

    def test01_getNode(self):
        """Node availability via `File.get_node()`."""

        h5f = self.h5file

        for vpath in self.visible:
            h5f.get_node(vpath)
        for hpath in self.hidden:
            h5f.get_node(hpath)

    def test02_walkGroups(self):
        """Hidden group absence in `File.walk_groups()`."""

        hidden = self.hidden

        for group in self.h5file.walk_groups('/'):
            pathname = group._v_pathname
            self.assertTrue(pathname not in hidden,
                            "Walked across hidden group ``%s``." % pathname)

    def test03_walkNodes(self):
        """Hidden node absence in `File.walk_nodes()`."""

        hidden = self.hidden

        for node in self.h5file.walk_nodes('/'):
            pathname = node._v_pathname
            self.assertTrue(pathname not in hidden,
                            "Walked across hidden node ``%s``." % pathname)

    def test04_listNodesVisible(self):
        """Listing visible nodes under a visible group (list_nodes)."""

        hidden = self.hidden

        for node in self.h5file.list_nodes('/g'):
            pathname = node._v_pathname
            self.assertTrue(pathname not in hidden,
                            "Listed hidden node ``%s``." % pathname)

    def test04b_listNodesVisible(self):
        """Listing visible nodes under a visible group (iter_nodes)."""

        hidden = self.hidden

        for node in self.h5file.iter_nodes('/g'):
            pathname = node._v_pathname
            self.assertTrue(pathname not in hidden,
                            "Listed hidden node ``%s``." % pathname)

    def test05_listNodesHidden(self):
        """Listing visible nodes under a hidden group (list_nodes)."""

        hidden = self.hidden

        node_to_find = '/_p_g/a'
        found_node = False
        for node in self.h5file.list_nodes('/_p_g'):
            pathname = node._v_pathname
            if pathname == node_to_find:
                found_node = True
            self.assertTrue(pathname in hidden,
                            "Listed hidden node ``%s``." % pathname)

        self.assertTrue(found_node,
                        "Hidden node ``%s`` was not listed." % node_to_find)

    def test05b_iterNodesHidden(self):
        """Listing visible nodes under a hidden group (iter_nodes)."""

        hidden = self.hidden

        node_to_find = '/_p_g/a'
        found_node = False
        for node in self.h5file.iter_nodes('/_p_g'):
            pathname = node._v_pathname
            if pathname == node_to_find:
                found_node = True
            self.assertTrue(pathname in hidden,
                            "Listed hidden node ``%s``." % pathname)

        self.assertTrue(found_node,
                        "Hidden node ``%s`` was not listed." % node_to_find)

    # The test behind commented out because the .objects dictionary
    # has been removed (as well as .leaves and .groups)
    def _test06_reopen(self):
        """Reopening a file with hidden nodes."""

        self.h5file.close()
        self.h5file = open_file(self.h5fname)
        self.test00_objects()

    def test07_move(self):
        """Moving a node between hidden and visible groups."""

        is_visible_node = self.h5file.is_visible_node

        self.assertFalse(is_visible_node('/_p_g/a'))
        self.h5file.move_node('/_p_g/a', '/g', 'a')
        self.assertTrue(is_visible_node('/g/a'))
        self.h5file.move_node('/g/a', '/_p_g', 'a')
        self.assertFalse(is_visible_node('/_p_g/a'))

    def test08_remove(self):
        """Removing a visible group with hidden children."""

        self.assertTrue('/g/_p_a' in self.h5file)
        self.h5file.root.g._f_remove(recursive=True)
        self.assertFalse('/g/_p_a' in self.h5file)


class CreateParentsTestCase(common.TempFileMixin, common.PyTablesTestCase):

    """Test the ``createparents`` flag.

    These are mainly for the user interface.  More thorough tests on the
    workings of the flag can be found in the ``test_do_undo.py`` module.

    """

    filters = Filters(complevel=4)  # simply non-default

    def setUp(self):
        super(CreateParentsTestCase, self).setUp()
        self.h5file.create_array('/', 'array', [1])
        self.h5file.create_group('/', 'group', filters=self.filters)

    def test00_parentType(self):
        """Using the right type of parent node argument."""

        h5file, root = self.h5file, self.h5file.root

        self.assertRaises(TypeError, h5file.create_array,
                          root.group, 'arr', [1], createparents=True)
        self.assertRaises(TypeError, h5file.copy_node,
                          '/array', root.group, createparents=True)
        self.assertRaises(TypeError, h5file.move_node,
                          '/array', root.group, createparents=True)
        self.assertRaises(TypeError, h5file.copy_children,
                          '/group', root, createparents=True)

    def test01_inside(self):
        """Placing a node inside a nonexistent child of itself."""
        self.assertRaises(NodeError, self.h5file.move_node,
                          '/group', '/group/foo/bar',
                          createparents=True)
        self.assertFalse('/group/foo' in self.h5file)
        self.assertRaises(NodeError, self.h5file.copy_node,
                          '/group', '/group/foo/bar',
                          recursive=True, createparents=True)
        self.assertFalse('/group/foo' in self.h5file)

    def test02_filters(self):
        """Propagating the filters of created parent groups."""
        self.h5file.create_group('/group/foo/bar', 'baz', createparents=True)
        self.assertTrue('/group/foo/bar/baz' in self.h5file)
        for group in self.h5file.walk_groups('/group'):
            self.assertEqual(self.filters, group._v_filters)


#----------------------------------------------------------------------
def suite():
    theSuite = unittest.TestSuite()
    # This counter is useful when detecting memory leaks
    niter = 1

    for i in range(niter):
        theSuite.addTest(unittest.makeSuite(TreeTestCase))
        theSuite.addTest(unittest.makeSuite(DeepTreeTestCase))
        theSuite.addTest(unittest.makeSuite(WideTreeTestCase))
        theSuite.addTest(unittest.makeSuite(HiddenTreeTestCase))
        theSuite.addTest(unittest.makeSuite(CreateParentsTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_types
# -*- coding: utf-8 -*-

from __future__ import print_function
import sys
import unittest
import os

import numpy

from tables import *
from tables.tests import common

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup

# Test Record class


class Record(IsDescription):
    var1 = StringCol(itemsize=4)  # 4-character String
    var2 = Col.from_kind('int')   # integer
    var3 = Col.from_kind('int', itemsize=2)  # short integer
    var4 = Col.from_kind('float')  # double (double-precision)
    var5 = Col.from_kind('float', itemsize=4)  # float  (single-precision)
    var6 = Col.from_kind('complex')  # double-precision
    var7 = Col.from_kind('complex', itemsize=8)  # single-precision
    if "Float16Atom" in globals():
        var8 = Col.from_kind('float', itemsize=2)  # half-precision
    if "Float96Atom" in globals():
        var9 = Col.from_kind('float', itemsize=12)  # extended-precision
    if "Float128Atom" in globals():
        var10 = Col.from_kind('float', itemsize=16)  # extended-precision
    if "Complex192Atom" in globals():
        var11 = Col.from_kind('complex', itemsize=24)  # extended-precision
    if "Complex256Atom" in globals():
        var12 = Col.from_kind('complex', itemsize=32)  # extended-precision


class RangeTestCase(unittest.TestCase):
    file = "test.h5"
    title = "This is the table title"
    expectedrows = 100
    maxshort = 2 ** 15
    maxint = 2147483648   # (2 ** 31)
    compress = 0

    def setUp(self):
        # Create an instance of HDF5 Table
        self.fileh = open_file(self.file, mode="w")
        self.rootgroup = self.fileh.root

        # Create a table
        self.table = self.fileh.create_table(self.rootgroup, 'table',
                                             Record, self.title)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_range(self):
        """Testing the range check."""
        rec = self.table.row
        # Save a record
        i = self.maxshort
        rec['var1'] = '%04d' % (i)
        rec['var2'] = i
        rec['var3'] = i
        rec['var4'] = float(i)
        rec['var5'] = float(i)
        rec['var6'] = float(i)
        rec['var7'] = complex(i, i)
        if "Float16Atom" in globals():
            rec['var8'] = float(i)
        if "Float96Atom" in globals():
            rec['var9'] = float(i)
        if "Float128Atom" in globals():
            rec['var10'] = float(i)
        try:
            rec.append()
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
            pass
        else:
            if common.verbose:
                print(
                    "\nNow, the range overflow no longer issues a ValueError")

    def test01_type(self):
        """Testing the type check."""
        rec = self.table.row
        # Save a record
        i = self.maxshort
        rec['var1'] = '%04d' % (i)
        rec['var2'] = i
        rec['var3'] = i % self.maxshort
        rec['var5'] = float(i)
        try:
            rec['var4'] = "124c"
        except TypeError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next TypeError was catched!")
                print(value)
            pass
        else:
            print(rec)
            self.fail("expected a TypeError")
        rec['var6'] = float(i)
        rec['var7'] = complex(i, i)
        if "Float16Atom" in globals():
            rec['var8'] = float(i)
        if "Float96Atom" in globals():
            rec['var9'] = float(i)
        if "Float128Atom" in globals():
            rec['var10'] = float(i)


# Check the dtype read-only attribute
class DtypeTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def test00a_table(self):
        """Check dtype accessor for Table objects."""
        a = self.h5file.create_table('/', 'table', Record)
        self.assertEqual(a.dtype, a.description._v_dtype)

    def test00b_column(self):
        """Check dtype accessor for Column objects."""
        a = self.h5file.create_table('/', 'table', Record)
        c = a.cols.var3
        self.assertEqual(c.dtype, a.description._v_dtype['var3'])

    def test01_array(self):
        """Check dtype accessor for Array objects."""
        a = self.h5file.create_array('/', 'array', [1, 2])
        self.assertEqual(a.dtype, a.atom.dtype)

    def test02_carray(self):
        """Check dtype accessor for CArray objects."""
        a = self.h5file.create_carray(
            '/', 'array', atom=FloatAtom(), shape=[1, 2])
        self.assertEqual(a.dtype, a.atom.dtype)

    def test03_carray(self):
        """Check dtype accessor for EArray objects."""
        a = self.h5file.create_earray(
            '/', 'array', atom=FloatAtom(), shape=[0, 2])
        self.assertEqual(a.dtype, a.atom.dtype)

    def test04_vlarray(self):
        """Check dtype accessor for VLArray objects."""
        a = self.h5file.create_vlarray('/', 'array', FloatAtom())
        self.assertEqual(a.dtype, a.atom.dtype)


class ReadFloatTestCase(common.PyTablesTestCase):
    filename = "float.h5"
    nrows = 5
    ncols = 6

    def setUp(self):
        self.fileh = open_file(self._testFilename(self.filename), mode="r")
        x = numpy.arange(self.ncols)
        y = numpy.arange(self.nrows)
        y.shape = (self.nrows, 1)
        self.values = x + y

    def tearDown(self):
        self.fileh.close()

    def test01_read_float16(self):
        dtype = "float16"
        if hasattr(numpy, dtype):
            ds = getattr(self.fileh.root, dtype)
            self.assertFalse(isinstance(ds, UnImplemented))
            self.assertEqual(ds.shape, (self.nrows, self.ncols))
            self.assertEqual(ds.dtype, dtype)
            self.assertTrue(common.allequal(
                ds.read(), self.values.astype(dtype)))
        else:
            ds = self.assertWarns(UserWarning, getattr, self.fileh.root, dtype)
            self.assertTrue(isinstance(ds, UnImplemented))

    def test02_read_float32(self):
        dtype = "float32"
        ds = getattr(self.fileh.root, dtype)
        self.assertFalse(isinstance(ds, UnImplemented))
        self.assertEqual(ds.shape, (self.nrows, self.ncols))
        self.assertEqual(ds.dtype, dtype)
        self.assertTrue(common.allequal(
            ds.read(), self.values.astype(dtype)))

    def test03_read_float64(self):
        dtype = "float64"
        ds = getattr(self.fileh.root, dtype)
        self.assertFalse(isinstance(ds, UnImplemented))
        self.assertEqual(ds.shape, (self.nrows, self.ncols))
        self.assertEqual(ds.dtype, dtype)
        self.assertTrue(common.allequal(
            ds.read(), self.values.astype(dtype)))

    def test04_read_longdouble(self):
        dtype = "longdouble"
        if "Float96Atom" in globals() or "Float128Atom" in globals():
            ds = getattr(self.fileh.root, dtype)
            self.assertFalse(isinstance(ds, UnImplemented))
            self.assertEqual(ds.shape, (self.nrows, self.ncols))
            self.assertEqual(ds.dtype, dtype)
            self.assertTrue(common.allequal(
                ds.read(), self.values.astype(dtype)))

            if "Float96Atom" in globals():
                self.assertEqual(ds.dtype, "float96")
            elif "Float128Atom" in globals():
                self.assertEqual(ds.dtype, "float128")
        else:
            # XXX: check
            # the behavior depends on the HDF5 lib configuration
            try:
                ds = self.assertWarns(UserWarning,
                                      getattr, self.fileh.root, dtype)
                self.assertTrue(isinstance(ds, UnImplemented))
            except AssertionError:
                from tables.utilsextension import _broken_hdf5_long_double
                if not _broken_hdf5_long_double():
                    ds = getattr(self.fileh.root, dtype)
                    self.assertEqual(ds.dtype, "float64")

    def test05_read_quadprecision_float(self):
        # XXX: check
        try:
            ds = self.assertWarns(UserWarning, getattr, self.fileh.root,
                                  "quadprecision")
            self.assertTrue(isinstance(ds, UnImplemented))
        except AssertionError:
            # NOTE: it would be nice to have some sort of message that warns
            #       against the potential precision loss: the quad-precision
            #       dataset actually uses 128 bits for each element, not just
            #       80 bits (longdouble)
            ds = self.fileh.root.quadprecision
            self.assertEqual(ds.dtype, "longdouble")


class AtomTestCase(common.PyTablesTestCase):
    def test_init_parameters_01(self):
        atom1 = StringAtom(itemsize=12)
        atom2 = atom1.copy()
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))
        self.assertFalse(atom1 is atom2)

    def test_init_parameters_02(self):
        atom1 = StringAtom(itemsize=12)
        atom2 = atom1.copy(itemsize=100, shape=(2, 2))
        self.assertEqual(atom2,
                         StringAtom(itemsize=100, shape=(2, 2), dflt=b''))

    def test_init_parameters_03(self):
        atom1 = StringAtom(itemsize=12)
        self.assertRaises(TypeError, atom1.copy, foobar=42)

    def test_from_dtype_01(self):
        atom1 = Atom.from_dtype(numpy.dtype((numpy.int16, (2, 2))))
        atom2 = Int16Atom(shape=(2, 2), dflt=0)
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_dtype_02(self):
        atom1 = Atom.from_dtype(numpy.dtype('S5'), dflt=b'hello')
        atom2 = StringAtom(itemsize=5, shape=(), dflt=b'hello')
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_dtype_03(self):
        atom1 = Atom.from_dtype(numpy.dtype('Float64'))
        atom2 = Float64Atom(shape=(), dflt=0.0)
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_kind_01(self):
        atom1 = Atom.from_kind('int', itemsize=2, shape=(2, 2))
        atom2 = Int16Atom(shape=(2, 2), dflt=0)
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_kind_02(self):
        atom1 = Atom.from_kind('int', shape=(2, 2))
        atom2 = Int32Atom(shape=(2, 2), dflt=0)
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_kind_03(self):
        atom1 = Atom.from_kind('int', shape=1)
        atom2 = Int32Atom(shape=(1,), dflt=0)
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_kind_04(self):
        atom1 = Atom.from_kind('string', itemsize=5, dflt=b'hello')
        atom2 = StringAtom(itemsize=5, shape=(), dflt=b'hello')
        self.assertEqual(atom1, atom2)
        self.assertEqual(str(atom1), str(atom2))

    def test_from_kind_05(self):
        # ValueError: no default item size for kind ``string``
        self.assertRaises(ValueError, Atom.from_kind, 'string', dflt=b'hello')

    def test_from_kind_06(self):
        # ValueError: unknown kind: 'Float'
        self.assertRaises(ValueError, Atom.from_kind, 'Float')


#----------------------------------------------------------------------

def suite():
    import doctest
    import tables.atom

    theSuite = unittest.TestSuite()

    for i in range(1):
        theSuite.addTest(doctest.DocTestSuite(tables.atom))
        theSuite.addTest(unittest.makeSuite(AtomTestCase))
        theSuite.addTest(unittest.makeSuite(RangeTestCase))
        theSuite.addTest(unittest.makeSuite(DtypeTestCase))
        theSuite.addTest(unittest.makeSuite(ReadFloatTestCase))

    return theSuite


if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = test_vlarray
# -*- coding: latin-1 -*-

from __future__ import print_function
import sys
import unittest
import os
import tempfile
#import cPickle

import numpy
import numpy.testing as npt

import tables
from tables import *
from tables.tests import common
from tables.tests.common import allequal
from tables.utils import byteorders

# To delete the internal attributes automagically
unittest.TestCase.tearDown = common.cleanup


class C:
    c = (3, 4.5)


class BasicTestCase(unittest.TestCase):
    compress = 0
    complib = "zlib"
    shuffle = 0
    fletcher32 = 0
    flavor = "numpy"

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        filters = Filters(complevel=self.compress,
                          complib=self.complib,
                          shuffle=self.shuffle,
                          fletcher32=self.fletcher32)
        vlarray = self.fileh.create_vlarray(group, 'vlarray1',
                                            atom=Int32Atom(),
                                            title="ragged array if ints",
                                            filters=filters,
                                            expectedrows=1000)
        vlarray.flavor = self.flavor

        # Fill it with 5 rows
        vlarray.append([1, 2])
        if self.flavor == "numpy":
            vlarray.append(numpy.array([3, 4, 5], dtype='int32'))
            vlarray.append(numpy.array([], dtype='int32'))     # Empty entry
        elif self.flavor == "python":
            vlarray.append((3, 4, 5))
            vlarray.append(())         # Empty entry
        vlarray.append([6, 7, 8, 9])
        vlarray.append([10, 11, 12, 13, 14])

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_attributes(self):
        self.fileh = open_file(self.file, "r")
        obj = self.fileh.get_node("/vlarray1")

        self.assertEqual(obj.flavor, self.flavor)
        self.assertEqual(obj.shape, (5,))
        self.assertEqual(obj.ndim, 1)
        self.assertEqual(obj.nrows, 5)
        self.assertEqual(obj.atom.type, 'int32')

    def test01_read(self):
        """Checking vlarray read."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_read..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.get_node("/vlarray1")

        # Choose a small value for buffer size
        vlarray.nrowsinbuf = 3
        # Read some rows
        row = vlarray.read(0)[0]
        row2 = vlarray.read(2)[0]
        if common.verbose:
            print("Flavor:", vlarray.flavor)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row)

        nrows = 5
        self.assertEqual(nrows, vlarray.nrows)
        if self.flavor == "numpy":
            self.assertEqual(type(row), numpy.ndarray)
            self.assertTrue(
                allequal(row, numpy.array([1, 2], dtype='int32'), self.flavor))
            self.assertTrue(
                allequal(row2, numpy.array([], dtype='int32'), self.flavor))
        elif self.flavor == "python":
            self.assertEqual(row, [1, 2])
            self.assertEqual(row2, [])
        self.assertEqual(len(row), 2)

        # Check filters:
        if self.compress != vlarray.filters.complevel and common.verbose:
            print("Error in compress. Class:", self.__class__.__name__)
            print("self, vlarray:", self.compress, vlarray.filters.complevel)
        self.assertEqual(vlarray.filters.complevel, self.compress)
        if self.compress > 0 and which_lib_version(self.complib):
            self.assertEqual(vlarray.filters.complib, self.complib)
        if self.shuffle != vlarray.filters.shuffle and common.verbose:
            print("Error in shuffle. Class:", self.__class__.__name__)
            print("self, vlarray:", self.shuffle, vlarray.filters.shuffle)
        self.assertEqual(self.shuffle, vlarray.filters.shuffle)
        if self.fletcher32 != vlarray.filters.fletcher32 and common.verbose:
            print("Error in fletcher32. Class:", self.__class__.__name__)
            print("self, vlarray:", self.fletcher32,
                  vlarray.filters.fletcher32)
        self.assertEqual(self.fletcher32, vlarray.filters.fletcher32)

    def test02a_getitem(self):
        """Checking vlarray __getitem__ (slices)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02a_getitem..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.get_node("/vlarray1")

        rows = [[1, 2], [3, 4, 5], [], [6, 7, 8, 9], [10, 11, 12, 13, 14]]

        slices = [
            slice(None, None, None), slice(1, 1, 1), slice(30, None, None),
            slice(0, None, None), slice(3, None, 1), slice(3, None, 2),
            slice(None, 1, None), slice(None, 2, 1), slice(None, 30, 2),
            slice(None, None, 1), slice(None, None, 2), slice(None, None, 3),
        ]
        for slc in slices:
            # Read the rows in slc
            rows2 = vlarray[slc]
            rows1 = rows[slc]
            rows1f = []
            if common.verbose:
                print("Flavor:", vlarray.flavor)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("Original rows ==>", rows1)
                print("Rows read in vlarray ==>", rows2)

            if self.flavor == "numpy":
                for val in rows1:
                    rows1f.append(numpy.array(val, dtype='int32'))
                for i in range(len(rows1f)):
                    self.assertTrue(allequal(rows2[i], rows1f[i], self.flavor))
            elif self.flavor == "python":
                    self.assertEqual(rows2, rows1)

    def test02b_getitem(self):
        """Checking vlarray __getitem__ (scalars)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_getitem..." % self.__class__.__name__)

        if self.flavor != "numpy":
            # This test is only valid for NumPy
            return

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.get_node("/vlarray1")

        # Get a numpy array of objects
        rows = numpy.array(vlarray[:], dtype=numpy.object)

        for slc in [0, numpy.array(1), 2, numpy.array([3]), [4]]:
            # Read the rows in slc
            rows2 = vlarray[slc]
            rows1 = rows[slc]
            if common.verbose:
                print("Flavor:", vlarray.flavor)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("Original rows ==>", rows1)
                print("Rows read in vlarray ==>", rows2)

            for i in range(len(rows1)):
                self.assertTrue(allequal(rows2[i], rows1[i], self.flavor))

    def test03_append(self):
        """Checking vlarray append."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_append..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.get_node("/vlarray1")
        # Append a new row
        vlarray.append([7, 8, 9, 10])

        # Choose a small value for buffer size
        vlarray.nrowsinbuf = 3
        # Read some rows:
        row1 = vlarray[0]
        row2 = vlarray[2]
        row3 = vlarray[-1]
        if common.verbose:
            print("Flavor:", vlarray.flavor)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row1)

        nrows = 6
        self.assertEqual(nrows, vlarray.nrows)
        if self.flavor == "numpy":
            self.assertEqual(type(row1), type(numpy.array([1, 2])))
            self.assertTrue(
                allequal(row1, numpy.array([1, 2], dtype='int32'),
                         self.flavor))
            self.assertTrue(
                allequal(row2, numpy.array([], dtype='int32'), self.flavor))
            self.assertTrue(
                allequal(row3, numpy.array([7, 8, 9, 10], dtype='int32'),
                         self.flavor))
        elif self.flavor == "python":
            self.assertEqual(row1, [1, 2])
            self.assertEqual(row2, [])
            self.assertEqual(row3, [7, 8, 9, 10])
        self.assertEqual(len(row3), 4)

    def test04_get_row_size(self):
        """Checking get_row_size method."""

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.get_node("/vlarray1")

        self.assertEqual(vlarray.get_row_size(0), 2 * vlarray.atom.size)
        self.assertEqual(vlarray.get_row_size(1), 3 * vlarray.atom.size)
        self.assertEqual(vlarray.get_row_size(2), 0 * vlarray.atom.size)
        self.assertEqual(vlarray.get_row_size(3), 4 * vlarray.atom.size)
        self.assertEqual(vlarray.get_row_size(4), 5 * vlarray.atom.size)


class BasicNumPyTestCase(BasicTestCase):
    flavor = "numpy"


class BasicPythonTestCase(BasicTestCase):
    flavor = "python"


class ZlibComprTestCase(BasicTestCase):
    compress = 1
    complib = "zlib"


class BloscComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 0
    complib = "blosc"


class BloscShuffleComprTestCase(BasicTestCase):
    compress = 6
    shuffle = 1
    complib = "blosc"


class BloscBloscLZComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 1
    complib = "blosc:blosclz"


class BloscLZ4ComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 1
    complib = "blosc:lz4"


class BloscLZ4HCComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 1
    complib = "blosc:lz4hc"


class BloscSnappyComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 1
    complib = "blosc:snappy"


class BloscZlibComprTestCase(BasicTestCase):
    compress = 9
    shuffle = 1
    complib = "blosc:zlib"


class LZOComprTestCase(BasicTestCase):
    compress = 1
    complib = "lzo"


class Bzip2ComprTestCase(BasicTestCase):
    compress = 1
    complib = "bzip2"


class ShuffleComprTestCase(BasicTestCase):
    compress = 1
    shuffle = 1


class Fletcher32TestCase(BasicTestCase):
    fletcher32 = 1


class AllFiltersTestCase(BasicTestCase):
    compress = 1
    shuffle = 1
    fletcher32 = 1


class TypesTestCase(unittest.TestCase):
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01_StringAtom(self):
        """Checking vlarray with NumPy string atoms ('numpy' flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_StringAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "numpy"
        vlarray.append(numpy.array(["1", "12", "123", "1234", "12345"]))
        vlarray.append(numpy.array(["1", "12345"]))

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        npt.assert_array_equal(
            row[0], numpy.array(["1", "12", "123", "123", "123"], 'S'))
        npt.assert_array_equal(row[1], numpy.array(["1", "123"], 'S'))
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 2)

    def test01a_StringAtom(self):
        """Checking vlarray with NumPy string atoms ('numpy' flavor,
        strided)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_StringAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "numpy"
        vlarray.append(numpy.array(["1", "12", "123", "1234", "12345"][::2]))
        vlarray.append(numpy.array(["1", "12345", "2", "321"])[::3])

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        npt.assert_array_equal(row[0], numpy.array(["1", "123", "123"], 'S'))
        npt.assert_array_equal(row[1], numpy.array(["1", "321"], 'S'))
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 2)

    def test01a_2_StringAtom(self):
        """Checking vlarray with NumPy string atoms (NumPy flavor, no conv)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_2_StringAtom..." %
                  self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "numpy"
        vlarray.append(numpy.array(["1", "12", "123", "123"]))
        vlarray.append(numpy.array(["1", "2", "321"]))

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        npt.assert_array_equal(
            row[0], numpy.array(["1", "12", "123", "123"], 'S'))
        npt.assert_array_equal(row[1], numpy.array(["1", "2", "321"], 'S'))
        self.assertEqual(len(row[0]), 4)
        self.assertEqual(len(row[1]), 3)

    def test01b_StringAtom(self):
        """Checking vlarray with NumPy string atoms (python flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_StringAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom2',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "python"
        vlarray.append(["1", "12", "123", "1234", "12345"])
        vlarray.append(["1", "12345"])

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Testing String flavor")
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], [b"1", b"12", b"123", b"123", b"123"])
        self.assertEqual(row[1], [b"1", b"123"])
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 2)

    def test01c_StringAtom(self):
        """Checking updating vlarray with NumPy string atoms.

        ('numpy' flavor)

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_StringAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "numpy"
        vlarray.append(numpy.array(["1", "12", "123", "1234", "12345"]))
        vlarray.append(numpy.array(["1", "12345"]))

        # Modify the rows
        vlarray[0] = numpy.array(["1", "123", "12", "", "12345"])
        vlarray[1] = numpy.array(["44", "4"])  # This should work as well

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(
            allequal(row[0], numpy.array([b"1", b"123", b"12", b"", b"123"])))
        self.assertTrue(allequal(row[1], numpy.array(["44", "4"], dtype="S3")))
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 2)

    def test01d_StringAtom(self):
        """Checking updating vlarray with string atoms (String flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01d_StringAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'stringAtom2',
                                            atom=StringAtom(itemsize=3),
                                            title="Ragged array of strings")
        vlarray.flavor = "python"
        vlarray.append(["1", "12", "123", "1234", "12345"])
        vlarray.append(["1", "12345"])

        # Modify the rows
        vlarray[0] = ["1", "123", "12", "", "12345"]
        vlarray[1] = ["44", "4"]

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Testing String flavor")
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], [b"1", b"123", b"12", b"", b"123"])
        self.assertEqual(row[1], [b"44", b"4"])
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 2)

    def test02_BoolAtom(self):
        """Checking vlarray with boolean atoms."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_BoolAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'BoolAtom',
                                            atom=BoolAtom(),
                                            title="Ragged array of Booleans")
        vlarray.append([1, 0, 3])
        vlarray.append([-1, 0])

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(allequal(row[0], numpy.array([1, 0, 1], dtype='bool')))
        self.assertTrue(allequal(row[1], numpy.array([1, 0], dtype='bool')))
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 2)

    def test02b_BoolAtom(self):
        """Checking setting vlarray with boolean atoms."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_BoolAtom..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', 'BoolAtom',
                                            atom=BoolAtom(),
                                            title="Ragged array of Booleans")
        vlarray.append([1, 0, 3])
        vlarray.append([-1, 0])

        # Modify the rows
        vlarray[0] = (0, 1, 3)
        vlarray[1] = (0, -1)

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(allequal(row[0], numpy.array([0, 1, 1], dtype='bool')))
        self.assertTrue(allequal(row[1], numpy.array([0, 1], dtype='bool')))
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 2)

    def test03_IntAtom(self):
        """Checking vlarray with integer atoms."""

        ttypes = [
            "Int8",
            "UInt8",
            "Int16",
            "UInt16",
            "Int32",
            "UInt32",
            "Int64",
            #"UInt64",  # Unavailable in some platforms
        ]
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_IntAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray('/', atype,
                                                atom=Atom.from_sctype(atype))
            vlarray.append([1, 2, 3])
            vlarray.append([-1, 0])

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[
                            0], numpy.array([1, 2, 3], dtype=atype)))
            self.assertTrue(allequal(row[
                            1], numpy.array([-1, 0], dtype=atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test03a_IntAtom(self):
        """Checking vlarray with integer atoms (byteorder swapped)"""

        ttypes = {
            "Int8": numpy.int8,
            "UInt8": numpy.uint8,
            "Int16": numpy.int16,
            "UInt16": numpy.uint16,
            "Int32": numpy.int32,
            "UInt32": numpy.uint32,
            "Int64": numpy.int64,
            #"UInt64": numpy.int64,  # Unavailable in some platforms
        }
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03a_IntAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]))
            a0 = numpy.array([1, 2, 3], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray.append(a0)
            a1 = numpy.array([-1, 0], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray.append(a1)

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0], numpy.array([1, 2, 3], dtype=ttypes[atype])))
            self.assertTrue(
                allequal(row[1], numpy.array([-1, 0], dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test03b_IntAtom(self):
        """Checking updating vlarray with integer atoms."""

        ttypes = [
            "Int8",
            "UInt8",
            "Int16",
            "UInt16",
            "Int32",
            "UInt32",
            "Int64",
            #"UInt64",  # Unavailable in some platforms
        ]
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_IntAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(atype))
            vlarray.append([1, 2, 3])
            vlarray.append([-1, 0])

            # Modify rows
            vlarray[0] = (3, 2, 1)
            vlarray[1] = (0, -1)

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[
                            0], numpy.array([3, 2, 1], dtype=atype)))
            self.assertTrue(allequal(row[
                            1], numpy.array([0, -1], dtype=atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test03c_IntAtom(self):
        """Checking updating vlarray with integer atoms (byteorder swapped)"""

        ttypes = {
            "Int8": numpy.int8,
            "UInt8": numpy.uint8,
            "Int16": numpy.int16,
            "UInt16": numpy.uint16,
            "Int32": numpy.int32,
            "UInt32": numpy.uint32,
            "Int64": numpy.int64,
            #"UInt64": numpy.int64,  # Unavailable in some platforms
        }
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03c_IntAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]))
            a0 = numpy.array([1, 2, 3], dtype=atype)
            vlarray.append(a0)
            a1 = numpy.array([-1, 0], dtype=atype)
            vlarray.append(a1)

            # Modify rows
            a0 = numpy.array([3, 2, 1], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray[0] = a0
            a1 = numpy.array([0, -1], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray[1] = a1

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0], numpy.array([3, 2, 1], dtype=ttypes[atype])))
            self.assertTrue(
                allequal(row[1], numpy.array([0, -1], dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test03d_IntAtom(self):
        """Checking updating vlarray with integer atoms (another byteorder)"""

        ttypes = {
            "Int8": numpy.int8,
            "UInt8": numpy.uint8,
            "Int16": numpy.int16,
            "UInt16": numpy.uint16,
            "Int32": numpy.int32,
            "UInt32": numpy.uint32,
            "Int64": numpy.int64,
            #"UInt64": numpy.int64,  # Unavailable in some platforms
        }
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03d_IntAtom..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]),
                byteorder=byteorder)
            a0 = numpy.array([1, 2, 3], dtype=atype)
            vlarray.append(a0)
            a1 = numpy.array([-1, 0], dtype=atype)
            vlarray.append(a1)

            # Modify rows
            a0 = numpy.array([3, 2, 1], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray[0] = a0
            a1 = numpy.array([0, -1], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray[1] = a1

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            byteorder2 = byteorders[row[0].dtype.byteorder]
            if byteorder2 != "irrelevant":
                self.assertEqual(byteorders[row[0].dtype.byteorder],
                                 sys.byteorder)
                self.assertEqual(vlarray.byteorder, byteorder)
            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0], numpy.array([3, 2, 1], dtype=ttypes[atype])))
            self.assertTrue(
                allequal(row[1], numpy.array([0, -1], dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04_FloatAtom(self):
        """Checking vlarray with floating point atoms."""

        ttypes = ["Float32",
                  "Float64",
                  ]
        for name in ("float16", "float96", "float128"):
            atomname = name.capitalize() + 'Atom'
            if atomname in globals():
                ttypes.append(name)

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_FloatAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(atype))
            vlarray.append([1.3, 2.2, 3.3])
            vlarray.append([-1.3e34, 1.e-32])

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[
                            0], numpy.array([1.3, 2.2, 3.3], atype)))
            self.assertTrue(allequal(row[
                            1], numpy.array([-1.3e34, 1.e-32], atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04a_FloatAtom(self):
        """Checking vlarray with float atoms (byteorder swapped)"""

        ttypes = {
            "Float32": numpy.float32,
            "Float64": numpy.float64,
        }
        if "Float16Atom" in globals():
            ttypes["float16"] = numpy.float16
        if "Float96Atom" in globals():
            ttypes["float96"] = numpy.float96
        if "Float128Atom" in globals():
            ttypes["float128"] = numpy.float128

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04a_FloatAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]))
            a0 = numpy.array([1.3, 2.2, 3.3], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray.append(a0)
            a1 = numpy.array([-1.3e34, 1.e-32], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray.append(a1)

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[0], numpy.array([1.3, 2.2, 3.3],
                                                         dtype=ttypes[atype])))
            self.assertTrue(allequal(row[1], numpy.array([-1.3e34, 1.e-32],
                                                         dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04b_FloatAtom(self):
        """Checking updating vlarray with floating point atoms."""

        ttypes = [
            "Float32",
            "Float64",
        ]
        for name in ("float16", "float96", "float128"):
            atomname = name.capitalize() + 'Atom'
            if atomname in globals():
                ttypes.append(name)

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b_FloatAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(atype))
            vlarray.append([1.3, 2.2, 3.3])
            vlarray.append([-1.3e34, 1.e-32])

            # Modifiy some rows
            vlarray[0] = (4.3, 2.2, 4.3)
            vlarray[1] = (-1.1e34, 1.3e-32)

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[
                            0], numpy.array([4.3, 2.2, 4.3], atype)))
            self.assertTrue(
                allequal(row[1], numpy.array([-1.1e34, 1.3e-32], atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04c_FloatAtom(self):
        """Checking updating vlarray with float atoms (byteorder swapped)"""

        ttypes = {
            "Float32": numpy.float32,
            "Float64": numpy.float64,
        }
        if "Float16Atom" in globals():
            ttypes["float16"] = numpy.float16
        if "Float96Atom" in globals():
            ttypes["float96"] = numpy.float96
        if "Float128Atom" in globals():
            ttypes["float128"] = numpy.float128

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04c_FloatAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]))
            a0 = numpy.array([1.3, 2.2, 3.3], dtype=atype)
            vlarray.append(a0)
            a1 = numpy.array([-1, 0], dtype=atype)
            vlarray.append(a1)

            # Modify rows
            a0 = numpy.array([4.3, 2.2, 4.3], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray[0] = a0
            a1 = numpy.array([-1.1e34, 1.3e-32], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray[1] = a1

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[0], numpy.array([4.3, 2.2, 4.3],
                                                         dtype=ttypes[atype])))
            self.assertTrue(allequal(row[1], numpy.array([-1.1e34, 1.3e-32],
                                                         dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04d_FloatAtom(self):
        """Checking updating vlarray with float atoms (another byteorder)"""

        ttypes = {
            "Float32": numpy.float32,
            "Float64": numpy.float64,
        }
        if "Float16Atom" in globals():
            ttypes["float16"] = numpy.float16
        if "Float96Atom" in globals():
            ttypes["float96"] = numpy.float96
        if "Float128Atom" in globals():
            ttypes["float128"] = numpy.float128

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04d_FloatAtom..." % self.__class__.__name__)

        byteorder = {'little': 'big', 'big': 'little'}[sys.byteorder]
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(ttypes[atype]),
                byteorder=byteorder)
            a0 = numpy.array([1.3, 2.2, 3.3], dtype=atype)
            vlarray.append(a0)
            a1 = numpy.array([-1, 0], dtype=atype)
            vlarray.append(a1)

            # Modify rows
            a0 = numpy.array([4.3, 2.2, 4.3], dtype=atype)
            a0 = a0.byteswap()
            a0 = a0.newbyteorder()
            vlarray[0] = a0
            a1 = numpy.array([-1.1e34, 1.3e-32], dtype=atype)
            a1 = a1.byteswap()
            a1 = a1.newbyteorder()
            vlarray[1] = a1

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.byteorder, byteorder)
            self.assertTrue(byteorders[row[0].dtype.byteorder], sys.byteorder)
            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(allequal(row[0], numpy.array([4.3, 2.2, 4.3],
                                                         dtype=ttypes[atype])))
            self.assertTrue(allequal(row[1], numpy.array([-1.1e34, 1.3e-32],
                                                         dtype=ttypes[atype])))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04_ComplexAtom(self):
        """Checking vlarray with numerical complex atoms."""

        ttypes = [
            "Complex32",
            "Complex64",
        ]

        if "Complex192Atom" in globals():
            ttypes.append("Complex96")
        if "Complex256Atom" in globals():
            ttypes.append("Complex128")

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_ComplexAtom..." % self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(atype))
            vlarray.append([(1.3 + 0j), (0+2.2j), (3.3+3.3j)])
            vlarray.append([(0-1.3e34j), (1.e-32 + 0j)])

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0],
                         numpy.array([(1.3 + 0j), (0+2.2j), (3.3+3.3j)],
                                     atype)))
            self.assertTrue(
                allequal(row[1],
                         numpy.array([(0-1.3e34j), (1.e-32 + 0j)], atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test04b_ComplexAtom(self):
        """Checking modifying vlarray with numerical complex atoms."""

        ttypes = [
            "Complex32",
            "Complex64",
        ]

        if "Complex192Atom" in globals():
            ttypes.append("Complex96")
        if "Complex256Atom" in globals():
            ttypes.append("Complex128")

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b_ComplexAtom..." %
                  self.__class__.__name__)

        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                '/', atype, atom=Atom.from_sctype(atype))
            vlarray.append([(1.3 + 0j), (0+2.2j), (3.3+3.3j)])
            vlarray.append([(0-1.3e34j), (1.e-32 + 0j)])

            # Modify the rows
            vlarray[0] = ((1.4 + 0j), (0+4.2j), (3.3+4.3j))
            vlarray[1] = ((4-1.3e34j), (1.e-32 + 4j))

            if self.reopen:
                name = vlarray._v_pathname
                self.fileh.close()
                self.fileh = open_file(self.file, "a")
                vlarray = self.fileh.get_node(name)

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0],
                         numpy.array([(1.4 + 0j), (0+4.2j), (3.3+4.3j)],
                                     atype)))
            self.assertTrue(
                allequal(row[1],
                         numpy.array([(4-1.3e34j), (1.e-32 + 4j)], atype)))
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 2)

    def test05_VLStringAtom(self):
        """Checking vlarray with variable length strings."""

        # Skip the test if the default encoding has been mangled.
        if sys.getdefaultencoding() != 'ascii':
            return

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_VLStringAtom..." %
                  self.__class__.__name__)

        vlarray = self.fileh.create_vlarray(
            '/', "VLStringAtom", atom=VLStringAtom())
        vlarray.append("asd")
        vlarray.append("asd\xe4")
        vlarray.append(u"aaana")
        vlarray.append("")
        # Check for ticket #62.
        self.assertRaises(TypeError, vlarray.append, ["foo", "bar"])
        # `VLStringAtom` makes no encoding assumptions.  See ticket #51.
        self.assertRaises(UnicodeEncodeError, vlarray.append, u"asd\xe4")

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 4)
        self.assertEqual(row[0], "asd")
        self.assertEqual(row[1], "asd\xe4")
        self.assertEqual(row[2], "aaana")
        self.assertEqual(row[3], "")
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        self.assertEqual(len(row[3]), 0)

    def test05b_VLStringAtom(self):
        """Checking updating vlarray with variable length strings."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_VLStringAtom..." %
                  self.__class__.__name__)

        vlarray = self.fileh.create_vlarray(
            '/', "VLStringAtom", atom=VLStringAtom())
        vlarray.append("asd")
        vlarray.append(u"aaana")

        # Modify values
        vlarray[0] = "as4"
        vlarray[1] = "aaanc"
        self.assertRaises(ValueError, vlarray.__setitem__, 1, "shrt")
        self.assertRaises(ValueError, vlarray.__setitem__, 1, "toolong")

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", repr(row[0]))
            print("Second row in vlarray ==>", repr(row[1]))

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], b"as4")
        self.assertEqual(row[1], b"aaanc")
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 5)

    def test06a_Object(self):
        """Checking vlarray with object atoms."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06a_Object..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray(
            '/', "Object", atom=ObjectAtom())
        vlarray.append([[1, 2, 3], "aaa", u"aaa"])
        vlarray.append([3, 4, C()])
        vlarray.append(42)

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 3)
        self.assertEqual(row[0], [[1, 2, 3], "aaa", u"aaa"])
        list1 = list(row[1])
        obj = list1.pop()
        self.assertEqual(list1, [3, 4])
        self.assertEqual(obj.c, C().c)
        self.assertEqual(row[2], 42)
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 3)
        self.assertRaises(TypeError, len, row[2])

    def test06b_Object(self):
        """Checking updating vlarray with object atoms."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06b_Object..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', "Object", atom=ObjectAtom())
        # When updating an object, this seems to change the number
        # of bytes that pickle.dumps generates
        # vlarray.append(([1,2,3], "aaa", u"aaa"))
        vlarray.append(([1, 2, 3], "aaa", u"4"))
        # vlarray.append([3,4, C()])
        vlarray.append([3, 4, [24]])

        # Modify the rows
        # vlarray[0] = ([1,2,4], "aa4", u"aaa4")
        vlarray[0] = ([1, 2, 4], "aa4", u"5")
        # vlarray[1] = (3,4, C())
        vlarray[1] = [4, 4, [24]]

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], ([1, 2, 4], "aa4", u"5"))
        list1 = list(row[1])
        obj = list1.pop()
        self.assertEqual(list1, [4, 4])
        # self.assertEqual(obj.c, C().c)
        self.assertEqual(obj, [24])
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 3)

    def test06c_Object(self):
        """Checking vlarray with object atoms (numpy arrays as values)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06c_Object..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', "Object", atom=ObjectAtom())
        vlarray.append(numpy.array([[1, 2], [0, 4]], 'i4'))
        vlarray.append(numpy.array([0, 1, 2, 3], 'i8'))
        vlarray.append(numpy.array(42, 'i1'))

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 3)
        self.assertTrue(allequal(row[0], numpy.array([[1, 2], [0, 4]], 'i4')))
        self.assertTrue(allequal(row[1], numpy.array([0, 1, 2, 3], 'i8')))
        self.assertTrue(allequal(row[2], numpy.array(42, 'i1')))

    def test06d_Object(self):
        """Checking updating vlarray with object atoms (numpy arrays)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test06d_Object..." % self.__class__.__name__)

        vlarray = self.fileh.create_vlarray('/', "Object", atom=ObjectAtom())
        vlarray.append(numpy.array([[1, 2], [0, 4]], 'i4'))
        vlarray.append(numpy.array([0, 1, 2, 3], 'i8'))
        vlarray.append(numpy.array(42, 'i1'))

        # Modify the rows.  Since PyTables 2.2.1 we use a binary
        # pickle for arrays and ObjectAtoms, so the next should take
        # the same space than the above.
        vlarray[0] = numpy.array([[1, 0], [0, 4]], 'i4')
        vlarray[1] = numpy.array([0, 1, 0, 3], 'i8')
        vlarray[2] = numpy.array(22, 'i1')

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 3)
        self.assertTrue(allequal(row[0], numpy.array([[1, 0], [0, 4]], 'i4')))
        self.assertTrue(allequal(row[1], numpy.array([0, 1, 0, 3], 'i8')))
        self.assertTrue(allequal(row[2], numpy.array(22, 'i1')))

    def test07_VLUnicodeAtom(self):
        """Checking vlarray with variable length Unicode strings."""

        # Skip the test if the default encoding has been mangled.
        if sys.getdefaultencoding() != 'ascii':
            return

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07_VLUnicodeAtom..." %
                  self.__class__.__name__)

        vlarray = self.fileh.create_vlarray(
            '/', "VLUnicodeAtom", atom=VLUnicodeAtom())
        vlarray.append("asd")
        vlarray.append(u"asd\u0140")
        vlarray.append(u"aaana")
        vlarray.append(u"")
        # Check for ticket #62.
        self.assertRaises(TypeError, vlarray.append, ["foo", "bar"])
        # `VLUnicodeAtom` makes no encoding assumptions.
        self.assertRaises(UnicodeDecodeError, vlarray.append, "asd\xe4")

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 4)
        self.assertEqual(row[0], u"asd")
        self.assertEqual(row[1], u"asd\u0140")
        self.assertEqual(row[2], u"aaana")
        self.assertEqual(row[3], u"")
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        self.assertEqual(len(row[3]), 0)

    def test07b_VLUnicodeAtom(self):
        """Checking updating vlarray with variable length Unicode strings."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test07b_VLUnicodeAtom..." %
                  self.__class__.__name__)

        vlarray = self.fileh.create_vlarray(
            '/', "VLUnicodeAtom", atom=VLUnicodeAtom())
        vlarray.append("asd")
        vlarray.append(u"aaan\xe4")

        # Modify values
        vlarray[0] = u"as\xe4"
        vlarray[1] = u"aaan\u0140"
        self.assertRaises(ValueError, vlarray.__setitem__, 1, "shrt")
        self.assertRaises(ValueError, vlarray.__setitem__, 1, "toolong")

        if self.reopen:
            name = vlarray._v_pathname
            self.fileh.close()
            self.fileh = open_file(self.file, "r")
            vlarray = self.fileh.get_node(name)

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", repr(row[0]))
            print("Second row in vlarray ==>", repr(row[1]))

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], u"as\xe4")
        self.assertEqual(row[1], u"aaan\u0140")
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 5)


class TypesReopenTestCase(TypesTestCase):
    title = "Reopen"
    reopen = True


class TypesNoReopenTestCase(TypesTestCase):
    title = "No reopen"
    reopen = False


class MDTypesTestCase(unittest.TestCase):
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01_StringAtom(self):
        """Checking vlarray with MD NumPy string atoms."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_StringAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'stringAtom',
                                            StringAtom(itemsize=3, shape=(2,)),
                                            "Ragged array of strings")
        vlarray.append([["123", "45"], ["45", "123"]])
        vlarray.append([["s", "abc"], ["abc", "f"],
                        ["s", "ab"], ["ab", "f"]])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        npt.assert_array_equal(
            row[0], numpy.array([["123", "45"], ["45", "123"]], 'S'))
        npt.assert_array_equal(
            row[1], numpy.array([["s", "abc"], ["abc", "f"],
                                 ["s", "ab"], ["ab", "f"]], 'S'))
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 4)

    def test01b_StringAtom(self):
        """Checking vlarray with MD NumPy string atoms ('python' flavor)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_StringAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'stringAtom',
                                            StringAtom(itemsize=3, shape=(2,)),
                                            "Ragged array of strings")
        vlarray.flavor = "python"
        vlarray.append([["123", "45"], ["45", "123"]])
        vlarray.append([["s", "abc"], ["abc", "f"],
                        ["s", "ab"], ["ab", "f"]])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], [[b"123", b"45"], [b"45", b"123"]])
        self.assertEqual(row[1], [[b"s", b"abc"], [b"abc", b"f"],
                                  [b"s", b"ab"], [b"ab", b"f"]])
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 4)

    def test01c_StringAtom(self):
        """Checking vlarray with MD NumPy string atoms (with offset)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01c_StringAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'stringAtom',
                                            StringAtom(itemsize=3, shape=(2,)),
                                            "Ragged array of strings")
        vlarray.flavor = "python"
        a = numpy.array([["a", "b"], ["123", "45"], ["45", "123"]], dtype="S3")
        vlarray.append(a[1:])
        a = numpy.array([["s", "a"], ["ab", "f"],
                         ["s", "abc"], ["abc", "f"],
                         ["s", "ab"], ["ab", "f"]])
        vlarray.append(a[2:])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], [[b"123", b"45"], [b"45", b"123"]])
        self.assertEqual(row[1], [[b"s", b"abc"], [b"abc", b"f"],
                                  [b"s", b"ab"], [b"ab", b"f"]])
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 4)

    def test01d_StringAtom(self):
        """Checking vlarray with MD NumPy string atoms (with stride)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01d_StringAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'stringAtom',
                                            StringAtom(itemsize=3, shape=(2,)),
                                            "Ragged array of strings")
        vlarray.flavor = "python"
        a = numpy.array([["a", "b"], ["123", "45"], ["45", "123"]], dtype="S3")
        vlarray.append(a[1::2])
        a = numpy.array([["s", "a"], ["ab", "f"],
                         ["s", "abc"], ["abc", "f"],
                         ["s", "ab"], ["ab", "f"]])
        vlarray.append(a[::3])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertEqual(row[0], [[b"123", b"45"]])
        self.assertEqual(row[1], [[b"s", b"a"], [b"abc", b"f"]])
        self.assertEqual(len(row[0]), 1)
        self.assertEqual(len(row[1]), 2)

    def test02_BoolAtom(self):
        """Checking vlarray with MD boolean atoms."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_BoolAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'BoolAtom',
                                            BoolAtom(shape=(3,)),
                                            "Ragged array of Booleans")
        vlarray.append([(1, 0, 3), (1, 1, 1), (0, 0, 0)])
        vlarray.append([(-1, 0, 0)])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(
            allequal(row[0], numpy.array([[1, 0, 1], [1, 1, 1], [0, 0, 0]],
                                         dtype='bool')))
        self.assertTrue(
            allequal(row[1], numpy.array([[1, 0, 0]], dtype='bool')))
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 1)

    def test02b_BoolAtom(self):
        """Checking vlarray with MD boolean atoms (with offset)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_BoolAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'BoolAtom',
                                            BoolAtom(shape=(3,)),
                                            "Ragged array of Booleans")
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (0, 0, 0)], dtype='bool')
        vlarray.append(a[1:])  # Create an offset
        a = numpy.array([(1, 1, 1), (-1, 0, 0)], dtype='bool')
        vlarray.append(a[1:])  # Create an offset

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(
            allequal(row[0], numpy.array([[1, 0, 1], [1, 1, 1], [0, 0, 0]],
                                         dtype='bool')))
        self.assertTrue(allequal(row[
                        1], numpy.array([[1, 0, 0]], dtype='bool')))
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 1)

    def test02c_BoolAtom(self):
        """Checking vlarray with MD boolean atoms (with strides)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02c_BoolAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'BoolAtom',
                                            BoolAtom(shape=(3,)),
                                            "Ragged array of Booleans")
        a = numpy.array([(0, 0, 0), (1, 0, 3), (
            1, 1, 1), (0, 0, 0)], dtype='bool')
        vlarray.append(a[1::2])  # Create an strided array
        a = numpy.array([(1, 1, 1), (-1, 0, 0), (0, 0, 0)], dtype='bool')
        vlarray.append(a[::2])  # Create an strided array

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, 2)
        self.assertTrue(
            allequal(row[0],
                     numpy.array([[1, 0, 1], [0, 0, 0]], dtype='bool')))
        self.assertTrue(
            allequal(row[1],
                     numpy.array([[1, 1, 1], [0, 0, 0]], dtype='bool')))
        self.assertEqual(len(row[0]), 2)
        self.assertEqual(len(row[1]), 2)

    def test03_IntAtom(self):
        """Checking vlarray with MD integer atoms."""

        ttypes = ["Int8",
                  "UInt8",
                  "Int16",
                  "UInt16",
                  "Int32",
                  "UInt32",
                  "Int64",
                  #"UInt64",  # Unavailable in some platforms
                  ]
        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_IntAtom..." % self.__class__.__name__)

        # Create an string atom
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                root, atype, atom=Atom.from_sctype(atype, (2, 3)))
            vlarray.append([numpy.ones((2, 3), atype),
                            numpy.zeros((2, 3), atype)])
            vlarray.append([numpy.ones((2, 3), atype)*100])

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("Second row in vlarray ==>", repr(row[1]))

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0], numpy.array([numpy.ones((2, 3)),
                                              numpy.zeros((2, 3))],
                                             atype)))
            self.assertTrue(
                allequal(row[1], numpy.array([numpy.ones((2, 3))*100], atype)))
            self.assertEqual(len(row[0]), 2)
            self.assertEqual(len(row[1]), 1)

    def test04_FloatAtom(self):
        """Checking vlarray with MD floating point atoms."""

        ttypes = [
            "Float32",
            "Float64",
            "Complex32",
            "Complex64",
        ]

        for name in ("float16", "float96", "float128"):
            atomname = name.capitalize() + "Atom"
            if atomname in globals():
                ttypes.append(name.capitalize())
        for itemsize in (192, 256):
            atomname = "Complex%dAtom" % itemsize
            if atomname in globals():
                ttypes.append("Complex%d" % (itemsize // 2))

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_FloatAtom..." % self.__class__.__name__)

        # Create an string atom
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(
                root, atype, atom=Atom.from_sctype(atype, (5, 2, 6)))
            vlarray.append([numpy.ones((5, 2, 6), atype)*1.3,
                            numpy.zeros((5, 2, 6), atype)])
            vlarray.append([numpy.ones((5, 2, 6), atype)*2.e4])

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing type:", atype)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("Second row in vlarray ==>", row[1])

            self.assertEqual(vlarray.nrows, 2)
            self.assertTrue(
                allequal(row[0], numpy.array([numpy.ones((5, 2, 6))*1.3,
                                              numpy.zeros((5, 2, 6))],
                                             atype)))
            self.assertTrue(
                allequal(row[1], numpy.array([numpy.ones((5, 2, 6))*2.e4],
                                             atype)))
            self.assertEqual(len(row[0]), 2)
            self.assertEqual(len(row[1]), 1)


class MDTypesNumPyTestCase(MDTypesTestCase):
    title = "MDTypes"


class AppendShapeTestCase(unittest.TestCase):
    mode = "w"

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test00_difinputs(self):
        """Checking vlarray.append() with different inputs."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test00_difinputs..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'vlarray',
                                            Int32Atom(),
                                            "Ragged array of ints")
        vlarray.flavor = "python"

        # Check different ways to input
        # All of the next should lead to the same rows
        vlarray.append((1, 2, 3))  # a tuple
        vlarray.append([1, 2, 3])  # a unique list
        vlarray.append(numpy.array([1, 2, 3], dtype='int32'))  # and array

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            vlarray = self.fileh.root.vlarray

        # Read all the vlarray
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 3)
        self.assertEqual(row[0], [1, 2, 3])
        self.assertEqual(row[1], [1, 2, 3])
        self.assertEqual(row[2], [1, 2, 3])

    def test01_toomanydims(self):
        """Checking vlarray.append() with too many dimensions."""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_toomanydims..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'vlarray',
                                            StringAtom(itemsize=3),
                                            "Ragged array of strings")
        # Adding an array with one dimensionality more than allowed
        try:
            vlarray.append([["123", "456", "3"]])
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next RuntimeError was catched!")
                print(value)
        else:
            self.fail("expected a ValueError")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            vlarray = self.fileh.root.vlarray

        # Read all the rows (there should be none)
        row = vlarray.read()
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        self.assertEqual(vlarray.nrows, 0)

    def test02_zerodims(self):
        """Checking vlarray.append() with a zero-dimensional array"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_zerodims..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'vlarray',
                                            Int32Atom(),
                                            "Ragged array of ints")
        vlarray.append(numpy.zeros(dtype='int32', shape=(6, 0)))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            vlarray = self.fileh.root.vlarray

        # Read the only row in vlarray
        row = vlarray.read(0)[0]
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", repr(row))

        self.assertEqual(vlarray.nrows, 1)
        self.assertTrue(allequal(row, numpy.zeros(dtype='int32', shape=(0,))))
        self.assertEqual(len(row), 0)

    def test03a_cast(self):
        """Checking vlarray.append() with a casted array (upgrading case)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03a_cast..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'vlarray',
                                            Int32Atom(),
                                            "Ragged array of ints")
        # This type has to be upgraded
        vlarray.append(numpy.array([1, 2], dtype='int16'))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            vlarray = self.fileh.root.vlarray

        # Read the only row in vlarray
        row = vlarray.read(0)[0]
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", repr(row))

        self.assertEqual(vlarray.nrows, 1)
        self.assertTrue(allequal(row, numpy.array([1, 2], dtype='int32')))
        self.assertEqual(len(row), 2)

    def test03b_cast(self):
        """Checking vlarray.append() with a casted array (downgrading case)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_cast..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, 'vlarray',
                                            Int32Atom(),
                                            "Ragged array of ints")
        # This type has to be downcasted
        vlarray.append(numpy.array([1, 2], dtype='float64'))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            vlarray = self.fileh.root.vlarray

        # Read the only row in vlarray
        row = vlarray.read(0)[0]
        if common.verbose:
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", repr(row))

        self.assertEqual(vlarray.nrows, 1)
        self.assertTrue(allequal(row, numpy.array([1, 2], dtype='int32')))
        self.assertEqual(len(row), 2)


class OpenAppendShapeTestCase(AppendShapeTestCase):
    close = 0


class CloseAppendShapeTestCase(AppendShapeTestCase):
    close = 1


class FlavorTestCase(unittest.TestCase):
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):

        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #----------------------------------------

    def test01a_EmptyVLArray(self):
        """Checking empty vlarrays with different flavors (closing the file)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_EmptyVLArray..." %
                  self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, "vlarray",
                                            Atom.from_kind('int', itemsize=4))
        vlarray.flavor = self.flavor
        self.fileh.close()
        self.fileh = open_file(self.file, "r")
        # Read all the rows (it should be empty):
        vlarray = self.fileh.root.vlarray
        row = vlarray.read()
        if common.verbose:
            print("Testing flavor:", self.flavor)
            print("Object read:", row, repr(row))
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
        # Check that the object read is effectively empty
        self.assertEqual(vlarray.nrows, 0)
        self.assertEqual(row, [])

    def test01b_EmptyVLArray(self):
        """Checking empty vlarrays with different flavors (no closing file)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_EmptyVLArray..." %
                  self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, "vlarray",
                                            Atom.from_kind('int', itemsize=4))
        vlarray.flavor = self.flavor
        # Read all the rows (it should be empty):
        row = vlarray.read()
        if common.verbose:
            print("Testing flavor:", self.flavor)
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
        # Check that the object read is effectively empty
        self.assertEqual(vlarray.nrows, 0)
        self.assertEqual(row, [])

    def test02_BooleanAtom(self):
        """Checking vlarray with different flavors (boolean versions)"""

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_BoolAtom..." % self.__class__.__name__)

        # Create an string atom
        vlarray = self.fileh.create_vlarray(root, "Bool", BoolAtom())
        vlarray.flavor = self.flavor
        vlarray.append([1, 2, 3])
        vlarray.append(())   # Empty row
        vlarray.append([100, 0])

        # Read all the rows:
        row = vlarray.read()
        if common.verbose:
            print("Testing flavor:", self.flavor)
            print("Object read:", row)
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])

        self.assertEqual(vlarray.nrows, 3)
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 0)
        self.assertEqual(len(row[2]), 2)
        if self.flavor == "python":
            arr1 = [1, 1, 1]
            arr2 = []
            arr3 = [1, 0]
        elif self.flavor == "numpy":
            arr1 = numpy.array([1, 1, 1], dtype="bool")
            arr2 = numpy.array([], dtype="bool")
            arr3 = numpy.array([1, 0], dtype="bool")

        if self.flavor == "numpy":
            self.assertTrue(allequal(row[0], arr1, self.flavor))
            self.assertTrue(allequal(row[1], arr2, self.flavor))
            self.assertTrue(allequal(row[1], arr2, self.flavor))
        else:
            # 'python' flavor
            self.assertEqual(row[0], arr1)
            self.assertEqual(row[1], arr2)
            self.assertEqual(row[2], arr3)

    def test03_IntAtom(self):
        """Checking vlarray with different flavors (integer versions)"""

        ttypes = ["Int8",
                  "UInt8",
                  "Int16",
                  "UInt16",
                  "Int32",
                  "UInt32",
                  "Int64",
                  # Not checked because some platforms does not support it
                  #"UInt64",
                  ]
        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_IntAtom..." % self.__class__.__name__)

        # Create an string atom
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(root, atype,
                                                Atom.from_sctype(atype))
            vlarray.flavor = self.flavor
            vlarray.append([1, 2, 3])
            vlarray.append(())
            vlarray.append([100, 0])

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing flavor:", self.flavor)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 3)
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 0)
            self.assertEqual(len(row[2]), 2)
            if self.flavor == "python":
                arr1 = [1, 2, 3]
                arr2 = []
                arr3 = [100, 0]
            elif self.flavor == "numpy":
                arr1 = numpy.array([1, 2, 3], dtype=atype)
                arr2 = numpy.array([], dtype=atype)
                arr3 = numpy.array([100, 0], dtype=atype)

            if self.flavor == "numpy":
                self.assertTrue(allequal(row[0], arr1, self.flavor))
                self.assertTrue(allequal(row[1], arr2, self.flavor))
                self.assertTrue(allequal(row[2], arr3, self.flavor))
            else:
                # "python" flavor
                self.assertEqual(row[0], arr1)
                self.assertEqual(row[1], arr2)
                self.assertEqual(row[2], arr3)

    def test03b_IntAtom(self):
        """Checking vlarray flavors (integer versions and closed file)"""

        ttypes = ["Int8",
                  "UInt8",
                  "Int16",
                  "UInt16",
                  "Int32",
                  "UInt32",
                  "Int64",
                  # Not checked because some platforms does not support it
                  #"UInt64",
                  ]
        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_IntAtom..." % self.__class__.__name__)

        # Create an string atom
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(root, atype,
                                                Atom.from_sctype(atype))
            vlarray.flavor = self.flavor
            vlarray.append([1, 2, 3])
            vlarray.append(())
            vlarray.append([100, 0])
            self.fileh.close()
            self.fileh = open_file(self.file, "a")  # open in "a"ppend mode
            root = self.fileh.root  # Very important!
            vlarray = self.fileh.get_node(root, str(atype))
            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing flavor:", self.flavor)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 3)
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 0)
            self.assertEqual(len(row[2]), 2)
            if self.flavor == "python":
                arr1 = [1, 2, 3]
                arr2 = []
                arr3 = [100, 0]
            elif self.flavor == "numpy":
                arr1 = numpy.array([1, 2, 3], dtype=atype)
                arr2 = numpy.array([], dtype=atype)
                arr3 = numpy.array([100, 0], dtype=atype)

            if self.flavor == "numpy":
                self.assertTrue(allequal(row[0], arr1, self.flavor))
                self.assertTrue(allequal(row[1], arr2, self.flavor))
                self.assertTrue(allequal(row[2], arr3, self.flavor))
            else:
                # Tuple or List flavors
                self.assertEqual(row[0], arr1)
                self.assertEqual(row[1], arr2)
                self.assertEqual(row[2], arr3)

    def test04_FloatAtom(self):
        """Checking vlarray with different flavors (floating point versions)"""

        ttypes = [
            "Float32",
            "Float64",
            "Complex32",
            "Complex64",
        ]

        for name in ("float16", "float96", "float128"):
            atomname = name.capitalize() + "Atom"
            if atomname in globals():
                ttypes.append(name.capitalize())

        for itemsize in (192, 256):
            atomname = "Complex%dAtom" % itemsize
            if atomname in globals():
                ttypes.append("Complex%d" % (itemsize // 2))

        root = self.rootgroup
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_FloatAtom..." % self.__class__.__name__)

        # Create an string atom
        for atype in ttypes:
            vlarray = self.fileh.create_vlarray(root, atype,
                                                Atom.from_sctype(atype))
            vlarray.flavor = self.flavor
            vlarray.append([1.3, 2.2, 3.3])
            vlarray.append(())
            vlarray.append([-1.3e34, 1.e-32])

            # Read all the rows:
            row = vlarray.read()
            if common.verbose:
                print("Testing flavor:", self.flavor)
                print("Object read:", row)
                print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
                print("First row in vlarray ==>", row[0])

            self.assertEqual(vlarray.nrows, 3)
            self.assertEqual(len(row[0]), 3)
            self.assertEqual(len(row[1]), 0)
            self.assertEqual(len(row[2]), 2)
            if self.flavor == "python":
                arr1 = list(numpy.array([1.3, 2.2, 3.3], atype))
                arr2 = list(numpy.array([], atype))
                arr3 = list(numpy.array([-1.3e34, 1.e-32], atype))
            elif self.flavor == "numpy":
                arr1 = numpy.array([1.3, 2.2, 3.3], dtype=atype)
                arr2 = numpy.array([], dtype=atype)
                arr3 = numpy.array([-1.3e34, 1.e-32], dtype=atype)

            if self.flavor == "numpy":
                self.assertTrue(allequal(row[0], arr1, self.flavor))
                self.assertTrue(allequal(row[1], arr2, self.flavor))
                self.assertTrue(allequal(row[2], arr3, self.flavor))
            else:
                # Tuple or List flavors
                self.assertEqual(row[0], arr1)
                self.assertEqual(row[1], arr2)
                self.assertEqual(row[2], arr3)


class NumPyFlavorTestCase(FlavorTestCase):
    flavor = "numpy"


class PythonFlavorTestCase(FlavorTestCase):
    flavor = "python"


class ReadRangeTestCase(unittest.TestCase):
    nrows = 100
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        filters = Filters(complevel=self.compress,
                          complib=self.complib)
        vlarray = self.fileh.create_vlarray(group, 'vlarray', Int32Atom(),
                                            "ragged array if ints",
                                            filters=filters,
                                            expectedrows=1000)

        # Fill it with 100 rows with variable length
        for i in range(self.nrows):
            vlarray.append(range(i))

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #------------------------------------------------------------------

    def test01_start(self):
        "Checking reads with only a start value"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        # Read some rows:
        row = []
        row.append(vlarray.read(0)[0])
        row.append(vlarray.read(10)[0])
        row.append(vlarray.read(99)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0], numpy.arange(0, dtype='int32')))
        self.assertTrue(allequal(row[1], numpy.arange(10, dtype='int32')))
        self.assertTrue(allequal(row[2], numpy.arange(99, dtype='int32')))

    def test01b_start(self):
        "Checking reads with only a start value in a slice"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        # Read some rows:
        row = []
        row.append(vlarray[0])
        row.append(vlarray[10])
        row.append(vlarray[99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0], numpy.arange(0, dtype='int32')))
        self.assertTrue(allequal(row[1], numpy.arange(10, dtype='int32')))
        self.assertTrue(allequal(row[2], numpy.arange(99, dtype='int32')))

    def test01np_start(self):
        "Checking reads with only a start value in a slice (numpy indexes)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01np_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        # Read some rows:
        row = []
        row.append(vlarray[numpy.int8(0)])
        row.append(vlarray[numpy.int32(10)])
        row.append(vlarray[numpy.int64(99)])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0], numpy.arange(0, dtype='int32')))
        self.assertTrue(allequal(row[1], numpy.arange(10, dtype='int32')))
        self.assertTrue(allequal(row[2], numpy.arange(99, dtype='int32')))

    def test02_stop(self):
        "Checking reads with only a stop value"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_stop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray.read(stop=1))
        row.append(vlarray.read(stop=10))
        row.append(vlarray.read(stop=99))
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 1)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0][0], numpy.arange(0, dtype='int32')))
        for x in range(10):
            self.assertTrue(allequal(row[1][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(99):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test02b_stop(self):
        "Checking reads with only a stop value in a slice"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_stop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[:1])
        row.append(vlarray[:10])
        row.append(vlarray[:99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 1)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        for x in range(1):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(0, dtype='int32')))
        for x in range(10):
            self.assertTrue(allequal(row[1][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(99):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test03_startstop(self):
        "Checking reads with a start and stop values"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_startstop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray.read(0, 10))
        row.append(vlarray.read(5, 15))
        row.append(vlarray.read(0, 100))  # read all the array
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 10)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 100)
        for x in range(0, 10):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(5, 15):
            self.assertTrue(
                allequal(row[1][x-5], numpy.arange(x, dtype='int32')))
        for x in range(0, 100):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test03b_startstop(self):
        "Checking reads with a start and stop values in slices"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_startstop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[0:10])
        row.append(vlarray[5:15])
        row.append(vlarray[:])  # read all the array
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 10)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 100)
        for x in range(0, 10):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(5, 15):
            self.assertTrue(
                allequal(row[1][x-5], numpy.arange(x, dtype='int32')))
        for x in range(0, 100):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test04_startstopstep(self):
        "Checking reads with a start, stop & step values"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_startstopstep..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray.read(0, 10, 2))
        row.append(vlarray.read(5, 15, 3))
        row.append(vlarray.read(0, 100, 20))
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertTrue(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test04np_startstopstep(self):
        "Checking reads with a start, stop & step values (numpy indices)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04np_startstopstep..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray.read(numpy.int8(0), numpy.int8(10), numpy.int8(2)))
        row.append(vlarray.read(numpy.int8(5), numpy.int8(15), numpy.int8(3)))
        row.append(vlarray.read(numpy.int8(
            0), numpy.int8(100), numpy.int8(20)))
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test04b_slices(self):
        "Checking reads with start, stop & step values in slices"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04b_slices..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[0:10:2])
        row.append(vlarray[5:15:3])
        row.append(vlarray[0:100:20])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test04bnp_slices(self):
        """Checking reads with start, stop & step values in slices.

        (numpy indices)

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04bnp_slices..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[numpy.int16(0):numpy.int16(10):numpy.int32(2)])
        row.append(vlarray[numpy.int16(5):numpy.int16(15):numpy.int64(3)])
        row.append(vlarray[numpy.uint16(0):numpy.int32(100):numpy.int8(20)])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test05_out_of_range(self):
        "Checking out of range reads"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_out_of_range..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        try:
            row = vlarray.read(1000)[0]
            print("row-->", row)
        except IndexError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IndexError was catched!")
                print(value)
            self.fileh.close()
        else:
            (type, value, traceback) = sys.exc_info()
            self.fail("expected a IndexError and got:\n%s" % value)


class GetItemRangeTestCase(unittest.TestCase):
    nrows = 100
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        filters = Filters(complevel=self.compress,
                          complib=self.complib)
        vlarray = self.fileh.create_vlarray(group, 'vlarray', Int32Atom(),
                                            "ragged array if ints",
                                            filters=filters,
                                            expectedrows=1000)

        # Fill it with 100 rows with variable length
        for i in range(self.nrows):
            vlarray.append(range(i))

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #------------------------------------------------------------------

    def test01_start(self):
        "Checking reads with only a start value"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        # Read some rows:
        row = []
        row.append(vlarray[0])
        # rank-0 array should work as a regular index (see #303)
        row.append(vlarray[numpy.array(10)])
        row.append(vlarray[99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')))
        self.assertTrue(
            allequal(row[2], numpy.arange(99, dtype='int32')))

    def test01b_start(self):
        "Checking reads with only a start value in a slice"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        # Read some rows:
        row = []
        row.append(vlarray[0])
        row.append(vlarray[10])
        row.append(vlarray[99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0], numpy.arange(0, dtype='int32')))
        self.assertTrue(allequal(row[1], numpy.arange(10, dtype='int32')))
        self.assertTrue(allequal(row[2], numpy.arange(99, dtype='int32')))

    def test02_stop(self):
        "Checking reads with only a stop value"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_stop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[:1])
        row.append(vlarray[:10])
        row.append(vlarray[:99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("First row in vlarray ==>", row[0])
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 1)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(allequal(row[0][0], numpy.arange(0, dtype='int32')))
        for x in range(10):
            self.assertTrue(allequal(row[1][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(99):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test02b_stop(self):
        "Checking reads with only a stop value in a slice"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02b_stop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[:1])
        row.append(vlarray[:10])
        row.append(vlarray[:99])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 1)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        for x in range(1):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(0, dtype='int32')))
        for x in range(10):
            self.assertTrue(allequal(row[1][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(99):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test03_startstop(self):
        "Checking reads with a start and stop values"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_startstop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[0:10])
        row.append(vlarray[5:15])
        row.append(vlarray[0:100])  # read all the array
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 10)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 100)
        for x in range(0, 10):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(5, 15):
            self.assertTrue(
                allequal(row[1][x-5], numpy.arange(x, dtype='int32')))
        for x in range(0, 100):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test03b_startstop(self):
        "Checking reads with a start and stop values in slices"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_startstop..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[0:10])
        row.append(vlarray[5:15])
        row.append(vlarray[:])  # read all the array
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 10)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 100)
        for x in range(0, 10):
            self.assertTrue(allequal(row[0][
                            x], numpy.arange(x, dtype='int32')))
        for x in range(5, 15):
            self.assertTrue(
                allequal(row[1][x-5], numpy.arange(x, dtype='int32')))
        for x in range(0, 100):
            self.assertTrue(allequal(row[2][
                            x], numpy.arange(x, dtype='int32')))

    def test04_slices(self):
        "Checking reads with a start, stop & step values"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_slices..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[0:10:2])
        row.append(vlarray[5:15:3])
        row.append(vlarray[0:100:20])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertTrue(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test04bnp_slices(self):
        "Checking reads with start, stop & step values (numpy indices)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04np_slices..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray
        # Choose a small value for buffer size
        vlarray._nrowsinbuf = 3

        # Read some rows:
        row = []
        row.append(vlarray[numpy.int8(0):numpy.int8(10):numpy.int8(2)])
        row.append(vlarray[numpy.int8(5):numpy.int8(15):numpy.int8(3)])
        row.append(vlarray[numpy.int8(0):numpy.int8(100):numpy.int8(20)])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 5)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        for x in range(0, 10, 2):
            self.assertTrue(
                allequal(row[0][x//2], numpy.arange(x, dtype='int32')))
        for x in range(5, 15, 3):
            self.assertTrue(
                allequal(row[1][(x-5)//3], numpy.arange(x, dtype='int32')))
        for x in range(0, 100, 20):
            self.assertTrue(
                allequal(row[2][x//20], numpy.arange(x, dtype='int32')))

    def test05_out_of_range(self):
        "Checking out of range reads"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_out_of_range..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        try:
            row = vlarray[1000]
            print("row-->", row)
        except IndexError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IndexError was catched!")
                print(value)
            self.fileh.close()
        else:
            (type, value, traceback) = sys.exc_info()
            self.fail("expected a IndexError and got:\n%s" % value)

    def test05np_out_of_range(self):
        "Checking out of range reads (numpy indexes)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05np_out_of_range..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "r")
        vlarray = self.fileh.root.vlarray

        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        try:
            row = vlarray[numpy.int32(1000)]
            print("row-->", row)
        except IndexError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IndexError was catched!")
                print(value)
            self.fileh.close()
        else:
            (type, value, traceback) = sys.exc_info()
            self.fail("expected a IndexError and got:\n%s" % value)


class SetRangeTestCase(unittest.TestCase):
    nrows = 100
    mode = "w"
    compress = 0
    complib = "zlib"  # Default compression library

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, self.mode)
        self.rootgroup = self.fileh.root
        self.populateFile()
        self.fileh.close()

    def populateFile(self):
        group = self.rootgroup
        filters = Filters(complevel=self.compress,
                          complib=self.complib)
        vlarray = self.fileh.create_vlarray(group, 'vlarray', Int32Atom(),
                                            "ragged array if ints",
                                            filters=filters,
                                            expectedrows=1000)

        # Fill it with 100 rows with variable length
        for i in range(self.nrows):
            vlarray.append(range(i))

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    #------------------------------------------------------------------

    def test01_start(self):
        "Checking updates that modifies a complete row"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[0] = vlarray[0]*2 + 3
        vlarray[10] = vlarray[10]*2 + 3
        vlarray[99] = vlarray[99]*2 + 3

        # Read some rows:
        row = []
        row.append(vlarray.read(0)[0])
        row.append(vlarray.read(10)[0])
        row.append(vlarray.read(99)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[2], numpy.arange(99, dtype='int32')*2 + 3))

    def test01np_start(self):
        "Checking updates that modifies a complete row"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01np_start..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[numpy.int8(0)] = vlarray[numpy.int16(0)]*2 + 3
        vlarray[numpy.int8(10)] = vlarray[numpy.int8(10)]*2 + 3
        vlarray[numpy.int32(99)] = vlarray[numpy.int64(99)]*2 + 3

        # Read some rows:
        row = []
        row.append(vlarray.read(numpy.int8(0))[0])
        row.append(vlarray.read(numpy.int8(10))[0])
        row.append(vlarray.read(numpy.int8(99))[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 99)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[2], numpy.arange(99, dtype='int32')*2 + 3))

    def test02_partial(self):
        "Checking updates with only a part of a row"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_partial..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[0] = vlarray[0]*2 + 3
        vlarray[10] = vlarray[10]*2 + 3
        vlarray[96] = vlarray[99][3:]*2 + 3

        # Read some rows:
        row = []
        row.append(vlarray.read(0)[0])
        row.append(vlarray.read(10)[0])
        row.append(vlarray.read(96)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 96)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')*2 + 3))
        a = numpy.arange(3, 99, dtype='int32')
        a = a * 2 + 3
        self.assertTrue(allequal(row[2], a))

    def test03a_several_rows(self):
        "Checking updating several rows at once (slice style)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03a_several_rows..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[3:6] = (vlarray[3]*2 + 3,
                        vlarray[4]*2 + 3,
                        vlarray[5]*2 + 3)

        # Read some rows:
        row = []
        row.append(vlarray.read(3)[0])
        row.append(vlarray.read(4)[0])
        row.append(vlarray.read(5)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 3)
        self.assertEqual(len(row[1]), 4)
        self.assertEqual(len(row[2]), 5)
        self.assertTrue(allequal(row[0], numpy.arange(3, dtype='int32')*2 + 3))
        self.assertTrue(allequal(row[1], numpy.arange(4, dtype='int32')*2 + 3))
        self.assertTrue(allequal(row[2], numpy.arange(5, dtype='int32')*2 + 3))

    def test03b_several_rows(self):
        "Checking updating several rows at once (list style)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03b_several_rows..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[[0, 10, 96]] = (vlarray[0]*2 + 3,
                                vlarray[10]*2 + 3,
                                vlarray[96]*2 + 3)

        # Read some rows:
        row = []
        row.append(vlarray.read(0)[0])
        row.append(vlarray.read(10)[0])
        row.append(vlarray.read(96)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 96)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[2], numpy.arange(96, dtype='int32')*2 + 3))

    def test03c_several_rows(self):
        "Checking updating several rows at once (NumPy's where style)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03c_several_rows..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        # Modify some rows:
        vlarray[(numpy.array([0, 10, 96]),)] = (vlarray[0]*2 + 3,
                                                vlarray[10]*2 + 3,
                                                vlarray[96]*2 + 3)

        # Read some rows:
        row = []
        row.append(vlarray.read(0)[0])
        row.append(vlarray.read(10)[0])
        row.append(vlarray.read(96)[0])
        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)
            print("Second row in vlarray ==>", row[1])

        self.assertEqual(vlarray.nrows, self.nrows)
        self.assertEqual(len(row[0]), 0)
        self.assertEqual(len(row[1]), 10)
        self.assertEqual(len(row[2]), 96)
        self.assertTrue(
            allequal(row[0], numpy.arange(0, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[1], numpy.arange(10, dtype='int32')*2 + 3))
        self.assertTrue(
            allequal(row[2], numpy.arange(96, dtype='int32')*2 + 3))

    def test04_out_of_range(self):
        "Checking out of range updates (first index)"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_out_of_range..." %
                  self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        try:
            vlarray[1000] = [1]
        except IndexError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next IndexError was catched!")
                print(value)
            self.fileh.close()
        else:
            (type, value, traceback) = sys.exc_info()
            self.fail("expected a IndexError and got:\n%s" % value)

    def test05_value_error(self):
        "Checking out value errors"
        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_value_error..." % self.__class__.__name__)

        self.fileh = open_file(self.file, "a")
        vlarray = self.fileh.root.vlarray

        if common.verbose:
            print("Nrows in", vlarray._v_pathname, ":", vlarray.nrows)

        try:
            vlarray[10] = [1]*100
            print("row-->", row)
        except ValueError:
            if common.verbose:
                (type, value, traceback) = sys.exc_info()
                print("\nGreat!, the next ValueError was catched!")
                print(value)
            self.fileh.close()
        else:
            (type, value, traceback) = sys.exc_info()
            self.fail("expected a ValueError and got:\n%s" % value)


class CopyTestCase(unittest.TestCase):
    close = True

    def test01a_copy(self):
        """Checking VLArray.copy() method."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01a_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Vlarray
        arr = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(
            fileh.root, 'array1', arr, "title array1")
        array1.flavor = "python"
        array1.append([[2, 3]])
        array1.append(())  # an empty row
        array1.append([[3, 457], [2, 4]])

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", repr(array1))
            print("array2-->", repr(array2))
            print("array1[:]-->", repr(array1.read()))
            print("array2[:]-->", repr(array2.read()))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertEqual(array1.read(), array2.read())

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(repr(array1.atom), repr(array2.atom))

        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test01b_copy(self):
        """Checking VLArray.copy() method.

        Pseudo-atom case.

        """

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Vlarray
        arr = VLStringAtom()
        array1 = fileh.create_vlarray(
            fileh.root, 'array1', arr, "title array1")
        array1.flavor = "python"
        array1.append("a string")
        array1.append("")  # an empty row
        array1.append("another string")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("array1-->", repr(array1))
            print("array2-->", repr(array2))
            print("array1[:]-->", repr(array1.read()))
            print("array2[:]-->", repr(array2.read()))
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertEqual(array1.read(), array2.read())

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.type, array2.atom.type)
        self.assertEqual(repr(array1.atom), repr(array2.atom))

        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test02_copy(self):
        """Checking VLArray.copy() method (where specified)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test02_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an VLArray
        arr = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(
            fileh.root, 'array1', arr, "title array1")
        array1.flavor = "python"
        array1.append([[2, 3]])
        array1.append(())  # an empty row
        array1.append([[3, 457], [2, 4]])

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        group1 = fileh.create_group("/", "group1")
        array2 = array1.copy(group1, 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.group1.array2

        if common.verbose:
            print("array1-->", repr(array1))
            print("array2-->", repr(array2))
            print("array1-->", array1.read())
            print("array2-->", array2.read())
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Check that all the elements are equal
        self.assertEqual(array1.read(), array2.read())

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.flavor, array2.flavor)
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(repr(array1.atom), repr(array1.atom))
        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test03_copy(self):
        """Checking VLArray.copy() method ('python' flavor)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test03_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an VLArray
        atom = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(fileh.root, 'array1', atom,
                                      title="title array1")
        array1.flavor = "python"
        array1.append(((2, 3),))
        array1.append(())  # an empty row
        array1.append(((3, 457), (2, 4)))

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another location
        array2 = array1.copy('/', 'array2')

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert other properties in array
        self.assertEqual(array1.nrows, array2.nrows)
        self.assertEqual(array1.shape, array2.shape)
        self.assertEqual(array1.flavor, array2.flavor)  # Very important here
        self.assertEqual(array1.atom.dtype, array2.atom.dtype)
        self.assertEqual(repr(array1.atom), repr(array1.atom))
        self.assertEqual(array1.title, array2.title)

        # Close the file
        fileh.close()
        os.remove(file)

    def test04_copy(self):
        """Checking VLArray.copy() method (checking title copying)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test04_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an VLArray
        atom = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(fileh.root, 'array1', atom=atom,
                                      title="title array1")
        array1.append(((2, 3),))
        array1.append(())  # an empty row
        array1.append(((3, 457), (2, 4)))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', title="title array2")

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        # Assert user attributes
        if common.verbose:
            print("title of destination array-->", array2.title)
        self.assertEqual(array2.title, "title array2")

        # Close the file
        fileh.close()
        os.remove(file)

    def test05_copy(self):
        """Checking VLArray.copy() method (user attributes copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an Array
        atom = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(fileh.root, 'array1', atom=atom,
                                      title="title array1")
        array1.append(((2, 3),))
        array1.append(())  # an empty row
        array1.append(((3, 457), (2, 4)))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(array2.attrs.attr1, "attr1")
        self.assertEqual(array2.attrs.attr2, 2)

        # Close the file
        fileh.close()
        os.remove(file)

    def notest05b_copy(self):
        """Checking VLArray.copy() method (user attributes not copied)"""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test05b_copy..." % self.__class__.__name__)

        # Create an instance of an HDF5 Table
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an VLArray
        atom = Int16Atom(shape=2)
        array1 = fileh.create_vlarray(fileh.root, 'array1', atom=atom,
                                      title="title array1")
        array1.append(((2, 3),))
        array1.append(())  # an empty row
        array1.append(((3, 457), (2, 4)))
        # Append some user attrs
        array1.attrs.attr1 = "attr1"
        array1.attrs.attr2 = 2

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy it to another Array
        array2 = array1.copy('/', 'array2', copyuserattrs=0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="r")
            array1 = fileh.root.array1
            array2 = fileh.root.array2

        if common.verbose:
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))

        # Assert user attributes
        self.assertEqual(array2.attrs.attr1, None)
        self.assertEqual(array2.attrs.attr2, None)

        # Close the file
        fileh.close()
        os.remove(file)


class CloseCopyTestCase(CopyTestCase):
    close = 1


class OpenCopyTestCase(CopyTestCase):
    close = 0


class CopyIndexTestCase(unittest.TestCase):

    def test01_index(self):
        """Checking VLArray.copy() method with indexes."""

        if common.verbose:
            print('\n', '-=' * 30)
            print("Running %s.test01_index..." % self.__class__.__name__)

        # Create an instance of an HDF5 Array
        file = tempfile.mktemp(".h5")
        fileh = open_file(file, "w")

        # Create an VLArray
        atom = Int32Atom(shape=(2,))
        array1 = fileh.create_vlarray(fileh.root, 'array1', atom, "t array1")
        array1.flavor = "python"
        # The next creates 20 rows of variable length
        r = []
        for row in range(20):
            r.append([[row, row + 1]])
            array1.append([row, row + 1])

        if self.close:
            if common.verbose:
                print("(closing file version)")
            fileh.close()
            fileh = open_file(file, mode="a")
            array1 = fileh.root.array1

        # Copy to another array
        array2 = array1.copy("/", 'array2',
                             start=self.start,
                             stop=self.stop,
                             step=self.step)

        r2 = r[self.start:self.stop:self.step]
        if common.verbose:
            print("r2-->", r2)
            print("array2-->", array2[:])
            print("attrs array1-->", repr(array1.attrs))
            print("attrs array2-->", repr(array2.attrs))
            print("nrows in array2-->", array2.nrows)
            print("and it should be-->", len(r2))
        # Check that all the elements are equal
        self.assertEqual(r2, array2[:])
        # Assert the number of rows in array
        self.assertEqual(len(r2), array2.nrows)

        # Close the file
        fileh.close()
        os.remove(file)


class CopyIndex1TestCase(CopyIndexTestCase):
    close = 0
    start = 0
    stop = 7
    step = 1


class CopyIndex2TestCase(CopyIndexTestCase):
    close = 1
    start = 0
    stop = -1
    step = 1


class CopyIndex3TestCase(CopyIndexTestCase):
    close = 0
    start = 1
    stop = 7
    step = 1


class CopyIndex4TestCase(CopyIndexTestCase):
    close = 1
    start = 0
    stop = 6
    step = 1


class CopyIndex5TestCase(CopyIndexTestCase):
    close = 0
    start = 3
    stop = 7
    step = 1


class CopyIndex6TestCase(CopyIndexTestCase):
    close = 1
    start = 3
    stop = 6
    step = 2


class CopyIndex7TestCase(CopyIndexTestCase):
    close = 0
    start = 0
    stop = 7
    step = 10


class CopyIndex8TestCase(CopyIndexTestCase):
    close = 1
    start = 6
    stop = -1  # Negative values means starting from the end
    step = 1


class CopyIndex9TestCase(CopyIndexTestCase):
    close = 0
    start = 3
    stop = 4
    step = 1


class CopyIndex10TestCase(CopyIndexTestCase):
    close = 1
    start = 3
    stop = 4
    step = 2


class CopyIndex11TestCase(CopyIndexTestCase):
    close = 0
    start = -3
    stop = -1
    step = 2


class CopyIndex12TestCase(CopyIndexTestCase):
    close = 1
    start = -1   # Should point to the last element
    stop = None  # None should mean the last element (including it)
    step = 1


class ChunkshapeTestCase(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp('.h5')
        self.fileh = open_file(self.file, 'w', title='Chunkshape test')
        atom = Int32Atom(shape=(2,))
        self.fileh.create_vlarray('/', 'vlarray', atom=atom,
                                  title="t array1",
                                  chunkshape=13)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)

    def test00(self):
        """Test setting the chunkshape in a table (no reopen)."""

        vla = self.fileh.root.vlarray
        if common.verbose:
            print("chunkshape-->", vla.chunkshape)
        self.assertEqual(vla.chunkshape, (13,))

    def test01(self):
        """Test setting the chunkshape in a table (reopen)."""

        self.fileh.close()
        self.fileh = open_file(self.file, 'r')
        vla = self.fileh.root.vlarray
        if common.verbose:
            print("chunkshape-->", vla.chunkshape)
        self.assertEqual(vla.chunkshape, (13,))


class VLUEndianTestCase(common.PyTablesTestCase):
    def test(self):
        """Accessing ``vlunicode`` data of a different endianness."""
        h5fname = self._testFilename('vlunicode_endian.h5')
        h5f = open_file(h5fname)
        try:
            bedata = h5f.root.vlunicode_big[0]
            ledata = h5f.root.vlunicode_little[0]
            self.assertEqual(bedata, u'para\u0140lel')
            self.assertEqual(ledata, u'para\u0140lel')
        finally:
            h5f.close()


class TruncateTestCase(unittest.TestCase):

    def setUp(self):
        # Create an instance of an HDF5 Table
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, "w")

        # Create an VLArray
        arr = Int16Atom(dflt=3)
        array1 = self.fileh.create_vlarray(
            self.fileh.root, 'array1', arr, "title array1")
        # Add a couple of rows
        array1.append(numpy.array([456, 2], dtype='Int16'))
        array1.append(numpy.array([3], dtype='Int16'))

    def tearDown(self):
        # Close the file
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test00_truncate(self):
        """Checking VLArray.truncate() method (truncating to 0 rows)"""

        array1 = self.fileh.root.array1
        # Truncate to 0 elements
        array1.truncate(0)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertEqual(array1.nrows, 0)
        self.assertEqual(array1[:], [])

    def test01_truncate(self):
        """Checking VLArray.truncate() method (truncating to 1 rows)"""

        array1 = self.fileh.root.array1
        # Truncate to 1 element
        array1.truncate(1)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertEqual(array1.nrows, 1)
        self.assertTrue(
            allequal(array1[0], numpy.array([456, 2], dtype='Int16')))

    def test02_truncate(self):
        """Checking VLArray.truncate() method (truncating to == self.nrows)"""

        array1 = self.fileh.root.array1
        # Truncate to 2 elements
        array1.truncate(2)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertEqual(array1.nrows, 2)
        self.assertTrue(
            allequal(array1[0], numpy.array([456, 2], dtype='Int16')))
        self.assertTrue(allequal(array1[1], numpy.array([3], dtype='Int16')))

    def test03_truncate(self):
        """Checking VLArray.truncate() method (truncating to > self.nrows)"""

        array1 = self.fileh.root.array1
        # Truncate to 4 elements
        array1.truncate(4)

        if self.close:
            if common.verbose:
                print("(closing file version)")
            self.fileh.close()
            self.fileh = open_file(self.file, mode="r")
            array1 = self.fileh.root.array1

        if common.verbose:
            print("array1-->", array1.read())

        self.assertEqual(array1.nrows, 4)
        # Check the original values
        self.assertTrue(
            allequal(array1[0], numpy.array([456, 2], dtype='Int16')))
        self.assertTrue(allequal(array1[1], numpy.array([3], dtype='Int16')))
        # Check that the added rows are empty
        self.assertTrue(allequal(array1[2], numpy.array([], dtype='Int16')))
        self.assertTrue(allequal(array1[3], numpy.array([], dtype='Int16')))


class TruncateOpenTestCase(TruncateTestCase):
    close = 0


class TruncateCloseTestCase(TruncateTestCase):
    close = 1


class PointSelectionTestCase(common.PyTablesTestCase):

    def setUp(self):

        # The next are valid selections for both NumPy and PyTables
        self.working_keyset = [
            [],                    # empty list
            [2],                   # single-entry list
            [0, 2],                 # list
            [0, -2],                # negative values
            ([0, 2],),              # tuple of list
            numpy.array([], dtype="i4"),       # empty array
            numpy.array([1], dtype="i4"),      # single-entry array
            numpy.array([True, False, True]),   # array of bools
        ]

        # The next are invalid selections for VLArrays
        self.not_working_keyset = [
            [1, 2, 100],               # coordinate 100 > len(vlarray)
            ([True, False, True],),   # tuple of bools
        ]

        # Create an instance of an HDF5 Array
        self.file = tempfile.mktemp(".h5")
        self.fileh = fileh = open_file(self.file, "w")
        # Create a sample array
        arr1 = numpy.array([5, 6], dtype="i4")
        arr2 = numpy.array([5, 6, 7], dtype="i4")
        arr3 = numpy.array([5, 6, 9, 8], dtype="i4")
        self.nparr = numpy.array([arr1, arr2, arr3], dtype="object")
        # Create the VLArray
        self.vlarr = vlarr = fileh.create_vlarray(
            fileh.root, 'vlarray', Int32Atom())
        vlarr.append(arr1)
        vlarr.append(arr2)
        vlarr.append(arr3)

    def tearDown(self):
        self.fileh.close()
        os.remove(self.file)
        common.cleanup(self)

    def test01a_read(self):
        """Test for point-selections (read, boolean keys)."""
        nparr = self.nparr
        vlarr = self.vlarr
        for key in self.working_keyset:
            if common.verbose:
                print("Selection to test:", repr(key))
            a = nparr[key].tolist()
            b = vlarr[key]
            # if common.verbose:
            #     print "NumPy selection:", a, type(a)
            #     print "PyTables selection:", b, type(b)
            self.assertEqual(
                repr(a), repr(b),
                "NumPy array and PyTables selections does not match.")

    def test01b_read(self):
        """Test for point-selections (not working selections, read)."""
        vlarr = self.vlarr
        for key in self.not_working_keyset:
            if common.verbose:
                print("Selection to test:", key)
            self.assertRaises(IndexError, vlarr.__getitem__, key)


class SizeInMemoryPropertyTestCase(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")

    def tearDown(self):
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def create_array(self, atom, complevel):
        filters = Filters(complevel=complevel, complib='blosc')
        self.array = self.fileh.create_vlarray('/', 'vlarray', atom=atom,
                                               filters=filters)

    def test_zero_length(self):
        atom = Int32Atom()
        complevel = 0
        self.create_array(atom, complevel)
        self.assertEqual(self.array.size_in_memory, 0)

    def int_tests(self, complevel, flavor):
        atom = Int32Atom()
        self.create_array(atom, complevel)
        self.array.flavor = flavor
        expected_size = 0
        for i in xrange(10):
            row = numpy.arange((i + 1) * 10, dtype='i4')
            self.array.append(row)
            expected_size += row.nbytes
        return expected_size

    def test_numpy_int_numpy_flavor(self):
        complevel = 0
        flavor = 'numpy'
        expected_size = self.int_tests(complevel, flavor)
        self.assertEqual(self.array.size_in_memory, expected_size)

    # compression will have no effect, since this is uncompressed size
    def test_numpy_int_numpy_flavor_compressed(self):
        complevel = 1
        flavor = 'numpy'
        expected_size = self.int_tests(complevel, flavor)
        self.assertEqual(self.array.size_in_memory, expected_size)

    # flavor will have no effect on what's stored in HDF5 file
    def test_numpy_int_python_flavor(self):
        complevel = 0
        flavor = 'python'
        expected_size = self.int_tests(complevel, flavor)
        self.assertEqual(self.array.size_in_memory, expected_size)

    # this relies on knowledge of the implementation, so it's not
    # a great test
    def test_object_atom(self):
        atom = ObjectAtom()
        complevel = 0
        self.create_array(atom, complevel)
        obj = [1, 2, 3]
        for i in xrange(10):
            self.array.append(obj)
        pickle_array = atom.toarray(obj)
        expected_size = 10 * pickle_array.nbytes
        self.assertEqual(self.array.size_in_memory, expected_size)


class SizeOnDiskPropertyTestCase(unittest.TestCase):

    def setUp(self):
        self.file = tempfile.mktemp(".h5")
        self.fileh = open_file(self.file, mode="w")

    def tearDown(self):
        self.fileh.close()
        # Then, delete the file
        os.remove(self.file)
        common.cleanup(self)

    def create_array(self, atom, complevel):
        filters = Filters(complevel=complevel, complib='blosc')
        self.fileh.create_vlarray('/', 'vlarray', atom, filters=filters)
        self.array = self.fileh.get_node('/', 'vlarray')

    def test_not_implemented(self):
        atom = IntAtom()
        complevel = 0
        self.create_array(atom, complevel)
        self.assertRaises(NotImplementedError, getattr, self.array,
                          'size_on_disk')


class AccessClosedTestCase(common.TempFileMixin, common.PyTablesTestCase):

    def setUp(self):
        super(AccessClosedTestCase, self).setUp()
        self.array = self.h5file.create_vlarray(
            self.h5file.root, 'array', atom=StringAtom(8))
        self.array.append([str(i) for i in range(5, 5005, 100)])

    def test_read(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.read)

    def test_getitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__getitem__, 0)

    def test_setitem(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.__setitem__, 0, '0')

    def test_append(self):
        self.h5file.close()
        self.assertRaises(ClosedNodeError, self.array.append, 'xxxxxxxxx')


class TestCreateVLArrayArgs(common.TempFileMixin, common.PyTablesTestCase):
    obj = numpy.array([1, 2, 3])
    where = '/'
    name = 'vlarray'
    atom = Atom.from_dtype(obj.dtype)
    title = 'title'
    filters = None
    expectedrows = None
    chunkshape = None
    byteorder = None
    createparents = False

    def test_positional_args_01(self):
        self.h5file.create_vlarray(self.where, self.name,
                                   self.atom,
                                   self.title, self.filters,
                                   self.expectedrows)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (0,))
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)

    def test_positional_args_02(self):
        ptarr = self.h5file.create_vlarray(self.where, self.name,
                                           self.atom,
                                           self.title,
                                           self.filters,
                                           self.expectedrows)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()[0]

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (1,))
        self.assertEqual(ptarr[0].shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, 1)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_positional_args_obj(self):
        self.h5file.create_vlarray(self.where, self.name,
                                   None,
                                   self.title,
                                   self.filters,
                                   self.expectedrows,
                                   self.chunkshape,
                                   self.byteorder,
                                   self.createparents,
                                   self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()[0]

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (1,))
        self.assertEqual(ptarr[0].shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, 1)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj(self):
        self.h5file.create_vlarray(self.where, self.name, title=self.title,
                                   obj=self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()[0]

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (1,))
        self.assertEqual(ptarr[0].shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, 1)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_01(self):
        ptarr = self.h5file.create_vlarray(self.where, self.name,
                                           title=self.title,
                                           atom=self.atom)
        ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()[0]

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (1,))
        self.assertEqual(ptarr[0].shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, 1)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_atom_02(self):
        ptarr = self.h5file.create_vlarray(self.where, self.name,
                                           title=self.title,
                                           atom=self.atom)
        #ptarr.append(self.obj)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (0,))
        self.assertEqual(ptarr.nrows, 0)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)

    def test_kwargs_obj_atom(self):
        ptarr = self.h5file.create_vlarray(self.where, self.name,
                                           title=self.title,
                                           obj=self.obj,
                                           atom=self.atom)
        self.h5file.close()

        self.h5file = open_file(self.h5fname)
        ptarr = self.h5file.get_node(self.where, self.name)
        nparr = ptarr.read()[0]

        self.assertEqual(ptarr.title, self.title)
        self.assertEqual(ptarr.shape, (1,))
        self.assertEqual(ptarr[0].shape, self.obj.shape)
        self.assertEqual(ptarr.nrows, 1)
        self.assertEqual(ptarr.atom, self.atom)
        self.assertEqual(ptarr.atom.dtype, self.atom.dtype)
        self.assertTrue(allequal(self.obj, nparr))

    def test_kwargs_obj_atom_error(self):
        atom = Atom.from_dtype(numpy.dtype('complex'))
        #shape = self.shape + self.shape
        self.assertRaises(TypeError,
                          self.h5file.create_vlarray,
                          self.where,
                          self.name,
                          title=self.title,
                          obj=self.obj,
                          atom=atom)


#----------------------------------------------------------------------

def suite():
    theSuite = unittest.TestSuite()
    niter = 1

    for n in range(niter):
        theSuite.addTest(unittest.makeSuite(BasicNumPyTestCase))
        theSuite.addTest(unittest.makeSuite(BasicPythonTestCase))
        theSuite.addTest(unittest.makeSuite(ZlibComprTestCase))
        theSuite.addTest(unittest.makeSuite(BloscComprTestCase))
        theSuite.addTest(unittest.makeSuite(BloscShuffleComprTestCase))
        theSuite.addTest(unittest.makeSuite(BloscBloscLZComprTestCase))
        if 'lz4' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscLZ4ComprTestCase))
            theSuite.addTest(unittest.makeSuite(BloscLZ4HCComprTestCase))
        if 'snappy' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscSnappyComprTestCase))
        if 'zlib' in tables.blosc_compressor_list():
            theSuite.addTest(unittest.makeSuite(BloscZlibComprTestCase))
        theSuite.addTest(unittest.makeSuite(LZOComprTestCase))
        theSuite.addTest(unittest.makeSuite(Bzip2ComprTestCase))
        theSuite.addTest(unittest.makeSuite(TypesReopenTestCase))
        theSuite.addTest(unittest.makeSuite(TypesNoReopenTestCase))
        theSuite.addTest(unittest.makeSuite(MDTypesNumPyTestCase))
        theSuite.addTest(unittest.makeSuite(OpenAppendShapeTestCase))
        theSuite.addTest(unittest.makeSuite(CloseAppendShapeTestCase))
        theSuite.addTest(unittest.makeSuite(PythonFlavorTestCase))
        theSuite.addTest(unittest.makeSuite(NumPyFlavorTestCase))
        theSuite.addTest(unittest.makeSuite(ReadRangeTestCase))
        theSuite.addTest(unittest.makeSuite(GetItemRangeTestCase))
        theSuite.addTest(unittest.makeSuite(SetRangeTestCase))
        theSuite.addTest(unittest.makeSuite(ShuffleComprTestCase))
        theSuite.addTest(unittest.makeSuite(Fletcher32TestCase))
        theSuite.addTest(unittest.makeSuite(AllFiltersTestCase))
        theSuite.addTest(unittest.makeSuite(CloseCopyTestCase))
        theSuite.addTest(unittest.makeSuite(OpenCopyTestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex1TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex2TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex3TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex4TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex5TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex6TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex7TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex8TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex9TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex10TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex11TestCase))
        theSuite.addTest(unittest.makeSuite(CopyIndex12TestCase))
        theSuite.addTest(unittest.makeSuite(ChunkshapeTestCase))
        theSuite.addTest(unittest.makeSuite(VLUEndianTestCase))
        theSuite.addTest(unittest.makeSuite(TruncateOpenTestCase))
        theSuite.addTest(unittest.makeSuite(TruncateCloseTestCase))
        theSuite.addTest(unittest.makeSuite(PointSelectionTestCase))
        theSuite.addTest(unittest.makeSuite(SizeInMemoryPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(SizeOnDiskPropertyTestCase))
        theSuite.addTest(unittest.makeSuite(AccessClosedTestCase))
        theSuite.addTest(unittest.makeSuite(TestCreateVLArrayArgs))

    return theSuite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = undoredo
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: February 15, 2005
# Author:  Ivan Vilata - reverse:net.selidor@ivan
#
# $Source$
# $Id$
#
########################################################################

"""Support for undoing and redoing actions.

Functions:

* undo(file, operation, *args)
* redo(file, operation, *args)
* move_to_shadow(file, path)
* move_from_shadow(file, path)
* attr_to_shadow(file, path, name)
* attr_from_shadow(file, path, name)

Misc variables:

`__docformat__`
    The format of documentation strings in this module.

"""

from tables.path import split_path
from tables._past import previous_api


__docformat__ = 'reStructuredText'
"""The format of documentation strings in this module."""


def undo(file_, operation, *args):
    if operation == 'CREATE':
        undo_create(file_, args[0])
    elif operation == 'REMOVE':
        undo_remove(file_, args[0])
    elif operation == 'MOVE':
        undo_move(file_, args[0], args[1])
    elif operation == 'ADDATTR':
        undo_add_attr(file_, args[0], args[1])
    elif operation == 'DELATTR':
        undo_del_attr(file_, args[0], args[1])
    else:
        raise NotImplementedError("the requested unknown operation %r can "
                                  "not be undone; please report this to the "
                                  "authors" % operation)


def redo(file_, operation, *args):
    if operation == 'CREATE':
        redo_create(file_, args[0])
    elif operation == 'REMOVE':
        redo_remove(file_, args[0])
    elif operation == 'MOVE':
        redo_move(file_, args[0], args[1])
    elif operation == 'ADDATTR':
        redo_add_attr(file_, args[0], args[1])
    elif operation == 'DELATTR':
        redo_del_attr(file_, args[0], args[1])
    else:
        raise NotImplementedError("the requested unknown operation %r can "
                                  "not be redone; please report this to the "
                                  "authors" % operation)


def move_to_shadow(file_, path):
    node = file_._get_node(path)

    (shparent, shname) = file_._shadow_name()
    node._g_move(shparent, shname)

moveToShadow = previous_api(move_to_shadow)


def move_from_shadow(file_, path):
    (shparent, shname) = file_._shadow_name()
    node = shparent._f_get_child(shname)

    (pname, name) = split_path(path)
    parent = file_._get_node(pname)
    node._g_move(parent, name)

moveFromShadow = previous_api(move_from_shadow)


def undo_create(file_, path):
    move_to_shadow(file_, path)

undoCreate = previous_api(undo_create)


def redo_create(file_, path):
    move_from_shadow(file_, path)

redoCreate = previous_api(redo_create)


def undo_remove(file_, path):
    move_from_shadow(file_, path)

undoRemove = previous_api(undo_remove)


def redo_remove(file_, path):
    move_to_shadow(file_, path)

redoRemove = previous_api(redo_remove)


def undo_move(file_, origpath, destpath):
    (origpname, origname) = split_path(origpath)

    node = file_._get_node(destpath)
    origparent = file_._get_node(origpname)
    node._g_move(origparent, origname)

undoMove = previous_api(undo_move)


def redo_move(file_, origpath, destpath):
    (destpname, destname) = split_path(destpath)

    node = file_._get_node(origpath)
    destparent = file_._get_node(destpname)
    node._g_move(destparent, destname)

redoMove = previous_api(redo_move)


def attr_to_shadow(file_, path, name):
    node = file_._get_node(path)
    attrs = node._v_attrs
    value = getattr(attrs, name)

    (shparent, shname) = file_._shadow_name()
    shattrs = shparent._v_attrs

    # Set the attribute only if it has not been kept in the shadow.
    # This avoids re-pickling complex attributes on REDO.
    if not shname in shattrs:
        shattrs._g__setattr(shname, value)

    attrs._g__delattr(name)

attrToShadow = previous_api(attr_to_shadow)


def attr_from_shadow(file_, path, name):
    (shparent, shname) = file_._shadow_name()
    shattrs = shparent._v_attrs
    value = getattr(shattrs, shname)

    node = file_._get_node(path)
    node._v_attrs._g__setattr(name, value)

    # Keeping the attribute in the shadow allows reusing it on Undo/Redo.
    # shattrs._g__delattr(shname)

attrFromShadow = previous_api(attr_from_shadow)


def undo_add_attr(file_, path, name):
    attr_to_shadow(file_, path, name)

undoAddAttr = previous_api(undo_add_attr)


def redo_add_attr(file_, path, name):
    attr_from_shadow(file_, path, name)

redoAddAttr = previous_api(redo_add_attr)


def undo_del_attr(file_, path, name):
    attr_from_shadow(file_, path, name)

undoDelAttr = previous_api(undo_del_attr)


def redo_del_attr(file_, path, name):
    attr_to_shadow(file_, path, name)

redoDelAttr = previous_api(redo_del_attr)


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## End:

########NEW FILE########
__FILENAME__ = unimplemented
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: January 14, 2004
# Author:  Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the UnImplemented class."""

import warnings

from tables import hdf5extension
from tables.utils import SizeType
from tables.node import Node
from tables.leaf import Leaf
from tables._past import previous_api_property


class UnImplemented(hdf5extension.UnImplemented, Leaf):
    """This class represents datasets not supported by PyTables in an HDF5
    file.

    When reading a generic HDF5 file (i.e. one that has not been created with
    PyTables, but with some other HDF5 library based tool), chances are that
    the specific combination of datatypes or dataspaces in some dataset might
    not be supported by PyTables yet. In such a case, this dataset will be
    mapped into an UnImplemented instance and the user will still be able to
    access the complete object tree of the generic HDF5 file. The user will
    also be able to *read and write the attributes* of the dataset, *access
    some of its metadata*, and perform *certain hierarchy manipulation
    operations* like deleting or moving (but not copying) the node. Of course,
    the user will not be able to read the actual data on it.

    This is an elegant way to allow users to work with generic HDF5 files
    despite the fact that some of its datasets are not supported by
    PyTables. However, if you are really interested in having full access to an
    unimplemented dataset, please get in contact with the developer team.

    This class does not have any public instance variables or methods, except
    those inherited from the Leaf class (see :ref:`LeafClassDescr`).

    """

    # Class identifier.
    _c_classid = 'UNIMPLEMENTED'

    _c_classId = previous_api_property('_c_classid')

    def __init__(self, parentnode, name):
        """Create the `UnImplemented` instance."""

        # UnImplemented objects always come from opening an existing node
        # (they can not be created).
        self._v_new = False
        """Is this the first time the node has been created?"""
        self.nrows = SizeType(0)
        """The length of the first dimension of the data."""
        self.shape = (SizeType(0),)
        """The shape of the stored data."""
        self.byteorder = None
        """The endianness of data in memory ('big', 'little' or
        'irrelevant')."""

        super(UnImplemented, self).__init__(parentnode, name)

    def _g_open(self):
        (self.shape, self.byteorder, object_id) = self._open_unimplemented()
        try:
            self.nrows = SizeType(self.shape[0])
        except IndexError:
            self.nrows = SizeType(0)
        return object_id

    def _g_copy(self, newparent, newname, recursive, _log=True, **kwargs):
        """Do nothing.

        This method does nothing, but a ``UserWarning`` is issued.
        Please note that this method *does not return a new node*, but
        ``None``.

        """

        warnings.warn(
            "UnImplemented node %r does not know how to copy itself; skipping"
            % (self._v_pathname,))
        return None  # Can you see it?

    def _f_copy(self, newparent=None, newname=None,
                overwrite=False, recursive=False, createparents=False,
                **kwargs):
        """Do nothing.

        This method does nothing, since `UnImplemented` nodes can not
        be copied.  However, a ``UserWarning`` is issued.  Please note
        that this method *does not return a new node*, but ``None``.

        """

        # This also does nothing but warn.
        self._g_copy(newparent, newname, recursive, **kwargs)
        return None  # Can you see it?

    def __repr__(self):
        return """%s
  NOTE: <The UnImplemented object represents a PyTables unimplemented
         dataset present in the '%s' HDF5 file.  If you want to see this
         kind of HDF5 dataset implemented in PyTables, please contact the
         developers.>
""" % (str(self), self._v_file.filename)


# Classes reported as H5G_UNKNOWN by HDF5
class Unknown(Node):
    """This class represents nodes reported as *unknown* by the underlying
    HDF5 library.

    This class does not have any public instance variables or methods, except
    those inherited from the Node class.

    """

    # Class identifier
    _c_classid = 'UNKNOWN'

    _c_classId = previous_api_property('_c_classid')

    def __init__(self, parentnode, name):
        """Create the `Unknown` instance."""

        self._v_new = False
        super(Unknown, self).__init__(parentnode, name)

    def _g_new(self, parentnode, name, init=False):
        pass

    def _g_open(self):
        return 0

    def _g_copy(self, newparent, newname, recursive, _log=True, **kwargs):
        # Silently avoid doing copies of unknown nodes
        return None

    def _g_delete(self, parent):
        pass

    def __str__(self):
        pathname = self._v_pathname
        classname = self.__class__.__name__
        return "%s (%s)" % (pathname, classname)

    def __repr__(self):
        return """%s
  NOTE: <The Unknown object represents a node which is reported as
         unknown by the underlying HDF5 library, but that might be
         supported in more recent HDF5 versions.>
""" % (str(self))


# These are listed here for backward compatibility with PyTables 0.9.x indexes
class OldIndexArray(UnImplemented):
    _c_classid = 'IndexArray'

    _c_classId = previous_api_property('_c_classid')

########NEW FILE########
__FILENAME__ = utils
# -*- coding: utf-8 -*-

########################################################################
#
#       License: BSD
#       Created: March 4, 2003
#       Author:  Francesc Alted - faltet@pytables.com
#
#       $Id$
#
########################################################################

"""Utility functions."""

from __future__ import print_function
import os
import sys
import warnings
import subprocess
from time import time

import numpy

from tables.flavor import array_of_flavor
from tables._past import previous_api

# The map between byteorders in NumPy and PyTables
byteorders = {
    '>': 'big',
    '<': 'little',
    '=': sys.byteorder,
    '|': 'irrelevant',
}

# The type used for size values: indexes, coordinates, dimension
# lengths, row numbers, shapes, chunk shapes, byte counts...
SizeType = numpy.int64


def correct_byteorder(ptype, byteorder):
    """Fix the byteorder depending on the PyTables types."""

    if ptype in ['string', 'bool', 'int8', 'uint8']:
        return "irrelevant"
    else:
        return byteorder


def is_idx(index):
    """Checks if an object can work as an index or not."""

    if type(index) in (int, long):
        return True
    elif hasattr(index, "__index__"):  # Only works on Python 2.5 (PEP 357)
        # Exclude the array([idx]) as working as an index.  Fixes #303.
        if (hasattr(index, "shape") and index.shape != ()):
            return False
        try:
            index.__index__()
            if isinstance(index, bool):
                warnings.warn(
                    'using a boolean instead of an integer will result in an '
                    'error in the future', DeprecationWarning, stacklevel=2)
            return True
        except TypeError:
            return False
    elif isinstance(index, numpy.integer):
        return True
    # For Python 2.4 one should test 0-dim and 1-dim, 1-elem arrays as well
    elif (isinstance(index, numpy.ndarray) and (index.shape == ()) and
          index.dtype.str[1] == 'i'):
        return True

    return False


def idx2long(index):
    """Convert a possible index into a long int."""

    try:
        return long(index)
    except:
        raise TypeError("not an integer type.")


# This is used in VLArray and EArray to produce NumPy object compliant
# with atom from a generic python type.  If copy is stated as True, it
# is assured that it will return a copy of the object and never the same
# object or a new one sharing the same memory.
def convert_to_np_atom(arr, atom, copy=False):
    """Convert a generic object into a NumPy object compliant with atom."""

    # First, convert the object into a NumPy array
    nparr = array_of_flavor(arr, 'numpy')
    # Copy of data if necessary for getting a contiguous buffer, or if
    # dtype is not the correct one.
    if atom.shape == ():
        # Scalar atom case
        nparr = numpy.array(nparr, dtype=atom.dtype, copy=copy)
    else:
        # Multidimensional atom case.  Addresses #133.
        # We need to use this strange way to obtain a dtype compliant
        # array because NumPy doesn't honor the shape of the dtype when
        # it is multidimensional.  See:
        # http://scipy.org/scipy/numpy/ticket/926
        # for details.
        # All of this is done just to taking advantage of the NumPy
        # broadcasting rules.
        newshape = nparr.shape[:-len(atom.dtype.shape)]
        nparr2 = numpy.empty(newshape, dtype=[('', atom.dtype)])
        nparr2['f0'][:] = nparr
        # Return a view (i.e. get rid of the record type)
        nparr = nparr2.view(atom.dtype)
    return nparr

convertToNPAtom = previous_api(convert_to_np_atom)


# The next is used in Array, EArray and VLArray, and it is a bit more
# high level than convert_to_np_atom
def convert_to_np_atom2(object, atom):
    """Convert a generic object into a NumPy object compliant with atom."""

    # Check whether the object needs to be copied to make the operation
    # safe to in-place conversion.
    copy = atom.type in ['time64']
    nparr = convert_to_np_atom(object, atom, copy)
    # Finally, check the byteorder and change it if needed
    byteorder = byteorders[nparr.dtype.byteorder]
    if (byteorder in ['little', 'big'] and byteorder != sys.byteorder):
        # The byteorder needs to be fixed (a copy is made
        # so that the original array is not modified)
        nparr = nparr.byteswap()

    return nparr

convertToNPAtom2 = previous_api(convert_to_np_atom2)


def check_file_access(filename, mode='r'):
    """Check for file access in the specified `mode`.

    `mode` is one of the modes supported by `File` objects.  If the file
    indicated by `filename` can be accessed using that `mode`, the
    function ends successfully.  Else, an ``IOError`` is raised
    explaining the reason of the failure.

    All this paraphernalia is used to avoid the lengthy and scaring HDF5
    messages produced when there are problems opening a file.  No
    changes are ever made to the file system.

    """

    if mode == 'r':
        # The file should be readable.
        if not os.access(filename, os.F_OK):
            raise IOError("``%s`` does not exist" % (filename,))
        if not os.path.isfile(filename):
            raise IOError("``%s`` is not a regular file" % (filename,))
        if not os.access(filename, os.R_OK):
            raise IOError("file ``%s`` exists but it can not be read"
                          % (filename,))
    elif mode == 'w':
        if os.access(filename, os.F_OK):
            # Since the file is not removed but replaced,
            # it must already be accessible to read and write operations.
            check_file_access(filename, 'r+')
        else:
            # A new file is going to be created,
            # so the directory should be writable.
            parentname = os.path.dirname(filename)
            if not parentname:
                parentname = '.'
            if not os.access(parentname, os.F_OK):
                raise IOError("``%s`` does not exist" % (parentname,))
            if not os.path.isdir(parentname):
                raise IOError("``%s`` is not a directory" % (parentname,))
            if not os.access(parentname, os.W_OK):
                raise IOError("directory ``%s`` exists but it can not be "
                              "written" % (parentname,))
    elif mode == 'a':
        if os.access(filename, os.F_OK):
            check_file_access(filename, 'r+')
        else:
            check_file_access(filename, 'w')
    elif mode == 'r+':
        check_file_access(filename, 'r')
        if not os.access(filename, os.W_OK):
            raise IOError("file ``%s`` exists but it can not be written"
                          % (filename,))
    else:
        raise ValueError("invalid mode: %r" % (mode,))

checkFileAccess = previous_api(check_file_access)


def lazyattr(fget):
    """Create a *lazy attribute* from the result of `fget`.

    This function is intended to be used as a *method decorator*.  It
    returns a *property* which caches the result of calling the `fget`
    instance method.  The docstring of `fget` is used for the property
    itself.  For instance:

    >>> class MyClass(object):
    ...     @lazyattr
    ...     def attribute(self):
    ...         'Attribute description.'
    ...         print('creating value')
    ...         return 10
    ...
    >>> type(MyClass.attribute)
    <type 'property'>
    >>> MyClass.attribute.__doc__
    'Attribute description.'
    >>> obj = MyClass()
    >>> obj.__dict__
    {}
    >>> obj.attribute
    creating value
    10
    >>> obj.__dict__
    {'attribute': 10}
    >>> obj.attribute
    10
    >>> del obj.attribute
    Traceback (most recent call last):
      ...
    AttributeError: can't delete attribute

    .. warning::

        Please note that this decorator *changes the type of the
        decorated object* from an instance method into a property.

    """

    name = fget.__name__

    def newfget(self):
        mydict = self.__dict__
        if name in mydict:
            return mydict[name]
        mydict[name] = value = fget(self)
        return value

    return property(newfget, None, None, fget.__doc__)


def show_stats(explain, tref, encoding=None):
    """Show the used memory (only works for Linux 2.6.x)."""

    if encoding is None:
        encoding = sys.getdefaultencoding()

    # Build the command to obtain memory info
    cmd = "cat /proc/%s/status" % os.getpid()
    sout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout
    for line in sout:
        line = line.decode(encoding)
        if line.startswith("VmSize:"):
            vmsize = int(line.split()[1])
        elif line.startswith("VmRSS:"):
            vmrss = int(line.split()[1])
        elif line.startswith("VmData:"):
            vmdata = int(line.split()[1])
        elif line.startswith("VmStk:"):
            vmstk = int(line.split()[1])
        elif line.startswith("VmExe:"):
            vmexe = int(line.split()[1])
        elif line.startswith("VmLib:"):
            vmlib = int(line.split()[1])
    sout.close()
    print("Memory usage: ******* %s *******" % explain)
    print("VmSize: %7s kB\tVmRSS: %7s kB" % (vmsize, vmrss))
    print("VmData: %7s kB\tVmStk: %7s kB" % (vmdata, vmstk))
    print("VmExe:  %7s kB\tVmLib: %7s kB" % (vmexe, vmlib))
    tnow = time()
    print("WallClock time:", round(tnow - tref, 3))
    return tnow


# truncate data before calling __setitem__, to improve compression ratio
# this function is taken verbatim from netcdf4-python
def quantize(data, least_significant_digit):
    """quantize data to improve compression.

    Data is quantized using around(scale*data)/scale, where scale is
    2**bits, and bits is determined from the least_significant_digit.

    For example, if least_significant_digit=1, bits will be 4.

    """

    precision = pow(10., -least_significant_digit)
    exp = numpy.log10(precision)
    if exp < 0:
        exp = int(numpy.floor(exp))
    else:
        exp = int(numpy.ceil(exp))
    bits = numpy.ceil(numpy.log2(pow(10., -exp)))
    scale = pow(2., bits)
    datout = numpy.around(scale * data) / scale

    return datout


# Utilities to detect leaked instances.  See recipe 14.10 of the Python
# Cookbook by Martelli & Ascher.
tracked_classes = {}
import weakref


def log_instance_creation(instance, name=None):
    if name is None:
        name = instance.__class__.__name__
        if name not in tracked_classes:
            tracked_classes[name] = []
        tracked_classes[name].append(weakref.ref(instance))

logInstanceCreation = previous_api(log_instance_creation)


def string_to_classes(s):
    if s == '*':
        c = sorted(tracked_classes.iterkeys())
        return c
    else:
        return s.split()


def fetch_logged_instances(classes="*"):
    classnames = string_to_classes(classes)
    return [(cn, len(tracked_classes[cn])) for cn in classnames]

fetchLoggedInstances = previous_api(fetch_logged_instances)


def count_logged_instances(classes, file=sys.stdout):
    for classname in string_to_classes(classes):
        file.write("%s: %d\n" % (classname, len(tracked_classes[classname])))

countLoggedInstances = previous_api(count_logged_instances)


def list_logged_instances(classes, file=sys.stdout):
    for classname in string_to_classes(classes):
        file.write('\n%s:\n' % classname)
        for ref in tracked_classes[classname]:
            obj = ref()
            if obj is not None:
                file.write('    %s\n' % repr(obj))

listLoggedInstances = previous_api(list_logged_instances)


def dump_logged_instances(classes, file=sys.stdout):
    for classname in string_to_classes(classes):
        file.write('\n%s:\n' % classname)
        for ref in tracked_classes[classname]:
            obj = ref()
            if obj is not None:
                file.write('    %s:\n' % obj)
                for key, value in obj.__dict__.iteritems():
                    file.write('        %20s : %s\n' % (key, value))

dumpLoggedInstances = previous_api(dump_logged_instances)


#
# A class useful for cache usage
#
class CacheDict(dict):
    """A dictionary that prevents itself from growing too much."""

    def __init__(self, maxentries):
        self.maxentries = maxentries
        super(CacheDict, self).__init__(self)

    def __setitem__(self, key, value):
        # Protection against growing the cache too much
        if len(self) > self.maxentries:
            # Remove a 10% of (arbitrary) elements from the cache
            entries_to_remove = self.maxentries / 10
            for k in self.keys()[:entries_to_remove]:
                super(CacheDict, self).__delitem__(k)
        super(CacheDict, self).__setitem__(key, value)


class NailedDict(object):
    """A dictionary which ignores its items when it has nails on it."""

    def __init__(self, maxentries):
        self.maxentries = maxentries
        self._cache = {}
        self._nailcount = 0

    # Only a restricted set of dictionary methods are supported.  That
    # is why we buy instead of inherit.

    # The following are intended to be used by ``Table`` code changing
    # the set of usable indexes.

    def clear(self):
        self._cache.clear()

    def nail(self):
        self._nailcount += 1

    def unnail(self):
        self._nailcount -= 1

    # The following are intended to be used by ``Table`` code handling
    # conditions.

    def __contains__(self, key):
        if self._nailcount > 0:
            return False
        return key in self._cache

    def __getitem__(self, key):
        if self._nailcount > 0:
            raise KeyError(key)
        return self._cache[key]

    def get(self, key, default=None):
        if self._nailcount > 0:
            return default
        return self._cache.get(key, default)

    def __setitem__(self, key, value):
        if self._nailcount > 0:
            return
        cache = self._cache
        # Protection against growing the cache too much
        if len(cache) > self.maxentries:
            # Remove a 10% of (arbitrary) elements from the cache
            entries_to_remove = self.maxentries // 10
            for k in cache.keys()[:entries_to_remove]:
                del cache[k]
        cache[key] = value


def detect_number_of_cores():
    """Detects the number of cores on a system.

    Cribbed from pp.

    """

    # Linux, Unix and MacOS:
    if hasattr(os, "sysconf"):
        if "SC_NPROCESSORS_ONLN" in os.sysconf_names:
            # Linux & Unix:
            ncpus = os.sysconf("SC_NPROCESSORS_ONLN")
            if isinstance(ncpus, int) and ncpus > 0:
                return ncpus
        else:  # OSX:
            return int(os.popen2("sysctl -n hw.ncpu")[1].read())
    # Windows:
    if "NUMBER_OF_PROCESSORS" in os.environ:
        ncpus = int(os.environ["NUMBER_OF_PROCESSORS"])
        if ncpus > 0:
            return ncpus
    return 1  # Default

detectNumberOfCores = previous_api(detect_number_of_cores)


# Main part
# =========
def _test():
    """Run ``doctest`` on this module."""

    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()


## Local Variables:
## mode: python
## py-indent-offset: 4
## tab-width: 4
## fill-column: 72
## End:

########NEW FILE########
__FILENAME__ = utilsExtension
from warnings import warn
from tables.utilsextension import *

_warnmsg = ("utilsExtension is pending deprecation, import utilsextension instead. "
            "You may use the pt2to3 tool to update your source code.")
warn(_warnmsg, DeprecationWarning, stacklevel=2)

########NEW FILE########
__FILENAME__ = vlarray
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: November 12, 2003
# Author: Francesc Alted - faltet@pytables.com
#
# $Id$
#
########################################################################

"""Here is defined the VLArray class."""

import sys

import numpy

from tables import hdf5extension
from tables.utils import (convert_to_np_atom, convert_to_np_atom2, idx2long,
                          correct_byteorder, SizeType, is_idx, lazyattr)


from tables.atom import ObjectAtom, VLStringAtom, VLUnicodeAtom
from tables.flavor import internal_to_flavor
from tables.leaf import Leaf, calc_chunksize
from tables._past import previous_api, previous_api_property

# default version for VLARRAY objects
# obversion = "1.0"    # initial version
# obversion = "1.0"    # add support for complex datatypes
# obversion = "1.1"    # This adds support for time datatypes.
# obversion = "1.2"    # This adds support for enumerated datatypes.
# obversion = "1.3"     # Introduced 'PSEUDOATOM' attribute.
obversion = "1.4"    # Numeric and numarray flavors are gone.


class VLArray(hdf5extension.VLArray, Leaf):
    """This class represents variable length (ragged) arrays in an HDF5 file.

    Instances of this class represent array objects in the object tree
    with the property that their rows can have a *variable* number of
    homogeneous elements, called *atoms*. Like Table datasets (see
    :ref:`TableClassDescr`), variable length arrays can have only one
    dimension, and the elements (atoms) of their rows can be fully
    multidimensional.

    When reading a range of rows from a VLArray, you will *always* get
    a Python list of objects of the current flavor (each of them for a
    row), which may have different lengths.

    This class provides methods to write or read data to or from
    variable length array objects in the file. Note that it also
    inherits all the public attributes and methods that Leaf (see
    :ref:`LeafClassDescr`) already provides.

    .. note::
    
          VLArray objects also support compression although compression
          is only performed on the data structures used internally by
          the HDF5 to take references of the location of the variable
          length data. Data itself (the raw data) are not compressed
          or filtered.
          
          Please refer to the `VLTypes Technical Note
          <http://www.hdfgroup.org/HDF5/doc/TechNotes/VLTypes.html>`_
          for more details on the topic.
          
    Parameters
    ----------
    parentnode
        The parent :class:`Group` object.

        .. versionchanged:: 3.0
           Renamed from *parentNode* to *parentnode*.

    name : str
        The name of this node in its parent group.
    atom
        An `Atom` instance representing the *type* and *shape* of the atomic
        objects to be saved.
    title
        A description for this node (it sets the ``TITLE`` HDF5 attribute on
        disk).
    filters
        An instance of the `Filters` class that provides information about the
        desired I/O filters to be applied during the life of this object.
    expectedrows
        A user estimate about the number of row elements that will
        be added to the growable dimension in the `VLArray` node.
        If not provided, the default value is ``EXPECTED_ROWS_VLARRAY``
        (see ``tables/parameters.py``).  If you plan to create either
        a much smaller or a much bigger `VLArray` try providing a guess;
        this will optimize the HDF5 B-Tree creation and management
        process time and the amount of memory used.

        .. versionadded:: 3.0

    chunkshape
        The shape of the data chunk to be read or written in a single HDF5 I/O
        operation.  Filters are applied to those chunks of data.  The
        dimensionality of `chunkshape` must be 1.  If ``None``, a sensible
        value is calculated (which is recommended).
    byteorder
        The byteorder of the data *on disk*, specified as 'little' or 'big'.
        If this is not specified, the byteorder is that of the platform.

    .. versionchanged:: 3.0
       The *expectedsizeinMB* parameter has been replaced by *expectedrows*.

    Examples
    --------
    See below a small example of the use of the VLArray class.  The code is
    available in :file:`examples/vlarray1.py`::

        import tables
        from numpy import *

        # Create a VLArray:
        fileh = tables.open_file('vlarray1.h5', mode='w')
        vlarray = fileh.create_vlarray(fileh.root, 'vlarray1',
        tables.Int32Atom(shape=()),
                        "ragged array of ints",
                        filters=tables.Filters(1))

        # Append some (variable length) rows:
        vlarray.append(array([5, 6]))
        vlarray.append(array([5, 6, 7]))
        vlarray.append([5, 6, 9, 8])

        # Now, read it through an iterator:
        print('-->', vlarray.title)
        for x in vlarray:
            print('%s[%d]--> %s' % (vlarray.name, vlarray.nrow, x))

        # Now, do the same with native Python strings.
        vlarray2 = fileh.create_vlarray(fileh.root, 'vlarray2',
        tables.StringAtom(itemsize=2),
                            "ragged array of strings",
                            filters=tables.Filters(1))
        vlarray2.flavor = 'python'

        # Append some (variable length) rows:
        print('-->', vlarray2.title)
        vlarray2.append(['5', '66'])
        vlarray2.append(['5', '6', '77'])
        vlarray2.append(['5', '6', '9', '88'])

        # Now, read it through an iterator:
        for x in vlarray2:
            print('%s[%d]--> %s' % (vlarray2.name, vlarray2.nrow, x))

        # Close the file.
        fileh.close()

    The output for the previous script is something like::

        --> ragged array of ints
        vlarray1[0]--> [5 6]
        vlarray1[1]--> [5 6 7]
        vlarray1[2]--> [5 6 9 8]
        --> ragged array of strings
        vlarray2[0]--> ['5', '66']
        vlarray2[1]--> ['5', '6', '77']
        vlarray2[2]--> ['5', '6', '9', '88']


    .. rubric:: VLArray attributes

    The instance variables below are provided in addition to those in
    Leaf (see :ref:`LeafClassDescr`).

    .. attribute:: atom

        An Atom (see :ref:`AtomClassDescr`)
        instance representing the *type* and
        *shape* of the atomic objects to be
        saved. You may use a *pseudo-atom* for
        storing a serialized object or variable length string per row.

    .. attribute:: flavor

        The type of data object read from this leaf.

        Please note that when reading several rows of VLArray data,
        the flavor only applies to the *components* of the returned
        Python list, not to the list itself.

    .. attribute:: nrow

        On iterators, this is the index of the current row.

    .. attribute:: nrows

        The current number of rows in the array.

    .. attribute:: extdim

       The index of the enlargeable dimension (always 0 for vlarrays).

    """

    # Class identifier.
    _c_classid = 'VLARRAY'

    _c_classId = previous_api_property('_c_classid')

    # Lazy read-only attributes
    # `````````````````````````
    @lazyattr
    def dtype(self):
        """The NumPy ``dtype`` that most closely matches this array."""
        return self.atom.dtype

    # Properties
    # ~~~~~~~~~~
    shape = property(
        lambda self: (self.nrows,), None, None,
        "The shape of the stored array.")

    def _get_size_on_disk(self):
        raise NotImplementedError('size_on_disk not implemented for VLArrays')

    size_on_disk = property(_get_size_on_disk, None, None,
                            """
        The HDF5 library does not include a function to determine size_on_disk
        for variable-length arrays.  Accessing this attribute will raise a
        NotImplementedError.
        """)

    size_in_memory = property(
        lambda self: self._get_memory_size(), None, None,
        """
        The size of this array's data in bytes when it is fully loaded
        into memory.

        .. note::

            When data is stored in a VLArray using the ObjectAtom type,
            it is first serialized using pickle, and then converted to
            a NumPy array suitable for storage in an HDF5 file.
            This attribute will return the size of that NumPy
            representation.  If you wish to know the size of the Python
            objects after they are loaded from disk, you can use this
            `ActiveState recipe
            <http://code.activestate.com/recipes/577504/>`_.
        """)

    # Other methods
    # ~~~~~~~~~~~~~
    def __init__(self, parentnode, name, atom=None, title="",
                 filters=None, expectedrows=None,
                 chunkshape=None, byteorder=None,
                 _log=True):

        self._v_version = None
        """The object version of this array."""

        self._v_new = new = atom is not None
        """Is this the first time the node has been created?"""

        self._v_new_title = title
        """New title for this node."""

        self._v_new_filters = filters
        """New filter properties for this array."""

        if expectedrows is None:
            expectedrows = parentnode._v_file.params['EXPECTED_ROWS_VLARRAY']
        self._v_expectedrows = expectedrows
        """The expected number of rows to be stored in the array.

        .. versionadded:: 3.0

        """

        self._v_chunkshape = None
        """Private storage for the `chunkshape` property of Leaf."""

        # Miscellaneous iteration rubbish.
        self._start = None
        """Starting row for the current iteration."""

        self._stop = None
        """Stopping row for the current iteration."""

        self._step = None
        """Step size for the current iteration."""

        self._nrowsread = None
        """Number of rows read up to the current state of iteration."""

        self._startb = None
        """Starting row for current buffer."""

        self._stopb = None
        """Stopping row for current buffer. """

        self._row = None
        """Current row in iterators (sentinel)."""

        self._init = False
        """Whether we are in the middle of an iteration or not (sentinel)."""

        self.listarr = None
        """Current buffer in iterators."""

        # Documented (*public*) attributes.
        self.atom = atom
        """
        An Atom (see :ref:`AtomClassDescr`) instance representing the
        *type* and *shape* of the atomic objects to be saved. You may
        use a *pseudo-atom* for storing a serialized object or
        variable length string per row.
        """
        self.nrow = None
        """On iterators, this is the index of the current row."""

        self.nrows = None
        """The current number of rows in the array."""

        self.extdim = 0   # VLArray only have one dimension currently
        """The index of the enlargeable dimension (always 0 for vlarrays)."""

        # Check the chunkshape parameter
        if new and chunkshape is not None:
            if isinstance(chunkshape, (int, numpy.integer, long)):
                chunkshape = (chunkshape,)
            try:
                chunkshape = tuple(chunkshape)
            except TypeError:
                raise TypeError(
                    "`chunkshape` parameter must be an integer or sequence "
                    "and you passed a %s" % type(chunkshape))
            if len(chunkshape) != 1:
                raise ValueError("`chunkshape` rank (length) must be 1: %r"
                                 % (chunkshape,))
            self._v_chunkshape = tuple(SizeType(s) for s in chunkshape)

        super(VLArray, self).__init__(parentnode, name, new, filters,
                                      byteorder, _log)

    def _g_post_init_hook(self):
        super(VLArray, self)._g_post_init_hook()
        self.nrowsinbuf = 100  # maybe enough for most applications

    # This is too specific for moving it into Leaf
    def _calc_chunkshape(self, expectedrows):
        """Calculate the size for the HDF5 chunk."""

        # For computing the chunkshape for HDF5 VL types, we have to
        # choose the itemsize of the *each* element of the atom and
        # not the size of the entire atom.  I don't know why this
        # should be like this, perhaps I should report this to the
        # HDF5 list.
        # F. Alted 2006-11-23
        # elemsize = self.atom.atomsize()
        elemsize = self._basesize

        # AV 2013-05-03
        # This is just a quick workaround tha allows to change the API for
        # PyTables 3.0 release and remove the expected_mb parameter.
        # The algorithm for computing the chunkshape should be rewritten as
        # requested by gh-35.
        expected_mb = expectedrows * elemsize / 1024. ** 2

        chunksize = calc_chunksize(expected_mb)

        # Set the chunkshape
        chunkshape = chunksize // elemsize
        # Safeguard against itemsizes being extremely large
        if chunkshape == 0:
            chunkshape = 1
        return (SizeType(chunkshape),)

    def _g_create(self):
        """Create a variable length array (ragged array)."""

        atom = self.atom
        self._v_version = obversion
        # Check for zero dims in atom shape (not allowed in VLArrays)
        zerodims = numpy.sum(numpy.array(atom.shape) == 0)
        if zerodims > 0:
            raise ValueError("When creating VLArrays, none of the dimensions "
                             "of the Atom instance can be zero.")

        if not hasattr(atom, 'size'):  # it is a pseudo-atom
            self._atomicdtype = atom.base.dtype
            self._atomicsize = atom.base.size
            self._basesize = atom.base.itemsize
        else:
            self._atomicdtype = atom.dtype
            self._atomicsize = atom.size
            self._basesize = atom.itemsize
        self._atomictype = atom.type
        self._atomicshape = atom.shape

        # Compute the optimal chunkshape, if needed
        if self._v_chunkshape is None:
            self._v_chunkshape = self._calc_chunkshape(self._v_expectedrows)

        self.nrows = SizeType(0)     # No rows at creation time

        # Correct the byteorder if needed
        if self.byteorder is None:
            self.byteorder = correct_byteorder(atom.type, sys.byteorder)

        # After creating the vlarray, ``self._v_objectid`` needs to be
        # set because it is needed for setting attributes afterwards.
        self._v_objectid = self._create_array(self._v_new_title)

        # Add an attribute in case we have a pseudo-atom so that we
        # can retrieve the proper class after a re-opening operation.
        if not hasattr(atom, 'size'):  # it is a pseudo-atom
            self.attrs.PSEUDOATOM = atom.kind

        return self._v_objectid

    def _g_open(self):
        """Get the metadata info for an array in file."""

        self._v_objectid, self.nrows, self._v_chunkshape, atom = \
            self._open_array()

        # Check if the atom can be a PseudoAtom
        if "PSEUDOATOM" in self.attrs:
            kind = self.attrs.PSEUDOATOM
            if kind == 'vlstring':
                atom = VLStringAtom()
            elif kind == 'vlunicode':
                atom = VLUnicodeAtom()
            elif kind == 'object':
                atom = ObjectAtom()
            else:
                raise ValueError(
                    "pseudo-atom name ``%s`` not known." % kind)
        elif self._v_file.format_version[:1] == "1":
            flavor1x = self.attrs.FLAVOR
            if flavor1x == "VLString":
                atom = VLStringAtom()
            elif flavor1x == "Object":
                atom = ObjectAtom()

        self.atom = atom
        return self._v_objectid

    def _getnobjects(self, nparr):
        """Return the number of objects in a NumPy array."""

        # Check for zero dimensionality array
        zerodims = numpy.sum(numpy.array(nparr.shape) == 0)
        if zerodims > 0:
            # No objects to be added
            return 0
        shape = nparr.shape
        atom_shape = self.atom.shape
        shapelen = len(nparr.shape)
        if isinstance(atom_shape, tuple):
            atomshapelen = len(self.atom.shape)
        else:
            atom_shape = (self.atom.shape,)
            atomshapelen = 1
        diflen = shapelen - atomshapelen
        if shape == atom_shape:
            nobjects = 1
        elif (diflen == 1 and shape[diflen:] == atom_shape):
            # Check if the leading dimensions are all ones
            # if shape[:diflen-1] == (1,)*(diflen-1):
            #    nobjects = shape[diflen-1]
            #    shape = shape[diflen:]
            # It's better to accept only inputs with the exact dimensionality
            # i.e. a dimensionality only 1 element larger than atom
            nobjects = shape[0]
            shape = shape[1:]
        elif atom_shape == (1,) and shapelen == 1:
            # Case where shape = (N,) and shape_atom = 1 or (1,)
            nobjects = shape[0]
        else:
            raise ValueError("The object '%s' is composed of elements with "
                             "shape '%s', which is not compatible with the "
                             "atom shape ('%s')." % (nparr, shape, atom_shape))
        return nobjects

    def get_enum(self):
        """Get the enumerated type associated with this array.

        If this array is of an enumerated type, the corresponding Enum instance
        (see :ref:`EnumClassDescr`) is returned. If it is not of an enumerated
        type, a TypeError is raised.

        """

        if self.atom.kind != 'enum':
            raise TypeError("array ``%s`` is not of an enumerated type"
                            % self._v_pathname)

        return self.atom.enum

    getEnum = previous_api(get_enum)

    def append(self, sequence):
        """Add a sequence of data to the end of the dataset.

        This method appends the objects in the sequence to a *single row* in
        this array. The type and shape of individual objects must be compliant
        with the atoms in the array. In the case of serialized objects and
        variable length strings, the object or string to append is itself the
        sequence.

        """

        self._g_check_open()
        self._v_file._check_writable()

        # Prepare the sequence to convert it into a NumPy object
        atom = self.atom
        if not hasattr(atom, 'size'):  # it is a pseudo-atom
            sequence = atom.toarray(sequence)
            statom = atom.base
        else:
            try:  # fastest check in most cases
                len(sequence)
            except TypeError:
                raise TypeError("argument is not a sequence")
            statom = atom

        if len(sequence) > 0:
            # The sequence needs to be copied to make the operation safe
            # to in-place conversion.
            nparr = convert_to_np_atom2(sequence, statom)
            nobjects = self._getnobjects(nparr)
        else:
            nobjects = 0
            nparr = None

        self._append(nparr, nobjects)
        self.nrows += 1

    def iterrows(self, start=None, stop=None, step=None):
        """Iterate over the rows of the array.

        This method returns an iterator yielding an object of the current
        flavor for each selected row in the array.

        If a range is not supplied, *all the rows* in the array are iterated
        upon. You can also use the :meth:`VLArray.__iter__` special method for
        that purpose.  If you only want to iterate over a given *range of rows*
        in the array, you may use the start, stop and step parameters.

        Examples
        --------

        ::

            for row in vlarray.iterrows(step=4):
                print('%s[%d]--> %s' % (vlarray.name, vlarray.nrow, row))

        .. versionchanged:: 3.0
           If the *start* parameter is provided and *stop* is None then the
           array is iterated from *start* to the last line.
           In PyTables < 3.0 only one element was returned.

        """

        (self._start, self._stop, self._step) = self._process_range(
            start, stop, step)
        self._init_loop()
        return self

    def __iter__(self):
        """Iterate over the rows of the array.

        This is equivalent to calling :meth:`VLArray.iterrows` with default
        arguments, i.e. it iterates over *all the rows* in the array.

        Examples
        --------

        ::

            result = [row for row in vlarray]

        Which is equivalent to::

            result = [row for row in vlarray.iterrows()]

        """

        if not self._init:
            # If the iterator is called directly, assign default variables
            self._start = 0
            self._stop = self.nrows
            self._step = 1
            # and initialize the loop
            self._init_loop()

        return self

    def _init_loop(self):
        """Initialization for the __iter__ iterator."""

        self._nrowsread = self._start
        self._startb = self._start
        self._row = -1   # Sentinel
        self._init = True  # Sentinel
        self.nrow = SizeType(self._start - self._step)    # row number

    _initLoop = previous_api(_init_loop)

    def next(self):
        """Get the next element of the array during an iteration.

        The element is returned as a list of objects of the current
        flavor.

        """

        if self._nrowsread >= self._stop:
            self._init = False
            raise StopIteration        # end of iteration
        else:
            # Read a chunk of rows
            if self._row + 1 >= self.nrowsinbuf or self._row < 0:
                self._stopb = self._startb + self._step * self.nrowsinbuf
                self.listarr = self.read(self._startb, self._stopb, self._step)
                self._row = -1
                self._startb = self._stopb
            self._row += 1
            self.nrow += self._step
            self._nrowsread += self._step
            return self.listarr[self._row]

    def __getitem__(self, key):
        """Get a row or a range of rows from the array.

        If key argument is an integer, the corresponding array row is returned
        as an object of the current flavor.  If key is a slice, the range of
        rows determined by it is returned as a list of objects of the current
        flavor.

        In addition, NumPy-style point selections are supported.  In
        particular, if key is a list of row coordinates, the set of rows
        determined by it is returned.  Furthermore, if key is an array of
        boolean values, only the coordinates where key is True are returned.
        Note that for the latter to work it is necessary that key list would
        contain exactly as many rows as the array has.

        Examples
        --------

        ::

            a_row = vlarray[4]
            a_list = vlarray[4:1000:2]
            a_list2 = vlarray[[0,2]]   # get list of coords
            a_list3 = vlarray[[0,-2]]  # negative values accepted
            a_list4 = vlarray[numpy.array([True,...,False])]  # array of bools

        """

        self._g_check_open()
        if is_idx(key):
            # Index out of range protection
            if key >= self.nrows:
                raise IndexError("Index out of range")
            if key < 0:
                # To support negative values
                key += self.nrows
            (start, stop, step) = self._process_range(key, key + 1, 1)
            return self.read(start, stop, step)[0]
        elif isinstance(key, slice):
            start, stop, step = self._process_range(
                key.start, key.stop, key.step)
            return self.read(start, stop, step)
        # Try with a boolean or point selection
        elif type(key) in (list, tuple) or isinstance(key, numpy.ndarray):
            coords = self._point_selection(key)
            return self._read_coordinates(coords)
        else:
            raise IndexError("Invalid index or slice: %r" % (key,))

    def _assign_values(self, coords, values):
        """Assign the `values` to the positions stated in `coords`."""

        for nrow, value in zip(coords, values):
            if nrow >= self.nrows:
                raise IndexError("First index out of range")
            if nrow < 0:
                # To support negative values
                nrow += self.nrows
            object_ = value
            # Prepare the object to convert it into a NumPy object
            atom = self.atom
            if not hasattr(atom, 'size'):  # it is a pseudo-atom
                object_ = atom.toarray(object_)
                statom = atom.base
            else:
                statom = atom
            value = convert_to_np_atom(object_, statom)
            nobjects = self._getnobjects(value)

            # Get the previous value
            nrow = idx2long(
                nrow)   # To convert any possible numpy scalar value
            nparr = self._read_array(nrow, nrow + 1, 1)[0]
            nobjects = len(nparr)
            if len(value) > nobjects:
                raise ValueError("Length of value (%s) is larger than number "
                                 "of elements in row (%s)" % (len(value),
                                                              nobjects))
            try:
                nparr[:] = value
            except Exception as exc:  # XXX
                raise ValueError("Value parameter:\n'%r'\n"
                                 "cannot be converted into an array object "
                                 "compliant vlarray[%s] row: \n'%r'\n"
                                 "The error was: <%s>" % (value, nrow,
                                                          nparr[:], exc))

            if nparr.size > 0:
                self._modify(nrow, nparr, nobjects)

    def __setitem__(self, key, value):
        """Set a row, or set of rows, in the array.

        It takes different actions depending on the type of the *key*
        parameter: if it is an integer, the corresponding table row is
        set to *value* (a record or sequence capable of being converted
        to the table structure).  If *key* is a slice, the row slice
        determined by it is set to *value* (a record array or sequence
        of rows capable of being converted to the table structure).

        In addition, NumPy-style point selections are supported.  In
        particular, if key is a list of row coordinates, the set of rows
        determined by it is set to value.  Furthermore, if key is an array of
        boolean values, only the coordinates where key is True are set to
        values from value.  Note that for the latter to work it is necessary
        that key list would contain exactly as many rows as the table has.

        .. note::

            When updating the rows of a VLArray object which uses a
            pseudo-atom, there is a problem: you can only update values
            with *exactly* the same size in bytes than the original row.
            This is very difficult to meet with object pseudo-atoms,
            because :mod:`pickle` applied on a Python object does not
            guarantee to return the same number of bytes than over another
            object, even if they are of the same class.
            This effectively limits the kinds of objects than can be
            updated in variable-length arrays.

        Examples
        --------

        ::

            vlarray[0] = vlarray[0] * 2 + 3
            vlarray[99] = arange(96) * 2 + 3

            # Negative values for the index are supported.
            vlarray[-99] = vlarray[5] * 2 + 3
            vlarray[1:30:2] = list_of_rows
            vlarray[[1,3]] = new_1_and_3_rows

        """

        self._g_check_open()
        self._v_file._check_writable()

        if is_idx(key):
            # If key is not a sequence, convert to it
            coords = [key]
            value = [value]
        elif isinstance(key, slice):
            (start, stop, step) = self._process_range(
                key.start, key.stop, key.step)
            coords = range(start, stop, step)
        # Try with a boolean or point selection
        elif type(key) in (list, tuple) or isinstance(key, numpy.ndarray):
            coords = self._point_selection(key)
        else:
            raise IndexError("Invalid index or slice: %r" % (key,))

        # Do the assignment row by row
        self._assign_values(coords, value)

    # Accessor for the _read_array method in superclass
    def read(self, start=None, stop=None, step=1):
        """Get data in the array as a list of objects of the current flavor.

        Please note that, as the lengths of the different rows are variable,
        the returned value is a *Python list* (not an array of the current
        flavor), with as many entries as specified rows in the range
        parameters.

        The start, stop and step parameters can be used to select only a
        *range of rows* in the array.  Their meanings are the same as in
        the built-in range() Python function, except that negative values
        of step are not allowed yet. Moreover, if only start is specified,
        then stop will be set to start + 1. If you do not specify neither
        start nor stop, then *all the rows* in the array are selected.

        """

        self._g_check_open()
        start, stop, step = self._process_range_read(start, stop, step)
        if start == stop:
            listarr = []
        else:
            listarr = self._read_array(start, stop, step)

        atom = self.atom
        if not hasattr(atom, 'size'):  # it is a pseudo-atom
            outlistarr = [atom.fromarray(arr) for arr in listarr]
        else:
            # Convert the list to the right flavor
            flavor = self.flavor
            outlistarr = [internal_to_flavor(arr, flavor) for arr in listarr]
        return outlistarr

    def _read_coordinates(self, coords):
        """Read rows specified in `coords`."""
        rows = []
        for coord in coords:
            rows.append(self.read(coord)[0])
        return rows

    def _g_copy_with_stats(self, group, name, start, stop, step,
                           title, filters, chunkshape, _log, **kwargs):
        """Private part of Leaf.copy() for each kind of leaf."""

        # Build the new VLArray object
        object = VLArray(
            group, name, self.atom, title=title, filters=filters,
            expectedrows=self._v_expectedrows, chunkshape=chunkshape,
            _log=_log)

        # Now, fill the new vlarray with values from the old one
        # This is not buffered because we cannot forsee the length
        # of each record. So, the safest would be a copy row by row.
        # In the future, some analysis can be done in order to buffer
        # the copy process.
        nrowsinbuf = 1
        (start, stop, step) = self._process_range_read(start, stop, step)
        # Optimized version (no conversions, no type and shape checks, etc...)
        nrowscopied = SizeType(0)
        nbytes = 0
        if not hasattr(self.atom, 'size'):  # it is a pseudo-atom
            atomsize = self.atom.base.size
        else:
            atomsize = self.atom.size
        for start2 in xrange(start, stop, step * nrowsinbuf):
            # Save the records on disk
            stop2 = start2 + step * nrowsinbuf
            if stop2 > stop:
                stop2 = stop
            nparr = self._read_array(start=start2, stop=stop2, step=step)[0]
            nobjects = nparr.shape[0]
            object._append(nparr, nobjects)
            nbytes += nobjects * atomsize
            nrowscopied += 1
        object.nrows = nrowscopied
        return (object, nbytes)

    _g_copyWithStats = previous_api(_g_copy_with_stats)

    def __repr__(self):
        """This provides more metainfo in addition to standard __str__"""

        return """%s
  atom = %r
  byteorder = %r
  nrows = %s
  flavor = %r""" % (self, self.atom, self.byteorder, self.nrows,
                    self.flavor)

########NEW FILE########
__FILENAME__ = _past
# -*- coding: utf-8 -*-

########################################################################
#
# License: BSD
# Created: April 9, 2013
# Author:  Anthony Scopatz - scopatz@gmail.com
#
# $Id$
#
########################################################################

"""A module with no PyTables dependencies that helps with deprecation
warnings."""
from inspect import getmembers, ismethod, isfunction
from warnings import warn


def previous_api(obj):
    """A decorator-like function for dealing with deprecations."""
    if not (ismethod(obj) or isfunction(obj)):
        # punt if not a function or method
        return obj
    for key, value in getmembers(obj):
        if key == '__name__':
            newname = value
            break
    oldname = new2oldnames[newname]
    warnmsg = ("{0}() is pending deprecation, use {1}() instead. "
               "You may use the pt2to3 tool to update your source code.")
    warnmsg = warnmsg.format(oldname, newname)

    def oldfunc(*args, **kwargs):
        warn(warnmsg, DeprecationWarning, stacklevel=2)
        return obj(*args, **kwargs)
    oldfunc.__doc__ = (
        obj.__doc__ or '') + "\n\n.. warning::\n\n    " + warnmsg + "\n"
    return oldfunc


def previous_api_property(newname):
    oldname = new2oldnames[newname]

    warnmsg = ("{0} is pending deprecation, use {1} instead. "
               "You may use the pt2to3 tool to update your source code.")
    warnmsg = warnmsg.format(oldname, newname)

    def _getter(self):
        warn(warnmsg, DeprecationWarning, stacklevel=1)
        return getattr(self, newname)

    def _setter(self, value):
        warn(warnmsg, DeprecationWarning, stacklevel=1)
        return setattr(self, newname, value)

    _getter.__name__ = _setter.__name__ = oldname

    doc = '.. deprecated:: 3.0\n\n' + warnmsg
    return property(_getter, _setter, None, doc=doc)


# old name, new name
old2newnames = dict([
    # from __init__.py
    ('hdf5Version', 'hdf5_version'),                    # data
    # from array.py
    ('parentNode', 'parentnode'),                       # kwarg
    ('getEnum', 'get_enum'),
    ('_initLoop', '_init_loop'),
    ('_fancySelection', '_fancy_selection'),
    ('_checkShape', '_check_shape'),
    ('_readSlice', '_read_slice'),
    ('_readCoords', '_read_coords'),
    ('_readSelection', '_read_selection'),
    ('_writeSlice', '_write_slice'),
    ('_writeCoords', '_write_coords'),
    ('_writeSelection', '_write_selection'),
    ('_g_copyWithStats', '_g_copy_with_stats'),
    ('_c_classId', '_c_classid'),                       # attr
    # from atom.py
    ('_checkBase', '_checkbase'),
    # from attributeset.py
    ('newSet', 'newset'),                               # kwarg
    ('copyClass', 'copyclass'),                         # kwarg
    ('_g_updateNodeLocation', '_g_update_node_location'),
    ('_g_logAdd', '_g_log_add'),
    ('_g_delAndLog', '_g_del_and_log'),
    ('_v__nodeFile', '_v__nodefile'),                   # attr (private)
    ('_v__nodePath', '_v__nodepath'),                   # attr (private)
    # from carray.py
    #('parentNode', 'parentnode'),                       # kwarg
    # from description.py
    ('_g_setNestedNamesDescr', '_g_set_nested_names_descr'),
    ('_g_setPathNames', '_g_set_path_names'),
    ('_v_colObjects', '_v_colobjects'),                 # attr
    ('_v_nestedFormats', '_v_nested_formats'),          # attr
    ('_v_nestedNames', '_v_nested_names'),              # attr
    ('_v_nestedDescr', '_v_nested_descr'),              # attr
    ('getColsInOrder', 'get_cols_in_order'),
    ('joinPaths', 'join_paths'),
    ('metaIsDescription', 'MetaIsDescription'),
    # from earray.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('_checkShapeAppend', '_check_shape_append'),
    # from expression.py
    ('_exprvarsCache', '_exprvars_cache'),              # attr (private)
    ('_requiredExprVars', '_required_expr_vars'),
    ('setInputsRange', 'set_inputs_range'),
    ('setOutput', 'set_output'),
    ('setOutputRange', 'set_output_range'),
    # from file.py
    ('_opToCode', '_op_to_code'),                       # data (private)
    ('_codeToOp', '_code_to_op'),                       # data (private)
    ('_transVersion', '_trans_version'),                # data (private)
    ('_transGroupParent', '_trans_group_parent'),       # data (private)
    ('_transGroupName', '_trans_group_name'),           # data (private)
    ('_transGroupPath', '_trans_group_path'),           # data (private)
    ('_actionLogParent', '_action_log_parent'),         # data (private)
    ('_actionLogName', '_action_log_name'),             # data (private)
    ('_actionLogPath', '_action_log_path'),             # data (private)
    ('_transParent', '_trans_parent'),                  # data (private)
    ('_transName', '_trans_name'),                      # data (private)
    ('_transPath', '_trans_path'),                      # data (private)
    ('_shadowParent', '_shadow_parent'),                # data (private)
    ('_shadowName', '_shadow_name'),                    # data (private)
    ('_shadowPath', '_shadow_path'),                    # data (private)
    ('copyFile', 'copy_file'),
    ('openFile', 'open_file'),
    ('_getValueFromContainer', '_get_value_from_container'),
    ('__getRootGroup', '__get_root_group'),
    ('rootUEP', 'root_uep'),                            # attr
    ('_getOrCreatePath', '_get_or_create_path'),
    ('_createPath', '_create_path'),
    ('createGroup', 'create_group'),
    ('createTable', 'create_table'),
    ('createArray', 'create_array'),
    ('createCArray', 'create_carray'),
    ('createEArray', 'create_earray'),
    ('createVLArray', 'create_vlarray'),
    ('createHardLink', 'create_hard_link'),
    ('createSoftLink', 'create_soft_link'),
    ('createExternalLink', 'create_external_link'),
    ('_getNode', '_get_node'),
    ('getNode', 'get_node'),
    ('isVisibleNode', 'is_visible_node'),
    ('renameNode', 'rename_node'),
    ('moveNode', 'move_node'),
    ('copyNode', 'copy_node'),
    ('removeNode', 'remove_node'),
    ('getNodeAttr', 'get_node_attr'),
    ('setNodeAttr', 'set_node_attr'),
    ('delNodeAttr', 'del_node_attr'),
    ('copyNodeAttrs', 'copy_node_attrs'),
    ('copyChildren', 'copy_children'),
    ('listNodes', 'list_nodes'),
    ('iterNodes', 'iter_nodes'),
    ('walkNodes', 'walk_nodes'),
    ('walkGroups', 'walk_groups'),
    ('_checkOpen', '_check_open'),
    ('_isWritable', '_iswritable'),
    ('_checkWritable', '_check_writable'),
    ('_checkGroup', '_check_group'),
    ('isUndoEnabled', 'is_undo_enabled'),
    ('_checkUndoEnabled', '_check_undo_enabled'),
    ('_createTransactionGroup', '_create_transaction_group'),
    ('_createTransaction', '_create_transaction'),
    ('_createMark', '_create_mark'),
    ('enableUndo', 'enable_undo'),
    ('disableUndo', 'disable_undo'),
    ('_getMarkID', '_get_mark_id'),
    ('_getFinalAction', '_get_final_action'),
    ('getCurrentMark', 'get_current_mark'),
    ('_updateNodeLocations', '_update_node_locations'),
    # from group.py
    #('parentNode', 'parentnode'),                       # kwarg
    #('ptFile', 'ptfile'),                               # kwarg
    ('_getValueFromContainer', '_get_value_from_container'),
    ('_g_postInitHook', '_g_post_init_hook'),
    ('_g_getChildGroupClass', '_g_get_child_group_class'),
    ('_g_getChildLeafClass', '_g_get_child_leaf_class'),
    ('_g_addChildrenNames', '_g_add_children_names'),
    ('_g_checkHasChild', '_g_check_has_child'),
    ('_f_walkNodes', '_f_walknodes'),
    ('_g_widthWarning', '_g_width_warning'),
    ('_g_refNode', '_g_refnode'),
    ('_g_unrefNode', '_g_unrefnode'),
    ('_g_copyChildren', '_g_copy_children'),
    ('_f_getChild', '_f_get_child'),
    ('_f_listNodes', '_f_list_nodes'),
    ('_f_iterNodes', '_f_iter_nodes'),
    ('_f_walkGroups', '_f_walk_groups'),
    ('_g_closeDescendents', '_g_close_descendents'),
    ('_f_copyChildren', '_f_copy_children'),
    ('_v_maxGroupWidth', '_v_max_group_width'),         # attr
    ('_v_objectID', '_v_objectid'),                     # attr
    ('_g_loadChild', '_g_load_child'),
    ('childName', 'childname'),                         # ???
    ('_c_shadowNameRE', '_c_shadow_name_re'),           # attr (private)
    # from hdf5extension.p{yx,xd}
    ('hdf5Extension', 'hdf5extension'),
    ('_getFileId', '_get_file_id'),
    ('_flushFile', '_flush_file'),
    ('_closeFile', '_close_file'),
    ('_g_listAttr', '_g_list_attr'),
    ('_g_setAttr', '_g_setattr'),
    ('_g_getAttr', '_g_getattr'),
    ('_g_listGroup', '_g_list_group'),
    ('_g_getGChildAttr', '_g_get_gchild_attr'),
    ('_g_getLChildAttr', '_g_get_lchild_attr'),
    ('_g_flushGroup', '_g_flush_group'),
    ('_g_closeGroup', '_g_close_group'),
    ('_g_moveNode', '_g_move_node'),
    ('_convertTime64', '_convert_time64'),
    ('_createArray', '_create_array'),
    ('_createCArray', '_create_carray'),
    ('_openArray', '_open_array'),
    ('_readArray', '_read_array'),
    ('_g_readSlice', '_g_read_slice'),
    ('_g_readCoords', '_g_read_coords'),
    ('_g_readSelection', '_g_read_selection'),
    ('_g_writeSlice', '_g_write_slice'),
    ('_g_writeCoords', '_g_write_coords'),
    ('_g_writeSelection', '_g_write_selection'),
    # from idxutils.py
    ('calcChunksize', 'calc_chunksize'),
    ('infinityF', 'infinityf'),                         # data
    ('infinityMap', 'infinitymap'),                     # data
    ('infType', 'inftype'),
    ('StringNextAfter', 'string_next_after'),
    ('IntTypeNextAfter', 'int_type_next_after'),
    ('BoolTypeNextAfter', 'bool_type_next_after'),
    # from index.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('defaultAutoIndex', 'default_auto_index'),         # data
    ('defaultIndexFilters', 'default_index_filters'),   # data
    ('_tableColumnPathnameOfIndex', '_table_column_pathname_of_index'),
    ('_is_CSI', '_is_csi'),
    ('is_CSI', 'is_csi'),                               # property
    ('appendLastRow', 'append_last_row'),
    ('read_sliceLR', 'read_slice_lr'),
    ('readSorted', 'read_sorted'),
    ('readIndices', 'read_indices'),
    ('_processRange', '_process_range'),
    ('searchLastRow', 'search_last_row'),
    ('getLookupRange', 'get_lookup_range'),
    ('_g_checkName', '_g_check_name'),
    # from indexes.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('_searchBin', '_search_bin'),
    # from indexesextension
    ('indexesExtension', 'indexesextension'),
    ('initRead', 'initread'),
    ('readSlice', 'read_slice'),
    ('_readIndexSlice', '_read_index_slice'),
    ('_initSortedSlice', '_init_sorted_slice'),
    ('_g_readSortedSlice', '_g_read_sorted_slice'),
    ('_readSortedSlice', '_read_sorted_slice'),
    ('getLRUbounds', 'get_lru_bounds'),
    ('getLRUsorted', 'get_lru_sorted'),
    ('_searchBinNA_b', '_search_bin_na_b'),
    ('_searchBinNA_ub', '_search_bin_na_ub'),
    ('_searchBinNA_s', '_search_bin_na_s'),
    ('_searchBinNA_us', '_search_bin_na_us'),
    ('_searchBinNA_i', '_search_bin_na_i'),
    ('_searchBinNA_ui', '_search_bin_na_ui'),
    ('_searchBinNA_ll', '_search_bin_na_ll'),
    ('_searchBinNA_ull', '_search_bin_na_ull'),
    ('_searchBinNA_e', '_search_bin_na_e'),
    ('_searchBinNA_f', '_search_bin_na_f'),
    ('_searchBinNA_d', '_search_bin_na_d'),
    ('_searchBinNA_g', '_search_bin_na_g'),
    # from leaf.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('objectID', 'object_id'),                          # property
    ('_processRangeRead', '_process_range_read'),
    ('_pointSelection', '_point_selection'),
    ('isVisible', 'isvisible'),
    ('getAttr', 'get_attr'),
    ('setAttr', 'set_attr'),
    ('delAttr', 'del_attr'),
    # from link.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('_g_getLinkClass', '_g_get_link_class'),
    # from linkextension
    ('linkExtension', 'linkextension'),
    ('_getLinkClass', '_get_link_class'),
    ('_g_createHardLink', '_g_create_hard_link'),
    # from lrucacheextension
    ('lrucacheExtension', 'lrucacheextension'),
    # from misc/enum.py
    ('_checkAndSetPair', '_check_and_set_pair'),
    ('_getContainer', '_get_container'),
    # from misc/proxydict.py
    ('containerRef', 'containerref'),                   # attr
    # from node.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('_g_logCreate', '_g_log_create'),
    ('_g_preKillHook', '_g_pre_kill_hook'),
    ('_g_checkOpen', '_g_check_open'),
    ('_g_setLocation', '_g_set_location'),
    ('_g_updateLocation', '_g_update_location'),
    ('_g_delLocation', '_g_del_location'),
    ('_g_updateDependent', '_g_update_dependent'),
    ('_g_removeAndLog', '_g_remove_and_log'),
    ('_g_logMove', '_g_log_move'),
    ('oldPathname', 'oldpathname'),                     # ??
    ('_g_copyAsChild', '_g_copy_as_child'),
    ('_f_isVisible', '_f_isvisible'),
    ('_g_checkGroup', '_g_check_group'),
    ('_g_checkNotContains', '_g_check_not_contains'),
    ('_g_maybeRemove', '_g_maybe_remove'),
    ('_f_getAttr', '_f_getattr'),
    ('_f_setAttr', '_f_setattr'),
    ('_f_delAttr', '_f_delattr'),
    ('_v_maxTreeDepth', '_v_maxtreedepth'),             # attr
    # from nodes/filenode.py
    ('newNode', 'new_node'),
    ('openNode', 'open_node'),
    ('_lineChunkSize', '_line_chunksize'),              # attr (private)
    ('_lineSeparator', '_line_separator'),              # attr (private)
    #('getLineSeparator', 'get_line_separator'),        # dropped
    #('setLineSeparator', 'set_line_separator'),        # dropped
    #('delLineSeparator', 'del_line_separator'),        # dropped
    #('lineSeparator', 'line_separator'),                # property -- dropped
    ('_notReadableError', '_not_readable_error'),
    ('_appendZeros', '_append_zeros'),
    ('getAttrs', '_get_attrs'),
    ('setAttrs', '_set_attrs'),
    ('delAttrs', '_del_attrs'),
    ('_setAttributes', '_set_attributes'),
    ('_checkAttributes', '_check_attributes'),
    ('_checkNotClosed', '_check_not_closed'),
    ('__allowedInitKwArgs', '__allowed_init_kwargs'),   # attr (private)
    ('_byteShape', '_byte_shape'),                      # attr (private)
    ('_sizeToShape', '_size_to_shape'),                 # attr (private)
    ('_vType', '_vtype'),                               # attr (private)
    ('_vShape', '_vshape'),                             # attr (private)
    # from path.py
    ('parentPath', 'parentpath'),                       # kwarg
    ('_pythonIdRE', '_python_id_re'),                   # attr (private)
    ('_reservedIdRE', '_reserved_id_re'),               # attr (private)
    ('_hiddenNameRE', '_hidden_name_re'),               # attr (private)
    ('_hiddenPathRE', '_hidden_path_re'),               # attr (private)
    ('checkNameValidity', 'check_name_validity'),
    ('joinPath', 'join_path'),
    ('splitPath', 'split_path'),
    ('isVisibleName', 'isvisiblename'),
    ('isVisiblePath', 'isvisiblepath'),
    # from registry.py
    ('className', 'classname'),                         # kwarg
    ('classNameDict', 'class_name_dict'),               # data
    ('classIdDict', 'class_id_dict'),                   # data
    ('getClassByName', 'get_class_by_name'),
    # from scripts/ptdump.py
    ('dumpLeaf', 'dump_leaf'),
    ('dumpGroup', 'dump_group'),
    # from scripts/ptrepack.py
    ('newdstGroup', 'newdst_group'),
    ('recreateIndexes', 'recreate_indexes'),
    ('copyLeaf', 'copy_leaf'),
    # from table.py
    #('parentNode', 'parentnode'),                       # kwarg
    ('_nxTypeFromNPType', '_nxtype_from_nptype'),       # data (private)
    ('_npSizeType', '_npsizetype'),                     # data (private)
    ('_indexNameOf', '_index_name_of'),
    ('_indexPathnameOf', '_index_pathname_of'),
    ('_indexPathnameOfColumn', '_index_pathname_of_column'),
    ('_indexNameOf_', '_index_name_of_'),
    ('_indexPathnameOf_', '_index_pathname_of_'),
    ('_indexPathnameOfColumn_', '_index_pathname_of_column_'),
    ('_table__setautoIndex', '_table__setautoindex'),
    ('_table__getautoIndex', '_table__getautoindex'),
    ('_table__autoIndex', '_table__autoindex'),         # data (private)
    ('_table__whereIndexed', '_table__where_indexed'),
    ('createIndexesTable', 'create_indexes_table'),
    ('createIndexesDescr', 'create_indexes_descr'),
    ('_column__createIndex', '_column__create_index'),
    ('_autoIndex', '_autoindex'),                       # attr
    ('autoIndex', 'autoindex'),                         # attr
    ('_useIndex', '_use_index'),
    ('_whereCondition', '_where_condition'),            # attr (private)
    ('_conditionCache', '_condition_cache'),            # attr (private)
    #('_exprvarsCache', '_exprvars_cache'),
    ('_enabledIndexingInQueries', '_enabled_indexing_in_queries'),  # attr (private)
    ('_emptyArrayCache', '_empty_array_cache'),         # attr (private)
    ('_getTypeColNames', '_get_type_col_names'),
    ('_getEnumMap', '_get_enum_map'),
    ('_cacheDescriptionData', '_cache_description_data'),
    ('_getColumnInstance', '_get_column_instance'),
    ('_checkColumn', '_check_column'),
    ('_disableIndexingInQueries', '_disable_indexing_in_queries'),
    ('_enableIndexingInQueries', '_enable_indexing_in_queries'),
    #('_requiredExprVars', '_required_expr_vars'),
    ('_getConditionKey', '_get_condition_key'),
    ('_compileCondition', '_compile_condition'),
    ('willQueryUseIndexing', 'will_query_use_indexing'),
    ('readWhere', 'read_where'),
    ('whereAppend', 'append_where'),
    ('getWhereList', 'get_where_list'),
    ('_check_sortby_CSI', '_check_sortby_csi'),
    ('_readCoordinates', '_read_coordinates'),
    ('readCoordinates', 'read_coordinates'),
    ('_saveBufferedRows', '_save_buffered_rows'),
    ('modifyCoordinates', 'modify_coordinates'),
    ('modifyRows', 'modify_rows'),
    ('modifyColumn', 'modify_column'),
    ('modifyColumns', 'modify_columns'),
    ('flushRowsToIndex', 'flush_rows_to_index'),
    ('_addRowsToIndex', '_add_rows_to_index'),
    ('removeRows', 'remove_rows'),
    ('_setColumnIndexing', '_set_column_indexing'),
    ('_markColumnsAsDirty', '_mark_columns_as_dirty'),
    ('_reIndex', '_reindex'),
    ('_doReIndex', '_do_reindex'),
    ('reIndex', 'reindex'),
    ('reIndexDirty', 'reindex_dirty'),
    ('_g_copyRows', '_g_copy_rows'),
    ('_g_copyRows_optim', '_g_copy_rows_optim'),
    ('_g_propIndexes', '_g_prop_indexes'),
    ('_g_updateTableLocation', '_g_update_table_location'),
    ('_tableFile', '_table_file'),                      # attr (private)
    ('_tablePath', '_table_path'),                      # attr (private)
    ('createIndex', 'create_index'),
    ('createCSIndex', 'create_csindex'),
    ('removeIndex', 'remove_index'),
    # from tableextension
    ('tableExtension', 'tableextension'),
    ('getNestedFieldCache', 'get_nested_field_cache'),
    ('getNestedType', 'get_nested_type'),
    ('_createTable', '_create_table'),
    ('_getInfo', '_get_info'),
    ('indexChunk', 'indexchunk'),                       # attr
    ('indexValid', 'indexvalid'),                       # attr
    ('indexValues', 'indexvalues'),                     # attr
    ('bufcoordsData', 'bufcoords_data'),                # attr
    ('indexValuesData', 'index_values_data'),           # attr
    ('chunkmapData', 'chunkmap_data'),                  # attr
    ('indexValidData', 'index_valid_data'),             # attr
    ('whereCond', 'wherecond'),                         # attr
    ('iterseqMaxElements', 'iterseq_max_elements'),     # attr
    ('IObuf', 'iobuf'),                                 # attr
    ('IObufcpy', 'iobufcpy'),                           # attr
    ('_convertTime64_', '_convert_time64_'),
    ('_convertTypes', '_convert_types'),
    ('_newBuffer', '_new_buffer'),
    ('__next__inKernel', '__next__inkernel'),
    ('_fillCol', '_fill_col'),
    ('_flushBufferedRows', '_flush_buffered_rows'),
    ('_getUnsavedNrows', '_get_unsaved_nrows'),
    ('_flushModRows', '_flush_mod_rows'),
    # from undoredo.py
    ('moveToShadow', 'move_to_shadow'),
    ('moveFromShadow', 'move_from_shadow'),
    ('undoCreate', 'undo_create'),
    ('redoCreate', 'redo_create'),
    ('undoRemove', 'undo_remove'),
    ('redoRemove', 'redo_remove'),
    ('undoMove', 'undo_move'),
    ('redoMove', 'redo_move'),
    ('attrToShadow', 'attr_to_shadow'),
    ('attrFromShadow', 'attr_from_shadow'),
    ('undoAddAttr', 'undo_add_attr'),
    ('redoAddAttr', 'redo_add_attr'),
    ('undoDelAttr', 'undo_del_attr'),
    ('redoDelAttr', 'redo_del_attr'),
    # from utils.py
    ('convertToNPAtom', 'convert_to_np_atom'),
    ('convertToNPAtom2', 'convert_to_np_atom2'),
    ('checkFileAccess', 'check_file_access'),
    ('logInstanceCreation', 'log_instance_creation'),
    ('fetchLoggedInstances', 'fetch_logged_instances'),
    ('countLoggedInstances', 'count_logged_instances'),
    ('listLoggedInstances', 'list_logged_instances'),
    ('dumpLoggedInstances', 'dump_logged_instances'),
    ('detectNumberOfCores', 'detect_number_of_cores'),
    # from utilsextension
    ('utilsExtension', 'utilsextension'),
    ('PTTypeToHDF5', 'pttype_to_hdf5'),                 # data
    ('PTSpecialKinds', 'pt_special_kinds'),             # data
    ('NPExtPrefixesToPTKinds', 'npext_prefixes_to_ptkinds'),    # data
    ('HDF5ClassToString', 'hdf5_class_to_string'),      # data
    ('setBloscMaxThreads', 'set_blosc_max_threads'),
    ('silenceHDF5Messages', 'silence_hdf5_messages'),
    ('isHDF5File', 'is_hdf5_file'),
    ('isPyTablesFile', 'is_pytables_file'),
    ('getHDF5Version', 'get_hdf5_version'),
    ('getPyTablesVersion', 'get_pytables_version'),
    ('whichLibVersion', 'which_lib_version'),
    ('whichClass', 'which_class'),
    ('getNestedField', 'get_nested_field'),
    ('getIndices', 'get_indices'),
    ('getFilters', 'get_filters'),
    ('getTypeEnum', 'get_type_enum'),
    ('enumFromHDF5', 'enum_from_hdf5'),
    ('enumToHDF5', 'enum_to_hdf5'),
    ('AtomToHDF5Type', 'atom_to_hdf5_type'),
    ('loadEnum', 'load_enum'),
    ('HDF5ToNPNestedType', 'hdf5_to_np_nested_type'),
    ('HDF5ToNPExtType', 'hdf5_to_np_ext_type'),
    ('AtomFromHDF5Type', 'atom_from_hdf5_type'),
    ('createNestedType', 'create_nested_type'),
    # from unimlemented.py
    ('_openUnImplemented', '_open_unimplemented'),
    # from vlarray.py
    #('parentNode', 'parentnode'),                       # kwarg
    #('expectedsizeinMB', 'expected_mb'),                # --> expectedrows
    #('_v_expectedsizeinMB', '_v_expected_mb'),          # --> expectedrows
])

new2oldnames = dict([(v, k) for k, v in old2newnames.iteritems()])

########NEW FILE########

__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Grab documentation build configuration file, created by
# sphinx-quickstart on Tue Nov  9 11:04:59 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os
ROOT = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('_themes'))
sys.path.insert(0, ROOT)

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Grab'
copyright = u'2011, Grigoriy Petukhov'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
try:
    import grab
    version = '.'.join(map(str, grab.version_info[:2]))
    release = grab.__version__
except ImportError:
    version = ''
    release = ''
    

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {
    #'index_logo': '',
#}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Grabdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Grab.tex', u'Grab Documentation',
   u'Grigority Petukhov', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'grab', u'Grab Documentation',
     [u'Grigority Petukhov'], 1)
]

########NEW FILE########
__FILENAME__ = flask_theme_support
# flasky extensions.  flasky pygments style based on tango style
from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Punctuation, Other, Literal


class FlaskyStyle(Style):
    background_color = "#f8f8f8"
    default_style = ""

    styles = {
        # No corresponding class for the following:
        #Text:                     "", # class:  ''
        Whitespace:                "underline #f8f8f8",      # class: 'w'
        Error:                     "#a40000 border:#ef2929", # class: 'err'
        Other:                     "#000000",                # class 'x'

        Comment:                   "italic #8f5902", # class: 'c'
        Comment.Preproc:           "noitalic",       # class: 'cp'

        Keyword:                   "bold #004461",   # class: 'k'
        Keyword.Constant:          "bold #004461",   # class: 'kc'
        Keyword.Declaration:       "bold #004461",   # class: 'kd'
        Keyword.Namespace:         "bold #004461",   # class: 'kn'
        Keyword.Pseudo:            "bold #004461",   # class: 'kp'
        Keyword.Reserved:          "bold #004461",   # class: 'kr'
        Keyword.Type:              "bold #004461",   # class: 'kt'

        Operator:                  "#582800",   # class: 'o'
        Operator.Word:             "bold #004461",   # class: 'ow' - like keywords

        Punctuation:               "bold #000000",   # class: 'p'

        # because special names such as Name.Class, Name.Function, etc.
        # are not recognized as such later in the parsing, we choose them
        # to look the same as ordinary variables.
        Name:                      "#000000",        # class: 'n'
        Name.Attribute:            "#c4a000",        # class: 'na' - to be revised
        Name.Builtin:              "#004461",        # class: 'nb'
        Name.Builtin.Pseudo:       "#3465a4",        # class: 'bp'
        Name.Class:                "#000000",        # class: 'nc' - to be revised
        Name.Constant:             "#000000",        # class: 'no' - to be revised
        Name.Decorator:            "#888",           # class: 'nd' - to be revised
        Name.Entity:               "#ce5c00",        # class: 'ni'
        Name.Exception:            "bold #cc0000",   # class: 'ne'
        Name.Function:             "#000000",        # class: 'nf'
        Name.Property:             "#000000",        # class: 'py'
        Name.Label:                "#f57900",        # class: 'nl'
        Name.Namespace:            "#000000",        # class: 'nn' - to be revised
        Name.Other:                "#000000",        # class: 'nx'
        Name.Tag:                  "bold #004461",   # class: 'nt' - like a keyword
        Name.Variable:             "#000000",        # class: 'nv' - to be revised
        Name.Variable.Class:       "#000000",        # class: 'vc' - to be revised
        Name.Variable.Global:      "#000000",        # class: 'vg' - to be revised
        Name.Variable.Instance:    "#000000",        # class: 'vi' - to be revised

        Number:                    "#990000",        # class: 'm'

        Literal:                   "#000000",        # class: 'l'
        Literal.Date:              "#000000",        # class: 'ld'

        String:                    "#4e9a06",        # class: 's'
        String.Backtick:           "#4e9a06",        # class: 'sb'
        String.Char:               "#4e9a06",        # class: 'sc'
        String.Doc:                "italic #8f5902", # class: 'sd' - like a comment
        String.Double:             "#4e9a06",        # class: 's2'
        String.Escape:             "#4e9a06",        # class: 'se'
        String.Heredoc:            "#4e9a06",        # class: 'sh'
        String.Interpol:           "#4e9a06",        # class: 'si'
        String.Other:              "#4e9a06",        # class: 'sx'
        String.Regex:              "#4e9a06",        # class: 'sr'
        String.Single:             "#4e9a06",        # class: 's1'
        String.Symbol:             "#4e9a06",        # class: 'ss'

        Generic:                   "#000000",        # class: 'g'
        Generic.Deleted:           "#a40000",        # class: 'gd'
        Generic.Emph:              "italic #000000", # class: 'ge'
        Generic.Error:             "#ef2929",        # class: 'gr'
        Generic.Heading:           "bold #000080",   # class: 'gh'
        Generic.Inserted:          "#00A000",        # class: 'gi'
        Generic.Output:            "#888",           # class: 'go'
        Generic.Prompt:            "#745334",        # class: 'gp'
        Generic.Strong:            "bold #000000",   # class: 'gs'
        Generic.Subheading:        "bold #800080",   # class: 'gu'
        Generic.Traceback:         "bold #a40000",   # class: 'gt'
    }

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Grab documentation build configuration file, created by
# sphinx-quickstart on Sun Sep  8 00:59:26 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os
#ROOT = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Grab'
copyright = u'2013, Gregory Petukhov'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4'
# The full version, including alpha/beta/rc tags.
release = '0.4'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = []#'_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Grabdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Grab.tex', u'Grab Documentation',
   u'Gregory Petukhov', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'grab', u'Grab Documentation',
     [u'Gregory Petukhov'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Grab', u'Grab Documentation',
   u'Gregory Petukhov', 'Grab', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = base
# -*- coding: utf-8 -*-
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
"""
The core of grab package: the Grab class.
"""
import logging
import os
from random import randint
from copy import copy, deepcopy
import threading
import itertools
import collections
try:
    from urlparse import urljoin
except ImportError:
    from urllib.parse import urljoin
import email
from datetime import datetime
import weakref

from grab.tools.html import find_refresh_url, find_base_url
from grab.document import Document
from grab import error
from grab.tools.http import normalize_http_values
from grab.cookie import CookieManager
from grab.proxy import ProxyList, parse_proxy_line
from grab.deprecated import DeprecatedThings
from grab.kit_interface import GrabKitInterface
from grab.ext.form import FormExtension

from grab.util.py2old_support import *
from grab.util.py3k_support import *

__all__ = ('Grab',)
# This counter will used in enumerating network queries.
# Its value will be displayed in logging messages and also used
# in names of dumps
# I use mutable module variable to allow different
# instances of Grab maintain single counter
# This could be helpful in debugging when your script
# creates multiple Grab instances - in case of shared counter
# grab instances do not overwrite dump logs
REQUEST_COUNTER = itertools.count(1)

GLOBAL_STATE = {
    'dom_build_time': 0,
    'selector_time': 0,
}
MUTABLE_CONFIG_KEYS = ['post', 'multipart_post', 'headers', 'cookies']
TRANSPORT_CACHE = {}

logger = logging.getLogger('grab.base')
# Logger to handle network activity
# It is done as separate logger to allow you easily
# control network logging separately from other grab logs
logger_network = logging.getLogger('grab.network')


def reset_request_counter():
    global REQUEST_COUNTER

    REQUEST_COUNTER = itertools.count(1)


def copy_config(config, mutable_config_keys=MUTABLE_CONFIG_KEYS):
    """
    Copy grab config with correct handling of mutable config values.
    """

    cloned_config = copy(config)
    # Apply ``copy`` function to mutable config values
    for key in mutable_config_keys:
        cloned_config[key] = copy(config[key])
    return cloned_config


def default_config():
    # TODO: Maybe config should be splitted into two entities:
    # 1) config which is not changed during request
    # 2) changable settings
    return dict(
        # Common
        url = None,

        # Debugging
        log_file = None,
        log_dir = False,
        debug_post = False,
        debug_post_limit = 150,
        # Only for curl transport
        debug = False,
        verbose_logging = False,

        # Only for selenium transport
        webdriver = 'firefox',
        selenium_wait = 1,  # in seconds

        # Proxy
        proxy = None,
        proxy_type = None,
        proxy_userpwd = None,
        proxy_auto_change = True,

        # Method, Post
        method = None,
        post = None,
        multipart_post = None,

        # Headers, User-Agent, Referer
        headers = {},
        common_headers = {},
        user_agent = None,
        user_agent_file = None,
        referer = None,
        reuse_referer = True,

        # Cookies
        cookies = {},
        reuse_cookies = True,
        cookiefile = None,

        # Timeouts
        timeout = 15,
        connect_timeout = 3,

        # Connection
        connection_reuse = True,

        # Response processing
        nobody = False,
        body_maxsize = None,
        body_inmemory = True,
        body_storage_dir = None,
        body_storage_filename = None,
        reject_file_size = None,

        # Content compression
        encoding = 'gzip',

        # Network interface
        interface = None,

        # Redirects
        follow_refresh = False,
        follow_location = True,
        refresh_redirect_count = 0,
        redirect_limit = 10,

        # Authentication
        userpwd = None,

        # Character set to which any unicode data should be encoded
        # before get placed in request
        # This setting is overwritten after each request with
        # charset of rertreived document
        charset = 'utf-8',

        # Charset to use for converting content of response
        # into unicode, by default it is detected automatically
        document_charset = None,

        # Conent type control how DOM are built
        # For html type HTML DOM builder is used
        # For xml type XML DOM builder is used
        content_type = 'html',

        # Fix &#X; entities, where X between 128 and 160
        # Such entities are parsed by modern browsers as
        # windows-1251 entities independently of the real charset of
        # the document, If this option is True then such entities
        # will be replaced with correct unicode entitites e.g.:
        # &#151; ->  &#8212;
        fix_special_entities = True,

        # Convert document body to lower case before bulding LXML tree
        # It does not affect `self.doc.body`
        lowercased_tree = False,

        # Strip null bytes from document body before building lXML tree
        # It does not affect `self.doc.body`
        strip_null_bytes = True,

        # Internal object to store
        state = {},
    )


class Grab(FormExtension, DeprecatedThings):

    __slots__ = ('request_head', 'request_log', 'request_body',
                 'proxylist', 'config', '_request_prepared',
                 'clone_counter', 'transport',
                 'transport_param', 'request_method', 'request_counter',
                 '__weakref__', 'cookies',

                 # Dirst hack to make it possbile to inherit Grab from
                 # multiple base classes with __slots__
                 '_lxml_form', '_file_fields',
                 '_pyquery', '_doc', '_kit',
                 )

    # Attributes which should be processed when clone
    # of Grab instance is creating
    clonable_attributes = ('request_head', 'request_log', 'request_body',
                           'proxylist')

    # Complex config items which points to mutable objects
    mutable_config_keys = copy(MUTABLE_CONFIG_KEYS)

    """
    Public methods
    """

    def __init__(self, document_body=None, transport='grab.transport.curl.CurlTransport',
                 **kwargs):
        """
        Create Grab instance
        """

        self._doc = None
        self.config = default_config()
        self.config['common_headers'] = self.common_headers()
        self._request_prepared = False
        self.cookies = CookieManager()
        self.proxylist = ProxyList()

        self.setup_transport(transport)

        self.reset()

        if kwargs:
            self.setup(**kwargs)
        self.clone_counter = 0
        if document_body is not None:
            self.setup_document(document_body)

    def _get_doc(self):
        if self._doc is None:
            self._doc = Document(self)
        return self._doc

    def _set_doc(self, obj):
        self._doc = obj

    doc = property(_get_doc, _set_doc)

    def setup_transport(self, transport_param):
        self.transport_param = transport_param
        if isinstance(transport_param, basestring):
            mod_path, cls_name = transport_param.rsplit('.', 1)
            try:
                cls = TRANSPORT_CACHE[(mod_path, cls_name)]
            except KeyError:
                mod = __import__(mod_path, globals(), locals(), ['foo'])
                cls = getattr(mod, cls_name)
                TRANSPORT_CACHE[(mod_path, cls_name)] = cls
            self.transport = cls()
        elif isinstance(transport_param, collections.Callable):
            self.transport = transport_param()
        else:
            raise error.GrabMisuseError('Option `transport` should be string or callable. '
                                        'Got %s' % type(transport_param))

    def reset(self):
        """
        Reset all attributes which could be modified during previous request
        or which is not initialized yet if this is the new Grab instance.

        This methods is automatically called before each network request.
        """

        self.request_head = None
        self.request_log = None
        self.request_body = None

        self.request_method = None
        self.transport.reset()

        # KIT
        self._kit = None
        # Form extension
        self._lxml_form = None
        self._file_fields = {}


    def clone(self, **kwargs):
        """
        Create clone of Grab instance.

        Cloned instance will have the same state: cookies, referer, response document data

        :param **kwargs: overrides settings of cloned grab instance
        """

        g = Grab(transport=self.transport_param)
        g.config = self.dump_config()

        g.doc = self.doc.copy()
        g.doc.grab = weakref.proxy(g)

        for key in self.clonable_attributes:
            setattr(g, key, getattr(self, key))
        g.cookies = deepcopy(self.cookies)
        g.clone_counter = self.clone_counter + 1

        if kwargs:
            g.setup(**kwargs)

        return g

    def adopt(self, g):
        """
        Copy the state of another `Grab` instance.

        Use case: create backup of current state to the cloned instance and
        then restore the state from it.
        """

        self.load_config(g.config)

        self.doc = g.doc.copy(new_grab=self)

        for key in self.clonable_attributes:
            setattr(self, key, getattr(g, key))
            self.cookies = deepcopy(g.cookies)
        self.clone_counter = g.clone_counter + 1

    def dump_config(self):
        """
        Make clone of current config.
        """

        conf = copy_config(self.config, self.mutable_config_keys)
        conf['state'] = {
            'cookiejar_cookies': list(self.cookies.cookiejar),
        }
        return conf

    def load_config(self, config):
        """
        Configure grab instance with external config object.
        """

        self.config = copy_config(config, self.mutable_config_keys)
        if 'cookiejar_cookies' in config['state']:
            self.cookies = CookieManager.from_cookie_list(config['state']['cookiejar_cookies'])

    def setup(self, **kwargs):
        """
        Setting up Grab instance configuration.
        """

        if 'hammer_mode' in kwargs:
            logging.error('Option hammer_mode is deprecated. Grab does not support hammer mode anymore.')
            del kwargs['hammer_mode']

        if 'hammer_timeouts' in kwargs:
            logging.error('Option hammer_timeouts is deprecated. Grab does not support hammer mode anymore.')
            del kwargs['hammer_timeouts']

        for key in kwargs:
            if not key in self.config.keys():
                raise error.GrabMisuseError('Unknown option: %s' % key)

        if 'url' in kwargs:
            if self.config.get('url'):
                kwargs['url'] = self.make_url_absolute(kwargs['url'])
        self.config.update(kwargs)

    def go(self, url, **kwargs):
        """
        Go to ``url``

        Args:
            :url: could be absolute or relative. If relative then t will be appended to the
                absolute URL of previous request.
        """

        return self.request(url=url, **kwargs)

    def download(self, url, location, **kwargs):
        """
        Fetch document located at ``url`` and save to to ``location``.
        """

        doc = self.go(url, **kwargs)
        with open(location, 'wb') as out:
            out.write(doc.body)
        return len(doc.body)

    def prepare_request(self, **kwargs):
        """
        Configure all things to make real network request.
        This method is called before doing real request via
        tranposrt extension.
        """

        # Reset the state set by previous request
        if not self._request_prepared:
            self.reset()
            self.request_counter = next(REQUEST_COUNTER)
            if kwargs:
                self.setup(**kwargs)
            if not self.proxylist.is_empty() and self.config['proxy_auto_change']:
                self.change_proxy()
            self.request_method = self.detect_request_method()
            self.transport.process_config(self)
            self._request_prepared = True

    def log_request(self, extra=''):
        """
        Send request details to logging system.
        """

        tname = threading.currentThread().getName().lower()
        if tname == 'mainthread':
            tname = ''
        else:
            tname = '-%s' % tname

        if self.config['proxy']:
            if self.config['proxy_userpwd']:
                auth = ' with authorization'
            else:
                auth = ''
            proxy_info = ' via %s proxy of type %s%s' % (
                self.config['proxy'], self.config['proxy_type'], auth)
        else:
            proxy_info = ''
        if extra:
            extra = '[%s] ' % extra
        logger_network.debug('[%02d%s] %s%s %s%s',
            self.request_counter, tname,
            extra, self.request_method or 'GET',
            self.config['url'], proxy_info)

    def request(self, **kwargs):
        """
        Perform network request.

        You can specify grab settings in ``**kwargs``.
        Any keyword argument will be passed to ``self.config``.

        Returns: ``Document`` objects.
        """

        self.prepare_request(**kwargs)
        self.log_request()

        try:
            self.transport.request()
        except error.GrabError:
            self._request_prepared = False
            self.save_failed_dump()
            raise
        else:
            # That builds `self.doc`
            self.process_request_result()
            return self.doc

    def process_request_result(self, prepare_response_func=None):
        """
        Process result of real request performed via transport extension.
        """

        now = datetime.now()
        # TODO: move into separate method
        if self.config['debug_post']:
            post = self.config['post'] or self.config['multipart_post']
            if isinstance(post, dict):
                post = list(post.items())
            if post:
                if isinstance(post, basestring):
                    post = post[:self.config['debug_post_limit']] + '...'
                else:
                    items = normalize_http_values(post, charset='utf-8')
                    new_items = []
                    for key, value in items:
                        if len(value) > self.config['debug_post_limit']:
                            value = value[:self.config['debug_post_limit']] + '...'
                        else:
                            value = value
                        new_items.append((key, value))
                    post = '\n'.join('%-25s: %s' % x for x in new_items)
            if post:
                logger_network.debug('[%02d] POST request:\n%s\n' % (self.request_counter, post))

        # It's important to delete old POST data after request is performed.
        # If POST data is not cleared then next request will try to use them again!
        old_refresh_count = self.config['refresh_redirect_count']
        self.reset_temporary_options()

        if prepare_response_func:
            self.doc = prepare_response_func(self.transport, self)
        else:
            self.doc = self.transport.prepare_response(self)

        # Warkaround
        if self.doc.grab is None:
            self.doc.grab = weakref.proxy(self)

        if self.config['reuse_cookies']:
            self.cookies.update(self.doc.cookies)

        self.doc.timestamp = now

        self.config['charset'] = self.doc.charset

        if self.config['log_file']:
            with open(self.config['log_file'], 'wb') as out:
                out.write(self.doc.body)

        if self.config['cookiefile']:
            self.cookies.save_to_file(self.config['cookiefile'])

        if self.config['reuse_referer']:
            self.config['referer'] = self.doc.url

        self.copy_request_data()

        # Should be called after `copy_request_data`
        self.save_dumps()

        self._request_prepared = False

        # TODO: check max redirect count
        if self.config['follow_refresh']:
            url = find_refresh_url(self.doc.unicode_body())
            print('URL', url)
            if url is not None:
                inc_count = old_refresh_count + 1
                if inc_count > self.config['redirect_limit']:
                    raise error.GrabTooManyRedirectsError()
                else:
                    print(inc_count)
                    return self.request(url=url, refresh_redirect_count=inc_count)

        return None

    def reset_temporary_options(self):
        self.config['post'] = None
        self.config['multipart_post'] = None
        self.config['method'] = None
        self.config['body_storage_filename'] = None
        self.config['refresh_redirect_count'] = 0

    def save_failed_dump(self):
        """
        Save dump of failed request for debugging.

        This method is called then fatal network exception is raised.
        The saved dump could be used for debugging the reason of the failure.
        """

        # This is very untested feature, so
        # I put it inside try/except to not break
        # live spiders
        try:
            self.doc = self.transport.prepare_response(self)
            self.copy_request_data()
            self.save_dumps()
        except Exception as ex:
            logging.error(unicode(ex))

    def copy_request_data(self):
        # TODO: Maybe request object?
        self.request_head = self.transport.request_head
        self.request_body = self.transport.request_body
        self.request_log = self.transport.request_log

    def setup_document(self, content, **kwargs):
        """
        Setup `response` object without real network requests.

        Useful for testing and debuging.

        All ``**kwargs`` will be passed to `Document` constructor.
        """

        self.reset()

        # Configure Document instance
        doc = Document(grab=self)
        doc.body = content
        doc.status = ''
        doc.head = ''
        doc.parse(charset=kwargs.get('document_charset'))
        doc.code = 200
        doc.total_time = 0
        doc.connect_time = 0
        doc.name_lookup_time = 0
        doc.url = ''

        for key, value in kwargs.items():
            setattr(doc, key, value)

        self.doc = doc

    def change_proxy(self):
        """
        Set random proxy from proxylist.
        """

        if not self.proxylist.is_empty():
            proxy = self.proxylist.get_random_proxy()
            self.setup(proxy=proxy.address, proxy_userpwd=proxy.userpwd,
                       proxy_type=proxy.proxy_type)
        else:
            logging.debug('Proxy list is empty')

    """
    Private methods
    """

    def common_headers(self):
        """
        Build headers which sends typical browser.
        """

        return {
            'Accept': 'text/xml,application/xml,application/xhtml+xml'
                      ',text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.%d' % randint(2, 5),
            'Accept-Language': 'en-us,en;q=0.%d' % (randint(5, 9)),
            'Accept-Charset': 'utf-8,windows-1251;q=0.7,*;q=0.%d' % randint(5, 7),
            'Keep-Alive': '300',
            'Expect': '',
        }

    def save_dumps(self):
        if self.config['log_dir']:
            tname = threading.currentThread().getName().lower()
            if tname == 'mainthread':
                tname = ''
            else:
                tname = '-%s' % tname
            fname = os.path.join(self.config['log_dir'], '%02d%s.log' % (
                self.request_counter, tname))
            with open(fname, 'w') as out:
                out.write('Request headers:\n')
                out.write(self.request_head)
                out.write('\n')
                out.write('Request body:\n')
                out.write(self.request_body)
                out.write('\n\n')
                out.write('Response headers:\n')
                out.write(self.doc.head)

            fext = 'html'
            fname = os.path.join(self.config['log_dir'], '%02d%s.%s' % (
                self.request_counter, tname, fext))
            self.doc.save(fname)

    def make_url_absolute(self, url, resolve_base=False):
        """
        Make url absolute using previous request url as base url.
        """

        if self.config['url']:
            if resolve_base:
                ubody = self.doc.unicode_body()
                base_url = find_base_url(ubody)
                if base_url:
                    return urljoin(base_url, url)
            return urljoin(self.config['url'], url)
        else:
            return url

    def detect_request_method(self):
        """
        Analize request config and find which
        request method will be used.

        Returns request method in upper case

        This method needs simetime when `process_config` method
        was not called yet.
        """

        method = self.config['method']
        if method:
            method = method.upper()
        else:
            if self.config['post'] or self.config['multipart_post']:
                method = 'POST'
            else:
                method = 'GET'
        return method

    def clear_cookies(self):
        """
        Clear all remembered cookies.
        """

        self.config['cookies'] = {}
        self.cookies.clear()

    def setup_with_proxyline(self, line, proxy_type='http'):
        # TODO: remove from base class
        # maybe to proxylist?
        host, port, user, pwd = parse_proxy_line(line)
        server_port = '%s:%s' % (host, port)
        self.setup(proxy=server_port, proxy_type=proxy_type)
        if user:
            userpwd = '%s:%s' % (user, pwd)
            self.setup(proxy_userpwd=userpwd)

    def __getstate__(self):
        """
        Reset cached lxml objects which could not be pickled.
        """
        state = {}
        for cls in type(self).mro():
            cls_slots = getattr(cls, '__slots__', ())
            for slot in cls_slots:
                if slot != '__weakref__':
                    if hasattr(self, slot):
                        state[slot] = getattr(self, slot)

        state['_lxml_form'] = None

        if state['_doc']:
            state['_doc'].grab = weakref.proxy(self)

        return state

    def __setstate__(self, state):
        for slot, value in state.items():
            setattr(self, slot, value)

    @property
    def request_headers(self):
        """
        Temporary hack till the time I'll understand
        where to store request details.
        """

        try:
            first_head = self.request_head.split('\r\n\r\n')[0]
            lines = first_head.split('\r\n')
            lines = [x for x in lines if ':' in x]
            headers = email.message_from_string('\n'.join(lines))
            return headers
        except Exception as ex:
            logging.error('Could not parse request headers', exc_info=ex)
            return {}

    @property
    def kit(self):
        """
        Return KitInterface object that provides some
        methods to communicate with Kit transport.
        """
        
        if not self._kit:
            self._kit = GrabKitInterface(self)
        return self._kit


# For backward compatibility
# WTF???
BaseGrab = Grab

########NEW FILE########
__FILENAME__ = antigate
from tempfile import mkstemp
from base64 import b64encode
try:
    from urllib import urlencode
except ImportError:
    from urllib.parse import urlencode

from grab import Grab
from grab.catpcha.backend.base import CaptchaBackend
from grab.captcha.error import (CaptchaServiceError, ServiceTooBusy, BalanceTooLow,
                                SolutionNotReady)

class AntigateBackend(CaptchaBackend):
    def setup(self, api_key):
        self.api_key = api_key

    def get_submit_captcha_request(self, data, **kwargs):
        g = Grab()
        post={
            'key': self.api_key,
            'method': 'base64',
            'body': b64encode(data),
        }
        post.update(kwargs)
        g.setup(post=post)
        g.setup(url='http://antigate.com/in.php')
        return g

    def parse_submit_captcha_response(self, res):
        if res.code == 200:
            if res.body.startswith('OK|'):
                return res.body.split('|', 1)[1]
            elif res.body == 'ERROR_NO_SLOT_AVAILABLE':
                raise ServiceTooBusy('Service too busy')
            elif res.body == 'ERROR_ZERO_BALANCE':
                raise BalanceTooLow('Balance too low')
            else:
                raise CaptchaServiceError(res.body)
        else:
            raise CaptchaServiceError('Returned HTTP code: %d' % res.code)
        
    def get_check_solution_request(self, captcha_id):
        params = {'key': self.api_key, 'action': 'get', 'id': captcha_id}
        url = 'http://antigate.com/res.php?%s' % urlencode(params)
        g = Grab()
        g.setup(url=url)
        return g

    def parse_check_solution_response(self, res):
        if res.code == 200:
            if res.body.startswith('OK|'):
                return res.body.split('|', 1)[1]
            elif res.body == 'CAPCHA_NOT_READY':
                raise SolutionNotReady('Solution not ready')
            else:
                raise CaptchaServiceError(res.body)
        else:
            raise CaptchaServiceError('Returned HTTP code: %d' % res.code)

########NEW FILE########
__FILENAME__ = base
class CaptchaBackend(object):
    def setup(self, **kwargs):
        pass

########NEW FILE########
__FILENAME__ = browser
import tempfile
import webbrowser
import time
import os

from grab import Grab
from grab.captcha.backend.base import CaptchaBackend

from grab.util.py3k_support import *

class BrowserBackend(CaptchaBackend):
    def get_submit_captcha_request(self, data):
        fd, path = tempfile.mkstemp()
        with open(path, 'w') as out:
            out.write(data)
        url = 'file://' + path
        g = Grab()
        g.setup(url=url)
        return g

    def parse_submit_captcha_response(self, res):
        return res.url.replace('file://', '')

    def get_check_solution_request(self, captcha_id):
        url = 'file://' + captcha_id
        g = Grab()
        g.setup(url=url)
        return g

    def parse_check_solution_response(self, res):
        webbrowser.open(url=res.url)
        # Wait some time, skip some debug messages
        # which browser could dump to console
        time.sleep(0.5)
        solution = raw_input('Enter solution: ')
        path = res.url.replace('file://', '')
        os.unlink(path)
        return solution

########NEW FILE########
__FILENAME__ = gui
import tempfile
import webbrowser
import time
import os
import pygtk
import gtk
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

from grab import Grab
from grab.captcha.backend.base import CaptchaBackend

pygtk.require('2.0')

class CaptchaWindow(object):
    def __init__(self, path, solution):
        self.solution = solution
        self.window = gtk.Window(gtk.WINDOW_TOPLEVEL)
        self.window.show()
        self.window.connect('destroy', self.destroy)
        self.box = gtk.HBox()
        self.image = gtk.Image()
        self.image.set_from_file(path)
        self.entry = gtk.Entry()
        self.entry.connect('activate', self.solve)
        self.button = gtk.Button('Go')
        self.button.connect('clicked', self.solve)

        self.window.add(self.box)
        self.box.pack_start(self.image)
        self.box.pack_start(self.entry)
        self.box.pack_start(self.button)
        self.box.show()
        self.image.show()
        self.button.show()
        self.entry.show()
        self.entry.grab_focus()

    def destroy(self, *args):
        gtk.main_quit()

    def solve(self, *args):
        self.solution.append(self.entry.get_text())
        self.window.hide()
        gtk.main_quit()

    def main(self):
        gtk.main()


class GuiBackend(CaptchaBackend):
    def get_submit_captcha_request(self, data):
        fd, path = tempfile.mkstemp()
        with open(path, 'w') as out:
            out.write(data)
        url = 'file://' + path
        g = Grab()
        g.setup(url=url)
        return g

    def parse_submit_captcha_response(self, res):
        return res.url.replace('file://', '')

    def get_check_solution_request(self, captcha_id):
        url = 'file://' + captcha_id
        g = Grab()
        g.setup(url=url)
        return g

    def parse_check_solution_response(self, res):
        path = res.url.replace('file://', '')
        solution = []
        window = CaptchaWindow(path, solution)
        window.main()
        os.unlink(path)
        return solution[0]

########NEW FILE########
__FILENAME__ = const
BACKEND_ALIAS = {
    'antigate': 'grab.captcha.backend.antigate.AntigateBackend',
    'browser': 'grab.captcha.backend.browser.BrowserBackend',
    'gui': 'grab.captcha.backend.gui.GuiBackend',
}

########NEW FILE########
__FILENAME__ = error
__all__ = ('CaptchaError', 'CaptchaServiceError', 'SolutionNotReady',
           'ServiceTooBusy', 'BalanceTooLow')

class CaptchaError(Exception):
    pass


class CaptchaServiceError(CaptchaError):
    pass


class SolutionNotReady(CaptchaServiceError):
    pass


class ServiceTooBusy(CaptchaServiceError):
    pass


class BalanceTooLow(CaptchaServiceError):
    pass

########NEW FILE########
__FILENAME__ = service
import logging

from grab.util.module import import_string
from grab.captcha.const import BACKEND_ALIAS

__all__ = ('CaptchaService',)
logger = logging.getLogger('grab.captcha')

class CaptchaService(object):
    """
    This class implements API to communicate with
    remote captcha solving service.
    """

    def __init__(self, backend, **kwargs):
        if backend in BACKEND_ALIAS:
            backend_path = BACKEND_ALIAS[backend]
        else:
            backend_path = backend
        self.backend = import_string(backend_path)()
        self.backend.setup(**kwargs)

    def submit_captcha(self, data, **kwargs):
        g = self.backend.get_submit_captcha_request(data, **kwargs)
        g.request()
        return self.backend.parse_submit_captcha_response(g.response)


    def check_solution(self, captcha_id):
        """
        Raises:
        * SolutionNotReady
        * ServiceTooBusy
        """

        g = self.backend.get_check_solution_request(captcha_id)
        g.request()
        return self.backend.parse_check_solution_response(g.response)

########NEW FILE########
__FILENAME__ = todo
import re
import random

RE_SCRIPT = re.compile(r'<script[^>]+recaptcha\.net[^>]+>', re.S)
RE_SCRIPT2 = re.compile(r'<script[^>]+google\.com/recaptcha/api/challenge[^>]+>', re.S)
RE_SCRIPT3 = re.compile(r'Recaptcha\.create\("([^"]+)', re.S | re.I)
RE_SRC = re.compile(r'src="([^"]+)"')

    #def solve_captcha(self, g, url=None, data=None):
        #if not g.clone_counter:
            #logging.error('Warning: maybe you forgot to make the clone of Grab instance')

        #if url:
            #logging.debug('Downloading captcha')
            #g.request(url=url)
            #data = g.response.body

        #logging.debug('Solving captcha')
        #solution = self.module.solve_captcha(key=self.key, data=data)

        #logging.debug('Captcha solved: %s' % solution)
        #return solution


    #def solve_recaptcha(self, g):
        #if not g.clone_counter:
            #logging.error('Warning: maybe you forgot to make the clone of Grab instance')

        #def fetch_challenge():
            #for x in xrange(5):
                #url = None
                #match = RE_SCRIPT.search(g.response.body)
                #if match:
                    #url = RE_SRC.search(match.group(0)).group(1)
                #if not url:
                    #match = RE_SCRIPT2.search(g.response.body)
                    #if match:
                        #url = RE_SRC.search(match.group(0)).group(1)
                #if not url:
                    #if 'google.com/recaptcha/api/js/recaptcha_ajax.js' in g.response.body:
                        ## It is type of google recaptcha
                        #match = RE_SCRIPT3.search(g.response.body)
                        #code = match.group(1)
                        #url = 'http://www.google.com/recaptcha/api/challenge'\
                              #'?k=%s&ajax=1&cachestop=%s' % (code, str(random.random()))
                        ##response = frame_loader.response.body
                        ##rex = re.compile(r"challenge : '[^\"\s]+',")
                        ##challenge_code = rex.search(response).group(0)[13:-2]
                        
                        ##image_loader = frame_loader.clone()
                        ##image_url = 'https://www.google.com/recaptcha/api/image?c=%s' % challenge_code
                        ##solution = solve_captcha(image_loader, url=image_url)

                #if not url:
                    #raise Exception('Unknown recaptcha implementation')

                #g.request(url=url)
                #html = g.response.body

                #if not html:
                    #logging.error('Empty response from recaptcha server')
                    #continue

                #server = re.compile(r'server\s*:\s*\'([^\']+)').search(html).group(1)
                #challenge = re.compile(r'challenge\s*:\s*\'([^\']+)').search(html).group(1)
                #url = server + 'image?c=' + challenge
                #return challenge, url
            #raise CaptchaError('Could not get valid response from recaptcha server')

        #challenge, url = fetch_challenge()
        #solution = self.solve_captcha(g, url=url)
        #return challenge, solution

########NEW FILE########
__FILENAME__ = cli
import os
from argparse import ArgumentParser
import logging
import sys 

from grab.tools.lock import assert_lock
from grab.tools.logs import default_logging
from grab.util.config import build_global_config
from grab.util.py3k_support import *

logger = logging.getLogger('grab.cli')

def activate_env(env_path):
    activate_script = os.path.join(env_path, 'bin/activate_this.py')
    # py3 hack
    if PY3K:
        exec(compile(open(activate_script).read(), activate_script, 'exec'),
             dict(__file__=activate_script))
    else:
        execfile(activate_script, dict(__file__=activate_script))


def setup_logging(action, level, clear_handlers=False):
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)

    if clear_handlers:
        for hdl in root.handlers:
            root.removeHandler(hdl)

    hdl = logging.StreamHandler()
    hdl.setLevel(level)
    #hdl.setFormatter(logging.Formatter('%(message)s'))
    #hdl.setFormatter(logging.Formatter('%(asctime)s: [%(name)s] %(message)s'))
    root.addHandler(hdl)

    # debug log
    #fname = 'var/log/%s.debug.log' % action
    #hdl = logging.FileHandler(fname, 'a')
    #hdl.setFormatter(logging.Formatter('%(asctime)s: [%(name)s] %(message)s'))
    #hdl.setLevel(logging.DEBUG)
    #root.addHandler(hdl)

    # error log
    #fname = 'var/log/%s.error.log' % action
    #hdl = logging.FileHandler(fname, 'a')
    #hdl.setFormatter(logging.Formatter('%(asctime)s: [%(name)s] %(message)s'))
    #hdl.setLevel(logging.ERROR)
    #root.addHandler(hdl)

    # common error log
    #fname = 'var/log/error.log'
    #hdl = logging.FileHandler(fname, 'a')
    #hdl.setFormatter(logging.Formatter('%(asctime)s: [%(name)s] %(message)s'))
    #hdl.setLevel(logging.ERROR)
    #root.addHandler(hdl)

    #root.setLevel(logging.DEBUG)
    #default_logging(level=level)


def process_env_option():
    parser = ArgumentParser()
    parser.add_argument('--env')
    args, trash = parser.parse_known_args()
    if args.env:
        activate_env(args.env)


def process_command_line():
    # Add current directory to python path
    cur_dir = os.path.realpath(os.getcwd())
    sys.path.insert(0, cur_dir)

    process_env_option()

    parser = ArgumentParser()
    parser.add_argument('action', type=str)
    parser.add_argument('--logging-level', default='debug')
    parser.add_argument('--lock-key')
    parser.add_argument('--ignore-lock', action='store_true', default=False)
    parser.add_argument('--settings', type=str, default='settings')
    parser.add_argument('--env', type=str)

    args, trash = parser.parse_known_args()

    config = build_global_config()
    if config and config['GRAB_DJANGO_SETTINGS']:
        os.environ['DJANGO_SETTINGS_MODULE'] = 'settings'
        # Turn off DEBUG to prevent memory leaks
        from django.conf import settings
        settings.DEBUG = False

    # Setup logging
    logging_level = getattr(logging, args.logging_level.upper())
    #if args.positional_args:
        #command_key = '_'.join([args.action] + args.positional_args)
    #else:
        #command_key = args.action
    # TODO: enable logs
    setup_logging(args.action, logging_level, clear_handlers=True)

    # Setup action handler
    action_name = args.action
    try:
        # First, try to import script from the grab package
        action_mod = __import__('grab.script.%s' % action_name, None, None, ['foo'])
    except ImportError as ex:
        if (unicode(ex).startswith('No module named') and
            action_name in unicode(ex)):
            pass
        else:
            logging.error('', exc_info=ex)
        # If grab does not provides the script
        # try to import it from the current project
        try:
            action_mod = __import__('script.%s' % action_name, None, None, ['foo'])
        except ImportError as ex:
            logging.error('', exc_info=ex)
            sys.stderr.write('Could not import %s script' % action_name)
            sys.exit(1)

    if hasattr(action_mod, 'setup_arg_parser'):
        action_mod.setup_arg_parser(parser)
    args, trash = parser.parse_known_args()

    # TODO: enable lock-file processing
    #lock_key = None
    #if not args.slave:
        #if not args.ignore_lock:
            #if not args.lock_key:
                #if hasattr(action_mod, 'setup_lock_key'):
                    #lock_key = action_mod.setup_lock_key(action_name, args)
                #else:
                    #lock_key = command_key
            #else:
                #lock_key = args.lock_key
    #if lock_key is not None:
        #lock_path = 'var/run/%s.lock' % lock_key
        #print 'Trying to lock file: %s' % lock_path
        #assert_lock(lock_path)

    logger.debug('Executing %s action' % action_name)
    try:
        action_mod.main(**vars(args))
    except Exception as ex:
        logging.error('Unexpected exception from action handler:', exc_info=ex)

########NEW FILE########
__FILENAME__ = const
NULL = object()

########NEW FILE########
__FILENAME__ = cookie
"""
RTFM:

* http://docs.python.org/2/library/cookielib.html#cookie-objects

Some code got from https://github.com/kennethreitz/requests/blob/master/requests/cookies.py
"""
try:
    from cookielib import CookieJar, Cookie
except ImportError:
    from http.cookiejar import CookieJar, Cookie
import json
import dummy_threading

from grab.error import GrabMisuseError

COOKIE_ATTRS = ('name', 'value', 'version', 'port', 'domain',
                'path', 'secure', 'expires', 'discard', 'comment',
                'comment_url', 'rfc2109')

def create_cookie(name, value, **kwargs):
    """Creates `cookielib.Cookie` instance.
    """

    config = dict(
        name=name,
        value=value,
        version=0,
        port=None,
        domain='',
        path='/',
        secure=False,
        expires=None,
        discard=True,
        comment=None,
        comment_url=None,
        rfc2109=False,
        rest={'HttpOnly': None},  # wtf?
    )

    bad_args = set(kwargs) - set(config.keys())
    if bad_args:
        raise TypeError('Unexpected arguments: %s' % tuple(bad_args))

    config.update(**kwargs)

    config['port_specified'] = bool(config['port'])
    config['domain_specified'] = bool(config['domain'])
    config['domain_initial_dot'] = config['domain'].startswith('.')
    config['path_specified'] = bool(config['path'])

    return Cookie(**config)


class CookieManager(object):
    """
    Each Grab instance has `cookies` attribute that is instance of `CookieManager` class.

    That class contains helpful methods to create, load, save cookies from/to
    different places.
    """

    __slots__ = ('cookiejar',)

    def __init__(self, cookiejar=None):
        if cookiejar is not None:
            self.cookiejar = cookiejar
        else:
            self.cookiejar = CookieJar()
        #self.disable_cookiejar_lock(self.cookiejar)

    #def disable_cookiejar_lock(self, cj):
        #cj._cookies_lock = dummy_threading.RLock()

    def set(self, name, value, **kwargs):
        """Add new cookie or replace existing cookie with same parameters.

        :param name: name of cookie
        :param value: value of cookie
        :param kwargs: extra attributes of cookie
        """

        self.cookiejar.set_cookie(create_cookie(name, value, **kwargs))

    def update(self, cookies):
        if isinstance(cookies, CookieJar):
            for cookie in cookies:
                self.cookiejar.set_cookie(cookie)
        elif isinstance(cookies, CookieManager):
            for cookie in cookies.cookiejar:
                self.cookiejar.set_cookie(cookie)
        else:
            raise GrabMisuseError('Unknown type of cookies argument: %s' % type(cookies))

    @classmethod
    def from_cookie_list(cls, clist):
        cj = CookieJar()
        for cookie in clist:
            cj.set_cookie(cookie)
        return cls(cj)

    def clear(self):
        self.cookiejar = CookieJar()

    def __getstate__(self):
        state = {}
        for cls in type(self).mro():
            cls_slots = getattr(cls, '__slots__', ())
            for slot in cls_slots:
                if slot != '__weakref__':
                    if hasattr(self, slot):
                        state[slot] = getattr(self, slot)

        state['_cookiejar_cookies'] = list(self.cookiejar)
        del state['cookiejar']

        return state

    def __setstate__(self, state):
        state['cookiejar'] = CookieJar()
        for cookie in state['_cookiejar_cookies']:
            state['cookiejar'].set_cookie(cookie)
        del state['_cookiejar_cookies']

        for slot, value in state.items():
            setattr(self, slot, value)

    def __getitem__(self, key):
        for cookie in self.cookiejar:
            if cookie.name == key:
                return cookie.value
        raise KeyError

    def items(self):
        res = []
        for cookie in self.cookiejar:
            res.append((cookie.name, cookie.value))
        return res

    def load_from_file(self, path):
        """
        Load cookies from the file.

        Content of file should be a JSON-serialized list of dicts.
        """

        with open(path) as inf:
            data = inf.read()
            if data:
                items = json.loads(data)
            else:
                items = {}
        jar = CookieJar()
        for item in items:
            jar.set_cookie(create_cookie(**item))
        self.update(jar)

    def get_dict(self):
        res = []
        for cookie in self.cookiejar:
            res.append(dict((x, getattr(cookie, x)) for x in COOKIE_ATTRS))
        return res

    def save_to_file(self, path):
        """
        Dump all cookies to file.

        Cookies are dumped as JSON-serialized dict of keys and values.
        """

        with open(path, 'w') as out:
            out.write(json.dumps(self.get_dict()))

########NEW FILE########
__FILENAME__ = deprecated
from grab.util.misc import deprecated
from grab.const import NULL
from grab.error import DataNotFound, GrabMisuseError
from grab.tools.text import find_number
from grab.tools.lxml_tools import get_node_text
from grab import error

class DeprecatedThings(object):
    """
    This super-class contains all deprecated things that are
    still in Grab class for back-ward compatibility.
    """

    # Deprecated methods from grab.ext.text module
    # ********************************************

    @deprecated(use_instead='grab.doc.text_search')
    def search(self, anchor, byte=False):
        return self.doc.text_search(anchor, byte=byte)

    @deprecated(use_instead='grab.doc.text_assert')
    def assert_substring(self, anchor, byte=False):
        return self.doc.text_assert(anchor, byte=byte)

    @deprecated(use_instead='grab.doc.text_assert_any')
    def assert_substrings(self, anchors, byte=False):
        return self.doc.text_assert_any(anchors, byte=byte)

    # Deprecated methods from grab.ext.rex module
    # ********************************************

    @deprecated(use_instead='grab.doc.rex_text')
    def rex_text(self, regexp, flags=0, byte=False, default=NULL):
        return self.doc.rex_text(regexp, flags=flags, byte=byte, default=default)

    @deprecated(use_instead='grab.doc.rex_search')
    def rex(self, regexp, flags=0, byte=False, default=NULL):
        return self.doc.rex_search(regexp, flags=flags, byte=byte, default=default)

    @deprecated(use_instead='grab.doc.rex_assert')
    def assert_rex(self, regexp, byte=False):
        return self.doc.rex_assert(regexp, byte=byte)

    # Deprecated methods from grab.ext.lxml
    # *************************************

    @property
    @deprecated(use_instead='grab.doc.tree')
    def tree(self):
        return self.doc.tree

    @deprecated(use_instead='grab.doc.build_html_tree')
    def build_html_tree(self):
        return self.doc.build_html_tree()

    @property
    @deprecated(use_instead='grab.doc.xml_tree')
    def xml_tree(self):
        return self.doc.xml_tree
    
    @deprecated(use_instead='grab.doc.build_xml_tree()')
    def build_xml_tree(self):
        return self.doc.build_xml_tree()

    @deprecated()
    def find_link(self, href_pattern, make_absolute=True):
        """
        Find link in response body which href value matches ``href_pattern``.

        Returns found url or None.
        """

        if make_absolute:
            self.tree.make_links_absolute(self.response.url)

        if isinstance(href_pattern, unicode):
            raise GrabMisuseError('find_link method accepts only '\
                                  'byte-string argument')
        for elem, attr, link, pos in self.tree.iterlinks():
            if elem.tag == 'a' and href_pattern in link:
                return link
        return None

    @deprecated()
    def find_link_rex(self, rex, make_absolute=True):
        """
        Find link matched the given regular expression in response body.

        Returns found url or None.
        """

        if make_absolute:
            self.tree.make_links_absolute(self.response.url)

        for elem, attr, link, pos in self.tree.iterlinks():
            if elem.tag == 'a':
                match = rex.search(link)
                if match:
                    # That does not work for string object
                    # link.match = match
                    return link
        return None

    @deprecated()
    def follow_link(self, anchor=None, href=None):
        """
        Find link and follow it.

        # TODO: refactor this shit
        """

        if anchor is None and href is None:
            raise Exception('You have to provide anchor or href argument')
        self.tree.make_links_absolute(self.config['url'])
        for item in self.tree.iterlinks():
            if item[0].tag == 'a':
                found = False
                text = item[0].text or ''
                url = item[2]
                # if object is regular expression
                if anchor:
                    if hasattr(anchor, 'finditer'):
                        if anchor.search(text):
                            found = True
                    else:
                        if text.find(anchor) > -1:
                            found = True
                if href:
                    if hasattr(href, 'finditer'):
                        if href.search(url):
                            found = True
                    else:
                        if url.startswith(href) > -1:
                            found = True
                if found:
                    url = urljoin(self.config['url'], item[2])
                    return self.request(url=item[2])
        raise DataNotFound('Cannot find link ANCHOR=%s, HREF=%s' % (anchor, href))

    @deprecated(use_instead='grab.doc.select().node()')
    def xpath(self, path, default=NULL, filter=None):
        if filter is not None:
            raise GrabMisuseError('Argument `filter` is not supported anymore')
        return self.doc.select(path).node(default=default)

    @deprecated(use_instead='grab.doc.select().one()')
    def xpath_one(self, path, default=NULL, filter=None):
        if filter is not None:
            raise GrabMisuseError('Argument `filter` is not supported anymore')
        return self.doc.select(path).node(default=default)

    @deprecated(use_instead='grab.doc.select()')
    def xpath_list(self, path, filter=None):
        if filter is not None:
            raise GrabMisuseError('Argument `filter` is not supported anymore')
        return self.doc.select(path).node_list()

    @deprecated(use_instead='grab.doc.select().text()')
    def xpath_text(self, path, default=NULL, filter=None, smart=False,
                   normalize_space=True):
        if filter is not None:
            raise GrabMisuseError('Argument `filter` is not supported anymore')
        return self.doc.select(path).text(default=default, smart=smart,
                                          normalize_space=normalize_space)

    @deprecated(use_instead='grab.doc.select().number()')
    def xpath_number(self, path, default=NULL, filter=None, ignore_spaces=False,
                     smart=False, make_int=True):

        if filter is not None:
            raise GrabMisuseError('Argument `filter` is not supported anymore')
        return self.doc.select(path).number(default=default, smart=smart,
                                            ignore_spaces=ignore_spaces, make_int=make_int)

    @deprecated(use_instead='grab.doc.select().exists()')
    def xpath_exists(self, path):
        return self.doc.select(path).exists()

    # TODO:
    # Make support of CSS queries in selector module
    @deprecated()
    def css(self, *args, **kwargs):
        return self.css_one(*args, **kwargs)

    @deprecated()
    def css_one(self, path, default=NULL):
        """
        Get first element which matches the given css path or raise DataNotFound.
        """

        try:
            return self.css_list(path)[0]
        except IndexError:
            if default is NULL:
                raise DataNotFound('CSS path not found: %s' % path)
            else:
                return default

    @deprecated()
    def css_list(self, path):
        """
        Find all elements which match given css path.
        """

        return self.tree.cssselect(path)

    @deprecated()
    def css_text(self, path, default=NULL, smart=False, normalize_space=True):
        """
        Get normalized text of node which matches the css path.
        """

        try:
            return get_node_text(self.css_one(path), smart=smart,
                                 normalize_space=normalize_space)
        except IndexError:
            if default is NULL:
                raise
            else:
                return default

    @deprecated()
    def css_number(self, path, default=NULL, ignore_spaces=False, smart=False,
                   make_int=True):
        """
        Find number in normalized text of node which matches the given css path.
        """

        try:
            text = self.css_text(path, smart=smart)
            return find_number(text, ignore_spaces=ignore_spaces, make_int=make_int)
        except IndexError:
            if default is NULL:
                raise
            else:
                return default

    @deprecated()
    def assert_css(self, path):
        """
        If css path is not found then raise `DataNotFound` exception.
        """

        self.css_one(path)

    @deprecated()
    def assert_xpath(self, path):
        """
        If xpath path is not found then raise `DataNotFound` exception.
        """

        self.xpath_one(path)

    @deprecated()
    def css_exists(self, path):
        """
        Return True if at least one element with specified css path exists.
        """

        return len(self.css_list(path)) > 0

    @deprecated()
    def strip_tags(self, content, smart=False):
        """
        Strip tags from the HTML content.
        """
        from lxml.html import fromstring

        return get_node_text(fromstring(content), smart=smart)

    # Methods from deprecated grab.ext.django module
    # **********************************************

    @deprecated(use_instead='grab.doc.django_file()')
    def django_file(self, name=None):
        return self.doc.django_file(name=name)

    # Methods from deprecated grab.ext.pquery module
    # **********************************************

    @deprecated(use_instead='grab.doc.pyquery()')
    def pyquery(self):
        return self.doc.pyquery()

    # Response related things
    # ***********************

    # Backward compat.
    def _get_response(self):
        return self.doc


    def _set_response(self, val):
        self.doc = val


    response = property(_get_response, _set_response)


    @deprecated(use_instead='grab.setup_document')
    def fake_response(self, *args, **kwargs):
        return self.setup_document(*args, **kwargs)


    # Cookies
    # *******
    @deprecated(use_instead='grab.cookies.load_from_file')
    def load_cookies(self, path, file_required=True):
        self.cookies.load_from_file(path)

    @deprecated(use_instead='grab.cookies.save_to_file')
    def dump_cookies(self, path):
        self.cookies.save_to_file(path)

    @deprecated(use_instead='grab.proxylist.set_source')
    def load_proxylist(self, source, source_type, proxy_type='http',
                       auto_init=True, auto_change=True,
                       **kwargs):
        #self.proxylist = ProxyList(source, source_type, proxy_type=proxy_type, **kwargs)
        if source_type == 'text_file':
            self.proxylist.set_source('file', location=source, proxy_type=proxy_type, **kwargs)
        elif source_type == 'url':
            self.proxylist.set_source('url', url=source, proxy_type=proxy_type, **kwargs)
        else:
            raise error.GrabMisuseError('Unknown proxy source type: %s' % source_type)

        #self.proxylist.setup(auto_change=auto_change, auto_init=auto_init)
        self.setup(proxy_auto_change=auto_change)
        if not auto_change and auto_init:
            self.change_proxy()

########NEW FILE########
__FILENAME__ = admin
# -*- coding: utf-8
import os

from django.contrib import admin

from models import Task

class TaskAdmin(admin.ModelAdmin):
    list_display = ['task_name', 'start_time', 'end_time', 'elapsed_time_formatted',
                    'is_done', 'is_ok', 'is_process_live', 'pid']
    list_filter = ['is_done', 'is_ok']

    def elapsed_time_formatted(self, obj):
        seconds = obj.elapsed_time
        if seconds < 60:
            return '%d sec.' % seconds
        else:
            minutes, seconds = divmod(seconds, 60)
            if minutes < 60:
                return '%d min.' % minutes
            else:
                hours, minutes = divmod(minutes, 60)
                return '%d hr. %d min.' % (hours, minutes)
    elapsed_time_formatted.short_description = 'Elapsed time'

    def is_process_live(self, obj):
        if obj.pid:
            return 'yes' if os.path.exists('/proc/%d' % obj.pid) else 'no'
        else:
            return 'no'
    is_process_live.short_description = 'Proc. live'



admin.site.register(Task, TaskAdmin)

########NEW FILE########
__FILENAME__ = forms
# coding: utf-8
from django import forms

#from grabstat.models import

########NEW FILE########
__FILENAME__ = 0001_initial
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'TaskResult'
        db.create_table(u'grabstat_taskresult', (
            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('record_date', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, db_index=True, blank=True)),
            ('start_time', self.gf('django.db.models.fields.DateTimeField')(db_index=True, null=True, blank=True)),
            ('stop_time', self.gf('django.db.models.fields.DateTimeField')(db_index=True, null=True, blank=True)),
            ('status', self.gf('django.db.models.fields.CharField')(default='new', max_length=10, db_index=True, blank=True)),
            ('error_traceback', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('spider_stats', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('spider_timing', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('work_time', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            ('pid', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            ('task_name', self.gf('django.db.models.fields.CharField')(max_length=40, blank=True)),
        ))
        db.send_create_signal(u'grabstat', ['TaskResult'])


    def backwards(self, orm):
        # Deleting model 'TaskResult'
        db.delete_table(u'grabstat_taskresult')


    models = {
        u'grabstat.taskresult': {
            'Meta': {'object_name': 'TaskResult'},
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'new'", 'max_length': '10', 'db_index': 'True', 'blank': 'True'}),
            'stop_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),
            'work_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = 0002_auto__del_taskresult__add_task
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Deleting model 'TaskResult'
        db.delete_table(u'grabstat_taskresult')

        # Adding model 'Task'
        db.create_table(u'grabstat_task', (
            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('record_date', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, db_index=True, blank=True)),
            ('start_time', self.gf('django.db.models.fields.DateTimeField')(db_index=True, null=True, blank=True)),
            ('stop_time', self.gf('django.db.models.fields.DateTimeField')(db_index=True, null=True, blank=True)),
            ('status', self.gf('django.db.models.fields.CharField')(default='new', max_length=10, db_index=True, blank=True)),
            ('error_traceback', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('spider_stats', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('spider_timing', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('work_time', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            ('pid', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            ('task_name', self.gf('django.db.models.fields.CharField')(max_length=40, blank=True)),
        ))
        db.send_create_signal(u'grabstat', ['Task'])


    def backwards(self, orm):
        # Adding model 'TaskResult'
        db.create_table(u'grabstat_taskresult', (
            ('status', self.gf('django.db.models.fields.CharField')(default='new', max_length=10, blank=True, db_index=True)),
            ('error_traceback', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('spider_timing', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('start_time', self.gf('django.db.models.fields.DateTimeField')(blank=True, null=True, db_index=True)),
            ('pid', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            ('work_time', self.gf('django.db.models.fields.IntegerField')(null=True, blank=True)),
            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('spider_stats', self.gf('django.db.models.fields.TextField')(blank=True)),
            ('stop_time', self.gf('django.db.models.fields.DateTimeField')(blank=True, null=True, db_index=True)),
            ('task_name', self.gf('django.db.models.fields.CharField')(max_length=40, blank=True)),
            ('record_date', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True, db_index=True)),
        ))
        db.send_create_signal(u'grabstat', ['TaskResult'])

        # Deleting model 'Task'
        db.delete_table(u'grabstat_task')


    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'new'", 'max_length': '10', 'db_index': 'True', 'blank': 'True'}),
            'stop_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),
            'work_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = 0003_auto__del_field_task_stop_time__add_field_task_end_time
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Deleting field 'Task.stop_time'
        db.delete_column(u'grabstat_task', 'stop_time')

        # Adding field 'Task.end_time'
        db.add_column(u'grabstat_task', 'end_time',
                      self.gf('django.db.models.fields.DateTimeField')(db_index=True, null=True, blank=True),
                      keep_default=False)


    def backwards(self, orm):
        # Adding field 'Task.stop_time'
        db.add_column(u'grabstat_task', 'stop_time',
                      self.gf('django.db.models.fields.DateTimeField')(blank=True, null=True, db_index=True),
                      keep_default=False)

        # Deleting field 'Task.end_time'
        db.delete_column(u'grabstat_task', 'end_time')


    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'end_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'new'", 'max_length': '10', 'db_index': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),
            'work_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = 0004_auto__del_field_task_status__add_field_task_is_done__add_field_task_is
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Deleting field 'Task.status'
        db.delete_column(u'grabstat_task', 'status')

        # Adding field 'Task.is_done'
        db.add_column(u'grabstat_task', 'is_done',
                      self.gf('django.db.models.fields.BooleanField')(default=False),
                      keep_default=False)

        # Adding field 'Task.is_failed'
        db.add_column(u'grabstat_task', 'is_failed',
                      self.gf('django.db.models.fields.BooleanField')(default=False),
                      keep_default=False)


    def backwards(self, orm):
        # Adding field 'Task.status'
        db.add_column(u'grabstat_task', 'status',
                      self.gf('django.db.models.fields.CharField')(default='new', max_length=10, blank=True, db_index=True),
                      keep_default=False)

        # Deleting field 'Task.is_done'
        db.delete_column(u'grabstat_task', 'is_done')

        # Deleting field 'Task.is_failed'
        db.delete_column(u'grabstat_task', 'is_failed')


    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'end_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'is_done': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'is_failed': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),
            'work_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = 0005_auto__del_field_task_is_failed__add_field_task_is_ok
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Deleting field 'Task.is_failed'
        db.delete_column(u'grabstat_task', 'is_failed')

        # Adding field 'Task.is_ok'
        db.add_column(u'grabstat_task', 'is_ok',
                      self.gf('django.db.models.fields.BooleanField')(default=True),
                      keep_default=False)


    def backwards(self, orm):
        # Adding field 'Task.is_failed'
        db.add_column(u'grabstat_task', 'is_failed',
                      self.gf('django.db.models.fields.BooleanField')(default=False),
                      keep_default=False)

        # Deleting field 'Task.is_ok'
        db.delete_column(u'grabstat_task', 'is_ok')


    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'end_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'is_done': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'is_ok': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'}),
            'work_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = 0006_auto__del_field_task_work_time__add_field_task_elapsed_time
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Deleting field 'Task.work_time'
        db.rename_column(u'grabstat_task', 'work_time', 'elapsed_time')

    def backwards(self, orm):
        db.rename_column(u'grabstat_task', 'elapsed_time', 'work_time')

    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'elapsed_time': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'end_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'is_done': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'is_ok': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']

########NEW FILE########
__FILENAME__ = 0007_auto__chg_field_task_elapsed_time
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):

        # Changing field 'Task.elapsed_time'
        db.alter_column(u'grabstat_task', 'elapsed_time', self.gf('django.db.models.fields.IntegerField')(default=0))

    def backwards(self, orm):

        # Changing field 'Task.elapsed_time'
        db.alter_column(u'grabstat_task', 'elapsed_time', self.gf('django.db.models.fields.IntegerField')(null=True))

    models = {
        u'grabstat.task': {
            'Meta': {'object_name': 'Task'},
            'elapsed_time': ('django.db.models.fields.IntegerField', [], {'blank': 'True'}),
            'end_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'error_traceback': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'is_done': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'is_ok': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'pid': ('django.db.models.fields.IntegerField', [], {'null': 'True', 'blank': 'True'}),
            'record_date': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'db_index': 'True', 'blank': 'True'}),
            'spider_stats': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'spider_timing': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'start_time': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True', 'null': 'True', 'blank': 'True'}),
            'task_name': ('django.db.models.fields.CharField', [], {'max_length': '40', 'blank': 'True'})
        }
    }

    complete_apps = ['grabstat']
########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-
from django.db import models
from django.core.urlresolvers import reverse

class Task(models.Model):
    task_name = models.CharField(max_length=40, blank=True)
    record_date = models.DateTimeField(auto_now_add=True, db_index=True)
    start_time = models.DateTimeField(null=True, db_index=True, blank=True)
    end_time = models.DateTimeField(null=True, db_index=True, blank=True)
    is_done = models.BooleanField(default=False, blank=True)
    is_ok = models.BooleanField(default=True, blank=True)
    error_traceback = models.TextField(blank=True)    
    spider_stats = models.TextField(blank=True)
    spider_timing = models.TextField(blank=True)
    elapsed_time = models.IntegerField(blank=True, default=0)
    pid = models.IntegerField(null=True, blank=True)

    def __unicode__(self):
        return self.task_name

########NEW FILE########
__FILENAME__ = signals
from django.db.models.signals import post_save, pre_save
from django.dispatch import receiver
from django.db.models import F

#from grabstat.models import

@receiver(post_save, sender=Foo)
def foo_post_save(instance, **kwargs):
    pass

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-
from django.conf.urls import *

urlpatterns = patterns('grab.djangoui.grabstat.views',
    url(r'admin/grab_control$', 'grab_control', name='grab_control'),
    url(r'admin/grab_control_api/(\w+)$', 'grab_control_api', name='grab_control_api'),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-
import logging
from grab.spider import Spider
from grab.util.module import build_spider_registry, load_spider_class
from grab.util.config import build_global_config

from django.shortcuts import redirect, get_object_or_404, render
from django.core.urlresolvers import reverse
from django.template.loader import render_to_string
from django.conf import settings
from django.contrib import messages
from django.contrib.auth.decorators import login_required
from django.contrib.admin.views.decorators import staff_member_required
from django import forms

from common.pagination import paginate
from common.decorators import ajax_get

class ControlForm(forms.Form):
    spider = forms.ChoiceField(required=False)
    command = forms.ChoiceField(required=False)


def grab_control(request):
    form = ControlForm(request.GET or None)
    spider_registry = build_spider_registry(build_global_config())
    spider_choices = [(x, x) for x in spider_registry.keys()]
    form.fields['spider'].choices = spider_choices
    form.fields['spider'].widget.choices = spider_choices

    command_choices = [(x, x) for x in Spider.get_available_command_names()]
    form.fields['command'].choices = command_choices
    form.fields['command'].widget.choices = command_choices

    context = {
        'form': form,
    }
    return render(request, 'grabstat/control_form.html', context)


@ajax_get
def grab_control_api(request, command):
    args = request.GET 
    cls = load_spider_class(build_global_config(), args['spider'])
    spider = cls()
    iface = spider.controller.add_interface('redis')
    if command == 'put_command':
        result_id = iface.put_command({'name': args['command']})
        return {'result_id': result_id}
    elif command == 'pop_result':
        result = iface.pop_result(args['result_id'])
        if result is None:
            return {'status': 'not-ready'}
        else:
            return {'data': result.get('data', ''),
                    'error': result.get('error', ''),
                    }
    else:
        return {'error': 'unknown-command'}

########NEW FILE########
__FILENAME__ = document
# Copyright: 2013, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: MIT
"""
The Document class is the result of network request made with Grab instance.
"""
import weakref
import re
from copy import copy
import logging
import email
try:
    from urllib2 import Request
except ImportError:
    from urllib.request import Request
import os
import json
try:
    from urlparse import urlsplit, parse_qs
except ImportError:
    from urllib.parse import urlsplit, parse_qs
import tempfile
import webbrowser
import codecs
from datetime import datetime
import time

from grab.selector import XpathSelector
import grab.tools.encoding
from grab.cookie import CookieManager
from grab.tools.files import hashed_path
from grab.tools.structured import TreeInterface
from grab.tools.text import normalize_space
from grab.tools.html import decode_entities
from grab.error import GrabMisuseError, DataNotFound
from grab.tools.rex import normalize_regexp
from grab.const import NULL
from grab.util.py3k_support import *

logger = logging.getLogger('grab.response')
NULL_BYTE = chr(0)
RE_XML_DECLARATION = re.compile(br'^[^<]{,100}<\?xml[^>]+\?>', re.I)
RE_DECLARATION_ENCODING = re.compile(br'encoding\s*=\s*["\']([^"\']+)["\']')
RE_META_CHARSET = re.compile(br'<meta[^>]+content\s*=\s*[^>]+charset=([-\w]+)', re.I)
RE_UNICODE_XML_DECLARATION = re.compile(RE_XML_DECLARATION.pattern.decode('utf-8'), re.I)

# Bom processing logic was copied from
# https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py
_BOM_TABLE = [
    (codecs.BOM_UTF32_BE, 'utf-32-be'),
    (codecs.BOM_UTF32_LE, 'utf-32-le'),
    (codecs.BOM_UTF16_BE, 'utf-16-be'),
    (codecs.BOM_UTF16_LE, 'utf-16-le'),
    (codecs.BOM_UTF8, 'utf-8')
]
_FIRST_CHARS = set(char[0] for (char, name) in _BOM_TABLE)


def read_bom(data):
    """Read the byte order mark in the text, if present, and 
    return the encoding represented by the BOM and the BOM.

    If no BOM can be detected, (None, None) is returned.
    """
    # common case is no BOM, so this is fast
    if data and data[0] in _FIRST_CHARS:
        for bom, encoding in _BOM_TABLE:
            if data.startswith(bom):
                return encoding, bom
    return None, None


class TextExtension(object):
    __slots__ = ()

    def text_search(self, anchor, byte=False):
        """
        Search the substring in response body.

        :param anchor: string to search
        :param byte: if False then `anchor` should be the
            unicode string, and search will be performed in `response.unicode_body()`
            else `anchor` should be the byte-string and
            search will be performed in `resonse.body`
        
        If substring is found return True else False.
        """

        if isinstance(anchor, unicode):
            if byte:
                raise GrabMisuseError('The anchor should be bytes string in byte mode')
            else:
                return anchor in self.unicode_body()

        if not isinstance(anchor, unicode):
            if byte:
                #if PY3K:
                    #return anchor in self.body_as_bytes()
                return anchor in self.body
            else:
                raise GrabMisuseError('The anchor should be byte string in non-byte mode')

    def text_assert(self, anchor, byte=False):
        """
        If `anchor` is not found then raise `DataNotFound` exception.
        """

        if not self.text_search(anchor, byte=byte): 
            raise DataNotFound(u'Substring not found: %s' % anchor)


    def text_assert_any(self, anchors, byte=False):
        """
        If no `anchors` were found then raise `DataNotFound` exception.
        """

        found = False
        for anchor in anchors:
            if self.text_search(anchor, byte=byte): 
                found = True
                break
        if not found:
            raise DataNotFound(u'Substrings not found: %s' % ', '.join(anchors))


class RegexpExtension(object):
    __slots__ = ()

    def rex_text(self, regexp, flags=0, byte=False, default=NULL):
        """
        Search regular expression in response body and return content of first
        matching group.

        :param byte: if False then search is performed in `response.unicode_body()`
            else the rex is searched in `response.body`.
        """

        try:
            match = self.rex_search(regexp, flags=flags, byte=byte)
        except DataNotFound:
            if default is NULL:
                raise DataNotFound('Regexp not found')
            else:
                return default
        else:
            return normalize_space(decode_entities(match.group(1)))

    def rex_search(self, regexp, flags=0, byte=False, default=NULL):
        """
        Search the regular expression in response body.

        :param byte: if False then search is performed in `response.unicode_body()`
            else the rex is searched in `response.body`.

        Note: if you use default non-byte mode than do not forget to build your
        regular expression with re.U flag.

        Return found match object or None

        """

        regexp = normalize_regexp(regexp, flags)
        match = None
        if byte:
            if not isinstance(regexp.pattern, unicode) or not PY3K:
                #if PY3K:
                    #body = self.body_as_bytes()
                #else:
                    #body = self.body
                match = regexp.search(self.body)
        else:
            if isinstance(regexp.pattern, unicode) or not PY3K:
                ubody = self.unicode_body()
                match = regexp.search(ubody)
        if match:
            return match
        else:
            if default is NULL:
                rstr = regexp#regexp.source if regexp.hasattr('source') else regexp
                raise DataNotFound('Could not find regexp: %s' % regexp)
            else:
                return default

    def rex_assert(self, rex, byte=False):
        """
        If `rex` expression is not found then raise `DataNotFound` exception.
        """

        self.rex_search(rex, byte=byte)


class DjangoExtension(object):
    def django_file(self, name=None):
        """
        Convert content of response into django `ContentFile` object.

        :param name: specify name of file, otherwise the last segment in
        URL path will be used as filename.
        """
       
        from django.core.files.base import ContentFile

        if not name:
            path = urlsplit(self.url).path
            name = path.rstrip('/').split('/')[-1]

        content_file = ContentFile(self.body)
        content_file.name = name
        return content_file


class PyqueryExtension(object):
    __slots__ = ()

    @property
    def pyquery(self):
        """
        Returns pyquery handler.
        """

        if not self._pyquery:
            from pyquery import PyQuery

            self._pyquery = PyQuery(self.body)
        return self._pyquery


class BodyExtension(object):
    __slots__ = ()

    #def unicode_runtime_body(self, ignore_errors=True, fix_special_entities=True):
        #"""
        #Return response body as unicode string.
        #"""

        #if not self._unicode_runtime_body:
            #self._unicode_runtime_body = self.convert_body_to_unicode(
                #body=self.runtime_body,
                #bom=None,
                #charset=self.charset,
                #ingore_errors=ignore_errors,
                #fix_special_entities=fix_special_entities,
            #)
        #return self._unicode_runtime_body

    #def _read_runtime_body(self):
        #if self._runtime_body is None:
            #return self._cached_body
        #else:
            #return self._runtime_body

    #def _write_runtime_body(self, body):
        #self._runtime_body = body
        #self._unicode_runtime_body = None

    #runtime_body = property(_read_runtime_body, _write_runtime_body)

    def get_body_chunk(self):
        body_chunk = None
        if self.body_path:
            with open(self.body_path, 'rb') as inp:
                body_chunk = inp.read(4096)
        elif self._cached_body:
            body_chunk = self._cached_body[:4096]
        return body_chunk

    def convert_body_to_unicode(self, body, bom, charset, ignore_errors, fix_special_entities):
        # How could it be unicode???
        #if isinstance(body, unicode):
            #body = body.encode('utf-8')
        if bom:
            body = body[len(self.bom):]
        if fix_special_entities:
            body = grab.tools.encoding.fix_special_entities(body)
        if ignore_errors:
            errors = 'ignore'
        else:
            errors = 'strict'
        return body.decode(charset, errors).strip()

    def _check_cached_body(self):
        """
        WTF???
        """
        if not self._cached_body:
            if self.body_path:
                self._cached_body = self.read_body_from_file()

    def read_body_from_file(self):
        with open(self.body_path, 'rb') as inp:
            return inp.read()

    def unicode_body(self, ignore_errors=True, fix_special_entities=True):
        """
        Return response body as unicode string.
        """

        #self._check_cached_body()
        if not self._unicode_body:
            self._unicode_body = self.convert_body_to_unicode(
                body=self.body,#_cached_body,
                bom=self.bom,
                charset=self.charset,
                ignore_errors=ignore_errors,
                fix_special_entities=fix_special_entities,
            )
        return self._unicode_body

    def _read_body(self):
        # py3 hack
        #if PY3K:
            #return self.unicode_body()

        #self._check_cached_body()
        if self.body_path:
            return self.read_body_from_file()
        else:
            return self._cached_body

    def _write_body(self, body):
        if self.body_path:
            with open(self.body_path, 'wb') as out:
                out.write(body)
            self._cached_body = None
        else:
            self._cached_body = body
        self._unicode_body = None

    body = property(_read_body, _write_body)

    #def body_as_bytes(self, encode=False):
        #self._check_cached_body()
        #if encode:
            #return self.body.encode(self.charset)
        #return self._cached_body


class DomTreeExtension(object):
    __slots__ = ()

    @property
    def tree(self):
        """
        Return DOM tree of the document built with HTML DOM builder.
        """

        if self.grab.config['content_type'] == 'xml':
            return self.build_xml_tree()
        else:
            return self.build_html_tree()

    def build_html_tree(self):
        from lxml.html import fromstring
        from lxml.etree import ParserError

        from grab.base import GLOBAL_STATE

        if self._lxml_tree is None:
            #body = self.unicode_runtime_body(
            body = self.unicode_body(
                fix_special_entities=self.grab.config['fix_special_entities']).strip()
            if self.grab.config['lowercased_tree']:
                body = body.lower()
            if self.grab.config['strip_null_bytes']:
                body = body.replace(NULL_BYTE, '')
            # py3 hack
            if PY3K:
                body = RE_UNICODE_XML_DECLARATION.sub('', body)
            else:
                body = RE_XML_DECLARATION.sub('', body)
            if not body:
                # Generate minimal empty content
                # which will not break lxml parser
                body = '<html></html>'
            start = time.time()

            #body = simplify_html(body)
            try:
                self._lxml_tree = fromstring(body)
            except Exception as ex:
                if (isinstance(ex, ParserError)
                    and 'Document is empty' in str(ex)
                    and not '<html' in body):

                    # Fix for "just a string" body
                    body = '<html>%s</html>'.format(body)
                    self._lxml_tree = fromstring(body)

                elif (isinstance(ex, TypeError)
                      and "object of type 'NoneType' has no len" in str(ex)
                      and not '<html' in body):

                    # Fix for smth like "<frameset></frameset>"
                    body = '<html>%s</html>'.format(body)
                    self._lxml_tree = fromstring(body)
                else:
                    raise

            GLOBAL_STATE['dom_build_time'] += (time.time() - start)
        return self._lxml_tree

    @property
    def xml_tree(self):
        """
        Return DOM-tree of the document built with XML DOM builder.
        """
    
        logger.debug('This method is deprecated. Please use `tree` property '\
                     'and content_type="xml" option instead.')
        return self.build_xml_tree()

    def build_xml_tree(self):
        from lxml.etree import fromstring

        if self._strict_lxml_tree is None:
            # py3 hack
            #if PY3K:
                #body = self.body_as_bytes(encode=True)
            #else:
                #body = self.body
            self._strict_lxml_tree = fromstring(self.body)
        return self._strict_lxml_tree


class Document(TextExtension, RegexpExtension, DjangoExtension, PyqueryExtension,
               BodyExtension, DomTreeExtension):
    """
    Document (in most cases it is a network response i.e. result of network request)
    """

    __slots__ = ('status', 'code', 'head', '_cached_body', '_runtime_body',
                 'body_path', 'headers', 'url', 'cookies',
                 'charset', '_unicode_body', '_unicode_runtime_body',
                 'bom', 'timestamp',
                 'name_lookup_time', 'connect_time', 'total_time',
                 'download_size', 'upload_size', 'download_speed',
                 'error_code', 'error_msg', 'grab',
                 '_lxml_tree', '_strict_lxml_tree', '_pyquery',
                 )

    def __init__(self, grab=None):
        if grab is None:
            self.grab = None
        else:
            if isinstance(grab, weakref.ProxyType):
                self.grab = grab
            else:
                self.grab = weakref.proxy(grab)

        self.status = None
        self.code = None
        self.head = None
        self.headers =None
        self.url = None
        self.cookies = CookieManager()
        self.charset = 'utf-8'
        self.bom = None
        self.timestamp = datetime.now()
        self.name_lookup_time = 0
        self.connect_time = 0
        self.total_time = 0
        self.download_size = 0
        self.upload_size = 0
        self.download_speed = 0
        self.error_code = None
        self.error_msg = None

        # Body
        self.body_path = None
        self._cached_body = None
        self._unicode_body = None
        self._runtime_body = None
        self._unicode_runtime_body = None

        # DOM Tree
        self._lxml_tree = None
        self._strict_lxml_tree = None

        # Pyquery
        self._pyquery = None

    def __call__(self, query):
        return self.select(query)

    def select(self, *args, **kwargs):
        return XpathSelector(self.tree).select(*args, **kwargs)

    def structure(self, *args, **kwargs):
        return TreeInterface(self.tree).structured_xpath(*args, **kwargs)

    def parse(self, charset=None):
        """
        Parse headers and cookies.

        This method is called after Grab instance performes network request.
        """

        # Extract only valid lines which contain ":" character
        valid_lines = []
        for line in self.head.split('\n'):
            line = line.rstrip('\r')
            if line:
                # Each HTTP line meand the start of new response
                # self.head could contains info about multiple responses
                # For example, then 301/302 redirect was processed automatically
                # Maybe it is a bug and should be fixed
                # Anyway, we handle this issue here and save headers
                # only from last response
                if line.startswith('HTTP'):
                    self.status = line
                    valid_lines = []
                else:
                    if ':' in line:
                        valid_lines.append(line)

        self.headers = email.message_from_string('\n'.join(valid_lines))

        if charset is None:
            if isinstance(self.body, unicode):
                self.charset = 'utf-8'
            else:
                self.detect_charset()
        else:
            self.charset = charset

        self._unicode_body = None

    def detect_charset(self):
        """
        Detect charset of the response.

        Try following methods:
        * meta[name="Http-Equiv"]
        * XML declaration
        * HTTP Content-Type header

        Ignore unknown charsets.

        Use utf-8 as fallback charset.
        """

        charset = None

        body_chunk = self.get_body_chunk()

        if body_chunk:
            # Try to extract charset from http-equiv meta tag
            try:
                charset = RE_META_CHARSET.search(body_chunk).group(1)
            except AttributeError:
                pass

            # TODO: <meta charset="utf-8" />
            bom_enc, bom = read_bom(body_chunk)
            if bom_enc:
                charset = bom_enc
                self.bom = bom

            # Try to process XML declaration
            if not charset:
                if body_chunk.startswith(b'<?xml'):
                    match = RE_XML_DECLARATION.search(body_chunk)
                    if match:
                        enc_match = RE_DECLARATION_ENCODING.search(match.group(0))
                        if enc_match:
                            charset = enc_match.group(1)

        if not charset:
            if 'Content-Type' in self.headers:
                pos = self.headers['Content-Type'].find('charset=')
                if pos > -1:
                    charset = self.headers['Content-Type'][(pos + 8):]

        if charset:
            if not isinstance(charset, str):
                # Convert to unicode (py2.x) or string (py3.x)
                charset = charset.decode('utf-8')
            # Check that python knows such charset
            try:
                codecs.lookup(charset)
            except LookupError:
                logger.error('Unknown charset found: %s' % charset)
                self.charset = 'utf-8'
            else:
                self.charset = charset

    def copy(self, new_grab=None):
        """
        Clone the Response object.
        """

        if new_grab is not None:
            obj = self.__class__(self.grab)#Response()
        else:
            obj = self.__class__(new_grab)

        copy_keys = ('status', 'code', 'head', 'body', 'total_time',
                     'connect_time', 'name_lookup_time',
                     'url', 'charset', '_unicode_body')
        for key in copy_keys:
            setattr(obj, key, getattr(self, key))

        obj.headers = copy(self.headers)
        # TODO: Maybe, deepcopy?
        obj.cookies = copy(self.cookies)

        return obj

    def save(self, path, create_dirs=False):
        """
        Save response body to file.
        """

        path_dir, path_fname = os.path.split(path)
        if not os.path.exists(path_dir):
            try:
                os.makedirs(path_dir)
            except OSError:
                pass

        with open(path, 'wb') as out:
            if isinstance(self._cached_body, unicode):
                out.write(self._cached_body.encode('utf-8'))
            else:
                out.write(self._cached_body)

    def save_hash(self, location, basedir, ext=None):
        """
        Save response body into file with special path
        builded from hash. That allows to lower number of files
        per directory.

        :param location: URL of file or something else. It is
            used to build the SHA1 hash.
        :param basedir: base directory to save the file. Note that
            file will not be saved directly to this directory but to
            some sub-directory of `basedir`
        :param ext: extension which should be appended to file name. The
            dot is inserted automatically between filename and extension.
        :returns: path to saved file relative to `basedir`

        Example::

            >>> url = 'http://yandex.ru/logo.png'
            >>> g.go(url)
            >>> g.response.save_hash(url, 'some_dir', ext='png')
            'e8/dc/f2918108788296df1facadc975d32b361a6a.png'
            # the file was saved to $PWD/some_dir/e8/dc/...

        TODO: replace `basedir` with two options: root and save_to. And
        returns save_to + path
        """

        if isinstance(location, unicode):
            location = location.encode('utf-8')
        rel_path = hashed_path(location, ext=ext)
        path = os.path.join(basedir, rel_path)
        if not os.path.exists(path):
            path_dir, path_fname = os.path.split(path)
            try:
                os.makedirs(path_dir)
            except OSError:
                pass
            with open(path, 'wb') as out:
                if isinstance(self._cached_body, unicode):
                    out.write(self._cached_body.encode('utf-8'))
                else:
                    out.write(self._cached_body)
        return rel_path

    @property
    def json(self):
        """
        Return response body deserialized into JSON object.
        """

        return json.loads(self.body)

    def url_details(self):
        """
        Return result of urlsplit function applied to response url.
        """

        return urlsplit(self.url) 

    def query_param(self, key):
        """
        Return value of parameter in query string.
        """

        return parse_qs(self.url_details().query)[key][0]

    def browse(self):
        """
        Save response in temporary file and open it in GUI browser.
        """

        fh, path = tempfile.mkstemp()
        self.save(path)
        webbrowser.open('file://' + path)

    @property
    def time(self):
        logger.error('Attribute Response.time is deprecated. Use Response.total_time instead.')
        return self.total_time

    def __getstate__(self):
        """
        Reset cached lxml objects which could not be pickled.
        """
        state = {}
        for cls in type(self).mro():
            cls_slots = getattr(cls, '__slots__', ())
            for slot in cls_slots:
                if slot != '__weakref__':
                    if hasattr(self, slot):
                        state[slot] = getattr(self, slot)

        state['_lxml_tree'] = None
        state['_strict_lxml_tree'] = None

        #state['doc'].grab = weakref.proxy(self)

        return state

    def __setstate__(self, state):
        for slot, value in state.items():
            setattr(self, slot, value)

########NEW FILE########
__FILENAME__ = error
"""
Custom exception which could generate Grab instance.

Taxonomy:

Exception
|-> GrabError
    |-> GrabNetworkError <- IOError 
    |-> DataNotFound <- IndexError
    |-> Grab*Error

"""
import warnings

class GrabError(Exception):
    """
    All custom Grab exception should be children of that class.
    """


class GrabNetworkError(IOError, GrabError):
    """
    Raises in case of network error.
    """


class GrabTimeoutError(GrabNetworkError):
    """
    Raises when configured time is outed for the request.

    In curl transport it is CURLE_OPERATION_TIMEDOUT (28)
    """


class DataNotFound(IndexError, GrabError):
    """
    Indictes that required data is not found.
    """


class GrabMisuseError(GrabError):
    """
    Indicates incorrect usage of grab API.
    """


class GrabConnectionError(GrabNetworkError):
    """
    Raised when it is not possible to establish network connection.

    In curl transport it is CURLE_COULDNT_CONNECT (7)
    """


class GrabAuthError(GrabError):
    """
    Raised when remote server denies authentication credentials.

    In curl transport it is CURLE_COULDNT_CONNECT (67)
    """


class GrabTooManyRedirectsError(GrabError):
    """
    Raised when Grab reached max. allowd number of redirects for
    one request.
    """


class GrabDeprecationWarning(Warning):
    """
    Raised when some deprecated feature is used.
    """

class GrabInvalidUrl(GrabError):
    """
    Raised when Grab have no idea how to handle the URL or when
    some error occured while normalizing URL e.g. IDN processing.
    """

def warn(msg):
    warnings.warn(msg, category=GrabDeprecationWarning, stacklevel=3)

########NEW FILE########
__FILENAME__ = csv_dumper
import csv

class CSVDumper(object):
    def __init__(self, path, fields=None, write_header=True, quoting=csv.QUOTE_ALL):
        self.path = path
        self.fields = fields
        self.write_header = write_header
        self.file_handler = open(path, 'w')
        self.writer = csv.writer(self.file_handler, quoting=quoting)
        if self.fields and self.write_header:
            self.writer.writerow(self.normalize_row(self.fields))

    def add_record(self, rec, ignore_fields={}):
        if self.fields is None:
            raise Exception('Can not use `add_record` if `fields` is not set')
        for key in rec:
            if not key in ignore_fields:
                if not key in self.fields:
                    raise Exception('Unknown record key: %s' % key)
        for key in self.fields:
            if not key in rec:
                raise Exception('Missing key in record: %s' % key)
        row = [rec[x] for x in self.fields]
        self.writer.writerow(self.normalize_row(row))

    def add_row(self, row):
        self.writer.writerow(self.normalize_row(row))

    def normalize_row(self, row):
        return map(self.normalize_value, row)

    def normalize_none_value(self, val):
        return ''

    def normalize_value(self, val):
        if val is None:
            return self.normalize_none_value(val)
        elif isinstance(val, unicode):
            return val.encode('utf-8')
        else:
            return str(val)

    def close(self):
        self.file_handler.close()

########NEW FILE########
__FILENAME__ = mysql_dumper
import csv

from grab.export.csv_dumper import CSVDumper

class MysqlCSVDumper(CSVDumper):
    """
    Difference from CSVDumper:
    * default `quoting` value is QUOTE_MINIMAL
    * default `write_header` value is False
    * None values are converted to r'\N'
    * \ symbols are converted to \\
    """

    def __init__(self, path, fields=None, write_header=False, quoting=csv.QUOTE_MINIMAL):
        super(MysqlCSVDumper, self).__init__(path, fields=fields, write_header=write_header,
                                             quoting=quoting)

    def normalize_none_value(self, val):
        return r'\N'

    def normalize_value(self, val):
        if val is None:
            return self.normalize_none_value(val)
        elif isinstance(val, basestring):
            if isinstance(val, basestring):
                val = val.encode('utf-8')
            val = val.replace('\\', '\\\\')
            return val
        else:
            return str(val)


def build_import_sql(path, table, columns):
    sql = r'''
        LOAD DATA LOCAL INFILE "%s"
        REPLACE INTO TABLE %s
        character set utf8
        fields terminated by "," optionally enclosed by '"'
        lines terminated by "\r\n"
        (%s);
    ''' % (path, table, ','.join(columns))
    sql = '\n'.join(x.lstrip() for x in sql.splitlines() if x.strip())
    return sql

########NEW FILE########
__FILENAME__ = form
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
try:
    from urlparse import urljoin
except ImportError:
    from urllib.parse import urljoin

from grab.error import DataNotFound, GrabMisuseError
from grab.tools.http import smart_urlencode

# TODO: refactor this hell

class FormExtension(object):
    __slots__ = ()
    # SLOTS: _lxml_form, _file_fields

    def extra_reset(self):
        self._lxml_form = None
        self._file_fields = {}

    def choose_form(self, number=None, id=None, name=None, xpath=None):
        """
        Set the default form.
        
        :param number: number of form (starting from zero)
        :param id: value of "id" atrribute
        :param name: value of "name" attribute
        :param xpath: XPath query
        :raises: :class:`DataNotFound` if form not found
        :raises: :class:`GrabMisuseError` if method is called without parameters

        Selected form will be available via `form` atribute of `Grab`
        instance. All form methods will work with defalt form.

        Examples::

            # Select second form
            g.choose_form(1)

            # Select by id
            g.choose_form(id="register")

            # Select by name
            g.choose_form(name="signup")

            # Select by xpath
            g.choose_form(xpath='//form[contains(@action, "/submit")]')
        """

        if id is not None:
            try:
                self._lxml_form = self.css_one('form[id="%s"]' % id)
            except IndexError:
                raise DataNotFound("There is no form with id: %s" % id)
        elif name is not None:
            try:
                self._lxml_form = self.css_one('form[name="%s"]' % name)
            except IndexError:
                raise DataNotFound('There is no form with name: %s' % name)
        elif number is not None:
            try:
                self._lxml_form = self.tree.forms[number]
            except IndexError:
                raise DataNotFound('There is no form with number: %s' % number)
        elif xpath is not None:
            try:
                self._lxml_form = self.doc.select(xpath).node()
            except IndexError:
                raise DataNotFound('Could not find form with xpath: %s' % xpath)
        else:
            raise GrabMisuseError('choose_form methods requires one of '
                                  '[number, id, name, xpath] arguments')
                
    @property
    def form(self):
        """
        This attribute points to default form.

        If form was not selected manually then select the form
        which has the biggest number of input elements.

        The form value is just an `lxml.html` form element.

        Example::

            g.go('some URL')
            # Choose form automatically
            print g.form

            # And now choose form manually
            g.choose_form(1)
            print g.form
        """

        if self._lxml_form is None:
            forms = [(idx, len(list(x.fields))) for idx, x in enumerate(self.tree.forms)]
            if len(forms):
                idx = sorted(forms, key=lambda x: x[1], reverse=True)[0][0]
                self.choose_form(idx)
            else:
                raise DataNotFound('Response does not contains any form')
        return self._lxml_form

    def set_input(self, name, value):
        """
        Set the value of form element by its `name` attribute.

        :param name: name of element
        :param value: value which should be set to element

        To check/uncheck the checkbox pass boolean value.

        Example::

            g.set_input('sex', 'male')

            # Check the checkbox
            g.set_input('accept', True)
        """

        if self._lxml_form is None:
            self.choose_form_by_element('.//*[@name="%s"]' % name)
        elem = self.form.inputs[name]

        processed = False
        if getattr(elem, 'type', None) == 'checkbox':
            if isinstance(value, bool):
                elem.checked = value
                processed = True
        
        if not processed:
            # We need to remember original values of file fields
            # Because lxml will convert UploadContent/UploadFile object to string
            if getattr(elem, 'type', '').lower() == 'file':
                self._file_fields[name] = value
            elem.value = value

    def set_input_by_id(self, _id, value):
        """
        Set the value of form element by its `id` attribute.

        :param _id: id of element
        :param value: value which should be set to element
        """

        xpath = './/*[@id="%s"]' % _id
        if self._lxml_form is None:
            self.choose_form_by_element(xpath)
        elem = self.form.xpath(xpath)[0]
        return self.set_input(elem.get('name'), value)

    def set_input_by_number(self, number, value):
        """
        Set the value of form element by its number in the form

        :param number: number of element
        :param value: value which should be set to element
        """

        elem = self.form.xpath('.//input[@type="text"]')[number]
        return self.set_input(elem.get('name'), value)

    def set_input_by_xpath(self, xpath, value):
        """
        Set the value of form element by xpath

        :param xpath: xpath path
        :param value: value which should be set to element
        """

        elem = self.tree.xpath(xpath)[0]

        if self._lxml_form is None:
            # Explicitly set the default form 
            # which contains found element
            parent = elem
            while True:
                parent = parent.getparent()
                if parent.tag == 'form':
                    self._lxml_form = parent
                    break

        return self.set_input(elem.get('name'), value)


    # TODO:
    # Remove set_input_by_id
    # Remove set_input_by_number
    # New method: set_input_by(id=None, number=None, xpath=None)

    def submit(self, submit_name=None, make_request=True,
               url=None, extra_post=None):
        """
        Submit default form.

        :param submit_name: name of buton which should be "clicked" to
            submit form
        :param make_request: if `False` then grab instance will be
            configured with form post data but request will not be
            performed
        :param url: explicitly specifi form action url
        :param extra_post: (dict or list of pairs) additional form data which
            will override data automatically extracted from the form.

        Following input elements are automatically processed:

        * input[type="hidden"] - default value
        * select: value of last option
        * radio - ???
        * checkbox - ???

        Multipart forms are corectly recognized by grab library.

        Example::

            # Assume that we going to some page with some form
            g.go('some url')
            # Fill some fields
            g.set_input('username', 'bob')
            g.set_input('pwd', '123')
            # Submit the form
            g.submit()
            
            # or we can just fill the form
            # and do manu submition
            g.set_input('foo', 'bar')
            g.submit(make_request=False)
            g.request()

            # for multipart forms we can specify files
            from grab import UploadFile
            g.set_input('img', UploadFile('/path/to/image.png'))
            g.submit()
        """

        # TODO: add .x and .y items
        # if submit element is image

        post = self.form_fields()
        submit_control = None

        # Build list of submit buttons which have a name
        submit_controls = {}
        for elem in self.form.inputs:
            if (elem.tag == 'input' and elem.type == 'submit' and
                elem.get('name') is not None):
                submit_controls[elem.name] = elem

        # All this code need only for one reason:
        # to not send multiple submit keys in form data
        # in real life only this key is submitted whose button
        # was pressed
        if len(submit_controls):
            # If name of submit control is not given then
            # use the name of first submit control
            if submit_name is None or not submit_name in submit_controls:
                controls = sorted(submit_controls.values(), key=lambda x: x.name)
                submit_name = controls[0].name

            # Form data should contain only one submit control
            for name in submit_controls:
                if name != submit_name:
                    if name in post:
                        del post[name]

        if url:
            action_url = urljoin(self.response.url, url)
        else:
            action_url = urljoin(self.response.url, self.form.action)


        # Values from `extra_post` should override values in form
        # `extra_post` allows multiple value of one key

        # Process saved values of file fields
        if self.form.method == 'POST':
            if 'multipart' in self.form.get('enctype', ''):
                for key, obj in self._file_fields.items():
                    post[key] = obj

        post_items = list(post.items())
        del post

        if extra_post:
            if isinstance(extra_post, dict):
                extra_post_items = extra_post.items()
            else:
                extra_post_items = extra_post

            # Drop existing post items with such key
            keys_to_drop = set([x for x, y in extra_post_items])
            for key in keys_to_drop:
                post_items = [(x, y) for x, y in post_items if x != key]

            for key, value in extra_post_items:
                post_items.append((key, value))

        if self.form.method == 'POST':
            if 'multipart' in self.form.get('enctype', ''):
                self.setup(multipart_post=post_items)
            else:
                self.setup(post=post_items)
            self.setup(url=action_url)

        else:
            url = action_url.split('?')[0] + '?' + smart_urlencode(post_items)
            self.setup(url=url)

        if make_request:
            return self.request()
        else:
            return None

    def form_fields(self):
        """
        Return fields of default form.

        Fill some fields with reasonable values.
        """

        fields = dict(self.form.fields)
        for elem in self.form.inputs:
            # Ignore elements without name
            if not elem.get('name'):
                continue

            # Do not submit disabled fields
            # http://www.w3.org/TR/html4/interact/forms.html#h-17.12
            if elem.get('disabled'):
                if elem.name in fields:
                    del fields[elem.name]

            elif elem.tag == 'select':
                if fields[elem.name] is None:
                    if len(elem.value_options):
                        fields[elem.name] = elem.value_options[0]

            elif getattr(elem, 'type', None) == 'radio':
                if fields[elem.name] is None:
                    fields[elem.name] = elem.get('value')

            elif getattr(elem, 'type', None) == 'checkbox':
                if not elem.checked:
                    if elem.name is not None:
                        if elem.name in fields:
                            del fields[elem.name]

        return fields

    def choose_form_by_element(self, xpath):
        forms = self.tree.xpath('//form')
        found_form = None
        for form in forms:
            if len(form.xpath(xpath)):
                found_form = form
                break
        self._lxml_form = found_form if found_form is not None else forms[0]

########NEW FILE########
__FILENAME__ = extension
"""
Simple extension system which allows to inherit class
from extension super-classes and cache all extension handlers.
"""
from copy import copy

class ExtensionSystemError(object):
    pass


def trigger_extensions(self, point):
    for func in self.extension_handlers[point]:
        func(self)


def register_extensions(cls):
    """
    Build and cache list of handlers for each extension point.
    """

    if hasattr(cls, 'extension_points'):
        points = cls.extension_points
    else:
        for base in cls.__bases__:
            tmp = getattr(base, 'extension_points', None)
            if tmp is not None:
                points = tmp
                break

    if not points:
        raise ExtensionSystemError('Could not find extension_points attribute nor in class neither in his parents')

    if not hasattr(cls, 'extension_points'):
        cls.extension_points = copy(points)
    handlers = dict((x, []) for x in cls.extension_points)

    for base in cls.__bases__:
        for key in cls.extension_points:
            func = getattr(base, 'extra_%s' % key, None)
            if func:
                handlers[key].append(func)

    cls.extension_handlers = handlers
    cls.trigger_extensions = trigger_extensions

########NEW FILE########
__FILENAME__ = decorator
from grab.const import NULL
from grab.error import DataNotFound

# *******************
# Internal decorators
# *******************

def cached(func):
    def internal(self, item, itemtype):
        if self.attr_name in item._cache:
            return item._cache[self.attr_name]
        else:
            value = func(self, item, itemtype)
            item._cache[self.attr_name] = value
            return value
    return internal


def default(func):
    def internal(self, item, itemtype):
        try:
            value = func(self, item, itemtype)
        except DataNotFound:
            if self.default is not NULL:
                value = self.default
            else:
                raise
        else:
            if self.empty_default is not NULL:
                if not value:
                    value = self.empty_default
        item._cache[self.attr_name] = value
        return value
    return internal


def processor(func):
    def internal(self, item, itemtype):
        value = func(self, item, itemtype)
        if self.processor:
            return self.processor(value)
        else:
            return value
    return internal


def empty(func):
    def internal(self, item, itemtype):
        if self.xpath_exp is None:
            if self.default is not NULL:
                return self.default
            else:
                return None
        else:
            return func(self, item, itemtype)
    return internal


def bind_item(func):
    def internal(self, item, itemtype):
        self.item = item
        return func(self, item, itemtype)
    return internal

########NEW FILE########
__FILENAME__ = error
from grab.error import GrabError

class ItemError(GrabError):
    pass


class ChoiceFieldError(ItemError):
    pass

########NEW FILE########
__FILENAME__ = field
from abc import ABCMeta, abstractmethod
from datetime import datetime
try:
    from cdecimal import Decimal
except ImportError:
    from decimal import Decimal

from grab.tools.lxml_tools import clean_html
from grab.tools.text import find_number, drop_space
from grab.item.decorator import default, empty, cached, bind_item
from grab.const import NULL
from grab.item.error import ChoiceFieldError

metaclass_ABCMeta = ABCMeta('metaclass_ABCMeta', (object, ), {})

class Field(metaclass_ABCMeta):
    """
    All custom fields should extend this class, and override the get method.
    """

    def __init__(self, xpath=None, default=NULL, empty_default=NULL,
                 processor=None, **kwargs):
        self.xpath_exp = xpath
        self.default = default
        self.empty_default = empty_default
        self.processor = processor

    @abstractmethod
    def __get__(self, obj, objtype):
        pass

    def __set__(self, obj, value):
        obj._cache[self.attr_name] = value

    def process(self, value):
        if self.processor:
            return self.processor(value)
        else:
            return value


class NullField(Field):
    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        return self.process(None)


class ItemListField(Field):
    def __init__(self, xpath, item_cls, *args, **kwargs):
        self.item_cls = item_cls
        super(ItemListField, self).__init__(xpath, *args, **kwargs)

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        subitems = []
        for sel in item._selector.select(self.xpath_exp):
            subitem = self.item_cls(sel.node)
            subitems.append(subitem)
        return self.process(subitems)


class IntegerField(Field):
    def __init__(self, *args, **kwargs):
        self.find_number = kwargs.get('find_number', False)
        self.ignore_spaces = kwargs.get('ignore_spaces', False)
        self.ignore_chars = kwargs.get('ignore_chars', None)
        self.multiple = kwargs.get('multiple', False)
        super(IntegerField, self).__init__(*args, **kwargs)

    def get_raw_values(self, item):
        return item._selector.select(self.xpath_exp).text_list()

    def get_raw_value(self, item):
        return item._selector.select(self.xpath_exp).text()

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        if self.multiple:
            result = []
            for raw_value in self.get_raw_values(item):
                result.append(self.process_raw_value(raw_value))
            return result
        else:
            raw_value = self.get_raw_value(item)
            return self.process_raw_value(raw_value)

    def process_raw_value(self, value):
        if self.empty_default is not NULL:
            if value == "":
                return self.empty_default

        if self.find_number or self.ignore_spaces or self.ignore_chars:
            return find_number(self.process(value), ignore_spaces=self.ignore_spaces,
                               ignore_chars=self.ignore_chars)
        else:
            # TODO: process ignore_chars and ignore_spaces in this case too
            if self.ignore_chars:
                for char in ignore_chars:
                    value = value.replace(char, '')
            if self.ignore_spaces:
                value = drop_space(value)
            return int(self.process(value).strip())


class DecimalField(Field):
    def __init__(self, *args, **kwargs):
        self.multiple = kwargs.get('multiple', False)
        super(DecimalField, self).__init__(*args, **kwargs)

    def get_raw_values(self, item):
        return item._selector.select(self.xpath_exp).text_list()

    def get_raw_value(self, item):
        return item._selector.select(self.xpath_exp).text()

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        if self.multiple:
            result = []
            for raw_value in self.get_raw_values(item):
                result.append(self.process_raw_value(raw_value))
            return result
        else:
            raw_value = self.get_raw_value(item)
            return self.process_raw_value(raw_value)

    def process_raw_value(self, value):
        if self.empty_default is not NULL:
            if value == "":
                return self.empty_default

        return Decimal(self.process(value).strip())


class StringField(Field):
    def __init__(self, *args, **kwargs):
        self.normalize_space = kwargs.pop('normalize_space', True)
        self.multiple = kwargs.get('multiple', False)
        super(StringField, self).__init__(*args, **kwargs)

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        #value = item._selector.select(self.xpath_exp)\
                    #.text(normalize_space=self.normalize_space)
        #return self.process(value)

        if self.multiple:
            result = []
            for raw_value in self.get_raw_values(item):
                result.append(self.process_raw_value(raw_value))
            return result
        else:
            raw_value = self.get_raw_value(item)
            return self.process_raw_value(raw_value)

    def process_raw_value(self, value):
        return self.process(value)

    def get_raw_values(self, item):
        return item._selector.select(self.xpath_exp).text_list()

    def get_raw_value(self, item):
        return item._selector.select(self.xpath_exp).text()


class HTMLField(Field):
    def __init__(self, *args, **kwargs):
        self.safe_attrs = kwargs.pop('safe_attrs', None)
        super(HTMLField, self).__init__(*args, **kwargs)

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        value = item._selector.select(self.xpath_exp).html()
        if self.safe_attrs is not None:
            return self.process(clean_html(value, output_encoding='unicode'))
        else:
            return self.process(value)


class ChoiceField(Field):
    def __init__(self, *args, **kwargs):
        self.choices = kwargs.pop('choices')
        super(ChoiceField, self).__init__(*args, **kwargs)

    @cached
    @default
    @empty
    @bind_item
    def __get__(self, item, itemtype):
        value = item._selector.select(self.xpath_exp).text()
        clean_value = self.process(value)
        try:
            return self.choices[clean_value]
        except KeyError:
            raise ChoiceFieldError('Unknown choice: %s' % clean_value)


class RegexField(Field):
    def __init__(self, xpath, regex, *args, **kwargs):
        self.regex = regex
        super(RegexField, self).__init__(xpath, *args, **kwargs)

    @cached
    @default
    @bind_item
    def __get__(self, item, itemtype):
        value = item._selector.select(self.xpath_exp).text()
        match = self.regex.search(value)
        if match:
            return self.process(match.group(1))
        else:
            raise DataNotFound('Could not find regex')


class DateTimeField(Field):
    def __init__(self, xpath, datetime_format='Y-m-d', *args, **kwargs):
        self.datetime_format = datetime_format
        super(DateTimeField, self).__init__(xpath, *args, **kwargs)

    @cached
    @default
    @bind_item
    def __get__(self, item, itemtype):
        value = item._selector.select(self.xpath_exp).text()
        return datetime.strptime(self.process(value),
                                 self.datetime_format)


class DateField(Field):
    def __init__(self, xpath, date_format='Y-m-d', *args, **kwargs):
        self.date_format = date_format
        super(DateField, self).__init__(xpath, *args, **kwargs)

    @cached
    @default
    @bind_item
    def __get__(self, item, itemtype):
        value = item._selector.select(self.xpath_exp).text()
        return datetime.strptime(self.process(value),
                                 self.date_format).date()


class FuncField(Field):
    def __init__(self, func, pass_item=False, *args, **kwargs):
        self.func = func
        self.pass_item = pass_item
        super(FuncField, self).__init__(*args, **kwargs)

    @cached
    @default
    @bind_item
    def __get__(self, item, itemtype):
        if self.pass_item:
            val = self.func(item, item._selector)
        else:
            val = self.func(item._selector)
        return self.process(val)


class BooleanField(Field):
    @cached
    @default
    @bind_item
    def __get__(self, item, itemtype):
        return item._selector.select(self.xpath_exp).exists()

########NEW FILE########
__FILENAME__ = item
import logging

from grab.item.field import Field, ItemListField
from grab.selector import XpathSelector
from grab.selector import JsonSelector
from grab.error import GrabMisuseError
from grab.document import Document

logger = logging.getLogger('grab.item.item')

class ItemBuilder(type):
    def __new__(cls, name, bases, namespace):
        fields = {}

        for attr in namespace:
            if isinstance(namespace[attr], Field):
                field = namespace[attr]
                field.attr_name = attr
                namespace[attr] = field
                fields[attr] = field

        for base in reversed(bases):
            if hasattr(base, '_fields'):
                for attr, field in base._fields.items():
                    if not attr in namespace:
                        fields[attr] = field

        namespace['_fields'] = fields

        cls = super(ItemBuilder, cls).__new__(cls, name, bases, namespace)
        return cls


ItemBuilderMetaClass = ItemBuilder('ItemBuilderMetaClass', (object, ), {})


class Item(ItemBuilderMetaClass):
    def __init__(self, tree, selector_type='xpath', **kwargs):
        self._cache = {}
        self._meta = kwargs
        self._selector = Item._build_selector(tree, selector_type)

    @classmethod
    def _build_selector(self, tree, selector_type):
        if selector_type == 'xpath':
            return XpathSelector(tree)
        elif selector_type == 'json':
            return JsonSelector(tree)
        else:
            raise GrabMisuseError('Unknown selector type: %s' % selector_type)

    @classmethod
    def _get_selector_type(cls, default='xpath'):
        return getattr(cls.Meta, 'selector_type', default)

    @classmethod
    def find(cls, tree, **kwargs):
        # Backward Compatibility
        # First implementations of Item module required
        # `grab.doc` to be passed in `tree` option
        if isinstance(tree, Document):
            tree = tree.grab.tree

        selector_type = cls._get_selector_type(kwargs.pop('selector_type', 'xpath'))
        root_selector = Item._build_selector(tree, selector_type)

        fallback_find_query = getattr(cls.Meta, 'find_selector', '.')
        if hasattr(cls.Meta, 'find_selector'):
            logger.error('Meta.find_selector attribute is deprecated. Please use Meta.find_query attribute instead.')
        find_query = getattr(cls.Meta, 'find_query', fallback_find_query)

        for count, sel in enumerate(root_selector.select(find_query)):
            item = cls(sel.node, selector_type=selector_type, **kwargs)
            item._position = count
            yield item


    @classmethod
    def find_one(cls, *args, **kwargs):
        return list(cls.find(*args, **kwargs))[0]

    def _render(self, exclude=(), prefix=''):
        out = []
        for key, field in self._fields.items():
            if not key in exclude:
                if not isinstance(field, ItemListField):
                    out.append(prefix + '%s: %s' % (key, getattr(self, key)))
        for key, field in self._fields.items():
            if not key in exclude:
                if isinstance(field, ItemListField):
                    out.append(prefix + key + ':')
                    child_out = []
                    for item in getattr(self, key):
                        child_out.append(item._render(prefix=prefix + '  '))
                    out.append('\n'.join(child_out))
        out.append(prefix + '---')
        return '\n'.join(out)

    def update_object(self, obj, keys):
        for key in keys:
            setattr(obj, key, getattr(self, key))

    def update_dict(self, dct, keys):
        for key in keys:
            dct[key] = getattr(self, key)

    def get_dict(self, keys=None):
        if keys is None:
            keys = self._fields.keys()
        dct = {}
        for key in keys:
            dct[key] = getattr(self, key)
        return dct

    @classmethod
    def get_function(cls, key):
        """
        Return standalone function which was used to build FuncField field.
        """
        field = cls._fields[key]
        if field.pass_item:
            def func_wrapper(*args, **kwargs):
                return field.func(None, *args, **kwargs)
            return func_wrapper
        else:
            return field.func

    def __getstate__(self):
        """
        Delete `self._selector` object because it is not
        possible to picklize lxml-tree.
        Also calculate all fields as after deserialization
        it will not be possible to calculate any field.
        """
        for key in self._fields.keys():
            # trigger fields' content calcualation
            getattr(self, key)
        state = self.__dict__.copy()
        state['_selector'] = None
        return state

    @classmethod
    def extract_document_data(cls, grab):
        """
        Extract document data from grab object in format that is
        suitable to pass to `cls` Item constructor.
        """

        sel_type = cls._get_selector_type()
        if sel_type == 'xpath':
            return grab.tree
        elif sel_type == 'json':
            return grab.response.json
        else:
            raise GrabMisuseError('Unknown selector type: %s' % sel_type)

########NEW FILE########
__FILENAME__ = const
NETWORK_ERROR = {
    0: 'no error condition. Note: When the HTTP protocol returns a redirect no error will be reported. You can check if there is a redirect with the QNetworkRequest::RedirectionTargetAttribute attribute.',

    1: 'the remote server refused the connection (the server is not accepting requests)',
    2: 'the remote server closed the connection prematurely, before the entire reply was received and processed',
    3: 'the remote host name was not found (invalid hostname)',
    4: 'the connection to the remote server timed out',
    5: 'the operation was canceled via calls to abort() or close() before it was finished.',
    6: 'the SSL/TLS handshake failed and the encrypted channel could not be established. The sslErrors() signal should have been emitted.',
    7: 'the connection was broken due to disconnection from the network, however the system has initiated roaming to another access point. The request should be resubmitted and will be processed as soon as the connection is re-established.',
    101: 'the connection to the proxy server was refused (the proxy server is not accepting requests)',
    102: 'the proxy server closed the connection prematurely, before the entire reply was received and processed',
    103: 'the proxy host name was not found (invalid proxy hostname)',
    104: 'the connection to the proxy timed out or the proxy did not reply in time to the request sent',
    105: 'the proxy requires authentication in order to honour the request but did not accept any credentials offered (if any)',
    201: 'the access to the remote content was denied (similar to HTTP error 401)',
    202: 'the operation requested on the remote content is not permitted',
    203: 'the remote content was not found at the server (similar to HTTP error 404)',
    204: 'the remote server requires authentication to serve the content but the credentials provided were not accepted (if any)',
    205: 'the request needed to be sent again, but this failed for example because the upload data could not be read a second time.',
    301: 'the Network Access API cannot honor the request because the protocol is not known',
    302: 'the requested operation is invalid for this protocol',
    99: 'an unknown network-related error was detected',
    199: 'an unknown proxy-related error was detected',
    299: 'an unknown error related to the remote content was detected',
    399: 'a breakdown in protocol was detected (parsing error, invalid or unexpected responses, etc.)',
}

########NEW FILE########
__FILENAME__ = error
class KitError(Exception):
    pass

########NEW FILE########
__FILENAME__ = network_access_manager
import logging
from PyQt4.QtNetwork import QNetworkAccessManager, QNetworkRequest
from PyQt4.QtWebKit import QWebFrame
from PyQt4.QtCore import QByteArray

from grab.kit.const import NETWORK_ERROR
from grab.kit.network_reply import KitNetworkReply
from grab.kit.error import KitError
from grab.util.py3k_support import *

logger = logging.getLogger('grab.kit.network_access_manager')

class KitNetworkAccessManager(QNetworkAccessManager):
    def __init__(self, forbidden_extensions=[]):
        QNetworkAccessManager.__init__(self)
        self.forbidden_extensions = forbidden_extensions

    def setupCache(self, size=100 * 1024 * 1024, location='/tmp/.webkit_wrapper'):
        QDesktopServices.storageLocation(QDesktopServices.CacheLocation)
        cache = QNetworkDiskCache()
        cache.setCacheDirectory(location)
        cache.setMaximumCacheSize(size)
        self.setCache(cache)

    def setupProxy(self, proxy, proxy_userpwd=None, proxy_type='http'):
        if proxy_userpwd:
            username, password = proxy_userpwd.split(':', 1)
        else:
            username, password = '', ''
        host, port = proxy.split(':', 1)
        if proxy_type == 'http':
            proxy_type_obj = QNetworkProxy.HttpProxy
        elif proxy_type == 'socks5':
            proxy_type_obj = QNetworkProxy.Socks5Proxy
        else:
            raise KitError('Unknown proxy type: %s' % proxy_type)
        proxy_obj = QNetworkProxy(proxy_type_obj, host, int(port), username, password)
        self.setProxy(self, proxy_obj)

    def createRequest(self, operation, request, data):
        if operation == self.GetOperation:
            if not self.is_request_allowed(request):
                request.setUrl(QUrl('forbidden://localhost/'))
            else:
                logger.debug(u'Quering URL: %s' % request.url().toString())
        
        request.setAttribute(QNetworkRequest.CacheLoadControlAttribute,
                             QNetworkRequest.PreferCache)
        reply = QNetworkAccessManager.createRequest(self, operation, request, data)
        reply.error.connect(self.catch_error)
        reply = KitNetworkReply(self, reply)
        
        # add Base-Url header, then we can get it from QWebView
        # WTF?
        if isinstance(request.originatingObject(), QWebFrame):
            try:
                reply.setRawHeader(QByteArray('Base-Url'), QByteArray('').append(request.originatingObject().page().mainFrame().baseUrl().toString()))
            except Exception as e:
                logger.debug(e)

        return reply

    def is_request_allowed(self, request):
        #url = unicode(request.url().toString())
        path = unicode(request.url().path())
        ext = ''
        if path:
            if '.' in path:
                ext = path.rsplit('.', 1)[-1].lower()

        if self.forbidden_extensions and ext in self.forbidden_extensions:
            logger.debug('Url %s is not allowed because ext %s is forbiddend' % url, ext)
            return False

        return True

    def catch_error(self, eid):
        """
        Interpret the HTTP error ID received
        """

        if eid not in (5, 301):
            logger.error('Error %d: %s (%s)' % (eid, NETWORK_ERROR.get(eid, 'unknown error'),
                                                self.sender().url().toString()))

########NEW FILE########
__FILENAME__ = network_reply
"""
I really have no idea how it works ;-)
"""
from PyQt4.QtNetwork import QNetworkReply, QNetworkRequest

from grab.util.py3k_support import *

class KitNetworkReply(QNetworkReply):
    """
    Override QNetworkReply so can save the original data

    Credits:
    * https://code.google.com/p/webscraping/source/browse/webkit.py#154
    * http://gitorious.org/qtwebkit/performance/blobs/master/host-tools/mirror/main.cpp
    """
    def __init__(self, parent, original_reply):
        QNetworkReply.__init__(self, parent)
        self.original_reply = original_reply # reply to proxy
        self.data = '' # contains downloaded data
        self.buffer = '' # contains buffer of data to read
        self.setOpenMode(QNetworkReply.ReadOnly | QNetworkReply.Unbuffered)
        #print dir(reply)
        
        # connect signal from proxy reply
        self.original_reply.metaDataChanged.connect(self.applyMetaData)
        self.original_reply.readyRead.connect(self.readInternal)
        self.original_reply.error.connect(self.error)
        self.original_reply.finished.connect(self.finished)
        self.original_reply.uploadProgress.connect(self.uploadProgress)
        self.original_reply.downloadProgress.connect(self.downloadProgress)

    
    def __getattribute__(self, attr):
        """Send undefined methods straight through to proxied reply
        """
        # send these attributes through to proxy reply 
        if attr in ('operation', 'request', 'url', 'abort', 'close'):#, 'isSequential'):
            value = self.original_reply.__getattribute__(attr)
        else:
            value = QNetworkReply.__getattribute__(self, attr)
        #print attr, value
        return value
    
    def abort(self):
        pass # qt requires that this be defined
    
    def isSequential(self):
        return True

    def applyMetaData(self):
        for header in self.original_reply.rawHeaderList():
            self.setRawHeader(header, self.original_reply.rawHeader(header))

        headers = (
            QNetworkRequest.ContentTypeHeader,
            QNetworkRequest.ContentLengthHeader,
            QNetworkRequest.LocationHeader,
            QNetworkRequest.LastModifiedHeader,
            QNetworkRequest.SetCookieHeader,
        )
        for header in headers:
            self.setHeader(header, self.original_reply.header(header))

        attributes = (
            QNetworkRequest.HttpStatusCodeAttribute,
            QNetworkRequest.HttpReasonPhraseAttribute,
            QNetworkRequest.RedirectionTargetAttribute,
            QNetworkRequest.ConnectionEncryptedAttribute,
            QNetworkRequest.CacheLoadControlAttribute,
            QNetworkRequest.CacheSaveControlAttribute,
            QNetworkRequest.SourceIsFromCacheAttribute,

        )
        for attr in attributes:
            self.setAttribute(attr, self.original_reply.attribute(attr))

        # attribute is undefined
        #self.setAttribute(QNetworkRequest.DoNotBufferUploadDataAttribute, self.original_reply.attribute(QNetworkRequest.DoNotBufferUploadDataAttribute))
        self.metaDataChanged.emit()

    def bytesAvailable(self):
        """
        How many bytes in the buffer are available to be read
        """

        return len(self.buffer) + QNetworkReply.bytesAvailable(self)

    def readInternal(self):
        """
        New data available to read
        """

        s = self.original_reply.readAll()
        self.data += s
        self.buffer += s
        self.readyRead.emit()

    def readData(self, size):
        """
        Return up to size bytes from buffer
        """

        size = min(size, len(self.buffer))
        data, self.buffer = self.buffer[:size], self.buffer[size:]
        # py3 hack
        if PY3K:
            return bytes(data)
        else:
            return str(data)

########NEW FILE########
__FILENAME__ = kit_interface
class GrabKitInterface(object):
    def __init__(self, grab):
        self.grab = weakref.proxy(grab)

    def select(self, *args, **kwargs):
        from grab.selector import KitSelector

        qt_doc = self.grab.transport.kit.page.mainFrame().documentElement()
        return KitSelector(qt_doc).select(*args, **kwargs)

########NEW FILE########
__FILENAME__ = proxy
"""
Module contents:
* `Proxy` class represent single proxy server
* `ProxyList` class is interface to work with list of proxy servers
* `LocalFileSource` contains logic to load list of proxies from local file
* `RemoteFileSource contains logic to load list of proxies from remote document.
"""
import re
import itertools
import time
import logging
import random

from grab.error import GrabError, GrabNetworkError
from grab.util.py2old_support import *
from grab.util.py3k_support import *

RE_SIMPLE_PROXY = re.compile(r'^([^:]+):([^:]+)$')
RE_AUTH_PROXY = re.compile(r'^([^:]+):([^:]+):([^:]+):([^:]+)$')
logger = logging.getLogger('grab.proxy')


def parse_proxy_line(line):
    """
    Extract proxy details from the text line.

    The text line could be in one of the following formats:
    * host:port
    * host:port:username:password
    """

    match = RE_SIMPLE_PROXY.search(line)
    if match:
        host, port = match.groups()
        return host, port, None, None
    else:
        match = RE_AUTH_PROXY.search(line)
        if match:
            host, port, user, pwd = match.groups()
            return host, port, user, pwd
    raise GrabError('Invalid proxy line: %s' % line)


class Proxy(object):
    """
    Represents single proxy server.
    """

    def __init__(self, server, port, username=None, password=None, proxy_type='http'):
        self.server = server
        self.port = port
        self.username = username
        self.password = password
        self.proxy_type = proxy_type

    @property
    def address(self):
        return '%s:%s' % (self.server, self.port)

    @property
    def userpwd(self):
        return '%s:%s' % (self.username, self.password)

    def __cmp__(self, obj):
        if (self.server == obj.server and self.port == obj.port and
                self.username == obj.username and self.password == obj.password and
                self.proxy_type == obj.proxy_type):
            return 0
        else:
            return 1


def parse_proxy_data(data, data_format='text', proxy_type='http'):
    """
    Yield `Proxy` objects found in the given `data`.
    """
    if data_format == 'text':
        for line in data.splitlines():
            if not PY3K and isinstance(line, unicode):
                line = line.encode('utf-8')
            line = line.strip().replace(' ', '')
            if line and not line.startswith('#'):
                try:
                    host, port, user, pwd = parse_proxy_line(line)
                except GrabError as ex:
                    logger.error('Invalid proxy line: %s' % line)
                else:
                    yield Proxy(host, port, user, pwd, proxy_type)
    else:
        raise GrabError('Unknown proxy data format: %s' % data_format)


class ProxySource(object):
    """
    Generic proxy source interface.
    """

    def load(self):
        return list(parse_proxy_data(
            self.load_data(),
            data_format=self.data_format,
            proxy_type=self.proxy_type,
        ))


class LocalFileSource(ProxySource):
    """
    Proxy source that loads data from the file in local file system.
    """

    def __init__(self, location, safe_load=False, data_format='text', proxy_type='http'):
        self.location = location
        self.data_format = data_format
        self.safe_load = False
        self.proxy_type = proxy_type

    def load_data(self):
        try:
            return open(self.location).read()
        except Exception as ex:
            if self.safe_load:
                logger.error('', format_exc=ex)
                return ''
            else:
                raise


class RemoteFileSource(ProxySource):
    """
    Proxy source that loads data from the remote document.
    """

    def __init__(self, url, safe_load=False, data_format='text', proxy_type='http'):
        self.url = url
        self.data_format = data_format
        self.safe_load = False
        self.proxy_type = proxy_type

    def load_data(self):
        from grab import Grab

        g = Grab()
        try:
            g.go(self.url)
        except GrabNetworkError as ex:
            if self.safe_load:
                logger.error('', format_exc=ex)
                return ''
            else:
                raise
        else:
            return g.response.body

# List of aliases that is used in `ProxyList::set_source` function
SOURCE_TYPE_ALIAS = {
    'file': LocalFileSource,
    'url': RemoteFileSource,
}


class ProxyList(object):
    """
    Main class to work with proxy list.
    """

    def __init__(self, **kwargs):
        """
        Args:
            accumulate_updates: if it is True, then update existing proxy list with
                new proxy list when do reloading
        """
        self.source = None
        self.proxy_list = []
        self.iterator_index = 0
        self.setup(**kwargs)

    def setup(self, accumulate_updates=False, reload_time=600):
        self.accumulate_updates = accumulate_updates
        self.reload_time = reload_time

    def set_source(self, source_type='file', proxy_type='http', **kwargs):
        """
        Configure proxy list source and load proxies from that source.
        """

        if source_type in SOURCE_TYPE_ALIAS:
            source_cls = SOURCE_TYPE_ALIAS[source_type]
            self.source = source_cls(proxy_type=proxy_type, **kwargs)
            self.reload(force=True)
        else:
            raise GrabError('Unknown source type: %s' % source_type)

    def reload(self, force=False):
        """
        Reload proxies from the configured proxy list source.
        """

        now = time.time()
        if force or now - self.load_timestamp > self.reload_time:
            self.load_timestamp = now
            if not self.accumulate_updates:
                self.proxy_list = self.source.load()
                self.iterator_index = 0
            else:
                new_list = self.source.load()
                for item in new_list:
                    if not item in self.proxy_list:
                        self.proxy_list.append(item)

            self.proxy_list_iter = itertools.cycle(self.proxy_list)

    def get_random_proxy(self):
        """
        Return random server from the list
        """

        self.reload()
        self.iterator_index = random.randint(0, len(self.proxy_list) - 1)
        return self.proxy_list[self.iterator_index]

    def get_next_proxy(self):
        """
        Return next server in the list.
        """

        self.reload()
        if (self.iterator_index + 1) > len(self.proxy_list):
            self.iterator_index = 0
        proxy = self.proxy_list[self.iterator_index]
        self.iterator_index += 1
        return proxy

    def is_empty(self):
        """
        Check if the proxy list is empty.
        """

        return len(self.proxy_list) == 0

########NEW FILE########
__FILENAME__ = proxylist
"""
THIS IS DEPRECATED MODULE.
USE grab.proxy MODULE INSTEAD.

Module to work with proxy list.

Usage:

    pl = ProxyList('var/proxy.txt', 'socks5')
    g = Grab()
    server, userpwd = pl.get_random()
    g.setup(proxy=server, userpwd=userpwd)

Or you can do even simplier:

    g = Grab()
    g.setup(proxylist=('var/proxy.txt', 'socks5'))
    g.change_proxy()

"""
import itertools
from random import choice
import re
import logging
from copy import deepcopy
import time
import logging
try:
    from urllib2 import urlopen, URLError, HTTPError
except ImportError:
    from urllib.request import urlopen
    from urllib.error import URLError, HTTPError

from grab.error import GrabError, GrabNetworkError, GrabMisuseError
from grab.util.py2old_support import *
from grab.util.py3k_support import *

READ_TIMEOUT = 60 * 10
RE_SIMPLE_PROXY = re.compile(r'^([^:]+):([^:]+)$')
RE_AUTH_PROXY = re.compile(r'^([^:]+):([^:]+):([^:]+):([^:]+)$')
logger = logging.getLogger('grab.proxylist')


def parse_proxyline(line):
    """
    Extract proxy details from the text line.
    """

    match = RE_SIMPLE_PROXY.search(line)
    if match:
        host, port = match.groups()
        return host, port, None, None
    else:
        match = RE_AUTH_PROXY.search(line)
        if match:
            host, port, user, pwd = match.groups()
            return host, port, user, pwd
    raise GrabError('Invalid proxy line: %s' % line)


class ProxySource(object):

    def __init__(self, source, read_timeout=READ_TIMEOUT, proxy_type='http'):
        self.source = source
        self.read_timeout = read_timeout
        self.proxy_type = proxy_type
        self.read_time = None


    def parse_lines(self, proxies):
        """
        Parse each line from proxies list step by step.
        Returns tuple with server (host:port) and user_pwd (user:password)
        :param lines: list which contains proxy servers
        """

        for proxy in proxies:
            if not PY3K and isinstance(proxy, unicode):
                # Convert to string (py2.x)
                proxy = proxy.encode('utf-8')
            proxy = proxy.strip().replace(' ', '')
            if proxy:
                host, port, user, pwd = parse_proxyline(proxy)
                server = '%s:%s' % (host, port)
                user_pwd = None
                if user:
                    user_pwd = '%s:%s' % (user, pwd)
                yield server, user_pwd

    def get_server_list(self, proxylist):
        if not PY3K and isinstance(proxylist, unicode):
            # Convert to string (py2.x)
            proxylist = proxylist.encode('utf-8')
        if isinstance(proxylist, str):
            proxylist = proxylist.split()
        servers = []
        for server, user_pwd in self.parse_lines(proxylist):
            servers.append((server, user_pwd, self.proxy_type))
        return servers

    def load(self):
        pass

    def reload(self):
        """
        Update proxy list.
        
        Re-read proxy file after each XX seconds.
        """
        
        if (self.read_time is None or
            (time.time() - self.read_time) > self.read_timeout):
            logger.debug('Reloading proxy list')
            self.load()
            return True
        else:
            return False


class TextFileSource(ProxySource):
    source_type = 'text_file'

    def load(self):
        """
        Load proxy list from specified source and validate loaded data.

        Each server could be in two forms:
        * simple: "server:port"
        * complex: "server:port:user:pwd"
        """

        with open(self.source) as src:
            lines = src.read().splitlines()

        self.read_time = time.time()
        self.server_list = self.get_server_list(lines)
        self.server_list_iterator = itertools.cycle(self.server_list)


class URLSource(ProxySource):
    source_type = 'url'

    def load(self):
        """
        Load proxy list from specified URL and validate loaded data.

        Each proxy server could be in two forms:
        * simple: "server:port"
        * complex: "server:port:user:pwd"
        """
        try:
            proxylist = urlopen(self.source).readlines()
        except (URLError, HTTPError):
            raise GrabNetworkError("Can't load proxies from URL (%s)" % self.source)

        self.read_time = time.time()
        self.server_list = self.get_server_list(proxylist)
        self.server_list_iterator = itertools.cycle(self.server_list)


class ListSource(ProxySource):
    source_type = 'list'

    def load(self):
        """
        Load proxies from given list.

        Each proxy server could be in two forms:
        * simple: "server:port"
        * complex: "server:port:user:pwd"
        """

        if not isinstance(self.source, list):
            raise GrabMisuseError("Given proxy list isn't a list type")
        self.server_list = self.get_server_list(self.source)
        self.server_list_iterator = itertools.cycle(self.server_list)

    def reload(self):
        pass


class StringSource(ProxySource):
    source_type = 'string'

    def load(self):
        """
        Load proxies from given string. String can be multiline.

        Each proxy server could be in two forms:
        * simple: "server:port"
        * complex: "server:port:user:pwd"
        """

        if not isinstance(self.source, (str, unicode)):
            raise GrabMisuseError("Given proxy list isn't a string or unicode type")
        self.server_list = self.get_server_list(self.source)
        self.server_list_iterator = itertools.cycle(self.server_list)

    def reload(self):
        pass



SOURCE_LIST = {
    'text_file': TextFileSource,
    'url': URLSource,
    'list': ListSource,
    'string': StringSource,
}


class ProxyList(object):
    """
    Class to work with proxy list which 
    is stored in the plain text file.
    """

    def __init__(self, source, source_type, proxy_type='http', **kwargs):
        """
        Create `ProxyList` object and load proxies from the specified source.

        You should specify type of source in second argument to let ProxyList
        instance know how to handle proxy source.

        :param source: source of the project (file name, string or some object)
        :param source_type: type of proxy source
        :param proxy_type: default type of proxy (if proxy source does not provide
            this information)
        :param **kwargs: any additional aruguments goes to specific proxy load method 
        """

        self.init_kwargs = deepcopy(kwargs)

        try:
            source_class = SOURCE_LIST[source_type]
        except AttributeError:
            raise GrabMisuseError('Unknown proxy source type: %s' % source_type)
        self.source = source_class(source, proxy_type=proxy_type, **kwargs)
        self.source.load()
        self.filter_config = {}
        self.geoip_resolver = None

    def filter_by_country(self, code, geoip_db_path):
        # geoip_db_path -yep, this is quick & crapy workaround
        self.filter_config['country'] = {'code': code.lower(),
                                         'geoip_db_path': geoip_db_path}
        self.apply_filter()

    def apply_filter(self):
        if self.filter_config.get('country'):
            geoip = self.get_geoip_resolver()
            new_list = []
            for row in self.source.server_list:
                server, userpwd, proxy_type = row
                host = server.split(':')[0]
                country = geoip.country_code_by_addr(host).lower()
                if country == self.filter_config['country']['code']:
                    new_list.append(row)
            self.source.server_list = row
            self.source.server_list_iterator = itertools.cycle(self.source.server_list)

    def get_geoip_resolver(self):
        if self.geoip_resolver is None:
            import pygeoip
            self.geoip_resolver = pygeoip.GeoIP(self.filter_config['country']['geoip_db_path'],
                                                pygeoip.MEMORY_CACHE)
        return self.geoip_resolver

    def get_random(self):
        """
        Return random server from the list
        """

        if self.source.reload():
            self.apply_filter()
        return choice(self.source.server_list)

    def get_next(self):
        """
        Return next server in the list.
        """

        logger.debug('Changing proxy')
        if self.source.reload():
            self.apply_filter()
        return next(self.source.server_list_iterator)

########NEW FILE########
__FILENAME__ = reference
from grab.selector import XpathSelector

class Reference(object):
    def __init__(self, node, query=None, query_args=None):
        self._node = node
        self._query = query
        self._query_args = {} if query_args is None else query_args

    def __getattr__(self, key):
        return Reference(self._node, query=key)

    def _selector(self):
        return XpathSelector(self._node)

    def _text(self):
        return self._selector().select('.//%s' % self._query).text()

    def _node(self):
        return self._selector().node()

    #def __call__(self, **kwargs):
        #self.query_args.update(kwargs)
        #return self

########NEW FILE########
__FILENAME__ = response
# Back-ward compatibility
from grab.document import *
from grab.document import Document as Response

########NEW FILE########
__FILENAME__ = crawl
import logging
import os
from argparse import ArgumentParser

from grab.util.config import build_spider_config, build_global_config
from grab.util.module import load_spider_class
from grab.tools.logs import default_logging
from grab.tools.lock import assert_lock
from grab.spider.save_result import save_result
from grab.tools.files import clear_directory

logger = logging.getLogger('grab.script.crawl')

def setup_arg_parser(parser):
    parser.add_argument('spider_name', type=str)
    parser.add_argument('-t', '--thread-number', default=None, type=int,
                        help='Number of network threads')
    parser.add_argument('--slave', action='store_true', default=False,
                        help='Enable the slave-mode')
    parser.add_argument('-n', '--network-logs', action='store_true', default=False,
                        help='Dump to console details about network requests')
    parser.add_argument('--save-result', action='store_true', default=False,
                        help='Save crawling state to database')
    parser.add_argument('--disable-proxy', action='store_true', default=False,
                        help='Disable proxy servers')


@save_result
def main(spider_name, thread_number=None, slave=False,
         settings='settings', network_logs=False,
         disable_proxy=False, 
         *args, **kwargs):
    default_logging(propagate_network_logger=network_logs)

    lock_key = None
    if not slave:
        lock_key = 'crawl.%s' % spider_name
    if lock_key is not None:
        lock_path = 'var/run/%s.lock' % lock_key
        logger.debug('Trying to lock file: %s' % lock_path)
        assert_lock(lock_path)

    config = build_global_config(settings)
    spider_class = load_spider_class(config, spider_name)
    spider_config = build_spider_config(spider_class, config)

    if hasattr(spider_class, 'setup_extra_args'):
        parser = ArgumentParser()
        spider_class.setup_extra_args(parser)
        extra_args, trash = parser.parse_known_args()
        spider_config['extra_args'] = vars(extra_args)

    if thread_number is None:
        thread_number = spider_config.getint('GRAB_THREAD_NUMBER')

    stat_task_object = kwargs.get('stat_task_object', None)

    bot = spider_class(
        thread_number=thread_number,
        slave=slave,
        config=spider_config,
        network_try_limit=spider_config.getint('GRAB_NETWORK_TRY_LIMIT'),
        task_try_limit=spider_config.getint('GRAB_TASK_TRY_LIMIT'),
    )
    if spider_config.get('GRAB_QUEUE'):
        bot.setup_queue(**spider_config['GRAB_QUEUE'])
    if spider_config.get('GRAB_CACHE'):
        bot.setup_cache(**spider_config['GRAB_CACHE'])
    if spider_config.get('GRAB_PROXY_LIST'):
        if disable_proxy:
            logger.debug('Proxy servers disabled via command line')
        else:
            bot.load_proxylist(**spider_config['GRAB_PROXY_LIST'])
    if spider_config.get('GRAB_COMMAND_INTERFACES'):
        for iface_config in spider_config['GRAB_COMMAND_INTERFACES']:
            bot.controller.add_interface(**iface_config)

    # Dirty hack
    # FIXIT: REMOVE
    bot.dump_spider_stats = kwargs.get('dump_spider_stats')
    bot.stats_object = kwargs.get('stats_object')

    try:
        bot.run()
    except KeyboardInterrupt:
        pass

    stats = bot.render_stats(timing=config.get('GRAB_DISPLAY_TIMING'))

    if spider_config.get('GRAB_DISPLAY_STATS'):
        logger.debug(stats)

    pid = os.getpid()
    logger.debug('Spider pid is %d' % pid)

    if config.get('GRAB_SAVE_REPORT'):
        for subdir in (str(pid), 'last'):
            dir_ = 'var/%s' % subdir
            if not os.path.exists(dir_):
                os.mkdir(dir_)
            else:
                clear_directory(dir_)
            bot.save_list('fatal', '%s/fatal.txt' % dir_)
            bot.save_list('task-count-rejected', '%s/task_count_rejected.txt' % dir_)
            bot.save_list('network-count-rejected', '%s/network_count_rejected.txt' % dir_)
            bot.save_list('task-with-invalid-url', '%s/task_with_invalid_url.txt' % dir_)
            with open('%s/report.txt' % dir_, 'wb') as out:
                out.write(stats)

    return {
        'spider_stats': bot.render_stats(timing=False),
        'spider_timing': bot.render_timing(),
    }

########NEW FILE########
__FILENAME__ = fix_mongo_cache
import time

from grab.spider import Spider

def setup_arg_parser(parser):
    parser.add_argument('--database')


def main(*args, **kwargs):
    bot = Spider()
    opts = {'database': kwargs['database']}
    bot.setup_cache(backend='mongo', **opts)

    ts = int(time.time())
    count = 0
    for item in bot.cache.db.cache.find({'timestamp': {'$exists': False}}, timeout=False):
        bot.cache.db.cache.update({'_id': item['_id']},
                                  {'$set': {'timestamp': ts}})
        count += 1
    print('Records updated: %d' % count)

########NEW FILE########
__FILENAME__ = fix_mysql_cache
import time

from grab.spider import Spider

def setup_arg_parser(parser):
    parser.add_argument('--user')
    parser.add_argument('--passwd')
    parser.add_argument('--database')


def main(*args, **kwargs):
    bot = Spider()
    opts = {'database': kwargs['database']}
    if kwargs.get('user'):
        opts['user'] = kwargs['user']
    if kwargs.get('passwd'):
        opts['passwd'] = kwargs['passwd']
    bot.setup_cache(backend='mysql', **opts)

    cursor = bot.cache.conn.cursor()
    cursor.execute('SELECT * FROM cache LIMIT 1')
    cols = [x[0] for x in cursor.description]
    ts = int(time.time())
    if not 'timestamp' in cols:
        print('Cache table does not have timestamp column. Adding it...')
        cursor.execute('''
            ALTER TABLE cache
            ADD COLUMN timestamp INT NOT NULL DEFAULT %s''' % ts)

########NEW FILE########
__FILENAME__ = start_project
import os
import logging
import shutil
import re

logger = logging.getLogger('grab.script.start_project')

def setup_arg_parser(parser):
    parser.add_argument('project_name')
    parser.add_argument('--template')


def process_macros(content, context):
    changed = False
    for key, value in context.items():
        re_macros = re.compile(r'\{\{\s*%s\s*\}\}' % re.escape(key))
        if re_macros.search(content):
            changed = True
            content = re_macros.sub(value, content)
    return changed, content


def underscore_to_camelcase(val):
    items = val.lower().split('_')
    return ''.join(x.title() for x in items)


def main(project_name, template, **kwargs):
    cur_dir = os.getcwd()
    project_dir = os.path.join(cur_dir, project_name)

    if template is None:
        grab_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
        template_path = os.path.join(grab_root, 'util/default_project')
    else:
        template_path = template

    if os.path.exists(project_dir):
        logger.error
        raise Exception('Directory %s already exists' % project_dir)
    else:
        logger.debug('Copying %s to %s' % (template_path, project_dir))
        shutil.copytree(template_path, project_dir)

        for base, dir_names, file_names in os.walk(project_dir):
            for file_name in file_names:
                if file_name.endswith('.py'):
                    file_path = os.path.join(base, file_name)
                    context = {
                        'PROJECT_NAME': project_name,
                        'PROJECT_NAME_CAMELCASE': underscore_to_camelcase(project_name),
                    }
                    changed, content = process_macros(open(file_path).read(), context)
                    if changed:
                        with open(file_path, 'w') as out:
                            out.write(content)

########NEW FILE########
__FILENAME__ = test_item
from grab import Grab

def setup_arg_parser(parser):
    parser.add_argument('item_path')
    parser.add_argument('--all', action='store_true', default=False)


def main(item_path, **kwargs):
    mod_path, cls_name = item_path.rsplit('.', 1)
    mod = __import__(mod_path, None, None, ['foo'])
    cls = getattr(mod, cls_name)

    if kwargs.get('all'):
        urls = cls.Meta.example_urls
    else:
        urls = [cls.Meta.example_url]

    g = Grab()
    for url in urls:
        try:
            g.go(url)
        except Exception as ex: 
            print('Fatal exception:')
            print(ex)
        else:
            print('URL: %s' % url )
            for count, item in enumerate(cls.find(cls.extract_document_data(g),
                                                  url=g.response.url)):
                print('%s #%d' % (cls.__name__, count))
                print(item._render())
                print('----------')

########NEW FILE########
__FILENAME__ = selector
"""
Selector module provides high usability interface to lxml tree
"""
import logging
import time
try:
    from pyquery import PyQuery
except ImportError:
    pass
from abc import ABCMeta, abstractmethod

from grab.tools.lxml_tools import get_node_text, render_html
from grab.tools.text import find_number, normalize_space as normalize_space_func
from grab.error import GrabMisuseError, DataNotFound, warn
from grab.tools import rex as rex_tools
from grab.tools.text import normalize_space
from grab.tools.html import decode_entities
import grab.base
from grab.const import NULL
from grab.util.py3k_support import *

__all__ = ['Selector', 'TextSelector', 'XpathSelector', 'PyquerySelector',
           'KitSelector', 'JsonSelector']
XPATH_CACHE = {}
logger = logging.getLogger('grab.selector.selector')

metaclass_ABCMeta = ABCMeta('metaclass_ABCMeta', (object, ), {})

class SelectorList(object):
    __slots__ = ('selector_list', 'origin_selector_class', 'origin_query')

    def __init__(self, selector_list, origin_selector_class, origin_query):
        self.selector_list = selector_list
        self.origin_selector_class = origin_selector_class
        self.origin_query = origin_query

    def __getitem__(self, x):
        return self.selector_list[x]

    def __len__(self):
        return self.count()

    def count(self):
        return len(self.selector_list)

    def one(self, default=NULL):
        try:
            return self.selector_list[0]
        except IndexError:
            if default is NULL:
                raise DataNotFound('Could not get first item for %s query of class %s' % (
                    self.origin_query, self.origin_selector_class.__name__))
            else:
                return default

    def node(self, default=NULL):
        try:
            return self.one().node
        except IndexError:
            if default is NULL:
                raise DataNotFound('Could not get first node for %s query of class %s' % (
                    self.origin_query, self.origin_selector_class.__name__))
            else:
                return default

    def text(self, default=NULL, smart=False, normalize_space=True):
        try:
            sel = self.one()
        except IndexError:
            if default is NULL:
                raise
            else:
                return default
        else:
            return sel.text(smart=smart, normalize_space=normalize_space)

    def text_list(self, smart=False, normalize_space=True):
        result_list = []
        for item in self.selector_list:
            result_list.append(item.text())
        return result_list

    def html(self, default=NULL, encoding='unicode'):
        try:
            sel = self.one()
        except IndexError:
            if default is NULL:
                raise
            else:
                return default
        else:
            return sel.html(encoding=encoding)

    def number(self, default=NULL, ignore_spaces=False,
               smart=False, make_int=True):
        """
        Find number in normalized text of node which matches the given xpath.
        """

        try:
            sel = self.one()
        except IndexError:
            if default is NULL:
                raise
            else:
                return default
        else:
            return sel.number(ignore_spaces=ignore_spaces, smart=smart,
                              default=default, make_int=make_int)

    def exists(self):
        """
        Return True if selctor list is not empty.
        """

        return len(self.selector_list) > 0


    def assert_exists(self):
        """
        Return True if selctor list is not empty.
        """

        if not self.exists():
            raise DataNotFound(u'Node does not exists, query: %s, query type: %s' % (
                self.origin_query, self.origin_selector_class.__name__))

    def attr(self, key, default=NULL):
        try:
            sel = self.one()
        except IndexError:
            if default is NULL:
                raise
            else:
                return default
        else:
            return sel.attr(key, default=default)

    def attr_list(self, key, default=NULL):
        result_list = []
        for item in self.selector_list:
            result_list.append(item.attr(key, default=default))
        return result_list

    def rex(self, regexp, flags=0, byte=False, default=NULL):
        try:
            sel = self.one()
        except IndexError:
            if default is NULL:
                raise
            else:
                return default
        else:
            return self.one().rex(regexp, flags=flags, byte=byte)

    def node_list(self):
        return [x.node for x in self.selector_list]

    def select(self, query):
        result = SelectorList([], self.origin_selector_class,
                              self.origin_query + ' + ' + query)
        for count, selector in enumerate(self.selector_list):
            result.selector_list.extend(selector.select(query))
        return result


class BaseSelector(metaclass_ABCMeta):
    __slots__ = ('node')

    def __init__(self, node):
        self.node = node

    def select(self, query):
        start = time.time()
        selector_list = self.wrap_node_list(self.process_query(query), query)
        total = time.time() - start
        grab.base.GLOBAL_STATE['selector_time'] += total
        return selector_list

    def wrap_node_list(self, nodes, query):
        selector_list = []
        for node in nodes:
            if isinstance(node, basestring):
                selector_list.append(TextSelector(node))
            else:
                selector_list.append(self.__class__(node))
        return SelectorList(selector_list, self.__class__, query)

    @abstractmethod
    def html(self):
        raise NotImplementedError

    @abstractmethod
    def attr(self):
        raise NotImplementedError

    @abstractmethod
    def text(self):
        raise NotImplementedError

    def number(self, default=NULL, ignore_spaces=False,
               smart=False, make_int=True):
        try:
            return find_number(self.text(smart=smart), ignore_spaces=ignore_spaces,
                               make_int=make_int)
        except IndexError:
            if default is NULL:
                raise
            else:
                return default

    def rex(self, regexp, flags=0, byte=False):
        norm_regexp = rex_tools.normalize_regexp(regexp, flags)
        matches = list(norm_regexp.finditer(self.html()))
        return RexResultList(matches, source_rex=norm_regexp)


class LxmlNodeBaseSelector(BaseSelector):
    __slots__ = ()

    def html(self, encoding='unicode'):
        return render_html(self.node, encoding=encoding)

    def attr(self, key, default=NULL):
        if default is NULL:
            if key in self.node.attrib:
                return self.node.get(key)
            else:
                raise DataNotFound(u'No such attribute: %s' % key)
        else:
            return self.node.get(key, default)

    def text(self, smart=False, normalize_space=True):
        elem = self.node
        if isinstance(elem, basestring):
            if normalize_space:
                return normalize_space_func(elem)
            else:
                return elem
        else:
            return get_node_text(elem, smart=smart, normalize_space=normalize_space)

    def number(self, default=NULL, ignore_spaces=False,
               smart=False, make_int=True):
        try:
            return find_number(self.text(smart=smart), ignore_spaces=ignore_spaces,
                               make_int=make_int)
        except IndexError:
            if default is NULL:
                raise
            else:
                return default

    def rex(self, regexp, flags=0, byte=False):
        norm_regexp = rex_tools.normalize_regexp(regexp, flags)
        matches = list(norm_regexp.finditer(self.html()))
        return RexResultList(matches, source_rex=norm_regexp)


class RexResultList(object):
    __slots__ = ('items', 'source_rex')

    def __init__(self, items, source_rex):
        self.items = items
        self.source_rex = source_rex

    def one(self):
        return self.items[0]

    def text(self, default=NULL):
        try:
            return normalize_space(decode_entities(self.one().group(1)))
        except (AttributeError, IndexError):
            if default is NULL:
                raise
            else:
                return default

    def number(self):
        return int(self.text())


class TextSelector(LxmlNodeBaseSelector):
    __slots__ = ()

    def select(self, xpath=None):
        raise GrabMisuseError('TextSelector does not allow select method') 

    def html(self, encoding='unicode'):
        return self.node

    def attr(self, key, default=NULL):
        raise GrabMisuseError('TextSelector does not allow attr method') 


class XpathSelector(LxmlNodeBaseSelector):
    __slots__ = ()

    def process_query(self, query):
        from lxml.etree import XPath

        if not query in XPATH_CACHE:
            obj = XPath(query)
            XPATH_CACHE[query] = obj
        xpath_obj = XPATH_CACHE[query]

        result = xpath_obj(self.node)

        # If you query XPATH like //some/crap/@foo="bar" then xpath function
        # returns boolean value instead of list of something.
        # To work around this problem I just returns empty list.
        # This is not great solutions but it produces less confusing error.
        if isinstance(result, bool):
            result = []

        return result


class PyquerySelector(LxmlNodeBaseSelector):
    __slots__ = ()

    def pyquery_node(self):
        return PyQuery(self.node)

    def process_query(self, query):
        return self.pyquery_node().find(pyquery)


class KitSelector(BaseSelector):
    __slots__ = ()

    def process_query(self, query):
        return self.node.findAll(query)

    def html(self, encoding='unicode'):
        xml = self.node.toOuterXml()
        if encoding == 'unicode':
            return xml
        else:
            return xml.encode(encoding)

    def attr(self, key, default=NULL):
        if default is NULL:
            val = unicode(self.node.attribute(key, u'@NOTFOUND@'))
            if val == u'@NOTFOUND@':
                raise DataNotFound(u'No such attribute: %s' % key)
            else:
                return val
        else:
            return unicode(self.node.attribute(key, default))

    def text(self, smart=False, normalize_space=True):
        return unicode(self.node.toPlainText())


class JsonSelector(BaseSelector):
    __slots__ = ()

    # TODO: It seems there is perfomance problem
    # see finnetrix, media_list.json
    def __init__(self, node):
        """
        `node` is deserialized JSON i.e. it is a native python structure
        """
        import jsonpath_rw

        self.node = jsonpath_rw.parse('`this`').find(node)[0]

    def process_query(self, query):
        import jsonpath_rw

        return jsonpath_rw.parse(query).find(self.node)

    def html(self, encoding='unicode'):
        raise NotImplementedError

    def attr(self, key, default=NULL):
        raise NotImplementedError

    def text(self, smart=False, normalize_space=True):
        return unicode(self.node.value)


# ****************
# Deprecated Stuff
# ****************

class Selector(XpathSelector):
    __slots__ = ()

    def __init__(self, *args, **kwargs):
        super(Selector, self).__init__(*args, **kwargs)
        warn('Selector class is deprecated. Please use XpathSelector class instead.')

########NEW FILE########
__FILENAME__ = base
import types
import signal
import inspect
import traceback
import logging
from collections import defaultdict
import os
import time
import json
try:
    import cPickle as pickle
except ImportError:
    import pickle
try:
    import anydbm as dbm
except ImportError:
    import dbm
import multiprocessing
import zlib
from hashlib import sha1
try:
    from urlparse import urljoin
except ImportError:
    from urllib.parse import urljoin
from random import randint
try:
    import Queue as queue
except ImportError:
    import queue
from copy import deepcopy

from grab.base import GLOBAL_STATE, Grab
from grab.error import GrabInvalidUrl
from grab.spider.error import (SpiderError, SpiderMisuseError, FatalError,
                               StopTaskProcessing, NoTaskHandler, NoDataHandler)
from grab.spider.task import Task, NullTask
from grab.spider.data import Data
from grab.spider.pattern import SpiderPattern
from grab.spider.stat  import SpiderStat
from grab.spider.transport.multicurl import MulticurlTransport
from grab.proxylist import ProxyList
from grab.spider.command_controller import CommandController
from grab.util.misc import camel_case_to_underscore
from grab.util.py2old_support import *
from grab.util.py3k_support import *

DEFAULT_TASK_PRIORITY = 100
RANDOM_TASK_PRIORITY_RANGE = (50, 100)
NULL = object()

logger = logging.getLogger('grab.spider.base')
logger_verbose = logging.getLogger('grab.spider.base.verbose')
# If you need verbose logging just
# change logging level of that logger
logger_verbose.setLevel(logging.FATAL)

class SpiderMetaClass(type):
    """
    This meta class does following things::
    
    * It creates Meta attribute if it does not defined in Spider descendant class by
        copying parent's Meta attribute
    * It reset Meta.abstract to False if Meta is copied from parent class
    * If defined Meta does not contains `abstract` attribute then define it and set to False
    """

    def __new__(cls, name, bases, namespace):
        if not 'Meta' in namespace:
            for base in bases:
                if hasattr(base, 'Meta'):
                    # copy contents of base Meta
                    meta = type('Meta', (object,), dict(base.Meta.__dict__))
                    # reset abstract attribute
                    meta.abstract = False
                    namespace['Meta'] = meta
                    break

        # Process special case (SpiderMetaClassMixin)
        if not 'Meta' in namespace:
            namespace['Meta'] = type('Meta', (object,), {})

        if not hasattr(namespace['Meta'], 'abstract'):
            namespace['Meta'].abstract = False

        return super(SpiderMetaClass, cls).__new__(cls, name, bases, namespace)


# See http://mikewatkins.ca/2008/11/29/python-2-and-3-metaclasses/
SpiderMetaClassMixin = SpiderMetaClass('SpiderMetaClassMixin', (object,), {})


class Spider(SpiderMetaClassMixin, SpiderPattern, SpiderStat):
    """
    Asynchronious scraping framework.
    """

    # You can define here some urls and initial tasks
    # with name "initial" will be created from these
    # urls
    # If the logic of generating initial tasks is complex
    # then consider to use `task_generator` method instead of
    # `initial_urls` attribute
    initial_urls = None

    # The base url which is used to resolve all relative urls
    # The resolving takes place in `add_task` method
    base_url = None

    middlewares = []
    middleware_points = {
        'response': [],
    }

    class Meta:
        # Meta.abstract means that this class whil not be
        # collected to spider registry by `grab crawl` CLI command.
        # The Meta is inherited by descendant classes BUT
        # Meta.abstract is reset to False in each desendant
        abstract = True

    def __init__(self, thread_number=3,
                 network_try_limit=10, task_try_limit=10,
                 request_pause=NULL,
                 priority_mode='random',
                 meta=None,
                 only_cache=False,
                 config=None,
                 slave=False,
                 max_task_generator_chunk=None,
                 # New options start here
                 waiting_shutdown_event=None,
                 taskq=None,
                 result_queue=None,
                 network_response_queue=None,
                 shutdown_event=None,
                 generator_done_event=None,
                 ng=False,
                 ):
        """
        Arguments:
        * thread-number - Number of concurrent network streams
        * network_try_limit - How many times try to send request
            again if network error was occuried, use 0 to disable
        * network_try_limit - Limit of tries to execute some task
            this is not the same as network_try_limit
            network try limit limits the number of tries which
            are performed automaticall in case of network timeout
            of some other physical error
            but task_try_limit limits the number of attempts which
            are scheduled manually in the spider business logic
        * priority_mode - could be "random" or "const"
        * meta - arbitrary user data
        * retry_rebuid_user_agent - generate new random user-agent for each
            network request which is performed again due to network error
        New options:
        * waiting_shutdown_event=None,
        * taskq=None,
        * result_queue=None,
        * newtork_response_queue=None,
        * shutdown_event=None,
        * generator_done_event=None):
        """

        # New options starts
        self.waiting_shutdown_event = waiting_shutdown_event
        self.taskq = taskq
        self.result_queue = result_queue
        self.shutdown_event = shutdown_event
        self.generator_done_event = generator_done_event
        self.network_response_queue = network_response_queue
        self.ng = ng
        # New options ends

        self.slave = slave

        self.max_task_generator_chunk = max_task_generator_chunk
        self.timers = {
            'network-name-lookup': 0,
            'network-connect': 0,
            'network-total': 0,
        }
        self.time_points = {}
        self.start_timer('total')
        if config is not None:
            self.config = config
        else:
            # Fix curcular import error
            from grab.util.config import Config
            self.config = Config()

        if meta:
            self.meta = meta
        else:
            self.meta = {}

        self.task_generator_enabled = False
        self.only_cache = only_cache
        self.thread_number = thread_number
        self.counters = defaultdict(int)
        self.grab_config = {}
        self.items = {}
        self.task_try_limit = task_try_limit
        self.network_try_limit = network_try_limit
        if priority_mode not in ['random', 'const']:
            raise SpiderMisuseError('Value of priority_mode option should be "random" or "const"')
        else:
            self.priority_mode = priority_mode

        try:
            signal.signal(signal.SIGUSR1, self.sigusr1_handler)
        except (ValueError, AttributeError):
            pass

        try:
            signal.signal(signal.SIGUSR2, self.sigusr2_handler)
        except (ValueError, AttributeError):
            pass

        # Initial cache-subsystem values
        self.cache_enabled = False
        self.cache = None

        self.work_allowed = True
        if request_pause is not NULL:
            logger.error('Option `request_pause` is deprecated and is not supported anymore')

        self.proxylist_enabled = None
        self.proxylist = None
        self.proxy = None
        self.proxy_auto_change = False

        # FIXIT: REMOVE
        self.dump_spider_stats = None

        self.controller = CommandController(self)

        # snapshots contains information about spider's state
        # for each 10 seconds interval
        self.snapshots = {}
        self.last_snapshot_values = {
            'timestamp': 0,
            'download-size': 0,
            'upload-size': 0,
            'download-size-with-cache': 0,
            'request-count': 0,
        }
        self.snapshot_timestamps = []
        self.snapshot_interval = self.config.get('GRAB_SNAPSHOT_CONFIG', {}).get('interval', 10)
        self.snapshot_file = self.config.get('GRAB_SNAPSHOT_CONFIG', {}).get('file', None)
        if self.snapshot_file:
            open(self.snapshot_file, 'w').write('')

    def setup_middleware(self, middleware_list):
        for item in middleware_list:
            self.middlewares.append(item)
            mod_path, cls_name = item.rsplit('.', 1)
            mod = __import__(mod_path, None, None, ['foo'])
            cls = getattr(mod, cls_name)
            mid = cls()
            if hasattr(mid, 'process_response'):
                self.middleware_points['response'].append(mid)

    def setup_cache(self, backend='mongo', database=None, use_compression=True, **kwargs):
        if database is None:
            raise SpiderMisuseError('setup_cache method requires database option')
        self.cache_enabled = True
        mod = __import__('grab.spider.cache_backend.%s' % backend,
                         globals(), locals(), ['foo'])
        self.cache = mod.CacheBackend(database=database, use_compression=use_compression,
                                      spider=self, **kwargs)

    def setup_queue(self, backend='memory', **kwargs):
        logger.debug('Using %s backend for task queue' % backend)
        mod = __import__('grab.spider.queue_backend.%s' % backend,
                         globals(), locals(), ['foo'])
        self.taskq = mod.QueueBackend(spider_name=self.get_name(),
                                      **kwargs)

    def prepare(self):
        """
        You can do additional spider customizatin here
        before it has started working. Simply redefine
        this method in your Spider class.
        """

    def sigusr1_handler(self, signal, frame):
        """
        Catches SIGUSR1 signal and dumps current state
        to temporary file
        """

        with open('/tmp/spider.state', 'w') as out:
            out.write(self.render_stats())

    def sigusr2_handler(self, signal, frame):
        """
        Catches SIGUSR1 signal and shutdowns spider.
        """
        
        logger.error('Received SIGUSR2 signal. Doing shutdown')
        self.stop()

    def setup_grab(self, **kwargs):
        self.grab_config.update(**kwargs)

    def check_task_limits(self, task):
        """
        Check that network/try counters are OK.

        Returns:
        * if success: (True, None)
        * if error: (False, reason)

        """

        if task.task_try_count > self.task_try_limit:
            logger.debug('Task tries (%d) ended: %s / %s' % (
                          self.task_try_limit, task.name, task.url))
            return False, 'task-count'

        if task.network_try_count > self.network_try_limit:
            logger.debug('Network tries (%d) ended: %s / %s' % (
                          self.network_try_limit, task.name, task.url))
            return False, 'network-count'

        return True, None

    def generate_task_priority(self):
        if self.priority_mode == 'const':
            return DEFAULT_TASK_PRIORITY
        else:
            return randint(*RANDOM_TASK_PRIORITY_RANGE)

    def add_task(self, task):
        """
        Add task to the task queue.

        Abort the task which was restarted too many times.
        """

        if self.taskq is None:
            raise SpiderMisuseError('You should configure task queue before adding tasks. Use `setup_queue` method.')
        if task.priority is None or not task.priority_is_custom:
            task.priority = self.generate_task_priority()
            task.priority_is_custom = False
        else:
            task.priority_is_custom = True

        if not isinstance(task, NullTask):
            if not task.url.startswith(('http://', 'https://', 'ftp://', 'file://')):
                if self.base_url is None:
                    #raise SpiderMisuseError('Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url))
                    msg = 'Could not resolve relative URL because base_url is not specified. Task: %s, URL: %s' % (task.name, task.url)
                    logger.error(msg)
                    self.add_item('task-with-invalid-url', task.url)
                    return False
                else:
                    task.url = urljoin(self.base_url, task.url)
                    # If task has grab_config object then update it too
                    if task.grab_config:
                        task.grab_config['url'] = task.url

        if self.config.get('GRAB_TASK_REFRESH_CACHE', {}).get(task.name, False):
            task.refresh_cache = True

        if not self.config.get('TASK_ENABLED', {}).get(task.name, True):
            logger.debug('Task %s disabled via config' % task.name)
            self.inc_count('task-disabled-via-config')
            is_valid = False
        else:
            # TODO: keep original task priority if it was set explicitly
            self.taskq.put(task, task.priority, schedule_time=task.schedule_time)
            is_valid = True
        return is_valid

    def load_initial_urls(self):
        """
        Create initial tasks from `self.initial_urls`.

        Tasks are created with name "initial".
        """

        if self.initial_urls:
            for url in self.initial_urls:
                self.add_task(Task('initial', url=url))

    def setup_default_queue(self):
        """
        If task queue is not configured explicitly
        then create task queue with default parameters

        This method is not the same as `self.setup_queue` because
        `self.setup_queue` works by default with in-memory queue.
        You can override `setup_default_queue` in your custom
        Spider and use other storage engines for you
        default task queue.
        """

        # If queue is still not configured
        # then configure it with default backend
        if self.taskq is None:
            self.setup_queue()
        
    def process_task_generator(self):
        """
        Load new tasks from `self.task_generator_object`
        Create new tasks.

        If task queue size is less than some value
        then load new tasks from tasks file.
        """

        if self.task_generator_enabled:
            if hasattr(self.taskq, 'qsize'):
                qsize = self.taskq.qsize()
            else:
                qsize = self.taskq.size()
            if self.max_task_generator_chunk is not None:
                min_limit = min(self.max_task_generator_chunk,
                                self.thread_number * 10)
            else:
                min_limit = self.thread_number * 10
            if qsize < min_limit:
                logger_verbose.debug('Task queue contains less tasks than limit. Tryring to add new tasks')
                try:
                    for x in xrange(min_limit - qsize):
                        item = next(self.task_generator_object)
                        logger_verbose.debug('Got new item from generator. Processing it.')
                        #self.add_task(item)
                        self.process_handler_result(item)
                except StopIteration:
                    # If generator have no values to yield
                    # then disable it
                    logger_verbose.debug('Task generator has no more tasks. Disabling it')
                    self.task_generator_enabled = False

    def init_task_generator(self):
        """
        Process `initial_urls` and `task_generator`.
        Generate first portion of tasks.

        TODO: task generator should work in separate OS process
        """

        self.task_generator_object = self.task_generator()
        self.task_generator_enabled = True

        logger_verbose.debug('Processing initial urls')
        self.load_initial_urls()

        # Initial call to task generator
        # before main cycle
        self.process_task_generator()

    def load_new_task(self):
        start = time.time()
        while True:
            try:
                with self.save_timer('task_queue'):
                    return self.taskq.get()
            except queue.Empty:
                if self.taskq.size():
                    logger_verbose.debug('Waiting for scheduled task')
                    return True
                if not self.slave:
                    logger_verbose.debug('Task queue is empty.')
                    return None
                else:
                    # Temporarly hack which force slave crawler
                    # to wait 5 seconds for new tasks, this solves
                    # the problem that sometimes slave crawler stop
                    # its work because it could not receive new
                    # tasks immediatelly
                    if not self.transport.active_task_number():
                        if time.time() - start < 5:
                            time.sleep(0.1)
                            logger.debug('Slave sleeping')
                        else:
                            break
                    else:
                        break

        logger_verbose.debug('Task queue is empty.')
        return None

    def process_task_counters(self, task):
        task.network_try_count += 1
        if task.task_try_count == 0:
            task.task_try_count = 1

    def create_grab_instance(self):
        return Grab(**self.grab_config)

    def setup_grab_for_task(self, task):
        grab = self.create_grab_instance()
        if task.grab_config:
            grab.load_config(task.grab_config)
        else:
            grab.setup(url=task.url)

        # Generate new common headers
        grab.config['common_headers'] = grab.common_headers()
        return grab

    def is_task_cacheable(self, task, grab):
        if (# cache is disabled for all tasks
            not self.cache_enabled
            # cache data should be refreshed
            or task.get('refresh_cache', False)
            # cache could not be used
            or task.get('disable_cache', False)
            # request type is not cacheable
            or grab.detect_request_method() != 'GET'):
            return False
        else:
            return True

    def load_task_from_cache(self, transport, task, grab, grab_config_backup):
        cache_item = self.cache.get_item(grab.config['url'],
                                         timeout=task.cache_timeout)
        if cache_item is None:
            return None
        else:
            with self.save_timer('cache.read.prepare_request'):
                grab.prepare_request()
            with self.save_timer('cache.read.load_response'):
                self.cache.load_response(grab, cache_item)

            grab.log_request('CACHED')
            self.inc_count('request')
            self.inc_count('request-cache')

            return {'ok': True, 'grab': grab,
                   'grab_config_backup': grab_config_backup,
                   'task': task, 'emsg': None}

    def valid_response_code(self, code, task):
        """
        Answer the question: if the response could be handled via
        usual task handler or the task faield and should be processed as error.
        """

        return (code < 400 or code == 404 or
                code in task.valid_status)

    def process_handler_error(self, func_name, ex, task, error_tb=None):
        self.inc_count('error-%s' % ex.__class__.__name__.lower())

        if error_tb is not None:
            logger.error('Error in %s function' % func_name)
            logger.error(error_tb)
        else:
            logger.error('Error in %s function' % func_name,
                          exc_info=ex)

        # Looks strange but I really have some problems with
        # serializing exception into string
        try:
            ex_str = unicode(ex)
        except TypeError:
            try:
                ex_str = unicode(ex, 'utf-8', 'ignore')
            except TypeError:
                ex_str = str(ex)

        task_url = task.url if task is not None else None
        self.add_item('fatal', '%s|%s|%s|%s' % (
            func_name, ex.__class__.__name__, ex_str, task_url))
        if isinstance(ex, FatalError):
            raise

    def find_data_handler(self, data):
        try:
            return getattr(data, 'handler')
        except AttributeError:
            try:
                handler = getattr(self, 'data_%s' % data.handler_key)
            except AttributeError:
                raise NoDataHandler('No handler defined for Data %s' % data.handler_key)
            else:
                return handler

    def execute_task_handler(self, res, handler):
        """
        Apply `handler` function to the network result.

        If network result is failed then submit task again
        to the network task queue.
        """

        try:
            handler_name = handler.__name__
        except AttributeError:
            handler_name = 'NONE'

        if (res['task'].get('raw') or (
            res['ok'] and self.valid_response_code(res['grab'].response.code, res['task']))):
            try:
                with self.save_timer('response_handler'):
                    with self.save_timer('response_handler.%s' % handler_name):
                        result = handler(res['grab'], res['task'])
                        if result is None:
                            pass
                        else:
                            for item in result:
                                self.process_handler_result(item, res['task'])
            except NoDataHandler as ex:
                raise
            except Exception as ex:
                self.process_handler_error(handler_name, ex, res['task'])
            else:
                self.inc_count('task-%s-ok' % res['task'].name)
        else:
            # Log the error
            if res['ok']:
                msg = res['emsg'] = 'HTTP %s' % res['grab'].response.code
            else:
                msg = res['emsg']

                # TODO: REMOVE
                #if 'Operation timed out after' in msg:
                    #num =  int(msg.split('Operation timed out after')[1].strip().split(' ')[0])
                    #if num > 20000:
                        #import pdb; pdb.set_trace()

            self.inc_count('network-error-%s' % res['emsg'][:20])
            logger.error(u'Network error: %s' % msg)

            # Try to repeat the same network query
            if self.network_try_limit > 0:
                task = res['task']
                task.refresh_cache = True
                # Should use task.grab_config or backup of grab_config
                task.setup_grab_config(res['grab_config_backup'])
                self.add_task(task)
            # TODO: allow to write error handlers
    
    def find_task_handler(self, task):
        callback = task.get('callback')
        if callback:
            return callback
        else:
            try:
                handler = getattr(self, 'task_%s' % task.name)
            except AttributeError:
                raise NoTaskHandler('No handler or callback defined for task %s' % task.name)
            else:
                return handler

    def process_network_result(self, res, from_cache=False):
        """
        Handle result received from network transport of
        from the cache layer.

        Find handler function for that task and call it.
        """

        # Increase stat counters
        self.inc_count('request-processed')
        self.inc_count('task')
        self.inc_count('task-%s' % res['task'].name)
        if (res['task'].network_try_count == 1 and
            res['task'].task_try_count == 1):
            self.inc_count('task-%s-initial' % res['task'].name)

        # Update traffic statistics
        if res['grab'] and res['grab'].response:
            self.timers['network-name-lookup'] += res['grab'].response.name_lookup_time
            self.timers['network-connect'] += res['grab'].response.connect_time
            self.timers['network-total'] += res['grab'].response.total_time
            if not from_cache:
                self.inc_count('download-size', res['grab'].response.download_size)
                self.inc_count('upload-size', res['grab'].response.upload_size)
            self.inc_count('download-size-with-cache', res['grab'].response.download_size)
            self.inc_count('upload-size-with-cache', res['grab'].response.upload_size)
        #self.inc_count('traffic-in

        # NG
        # FIX: Understand how it should work in NG spider
        # TOFIX: start
        stop = False
        for mid in self.middleware_points['response']:
            try:
                mid_response = mid.process_response(self, res)
            except StopTaskProcessing:
                logger.debug('Got StopTaskProcessing exception')
                stop = True
                break
            else:
                if isinstance(mid_response, Task):
                    logger.debug('Got task from middleware')
                    self.add_task(mid_response)
                    stop = True
                    break
                elif mid_response is None:
                    pass
                else:
                    raise Exception('Unknown response from middleware %s' % mid)
        # TOFIX: end

        if stop:
            return

        if self.ng:
            logger_verbose.debug('Submitting result for task %s to response queue' % res['task'])
            self.network_response_queue.put(res)
        else:
            handler = self.find_task_handler(res['task'])
            self.execute_task_handler(res, handler)

    def change_proxy(self, task, grab):
        """
        Assign new proxy from proxylist to the task.
        """

        if task.use_proxylist and self.proxylist_enabled:
            if self.proxy_auto_change:
                self.proxy = self.proxylist.get_random()
            if self.proxy:
                proxy, proxy_userpwd, proxy_type = self.proxy
                grab.setup(proxy=proxy, proxy_userpwd=proxy_userpwd,
                           proxy_type=proxy_type)

    def process_new_task(self, task):
        """
        Handle new task.

        1) Setup Grab object for that task
        2) Try to load task from the cache
        3) If no cached data then submit task to network transport
        """

        grab = self.setup_grab_for_task(task)
        grab_config_backup = grab.dump_config()

        cache_result = None
        if self.is_task_cacheable(task, grab):
            with self.save_timer('cache'):
                with self.save_timer('cache.read'):
                    cache_result = self.load_task_from_cache(
                        self.transport, task, grab, grab_config_backup)

        if cache_result:
            logger_verbose.debug('Task data is loaded from the cache. Yielding task result.')
            self.process_network_result(cache_result, from_cache=True)
            self.inc_count('task-%s-cache' % task.name)
        else:
            if self.only_cache:
                logger.debug('Skipping network request to %s' % grab.config['url'])
            else:
                self.inc_count('request-network')
                self.inc_count('task-%s-network' % task.name)
                self.change_proxy(task, grab)
                with self.save_timer('network_transport'):
                    logger_verbose.debug('Submitting task to the transport layer')
                    try:
                        self.transport.process_task(task, grab, grab_config_backup)
                    except GrabInvalidUrl as ex:
                        logger.debug('Task %s has invalid URL: %s' % (
                            task.name, task.url))
                        self.add_item('invalid-url', task.url)
                    else:
                        logger_verbose.debug('Asking transport layer to do something')

    def is_valid_for_cache(self, res):
        """
        Check if network transport result could
        be saved to cache layer.

        res: {ok, grab, grab_config_backup, task, emsg}
        """


        if res['ok']:
            if self.cache_enabled:
                if res['grab'].request_method == 'GET':
                    if not res['task'].get('disable_cache'):
                        if self.valid_response_code(res['grab'].response.code, res['task']):
                            return True
        return False

    def stop(self):
        """
        This method set internal flag which signal spider
        to stop processing new task and shuts down.
        """

        logger_verbose.debug('Method `stop` was called')
        self.work_allowed = False

    def run(self):
        """
        Main method. All work is done here.
        """

        self.start_timer('total')

        self.transport = MulticurlTransport(self.thread_number)

        try:
            self.setup_default_queue()
            self.prepare()

            self.start_timer('task_generator')
            if not self.slave:
                if not self.ng:
                    self.init_task_generator()
            self.stop_timer('task_generator')


            while self.work_allowed:

                now = int(time.time())
                if now - self.last_snapshot_values['timestamp'] > self.snapshot_interval:
                    snapshot = {'timestamp': now}
                    for key in ('download-size', 'upload-size',
                                'download-size-with-cache'):
                        snapshot[key] = self.counters[key] - self.last_snapshot_values[key]
                        self.last_snapshot_values[key] = self.counters[key]

                    snapshot['request-count'] = self.counters['request'] -\
                        self.last_snapshot_values['request-count']
                    self.last_snapshot_values['request-count'] = self.counters['request']
                    self.last_snapshot_values['timestamp'] = now

                    self.snapshots[now] = snapshot
                    self.snapshot_timestamps.append(now)

                    if self.snapshot_file:
                        with open(self.snapshot_file, 'a') as out:
                            out.write(json.dumps(snapshot) + '\n')

                # FIXIT: REMOVE
                # Run update task handler which
                # updates database object which stores
                # info about current scraping process
                if self.dump_spider_stats:
                    self.dump_spider_stats(self)

                if self.controller.enabled:
                    self.controller.process_commands()

                if not self.ng:
                    # NG
                    self.start_timer('task_generator')
                    # star
                    if self.task_generator_enabled:
                        self.process_task_generator()
                    self.stop_timer('task_generator')

                if self.transport.ready_for_task():
                    logger_verbose.debug('Transport has free resources. '\
                                         'Trying to add new task (if exists)')

                    # Try five times to get new task and proces task generator
                    # because slave parser could agressively consume
                    # tasks from task queue
                    for x in xrange(5):
                        task = self.load_new_task()
                        if task is None:
                            if not self.transport.active_task_number():
                                self.process_task_generator()
                        elif task is True:
                            # If only delayed tasks in queue
                            break
                        else:
                            # If got some task
                            break

                    if not task:
                        if not self.transport.active_task_number():
                            logger_verbose.debug('Network transport has no active tasks')
                            # NG
                            if self.ng:
                                self.waiting_shutdown_event.set()
                                if self.shutdown_event.is_set():
                                    logger_verbose.debug('Got shutdown signal')
                                    self.stop()
                                else:
                                    logger_verbose.debug('Shutdown event has not been set yet')
                            else:
                                if not self.task_generator_enabled:
                                    self.stop()
                        else:
                            logger_verbose.debug('Transport active tasks: %d' %
                                                 self.transport.active_task_number())
                    elif isinstance(task, NullTask):
                        logger_verbose.debug('Got NullTask')
                        if not self.transport.active_task_number():
                            if task.sleep:
                                logger.debug('Got NullTask with sleep instruction. Sleeping for %.2f seconds' % task.sleep)
                                time.sleep(task.sleep)
                    elif isinstance(task, bool) and (task == True):
                        pass
                    else:
                        if self.ng:
                            if self.waiting_shutdown_event.is_set():
                                self.waiting_shutdown_event.clear()
                        logger_verbose.debug('Got new task from task queue: %s' % task)
                        self.process_task_counters(task)

                        is_valid, reason = self.check_task_limits(task)
                        if not is_valid:
                            logger_verbose.debug('Task %s is rejected due to %s limit' % (task.name, reason))
                            if reason == 'task-count':
                                self.add_item('task-count-rejected', task.url)
                            elif reason == 'network-count':
                                self.add_item('network-count-rejected', task.url)
                            else:
                                raise Exception('Unknown response from check_task_limits: %s' % reason)
                            handler = task.get_fallback_handler(self)
                            if handler:
                                handler(task)
                            # TODO: not do following line
                            # TODO: middleware: TaskFails
                        else:
                            self.process_new_task(task)
                            self.transport.process_handlers()

                with self.save_timer('network_transport'):
                    logger_verbose.debug('Asking transport layer to do something')
                    # Process active handlers
                    #print '[select]'
                    #self.transport.select(0.01)
                    #print '[done]'

                    #print '[process handlers #2]'
                    self.transport.process_handlers()
                    #print '[done]'

                logger_verbose.debug('Processing network results (if any).')
                # Iterate over network trasport ready results
                # Each result could be valid or failed
                # Result format: {ok, grab, grab_config_backup, task, emsg}
                
                #print '[transport iterate results - start]'
                for result in self.transport.iterate_results():
                    if self.is_valid_for_cache(result):
                        with self.save_timer('cache'):
                            with self.save_timer('cache.write'):
                                self.cache.save_response(result['task'].url, result['grab'])

                    #print '[process network results]'
                    self.process_network_result(result)
                    #print '[done]'
                    self.inc_count('request')

                #print '[transport iterate results - end]'

            logger_verbose.debug('Work done')
        except KeyboardInterrupt:
            print('\nGot ^C signal. Stopping.')
            raise
        finally:
            # This code is executed when main cycles is breaked
            self.stop_timer('total')
            self.shutdown()

    def load_proxylist(self, source, source_type, proxy_type='http',
                       auto_init=True, auto_change=True,
                       **kwargs):
        self.proxylist = ProxyList(source, source_type, proxy_type=proxy_type, **kwargs)

        self.proxylist_enabled = True
        self.proxy = None
        if not auto_change and auto_init:
            self.proxy = self.proxylist.get_random()
        self.proxy_auto_change = auto_change

    def get_name(self):
        if hasattr(self, 'spider_name'):
            return self.spider_name
        else:
            return camel_case_to_underscore(self.__class__.__name__)

    # ****************
    # Abstract methods
    # ****************

    def shutdown(self):
        """
        You can override this method to do some final actions
        after parsing has been done.
        """

        logger.debug('Job done!')

    def task_generator(self):
        """
        You can override this method to load new tasks smoothly.

        It will be used each time as number of tasks
        in task queue is less then number of threads multiplied on 2
        This allows you to not overload all free memory if total number of
        tasks is big.
        """

        if False:
            # Some magic to make this function empty generator
            yield ':-)'
        return

    def process_handler_result(self, result, task=None):
        """
        Process result received from the task handler.

        Result could be:
        * None
        * Task instance
        * Data instance.
        """

        if isinstance(result, Task):
            self.add_task(result)
        elif isinstance(result, Data):
            handler = self.find_data_handler(result)
            try:
                data_result = handler(**result.storage)
                if data_result is None:
                    pass
                else:
                    for something in data_result:
                        self.process_handler_result(something, task)

            except Exception as ex:
                self.process_handler_error('data_%s' % result.handler_key, ex, task)
        elif result is None:
            pass
        else:
            raise SpiderError('Unknown result type: %s' % result)
        
    @classmethod
    def get_spider_name(cls):
        if hasattr(cls, 'spider_name'):
            return cls.spider_name
        else:
            return camel_case_to_underscore(cls.__name__)

    @classmethod
    def update_spider_config(cls, config):
        pass

    # ***********
    # NG Features
    # ***********

    def run_generator(self):
        """
        Generate tasks and put them into Task Queue.

        This is main method for Generator Process
        """

        self.init_task_generator()

        while True:
            if not self.task_generator_enabled:
                self.generator_done_event.set()
            if self.shutdown_event.is_set():
                logger.info('Got shutdown event')
                break
            time.sleep(1)
            self.process_task_generator()

    def run_parser(self):
        """
        Process items received from Network Response Queue.

        Network Response Queue are filled by Downloader Process.

        This is main method for Parser Process.
        """
        should_work = True
        while should_work:
            try:
                response = self.network_response_queue.get(True, 0.1)
            except queue.Empty:
                logger_verbose.debug('Response queue is empty.')
                response = None

            if not response:
                self.waiting_shutdown_event.set()
                if self.shutdown_event.is_set():
                    logger_verbose.debug('Got shutdown signal')
                    should_work = False
                else:
                    logger_verbose.debug('Shutdown event has not been set yet')
            else:
                if self.waiting_shutdown_event.is_set():
                    self.waiting_shutdown_event.clear()
                logger_verbose.debug('Got new response from response '\
                                     'queue: %s' % response['task'].url)

                handler = self.find_task_handler(response['task'])
                self.execute_task_handler(response, handler)

        logger_verbose.debug('Work done')

    # TODO:
    # Develop Manager Process which contains logic of accepting or rejecting
    # task objects recivied from Parser Processes
    # Maybe Manager Process also should controls the Data flow
    # TODO2:
    # Data handler process
    #def run_manager(self):
        #try:
            #self.start_time = time.time()
            #self.prepare()
            #res_count = 0

            #while True:
                #try:
                    #res = self.result_queue.get(block=True, timeout=2)
                #except Queue.Empty:
                    ##pass
                    #res = None

                #if res is None:
                    #logging.error('res is None: stopping')
                    #break

                #if self.should_stop:
                    #break

                #if self.task_generator_enabled:
                    #self.process_task_generator()

                #for task, original_task in res['task_list']:
                    #logging.debug('Processing task items from result queue')
                    #self.process_handler_result(task)

                ##for data, original_task in res['data_list']:
                    ##logging.debug('Processing data items from result queue')
                    ##self.process_handler_result(data)

        #except KeyboardInterrupt:
            #print '\nGot ^C signal. Stopping.'
            #print self.render_stats()
            #raise
        #finally:
            ## This code is executed when main cycles is breaked
            #self.shutdown()
                
    def command_get_stats(self, command):
        return {'data': self.render_stats()}

    @classmethod
    def get_available_command_names(cls):
        spider = cls()
        clist = []
        for key in dir(spider):
            if key.startswith('command_'):
                clist.append(key.split('command_', 1)[1])
        return sorted(clist)

########NEW FILE########
__FILENAME__ = mongo
"""
CacheItem interface:
'_id': string,
'url': string,
'response_url': string,
'body': string,
'head': string,
'response_code': int,
'cookies': None,#grab.response.cookies,

TODO: WTF with cookies???
"""
from hashlib import sha1
import zlib
import logging
import pymongo
try:
    from bson import Binary
except ImportError:
    from pymongo.binary import Binary
import time

from grab.response import Response
from grab.cookie import CookieManager
from grab.util.py3k_support import *

logger = logging.getLogger('grab.spider.cache_backend.mongo')


class CacheBackend(object):
    def __init__(self, database, use_compression=True, spider=None):
        self.spider = spider
        self.db = pymongo.Connection()[database]
        self.use_compression = use_compression

    def get_item(self, url, timeout=None):
        """
        Returned item should have specific interface. See module docstring.
        """

        _hash = self.build_hash(url)
        if timeout is not None:
            ts = int(time.time()) - timeout
            query = {'_id': _hash, 'timestamp': {'$gt': ts}}
        else:
            query = {'_id': _hash}
        return self.db.cache.find_one(query)

    def build_hash(self, url):
        if isinstance(url, unicode):
            utf_url = url.encode('utf-8')
        else:
            utf_url = url
        return sha1(utf_url).hexdigest()

    def remove_cache_item(self, url):
        _hash = self.build_hash(url)
        self.db.cache.remove({'_id': _hash})

    def load_response(self, grab, cache_item):
        grab.fake_response(cache_item['body'])

        body = cache_item['body']
        if self.use_compression:
            body = zlib.decompress(body)

        def custom_prepare_response_func(transport, g):
            response = Response()
            response.head = cache_item['head'].decode('utf-8')
            response.body = body
            response.code = cache_item['response_code']
            response.download_size = len(body)
            response.upload_size = 0
            response.download_speed = 0

            # Hack for deprecated behaviour
            if 'response_url' in cache_item:
                response.url = cache_item['response_url']
            else:
                logger.debug('You cache contains items without `response_url` key. It is depricated data format. Please re-download you cache or build manually `response_url` keys.')
                response.url = cache_item['url']

            response.parse()
            response.cookies = CookieManager(transport.extract_cookiejar())

            return response

        grab.process_request_result(custom_prepare_response_func)

    def save_response(self, url, grab):
        body = grab.response.body
        if self.use_compression:
            body = zlib.compress(body)

        _hash = self.build_hash(url)
        item = {
            '_id': _hash,
            'timestamp': int(time.time()),
            'url': url,
            'response_url': grab.response.url,
            'body': Binary(body),
            'head': Binary(grab.response.head.encode('utf-8')),
            'response_code': grab.response.code,
            'cookies': None,
        }
        try:
            self.db.cache.save(item, safe=True)
        except Exception as ex:
            if 'document too large' in unicode(ex):
                logging.error('Document too large. It was not saved into mongo '\
                              'cache. Url: %s' % url)
            else:
                raise

    def clear(self):
        self.db.cache.remove()

########NEW FILE########
__FILENAME__ = mysql
"""
CacheItem interface:
'_id': string,
'url': string,
'response_url': string,
'body': string,
'head': string,
'response_code': int,
'cookies': None,#grab.response.cookies,

TODO: WTF with cookies???
"""
from hashlib import sha1
import zlib
import logging
import MySQLdb
import marshal
import time

from grab.response import Response
from grab.cookie import CookieManager
from grab.util.py3k_support import *

logger = logging.getLogger('grab.spider.cache_backend.mysql')

# py3 hack
if PY3K:
    import re
    from functools import reduce

    RE_HEXS = re.compile('0x[a-fA-F0-9]{2}')

    def _str_to_hexbytes(val):
        val = val.replace('\\x', '0x')
        # Finds all hexadecimals
        xs = re.findall(RE_HEXS, val)
        xc = [chr(int(s, 16)) for s in xs]
        # Plus escape sequences
        xs += ['\\\\', "\\'", '\\"', '\\a', '\\b', '\\f', '\\n', '\\r', '\\t', '\\v', '_\\\\_']
        xc += ['_\\\\_', "\'", '\"', '\a', '\b', '\f', '\n', '\r', '\t', '\v', '\\']
        # Replaces all
        val = reduce(lambda acc, args: acc.replace(*args), zip(xs, xc), val)
        # Converts to bytes
        return val.encode('raw_unicode_escape')

    def _hexbytes_to_str(val):
        return str(val)[2:-1]


class CacheBackend(object):
    def __init__(self, database, use_compression=True,
                 mysql_engine='innodb', spider=None, **kwargs):
        self.spider = spider
        self.conn = MySQLdb.connect(**kwargs)
        self.mysql_engine = mysql_engine
        self.conn.select_db(database)
        self.cursor = self.conn.cursor()
        self.cursor.execute('SET TRANSACTION ISOLATION LEVEL READ COMMITTED')
        res = self.cursor.execute('show tables')
        found = False
        for row in self.cursor:
            if row[0] == 'cache':
                found = True
                break
        if not found:
            self.create_cache_table(self.mysql_engine)

    def create_cache_table(self, engine):
        self.cursor.execute('begin')
        self.cursor.execute('''
            create table cache (
                id binary(20) not null,
                timestamp int not null,
                data mediumblob not null,
                primary key (id),
                index timestamp_idx(timestamp)
            ) engine = %s
        ''' % engine)
        self.cursor.execute('commit')

    def get_item(self, url, timeout=None):
        """
        Returned item should have specific interface. See module docstring.
        """

        _hash = self.build_hash(url)
        with self.spider.save_timer('cache.read.mysql_query'):
            self.cursor.execute('BEGIN')
            if timeout is None:
                query = ""
            else:
                ts = int(time.time()) - timeout
                query = " AND timestamp > %d" % ts
            # py3 hack
            if PY3K:
                sql = '''
                      SELECT data
                      FROM cache
                      WHERE id = x{0} %(query)s
                      ''' % {'query': query}
            else:
                sql = '''
                      SELECT data
                      FROM cache
                      WHERE id = x%%s %(query)s
                      ''' % {'query': query}
            res = self.cursor.execute(sql, (_hash,))
            row = self.cursor.fetchone()
            self.cursor.execute('COMMIT')
        if row:
            data = row[0]
            # py3 hack
            if PY3K:
                # A temporary solution for MySQLdb (Py3k port)
                # [https://github.com/davispuh/MySQL-for-Python-3]
                data = _str_to_hexbytes(data)
            return self.unpack_database_value(data)
        else:
            return None

    def unpack_database_value(self, val):
        with self.spider.save_timer('cache.read.unpack_data'):
            dump = zlib.decompress(val)
            return marshal.loads(dump)

    def build_hash(self, url):
        with self.spider.save_timer('cache.read.build_hash'):
            if isinstance(url, unicode):
                utf_url = url.encode('utf-8')
            else:
                utf_url = url
            return sha1(utf_url).hexdigest()

    def remove_cache_item(self, url):
        _hash = self.build_hash(url)
        self.cursor.execute('begin')
        self.cursor.execute('''
            delete from cache where id = x%s
        ''', (_hash,))
        self.cursor.execute('commit')

    def load_response(self, grab, cache_item):
        grab.fake_response(cache_item['body'])

        body = cache_item['body']

        def custom_prepare_response_func(transport, g):
            response = Response()
            response.head = cache_item['head']
            response.body = body
            response.code = cache_item['response_code']
            response.download_size = len(body)
            response.upload_size = 0
            response.download_speed = 0

            # Hack for deprecated behaviour
            if 'response_url' in cache_item:
                response.url = cache_item['response_url']
            else:
                logger.debug('You cache contains items without `response_url` key. It is depricated data format. Please re-download you cache or build manually `response_url` keys.')
                response.url = cache_item['url']

            response.parse()
            response.cookies = CookieManager(transport.extract_cookiejar())
            return response

        grab.process_request_result(custom_prepare_response_func)

    def save_response(self, url, grab):
        body = grab.response.body

        item = {
            'url': url,
            'response_url': grab.response.url,
            'body': body,
            'head': grab.response.head,
            'response_code': grab.response.code,
            'cookies': None,
        }
        self.set_item(url, item)

    def set_item(self, url, item):
        _hash = self.build_hash(url)
        data = self.pack_database_value(item)
        # py3 hack
        if PY3K:
            # A temporary solution for MySQLdb (Py3k port)
            # [https://github.com/davispuh/MySQL-for-Python-3]
            data = _hexbytes_to_str(data)
        self.cursor.execute('BEGIN')
        ts = int(time.time())
        # py3 hack
        if PY3K:
            sql = '''
                  INSERT INTO cache (id, timestamp, data)
                  VALUES(x{0}, {1}, {2})
                  ON DUPLICATE KEY UPDATE timestamp = {3}, data = {4}
                  '''
        else:
            sql = '''
                  INSERT INTO cache (id, timestamp, data)
                  VALUES(x%s, %s, %s)
                  ON DUPLICATE KEY UPDATE timestamp = %s, data = %s
                  '''
        res = self.cursor.execute(sql, (_hash, ts, data, ts, data))
        self.cursor.execute('COMMIT')

    def pack_database_value(self, val):
        dump = marshal.dumps(val)
        return zlib.compress(dump)

    def clear(self):
        self.cursor.execute('BEGIN')
        self.cursor.execute('TRUNCATE cache')
        self.cursor.execute('COMMIT')

    def has_item(self, url, timeout=None):
        """
        Test if required item exists in the cache.
        """

        _hash = self.build_hash(url)
        with self.spider.save_timer('cache.read.mysql_query'):
            if timeout is None:
                query = ""
            else:
                ts = int(time.time()) - timeout
                query = " AND timestamp > %d" % ts
            res = self.cursor.execute('''
                SELECT id
                FROM cache
                WHERE id = x%%s %(query)s
                LIMIT 1
                ''' % {'query': query},
                (_hash,))
            row = self.cursor.fetchone()
        return True if row else False

########NEW FILE########
__FILENAME__ = postgresql
"""
CacheItem interface:
'_id': string,
'url': string,
'response_url': string,
'body': string,
'head': string,
'response_code': int,
'cookies': None,#grab.response.cookies,
"""
from hashlib import sha1
import zlib
import logging
import marshal
import time

from grab.response import Response
from grab.cookie import CookieManager
from grab.util.py3k_support import *


logger = logging.getLogger('grab.spider.cache_backend.postgresql')

class CacheBackend(object):
    def __init__(self, database, use_compression=True, spider=None, **kwargs):
        import psycopg2
        from psycopg2.extensions import ISOLATION_LEVEL_READ_COMMITTED

        self.spider = spider
        self.conn = psycopg2.connect(dbname=database, **kwargs)
        self.conn.set_isolation_level(ISOLATION_LEVEL_READ_COMMITTED)
        self.cursor = self.conn.cursor()
        res = self.cursor.execute("""
            SELECT
                TABLE_NAME
            FROM
                INFORMATION_SCHEMA.TABLES
            WHERE
                TABLE_TYPE = 'BASE TABLE'
            AND
                table_schema NOT IN ('pg_catalog', 'information_schema')"""
        )
        found = False
        for row in self.cursor:
            if row[0] == 'cache':
                found = True
                break
        if not found:
            self.create_cache_table()

    def create_cache_table(self):
        self.cursor.execute('BEGIN')
        self.cursor.execute('''
            CREATE TABLE cache (
                id BYTEA NOT NULL CONSTRAINT primary_key PRIMARY KEY,
                timestamp INT NOT NULL,
                data BYTEA NOT NULL,
            );
            CREATE INDEX timestamp_idx ON cache (timestamp);
        ''')
        self.cursor.execute('COMMIT')

    def get_item(self, url, timeout=None):
        """
        Returned item should have specific interface. See module docstring.
        """

        _hash = self.build_hash(url)
        with self.spider.save_timer('cache.read.postgresql_query'):
            self.cursor.execute('BEGIN')
            if timeout is None:
                query = ""
            else:
                ts = int(time.time()) - timeout
                query = " AND timestamp > %d" % ts
            # py3 hack
            if PY3K:
                sql = '''
                      SELECT data
                      FROM cache
                      WHERE id = {0} %(query)s
                      ''' % {'query': query}
            else:
                sql = '''
                      SELECT data
                      FROM cache
                      WHERE id = %%s %(query)s
                      ''' % {'query': query}
            res = self.cursor.execute(sql, (_hash,))
            row = self.cursor.fetchone()
            self.cursor.execute('COMMIT')
        if row:
            data = row[0]
            return self.unpack_database_value(data)
        else:
            return None

    def unpack_database_value(self, val):
        with self.spider.save_timer('cache.read.unpack_data'):
            dump = zlib.decompress(str(val))
            return marshal.loads(dump)

    def build_hash(self, url):
        with self.spider.save_timer('cache.read.build_hash'):
            if isinstance(url, unicode):
                utf_url = url.encode('utf-8')
            else:
                utf_url = url
            return sha1(utf_url).hexdigest()

    def remove_cache_item(self, url):
        _hash = self.build_hash(url)
        self.cursor.execute('begin')
        self.cursor.execute('''
            DELETE FROM cache WHERE id = x%s
        ''', (_hash,))
        self.cursor.execute('commit')

    def load_response(self, grab, cache_item):
        grab.fake_response(cache_item['body'])

        body = cache_item['body']

        def custom_prepare_response_func(transport, g):
            response = Response()
            response.head = cache_item['head']
            response.body = body
            response.code = cache_item['response_code']
            response.download_size = len(body)
            response.upload_size = 0
            response.download_speed = 0

            # Hack for deprecated behaviour
            if 'response_url' in cache_item:
                response.url = cache_item['response_url']
            else:
                logger.debug('You cache contains items without `response_url` key. It is depricated data format. Please re-download you cache or build manually `response_url` keys.')
                response.url = cache_item['url']

            response.parse()
            response.cookies = CookieManager(transport.extract_cookiejar())
            return response

        grab.process_request_result(custom_prepare_response_func)

    def save_response(self, url, grab):
        body = grab.response.body

        item = {
            'url': url,
            'response_url': grab.response.url,
            'body': body,
            'head': grab.response.head,
            'response_code': grab.response.code,
            'cookies': None,
        }
        self.set_item(url, item)

    def set_item(self, url, item):
        import psycopg2

        _hash = self.build_hash(url)
        data = self.pack_database_value(item)
        self.cursor.execute('BEGIN')
        ts = int(time.time())
        # py3 hack
        if PY3K:
            sql = '''
                  UPDATE cache SET timestamp = {0}, data = {1} WHERE id = {2};
                  INSERT INTO cache (id, timestamp, data)
                  SELECT {2}, {0}, {1} WHERE NOT EXISTS (SELECT 1 FROM cache WHERE id = {2});
                  '''
        else:
            sql = '''
                  UPDATE cache SET timestamp = %s, data = %s WHERE id = %s;
                  INSERT INTO cache (id, timestamp, data)
                  SELECT %s, %s, %s WHERE NOT EXISTS (SELECT 1 FROM cache WHERE id = %s);
                  '''
        res = self.cursor.execute(sql, (ts, psycopg2.Binary(data), _hash, _hash, ts, psycopg2.Binary(data), _hash))
        self.cursor.execute('COMMIT')

    def pack_database_value(self, val):
        dump = marshal.dumps(val)
        return  zlib.compress(dump)

    def clear(self):
        self.cursor.execute('BEGIN')
        self.cursor.execute('TRUNCATE cache')
        self.cursor.execute('COMMIT')

    def has_item(self, url, timeout=None):
        """
        Test if required item exists in the cache.
        """

        _hash = self.build_hash(url)
        with self.spider.save_timer('cache.read.postgresql_query'):
            if timeout is None:
                query = ""
            else:
                ts = int(time.time()) - timeout
                query = " AND timestamp > %d" % ts
            res = self.cursor.execute('''
                SELECT id
                FROM cache
                WHERE id = %%s %(query)s
                LIMIT 1
                ''' % {'query': query},
                (_hash,))
            row = self.cursor.fetchone()
        return True if row else False

########NEW FILE########
__FILENAME__ = tokyo_cabinet
"""
CacheItem interface:
'url': string,
'response_url': string,
'body': string,
'head': string,
'response_code': int,
'cookies': None,#grab.response.cookies,

TODO: Cookie support???
"""
import tc
import os
import logging
import marshal

from grab.response import Response
from grab.util.py3k_support import *

logger = logging.getLogger('grab.spider.cache_backend.mongo')

def tc_open(path, mode='a+', compress=True, makedirs=True):
    if makedirs:
        try:
            os.makedirs(os.path.dirname(path))
        except OSError:
            pass

    db = tc.HDB()
    if compress:
        db.tune(-1, -1, -1, tc.HDBTDEFLATE)
    db.open(path,
        {
            'r': tc.HDBOREADER,
            'w': tc.HDBOWRITER | tc.HDBOCREAT | tc.HDBOTRUNC,
            'a': tc.HDBOWRITER,
            'a+': tc.HDBOWRITER | tc.HDBOCREAT,
        }[mode]
    )
    return db


class CacheBackend(object):
    def __init__(self, database, use_compression=True, spider=None):
        # database == filename
        self.spider = spider
        self.db = tc_open(database, compress=use_compression)
        self.use_compression = use_compression

    def get_item(self, url, timeout=None):
        """
        Returned item should have specific interface. See module docstring.
        """

        if timeout is None:
            raise NotImplemented('timeout option for tokyo cabinet cache backend is not supported')
        try:
            dump = self.db[self.build_key(url)]
        except KeyError:
            return
        return marshal.loads(dump)

    def build_key(self, url):
        return url.encode('utf-8') if isinstance(url, unicode) else url

    def remove_cache_item(self, url):
        del self.db[self.build_key(url)]

    def load_response(self, grab, cache_item):
        grab.fake_response(cache_item['body'])
        body = cache_item['body']

        def custom_prepare_response_func(transport, g):
            response = Response()
            response.head = cache_item['head']
            response.body = body
            response.code = cache_item['response_code']
            response.download_size = len(body)
            response.upload_size = 0
            response.download_speed = 0

            # Hack for deprecated behaviour
            if 'response_url' in cache_item:
                response.url = cache_item['response_url']
            else:
                logger.debug('You cache contains items without `response_url` key. It is depricated data format. Please re-download you cache or build manually `response_url` keys.')
                response.url = cache_item['url']

            response.parse()
            response.cookies = transport.extract_cookies()
            return response

        grab.process_request_result(custom_prepare_response_func)

    def save_response(self, url, grab):
        body = grab.response.body
        item = {
            'url': url,
            'response_url': grab.response.url,
            'body': body,
            'head': grab.response.head,
            'response_code': grab.response.code,
            'cookies': None,
        }
        self.db[self.build_key(url)] = marshal.dumps(item)

    def clear(self):
        raise NotImplemented

########NEW FILE########
__FILENAME__ = command_controller
import random
try:
    import Queue as queue
except ImportError:
    import queue
import logging
import uuid
import pickle

from grab.spider.error import SpiderMisuseError

class RedisCommandInterface(object):
    def __init__(self, spider_name, **kwargs):
        import redis

        self.redisdb = redis.StrictRedis()
        self.spider_name = spider_name
        self.input_queue_name = 'command_input_%s' % spider_name
        self.output_hash_name = 'command_output_%s' % spider_name
        logging.debug('Command input queue redis key: %s' % self.input_queue_name)
        logging.debug('Command ouput hash redis key: %s' % self.output_hash_name)

    def put_command(self, command):
        command['uid'] = str(uuid.uuid4())
        self.redisdb.rpush(self.input_queue_name, pickle.dumps(command))
        return command['uid']

    def pop_command(self):
        command_dump = self.redisdb.lpop(self.input_queue_name)
        if command_dump is None:
            return None
        else:
            return pickle.loads(command_dump)

    def put_result(self, key, result):
        result_dump = pickle.dumps(result)
        self.redisdb.hset(self.output_hash_name, key, result_dump)

    def pop_result(self, key):
        result_dump = self.redisdb.hget(self.output_hash_name, key)
        if result_dump is None:
            return None
        else:
            return pickle.loads(result_dump)

    #def size(self):
        #return len(self.queue_object)

    def clear(self):
        self.redisdb.delete(self.input_queue_name)
        self.redisdb.delete(self.output_hash_name)


class CommandController(object):
    def __init__(self, spider):
        self.spider = spider
        self.enabled = False
        self.ifaces = {}

    def add_interface(self, backend=None, **kwargs):
        if backend == 'redis':
            iface = RedisCommandInterface(self.spider.get_name(), **kwargs)
            self.ifaces[backend] = iface
            self.enabled = True
            return iface
        else:
            raise SpiderMisuseError('Unknown command interface: %s' % backend)

    def process_commands(self):
        for iface in self.ifaces.values():
            command = iface.pop_command()
            if command is not None:
                iface.put_result(command['uid'], self.process_command(command))

    def process_command(self, command):
        cname = command['name']
        handler = getattr(self.spider, 'command_%s' % cname, None)
        if handler is not None:
            return handler(command)
        else:
            return {'error': 'unknown command'}

########NEW FILE########
__FILENAME__ = base
from grab.const import NULL

class Data(object):
    """
    Task handlers should return instances of that class.
    """

    def __init__(self, handler_key=None, **kwargs):
        self.handler_key = handler_key
        self.storage = kwargs

    def __getitem__(self, key):
        return self.storage[key]

    def get(self, key, default=NULL):
        try:
            return self.storage[key]
        except KeyError:
            if default is NULL:
                raise
            else:
                return default

########NEW FILE########
__FILENAME__ = shortcut
import os
from copy import deepcopy
from urlparse import urlsplit
import imghdr
from StringIO import StringIO

from grab.spider.data.base import Data
from grab.tools.files import hashed_path
from grab.spider.task import Task
from grab import Grab

def build_image_hosting_referer(url):
    from database import db

    host = urlsplit(url).netloc
    return '.'.join(host.split('.')[-2:])


def image_handler(grab, task):
    from database import db

    if grab.response.code == 200:
        if len(grab.response.body):
            if imghdr.what(StringIO(grab.response.body)):
                grab.response.save(task.path)
                db[task.collection].update({'_id': task.obj['_id']},
                                           {'$set': {task.path_field: task.path}})


def image_set_handler(grab, task):
    from database import db

    if grab.response.code == 200:
        if len(grab.response.body):
            if imghdr.what(StringIO(grab.response.body)):
                grab.response.save(task.path)
                db[task.collection].update(
                    {'_id': task.obj['_id'], ('%s.url' % task.set_field): task.image['url']},
                    {'$set': {('%s.$.path' % task.set_field): task.path}}
                )   



class MongoObjectImageData(Data):
    def handler(self, url, collection, obj, path_field, base_dir, task_args=None,
                grab_args=None, callback=None):
        from database import db
        path = hashed_path(url, base_dir=base_dir)
        if os.path.exists(path):
            if path != obj.get(path_field, None):
                db[collection].update({'_id': obj['_id']},
                                      {'$set': {path_field: path}})
        else:
            kwargs = {}
            if task_args:
                kwargs = deepcopy(task_args)

            g = Grab()
            g.setup(url=url)
            if grab_args:
                g.setup(**grab_args)
            g.setup(referer=build_image_hosting_referer(url))

            yield Task(
                callback=callback or image_handler,
                grab=g,
                collection=collection,
                path=path,
                obj=obj,
                path_field=path_field,
                disable_cache=True,
                backup=g.dump_config(),
                **kwargs
            )


class MongoObjectImageSetData(Data):
    def handler(self, collection, obj, set_field, base_dir, task_args=None,
                grab_args=None, callback=None):
        from database import db

        for image in obj.get(set_field, []):
            path = hashed_path(image['url'], base_dir=base_dir)
            if os.path.exists(path):
                if path != image['path']:
                    db[collection].update(
                        {'_id': obj['_id'], ('%s.url' % set_field): image['url']},
                        {'$set': {('%s.$.path' % set_field): path}})
            else:
                kwargs = {}
                if task_args:
                    kwargs = deepcopy(task_args)

                g = Grab()
                g.setup(url=image['url'])
                if grab_args:
                    g.setup(**grab_args)
                g.setup(referer=build_image_hosting_referer(image['url']))

                yield Task(
                    callback=callback or image_set_handler,
                    grab=g,
                    collection=collection,
                    path=path,
                    obj=obj,
                    image=image,
                    set_field=set_field,
                    disable_cache=True,
                    backup=g.dump_config(),
                    **kwargs
                )


########NEW FILE########
__FILENAME__ = data
from grab.const import NULL

class Data(object):
    """
    Task handlers should return instances of that class.
    """

    def __init__(self, handler_key=None, **kwargs):
        self.handler_key = handler_key
        self.storage = kwargs

    def __getitem__(self, key):
        return self.storage[key]

    def get(self, key, default=NULL):
        try:
            return self.storage[key]
        except KeyError:
            if default is NULL:
                raise
            else:
                return default

########NEW FILE########
__FILENAME__ = error
__all__ = ('SpiderError', 'SpiderMisuseError', 'FatalError',
           'StopTaskProcessing', 'SpiderInternalError',
           'NoTaskHandler', 'NoDataHandler')

class SpiderError(Exception):
    "Base class for Spider exceptions"


class SpiderMisuseError(SpiderError):
    "Improper usage of Spider framework"


class FatalError(SpiderError):
    "Fatal error which should stop parsing process"


class StopTaskProcessing(SpiderError):
    """
    Used in middlewares to stop task processing
    """


class SpiderInternalError(SpiderError):
    """
    Used to indicate error in some internal spider services
    like spider class discovering, CLI error
    """


class NoTaskHandler(SpiderError):
    """
    Used then it is not possible to find which
    handler should be used to process network response.
    """


class NoDataHandler(SpiderError):
    """
    Used then it is not possible to find which
    handler should be used to process Data object.
    """

########NEW FILE########
__FILENAME__ = middleware
from grab.spider.error import StopTaskProcessing

class TestMiddleware(object):
    def process_response(self, spider, resp):
        raise StopTaskProcessing
        #return resp['task'].clone()

########NEW FILE########
__FILENAME__ = captcha_solver
import logging

from grab.spider.task import Task
from grab.captcha import SolutionNotReady

logger = logging.getLogger('grab.spider.mixin.captcha_solver')

class CaptchaSolverInterface(object):
    def task_download_captcha(self, grab, task):
        logger.debug('Got captcha image')
        g_new = self.solver.backend.get_submit_captcha_request(grab.response.body)
        yield Task('submit_captcha', grab=g_new, meta=task.meta)

    def task_submit_captcha(self, grab, task):
        captcha_id = self.solver.backend.parse_submit_captcha_response(grab.response)
        g_new = self.solver.backend.get_check_solution_request(captcha_id)
        yield Task('check_solution', grab=g_new, delay=5, meta=task.meta)

    def task_check_solution(self, grab, task):
        try:
            solution = self.solver.backend.parse_check_solution_response(grab.response)
        except SolutionNotReady:
            logger.debug('SOLUTION IS NOT READY')
            yield task.clone(delay=task.original_delay)
        else:
            logger.debug('GOT CAPTCHA SOLUTION: %s' % solution)
            yield task.meta['handler'](solution, task.meta)



########NEW FILE########
__FILENAME__ = pattern
import os.path
import logging

from grab.spider.task import Task
from grab.tools.files import hashed_path

logger = logging.getLogger('grab.spider.pattern')

class SpiderPattern(object):
    """
    This is base class for Spider class which contains
    methods for automating common task in scraping the typical
    web site, for example, iterating over pagination and fetching images
    associated with some scraped object.
    """

    def process_object_image(self, task_name, collection, obj, image_field, image_url,
                             base_dir, ext='jpg', skip_existing=True):
        path = os.path.join(base_dir, hashed_path(image_url, ext=ext))
        if os.path.exists(path) and skip_existing:
            collection.update({'_id': obj['_id']},
                              {'$set': {'%s_path' % image_field: path,
                                        '%s_url' % image_field: image_url}})
        else:
            self.add_task(Task(task_name, url=image_url, obj=obj, disable_cache=True,
                               image_field=image_field,
                               collection=collection, base_dir=base_dir, ext=ext))


    def generic_task_image(self, grab, task):
        relpath = grab.response.save_hash(task.url, task.base_dir, ext=task.ext)
        path = os.path.join(task.base_dir, relpath)
        task.collection.update({'_id': task.obj['_id']},
                              {'$set': {'%s_path' % task.image_field: path,
                                        '%s_url' % task.image_field: task.url}})


    def process_next_page(self, grab, task, xpath, resolve_base=False, **kwargs):
        """
        Generate task for next page.

        :param grab: Grab instance
        :param task: Task object which should be assigned to next page url
        :param xpath: xpath expression which calculates list of URLS
        :param **kwargs: extra settings for new task object

        Example::

            self.follow_links(grab, 'topic', '//div[@class="topic"]/a/@href')
        """
        try:
            #next_url = grab.xpath_text(xpath)
            next_url = grab.doc.select(xpath).text()
        except IndexError:
            return False
        else:
            url = grab.make_url_absolute(next_url, resolve_base=resolve_base)
            page = task.get('page', 1) + 1
            grab2 = grab.clone()
            grab2.setup(url=url)
            task2 = task.clone(task_try_count=0, grab=grab2, page=page, **kwargs)
            self.add_task(task2)
            return True

    def process_links(self, grab, task_name, xpath,
                      resolve_base=False, limit=None, **kwargs):
        """
        :param grab: Grab instance
        :param xpath: xpath expression which calculates list of URLS
        :param task_name: name of task to generate

        Example::

            self.follow_links(grab, 'topic', '//div[@class="topic"]/a/@href')
        """
        urls = set()
        count = 0
        for url in grab.xpath_list(xpath):
            url = grab.make_url_absolute(url, resolve_base=resolve_base)
            if not url in urls:
                urls.add(url)
                g2 = grab.clone(url=url)
                self.add_task(Task(task_name, grab=g2, **kwargs))
                count += 1
                if limit is not None and count >= limit:
                    break

    # Deprecated methods

    def next_page_task(self, grab, task, xpath, **kwargs):
        """
        DEPRECATED, WILL BE REMOVED

        Return new `Task` object if link that mathes the given `xpath`
        was found.
        """

        logger.error('Method next_page_task is deprecated. Use process_next_page method instead.')
        nav = grab.xpath_one(xpath, None)
        if nav is not None:
            url = grab.make_url_absolute(nav.get('href'))
            page = task.get('page', 1) + 1
            grab2 = grab.clone()
            grab2.setup(url=url)
            task2 = task.clone(task_try_count=0, grab=grab2, page=page, **kwargs)
            return task2

    def follow_links(self, grab, xpath, task_name, task=None):
        """
        DEPRECATED, WILL BE REMOVED

        Args:
            :xpath: xpath expression which calculates list of URLS

        Example::

            self.follow_links(grab, '//div[@class="topic"]/a/@href', 'topic')
        """
        logger.error('Method follow_links is deprecated. Use process_links method instead.')

        urls = []
        for url in grab.xpath_list(xpath):
            #if not url.startswith('http') and self.base_url is None:
            #    raise SpiderError('You should define `base_url` attribute to resolve relative urls')
            url = urljoin(grab.config['url'], url)
            if not url in urls:
                urls.append(url)
                g2 = grab.clone()
                g2.setup(url=url)
                self.add_task(Task(task_name, grab=g2))

########NEW FILE########
__FILENAME__ = base
"""
QueueInterface defines interface of queue backend.
"""

class QueueInterface(object):
    def __init__(self, spider_name, **kwargs):
        pass

    def put(self, task, priority):
        pass

    def get(self):
        """
        Return `Task` object or raise `Queue.Empty` exception

        @returns: `grab.spider.task.Task` object
        @raises: `Queue.Empty` exception
        """

    def size(self):
        pass

    def clear(self):
        """
        Remove all tasks from the queue.
        """

########NEW FILE########
__FILENAME__ = memory
from datetime import datetime
try:
    from Queue import PriorityQueue, Empty
except ImportError:
    from queue import PriorityQueue, Empty

from grab.spider.queue_backend.base import QueueInterface

class QueueBackend(QueueInterface):
    def __init__(self, spider_name, **kwargs):
        super(QueueInterface, self).__init__(**kwargs)
        self.queue_object = PriorityQueue()
        self.schedule_list = []

    def put(self, task, priority, schedule_time=None):
        if schedule_time is None:
            self.queue_object.put((priority, task))
        else:
            self.schedule_list.append((schedule_time, task))

    def get(self):
        now = datetime.now()

        removed_indexes = []
        idx = 0
        for schedule_time, task in self.schedule_list:
            if schedule_time <= now:
                self.put(task, 1)
                removed_indexes.append(idx)
            idx += 1

        self.schedule_list = [x for idx, x in enumerate(self.schedule_list)
                              if not idx in removed_indexes]

        priority, task = self.queue_object.get(block=False)
        return task

    def size(self):
        return self.queue_object.qsize() + len(self.schedule_list)

    def clear(self):
        try:
            while True:
                self.queue_object.get(False)
        except Empty:
            pass
        self.schedule_list = []

########NEW FILE########
__FILENAME__ = mongo
try:
    import Queue as queue
except ImportError:
    import queue
from time import time
try:
    import cPickle as pickle
except ImportError:
    import pickle
import uuid
from bson import Binary
import logging
import pymongo

from grab.spider.queue_backend.base import QueueInterface
from grab.spider.error import SpiderMisuseError

logger = logging.getLogger('grab.spider.queue_backend.mongo')

class QueueBackend(QueueInterface):
    def __init__(self, spider_name, database=None, queue_name=None,
                 **kwargs):
        """
        All "unexpected" kwargs goes to `pymongo.Connection()` method
        """
        if queue_name is None:
            queue_name = 'task_queue_%s' % spider_name

        self.database = database
        self.queue_name = queue_name
        conn = pymongo.Connection(**kwargs)
        self.collection = conn[self.database][self.queue_name]
        logger.debug('Using collection: %s' % self.collection)

        self.collection.ensure_index('priority')

        super(QueueInterface, self).__init__(**kwargs)

    def clear_collection(self):
        logger.debug('Deleting collection: %s' % self.collection)
        self.collection.drop()

    def size(self):
        return self.collection.count()

    def put(self, task, priority, schedule_time=None):
        if schedule_time is not None:
            raise SpiderMisuseError('Mongo task queue does not support delayed task') 
        item = {
            'task': Binary(pickle.dumps(task)),
            'priority': priority,
        }
        self.collection.save(item)

    def get(self):
        item = self.collection.find_and_modify(
            sort=[('priority', pymongo.ASCENDING)],
            remove=True
        )
        if item is None:
            raise queue.Empty()
        else:
            return pickle.loads(item['task'])

    def clear(self):
        self.collection.remove()

########NEW FILE########
__FILENAME__ = redis
"""
Spider task queue backend powered by redis
"""
from __future__ import absolute_import
from redis import StrictRedis
from qr import PriorityQueue
try:
    import Queue as queue
except ImportError:
    import queue
import random
import logging

from grab.spider.queue_backend.base import QueueInterface
from grab.spider.error import SpiderMisuseError

class QueueBackend(QueueInterface):
    def __init__(self, spider_name, queue_name=None, **kwargs):
        super(QueueInterface, self).__init__(**kwargs)
        self.spider_name = spider_name
        if queue_name is None:
            queue_name = 'task_queue_%s' % spider_name
        self.queue_name = queue_name
        self.queue_object = PriorityQueue(queue_name)
        logging.debug('Redis queue key: %s' % self.queue_name)

    def put(self, task, priority, schedule_time=None):
        # Add attribute with random value
        # This is required because qr library
        # does not allow to store multiple values with same hash
        # in the PriorityQueue

        if schedule_time is not None:
            raise SpiderMisuseError('Redis task queue does not support delayed task') 
        task._rnd = random.random()
        self.queue_object.push(task, priority)


    def get(self):
        task = self.queue_object.pop()
        if task is None:
            raise queue.Empty()
        else:
            return task

    def size(self):
        return len(self.queue_object)

    def clear(self):
        con = StrictRedis()
        con.delete(self.queue_name)
        #try:
            #while True:
                #self.get()
        #except queue.Empty:
            #pass

########NEW FILE########
__FILENAME__ = save_result
from datetime import datetime
from traceback import format_exc
import os
import time

def save_result(func):
    def decorated(spider_name, *args, **kwargs):
        if not kwargs.get('save_result', False):
            return func(spider_name, *args, **kwargs)
        else:
            from grab.djangoui.grabstat.models import Task

            task = Task(
                task_name=spider_name,
                start_time=datetime.now(),
                pid=os.getpid(),
            )
            task.save()

            def dump_spider_stats(spider):
                now = time.time()
                if not hasattr(spider, '_log_task_timer') or now - spider._log_task_timer  > 60:
                    spider._log_task_timer = now
                    elapsed = datetime.now() - task.start_time
                    elapsed_time = (elapsed.days * 3600 * 24) + elapsed.seconds
                    Task.objects.filter(pk=task.pk)\
                        .update(spider_stats=spider.render_stats(timing=False),
                                spider_timing=spider.render_timing(),
                                elapsed_time=elapsed_time)

            kwargs['dump_spider_stats'] = dump_spider_stats
            kwargs['stats_object'] = task

            try:
                func_res = func(spider_name, *args, **kwargs)
            except Exception as ex:
                task.error_traceback = format_exc()
                task.is_ok = False
                raise
            else:
                task.spider_stats = func_res['spider_stats']
                task.spider_timing = func_res['spider_timing']
            finally:
                task.is_done = True
                task.end_time = datetime.now()
                elapsed = task.end_time - task.start_time
                task.elapsed_time = (elapsed.days * 3600 * 24) + elapsed.seconds
                task.save()
    return decorated

########NEW FILE########
__FILENAME__ = stat
import logging
import time
from grab.base import GLOBAL_STATE
from grab.tools.encoding import smart_str
import os
from contextlib import contextmanager

from grab.tools import metric
from grab.util.py3k_support import *

logger = logging.getLogger('grab.spider.stat')

class SpiderStat(object):
    """
    This base-class defines methods to use for
    collecting statistics about spider work.
    """

    def add_item(self, list_name, item, display=False):
        """
        You can call multiply time this method in process of parsing.

        self.add_item('foo', 4)
        self.add_item('foo', 'bar')

        and after parsing you can acces to all saved values:

        spider_instance.items['foo']
        """

        lst = self.items.setdefault(list_name, [])
        lst.append(item)
        if display:
            logger.debug(list_name)

    def save_list(self, list_name, path):
        """
        Save items from list to the file.
        """

        with open(path, 'w') as out:
            lines = []
            for item in self.items.get(list_name, []):
                if isinstance(item, basestring):
                    lines.append(smart_str(item))
                else:
                    lines.append(json.dumps(item))
            out.write('\n'.join(lines) + '\n')

    def render_stats(self, timing=True):
        out = []
        out.append('Counters:')
        # Sort counters by its names
        items = sorted(self.counters.items(), key=lambda x: x[0], reverse=True)
        out.append('  %s' % '\n  '.join('%s: %s' % x for x in items))
        out.append('\nLists:')
        # Sort lists by number of items
        items = [(x, len(y)) for x, y in self.items.items()]
        items = sorted(items, key=lambda x: x[1], reverse=True)
        out.append('  %s' % '\n  '.join('%s: %s' % x for x in items))

        if 'download-size' in self.counters:
            out.append('Network download: %s' % metric.format_traffic_value(self.counters['download-size']))
        if hasattr(self.taskq, 'qsize'):
            out.append('Queue size: %d' % self.taskq.qsize())
        else:
            out.append('Queue size: %d' % self.taskq.size())
        out.append('Threads: %d' % self.thread_number)

        if timing:
            out.append(self.render_timing())
        return '\n'.join(out) + '\n'

    def render_timing(self):
        out = []
        out.append('Timers:')
        out.append('  DOM: %.3f' % GLOBAL_STATE['dom_build_time'])
        out.append('  selector: %.03f' % GLOBAL_STATE['selector_time'])
        items = [(x, y) for x, y in self.timers.items()]
        items = sorted(items, key=lambda x: x[1])
        out.append('  %s' % '\n  '.join('%s: %.03f' % x for x in items))
        return '\n'.join(out) + '\n'

    def save_all_lists(self, dir_path):
        """
        Save each list into file in specified diretory.
        """

        for key, items in self.items.items():
            path = os.path.join(dir_path, '%s.txt' % key)
            self.save_list(key, path)

    def inc_count(self, key, count=1):
        """
        You can call multiply time this method in process of parsing.

        self.inc_count('regurl')
        self.inc_count('captcha')

        and after parsing you can acces to all saved values:

        print 'Total: %(total)s, captcha: %(captcha)s' % spider_obj.counters
        """

        self.counters[key] += count
        return self.counters[key]

    def start_timer(self, key):
        self.time_points['start-%s' % key] = time.time()

    def stop_timer(self, key):
        now = time.time()
        start_key = 'start-%s' % key
        try:
            start = self.time_points[start_key]
        except KeyError:
            logger.error('Could not find start point with key %s' % key)
            return 0
        else:
            total = now - start
            if not key in self.timers:
                self.timers[key] = 0
            self.timers[key] += total
            del self.time_points[start_key]
            return total

    @contextmanager
    def save_timer(self, key):
        self.start_timer(key)
        try:
            yield
        finally:
            self.stop_timer(key)

########NEW FILE########
__FILENAME__ = task
from random import randint
from datetime import datetime, timedelta

from grab.spider.error import SpiderMisuseError
from grab.base import copy_config

class BaseTask(object):
    pass


class Task(BaseTask):
    """
    Task for spider.
    """

    def __init__(self, name=None, url=None, grab=None, grab_config=None,
                 priority=None, priority_is_custom=True,
                 network_try_count=0, task_try_count=0, 
                 disable_cache=False, refresh_cache=False,
                 valid_status=[], use_proxylist=True,
                 cache_timeout=None, delay=0,
                 raw=False, callback=None,
                 fallback_name=None,
                 **kwargs):
        """
        Create `Task` object.

        If more than one of url, grab and grab_config options are non-empty then they
        processed in following order:
        * grab overwrite grab_config
        * grab_config overwrite url

        Args:
            :param name: name of the task. After successfull network operation
                task's result will be passed to `task_<name>` method.
            :param url: URL of network document. Any task requires `url` or `grab`
                option to be specified.
            :param grab: configured `Grab` instance. You can use that option in case
                when `url` option is not enough. Do not forget to configure `url` option
                of `Grab` instance because in this case the `url` option of `Task`
                constructor will be overwritten with `grab.config['url']`.
            :param priority: - priority of the Task. Tasks with lower priority will be
                processed earlier. By default each new task is assigned with random
                priority from (80, 100) range.
            :param priotiy_is_custom: - internal flag which tells if that task priority was
                assigned manually or generated by spider according to pririty generation rules.
            :param network_try_count: you'll probably will not need to use it. It is used
                internally to control how many times this task was restarted due to network
                errors. The `Spider` instance has `network_try_limit` option. When
                `network_try_count` attribut of the task exceeds the `network_try_limit`
                attribut then processing of the task is abandoned.
            :param task_try_count: the as `network_try_count` but it increased only then you
                use `clone` method. Also you can set it manually. It is usefull if you want
                to restart the task after it was cacelled due to multiple network errors.
                As you might guessed there is `task_try_limit` option in `Spider` instance.
                Both options `network_try_count` and `network_try_limit` guarantee you that
                you'll not get infinite loop of restarting some task.
            :param disable_cache: if `True` disable cache subsystem. The document will be
                fetched from the Network and it will not be saved to cache.
            :param refresh_cache: if `True` the document will be fetched from the Network
                and saved to cache.
            :param valid_status: extra status codes which counts as valid
            :param use_proxylist: it means to use proxylist which was configured
                via `setup_proxylist` method of spider
            :param cache_timeout: maximum age (in seconds) of cache record to be valid
            :param delay: if specified tells the spider to schedule the task and execute
                it after `delay` seconds
            :param raw: if `raw` is True then the network response is forwarding to the
                corresponding handler without any check of HTTP status code of network error,
                if `raw` is False (by default) then failed response is putting back
                to task queue or if tries limit is reached then the processing of this 
                request is finished.
            :param callback: if you pass some function in `callback` option then the
                network resposne will be passed to this callback and the usual 'task_*'
                handler will be ignored and no error will be raised if such 'task_*' handler
                does not exist.
            :param fallback_name: the name of method that is called when spider gives up to
                do the task (due to multiple network errors)
            Any non-standard named arguments passed to `Task` constructor will be saved as
            attributes of the object. You can get their values later as attributes or with
            `get` method which allows to use default value if attrubute does not exist.
        """

        if name == 'generator':
            # The name "generator" is restricted because
            # `task_generator` handler could not be created because
            # this name is already used for special method which
            # generates new tasks
            raise SpiderMisuseError('Task name could not be "generator"')

        self.name = name

        if url is None and grab is None and grab_config is None:
            raise SpiderMisuseError('Either url, grab or grab_config argument of Task constructor should not be None')

        if url is not None and grab is not None:
            raise SpiderMisuseError('Options url and grab could not be used together')

        if url is not None and grab_config is not None:
            raise SpiderMisuseError('Options url and grab_config could not be used together')

        if grab is not None and grab_config is not None:
            raise SpiderMisuseError('Options grab and grab_config could not be used together')

        if grab:
            self.setup_grab_config(grab.dump_config())
        elif grab_config:
            self.setup_grab_config(grab_config)
        else:
            self.grab_config = None
            self.url = url

        self.process_delay_option(delay)

        self.fallback_name = fallback_name
        self.priority_is_custom = priority_is_custom
        self.priority = priority
        self.network_try_count = network_try_count
        self.task_try_count = task_try_count
        self.disable_cache = disable_cache
        self.refresh_cache = refresh_cache
        self.valid_status = valid_status
        self.use_proxylist = use_proxylist
        self.cache_timeout = cache_timeout
        self.raw = raw
        self.callback = callback
        for key, value in kwargs.items():
            setattr(self, key, value)

    def get(self, key, default=None):
        """
        Return value of attribute or None if such attribute
        does not exist.
        """
        return getattr(self, key, default)

    def process_delay_option(self, delay):
        if delay:
            self.schedule_time = datetime.now() + timedelta(seconds=delay)
            self.original_delay = delay
        else:
            self.schedule_time = None
            self.original_delay = None

    def setup_grab_config(self, grab_config):
        self.grab_config = copy_config(grab_config)
        self.url = grab_config['url']

    def clone(self, **kwargs):
        """
        Clone Task instance.

        Reset network_try_count, increase task_try_count.
        """

        # First, create exact copy of the current Task object
        attr_copy = self.__dict__.copy()
        if attr_copy.get('grab_config') is not None:
            del attr_copy['url']
        task = Task(**attr_copy)

        # Reset some task properties if the have not
        # been set explicitly in kwargs
        if not 'network_try_count' in kwargs:
            task.network_try_count = 0
        if not 'task_try_count' in kwargs:
            task.task_try_count = self.task_try_count + 1
        if not 'refresh_cache' in kwargs:
            task.refresh_cache = False
        if not 'disable_cache' in kwargs:
            task.disable_cache = False

        if kwargs.get('url') is not None and kwargs.get('grab') is not None:
            raise SpiderMisuseError('Options url and grab could not be used together')

        if kwargs.get('url') is not None and kwargs.get('grab_config') is not None:
            raise SpiderMisuseError('Options url and grab_config could not be used together')

        if kwargs.get('grab') is not None and kwargs.get('grab_config') is not None:
            raise SpiderMisuseError('Options grab and grab_config could not be used together')

        if kwargs.get('grab'):
            task.setup_grab_config(kwargs['grab'].dump_config())
            del kwargs['grab']
        elif kwargs.get('grab_config'):
            task.setup_grab_config(kwargs['grab_config'])
            del kwargs['grab_config']
        elif kwargs.get('url'):
            task.url = kwargs['url']
            if task.grab_config:
                task.grab_config['url'] = kwargs['url']
            del kwargs['url']

        for key, value in kwargs.items():
            setattr(task, key, value)

        task.process_delay_option(task.get('delay', None))

        return task

    def __repr__(self):
        return '<Task: %s>' % self.url

    def __lt__(self, other):
        if self.priority and other.priority:
            return (self.priority < other.priority)
        else:
            return False

    def __eq__(self, other):
        return (self.priority == other.priority)

    def get_fallback_handler(self, spider):
        if self.fallback_name:
            return getattr(spider, self.fallback_name)
        elif self.name:
            fb_name = 'task_%s_fallback' % self.name
            if hasattr(spider, fb_name):
                return getattr(spider, fb_name)
        else:
            return None


class NullTask(BaseTask):
    def __init__(self, name='initial', sleep=0, priority=None,
                 priority_is_custom=True, network_try_count=0,
                 task_try_count=0):
        self.name = name
        self.sleep = sleep
        self.priority = None
        self.priority_is_custom = False
        self.network_try_count = network_try_count
        self.task_try_count = task_try_count

########NEW FILE########
__FILENAME__ = multicurl
import pycurl

from grab.util.py3k_support import *

class MulticurlTransport(object):
    def __init__(self, thread_number):
        self.thread_number = thread_number
        self.multi = pycurl.CurlMulti()
        self.multi.handles = []
        self.freelist = []
        self.registry = {}
        self.connection_count = {}

        # Create curl instances
        for x in xrange(self.thread_number):
            curl = pycurl.Curl()
            self.connection_count[id(curl)] = 0
            self.freelist.append(curl)
            #self.multi.handles.append(curl)

    def ready_for_task(self):
        return len(self.freelist)

    def active_task_number(self):
        return self.thread_number - len(self.freelist)

    def process_connection_count(self, curl):
        curl_id = id(curl)
        self.connection_count[curl_id] += 1
        if self.connection_count[curl_id] > 100:
            del self.connection_count[curl_id]
            del curl
            new_curl = pycurl.Curl()
            self.connection_count[id(new_curl)] = 1
            return new_curl
        else:
            return curl

    def process_task(self, task, grab, grab_config_backup):
        curl = self.process_connection_count(self.freelist.pop())

        self.registry[id(curl)] = {
            'grab': grab,
            'grab_config_backup': grab_config_backup,
            'task': task,
        }
        grab.transport.curl = curl
        try:
            grab.prepare_request()
            grab.log_request()
        except Exception as ex:
            # If some error occured while processing the request arguments
            # then we should put curl object back to free list
            del self.registry[id(curl)]
            self.freelist.append(curl)
            raise
        else:
            # Add configured curl instance to multi-curl processor
            self.multi.add_handle(curl)

    def process_handlers(self):
        # http://curl.haxx.se/libcurl/c/curl_multi_perform.html
        #res = self.multi.select(0.0001)
        #if res == -1:
            #return
        while True:
            #print '[inside PH]'
            status, active_objects = self.multi.perform()
            if status != pycurl.E_CALL_MULTI_PERFORM:
                break

    def iterate_results(self):
        while True:
            queued_messages, ok_list, fail_list = self.multi.info_read()

            results = []
            for curl in ok_list:
                results.append((True, curl, None, None))
            for curl, ecode, emsg in fail_list:
                # CURLE_WRITE_ERROR (23)
                # An error occurred when writing received data to a local file, or
                # an error was returned to libcurl from a write callback.
                # This exception should be ignored if _callback_interrupted flag
                # is enabled (this happens when nohead or nobody options enabeld)
                #
                # Also this error is raised when curl receives KeyboardInterrupt
                # while it is processing some callback function
                # (WRITEFUNCTION, HEADERFUNCTIO, etc)
                if ecode == 23:
                    if getattr(curl, '_callback_interrupted', None) == True:
                        curl._callback_interrupted = False
                        ecode = None
                        emsge = None
                        results.append((True, curl, None, None))
                    else:
                        results.append((False, curl, ecode, emsg))
                else:
                    results.append((False, curl, ecode, emsg))

            for ok, curl, ecode, emsg in results:
                # FORMAT: {ok, grab, grab_config_backup, task, emsg}

                curl_id = id(curl)
                task = self.registry[curl_id]['task']
                grab = self.registry[curl_id]['grab']
                grab_config_backup = self.registry[curl_id]['grab_config_backup']

                grab.process_request_result()
                grab.response.error_code = ecode
                grab.response.error_msg = emsg

                # Free resources
                del self.registry[curl_id]
                grab.transport.curl = None

                #if emsg and 'Operation timed out after' in emsg:
                    #num =  int(emsg.split('Operation timed out after')[1].strip().split(' ')[0])
                    #if num > 20000:
                        #import pdb; pdb.set_trace()

                yield {'ok': ok, 'emsg': emsg, 'grab': grab,
                       'grab_config_backup': grab_config_backup, 'task': task}

                self.multi.remove_handle(curl)
                self.freelist.append(curl)

            if not queued_messages:
                break

    def select(self, timeout=0.01):
        return self.multi.select(timeout)

########NEW FILE########
__FILENAME__ = threadpool
try:
    from Queue import Queue, Empty
except ImportError:
    from queue import Queue, Empty
from threading import Thread

from grab.error import GrabNetworkError
from grab.tools.work import make_work
from grab.util.py3k_support import *

STOP = object()

class Worker(Thread):
    def __init__(self, taskq, resultq, *args, **kwargs):
        self.taskq = taskq
        self.resultq = resultq
        self.busy = False
        Thread.__init__(self, *args, **kwargs)


    def run(self):
        while True:
            self.busy = False
            info = self.taskq.get()
            if info is STOP:
                return
            else:
                self.busy = True
                try:
                    info['grab'].request()
                except GrabNetworkError as ex:
                    ok = False
                    emsg = unicode(ex)
                except Exception as ex:
                    raise
                    # TODO: WTF?
                else:
                    ok = True
                    emsg = None
                self.resultq.put({'ok': ok, 'emsg': emsg, 'task': info['task'],
                                  'grab': info['grab'],
                                  'grab_config_backup': info['grab_config_backup']})


class ThreadPoolTransport(object):
    def __init__(self, thread_number):
        self.thread_number = thread_number
        self.taskq = Queue()
        self.resultq = Queue()
        self.threads = []
        for x in xrange(self.thread_number):
            t = Worker(self.taskq, self.resultq)
            t.daemon = True
            t.start()
            self.threads.append(t)

    def ready_for_task(self):
        return self.active_task_number() < self.thread_number

    def active_task_number(self):
        return sum(1 if x.busy else 0 for x in self.threads)

    def process_task(self, task, grab, grab_config_backup):
        grab.prepare_request()
        grab.log_request()
        self.taskq.put({'grab': grab, 'grab_config_backup': grab_config_backup,
                        'task': task})

    def wait_result(self):
        pass

    def iterate_results(self):
        while True:
            try:
                yield self.resultq.get(False)
            except Empty:
                break

    def select(self):
        pass
        #self.multi.select(0.01)

    def repair_grab(self, grab):
        # `curl` attribute should not be None
        # If it is None (which could be if we fire Task
        # object with grab object which was recevied in
        # as input argument of response handler function)
        # then `prepare_request` method will failed
        # because it asssumes that Grab instance
        # has valid `curl` attribute
        # TODO: Looks strange
        # Maybe refactor prepare_request method
        # to not fail on grab instance with empty curl instance
        if grab.curl is None:
            grab.curl = CURL_OBJECT

########NEW FILE########
__FILENAME__ = extract
"""
Facebook last name list converter
"""

def parse_line(line):
	line = line.strip()
	counter, lname = line.split(' ', 1)
	if int(counter) > 300:
		return lname.capitalize()
	else:
		return None


with open('common.txt', 'r') as f:
	lines = f.readlines()

print(len(lines))
data = [_f for _f in [parse_line(line) for line in lines] if _f]
print(len(data))

with open('result.txt', 'w') as f:
	f.write('lname\n')
	for item in data:
		f.write('%s\n' % item)

########NEW FILE########
__FILENAME__ = parse_ru_lname
try:
    from urllib import urlopen
except ImportError:
    from urllib.request import urlopen
import re

urls = """
http://genofond.binec.ru/default2.aspx?p=98
http://genofond.binec.ru/default2.aspx?s=0&p=69
http://genofond.binec.ru/default2.aspx?s=0&p=70
http://genofond.binec.ru/default2.aspx?s=0&p=71
http://genofond.binec.ru/default2.aspx?s=0&p=72
http://genofond.binec.ru/default2.aspx?s=0&p=73
http://genofond.binec.ru/default2.aspx?s=0&p=74
http://genofond.binec.ru/default2.aspx?s=0&p=75
http://genofond.binec.ru/default2.aspx?s=0&p=76
http://genofond.binec.ru/default2.aspx?s=0&p=77
http://genofond.binec.ru/default2.aspx?s=0&p=78
http://genofond.binec.ru/default2.aspx?s=0&p=79
http://genofond.binec.ru/default2.aspx?s=0&p=80
http://genofond.binec.ru/default2.aspx?s=0&p=81
http://genofond.binec.ru/default2.aspx?s=0&p=82
http://genofond.binec.ru/default2.aspx?s=0&p=83
http://genofond.binec.ru/default2.aspx?s=0&p=84
http://genofond.binec.ru/default2.aspx?s=0&p=85
http://genofond.binec.ru/default2.aspx?s=0&p=88
http://genofond.binec.ru/default2.aspx?s=0&p=89
http://genofond.binec.ru/default2.aspx?s=0&p=90
http://genofond.binec.ru/default2.aspx?s=0&p=91
http://genofond.binec.ru/default2.aspx?s=0&p=92
http://genofond.binec.ru/default2.aspx?s=0&p=93
http://genofond.binec.ru/default2.aspx?s=0&p=94
http://genofond.binec.ru/default2.aspx?s=0&p=95
http://genofond.binec.ru/default2.aspx?s=0&p=96
http://genofond.binec.ru/default2.aspx?s=0&p=97
"""

urls = [x.strip() for x in urls.strip().splitlines()]

re_lname = re.compile(r'<FONT face=Calibri color=#000000 size=3>([^\d][^<]+)</FONT>')
outfile = file('ru_lname.txt', 'w')

for url in urls:
    print(url)
    data = urlopen(url).read().decode('cp1251')

    items = []
    for lname in re_lname.findall(data):
        lname = lname.lower().capitalize()
        outfile.write(lname.encode('utf-8') + '\n')
        print(lname)

########NEW FILE########
__FILENAME__ = util
import os.path
try:
    from string import letters
except ImportError:
    from string import ascii_letters as letters
from string import digits, ascii_lowercase, ascii_uppercase
from random import choice, randint
from datetime import date
from functools import reduce

from grab.util.py3k_support import *

SAFE_CHARS = letters + digits
S_CHARS = 'bcdfghjklmnpqrstvwxz'
G_CHARS = 'aeiouy'
EMAIL_SERVERS = ['gmail.com', 'yahoo.com']

FILES_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'files')
CACHE = {}

def load_items(name):
    if not name in CACHE:
        path = os.path.join(FILES_DIR, name)
        items = [x.strip().decode('utf-8') for x in open(path) if x.strip()]
        CACHE[name] = items
    return CACHE[name]


def random_password(min_length=8, max_length=10):
    """
    Make random password.

    Ensures that password has at least one digit, upper case letter and
    lower case letter.
    """

    chars = ''.join(map(choice, (ascii_lowercase, ascii_uppercase, digits)))
    length = list(range(randint(min_length, max_length) - len(chars)))
    return reduce(lambda a, b: a + choice(SAFE_CHARS), length, chars)


def random_login(min_length=8, max_length=8):
    """
    Make random login.
    """

    length = randint(min_length, max_length)
    chars = []
    for x in xrange(0, length, 2):
       chars.extend((choice(G_CHARS), choice(S_CHARS)))
    return ''.join(chars[:length])


def random_birthday(start_year=1960, end_year=1990):
    """
    Make random birth date.
    """

    date_obj = date(randint(start_year, end_year), randint(1, 12), randint(1, 28))
    return {
        'day': str(date_obj.day),
        'month': str(date_obj.month),
        'year': str(date_obj.year),
        'date': date_obj,
    }


def random_email(login=None):
    """
    Make random email.
    """

    if not login:
        login = random_login()
    return '%s@%s' % (login, choice(EMAIL_SERVERS))


def random_fname(lang='en'):
    """
    Return random first name.
    """

    return choice(load_items('%s_fname.txt' % lang))


def random_lname(lang='en'):
    """
    Return random last name.
    """

    return choice(load_items('%s_lname.txt' % lang))


def random_city(lang='en'):
    """
    Return random city.
    """

    return choice(load_items('%s_city.txt' % lang))


def random_icq():
    """
    Make random ICQ number.
    """

    return  str(randint(100000000, 999999999))


def random_phone():
    """
    Return random phone
    """

    return '+%d%d%d' % (randint(1, 9), randint(100, 999),
                        randint(1000000, 9999999))


def random_zip():
    return str(randint(10000, 99999))


def get_random_avatar(folder):
    """
    Return random avatar file form folder
    """
    avatar = choice(os.listdir(folder))
    return os.path.join(folder, avatar)


class AccountData(object):
    def random_fname(self, *args, **kwargs):
        self.fname = random_fname(*args, **kwargs)
        return self.fname

    def random_lname(self, *args, **kwargs):
        self.lname = random_lname(*args, **kwargs)
        return self.lname

    def random_password(self, *args, **kwargs):
        self.password = random_password(*args, **kwargs)
        return self.password

    def random_login(self, *args, **kwargs):
        self.login = random_login(*args, **kwargs)
        return self.login

########NEW FILE########
__FILENAME__ = antigate
try:
    from urllib2 import Request
    from urllib2 import urlopen as urlopen2
except ImportError:
    from urllib.request import Request
    from urllib.request import urlopen as urlopen2
try:
    from urllib import urlencode, urlopen
except ImportError:
    from urllib.parse import urlencode
    from urllib.request import urlopen
import logging
import time
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

from grab.tools.captcha.contrib.poster.encode import multipart_encode, MultipartParam
from grab.tools.captcha.contrib.poster.streaminghttp import register_openers
from grab.tools.captcha.error import CaptchaError
from grab.util.py3k_support import *

register_openers()
logger = logging.getLogger('grab.tools.captcha.antigate')

def send_captcha(key, fobj):
    items = []
    items.append(MultipartParam(name='key', value=key))
    items.append(MultipartParam(name='method', value='post'))
    items.append(MultipartParam(name='file', filename='captcha.jpg',
                                fileobj=fobj))

    data, headers = multipart_encode(items)
    req = Request('http://antigate.com/in.php', data, headers)
    res = urlopen2(req)
    if res.code == 200:
        chunks = res.read().split('|')
        if len(chunks) == 2:
            return int(chunks[1])
        else:
            msg = chunks[0]
            raise CaptchaError(msg)
    else:
        msg = '%s %s' % (res.code, res.msg)
        raise CaptchaError(msg)
    print(res.info())
    return res.read()

def get_solution(key, captcha_id):
    #logger.debug('Getting solution for captcha %s' % captcha_id)
    params = {'key': key, 'action': 'get', 'id': captcha_id}
    url = 'http://antigate.com/res.php?%s' % urlencode(params)
    data = urlopen(url).read()
    chunks = data.split('|')
    if len(chunks) == 2:
        return chunks[1]
    else:
        msg = chunks[0]
        raise CaptchaError(msg)


def solve_captcha(key, data):
    if key is None:
        raise Exception('antigate key could not None')

    fobj = StringIO(data)

    captcha_id = None
    for x in xrange(30):
        try:
            captcha_id = send_captcha(key, fobj)
        except CaptchaError as ex:
            if ex.args[0] == 'ERROR_NO_SLOT_AVAILABLE':
                logger.debug('No antigate slot available')
                time.sleep(1)
            else:
                raise
        else:
            break
            
    if captcha_id is None:
        raise CaptchaError('No antigate slot available')

    logger.debug('Getting solution for captcha#%d' % captcha_id)
    for x in xrange(20):
        try:
            return get_solution(key, captcha_id)
        except CaptchaError as ex:
            if ex.args[0] == 'CAPCHA_NOT_READY':
                #logger.debug('Waiting for captcha solution')
                time.sleep(3)
            else:
                raise

    raise CaptchaError('No antigate slot available')

########NEW FILE########
__FILENAME__ = browser
"""
This module just displays captcha image in the browser and
then read captcha solution from the console input.
"""
import tempfile
import webbrowser
import time
import os

from grab.util.py3k_support import *

#def build_html(image_url):
    #return """
        #<img src="%s" />
        #<script type="text/javascript">
            #setTimeout(5000, window.close);
        #</script>
    #""" % image_url

    
def solve_captcha(data, *args, **kwargs):
    fd, image_path = tempfile.mkstemp()
    open(image_path, 'w').write(data)
    image_url = 'file://' + image_path
    #page_path = tempfile.mkstemp()
    #html = build_html(image_url)
    #open(page_path, 'w').write(html)
    #page_url = 'file://' + page_path

    webbrowser.open(url=image_url)
    # Sleep for short to display rrors which
    # browser could display to stdout
    time.sleep(0.2)
    solution = raw_input('Solution: ')
    os.unlink(image_path)
    return solution

########NEW FILE########
__FILENAME__ = encode
"""multipart/form-data encoding module

This module provides functions that faciliate encoding name/value pairs
as multipart/form-data suitable for a HTTP POST or PUT request.

multipart/form-data is the standard way to upload files over HTTP"""

__all__ = ['gen_boundary', 'encode_and_quote', 'MultipartParam',
        'encode_string', 'encode_file_header', 'get_body_size', 'get_headers',
        'multipart_encode']

try:
    import uuid
    def gen_boundary():
        """Returns a random string to use as the boundary for a message"""
        return uuid.uuid4().hex
except ImportError:
    import random, sha
    def gen_boundary():
        """Returns a random string to use as the boundary for a message"""
        bits = random.getrandbits(160)
        return sha.new(str(bits)).hexdigest()

try:
    from urllib import quote_plus
except ImportError:
    from urllib.parse import quote_plus
import re, os, mimetypes

from grab.util.py3k_support import *

def encode_and_quote(data):
    """If ``data`` is unicode, return quote_plus(data.encode("utf-8"))
    otherwise return quote_plus(data)"""
    if data is None:
        return None

    if isinstance(data, unicode):
        data = data.encode("utf-8")
    return quote_plus(data)

def _strify(s):
    """If s is a unicode string, encode it to UTF-8 and return the results,
    otherwise return str(s), or None if s is None"""
    if s is None:
        return None
    if isinstance(s, unicode):
        return s.encode("utf-8")
    return str(s)

class MultipartParam(object):
    """Represents a single parameter in a multipart/form-data request

    ``name`` is the name of this parameter.

    If ``value`` is set, it must be a string or unicode object to use as the
    data for this parameter.

    If ``filename`` is set, it is what to say that this parameter's filename
    is.  Note that this does not have to be the actual filename any local file.

    If ``filetype`` is set, it is used as the Content-Type for this parameter.
    If unset it defaults to "text/plain; charset=utf8"

    If ``filesize`` is set, it specifies the length of the file ``fileobj``

    If ``fileobj`` is set, it must be a file-like object that supports
    .read().

    Both ``value`` and ``fileobj`` must not be set, doing so will
    raise a ValueError assertion.

    If ``fileobj`` is set, and ``filesize`` is not specified, then
    the file's size will be determined first by stat'ing ``fileobj``'s
    file descriptor, and if that fails, by seeking to the end of the file,
    recording the current position as the size, and then by seeking back to the
    beginning of the file.
    """
    def __init__(self, name, value=None, filename=None, filetype=None,
                        filesize=None, fileobj=None):
        self.name = encode_and_quote(name)
        self.value = _strify(value)
        if filename is None:
            self.filename = None
        else:
            if isinstance(filename, unicode):
                # Encode with XML entities
                self.filename = filename.encode("ascii", "xmlcharrefreplace")
            else:
                self.filename = str(filename)
            self.filename = self.filename.encode("string_escape").\
                    replace('"', '\\"')
        self.filetype = _strify(filetype)

        self.filesize = filesize
        self.fileobj = fileobj

        if self.value is not None and self.fileobj is not None:
            raise ValueError("Only one of value or fileobj may be specified")

        if fileobj is not None and filesize is None:
            # Try and determine the file size
            try:
                self.filesize = os.fstat(fileobj.fileno()).st_size
            except (OSError, AttributeError):
                try:
                    fileobj.seek(0, 2)
                    self.filesize = fileobj.tell()
                    fileobj.seek(0)
                except:
                    raise ValueError("Could not determine filesize")

    def __cmp__(self, other):
        attrs = ['name', 'value', 'filename', 'filetype', 'filesize', 'fileobj']
        myattrs = [getattr(self, a) for a in attrs]
        oattrs = [getattr(other, a) for a in attrs]
        return cmp(myattrs, oattrs)

    @classmethod
    def from_file(cls, paramname, filename):
        """Returns a new MultipartParam object constructed from the local
        file at ``filename``.

        ``filesize`` is determined by os.path.getsize(``filename``)

        ``filetype`` is determined by mimetypes.guess_type(``filename``)[0]

        ``filename`` is set to os.path.basename(``filename``)
        """

        return cls(paramname, filename=os.path.basename(filename),
                filetype=mimetypes.guess_type(filename)[0],
                filesize=os.path.getsize(filename),
                fileobj=open(filename, "rb"))

    @classmethod
    def from_params(cls, params):
        """Returns a list of MultipartParam objects from a sequence of
        name, value pairs, MultipartParam instances,
        or from a mapping of names to values

        The values may be strings or file objects."""
        if hasattr(params, 'items'):
            params = params.items()

        retval = []
        for item in params:
            if isinstance(item, cls):
                retval.append(item)
                continue
            name, value = item
            if hasattr(value, 'read'):
                # Looks like a file object
                filename = getattr(value, 'name', None)
                if filename is not None:
                    filetype = mimetypes.guess_type(filename)[0]
                else:
                    filetype = None

                retval.append(cls(name=name, filename=filename,
                    filetype=filetype, fileobj=value))
            else:
                retval.append(cls(name, value))
        return retval

    def encode_hdr(self, boundary):
        """Returns the header of the encoding of this parameter"""
        boundary = encode_and_quote(boundary)

        headers = ["--%s" % boundary]

        if self.filename:
            disposition = 'form-data; name="%s"; filename="%s"' % (self.name,
                    self.filename)
        else:
            disposition = 'form-data; name="%s"' % self.name

        headers.append("Content-Disposition: %s" % disposition)

        if self.filetype:
            filetype = self.filetype
        else:
            filetype = "text/plain; charset=utf-8"

        headers.append("Content-Type: %s" % filetype)

        if self.filesize is not None:
            headers.append("Content-Length: %i" % self.filesize)
        else:
            headers.append("Content-Length: %i" % len(self.value))

        headers.append("")
        headers.append("")

        return "\r\n".join(headers)

    def encode(self, boundary):
        """Returns the string encoding of this parameter"""
        if self.value is None:
            value = self.fileobj.read()
        else:
            value = self.value

        if re.search("^--%s$" % re.escape(boundary), value, re.M):
            raise ValueError("boundary found in encoded string")

        return "%s%s\r\n" % (self.encode_hdr(boundary), value)

    def iter_encode(self, boundary, blocksize=4096):
        """Yields the encoding of this parameter
        If self.fileobj is set, then blocks of ``blocksize`` bytes are read and
        yielded."""
        if self.value is not None:
            yield self.encode(boundary)
        else:
            yield self.encode_hdr(boundary)
            last_block = ""
            encoded_boundary = "--%s" % encode_and_quote(boundary)
            boundary_exp = re.compile("^%s$" % re.escape(encoded_boundary),
                    re.M)
            while True:
                block = self.fileobj.read(blocksize)
                if not block:
                    yield "\r\n"
                    break
                last_block += block
                if boundary_exp.search(last_block):
                    raise ValueError("boundary found in file data")
                last_block = last_block[-len(encoded_boundary)-2:]
                yield block

    def get_size(self, boundary):
        """Returns the size in bytes that this param will be when encoded
        with the given boundary."""
        if self.filesize is not None:
            valuesize = self.filesize
        else:
            valuesize = len(self.value)

        return len(self.encode_hdr(boundary)) + 2 + valuesize

def encode_string(boundary, name, value):
    """Returns ``name`` and ``value`` encoded as a multipart/form-data
    variable.  ``boundary`` is the boundary string used throughout
    a single request to separate variables."""

    return MultipartParam(name, value).encode(boundary)

def encode_file_header(boundary, paramname, filesize, filename=None,
        filetype=None):
    """Returns the leading data for a multipart/form-data field that contains
    file data.

    ``boundary`` is the boundary string used throughout a single request to
    separate variables.

    ``paramname`` is the name of the variable in this request.

    ``filesize`` is the size of the file data.

    ``filename`` if specified is the filename to give to this field.  This
    field is only useful to the server for determining the original filename.

    ``filetype`` if specified is the MIME type of this file.

    The actual file data should be sent after this header has been sent.
    """

    return MultipartParam(paramname, filesize=filesize, filename=filename,
            filetype=filetype).encode_hdr(boundary)

def get_body_size(params, boundary):
    """Returns the number of bytes that the multipart/form-data encoding
    of ``params`` will be."""
    size = sum(p.get_size(boundary) for p in MultipartParam.from_params(params))
    return size + len(boundary) + 6

def get_headers(params, boundary):
    """Returns a dictionary with Content-Type and Content-Length headers
    for the multipart/form-data encoding of ``params``."""
    headers = {}
    boundary = quote_plus(boundary)
    headers['Content-Type'] = "multipart/form-data; boundary=%s" % boundary
    headers['Content-Length'] = get_body_size(params, boundary)
    return headers

def multipart_encode(params, boundary=None):
    """Encode ``params`` as multipart/form-data.

    ``params`` should be a sequence of (name, value) pairs or MultipartParam
    objects, or a mapping of names to values.
    Values are either strings parameter values, or file-like objects to use as
    the parameter value.  The file-like objects must support .read() and either
    .fileno() or both .seek() and .tell().

    If ``boundary`` is set, then it as used as the MIME boundary.  Otherwise
    a randomly generated boundary will be used.  In either case, if the
    boundary string appears in the parameter values a ValueError will be
    raised.

    Returns a tuple of `datagen`, `headers`, where `datagen` is a
    generator that will yield blocks of data that make up the encoded
    parameters, and `headers` is a dictionary with the assoicated
    Content-Type and Content-Length headers.

    Examples:

    >>> datagen, headers = multipart_encode( [("key", "value1"), ("key", "value2")] )
    >>> s = "".join(datagen)
    >>> assert "value2" in s and "value1" in s

    >>> p = MultipartParam("key", "value2")
    >>> datagen, headers = multipart_encode( [("key", "value1"), p] )
    >>> s = "".join(datagen)
    >>> assert "value2" in s and "value1" in s

    >>> datagen, headers = multipart_encode( {"key": "value1"} )
    >>> s = "".join(datagen)
    >>> assert "value2" not in s and "value1" in s

    """
    if boundary is None:
        boundary = gen_boundary()
    else:
        boundary = quote_plus(boundary)

    headers = get_headers(params, boundary)
    params = MultipartParam.from_params(params)

    def yielder():
        """generator function to yield multipart/form-data representation
        of parameters"""
        for param in params:
            for block in param.iter_encode(boundary):
                yield block
        yield "--%s--\r\n" % boundary

    return yielder(), headers

########NEW FILE########
__FILENAME__ = streaminghttp
"""Streaming HTTP uploads module.

This module extends the standard httplib and urllib2 objects so that
iterable objects can be used in the body of HTTP requests.

In most cases all one should have to do is call :func:`register_openers()`
to register the new streaming http handlers which will take priority over
the default handlers, and then you can use iterable objects in the body
of HTTP requests.

**N.B.** You must specify a Content-Length header if using an iterable object
since there is no way to determine in advance the total size that will be
yielded, and there is no way to reset an interator.

Example usage:

>>> from StringIO import StringIO
>>> import urllib2, poster.streaminghttp

>>> opener = poster.streaminghttp.register_openers()

>>> s = "Test file data"
>>> f = StringIO(s)

>>> req = urllib2.Request("http://localhost:5000", f, \
        {'Content-Length': len(s)})
"""

import socket
try:
    import urllib2
    from urllib2 import HTTPError
except ImportError:
    import urllib.request as urllib2
    from urllib.error import HTTPError
try:
    import httplib
except ImportError:
    import http.client as httplib

__all__ = ['StreamingHTTPConnection', 'StreamingHTTPRedirectHandler',
        'StreamingHTTPHandler', 'register_openers']

if hasattr(httplib, 'HTTPS'):
    __all__.extend(['StreamingHTTPSHandler', 'StreamingHTTPSConnection'])

class _StreamingHTTPMixin:
    """Mixin class for HTTP and HTTPS connections that implements a streaming
    send method."""
    def send(self, value):
        """Send ``value`` to the server.

        ``value`` can be a string object, a file-like object that supports
        a .read() method, or an iterable object that supports a .next()
        method.
        """
        # Based on python 2.6's httplib.HTTPConnection.send()
        if self.sock is None:
            if self.auto_open:
                self.connect()
            else:
                raise httplib.NotConnected()

        # send the data to the server. if we get a broken pipe, then close
        # the socket. we want to reconnect when somebody tries to send again.
        #
        # NOTE: we DO propagate the error, though, because we cannot simply
        #       ignore the error... the caller will know if they can retry.
        if self.debuglevel > 0:
            print("send:", repr(value))
        try:
            blocksize = 8192
            if hasattr(value, 'read') :
                if self.debuglevel > 0:
                    print("sendIng a read()able")
                data = value.read(blocksize)
                while data:
                    self.sock.sendall(data)
                    data = value.read(blocksize)
            elif hasattr(value, 'next'):
                if self.debuglevel > 0:
                    print("sendIng an iterable")
                for data in value:
                    self.sock.sendall(data)
            else:
                self.sock.sendall(value)
        except socket.error as v:
            if v[0] == 32:      # Broken pipe
                self.close()
            raise

class StreamingHTTPConnection(_StreamingHTTPMixin, httplib.HTTPConnection):
    """Subclass of `httplib.HTTPConnection` that overrides the `send()` method
    to support iterable body objects"""

class StreamingHTTPRedirectHandler(urllib2.HTTPRedirectHandler):
    """Subclass of `urllib2.HTTPRedirectHandler` that overrides the
    `redirect_request` method to properly handle redirected POST requests

    This class is required because python 2.5's HTTPRedirectHandler does
    not remove the Content-Type or Content-Length headers when requesting
    the new resource, but the body of the original request is not preserved.
    """

    handler_order = urllib2.HTTPRedirectHandler.handler_order - 1

    # From python2.6 urllib2's HTTPRedirectHandler
    def redirect_request(self, req, fp, code, msg, headers, newurl):
        """Return a Request or None in response to a redirect.

        This is called by the http_error_30x methods when a
        redirection response is received.  If a redirection should
        take place, return a new Request to allow http_error_30x to
        perform the redirect.  Otherwise, raise HTTPError if no-one
        else should try to handle this url.  Return None if you can't
        but another Handler might.
        """
        m = req.get_method()
        if (code in (301, 302, 303, 307) and m in ("GET", "HEAD")
            or code in (301, 302, 303) and m == "POST"):
            # Strictly (according to RFC 2616), 301 or 302 in response
            # to a POST MUST NOT cause a redirection without confirmation
            # from the user (of urllib2, in this case).  In practice,
            # essentially all clients do redirect in this case, so we
            # do the same.
            # be conciliant with URIs containing a space
            newurl = newurl.replace(' ', '%20')
            newheaders = dict((k, v) for k, v in req.headers.items()
                              if k.lower() not in (
                                  "content-length", "content-type")
                             )
            return urllib2.Request(newurl,
                           headers=newheaders,
                           origin_req_host=req.get_origin_req_host(),
                           unverifiable=True)
        else:
            raise HTTPError(req.get_full_url(), code, msg, headers, fp)

class StreamingHTTPHandler(urllib2.HTTPHandler):
    """Subclass of `urllib2.HTTPHandler` that uses
    StreamingHTTPConnection as its http connection class."""

    handler_order = urllib2.HTTPHandler.handler_order - 1

    def http_open(self, req):
        """Open a StreamingHTTPConnection for the given request"""
        return self.do_open(StreamingHTTPConnection, req)

    def http_request(self, req):
        """Handle a HTTP request.  Make sure that Content-Length is specified
        if we're using an interable value"""
        # Make sure that if we're using an iterable object as the request
        # body, that we've also specified Content-Length
        if req.has_data():
            data = req.get_data()
            if hasattr(data, 'read') or hasattr(data, 'next'):
                if not req.has_header('Content-length'):
                    raise ValueError(
                            "No Content-Length specified for iterable body")
        return urllib2.HTTPHandler.do_request_(self, req)

if hasattr(httplib, 'HTTPS'):
    class StreamingHTTPSConnection(_StreamingHTTPMixin,
            httplib.HTTPSConnection):
        """Subclass of `httplib.HTTSConnection` that overrides the `send()`
        method to support iterable body objects"""

    class StreamingHTTPSHandler(urllib2.HTTPSHandler):
        """Subclass of `urllib2.HTTPSHandler` that uses
        StreamingHTTPSConnection as its http connection class."""

        handler_order = urllib2.HTTPSHandler.handler_order - 1

        def https_open(self, req):
            return self.do_open(StreamingHTTPSConnection, req)

        def https_request(self, req):
            # Make sure that if we're using an iterable object as the request
            # body, that we've also specified Content-Length
            if req.has_data():
                data = req.get_data()
                if not hasattr(data, 'read') and hasattr(data, 'next'):
                    if not req.has_header('Content-length'):
                        raise ValueError(
                                "No Content-Length specified for iterable body")
            return urllib2.HTTPSHandler.do_request_(self, req)


def register_openers():
    """Register the streaming http handlers in the global urllib2 default
    opener object.

    Returns the created OpenerDirector object."""
    handlers = [StreamingHTTPHandler, StreamingHTTPRedirectHandler]
    if hasattr(httplib, "HTTPS"):
        handlers.append(StreamingHTTPSHandler)

    opener = urllib2.build_opener(*handlers)

    urllib2.install_opener(opener)

    return opener

########NEW FILE########
__FILENAME__ = error
class CaptchaError(Exception):
    pass

########NEW FILE########
__FILENAME__ = gui
import pygtk
pygtk.require('2.0')
import gtk
import os
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO
import tempfile

class CaptchaWindow(object):
    def __init__(self, path, solution):
        self.solution = solution
        self.window = gtk.Window(gtk.WINDOW_TOPLEVEL)
        self.window.show()
        self.window.connect('destroy', self.destroy)
        self.box = gtk.HBox()
        self.image = gtk.Image()
        self.image.set_from_file(path)
        self.entry = gtk.Entry()
        self.entry.connect('activate', self.solve)
        self.button = gtk.Button('Go')
        self.button.connect('clicked', self.solve)

        self.window.add(self.box)
        self.box.pack_start(self.image)
        self.box.pack_start(self.entry)
        self.box.pack_start(self.button)
        self.box.show()
        self.image.show()
        self.button.show()
        self.entry.show()
        self.entry.grab_focus()

    def destroy(self, *args):
        gtk.main_quit()

    def solve(self, *args):
        self.solution.append(self.entry.get_text())
        self.window.hide()
        gtk.main_quit()

    def main(self):
        gtk.main()


def solve_captcha(data, key=None):
    fobj = StringIO(data)
    solution = []
    fh, path = tempfile.mkstemp()
    open(path, 'w').write(fobj.read())
    window = CaptchaWindow(path, solution)
    window.main()
    os.unlink(path)
    return solution[0]

########NEW FILE########
__FILENAME__ = util
import logging
import re
import random

from grab.tools.captcha.error import CaptchaError
from grab.util.py3k_support import *

RE_SCRIPT = re.compile(r'<script[^>]+recaptcha\.net[^>]+>', re.S)
RE_SCRIPT2 = re.compile(r'<script[^>]+google\.com/recaptcha/api/challenge[^>]+>', re.S)
RE_SCRIPT3 = re.compile(r'Recaptcha\.create\("([^"]+)', re.S | re.I)
RE_SRC = re.compile(r'src="([^"]+)"')


class CaptchaSolver(object):
    def __init__(self, module='antigate', key=None):
        self.module = __import__('grab.tools.captcha.%s' % module,
                                 globals(), locals(), ['foo'])
        self.key = key

    def solve_captcha(self, g, url=None, data=None):
        if not g.clone_counter:
            logging.error('Warning: maybe you forgot to make the clone of Grab instance')

        if url:
            logging.debug('Downloading captcha')
            g.request(url=url)
            data = g.response.body

        logging.debug('Solving captcha')
        solution = self.module.solve_captcha(key=self.key, data=data)

        logging.debug('Captcha solved: %s' % solution)
        return solution


    def solve_recaptcha(self, g):
        if not g.clone_counter:
            logging.error('Warning: maybe you forgot to make the clone of Grab instance')

        def fetch_challenge():
            for x in xrange(5):
                url = None
                match = RE_SCRIPT.search(g.response.body)
                if match:
                    url = RE_SRC.search(match.group(0)).group(1)
                if not url:
                    match = RE_SCRIPT2.search(g.response.body)
                    if match:
                        url = RE_SRC.search(match.group(0)).group(1)
                if not url:
                    if 'google.com/recaptcha/api/js/recaptcha_ajax.js' in g.response.body:
                        # It is type of google recaptcha
                        match = RE_SCRIPT3.search(g.response.body)
                        code = match.group(1)
                        url = 'http://www.google.com/recaptcha/api/challenge'\
                              '?k=%s&ajax=1&cachestop=%s' % (code, str(random.random()))
                        #response = frame_loader.response.body
                        #rex = re.compile(r"challenge : '[^\"\s]+',")
                        #challenge_code = rex.search(response).group(0)[13:-2]
                        
                        #image_loader = frame_loader.clone()
                        #image_url = 'https://www.google.com/recaptcha/api/image?c=%s' % challenge_code
                        #solution = solve_captcha(image_loader, url=image_url)

                if not url:
                    raise Exception('Unknown recaptcha implementation')

                g.request(url=url)
                html = g.response.body

                if not html:
                    logging.error('Empty response from recaptcha server')
                    continue

                server = re.compile(r'server\s*:\s*\'([^\']+)').search(html).group(1)
                challenge = re.compile(r'challenge\s*:\s*\'([^\']+)').search(html).group(1)
                url = server + 'image?c=' + challenge
                return challenge, url
            raise CaptchaError('Could not get valid response from recaptcha server')

        challenge, url = fetch_challenge()
        solution = self.solve_captcha(g, url=url)
        return challenge, solution

########NEW FILE########
__FILENAME__ = content
import re

from grab.tools.text import normalize_space as normalize_space_func, find_number

def find_content_blocks(tree, min_length=None):
    """
    Iterate over content blocks (russian version)
    """
    from lxml.html import tostring
    from lxml.etree import strip_tags, strip_elements, Comment

    # Completely remove content of following tags
    nondata_tags = ['head', 'style', 'script', Comment]
    strip_elements(tree, *nondata_tags)

    # Remove links
    strip_elements(tree, 'a')

    # Drop inlines tags
    inline_tags = ('br', 'hr', 'p', 'b', 'i', 'strong', 'em', 'a',
                   'span', 'font')
    strip_tags(tree, *inline_tags)

    # Cut of images
    media_tags = ('img',)
    strip_tags(tree, *media_tags)

    body = tostring(tree, encoding='utf-8').decode('utf-8')

    # Normalize spaces
    body = normalize_space_func(body)

    # Find text blocks
    block_rex = re.compile(r'[^<>]+')

    blocks = []
    for match in block_rex.finditer(body):
        block = match.group(0)
        if len(block) > 100:
            ratio = _trash_ratio(block)
            if ratio < 0.05:
                block = block.strip()
                if min_length is None or len(block) >= min_length:
                    words = block.split()
                    if not any(len(x) > 50 for x in words):
                        blocks.append(block)
    return blocks

def _trash_ratio(text):
    """
    Return ratio of non-common symbols.
    """

    trash_count = 0
    for char in text:
        if char in list(u'.\'"+-!?()[]{}*+@#$%^&_=|/\\'):
            trash_count += 1
    return trash_count / float(len(text))

########NEW FILE########
__FILENAME__ = control
import time
import logging
from random import randint

from grab.util.py3k_support import *

logger = logging.getLogger('grab.tools.control')

def sleep(lower_limit, upper_limit):
    """
    Sleep for random number of seconds in interval
    between `lower_limit` and `upper_limit`
    """

    # Doing this math calculations
    # to call randint function with integer arugments
    # There is no random function which accepts float arguments
    lower_limit_float = int(lower_limit * 1000)
    upper_limit_float = int(upper_limit * 1000)
    sleep_time = randint(lower_limit_float, upper_limit_float) / 1000.0
    logger.debug('Sleeping for %f seconds' % sleep_time)
    time.sleep(sleep_time)


def repeat(func, limit=3, args=None, kwargs=None,
           fatal_exceptions=(), valid_exceptions=()):
    """
    Return value of execution `func` function.

    In case of error try to execute `func` maximum `limit` times
    and then raise latest exception.

    Example::

        def download(url):
            return urllib.urlopen(url).read()

        data = repeat(download, 3, args=['http://google.com/'])

    """
    for try_count in xrange(1, limit + 1):
        try:
            res = func(*(args or ()), **(kwargs or {}))
        except Exception as ex:
            if isinstance(ex, fatal_exceptions):
                raise
            elif valid_exceptions and not isinstance(ex, valid_exceptions):
                raise
            else:
                logging.error('', exc_info=ex)
                if try_count >= limit:
                    logger.error('Too many errors while executing function %s' % func.__name__)
                    raise
        else:
            return res

########NEW FILE########
__FILENAME__ = debug
import os

from grab.util.py3k_support import *

SCALE = {'kB': 1024.0, 'mB': 1024.0 * 1024.0,
         'KB': 1024.0, 'MB': 1024.0 * 1024.0}

def memory_usage(since=0, render=True, pid=None):
    """
    Return resident memory usage in bytes.
    """

    if pid is None:
        pid = os.getpid()

    proc_status = '/proc/%d/status' % pid
    try:
        status = open(proc_status).read()
    except:
        return 0
    else:
        line = [x for x in status.splitlines() if 'VmRSS:' in x][0]
        items = line.split('VmRSS:')[1].strip().split(' ')
        mem = float(items[0]) * SCALE[items[1]] - since
        if render:
            metrics = ['b', 'Kb', 'Mb', 'Gb']
            metric = metrics.pop(0)
            for x in xrange(3):
                if mem > 1024:
                    mem = mem / 1024.0
                    metric = metrics.pop(0)
            return '%s %s' % (str(round(mem, 2)), metric)
        else:
            return mem

########NEW FILE########
__FILENAME__ = encoding
import re

from grab.util.py3k_support import *

RE_SPECIAL_ENTITY = re.compile(b'&#(1[2-6][0-9]);')

def make_str(value, encoding='utf-8'):
    """
    Normalize unicode/byte string to byte string.
    """

    if isinstance(value, unicode):
        # Convert to string (py2.x) or bytes (py3.x)
        value = value.encode(encoding)
    elif isinstance(value, str):
        pass
    else:
        value = str(value)
    return value


def make_unicode(value, encoding='utf-8'):
    """
    Normalize unicode/byte string to unicode string.
    """

    if not isinstance(value, unicode):
        # Convert to unicode (py2.x and py3.x)
        value = value.decode(encoding)
    return value


def special_entity_handler(match):
    num = int(match.group(1))
    if 128 <= num <= 160:
        try:
            num = unichr(num).encode('utf-8')
            return smart_str('&#%d;' % ord(num.decode('cp1252')[1]))
        except UnicodeDecodeError:
            return match.group(0)
    else:
        return match.group(0)


def fix_special_entities(body):
    return RE_SPECIAL_ENTITY.sub(special_entity_handler, body)


def decode_list(values, encoding='utf-8'):
    if not isinstance(values, list):
        raise TypeError('unsupported values type: %s' % type(values))
    return [smart_unicode(value, encoding) for value in values]


def decode_dict(values, encoding='utf-8'):
    if not isinstance(values, dict):
        raise TypeError('unsupported values type: %s' % type(values))
    return dict(decode_pairs(values.items(), encoding))


def decode_pairs(pairs, encoding='utf-8'):
    def decode(value):
        return smart_unicode(value, encoding)

    return [(decode(pair[0]), decode(pair[1])) for pair in pairs]

# Backward compatibility
smart_str = make_str
smart_unicode = make_unicode

########NEW FILE########
__FILENAME__ = feed
# Copyright: 2012, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
import logging
from hashlib import sha1
from time import mktime
from datetime import datetime
import feedparser
from lxml.html.clean import clean_html

from grab.tools.lxml_tools import truncate_html
from grab.tools.html import strip_tags
from grab.tools.text import remove_bom
from grab.error import DataNotFound, GrabMisuseError

log = logging.getLogger('grab.tools.feed')

def parse_entry_date(entry):
    date_fields = ('published', 'created', 'updated', 'modified')

    for key in date_fields:
        value = getattr(entry, '%s_parsed' % key, None)
        if value:
            return datetime.fromtimestamp(mktime(value))

    raise Exception('Could not parse date of entry %s' % entry.link)


def parse_entry_tags(entry):
    "Return a list of tag objects of the entry"

    tags = set()

    for tag in entry.get('tags', []):
        term = tag.get('label') or tag.get('term') or ''
        for item in term.split(','):
            item = item.strip().lower()
            if item:
                tags.add(item)

    return list(tags)


def parse_entry_content(entry):

    body = ''
    if hasattr(entry, 'content'):
        mapping = dict((x.type, x.value) for x in entry.content)
        if 'text/html' in mapping:
            body = mapping['text/html']
        elif 'application/xhtml+xml' in mapping:
            body = mapping['application/xhtml+xml']
        else:
            body = list(mapping.values())[0]

    if hasattr(entry, 'summary') and len(entry.summary) > len(body):
        body = entry.summary

    if hasattr(entry, 'description') and len(entry.description) > len(body):
        body = entry.description

    return body


def parse_entry_teaser(entry, size):
    content = truncate_html(parse_entry_content(entry), size)
    return content


def build_entry_content(entry, teaser=False, teaser_size=None):
    content = clean_html(parse_entry_content(entry))
    if teaser:
        content = truncate_html(content, teaser_size)
    return content


def parse_feed(grab, teaser_size=1000):
    """
    Extract details of feed fetched with Grab.

    Returns dict with keys:
    * feed
    * entries
    """

    # BOM removing is required because without it
    # sometimes feedparser just raise SegmentationFault o_O
    feed = feedparser.parse(remove_bom(grab.response.body))

    entries = []
    for entry in feed.entries:
        try:
            entries.append(parse_entry(entry, feed, teaser_size=teaser_size))
        except Exception as ex:
            log.error('Entry parsing error', exc_info=ex)
    
    return {'feed': feed, 'entries': entries}


def parse_entry(entry, feed, teaser_size):
    details = {
        'url': entry.link,
        'title': strip_tags(entry.title),
        'content': build_entry_content(entry),
        'teaser': build_entry_content(entry, teaser=True, teaser_size=teaser_size),
        'date': parse_entry_date(entry),
        'tags': parse_entry_tags(entry),
    }

    guid_token = (entry.get('id') or entry.link).encode('utf-8')
    details['guid']  = sha1(guid_token).hexdigest()

    if not details['date']:
        raise Exception('Entry %s does not has publication date' % entry.link)

    return details

########NEW FILE########
__FILENAME__ = files
"""
Miscelanius utilities which are helpful sometime.
"""
import logging
try:
    from urlparse import urlsplit
except ImportError:
    from urllib.parse import urlsplit
from hashlib import sha1
import os
import shutil

def unique_file(path):
    """
    Drop non-unique lines in the file.
    Return number of unique lines.
    """

    lines = set()
    count = 0
    with open(path) as inf:
        for line in inf:
            lines.add(line)
            count += 1
    logging.debug('Read %d lines from %s, unique: %d' % (count, path,
                                                         len(lines)))
    with open(path, 'w') as out:
        out.write(''.join(lines))
    return len(lines)


def unique_host(path):
    """
    Filter out urls with duplicated hostnames.
    """

    hosts = set()
    lines = []
    count = 0
    with open(path) as inf:
        for line in inf:
            host = urlsplit(line).netloc
            if not host in hosts:
                lines.append(line)
                hosts.add(host)
            count += 1
    logging.debug('Read %d lines from %s, unique hosts: %d' % (count, path,
                                                               len(lines)))
    with open(path, 'w') as out:
        out.write(''.join(lines))
    return len(lines)


def hashed_path_details(url, ext='jpg', base_dir=None):
    _hash = sha1(url).hexdigest()
    a, b, tail = _hash[:2], _hash[2:4], _hash[4:]
    directory = '%s/%s' % (a, b)
    if base_dir is not None:
        directory = '%s/%s' % (base_dir, directory)
    if ext is not None:
        filename = '%s.%s' % (tail, ext)
    else:
        filename = tail
    full_path = '%s/%s' % (directory, filename)
    return {'directory': directory,
            'filename': filename,
            'full_path': full_path,
            }


def hashed_path(url, ext='jpg', base_dir=None):
    dtl = hashed_path_details(url, ext=ext, base_dir=base_dir)
    return dtl['full_path']


# Alias for back-ward compatibility
def hash_path(*args, **kwargs):
    logging.debug('This function name is depricated. Please use hashed_path function')
    return hashed_path(*args, **kwargs)


def clear_directory(path):
    """
    Delete recursively all directories and files in
    specified directory.
    """

    for root, dirs, files in os.walk(path):
        for fname in files:
            os.unlink(os.path.join(root, fname))
        for _dir in dirs:
            shutil.rmtree(os.path.join(root, _dir))


# Bad name, not clear logic
#def smart_copy_file(filename, dst_root):
    #dir_path, fname = os.path.split(filename)
    #dst_dir = os.path.join(dst_root, dir_path)
    #if not os.path.exists(dst_dir):
        #os.makedirs(dst_dir)
    #import pdb; pdb.set_trace()
    #shutil.copy(filename, dst_dir)

########NEW FILE########
__FILENAME__ = google
# coding: utf-8
"""
Google parser.

Generic search algorithm:

    With some query:
        For page in 1...9999:
            Build url for given query and page
            Request the url
            If captcha found:
                Solve captcha or change proxy or do something else
            If last page found:
                Stop parsing


Module contents:

* CaptchaError
* ParsingError
* build_search_url
* parse_index_size
* is_last_page
* parse_search_results

"""
try:
    from urllib import quote, unquote_plus
except ImportError:
    from urllib.parse import quote, unquote_plus
import logging
import re
import base64

from grab.tools.html import decode_entities
from grab.tools.lxml_tools import get_node_text, drop_node, render_html
from grab.tools.http import urlencode
from grab.tools.encoding import smart_str
from grab.tools.text import find_number

class CaptchaError(Exception):
    """
    Raised when google fucks you with captcha.
    """


class ParsingError(Exception):
    """
    Raised when some unexpected HTML is found.
    """


def build_search_url(query, page=None, per_page=None, lang=None, filter=None, **kwargs):
    """
    Build google search url with specified query and pagination options.

    :param per_page: 10, 20, 30, 50, 100
    kwargs:
        tbs=qdr:h
        tbs=qdr:d
        tbs=qdr:w
        tbs=qdr:m
        tbs=qdr:y
    """

    if per_page is None:
        per_page = 10
    if page is None:
        page = 1
    if lang is None:
        lang = 'en'
    if filter is None:
        filter = True
    start = per_page * (page - 1)

    if not 'hl' in kwargs:
        kwargs['hl'] = lang
    if not 'num' in kwargs:
        kwargs['num'] = per_page
    if not 'start' in kwargs:
        kwargs['start'] = start
    if not 'filter' in kwargs:
        if not filter:
            kwargs['filter'] = '0'


    url = 'http://google.com/search?q=%s' % quote(smart_str(query))
    if kwargs:
        url += '&' + urlencode(kwargs)
    return url


def parse_index_size(grab):
    """
    Extract number of results from grab instance which
    has received google search results.
    """

    text = None
    if grab.search(u'did not match any documents'):
        return 0
    if len(grab.css_list('#resultStats')):
        text = grab.css_text('#resultStats')
    if len(grab.xpath_list('//div[@id="subform_ctrl"]/div[2]')):
        text = grab.xpath_text('//div[@id="subform_ctrl"]/div[2]')
    if text is None:
        logging.error('Unknown google page format')
        return 0
    text = text.replace(',', '').replace('.', '')
    if 'about' in text:
        number = find_number(text.split('about')[1])
        return int(number)
    elif 'of' in text:
        number = find_number(text.split('of')[1])
        return int(number)
    else:
        number = find_number(text)
        return int(number)


#def search(query, grab=None, limit=None, per_page=None):

    #if not grab:
        #grab = Grab()
    #stop = False
    #count = 0

    #grab.clear_cookies()
    #if grab.proxylist:
        #grab.change_proxy()

    #for page in xrange(1, 9999):
        #if stop:
            #break
        #url = build_search_url(query, page, per_page=per_page)
        #index_size = None
        #grab = google_request(url, grab=grab)

        #count = 0
        #for item in parse_search_results(grab):
            #yield item # {url, title, index_size}
            #count += 1

        #if not count:
            #stop = True

        #if is_last_page(grab):
            #logging.debug('Last page found')
            #stop = True

        #if limit is not None and count >= limit:
            #logging.debug('Limit %d reached' % limit)
            #stop = True

        #grab.sleep(3, 5)


def is_last_page(grab):
    """
    Detect if the fetched page is last page of search results.
    """

    # <td class="b" style="text-align:left"><a href="/search?q=punbb&amp;num=100&amp;hl=ru&amp;prmd=ivns&amp;ei=67DBTs3TJMfpOfrhkcsB&amp;start=100&amp;sa=N" style="text-align:left"><span class="csb ch" style="background-position:-96px 0;width:71px"></span><span style="display:block;margin-left:53px">{NEXT MESSAGE}</span></a></td>

    try:
        #next_link_text = grab.xpath_list('//span[contains(@class, "csb ") and '\
                                         #'contains(@class, " ch")]/..')[-1]\
                             #.text_content().strip()
        next_link = grab.xpath_one('//a[@id="pnnext"]')
    except IndexError:
        logging.debug('No results found')
        return True
    else:
        return False
        #return not len(next_link_text)



def parse_search_results(grab, parse_index_size=False, strict_query=False):
    """
    Parse google search results page content.
    """

    #elif grab.search(u'please type the characters below'):
    if grab.search(u'src="/sorry/image'):

        # Captcha!!!
        raise CaptchaError('Captcha found')

    elif grab.css_exists('#ires'):
        if (strict_query and (
            grab.search(u'  ') or grab.search(u'No results found for'))):
            pass
            logging.debug('Query modified')
        else:
            if len(grab.css_list('#ires h3')):

                # Something was found
                if parse_index_size:
                    index_size = parse_index_size(grab)
                else:
                    index_size = None

                # Yield found results
                results = []

                for elem in grab.xpath_list('//*[h3[@class="r"]/a]'):
                    title_elem = elem.xpath('h3/a')[0]

                    # url
                    url = title_elem.get('href')
                    if url.startswith('/url?'):
                        url = url.split('?q=')[1].split('&')[0]
                        url = unquote_plus(url)

                    # title
                    title = get_node_text(title_elem)

                    # snippet
                    # Google could offer two type of snippet format: simple and extended
                    # It depends on user agent
                    # For <IE8, Opera, <FF3 you probably get simple format
                    try:
                        snippet_node = elem.xpath('div[@class="s"]')[0]
                    except IndexError as ex:
                        # Probably it is video or some other result
                        # Such result type is not supported yet
                        continue

                    try:
                        subnode = snippet_node.xpath('span[@class="st"]')[0]
                        snippet = get_node_text(subnode, smart=False)
                        extended_result = True
                    except IndexError:
                        drop_node(snippet_node, 'div')
                        drop_node(snippet_node, 'span[@class="f"]')
                        snippet = get_node_text(snippet_node, smart=False)
                        extended_result = False

                    # filetype
                    try:
                        filetype = elem.xpath('.//span[contains(@class, "xsm")]'\
                                              '/text()')[0].lower().strip('[]')
                    except IndexError:
                        filetype = None

                    #if 'File Format':
                    if url:
                        results.append({
                            'url': url,
                            'title': title,
                            'snippet': snippet,
                            'filetype': filetype,
                            'index_size': index_size,
                            'extended': extended_result,
                        })
                return results
            else:
                pass
                #return []
    elif grab.css_exists('#res'):
        # Could be search results here?
        # or just message "nothing was found"?
        pass
    else:
        raise ParsingError('Could not identify google page format')

########NEW FILE########
__FILENAME__ = html
# -*- coding: utf-8 -*-
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
import re
try:
    from htmlentitydefs import name2codepoint
except ImportError:
    from html.entities import name2codepoint
import logging

from grab.tools.text import normalize_space as normalize_space_func
from grab.util.py3k_support import *

RE_TAG = re.compile(r'<[^>]+>')
RE_REFRESH_TAG = re.compile(r'<meta[^>]+http-equiv\s*=\s*["\']*Refresh[^>]+', re.I)
# <meta http-equiv='REFRESH' content='0;url= http://www.bk55.ru/mc2/news/article/855'>
RE_REFRESH_URL = re.compile(r'''
    content \s* = \s*
    ["\']* \d+
    (?: ; \s* url \s* = \s*)? ["\']* ([^\'"> ]*)
''', re.I | re.X)

RE_ENTITY = re.compile(r'(&[a-z]+;)')
RE_NUM_ENTITY = re.compile(r'(&#[0-9]+;)')
RE_HEX_ENTITY = re.compile(r'(&#x[a-f0-9]+;)', re.I)
RE_BASE_URL = re.compile(r'<base[^>]+href\s*=["\']*([^\'"> ]+)', re.I)
RE_BR = re.compile(r'<br\s*/?>', re.I)

def decode_entities(html):
    """
    Convert all HTML entities into their unicode
    representations.

    This functions processes following entities:
     * &XXX;
     * &#XXX;

    Example::

        >>> print html.decode_entities('&rarr;ABC&nbsp;&#82;&copy;')
        ABCR
    """


    def process_entity(match):
        entity = match.group(1)
        name = entity[1:-1]
        if name in name2codepoint:
            return unichr(name2codepoint[name])
        else:
            return entity

    
    def process_num_entity(match):
        entity = match.group(1)
        num = entity[2:-1]
        try:
            return unichr(int(num))
        except ValueError:
            return entity

    def process_hex_entity(match):
        entity = match.group(1)
        code = entity[3:-1]
        try:
            return unichr(int(code, 16))
        except ValueError:
            return entity

    html = RE_NUM_ENTITY.sub(process_num_entity, html)
    html = RE_HEX_ENTITY.sub(process_hex_entity, html)
    html = RE_ENTITY.sub(process_entity, html)
    return html


def find_refresh_url(html):
    """
    Find value of redirect url from http-equiv refresh meta tag.
    """

    # We should decode quote values to correctly find
    # the url value
    #html = html.replace('&#39;', '\'')
    #html = html.replace('&#34;', '"').replace('&quot;', '"')
    html = decode_entities(html)

    match = RE_REFRESH_TAG.search(html)
    if match:
        match = RE_REFRESH_URL.search(match.group(0))
        if match:
            return match.group(1)
    return None


def find_base_url(html):
    """
    Find url of <base> tag.
    """

    html = decode_entities(html)

    match = RE_BASE_URL.search(html)
    if match:
        return match.group(1)
    else:
        return None


def strip_tags(html, normalize_space=True, convert_br=False):
    if convert_br:
        html = RE_BR.sub('\n', html)
    text = RE_TAG.sub(' ', html)
    if normalize_space:
        return normalize_space_func(text)
    else:
        return text


def escape(html):
    """
    Returns the given HTML with ampersands, quotes and angle brackets encoded.
    """

    return html.replace('&', '&amp;')\
               .replace('<', '&lt;')\
               .replace('>', '&gt;')\
               .replace('"', '&quot;')\
               .replace("'", '&#39;')

########NEW FILE########
__FILENAME__ = http
try:
    import urllib.parse as urllib
except ImportError:
    import urllib
try:
    from urlparse import urlsplit, urlunsplit
except ImportError:
    from urllib.parse import urlsplit, urlunsplit
import re
import logging

from grab.upload import UploadFile, UploadContent
from grab.error import GrabMisuseError
from grab.tools.encoding import smart_str, smart_unicode, decode_pairs

from grab.util.py3k_support import *

# I do not know, what the hell is going on, but sometimes
# when IDN url should be requested grab fails with error
# LookupError: unknown encoding: punycode
# That happens in grab/base.py near by 347 line on the line::
# kwargs['url'] = normalize_url(kwargs['url'])
# If you try to catch the error with except and import pdb; pdb.set_trace()
# then you'll get "no pdb module" error. WTF??
# But if you import pdb at the top of the module then you can use it
# So.... I import here this module and I hope that will helps
# My idea is that some mystical shit does some thing that breaks python
# environment,, breaks sys.path So, when special case occures and some new module
# is need to be imported then that can't be done due to the unknown magical influence
import encodings.punycode

logger = logging.getLogger('grab.tools.http')
RE_NON_ASCII = re.compile(r'[^-.a-zA-Z0-9]')
RE_NOT_SAFE_URL = re.compile(r'[^-.:/?&;#a-zA-Z0-9]')

def urlencode(*args, **kwargs):
    logger.debug('Method grab.tools.http.urlencode is deprecated. Please use grab.tools.http.smart_urlencode')
    return smart_urlencode(*args, **kwargs)


def smart_urlencode(items, charset='utf-8'):
    """
    Convert sequence of items into bytestring which could be submitted
    in POST or GET request.

    It differs from ``urllib.urlencode`` in that it can process unicode
    and some special values.

    ``items`` could dict or tuple or list.
    """

    if isinstance(items, dict):
        items = items.items()
    return urllib.urlencode(normalize_http_values(items, charset=charset))


def encode_cookies(items, join=True, charset='utf-8'):
    """
    Serialize dict or sequence of two-element items into string suitable
    for sending in Cookie http header.
    """

    def encode(val):
        """
        URL-encode special characters in the text.

        In cookie value only ",", " ", "\t" and ";" should be encoded
        """

        return val.replace(b' ', b'%20').replace(b'\t', b'%09')\
                  .replace(b';', b'%3B').replace(b',', b'%2C')

    if isinstance(items, dict):
        items = items.items()
    items = normalize_http_values(items, charset=charset)

    # py3 hack
    #if PY3K:
    #    items = decode_pairs(items, charset)

    tokens = []
    for key, value in items:
        tokens.append(b'='.join((encode(key), encode(value))))
    if join:
        return b'; '.join(tokens)
    else:
        return tokens


def normalize_http_values(items, charset='utf-8'):
    """
    Accept sequence of (key, value) paris or dict and convert each
    value into bytestring.

    Unicode is converted into bytestring using charset of previous response
    (or utf-8, if no requests were performed)

    None is converted into empty string. 

    Instances of ``UploadContent`` or ``UploadFile`` is converted
    into special pycurl objects.
    """

    if isinstance(items, dict):
        items = items.items()

    def process(item):
        key, value = item

        # normalize value
        if isinstance(value, (UploadContent, UploadFile)):
            value = value.field_tuple()
        elif isinstance(value, unicode):
            value = normalize_unicode(value, charset=charset)
        elif value is None:
            value = ''

        # normalize key
        if isinstance(key, unicode):
            key = normalize_unicode(key, charset=charset)

        return key, value

    items =  list(map(process, items))
    #items = sorted(items, key=lambda x: x[0])
    return items


def normalize_unicode(value, charset='utf-8'):
    """
    Convert unicode into byte-string using detected charset (default or from
    previous response)

    By default, charset from previous response is used to encode unicode into
    byte-string but you can enforce charset with ``charset`` option
    """

    if not isinstance(value, unicode):
        return value
    else:
        #raise GrabMisuseError('normalize_unicode function accepts only unicode values')
        return value.encode(charset, 'ignore')


def quote(data):
    return urllib.quote_plus(smart_str(data))


def normalize_url(url):
    # The idea is to quick check that URL contains only safe chars
    # If whole URL is safe then there is no need to extract hostname part
    # and check if it is IDN
    if RE_NOT_SAFE_URL.search(url):
        parts = list(urlsplit(url))
        if RE_NON_ASCII.search(parts[1]):
            parts[1] = str(smart_unicode(parts[1]).encode('idna').decode())
            url = urlunsplit(parts)
            return url
    return url

def normalize_post_data(data, charset):
    if isinstance(data, basestring):
        # bytes-string should be posted as-is
        # unicode should be converted into byte-string
        if isinstance(data, unicode):
            return normalize_unicode(data, charset)
        else:
            return data
    else:
        # dict, tuple, list should be serialized into byte-string
        return smart_urlencode(data, charset)

########NEW FILE########
__FILENAME__ = internal
import warnings
from functools import wraps
import logging
#from grab.error import GrabMisuseError

# from https://github.com/scrapy/scrapy/blob/master/scrapy/utils/decorator.py
def deprecated(use_instead=None):
    """This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used."""

    def wrapped(func):
        @wraps(func)
        def new_func(*args, **kwargs):
            message = "Call to deprecated function %s." % func.__name__
            if use_instead:
                message += " Use %s instead." % use_instead
            #warnings.warn(message, category=GrabMisuseError, stacklevel=2)
            logging.error(message)
            return func(*args, **kwargs)
        return new_func
    return wrapped

########NEW FILE########
__FILENAME__ = lock
"""
Provide functions for check if file is locked.
"""

import os.path
import sys
import logging
import os

logger = logging.getLogger('grab.tools.lock')
fh = None

def set_lock(fname):
    """
    Try to lock file and write PID.
    
    Return the status of operation.
    """
    
    global fh
    fh = open(fname, 'w')

    if os.name == 'nt':
        # Code for NT systems got from: http://code.activestate.com/recipes/65203/
        
        import win32con
        import win32file
        import pywintypes
        
        LOCK_EX = win32con.LOCKFILE_EXCLUSIVE_LOCK
        LOCK_SH = 0 # the default
        LOCK_NB = win32con.LOCKFILE_FAIL_IMMEDIATELY
        
        # is there any reason not to reuse the following structure?
        __overlapped = pywintypes.OVERLAPPED()
        
        hfile = win32file._get_osfhandle(fh.fileno())
        try:
            win32file.LockFileEx(hfile, LOCK_EX | LOCK_NB, 0, -0x10000, __overlapped)
        except pywintypes.error as exc_value:
            # error: (33, 'LockFileEx', 'The process cannot access 
            # the file because another process has locked a portion
            # of the file.')
            if exc_value[0] == 33:
                return False
    else:
        from fcntl import flock, LOCK_EX, LOCK_NB
        try:
            flock(fh.fileno(), LOCK_EX | LOCK_NB)
        except Exception as ex:
            return False
    
    fh.write(str(os.getpid()))
    fh.flush()
    return True


def assert_lock(fname):
    """
    If file is locked then terminate program else lock file.
    """

    logger.debug('Trying to lock: %s' % fname)
    if not set_lock(fname):
        logger.error(u'%s is already locked. Terminating.' % fname)
        sys.exit()

########NEW FILE########
__FILENAME__ = logs
import logging

def default_logging(grab_log='/tmp/grab.log', level=logging.DEBUG, mode='a',
                    propagate_network_logger=False,
                    network_log='/tmp/grab.network.log'):
    """
    Customize logging output to display all log messages
    except grab network logs.

    Redirect grab network logs into file.
    """

    logging.basicConfig(level=level)

    network_logger = logging.getLogger('grab.network')
    network_logger.propagate = propagate_network_logger
    if network_log:
        hdl = logging.FileHandler(network_log, mode)
        network_logger.addHandler(hdl)
        network_logger.setLevel(level)

    grab_logger = logging.getLogger('grab')
    if grab_log:
        hdl = logging.FileHandler(grab_log, mode)
        grab_logger.addHandler(hdl)
        grab_logger.setLevel(level)


#LOGGING_BUFFER = defaultdict(list)
#GLOBAL = {'key': None}

#class MemoryHandler(logging.Handler):
    #def emit(self, record):
        #LOGGING_BUFFER[GLOBAL['key']].append(self.format(record))


#logger = logging.getLogger('calculate')
#logger.addHandler(MemoryHandler())


#def save_logging(func):
    #def inner(*args, **kwargs):
        #key = str(time.time()) + str(id({}))
        #GLOBAL['key'] = key 
        #try:
            #result = func(*args, **kwargs)
            #result['logs'] = copy(LOGGING_BUFFER[key])
        #finally:
            #if key in LOGGING_BUFFER:
                #del LOGGING_BUFFER[key]
        #return result
    #return inner

########NEW FILE########
__FILENAME__ = lxml_tools
"""
Functions to process content of lxml nodes.
"""
import re

from grab.tools.text import normalize_space as normalize_space_func, find_number
from grab.tools.encoding import smart_str, smart_unicode

from grab.util.py3k_support import *

RE_TAG_START = re.compile(r'<[a-z]')

def get_node_text(node, smart=False, normalize_space=True):
    """
    Extract text content of the `node` and all its descendants.

    In smart mode `get_node_text` insert spaces between <tag><another tag>
    and also ignores content of the script and style tags.

    In non-smart mode this func just return text_content() of node
    with normalized spaces
    """

    # If xpath return a attribute value, it value will be string not a node
    if isinstance(node, basestring):
        if normalize_space:
            node = normalize_space_func(node)
        return node

    if smart:
        value = ' '.join(node.xpath(
            './descendant-or-self::*[name() != "script" and '\
            'name() != "style"]/text()[normalize-space()]'))
    else:
        # If DOM tree was built with lxml.etree.fromstring
        # then tree nodes do not have text_content() method
        try:
            value = node.text_content()
        except AttributeError:
            value = ''.join(node.xpath('.//text()'))
    if normalize_space:
        value = normalize_space_func(value)
    return value

def find_node_number(node, ignore_spaces=False, make_int=True):
    """
    Find number in text content of the `node`.
    """

    text = get_node_text(node)
    return find_number(text, ignore_spaces=ignore_spaces, make_int=make_int)


def truncate_tail(node, xpath):
    """
    Find sub-node by its xpath and remove it and all adjacent nodes following
    after found node.
    """

    subnode = node.xpath(xpath)[0]
    for item in subnode.xpath('following-sibling::*'):
        item.getparent().remove(item)
    subnode.getparent().remove(subnode)


def parse_html(html, encoding='utf-8'):
    """
    Parse html into ElementTree node.
    """
    import lxml.html

    parser = lxml.html.HTMLParser(encoding=encoding)
    return lxml.html.fromstring(html, parser=parser)


def render_html(node, encoding='utf-8', make_unicode=False):
    """
    Render Element node.
    """
    import lxml.html

    if make_unicode or encoding == 'unicode':
        return lxml.html.tostring(node, encoding='utf-8').decode('utf-8')
    else:
        return lxml.html.tostring(node, encoding=encoding)


def truncate_html(html, limit, encoding='utf-8'):
    """
    Truncate html data to specified length and then fix broken tags.
    """

    if not isinstance(html, unicode):
        html = html.decode(encoding)
    truncated_html = html[:limit]
    elem = parse_html(truncated_html, encoding=encoding)
    fixed_html = render_html(elem, encoding=encoding)
    return fixed_html


def clone_node(elem):
    """
    Create clone of Element node.

    The resulted clone is not connected ot original DOM tree.
    """

    return parse_html(render_html(elem))


def disable_links(elem):
    """
    Replace all links with span tags and drop href atrributes.
    """

    for node in elem.xpath('.//a'):
        node.tag = 'span'
        if 'href' in node.attrib:
            del node.attrib['href']


def sanitize_html(html, encoding='utf-8', return_unicode=False):
    html = smart_str(html, encoding=encoding)
    if RE_TAG_START.search(html):
        html = render_html(parse_html(html))
    if return_unicode:
        return html.decode('utf-8')
    else:
        return html


def drop_node(tree, xpath, keep_content=False):
    """
    Find sub-node by its xpath and remove it.
    """

    for node in tree.xpath(xpath):
        parent = node.getparent()
        if keep_content:
            # Find position of node in list of adjacent nodes
            pos = parent.index(node) + 1
            # move all node's childrent to level higher
            for subnode in node:
                parent.insert(pos, subnode)
                pos += 1
            # now replace node with its text
            node_text = (node.text or '') + (node.tail or '')
            replace_rawnode_with_text(node, node_text)
        else:
            replace_rawnode_with_text(node, node.tail or '')




def replace_node_with_text(root, xpath, text):
    for node in root.xpath(xpath):
        new_text = (text + node.tail) if node.tail else text
        replace_rawnode_with_text(node, new_text)


def replace_rawnode_with_text(node, text):
    parent = node.getparent()
    if parent is not None:
        previous = node.getprevious()
        if previous is not None:
            previous.tail = (previous.tail or '') + text
        else:
            parent.text = (parent.text or '') + text
        parent.remove(node)


def clean_html(html, safe_attrs=('src', 'href'),
               input_encoding='unicode',
               output_encoding='unicode',
               **kwargs):
    """
    Fix HTML structure and remove non-allowed attributes from all tags.
    """

    from lxml.html.clean import Cleaner

    # Conver HTML to Unicode
    html = render_html(parse_html(html, encoding=input_encoding), make_unicode=True)

    # Strip some shit with default lxml tools
    cleaner = Cleaner(page_structure=True, **kwargs)
    html = cleaner.clean_html(html)

    # Keep only allowed attributes
    tree = parse_html(html)
    for elem in tree.xpath('./descendant-or-self::*'):
        for key in elem.attrib.keys():
            if safe_attrs:
                if key not in safe_attrs:
                    del elem.attrib[key]

    return render_html(tree, encoding=output_encoding)

########NEW FILE########
__FILENAME__ = metric
# coding: utf-8
KB = 1024
MB = 1024 * KB
GB = MB * 1024


metric_labels = {
    u'mb': MB,
    u'': MB,
    u'kb': KB,
    u'': KB,
    u'gb': GB,
    u'': GB,
} 


def in_unit(num, unit):
    if unit == 'b':
        return num
    elif unit == 'kb':
        return round(num / float(KB), 2)
    elif unit == 'mb':
        return round(num / float(MB), 2)
    elif unit == 'gb':
        return round(num / float(GB), 2)
    else:
        return num


def parse_size(size, unit='b'):
    size = size.lower().strip()
    if size.isdigit():
        return int(size)
    else:
        for anchor, mult in metric_labels.items():
            if anchor in size:
                size = size.replace(anchor, '')
                size = size.replace(',', '.')
                size = size.strip()
                val = int(float(size) * mult)
                return in_unit(val, unit)
        return 0


def format_traffic_value(num):
    if num < KB:
        return '%s B' % in_unit(num, 'b')
    elif num < MB:
        return '%s KB' % in_unit(num, 'kb')
    elif num < GB:
        return '%s MB' % in_unit(num, 'mb')
    else:
        return '%s GB' % in_unit(num, 'gb')

########NEW FILE########
__FILENAME__ = parser
class InvalidMonthName(Exception):
    pass


def parse_int(val):
    if val is None:
        return None
    else:
        return int(val)


def parse_en_month(val):
    names = (None, u'january', u'february', u'march', u'april',
             u'may', u'june', u'july', u'august',
             u'september', u'october', u'november', u'december')
    try:
        return names.index(val.lower())
    except ValueError:
        raise InvalidMonthName(u'Invalid month name: %s' % val)

########NEW FILE########
__FILENAME__ = ping
from grab import Grab
import logging
import os

from grab.tools import html
from grab.tools.pwork import make_work
from grab.tools.encoding import smart_str
from grab.util.py3k_support import *

PING_XML = """<?xml version="1.0"?>
<methodCall>
 	<methodName>weblogUpdates.ping</methodName>
 	<params>
        <param><value>%(name)s</value></param>
        <param><value>%(url)s</value></param>
 	</params>
</methodCall>
"""

SERVER_LIST = """
http://audiorpc.weblogs.com/RPC2
http://blogsearch.google.com.ua/ping/RPC2
http://blogsearch.google.com/ping/RPC2
http://blogsearch.google.ru/ping/RPC2
http://ping.blogs.yandex.ru/RPC2
http://ping.myblog.jp/
http://rpc.weblogs.com/RPC2
http://xping.pubsub.com/ping
""".strip().splitlines()

def ping(name, url, grab, thread_number=10):
    """
    Do XMLRPC ping of given site.
    """
    
    name = smart_str(name)
    url = smart_str(url)

    def worker(rpc_url):
        post = PING_XML % {
            'url': html.escape(url),
            'name': html.escape(name),
        }
        ok = False
        try:
            grab.go(rpc_url, post=post)
        except Exception as ex:
            logging.error(unicode(ex))
        else:
            if not '<boolean>0' in grab.response.body:
                logging.error('%s : FAIL' % rpc_url)
                logging.error(grab.response.body[:1000])
            else:
                ok = True
        return rpc_url, ok

    results = []
    for rpc_url, ok in make_work(worker, SERVER_LIST, thread_number):
        results.append((rpc_url, ok))
    return results


if __name__ == '__main__':
    #logging.basicConfig(level=logging.DEBUG)
    g = Grab(timeout=15)
    g.setup_proxylist('/web/proxy.txt', 'http', auto_change=True) 
    items = ping('seobeginner.ru', 'http://feeds2.feedburner.com/seobeginner',
                 g, thread_number=30)
    print('RESULT:')
    for rpc, ok in items:
        print(rpc, ok)

########NEW FILE########
__FILENAME__ = progress
import sys 
import logging

logger = logging.getLogger('grab.tools.progress')

class Progress(object):
    def __init__(self, step=None, total=None, stop=None, name='items', level=logging.DEBUG):
        if total is None and step is None:
            raise Exception('Both step and total arguments are None')
        if total and not step:
            step = int(total / 20) 
        if step == 0:
            step = total
        self.step = step
        self.count = 0 
        self.total = total
        self.stop = stop
        self.name = name
        self.logging_level = level
    
    def tick(self):
        self.count += 1
        if not self.count % self.step:
            if self.total:
                percents = ' [%d%%]' % int((self.count / float(self.total)) * 100)
            else:
                percents = ''
            logger.log(self.logging_level, 'Processed %d %s%s' % (self.count, self.name, percents))
        if self.count == self.stop:
            logger.log(self.logging_level, 'Reached stop value %d' % self.stop)
            sys.exit()

########NEW FILE########
__FILENAME__ = pwork
from multiprocessing import Process, Queue
import time
try:
    from Queue import Empty
except ImportError:
    from queue import Empty
import logging

from grab.util.py3k_support import *

class Stop(object):
    pass

STOP = Stop()

class Worker(Process):
    def __init__(self, callback, taskq, resultq, ignore_exceptions, *args, **kwargs):
        self.callback = callback
        self.taskq = taskq
        self.resultq = resultq
        self.ignore_exceptions = ignore_exceptions
        Process.__init__(self, *args, **kwargs)

    def run(self):
        while True:
            task = self.taskq.get()
            if isinstance(task, Stop):
                return
            else:
                try:
                    res = self.callback(task)
                    self.resultq.put(res)
                except Exception as ex:
                    if self.ignore_exceptions:
                        logging.error('', exc_info=ex)
                    else:
                        raise


def make_work(callback, tasks, limit, ignore_exceptions=True,
              taskq_size=50):
    """
    Run up to "limit" processes, do tasks and yield results.

    :param callback:  the function that will process single task
    :param tasks:  the sequence or iterator or queue of tasks, each task
        in turn is sequence of arguments, if task is just signle argument
        it should be wrapped into list or tuple
    :param limit: the maximum number of processes
    """
    
    # If tasks is number convert it to the list of number
    if isinstance(tasks, int):
        tasks = xrange(tasks)

    # Ensure that tasks sequence is iterator
    tasks = iter(tasks)    

    taskq = Queue(taskq_size)

    # Here results of task processing will be saved
    resultq = Queue()

    # Prepare and run up to "limit" processes
    processes = []
    for x in xrange(limit):
        process = Worker(callback, taskq, resultq, ignore_exceptions)
        process.daemon = True
        process.start()
        processes.append(process)

    # Put tasks from tasks iterator to taskq queue
    # until tasks iterator ends
    # Do it in separate process
    def task_processor(task_iter, task_queue, limit):
        try:
            for task in task_iter:
                task_queue.put(task)
        finally:
            for x in xrange(limit):
                task_queue.put(STOP)

    processor = Process(target=task_processor, args=[tasks, taskq, limit])
    processor.daemon = True
    processor.start()

    while True:
        try:
            yield resultq.get(True, 0.2)
        except Empty:
            pass
        if not any(x.is_alive() for x in processes):
            break

    while True:
        try:
            yield resultq.get(False)
        except Empty:
            break



if __name__ == '__main__':
    """
    Usage example
    """

    from multiprocessing import current_process
    import logging
    from random import random
    import time

    def worker(arg):
        logging.debug('Processing %s' % arg)
        time.sleep(random())
        return (current_process().name, arg)


    def tasks():
        for x in xrange(3):
            logging.debug('Generating task #%d' % x)
            time.sleep(random())
            yield (x,)


    def main():
        for res in make_work(worker, tasks(), 2):
            logging.debug('Result %s received from process %s' % (res[1], res[0]))


    if __name__ == '__main__':
        logging.basicConfig(level=logging.DEBUG, format='%(processName)s %(message)s')
        main()

########NEW FILE########
__FILENAME__ = rex
import re

from grab.error import DataNotFound
from grab.tools.text import normalize_space
from grab.tools.html import decode_entities
from grab.util.py3k_support import *

REGEXP_CACHE = {}
NULL = object()

def extract_rex_list(rex, body):
    """
    Return found matches.
    """

    return rex.findall(body)


def cache_regexp(rex, flags=0):
    key = (rex, flags)
    try:
        return REGEXP_CACHE[key]
    except KeyError:
        obj = re.compile(rex, flags)
        #obj.source = rex
        REGEXP_CACHE[key] = obj
        return obj


def rex(body, regexp, flags=0, byte=False, default=NULL):
    """
    Search `regexp` expression in `body` text.
    """

    regexp = normalize_regexp(regexp, flags)
    match =  regexp.search(body)
    if match:
        return match
    else:
        if default is NULL:
            raise DataNotFound('Could not find regexp: %s' % regexp)
        else:
            return default


def rex_text(body, regexp, flags=0, default=NULL):
    """
    Search `regexp` expression in `body` text and then strip tags in found result.
    """

    match = rex(body, regexp, flags=flags, default=default)
    try:
        return normalize_space(decode_entities(match.group(1)))
    except AttributeError:
        if default is NULL:
            raise DataNotFound('Regexp not found')
        else:
            return default


def normalize_regexp(regexp, flags=0):
    """
    Accept string or compiled regular expression object.

    Compile string into regular expression object.
    """

    if isinstance(regexp, basestring):
        return cache_regexp(regexp, flags)
    else:
        return regexp


def rex_list(body, rex, flags=0):
    """
    Return found matches.
    """

    rex = normalize_regexp(rex, flags)
    return list(rex.finditer(body))


def rex_text_list(body, rex, flags=0):
    """
    Return found matches with stripped tags.
    """

    items = []
    for match in rex_list(body, rex, flags=flags):
        items.append(normalize_space(decode_entities(match.group(1))))
    return items

########NEW FILE########
__FILENAME__ = russian
# coding: utf-8
from pytils.translit import translify
import re

from grab.tools.encoding import smart_unicode

class InvalidMonthName(Exception):
    pass


RE_NOT_ENCHAR = re.compile(u'[^-a-zA-Z0-9]', re.U)
RE_NOT_ENRUCHAR = re.compile(u'[^-a-zA-Z--0-9]', re.U)

RE_NOT_ENCHAR_DOT = re.compile(u'[^-.a-zA-Z0-9]', re.U)
RE_NOT_ENRUCHAR_DOT = re.compile(u'[^-.a-zA-Z--0-9]', re.U)

RE_DASH = re.compile(r'-+')


def slugify(value, limit=None, default='', lower=True, dot_allowed=False):
    value = smart_unicode(value)

    # Replace all non-allowed chars with "-" char
    # to help pytils not to crash
    if dot_allowed:
        value = RE_NOT_ENRUCHAR_DOT.sub('-', value)
    else:
        value = RE_NOT_ENRUCHAR.sub('-', value)

    # Do transliteration
    value = translify(value)

    # Replace trash with safe "-" char
    if dot_allowed:
        value = RE_NOT_ENCHAR_DOT.sub('-', value)
    else:
        value = RE_NOT_ENCHAR.sub('-', value)
    
    # Replace "-" from both side of the string
    value = value.strip('-')

    if lower:
        value = value.lower()

    # Replace sequences of dashes
    value = RE_DASH.sub('-', value)

    if limit is not None:
        value = value[:limit]

    if value != "":
        return value
    else:
        return default


def parse_ru_month(val):
    names = (None, u'', u'', u'', u'',
             u'', u'', u'', u'',
             u'', u'', u'', u'')
    names2 = (None, u'', u'', u'', u'',
             u'', u'', u'', u'',
             u'', u'', u'', u'')
    names3 = (None, u'', u'', u'', u'',
             u'', u'', u'', u'',
             u'', u'', u'', u'')
    try:
        return names.index(val.lower())
    except ValueError:
        try:
            return names2.index(val.lower())
        except ValueError:
            try:
                return names3.index(val.lower())
            except ValueError:
                raise InvalidMonthName(u'Invalid month name: %s' % val)

########NEW FILE########
__FILENAME__ = selenium_tools
"""
Documentation:
* http://selenium.googlecode.com/svn/trunk/docs/api/py/webdriver/selenium.webdriver.common.action_chains.html
"""
import os
import logging
import time
import shutil
from selenium.webdriver.firefox.firefox_profile import FirefoxProfile
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import WebDriverException
from random import randint

from grab.util.py3k_support import *

def delete_dir(path):
    """
    Delete directory.
    """

    logging.debug('Deleting directory: %s' % path)
    for root, dirs, files in os.walk(path):
        for fname in files:
            os.unlink(os.path.join(root, fname))
        for _dir in dirs:
            shutil.rmtree(os.path.join(root, _dir))
    os.rmdir(path)


#def delete_selenium_profile(path):
    #"""
    #Delete temporary selenium profile directory.
    #"""

    #if os.path.exists(path):
        #if path.rstrip('/').endswith('webdriver-py-profilecopy'):
            #path = os.path.dirname(path)
        #delete_dir(path)


#def remove_old_profiles(tmp_dir, timeout):
    #"""
    #Delete any directory in given `tmp_dir` which looks like
    #firefox profile and which age is more than `timeout`.
    #"""

    #for fname in os.listdir(tmp_dir):
        #path = os.path.join(tmp_dir, fname)
        #age = time.time() - int(os.path.getctime(path))
        #if age > timeout:
            ## if firefox profile
            #if (os.path.exists(os.path.join(path, 'webdriver-py-profilecopy'))
                #or os.path.exists(os.path.join(path, 'prefs.js'))):
                #delete_dir(path)


def create_profile(path=None, user_agent=None, accept_language=None,
                   proxy=None, proxy_type=None, no_proxy_hosts=None,
                   download_directory=None,
                   download_content_type=None):
    """
    @paramDownload_content_type: CSV string
    """

    if path is not None:
        profile = FirefoxProfile(path)
    else:
        profile = FirefoxProfile()

    # Memory and cpu optimization
    profile.set_preference('browser.sessionhistory.max_total_viewers', 0)
    #profile.set_preference('browser.cache.memory.enable', False)
    #profile.set_preference('browser.cache.offline.enable', False)
    #profile.set_preference('browser.cache.disk.enable', False)
    profile.set_preference('browser.safebrowsing.enabled', False)
    profile.set_preference('browser.shell.checkDefaultBrowser', False)
    profile.set_preference('browser.startup.page', 0)
    profile.set_preference('dom.ipc.plugins.enabled.timeoutSecs', 15)
    profile.set_preference('dom.max_script_run_time', 10)
    profile.set_preference('extensions.checkCompatibility', False)
    profile.set_preference('extensions.checkUpdateSecurity', False)
    profile.set_preference('extensions.update.autoUpdateEnabled', False)
    profile.set_preference('extensions.update.enabled', False)
    profile.set_preference('network.http.max-connections-per-server', 30)
    profile.set_preference('network.prefetch-next', False)
    profile.set_preference('plugin.default_plugin_disabled', False)
    profile.set_preference('print.postscript.enabled', False)
    profile.set_preference('toolkit.storage.synchronous', 0)
    profile.set_preference('image.animation_mode', 'none')
    profile.set_preference('images.dither', False)
    profile.set_preference('content.notify.interval', 1000000)
    profile.set_preference('content.switch.treshold', 100000)
    profile.set_preference('nglayout.initialpaint.delay', 1000000)
    profile.set_preference('network.dnscacheentries', 200)
    profile.set_preference('network.dnscacheexpiration', 600)

    if user_agent is not None:
        profile.set_preference("general.useragent.override", user_agent)

    if accept_language is not None:
        profile.set_preference('intl.accept_languages', accept_language)

    if proxy is not None:
        logging.debug('Setting up proxy %s [%s]' % (proxy, proxy_type))
        server, port = proxy.split(':')
        if proxy_type == 'socks5':
            profile.set_preference("network.proxy.socks", server)
            profile.set_preference("network.proxy.socks_port", int(port))
        elif proxy_type == 'http':
            profile.set_preference("network.proxy.http", server)
            profile.set_preference("network.proxy.http_port", int(port))
        else:
            raise Exception('Unkown proxy type: %s' % proxy_type)
        profile.set_preference("network.proxy.type", 1)

    if no_proxy_hosts is not None:
        csv = ', '.join(no_proxy_hosts)
        profile.set_preference('network.proxy.no_proxies_on',
                               'localhost, 127.0.0.1, %s' % csv)


    if download_directory is not None and download_content_type is not None:
        profile.set_preference("browser.download.folderList", 2)
        profile.set_preference("browser.download.manager.showWhenStarting", False)
        profile.set_preference("browser.download.dir", download_directory)
        profile.set_preference("browser.helperApps.neverAsk.saveToDisk",
                               download_content_type)

    profile.update_preferences()
    return profile


def close_alert(browser, times=3):
    """
    Send ENTER keys which should close any alert/prompt/etc UI dialog window.
    By default, send multiple ENTER signals.
    """

    for x in xrange(times):
        try:
            ActionChains(browser).send_keys(Keys.ENTER).perform()
        except WebDriverException as ex:
            if 'Modal dialog present' in str(ex):
                logging.debug('Ignoring exception about modal dialog')
                pass
            else:
                raise


def safe_integer(value, default):
    try:
        int_value = int(value)
    except (ValueError, TypeError) as ex:
        logging.debug('Non-fatal error', exc_info=ex)
        int_value = 0
    if not int_value:
        int_value = default
    return int_value


def mouse_move(browser, sleep=0.1, absolute=False, x=None, y=None):
    """
    Move mouse to random location inside visible viewport.
    """

    # This thing is disabled because it seems selenium do not want to
    # maximize window utill the page is loaded completely
    # It is enabled again because it anyway wait for something to
    # get window size and cooridinates ))
    logging.debug('Maximizing window')
    browser.maximize_window()

    logging.debug('Getting viewport size and offset')
    # View is the visible area of web page

    # Find view size
    view_width = safe_integer(browser.execute_script('return window.innerWidth'), 200)
    view_height = safe_integer(browser.execute_script('return window.innerHeight'), 200)

    # Find view offset
    view_x = safe_integer(browser.execute_script('return window.pageXOffset'), 0)
    view_y = safe_integer(browser.execute_script('return window.pageYOffset'), 0)

    # Find left-top and right-bottom points of the view
    view_ltop = (view_x, view_y)
    view_rbottom = (view_x + view_width, view_y + view_height)
    logging.debug('Viewport coordinates: (%d, %d) - (%d, %d)' % (
        view_ltop[0], view_ltop[1],
        view_rbottom[0], view_rbottom[1]))

    # If x or y is not given then create random coordinates
    if x is None:
        x = randint(0, view_width)
    if y is None:
        y = randint(0, view_height)
    mode = 'absolute' if absolute else 'relative'
    logging.debug('Moving cursor to position (%s): (%d, %d)' % (mode, x, y))

    # If coordinates is not absolute then
    # find absolute position using viewport position
    if not absolute:
        x += view_x
        y += view_y

    # Get HTML document which position probably alwasy is 0
    html_element = browser.find_element_by_xpath('//html')

    # Use move_to_element_with_offset instead move_by_offset
    # because move_by_offset accept relative coordinates
    ActionChains(browser).move_to_element_with_offset(html_element, x, y).perform()

    if sleep:
        time.sleep(sleep)


def click(browser, elem=None):
    """
    Just click at current cursor position.
    If `elem` is geven then move cursor to its location and
    then click on it.
    """

    if elem:
        logging.debug('Moving cursor to element %s' % elem)
        ActionChains(browser).move_to_element(elem).perform()
        logging.debug('Clicking on element %s' % elem)
        elem.click()
    else:
        logging.debug('Clicking at current position')
        ActionChains(browser).click().perform()


def send_keys(browser, *keys):
    """
    Emulate typing on keyboard.
    Any key with length more than one character is treated as 
    attribute of `Keys` class i.e. "PAGE_DOWN" --> Keys.PAGE_DOWN
    """
    for key in keys:
        if len(key) > 1:
            code = getattr(Keys, key)
        else:
            code = key
        ActionChains(browser).send_keys(code).perform()

########NEW FILE########
__FILENAME__ = formyip
class ParsingError(Exception):
    """
    Raised when some unexpected HTML is found.
    """


def build_url():
    return 'http://formyip.com/'


def parse(g):
    """
    Parse HTML reponse from formyip.com website.
    """

    try:
        ip = g.doc.select('//strong').text().split('is ')[1]
        country = g.doc.select('//b[contains(text(), "Your Country")]')\
                   .text().split(':')[1].strip()
        return {'ip': ip, 'country': country}
    except IndexError:
        raise ParsingError('Could not parser formyip.com response')

########NEW FILE########
__FILENAME__ = structured
from grab.util.py3k_support import *

class DotDict(dict):
    def __getattr__(self, item):
        if hasattr(self, item):
            return self[item]

    def __setattr__(self, key, value):
        self[key] = value


class Chunk(object):
    def __init__(self, xpath, apply_func=None, filter_func=None, one=None):
        self._xpath = xpath
        self._one = one if one is not None else False if filter_func else True
        self._filter_func = filter_func
        self._apply_func = apply_func

    def prepare_element(self, element):
        items = element.xpath(self._xpath)
        if not items:
            return
        if self._one:
            items = items[:1]
        elif self._filter_func:
            items = filter(
                lambda item: self._filter_func(item),
                items
            )
        if self._apply_func and items:
            items = map(
                self._apply_func,
                items
            )
        if items and self._one:
            return list(items)[0]
        else:
            return items


class Structure(object):
    def __init__(self, xpath, *args, **kwargs):
        self._xpath = xpath
        self._args = args
        self._kwargs = kwargs

    def __repr__(self):
        return '<%s %s %s>' % (self._xpath, self._args, self._kwargs)


class TreeInterface(object):
    def __init__(self, tree):
        self._tree = tree

    @property
    def tree(self):
        return self._tree

    def xpath(self, path, default=None, all=False):
        items = self.tree.xpath(path)
        if all:
            return items
        try:
            return items[0]
        except IndexError:
            return default

    def structured_xpath(self, xpath='./', *args, **kwargs):
        def parser(element, structure):
            items = []
            for element in element.xpath(structure._xpath):
                item = DotDict()
                for substructure in structure._args:
                    res = parser(element, substructure)
                    if res:
                        item.update(res[0])
                for key, value in structure._kwargs.items():
                    if isinstance(value, basestring):
                        chunk = Chunk(value, apply_func=lambda item: unicode(item).strip())
                        item[key] = chunk.prepare_element(element)
                    if isinstance(value, Structure):
                        item[key] = parser(element, value)
                    elif isinstance(value, (list, tuple, set)):
                        chunk = Chunk(*value)
                        item[key] = chunk.prepare_element(element)
                    elif isinstance(value, Chunk):
                        item[key] = value.prepare_element(element)
                    else:
                        TypeError('Unknown type for structured type!')
                items.append(item)
            return items

        structure = None
        if isinstance(xpath, basestring):
            structure = Structure(xpath, *args, **kwargs)
        elif isinstance(xpath, Structure):
            structure = xpath
        if structure is None:
            Exception('Unknown type for structured type!')

        return parser(
            self.tree,
            structure
        )

########NEW FILE########
__FILENAME__ = system
def check_ares_support():
    import pycurl

    return 'c-ares' in pycurl.version

########NEW FILE########
__FILENAME__ = text
"""
Text parsing and processing utilities.
"""
import re

from grab.error import GrabMisuseError, DataNotFound
from grab.util.py3k_support import *

RE_NUMBER = re.compile(r'\d+')
RE_NUMBER_WITH_SPACES = re.compile(r'\d[\s\d]*', re.U)
RE_SPACE = re.compile(r'\s+', re.U)
BOM_TOKEN = '\xef\xbb\xbf'

def find_number(text, ignore_spaces=False, make_int=True,
                ignore_chars=None):
    """
    Find the number in the `text`.

    :param text: unicode or byte-string text
    :param ignore_spacess: if True then groups of digits delimited
        by spaces are considered as one number
    :raises: :class:`DataNotFound` if number was not found.
    """

    if ignore_chars:
        for char in ignore_chars:
            text = text.replace(char, '')
    if ignore_spaces:
        match = RE_NUMBER_WITH_SPACES.search(text)
    else:
        match = RE_NUMBER.search(text)
    if match:
        val = match.group(0)
        if ignore_spaces:
            val = drop_space(val)
        if make_int:
            val = int(val)
        return val
    else:
        raise DataNotFound


def drop_space(text):
    """
    Drop all space-chars in the `text`.
    """

    return RE_SPACE.sub('', text)


def normalize_space(text, replace=' '):
    """
    Replace sequence of space-chars with one space char.

    Also drop leading and trailing space-chars.
    """

    return RE_SPACE.sub(replace, text.strip()).strip()


def remove_bom(text):
    """
    Remove BOM-sequence from the start of byte string.
    """
    if isinstance(text, unicode):
        raise GrabMisuseError('remove_bom function accepts only byte strings')
    if text.startswith(BOM_TOKEN):
        return text[3:]
    else:
        return text


#def strip_space(text):
    #"""Strip all spaces at begin or end of the text"""

    #return RE_STRIP_SPACE.sub('', text)

########NEW FILE########
__FILENAME__ = clck
try:
    from urllib import quote
except ImportError:
    from urllib.parse import quote

from grab import Grab
from grab.tools.encoding import smart_str

name = 'clck.ru'

def get_url(url):
    g = Grab()
    g.go('http://clck.ru/--?url=%s' % quote(smart_str(url)))
    return g.response.body

########NEW FILE########
__FILENAME__ = tinyurl
"""
tinyurl.com
"""
from grab import Grab

name = 'tinyurl.com'

def get_url(url):
    g = Grab()
    g.go('http://tinyurl.com/')
    g.assert_substring(u'Welcome to TinyURL')
    g.set_input('url', url)
    g.submit()
    g.assert_substring(u'TinyURL was created')
    return g.xpath_text('//td/blockquote[2]/b')

########NEW FILE########
__FILENAME__ = user_agent
from random import choice

# ************
# Firefox disabled as twitter gives strange results for firefox user agents
# ************

#Mozilla/5.0 (compatible; Windows; U; Windows NT 6.2; WOW64; en-US; rv:12.0) Gecko/20120403211507 Firefox/12.0
#Mozilla/5.0 (Macintosh; I; Intel Mac OS X 11_7_9; de-LI; rv:1.9b4) Gecko/2012010317 Firefox/10.0a4
#Mozilla/5.0 (Macintosh; Intel Mac OS X 10.5; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0a2) Gecko/20111101 Firefox/9.0a2
#Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0) Gecko/20100101 Firefox/9.0
#Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Macintosh; PPC MacOS X; rv:5.0) Gecko/20110615 Firefox/5.0
#Mozilla/5.0 (U; Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 5.0; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 5.0; WOW64; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 5.0; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0
#Mozilla/5.0 (Windows NT 5.1; rv:14.0) Gecko/20120405 Firefox/14.0a1
#Mozilla/5.0 (Windows NT 5.1; rv:2.0.1) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 5.1; rv:6.0) Gecko/20100101 Firefox/6.0 FirePHP/0.6
#Mozilla/5.0 (Windows NT 5.1; U; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 5.2; WOW64; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 6.1.1; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 6.1; rv:12.0) Gecko/20120403211507 Firefox/12.0
#Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0
#Mozilla/5.0 (Windows NT 6.1; U; ru; rv:5.0.1.6) Gecko/20110501 Firefox/5.0.1 Firefox/5.0.1
#Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:14.0) Gecko/20120405 Firefox/14.0a1
#Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:5.0) Gecko/20110619 Firefox/5.0
#Mozilla/5.0 (Windows NT 6.1; WOW64; rv:11.0) Gecko Firefox/11.0
#Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110612 Firefox/6.0a2
#Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110613 Firefox/6.0a2
#Mozilla/5.0 (Windows NT 6.2; rv:9.0.1) Gecko/20100101 Firefox/9.0.1
#Mozilla/5.0 (Windows NT 6.2; WOW64; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (X11; FreeBSD amd64; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (X11; Linux AMD64) Gecko Firefox/5.0
#Mozilla/5.0 (X11; Linux) Gecko Firefox/5.0
#Mozilla/5.0 (X11; Linux i686 on x86_64; rv:5.0a2) Gecko/20110524 Firefox/5.0a2
#Mozilla/5.0 (X11; Linux i686; rv:6.0) Gecko/20100101 Firefox/6.0
#Mozilla/5.0 (X11; Linux ppc; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/5.0 (X11; Linux x86_64) Gecko Firefox/5.0
#Mozilla/5.0 (X11; Linux x86_64; rv:5.0) Gecko/20100101 Firefox/5.0 Firefox/5.0
#Mozilla/5.0 (X11; Linux x86_64; rv:5.0) Gecko/20100101 Firefox/5.0 FirePHP/0.5
#Mozilla/5.0 (X11; U; Linux amd64; en-US; rv:5.0) Gecko/20110619 Firefox/5.0
#Mozilla/5.0 (X11; U; Linux amd64; rv:5.0) Gecko/20100101 Firefox/5.0 (Debian)
#Mozilla/5.0 (X11; U; Linux i586; de; rv:5.0) Gecko/20100101 Firefox/5.0
#Mozilla/6.0 (Macintosh; I; Intel Mac OS X 11_7_9; de-LI; rv:1.9b4) Gecko/2012010317 Firefox/10.0a4

USER_AGENT_LIST = """
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6 Chrome 19.0.1084.9
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3
Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3
Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3
Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24
Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.22 (KHTML, like Gecko) Chrome/19.0.1047.0 Safari/535.22
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.21 (KHTML, like Gecko) Chrome/19.0.1042.0 Safari/535.21
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.21 (KHTML, like Gecko) Chrome/19.0.1041.0 Safari/535.21
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/18.6.872.0 Safari/535.2 UNTRUSTED/1.0 3gpp-gba UNTRUSTED/1.0
Mozilla/5.0 (X11; CrOS i686 1660.57.0) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.46 Safari/535.19
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.45 Safari/535.19
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.45 Safari/535.19
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.45 Safari/535.19
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.151 Safari/535.19
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.19 (KHTML, like Gecko) Ubuntu/11.10 Chromium/18.0.1025.142 Chrome/18.0.1025.142 Safari/535.19
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.11 Safari/535.19
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.66 Safari/535.11
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Ubuntu/11.10 Chromium/17.0.963.65 Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Ubuntu/11.04 Chromium/17.0.963.65 Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Ubuntu/10.10 Chromium/17.0.963.65 Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.11 (KHTML, like Gecko) Ubuntu/11.10 Chromium/17.0.963.65 Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; FreeBSD amd64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_4) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.65 Safari/535.11
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Ubuntu/11.04 Chromium/17.0.963.56 Chrome/17.0.963.56 Safari/535.11
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.8 (KHTML, like Gecko) Chrome/17.0.940.0 Safari/535.8
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.77 Safari/535.7ad-imcjapan-syosyaman-xkgi3lqg03!wgz
Mozilla/5.0 (X11; CrOS i686 1193.158.0) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.75 Safari/535.7
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.75 Safari/535.7
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.75 Safari/535.7
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.63 Safari/535.7xs5D9rRDFpg2g
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.8 (KHTML, like Gecko) Chrome/16.0.912.63 Safari/535.8
Mozilla/5.0 (Windows NT 5.2; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.63 Safari/535.7
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.6 (KHTML, like Gecko) Chrome/16.0.897.0 Safari/535.6
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.54 Safari/535.2
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.2 (KHTML, like Gecko) Ubuntu/11.10 Chromium/15.0.874.120 Chrome/15.0.874.120 Safari/535.2
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.872.0 Safari/535.2
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.2 (KHTML, like Gecko) Ubuntu/11.04 Chromium/15.0.871.0 Chrome/15.0.871.0 Safari/535.2
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.864.0 Safari/535.2
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.861.0 Safari/535.2
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.861.0 Safari/535.2
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.861.0 Safari/535.2
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.860.0 Safari/535.2
Chrome/15.0.860.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/15.0.860.0
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.186 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/11.04 Chromium/14.0.825.0 Chrome/14.0.825.0 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.824.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.815.10913 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.815.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/11.04 Chromium/14.0.814.0 Chrome/14.0.814.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.814.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/10.04 Chromium/14.0.813.0 Chrome/14.0.813.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.813.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.813.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.813.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_7) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.813.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.812.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.811.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.810.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.810.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.809.0 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/10.10 Chromium/14.0.808.0 Chrome/14.0.808.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/10.04 Chromium/14.0.808.0 Chrome/14.0.808.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/10.04 Chromium/14.0.804.0 Chrome/14.0.804.0 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/11.04 Chromium/14.0.803.0 Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_7) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.803.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.801.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.801.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.794.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.794.0 Safari/535.1
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.792.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.792.0 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.792.0 Safari/535.1
Mozilla/5.0 (Macintosh; PPC Mac OS X 10_6_7) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.790.0 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_7) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.790.0 Safari/535.1
Mozilla/5.0 (X11; CrOS i686 13.587.48) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.43 Safari/535.1
Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41
Mozilla/5.0 ArchLinux (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Ubuntu/11.04 Chromium/13.0.782.41 Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Windows NT 5.2; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_7) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_3) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_3) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.32 Safari/535.1
Mozilla/5.0 (X11; Linux amd64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.220 Safari/535.1
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.220 Safari/535.1
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.220 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.215 Safari/535.1
Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.215 Safari/535.1
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.215 Safari/535.1
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.215 Safari/535.1
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.20 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.20 Safari/535.1
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.20 Safari/535.1
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.20 Safari/535.1
Mozilla/5.0 (X11; CrOS i686 0.13.587) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.14 Safari/535.1
Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.107 Safari/535.1
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.1 Safari/535.1
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.36 (KHTML, like Gecko) Chrome/13.0.766.0 Safari/534.36
Mozilla/5.0 (X11; Linux amd64) AppleWebKit/534.36 (KHTML, like Gecko) Chrome/13.0.766.0 Safari/534.36
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.35 (KHTML, like Gecko) Ubuntu/10.10 Chromium/13.0.764.0 Chrome/13.0.764.0 Safari/534.35
Mozilla/5.0 (X11; CrOS i686 0.13.507) AppleWebKit/534.35 (KHTML, like Gecko) Chrome/13.0.763.0 Safari/534.35
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.33 (KHTML, like Gecko) Ubuntu/9.10 Chromium/13.0.752.0 Chrome/13.0.752.0 Safari/534.33
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/534.31 (KHTML, like Gecko) Chrome/13.0.748.0 Safari/534.31
Mozilla/5.0 (Windows NT 6.1; en-US) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.750.0 Safari/534.30
Mozilla/5.0 (X11; CrOS i686 12.433.109) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.93 Safari/534.30
Mozilla/5.0 (X11; CrOS i686 12.0.742.91) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.93 Safari/534.30
Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/12.0.742.91
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.91 Chromium/12.0.742.91 Safari/534.30
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.68 Safari/534.30
Mozilla/5.0 ArchLinux (X11; U; Linux x86_64; en-US) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.60 Safari/534.30
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.53 Safari/534.30
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.113 Safari/534.30
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/11.04 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/10.10 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/10.04 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/11.04 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/10.10 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Ubuntu/10.04 Chromium/12.0.742.112 Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (Windows NT 7.1) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (Windows NT 5.2) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (Windows 8) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_6) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_4) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.112 Safari/534.30
Mozilla/5.0 (X11; CrOS i686 12.433.216) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.105 Safari/534.30
Mozilla/5.0 ArchLinux (X11; U; Linux x86_64; en-US) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 ArchLinux (X11; U; Linux x86_64; en-US) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Slackware/Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_4) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.100 Safari/534.30
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.724.100 Safari/534.30
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/534.25 (KHTML, like Gecko) Chrome/12.0.706.0 Safari/534.25
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/534.25 (KHTML, like Gecko) Chrome/12.0.704.0 Safari/534.25
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.24 (KHTML, like Gecko) Ubuntu/10.10 Chromium/12.0.703.0 Chrome/12.0.703.0 Safari/534.24
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Ubuntu/10.10 Chromium/12.0.702.0 Chrome/12.0.702.0 Safari/534.24
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/12.0.702.0 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/12.0.702.0 Safari/534.24
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.700.3 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.699.0 Safari/534.24
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.699.0 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_6) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.698.0 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.697.0 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.71 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.68 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_7) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.68 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_8) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.68 Safari/534.24
Mozilla/5.0 Slackware/13.37 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/11.0.696.50
Mozilla/5.0 (Windows NT 5.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.43 Safari/534.24
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.34 Safari/534.24
Mozilla/5.0 (Windows NT 6.0; WOW64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.34 Safari/534.24
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.3 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.3 Safari/534.24
Mozilla/5.0 (Windows NT 6.0) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.3 Safari/534.24
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.14 Safari/534.24
Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.12 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_6) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.12 Safari/534.24
Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/534.24 (KHTML, like Gecko) Ubuntu/10.04 Chromium/11.0.696.0 Chrome/11.0.696.0 Safari/534.24
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.0 Safari/534.24
Mozilla/5.0 (Windows NT 6.1) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.694.0 Safari/534.24
Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.23 (KHTML, like Gecko) Chrome/11.0.686.3 Safari/534.23
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.21 (KHTML, like Gecko) Chrome/11.0.682.0 Safari/534.21
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.21 (KHTML, like Gecko) Chrome/11.0.678.0 Safari/534.21
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_7_0; en-US) AppleWebKit/534.21 (KHTML, like Gecko) Chrome/11.0.678.0 Safari/534.21
Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20
Mozilla/5.0 (Windows NT) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.669.0 Safari/534.20
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.19 (KHTML, like Gecko) Chrome/11.0.661.0 Safari/534.19
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.18 (KHTML, like Gecko) Chrome/11.0.661.0 Safari/534.18
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; en-US) AppleWebKit/534.18 (KHTML, like Gecko) Chrome/11.0.660.0 Safari/534.18
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/11.0.655.0 Safari/534.17
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/11.0.655.0 Safari/534.17
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/11.0.654.0 Safari/534.17
Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/11.0.652.0 Safari/534.17
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/10.0.649.0 Safari/534.17
Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE) AppleWebKit/534.17 (KHTML, like Gecko) Chrome/10.0.649.0 Safari/534.17
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.82 Safari/534.16
Mozilla/5.0 (X11; U; Linux armv7l; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16
Mozilla/5.0 (X11; U; FreeBSD x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16
Mozilla/5.0 (X11; U; FreeBSD i386; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_5; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.134 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.134 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.134 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.134 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.648.133 Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.648.133 Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_3; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_2; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.648.127 Chrome/10.0.648.127 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.127 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.127 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_8; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.127 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.11 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 6.1; ru-RU; AppleWebKit/534.16; KHTML; like Gecko; Chrome/10.0.648.11;Safari/534.16)
Mozilla/5.0 (Windows; U; Windows NT 6.1; ru-RU) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.11 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.11 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.648.0 Chrome/10.0.648.0 Safari/534.16
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.648.0 Chrome/10.0.648.0 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.0 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.642.0 Chrome/10.0.642.0 Safari/534.16
Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_5; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.639.0 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.638.0 Safari/534.16
Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.634.0 Safari/534.16
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.634.0 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.16 SUSE/10.0.626.0 (KHTML, like Gecko) Chrome/10.0.626.0 Safari/534.16
Mozilla/5.0 (X11; U; Linux x86_64; en-US) AppleWebKit/534.15 (KHTML, like Gecko) Chrome/10.0.613.0 Safari/534.15
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.15 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.613.0 Chrome/10.0.613.0 Safari/534.15
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.15 (KHTML, like Gecko) Ubuntu/10.04 Chromium/10.0.612.3 Chrome/10.0.612.3 Safari/534.15
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.15 (KHTML, like Gecko) Chrome/10.0.612.1 Safari/534.15
Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.15 (KHTML, like Gecko) Ubuntu/10.10 Chromium/10.0.611.0 Chrome/10.0.611.0 Safari/534.15
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.602.0 Safari/534.14
Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14
Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14""".strip().splitlines()

def random_user_agent():
    return choice(USER_AGENT_LIST)

########NEW FILE########
__FILENAME__ = w3lib_encoding
"""
Functions for handling encoding of web pages

Original code is https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py
"""
import re, codecs, encodings

# Python decoder doesn't follow unicode standard when handling
# bad utf-8 encoded strings. see http://bugs.python.org/issue8271
codecs.register_error('w3lib_replace', lambda exc: (u'\ufffd', exc.start+1))

_HEADER_ENCODING_RE = re.compile(r'charset=([\w-]+)', re.I)

# regexp for parsing HTTP meta tags
_TEMPLATE = r'''%s\s*=\s*["']?\s*%s\s*["']?'''
_HTTPEQUIV_RE = _TEMPLATE % ('http-equiv', 'Content-Type')
_CONTENT_RE = _TEMPLATE % ('content', r'(?P<mime>[^;]+);\s*charset=(?P<charset>[\w-]+)')
_CONTENT2_RE = _TEMPLATE % ('charset', r'(?P<charset2>[\w-]+)')
_XML_ENCODING_RE = _TEMPLATE % ('encoding', r'(?P<xmlcharset>[\w-]+)')

# check for meta tags, or xml decl. and stop search if a body tag is encountered
_BODY_ENCODING_RE = re.compile(
    r'<\s*(?:meta(?:(?:\s+%s|\s+%s){2}|\s+%s)|\?xml\s[^>]+%s|body)' % \
        (_HTTPEQUIV_RE, _CONTENT_RE, _CONTENT2_RE, _XML_ENCODING_RE), re.I)

# Default encoding translation
# this maps cannonicalized encodings to target encodings
# see http://www.whatwg.org/specs/web-apps/current-work/multipage/parsing.html#character-encodings-0
# in addition, gb18030 supercedes gb2312 & gbk
# the keys are converted using _c18n_encoding and in sorted order
DEFAULT_ENCODING_TRANSLATION = {
    'ascii': 'cp1252',
    'euc_kr': 'cp949',
    'gb2312': 'gb18030',
    'gbk': 'gb18030',
    'iso8859_11': 'cp874',
    'iso8859_9': 'cp1254',
    'latin_1': 'cp1252',
    'macintosh': 'mac_roman',
    'shift_jis': 'cp932',
    'tis_620': 'cp874',
    'win_1251': 'cp1251',
    'windows_31j': 'cp932',
    'win_31j': 'cp932',
    'windows_874': 'cp874',
    'win_874': 'cp874',
    'x_sjis': 'cp932',
    'zh_cn': 'gb18030'
}

_BOM_TABLE = [
    (codecs.BOM_UTF32_BE, 'utf-32-be'),
    (codecs.BOM_UTF32_LE, 'utf-32-le'),
    (codecs.BOM_UTF16_BE, 'utf-16-be'),
    (codecs.BOM_UTF16_LE, 'utf-16-le'),
    (codecs.BOM_UTF8, 'utf-8')
]
_FIRST_CHARS = set(c[0] for (c, _) in _BOM_TABLE)


def http_content_type_encoding(content_type):
    """Extract the encoding in the content-type header"""
    if content_type:
        match = _HEADER_ENCODING_RE.search(content_type)
        if match:
            return resolve_encoding(match.group(1))


def html_body_declared_encoding(html_body_str):
    """encoding specified in meta tags in the html body, or None if no 
    suitable encoding was found
    """
    # html5 suggests the first 1024 bytes are sufficient, we allow for more
    chunk = html_body_str[:4096]
    match = _BODY_ENCODING_RE.search(chunk)
    if match:
        encoding = match.group('charset') or match.group('charset2') \
                or match.group('xmlcharset')
        if encoding:
            return resolve_encoding(encoding)


def _c18n_encoding(encoding):
    """Cannonicalize an encoding name

    This performs normalization and translates aliases using python's 
    encoding aliases
    """
    normed = encodings.normalize_encoding(encoding).lower()
    return encodings.aliases.aliases.get(normed, normed)


def resolve_encoding(encoding_alias):
    """Return the encoding the given encoding alias maps to, or None if the
    encoding cannot be interpreted
    """
    c18n_encoding = _c18n_encoding(encoding_alias)
    translated = DEFAULT_ENCODING_TRANSLATION.get(c18n_encoding, c18n_encoding)
    try:
        return codecs.lookup(translated).name
    except LookupError:
        return None


def read_bom(data):
    """Read the byte order mark in the text, if present, and 
    return the encoding represented by the BOM and the BOM.

    If no BOM can be detected, (None, None) is returned.
    """
    # common case is no BOM, so this is fast
    if data and data[0] in _FIRST_CHARS:
        for bom, encoding in _BOM_TABLE:
            if data.startswith(bom):
                return encoding, bom
    return None, None


def to_unicode(data_str, encoding):
    """Convert a str object to unicode using the encoding given

    Characters that cannot be converted will be converted to '\ufffd' (the
    unicode replacement character).
    """
    return data_str.decode(encoding, 'w3lib_replace')


def html_to_unicode(content_type_header, html_body_str, 
        default_encoding='utf8', auto_detect_fun=None):
    """Convert raw html bytes to unicode
    
    This attempts to make a reasonable guess at the content encoding of the
    html body, following a similar process as a web browser. 

    It will try in order:
    * http content type header
    * BOM (byte-order mark)
    * meta or xml tag declarations
    * auto-detection, if the `auto_detect_fun` keyword argument is not None
    * default encoding in keyword arg (which defaults to utf8)
    
    If an encoding other than the auto-detected or default encoding is used,
    overrides will be applied, converting some character encodings to more
    suitable alternatives.
    
    If a BOM is found matching the encoding, it will be stripped.
    
    The `auto_detect_fun` argument can be used to pass a function that will
    sniff the encoding of the text. This function must take the raw text as an
    argument and return the name of an encoding that python can process, or
    None.  To use chardet, for example, you can define the function as:
        auto_detect_fun=lambda x: chardet.detect(x).get('encoding')
    or to use UnicodeDammit (shipped with the BeautifulSoup library):
        auto_detect_fun=lambda x: UnicodeDammit(x).originalEncoding

    If the locale of the website or user language preference is known, then a
    better default encoding can be supplied.

    If the content type header is not present, None can be passed signifying
    that the header was not present.

    This method will not fail, if characters cannot be converted to unicode, 
    '\ufffd' (the unicode replacement character) will be inserted instead.

    returns a tuple of (encoding used, unicode)
    """
    enc = http_content_type_encoding(content_type_header)
    bom_enc, bom = read_bom(html_body_str)

    detected_enc = None
    detected_bom = None

    if enc is not None:
        # remove BOM if it agrees with the encoding
        if enc == bom_enc:
            detected_bom = bom
            #html_body_str = html_body_str[len(bom):]
        elif enc == 'utf-16' or enc == 'utf-32':
            # read endianness from BOM, or default to big endian 
            # tools.ietf.org/html/rfc2781 section 4.3
            if bom_enc is not None and bom_enc.startswith(enc):
                enc = bom_enc
                detected_bom = bom
                #html_body_str = html_body_str[len(bom):]
            else:
                enc += '-be'
        detected_enc = enc
        #return enc, to_unicode(html_body_str, enc)
    else:
        if bom_enc is not None:
            detected_enc = bom_enc
            #return bom_enc, to_unicode(html_body_str[len(bom):], bom_enc)
        else:
            enc = html_body_declared_encoding(html_body_str)
            if enc is None and (auto_detect_fun is not None):
                enc = auto_detect_fun(html_body_str)
            if enc is None:
                enc = default_encoding
            detected_enc = enc
            #return enc, to_unicode(html_body_str, enc)
    return detected_enc, bom_found

########NEW FILE########
__FILENAME__ = watch
import os
import signal
import sys
import logging
import time

class Watcher(object):
    """this class solves two problems with multithreaded
    programs in Python, (1) a signal might be delivered
    to any thread (which is just a malfeature) and (2) if
    the thread that gets the signal is waiting, the signal
    is ignored (which is a bug).

    The watcher is a concurrent process (not thread) that
    waits for a signal and the process that contains the
    threads.  See Appendix A of The Little Book of Semaphores.
    http://greenteapress.com/semaphores/

    I have only tested this on Linux.  I would expect it to
    work on the Macintosh and not work on Windows.
    """
    
    def __init__(self):
        """ Creates a child thread, which returns.  The parent
            thread waits for a KeyboardInterrupt and then kills
            the child thread.
        """
        self.child = os.fork()
        if self.child == 0:
            return
        else:
            self.watch()

    def watch(self):
        try:
            os.wait()
        except KeyboardInterrupt:
            logging.debug('Watcher process received KeyboardInterrupt')
            signals = (
                ('SIGUSR2', 1),
                ('SIGTERM', 3),
                ('SIGKILL', 5),
            )
            for sig, sleep_time in signals:
                if not os.path.exists('/proc/%d' % self.child):
                    logging.debug('Process terminated!')
                    break
                else:
                    logging.debug('Sending %s signal to child process' % sig)
                    try:
                        os.kill(self.child, getattr(signal, sig))
                    except OSError:
                        pass
                    logging.debug('Waiting 1 second after sending %s' % sig)
                    time.sleep(sleep_time)
        sys.exit()


def watch():
    Watcher()

########NEW FILE########
__FILENAME__ = work
from threading import Thread, currentThread
import time
try:
    from Queue import Queue, Empty
except ImportError:
    from queue import Queue, Empty
import logging

from grab.util.py3k_support import *

STOP = object()


class Worker(Thread):
    def __init__(self, callback, taskq, resultq, ignore_exceptions, *args, **kwargs):
        self.callback = callback
        self.taskq = taskq
        self.resultq = resultq
        self.ignore_exceptions = ignore_exceptions
        Thread.__init__(self, *args, **kwargs)

    def run(self):
        while True:
            task = self.taskq.get()
            if task is STOP:
                return
            else:
                try:
                    self.resultq.put(self.callback(task))
                except Exception as ex:
                    if self.ignore_exceptions:
                        logging.error('', exc_info=ex)
                    else:
                        raise


def make_work(callback, tasks, limit, ignore_exceptions=True,
              taskq_size=50):
    """
    Run up to "limit" threads, do tasks and yield results.

    :param callback:  the function that will process single task
    :param tasks:  the sequence or iterator or queue of tasks, each task
        in turn is sequence of arguments, if task is just signle argument
        it should be wrapped into list or tuple
    :param limit: the maximum number of threads
    """
    
    # If tasks is number convert it to the list of number
    if isinstance(tasks, int):
        tasks = xrange(tasks)

    # Ensure that tasks sequence is iterator
    tasks = iter(tasks)    

    taskq= Queue(taskq_size)

    # Here results of task processing will be saved
    resultq= Queue()

    # Prepare and run up to "limit" threads
    threads = []
    for x in xrange(limit):
        thread = Worker(callback, taskq, resultq, ignore_exceptions)
        thread.daemon = True
        thread.start()
        threads.append(thread)

    # Put tasks from tasks iterator to taskq queue
    # until tasks iterator ends
    # Do it in separate thread
    def task_processor(task_iter, task_queue, limit):
        try:
            for task in task_iter:
                task_queue.put(task)
        finally:
            for x in xrange(limit):
                task_queue.put(STOP)

    processor = Thread(target=task_processor, args=[tasks, taskq, limit])
    processor.daemon = True
    processor.start()

    while True:
        try:
            yield resultq.get(True, 0.2)
        except Empty:
            pass
        if not any(x.isAlive() for x in threads):
            break

    while True:
        try:
            yield resultq.get(False)
        except Empty:
            break



if __name__ == '__main__':
    """
    Usage example
    """

    from threading import currentThread
    import logging
    from random import random
    import time

    def worker(arg):
        logging.debug('Processing %s' % arg)
        time.sleep(random())
        return (currentThread().name, arg)


    def tasks():
        for x in xrange(10):
            logging.debug('Generating task #%d' % x)
            time.sleep(random())
            yield (x,)


    def main():
        for res in make_work(worker, tasks(), 3):
            logging.debug('Result %s received from thread %s' % (res[1], res[0]))


    if __name__ == '__main__':
        logging.basicConfig(level=logging.DEBUG, format='%(threadName)s %(message)s')
        main()

########NEW FILE########
__FILENAME__ = yandex
# coding: utf-8
try:
    from urllib import quote #, unquote_plus
except ImportError:
    from urllib.parse import quote #, unquote_plus
from grab.tools.lxml_tools import get_node_text
import logging

from grab.tools.encoding import smart_str

class CaptchaError(Exception):
    """
    Raised when yandex shows captcha.
    """


def is_banned(grab):
    if grab.xpath_exists('//input[@class="b-captcha__input"]'):
        return True
    if grab.xpath_text('//title') == '403':
        return True
    return False


def build_search_url(query, page=1, per_page=None, lang='en', filter=True,
                     region=213, **kwargs):
    """
    Build yandex search url with specified query and pagination options.

    :param per_page: 10, 20, 30, 50, 100

    213 region is Moscow
    """

    query = smart_str(query)
    url = 'http://yandex.ru/yandsearch?text=%s&lr=%s' % (
        quote(query), region)
    if kwargs:
        url += '&' + urlencode(kwargs)
    url += '&p=%d' % (page - 1)
    return url


def is_last_page(grab):
    """
    Detect if the fetched page is last page of search results.
    """

    try:
        next_link = grab.xpath_one('//a[contains(@class, "b-pager__next")]')
    except IndexError:
        logging.debug('No results found')
        return True
    else:
        return False


def parse_search_results(grab, parse_index_size=False, strict_query=False):
    """
    Parse yandex search results page content.
    """

    if is_banned(grab):
        raise CaptchaError('Captcha found')

    elif grab.xpath_exists('//div[contains(@class, "b-error")]'):
        err_msg = grab.xpath_text('//div[contains(@class, "b-error")]')
        logging.debug('Found error message: %s' % err_msg)
        return []
    elif grab.xpath_exists('//ol[contains(@class, "b-serp-list")]'):
        # TODO:
        #if (strict_query and (
            #grab.search(u'  ') or grab.search(u'No results found for'))):
            #pass
            #logging.debug('Query modified')
        results = []
        # TODO: parse_index_size
        # Yield found results
        results = []

        page_num = int(grab.xpath_text('//b[contains(@class, "b-pager__current")]'))

        for elem in grab.xpath_list('//li[contains(@class, "b-serp-item")]'):
            try:
                try:
                    title_elem = elem.xpath('.//h2/a')[0]
                    snippet = get_node_text(
                        elem.xpath('.//div[contains(@class, "b-serp-item__text")]')[0])
                except IndexError:
                    # this is video item or something like that
                    pass
                else:
                    item = {
                        'page': page_num,
                    }

                    # url
                    item['url'] = title_elem.get('href')
                    #if url.startswith('/url?'):
                        #url = url.split('?q=')[1].split('&')[0]
                        #url = unquote_plus(url)

                    item['position'] = int(elem.xpath(
                        './/h2/b[contains(@class, "b-serp-item__number")]/text()')[0])

                    # title
                    item['title'] = get_node_text(title_elem)

                    item['snippet'] = snippet

                    results.append(item)
            except Exception as ex:
                logging.error('', exc_info=ex)

        return results
    else:
        print('parsing error')
        raise ParsingError('Could not identify yandex page format')

########NEW FILE########
__FILENAME__ = curl
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
import email
import logging
#import urllib
try:
    from cStringIO import StringIO
except ImportError:
    from io import BytesIO as StringIO
import threading
import random
try:
    from urlparse import urlsplit, urlunsplit
except ImportError:
    from urllib.parse import urlsplit, urlunsplit
import pycurl
import tempfile
import os
import pdb
try:
    from cookielib import CookieJar
except ImportError:
    from http.cookiejar import CookieJar

from grab.cookie import create_cookie, CookieManager
from grab.upload import UploadContent, UploadFile
from grab import error
from grab.response import Response
from grab.tools.http import (encode_cookies, smart_urlencode, normalize_unicode,
                             normalize_http_values, normalize_post_data, normalize_url)
from grab.tools.user_agent import random_user_agent
from grab.tools.encoding import smart_str, smart_unicode, decode_list, decode_pairs
from grab.util.py3k_support import *

logger = logging.getLogger('grab.transport.curl')

# @lorien: I do not understand these signals. Maybe you?

# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see
# the libcurl tutorial for more info.

# http://curl.haxx.se/mail/curlpython-2005-06/0004.html
# http://curl.haxx.se/mail/lib-2010-03/0114.html

#CURLOPT_NOSIGNAL
#Pass a long. If it is 1, libcurl will not use any functions that install
# signal handlers or any functions that cause signals to be sent to the
# process. This option is mainly here to allow multi-threaded unix applications
# to still set/use all timeout options etc, without risking getting signals.
# (Added in 7.10)

#If this option is set and libcurl has been built with the standard name
# resolver, timeouts will not occur while the name resolve takes place.
# Consider building libcurl with c-ares support to enable asynchronous DNS 
# lookups, which enables nice timeouts for name resolves without signals.

try:
    import signal
    from signal import SIGPIPE, SIG_IGN
    try:
        signal.signal(SIGPIPE, SIG_IGN)
    except ValueError:
        # Ignore the exception
        # ValueError: signal only works in main thread
        pass
except ImportError:
    pass


class CurlTransport(object):
    """
    Grab transport layer using pycurl.
    """

    def __init__(self):
        self.curl = pycurl.Curl()

    def setup_body_file(self, storage_dir, storage_filename):
        if storage_filename is None:
            handle, path = tempfile.mkstemp(dir=storage_dir)
            self.body_file = os.fdopen(handle, 'wb')
        else:
            path = os.path.join(storage_dir, storage_filename)
            self.body_file = open(path, 'wb')
        self.body_path = path

    def reset(self):
        self.response_head_chunks = []
        self.response_body_chunks = []
        self.response_body_bytes_read = 0
        self.verbose_logging = False
        self.body_file = None
        self.body_path = None

        # Maybe move to super-class???
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

    def head_processor(self, chunk):
        """
        Process head of response.
        """

        #if self.config['nohead']:
            #return 0
        self.response_head_chunks.append(chunk)
        # Returning None implies that all bytes were written
        return None

    def body_processor(self, chunk):
        """
        Process body of response.
        """

        if self.config_nobody:
            self.curl._callback_interrupted = True
            return 0

        bytes_read = len(chunk)

        self.response_body_bytes_read += bytes_read
        if self.body_file:
            self.body_file.write(chunk)
        else:
            self.response_body_chunks.append(chunk)
        if self.config_body_maxsize is not None:
            if self.response_body_bytes_read > self.config_body_maxsize:
                logger.debug('Response body max size limit reached: %s' %
                             self.config_body_maxsize)
                self.curl._callback_interrupted = True
                return 0

        # Returning None implies that all bytes were written
        return None

    def debug_processor(self, _type, text):
        """
        Process request details.

        0: CURLINFO_TEXT
        1: CURLINFO_HEADER_IN
        2: CURLINFO_HEADER_OUT
        3: CURLINFO_DATA_IN
        4: CURLINFO_DATA_OUT
        5: CURLINFO_unrecognized_type
        """

        if _type == pycurl.INFOTYPE_HEADER_OUT:
            self.request_head += text

        if _type == pycurl.INFOTYPE_DATA_OUT:
            self.request_body += text

        #if _type == pycurl.INFOTYPE_TEXT:
            #if self.request_log is None:
                #self.request_log = ''
            #self.request_log += text

        if self.verbose_logging:
            if _type in (pycurl.INFOTYPE_TEXT, pycurl.INFOTYPE_HEADER_IN,
                         pycurl.INFOTYPE_HEADER_OUT):
                marker_types = {
                    pycurl.INFOTYPE_TEXT: 'i',
                    pycurl.INFOTYPE_HEADER_IN: '<',
                    pycurl.INFOTYPE_HEADER_OUT: '>',
                }
                marker = marker_types[_type]
                logger.debug('%s: %s' % (marker, text.rstrip()))

    def process_config(self, grab):
        """
        Setup curl instance with values from ``self.config``.
        """

        # Copy some config for future usage
        self.config_nobody = grab.config['nobody']
        self.config_body_maxsize = grab.config['body_maxsize']

        try:
            request_url = normalize_url(grab.config['url'])
        except Exception as ex:
            raise error.GrabInvalidUrl(u'%s: %s' % (unicode(ex), grab.config['url']))

        # py3 hack
        if not PY3K:
            request_url = smart_str(request_url)

        self.curl.setopt(pycurl.URL, request_url)

        self.curl.setopt(pycurl.FOLLOWLOCATION, 1 if grab.config['follow_location'] else 0)
        self.curl.setopt(pycurl.MAXREDIRS, grab.config['redirect_limit'])
        self.curl.setopt(pycurl.CONNECTTIMEOUT, grab.config['connect_timeout'])
        self.curl.setopt(pycurl.TIMEOUT, grab.config['timeout'])
        self.curl.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_V4)
        #self.curl.setopt(pycurl.DNS_CACHE_TIMEOUT, 0)
        if not grab.config['connection_reuse']:
            self.curl.setopt(pycurl.FRESH_CONNECT, 1)
            self.curl.setopt(pycurl.FORBID_REUSE, 1)

        self.curl.setopt(pycurl.NOSIGNAL, 1)
        self.curl.setopt(pycurl.HEADERFUNCTION, self.head_processor)

        if grab.config['body_inmemory']:
            self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)
        else:
            if not grab.config['body_storage_dir']:
                raise error.GrabMisuseError('Option body_storage_dir is not defined')
            self.setup_body_file(grab.config['body_storage_dir'],
                                 grab.config['body_storage_filename'])
            self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)

        if grab.config['verbose_logging']:
            self.verbose_logging = True

        # User-Agent
        if grab.config['user_agent'] is None:
            if grab.config['user_agent_file'] is not None:
                with open(grab.config['user_agent_file']) as inf:
                    lines = inf.read().splitlines()
                grab.config['user_agent'] = random.choice(lines)
            else:
                grab.config['user_agent'] = random_user_agent()

        # If value is None then set empty string
        # None is not acceptable because in such case
        # pycurl will set its default user agent "PycURL/x.xx.x"
        if not grab.config['user_agent']:
            grab.config['user_agent'] = ''

        self.curl.setopt(pycurl.USERAGENT, grab.config['user_agent'])

        if grab.config['debug']:
            self.curl.setopt(pycurl.VERBOSE, 1)
            self.curl.setopt(pycurl.DEBUGFUNCTION, self.debug_processor)

        # Ignore SSL errors
        self.curl.setopt(pycurl.SSL_VERIFYPEER, 0)
        self.curl.setopt(pycurl.SSL_VERIFYHOST, 0)
        self.curl.setopt(pycurl.SSLVERSION, pycurl.SSLVERSION_SSLv3)

        if grab.request_method == 'POST':
            self.curl.setopt(pycurl.POST, 1)
            if grab.config['multipart_post']:
                if isinstance(grab.config['multipart_post'], basestring):
                    raise error.GrabMisuseError('multipart_post option could not be a string')
                post_items = normalize_http_values(grab.config['multipart_post'],
                                                   charset=grab.config['charset'])
                # py3 hack
                if PY3K:
                    post_items = decode_pairs(post_items, grab.config['charset'])
                #import pdb; pdb.set_trace()
                self.curl.setopt(pycurl.HTTPPOST, post_items) 
            elif grab.config['post']:
                post_data = normalize_post_data(grab.config['post'], grab.config['charset'])
                # py3 hack
                #if PY3K:
                #    post_data = smart_unicode(post_data, grab.config['charset'])
                self.curl.setopt(pycurl.COPYPOSTFIELDS, post_data)
            else:
                self.curl.setopt(pycurl.POSTFIELDS, '')
        elif grab.request_method == 'PUT':
            data = grab.config['post']
            if isinstance(data, unicode) or (not PY3K and not isinstance(data, basestring)):
                # py3 hack
                #if PY3K:
                #    data = data.encode('utf-8')
                #else:
                raise error.GrabMisuseError('Value of post option could be only '\
                                            'byte string if PUT method is used')
            self.curl.setopt(pycurl.UPLOAD, 1)
            self.curl.setopt(pycurl.READFUNCTION, StringIO(data).read) 
            self.curl.setopt(pycurl.INFILESIZE, len(data))
        elif grab.request_method == 'PATCH':
            data = grab.config['post']
            if isinstance(data, unicode) or not isinstance(data, basestring):
                # py3 hack
                if PY3K:
                    data = data.encode('utf-8')
                else:
                    raise error.GrabMisuseError('Value of post option could be only byte '\
                                                'string if PATCH method is used')
            self.curl.setopt(pycurl.UPLOAD, 1)
            self.curl.setopt(pycurl.CUSTOMREQUEST, 'PATCH')
            self.curl.setopt(pycurl.READFUNCTION, StringIO(data).read) 
            self.curl.setopt(pycurl.INFILESIZE, len(data))
        elif grab.request_method == 'DELETE':
            self.curl.setopt(pycurl.CUSTOMREQUEST, 'delete')
        elif grab.request_method == 'HEAD':
            self.curl.setopt(pycurl.NOBODY, 1)
        elif grab.request_method == 'UPLOAD':
            self.curl.setopt(pycurl.UPLOAD, 1)
        elif grab.request_method == 'GET':
            self.curl.setopt(pycurl.HTTPGET, 1)
        else:
            raise error.GrabMisuseError('Invalid method: %s' % grab.request_method)
        
        headers = grab.config['common_headers']
        if grab.config['headers']:
            headers.update(grab.config['headers'])
        header_tuples = [str('%s: %s' % x) for x\
                         in headers.items()]
        self.curl.setopt(pycurl.HTTPHEADER, header_tuples)

        self.process_cookie_options(grab, request_url)

        if grab.config['referer']:
            self.curl.setopt(pycurl.REFERER, str(grab.config['referer']))

        if grab.config['proxy']:
            self.curl.setopt(pycurl.PROXY, str(grab.config['proxy'])) 
        else:
            self.curl.setopt(pycurl.PROXY, '')

        if grab.config['proxy_userpwd']:
            self.curl.setopt(pycurl.PROXYUSERPWD, str(grab.config['proxy_userpwd']))

        if grab.config['proxy_type']:
            ptype = getattr(pycurl, 'PROXYTYPE_%s' % grab.config['proxy_type'].upper())
            self.curl.setopt(pycurl.PROXYTYPE, ptype)

        if grab.config['encoding']:
            if 'gzip' in grab.config['encoding'] and not 'zlib' in pycurl.version:
                raise error.GrabMisuseError('You can not use gzip encoding because '\
                                      'pycurl was built without zlib support')
            self.curl.setopt(pycurl.ENCODING, grab.config['encoding'])

        if grab.config['userpwd']:
            self.curl.setopt(pycurl.USERPWD, str(grab.config['userpwd']))

        if grab.config.get('interface') is not None:
            self.curl.setopt(pycurl.INTERFACE, grab.config['interface'])

        if grab.config.get('reject_file_size') is not None:
            self.curl.setopt(pycurl.MAXFILESIZE, grab.config['reject_file_size'])

    def process_cookie_options(self, grab, request_url):
        host = urlsplit(request_url).netloc.split(':')[0]
        host_nowww = host
        if host_nowww.startswith('www.'):
            host_nowww = host_nowww[4:]

        # `cookiefile` option should be processed before `cookies` option
        # because `load_cookies` updates `cookies` option
        if grab.config['cookiefile']:
            grab.cookies.load_from_file(grab.config['cookiefile'])

        if grab.config['cookies']:
            if not isinstance(grab.config['cookies'], dict):
                raise error.GrabMisuseError('cookies option shuld be a dict')
            for name, value in grab.config['cookies'].items():
                if '.' in host_nowww:
                    domain = host_nowww
                else:
                    domain = ''
                grab.cookies.set(
                    #name=normalize_unicode(name, grab.config['charset']),
                    #value=normalize_unicode(value, grab.config['charset']),
                    name=name,
                    value=value,
                    domain=domain
                )

        # Erase known cookies stored in pycurl handler
        self.curl.setopt(pycurl.COOKIELIST, 'ALL')

        # Enable pycurl cookie processing mode
        self.curl.setopt(pycurl.COOKIELIST, '')

        # TODO: At this point we should use cookielib magic
        # to pick up cookies for the current requests
        for cookie in grab.cookies.cookiejar:
            cookie_domain = cookie.domain
            if cookie_domain.startswith('#httponly_'):
                cookie_domain = cookie_domain.replace('#httponly_', '')
            if not cookie_domain or host_nowww in cookie_domain:
                if '.' in host_nowww:
                    tail = b'; domain=%s' % cookie_domain
                else:
                    tail = b''
                encoded = encode_cookies({cookie.name: cookie.value}, join=True,
                                         charset=grab.config['charset'])
                self.curl.setopt(pycurl.COOKIELIST, b'Set-Cookie: ' + encoded + tail)

    def request(self):

        try:
            self.curl.perform()
        except pycurl.error as ex:
            # CURLE_WRITE_ERROR (23)
            # An error occurred when writing received data to a local file, or
            # an error was returned to libcurl from a write callback.
            # This exception should be ignored if _callback_interrupted flag
            # is enabled (this happens when nohead or nobody options enabeld)
            #
            # Also this error is raised when curl receives KeyboardInterrupt
            # while it is processing some callback function
            # (WRITEFUNCTION, HEADERFUNCTIO, etc)
            if 23 == ex.args[0]:
                if getattr(self.curl, '_callback_interrupted', None) == True:
                    self.curl._callback_interrupted = False
                else:
                    raise error.GrabNetworkError(ex.args[0], ex.args[1])
            else:
                if ex.args[0] == 28:
                    raise error.GrabTimeoutError(ex.args[0], ex.args[1])
                elif ex.args[0] == 7:
                    raise error.GrabConnectionError(ex.args[0], ex.args[1])
                elif ex.args[0] == 67:
                    raise error.GrabAuthError(ex.args[0], ex.args[1])
                elif ex.args[0] == 47:
                    raise error.GrabTooManyRedirectsError(ex.args[0], ex.args[1])
                else:
                    raise error.GrabNetworkError(ex.args[0], ex.args[1])

    def prepare_response(self, grab):
        # py3 hack
        if PY3K:
            self.response_head_chunks = decode_list(self.response_head_chunks)

        if self.body_file:
            self.body_file.close()
        response = Response()
        response.head = ''.join(self.response_head_chunks)
        if self.body_path:
            response.body_path = self.body_path
        else:
            response.body = b''.join(self.response_body_chunks)

        # Clear memory
        self.response_head_chunks = []
        self.response_body_chunks = []

        response.code = self.curl.getinfo(pycurl.HTTP_CODE)
        response.total_time = self.curl.getinfo(pycurl.TOTAL_TIME)
        response.connect_time = self.curl.getinfo(pycurl.CONNECT_TIME)
        response.name_lookup_time = self.curl.getinfo(pycurl.NAMELOOKUP_TIME)
        response.download_size = self.curl.getinfo(pycurl.SIZE_DOWNLOAD)
        response.upload_size = self.curl.getinfo(pycurl.SIZE_UPLOAD)
        response.download_speed = self.curl.getinfo(pycurl.SPEED_DOWNLOAD)

        response.url = self.curl.getinfo(pycurl.EFFECTIVE_URL)

        if grab.config['document_charset'] is not None:
            response.parse(charset=grab.config['document_charset'])
        else:
            response.parse()

        response.cookies = CookieManager(self.extract_cookiejar())

        # We do not need anymore cookies stored in the
        # curl instance so drop them
        self.curl.setopt(pycurl.COOKIELIST, 'ALL')
        return response

    def extract_cookiejar(self):
        """
        Extract cookies that pycurl instance knows.

        Returns `CookieJar` object.
        """

        # Example of line:
        # www.google.com\tFALSE\t/accounts/\tFALSE\t0\tGoogleAccountsLocale_session\ten
        # Fields:
        # * domain
        # * whether or not all machines under that domain can read the cookie's information.
        # * path
        # * Secure Flag: whether or not a secure connection (HTTPS) is required to read the cookie.
        # * exp. timestamp
        # * name
        # * value
        cookiejar = CookieJar()
        for line in self.curl.getinfo(pycurl.INFO_COOKIELIST):
            values = line.split('\t')
            # old
            #cookies[values[-2]] = values[-1]
            # new
            cookie = create_cookie(
                name=values[5],
                value=values[6],
                domain=values[0],
                path=values[2],
                secure=values[3] == "TRUE",
                expires=int(values[4]) if values[4] else None,
            )
            cookiejar.set_cookie(cookie)
        return cookiejar

    def __getstate__(self):
        """
        Reset curl attribute which could not be pickled.
        """
        state = self.__dict__.copy()
        state['curl'] = None
        return state

    def __setstate__(self, state):
        """
        Create pycurl instance after Grag instance was restored
        from pickled state.
        """

        state['curl'] = pycurl.Curl()
        self.__dict__ = state


#from grab.base import BaseGrab
#class GrabCurl(CurlTransportExtension, BaseGrab):
    #pass

########NEW FILE########
__FILENAME__ = ghost
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
#import email
import logging
#import urllib
#from StringIO import StringIO
#import threading
#import random
#from urlparse import urlsplit, urlunsplit
#import pycurl
from ghost import Ghost

from grab.response import Response
from grab.tools.user_agent import random_user_agent
from grab.util.py3k_support import *

logger = logging.getLogger('grab.transport.ghost')

class GhostTransport(object):
    """
    Grab transport layer using pycurl.
    """

    def __init__(self):
        self.ghost = Ghost()

    def reset(self):
        pass
        #self.response_head_chunks = []
        #self.response_body_chunks = []
        #self.response_body_bytes_read = 0
        #self.verbose_logging = False

        ## Maybe move to super-class???
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

    #def head_processor(self, chunk):
        #"""
        #Process head of response.
        #"""

        ##if self.config['nohead']:
            ##return 0
        #self.response_head_chunks.append(chunk)
        ## Returning None implies that all bytes were written
        #return None

    #def body_processor(self, chunk):
        #"""
        #Process body of response.
        #"""

        #if self.config_nobody:
            #return 0

        #bytes_read = len(chunk)
        #self.response_body_bytes_read += bytes_read
        #self.response_body_chunks.append(chunk)
        #if self.config_body_maxsize is not None:
            #if self.response_body_bytes_read > self.config_body_maxsize:
                #logger.debug('Response body max size limit reached: %s' %
                             #self.config_body_maxsize)
                #return 0

        ## Returning None implies that all bytes were written
        #return None

    #def debug_processor(self, _type, text):
        #"""
        #Process request details.

        #0: CURLINFO_TEXT
        #1: CURLINFO_HEADER_IN
        #2: CURLINFO_HEADER_OUT
        #3: CURLINFO_DATA_IN
        #4: CURLINFO_DATA_OUT
        #5: CURLINFO_unrecognized_type
        #"""

        #if _type == pycurl.INFOTYPE_HEADER_OUT:
            #self.request_head += text

        #if _type == pycurl.INFOTYPE_DATA_OUT:
            #self.request_body += text

        ##if _type == pycurl.INFOTYPE_TEXT:
            ##if self.request_log is None:
                ##self.request_log = ''
            ##self.request_log += text

        #if self.verbose_logging:
            #if _type in (pycurl.INFOTYPE_TEXT, pycurl.INFOTYPE_HEADER_IN,
                         #pycurl.INFOTYPE_HEADER_OUT):
                #marker_types = {
                    #pycurl.INFOTYPE_TEXT: 'i',
                    #pycurl.INFOTYPE_HEADER_IN: '<',
                    #pycurl.INFOTYPE_HEADER_OUT: '>',
                #}
                #marker = marker_types[_type]
                #logger.debug('%s: %s' % (marker, text.rstrip()))

    def process_config(self, grab):
        """
        Setup curl instance with values from ``self.config``.
        """

        # Copy some config for future usage
        #self.config_nobody = grab.config['nobody']
        #self.config_body_maxsize = grab.config['body_maxsize']


        request_url = grab.config['url']
        if isinstance(request_url, unicode):
            request_url = request_url.encode('utf-8')
        #self.curl.setopt(pycurl.URL, request_url)
        self.request_config = {'url': request_url}

        #self.curl.setopt(pycurl.FOLLOWLOCATION, 1 if grab.config['follow_location'] else 0)
        #self.curl.setopt(pycurl.MAXREDIRS, grab.config['redirect_limit'])
        #self.curl.setopt(pycurl.CONNECTTIMEOUT, grab.config['connect_timeout'])
        #self.curl.setopt(pycurl.TIMEOUT, grab.config['timeout'])

        #self.curl.setopt(pycurl.NOSIGNAL, 1)
        #self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)
        #self.curl.setopt(pycurl.HEADERFUNCTION, self.head_processor)

        #if grab.config['verbose_logging']:
            #self.verbose_logging = True

        ## User-Agent
        if grab.config['user_agent'] is None:
            if grab.config['user_agent_file'] is not None:
                with open(grab.config['user_agent_file']) as inf:
                    lines = inf.read().splitlines()
                grab.config['user_agent'] = random.choice(lines)
            else:
                grab.config['user_agent'] = random_user_agent()

        ## If value is None then set empty string
        ## None is not acceptable because in such case
        ## pycurl will set its default user agent "PycURL/x.xx.x"
        if not grab.config['user_agent']:
            grab.config['user_agent'] = ''

        #self.curl.setopt(pycurl.USERAGENT, grab.config['user_agent'])
        self.request_config['user_agent'] = grab.config['user_agent']

        #self.curl.setopt(pycurl.VERBOSE, 1)
        #self.curl.setopt(pycurl.DEBUGFUNCTION, self.debug_processor)

        ## Ignore SSL errors
        #self.curl.setopt(pycurl.SSL_VERIFYPEER, 0)
        #self.curl.setopt(pycurl.SSL_VERIFYHOST, 0)

        #if grab.request_method == 'POST':
            #self.curl.setopt(pycurl.POST, 1)
            #if grab.config['multipart_post']:
                #if isinstance(grab.config['multipart_post'], basestring):
                    #raise error.GrabMisuseError('multipart_post option could not be a string')
                #post_items = normalize_http_values(grab.config['multipart_post'],
                                                   #charset=grab.config['charset'])
                ##import pdb; pdb.set_trace()
                #self.curl.setopt(pycurl.HTTPPOST, post_items) 
            #elif grab.config['post']:
                #if isinstance(grab.config['post'], basestring):
                    ## bytes-string should be posted as-is
                    ## unicode should be converted into byte-string
                    #if isinstance(grab.config['post'], unicode):
                        #post_data = normalize_unicode(grab.config['post'])
                    #else:
                        #post_data = grab.config['post']
                #else:
                    ## dict, tuple, list should be serialized into byte-string
                    #post_data = urlencode(grab.config['post'])
                #self.curl.setopt(pycurl.POSTFIELDS, post_data)
            #else:
                #self.curl.setopt(pycurl.POSTFIELDS, '')
        #elif grab.request_method == 'PUT':
            #self.curl.setopt(pycurl.PUT, 1)
            #self.curl.setopt(pycurl.READFUNCTION, StringIO(grab.config['post']).read) 
        #elif grab.request_method == 'DELETE':
            #self.curl.setopt(pycurl.CUSTOMREQUEST, 'delete')
        #elif grab.request_method == 'HEAD':
            #self.curl.setopt(pycurl.NOBODY, 1)
        #elif grab.request_method == 'UPLOAD':
            #self.curl.setopt(pycurl.UPLOAD, 1)
        #else:
            #self.curl.setopt(pycurl.HTTPGET, 1)
        
        #headers = grab.config['common_headers']
        #if grab.config['headers']:
            #headers.update(grab.config['headers'])
        #header_tuples = [str('%s: %s' % x) for x\
                         #in headers.iteritems()]
        #self.curl.setopt(pycurl.HTTPHEADER, header_tuples)


        ## CURLOPT_COOKIELIST
        ## Pass a char * to a cookie string. Cookie can be either in
        ## Netscape / Mozilla format or just regular HTTP-style
        ## header (Set-Cookie: ...) format.
        ## If cURL cookie engine was not enabled it will enable its cookie
        ## engine.
        ## Passing a magic string "ALL" will erase all cookies known by cURL.
        ## (Added in 7.14.1)
        ## Passing the special string "SESS" will only erase all session
        ## cookies known by cURL. (Added in 7.15.4)
        ## Passing the special string "FLUSH" will write all cookies known by
        ## cURL to the file specified by CURLOPT_COOKIEJAR. (Added in 7.17.1)

        ## CURLOPT_COOKIE
        ## Pass a pointer to a zero terminated string as parameter. It will be used to set a cookie in the http request. The format of the string should be NAME=CONTENTS, where NAME is the cookie name and CONTENTS is what the cookie should contain.
        ## If you need to set multiple cookies, you need to set them all using a single option and thus you need to concatenate them all in one single string. Set multiple cookies in one string like this: "name1=content1; name2=content2;" etc.
        ## Note that this option sets the cookie header explictly in the outgoing request(s). If multiple requests are done due to authentication, followed redirections or similar, they will all get this cookie passed on.
        ## Using this option multiple times will only make the latest string override the previous ones. 

        ## `cookiefile` option shoul be processed before `cookies` option
        ## because `load_cookies` updates `cookies` option
        #if grab.config['cookiefile']:
            #grab.load_cookies(grab.config['cookiefile'])

        #if grab.config['cookies']:
            #if not isinstance(grab.config['cookies'], dict):
                #raise error.GrabMisuseError('cookies option shuld be a dict')
            #items = encode_cookies(grab.config['cookies'], join=False)
            #self.curl.setopt(pycurl.COOKIELIST, 'ALL')
            #for item in items:
                #self.curl.setopt(pycurl.COOKIELIST, 'Set-Cookie: %s' % item)
        #else:
            ## Turn on cookies engine anyway
            ## To correctly support cookies in 302-redirects
            #self.curl.setopt(pycurl.COOKIEFILE, '')

        #if grab.config['referer']:
            #self.curl.setopt(pycurl.REFERER, str(grab.config['referer']))

        #if grab.config['proxy']:
            #self.curl.setopt(pycurl.PROXY, str(grab.config['proxy'])) 
        #else:
            #self.curl.setopt(pycurl.PROXY, '')

        #if grab.config['proxy_userpwd']:
            #self.curl.setopt(pycurl.PROXYUSERPWD, grab.config['proxy_userpwd'])

        ## PROXYTYPE
        ## Pass a long with this option to set type of the proxy. Available options for this are CURLPROXY_HTTP, CURLPROXY_HTTP_1_0 (added in 7.19.4), CURLPROXY_SOCKS4 (added in 7.15.2), CURLPROXY_SOCKS5, CURLPROXY_SOCKS4A (added in 7.18.0) and CURLPROXY_SOCKS5_HOSTNAME (added in 7.18.0). The HTTP type is default. (Added in 7.10) 

        #if grab.config['proxy_type']:
            #ptype = getattr(pycurl, 'PROXYTYPE_%s' % grab.config['proxy_type'].upper())
            #self.curl.setopt(pycurl.PROXYTYPE, ptype)

        #if grab.config['encoding']:
            #if 'gzip' in grab.config['encoding'] and not 'zlib' in pycurl.version:
                #raise error.GrabMisuseError('You can not use gzip encoding because '\
                                      #'pycurl was built without zlib support')
            #self.curl.setopt(pycurl.ENCODING, grab.config['encoding'])

        #if grab.config['userpwd']:
            #self.curl.setopt(pycurl.USERPWD, str(grab.config['userpwd']))

    def request(self):

        self.ghost.user_agent = self.request_config['user_agent']
        items = self.ghost.open(self.request_config['url'])
        self.response_page, self.response_extra = items
                #if ex[0] == 28:
                    #raise error.GrabTimeoutError(ex[0], ex[1])
                #elif ex[0] == 7:
                    #raise error.GrabConnectionError(ex[0], ex[1])
                #elif ex[0] == 67:
                    #raise error.GrabAuthError(ex[0], ex[1])
                #else:
                    #raise error.GrabNetworkError(ex[0], ex[1])

    def prepare_response(self, grab):
        response = Response()
        response.head = ''
        response.body = self.ghost.content.encode('utf-8')
        response.code = self.response_page.http_status
        response.time = 0
        response.url = self.response_page.url

        #if grab.config['document_charset'] is not None:
            #response.parse(charset=grab.config['document_charset'])
        #else:
            #response.parse()
        response.parse(charset='utf-8')

        response.cookies = self.extract_cookies()

        # We do not need anymore cookies stored in the
        # curl instance so drop them
        #self.curl.setopt(pycurl.COOKIELIST, 'ALL')
        return response

    def extract_cookies(self):
        """
        Extract cookies.
        """

        #return self.ghost.cookies
        return {}

    def __getstate__(self):
        """
        Reset curl attribute which could not be pickled.
        """
        state = self.__dict__.copy()
        state['curl'] = None
        return state



#from grab.base import BaseGrab
#class GrabCurl(CurlTransportExtension, BaseGrab):
    #pass

########NEW FILE########
__FILENAME__ = kit
# Copyright: 2013, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
#import email
import logging
#import urllib
#try:
    #from cStringIO import StringIO
#except ImportError:
    #from io import BytesIO as StringIO
#import threading
import random
#try:
    #from urlparse import urlsplit, urlunsplit
#except ImportError:
    #from urllib.parse import urlsplit, urlunsplit
#import pycurl
#import tempfile
#import os.path

from grab.response import Response
from grab.tools.http import (encode_cookies, smart_urlencode, normalize_unicode,
                             normalize_http_values, normalize_post_data)
from grab.tools.user_agent import random_user_agent
from grab.base import Grab
from grab.kit import Kit

logger = logging.getLogger('grab.transport.kit')

class KitTransport(object):
    """
    Grab network transport powered with QtWebKit module
    """

    def __init__(self):
        self.kit = Kit()

    #def setup_body_file(self, storage_dir, storage_filename):
        #if storage_filename is None:
            #handle, path = tempfile.mkstemp(dir=storage_dir)
        #else:
            #path = os.path.join(storage_dir, storage_filename)
        #self.body_file = open(path, 'wb')
        #self.body_path = path

    def reset(self):
        self.request_object = {
            'url': None,
            'cookies': {},
            'method': None,
            'data': None,
            'user_agent': None,
        }
        self.response = None
        #self.response_head_chunks = []
        #self.response_body_chunks = []
        #self.response_body_bytes_read = 0
        #self.verbose_logging = False
        #self.body_file = None
        #self.body_path = None

        ## Maybe move to super-class???
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

    def process_config(self, grab):
        self.request_object['url'] = grab.config['url']
        self.request_object['method'] = grab.request_method.lower()

        if grab.config['cookiefile']:
            grab.load_cookies(grab.config['cookiefile'])

        if grab.config['cookies']:
            if not isinstance(grab.config['cookies'], dict):
                raise error.GrabMisuseError('cookies option shuld be a dict')
            self.request_object['cookies'] = grab.config['cookies']

        if grab.request_method == 'POST':
            if grab.config['multipart_post']:
                raise NotImplementedError
            elif grab.config['post']:
                post_data = normalize_post_data(grab.config['post'], grab.config['charset'])
            else:
                post_data = None
            self.request_object['data'] = post_data

        if grab.config['user_agent'] is None:
            if grab.config['user_agent_file'] is not None:
                with open(grab.config['user_agent_file']) as inf:
                    lines = inf.read().splitlines()
                grab.config['user_agent'] = random.choice(lines)
            else:
                pass
                # I think that it does not make sense
                # to create random user agents for webkit transport
                #grab.config['user_agent'] = random_user_agent()
        self.request_object['user_agent'] = grab.config['user_agent']

    def request(self):
        req = self.request_object
        self.kit_response = self.kit.request(
            url=req['url'],
            cookies=req['cookies'],
            method=req['method'],
            data=req['data'],
            user_agent=req['user_agent'],
        )

    def prepare_response(self, grab):
        return self.kit_response

    def extract_cookies(self):
        """
        Extract cookies.
        """

        return self.kit_response.cookies

    def __getstate__(self):
        """
        Reset curl attribute which could not be pickled.
        """
        state = self.__dict__.copy()
        state['kit'] = None
        return state

    def __setstate__(self, state):
        """
        Create pycurl instance after Grag instance was restored
        from pickled state.
        """

        state['kit'] = Kit()
        self.__dict__ = state


class GrabKit(Grab):
    def __init__(self, response_body=None, transport='grab.transport.curl.CurlTransport',
                 **kwargs):
        super(GrabKit, self).__init__(response_body=response_body,
                                       transport='grab.transport.kit.KitTransport',
                                       **kwargs)

########NEW FILE########
__FILENAME__ = mock
# Copyright: 2013, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
from datetime import datetime
import logging
try:
    from cookielib import CookieJar
except ImportError:
    from http.cookiejar import CookieJar

from grab.base import Grab
from grab.response import Response
from grab.error import GrabNetworkError
from grab.cookie import CookieManager

class GrabMockNotFoundError(GrabNetworkError):
    """
    Raised when MOCK_REGISTRY does not have required URL.
    """

logger = logging.getLogger('grab.transport.mock')

MOCK_REGISTRY = {
}

class MockTransport(object):
    """
    Grab transport layer using pycurl.
    """

    #def __init__(self):
        #self.curl = pycurl.Curl()

    def reset(self):
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

    def process_config(self, grab):
        """
        Setup curl instance with values from ``self.config``.
        """

        self.request_url = grab.config['url']

    def request(self):
        pass

    def prepare_response(self, grab):
        response = Response()
        
        try:
            response.body = MOCK_REGISTRY[self.request_url]['body']
        except KeyError:
            raise GrabMockNotFoundError(
                'Mock registry does not have information about '\
                'following URL: %s' % self.request_url)

        now_str = datetime.now().strftime('%a, %d %B %Y %H:%M:%S')
        response.head = '\r\n'.join((
            'Accept-Ranges:bytes',
            'Content-Length:%d' % len(response.body),
            'Content-Type:text/plain',
            'Date:%s GMT' % now_str,
            'Last-Modified:%s GMT' % now_str,
            'Vary:Accept-Encoding',
        ))

        response.code = 200
        response.total_time = 0
        response.name_lookup_time = 0
        response.connect_time = 0
        response.url = self.request_url
        response.parse()
        response.cookies = CookieManager(self.extract_cookiejar())

        return response

    def extract_cookiejar(self):
        return CookieJar()


class GrabMock(Grab):
    def __init__(self, document_body=None, transport='grab.transport.curl.CurlTransport',
                 **kwargs):
        super(GrabMock, self).__init__(document_body=document_body,
                                       transport='grab.transport.mock.MockTransport',
                                       **kwargs)

########NEW FILE########
__FILENAME__ = requests
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
import email
import logging
#import urllib
#try:
#    from StringIO import StringIO
#except ImportError:
#    from io import StringIO
import threading
import random
import requests 

from grab.error import GrabError, GrabMisuseError
from grab.base import UploadContent, UploadFile
from grab.response import Response
from grab.tools.http import urlencode, normalize_http_values, normalize_unicode
from grab.util.py3k_support import *

logger = logging.getLogger('grab.transport.requests')

class RequestsTransport(object):
    def __init__(self):
        self.session = requests.session()

    def reset(self):
        # TODO: WTF???
        # Maybe move to super-class???
        self.request_headers = ''
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

        #self.request_method = None
        self.requests_config = None

    #def head_processor(self, chunk):
        #"""
        #Process head of response.
        #"""

        #if grab.config['nohead']:
            #return 0
        #self.response_head_chunks.append(chunk)
        ## Returning None implies that all bytes were written
        #return None

    #def body_processor(self, chunk):
        #"""
        #Process body of response.
        #"""

        #if grab.config['nobody']:
            #return 0
        #self.response_body_chunks.append(chunk)
        ## Returning None implies that all bytes were written
        #return None

    #def debug_processor(self, _type, text):
        #"""
        #Parse request headers and save to ``self.request_headers``

        #0: CURLINFO_TEXT
        #1: CURLINFO_HEADER_IN
        #2: CURLINFO_HEADER_OUT
        #3: CURLINFO_DATA_IN
        #4: CURLINFO_DATA_OUT
        #5: CURLINFO_unrecognized_type
        #"""

        #if _type == pycurl.INFOTYPE_HEADER_OUT:
            #self.request_head += text
            #lines = text.splitlines()
            #text = '\n'.join(lines[1:])
            #self.request_headers = dict(email.message_from_string(text))

        #if _type == pycurl.INFOTYPE_DATA_OUT:
            #self.request_body += text

        #if _type == pycurl.INFOTYPE_TEXT:
            #if self.request_log is None:
                #self.request_log = ''
            #self.request_log += text

    def process_config(self, grab):
        """
        Setup curl instance with values from ``grab.config``.
        """
        
        # Accumulate all request options into `self.requests_config`
        self.requests_config = {
            'headers': {},
            'payload': None,
            'cookies': None,
            'proxy': None,
        }

        if isinstance(grab.config['url'], unicode):
            grab.config['url'] = grab.config['url'].encode('utf-8')

        self.requests_config['url'] = grab.config['url']

        #self.curl.setopt(pycurl.URL, url)
        #self.curl.setopt(pycurl.FOLLOWLOCATION, 1)
        #self.curl.setopt(pycurl.MAXREDIRS, 5)
        #self.curl.setopt(pycurl.CONNECTTIMEOUT, grab.config['connect_timeout'])
        #self.curl.setopt(pycurl.TIMEOUT, grab.config['timeout'])
        #self.curl.setopt(pycurl.NOSIGNAL, 1)
        #self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)
        #self.curl.setopt(pycurl.HEADERFUNCTION, self.head_processor)

        # User-Agent
        # TODO: move to base class
        if grab.config['user_agent'] is None:
            if grab.config['user_agent_file'] is not None:
                lines = open(grab.config['user_agent_file']).read().splitlines()
                grab.config['user_agent'] = random.choice(lines)

        # If value is None then set empty string
        # None is not acceptable because in such case
        # pycurl will set its default user agent "PycURL/x.xx.x"
        # For consistency we send empty User-Agent in case of None value
        # in all other transports too
        if not grab.config['user_agent']:
            grab.config['user_agent'] = ''
        self.requests_config['headers']['User-Agent'] = grab.config['user_agent']

        #if grab.config['debug']:
            #self.curl.setopt(pycurl.VERBOSE, 1)
            #self.curl.setopt(pycurl.DEBUGFUNCTION, self.debug_processor)

        ## Ignore SSL errors
        #self.curl.setopt(pycurl.SSL_VERIFYPEER, 0)
        #self.curl.setopt(pycurl.SSL_VERIFYHOST, 0)

        self.requests_config['method'] = grab.request_method.lower()

        if grab.request_method == 'POST' or grab.request_method == 'PUT':
            if grab.config['multipart_post']:
                raise NotImplementedError
                #if isinstance(grab.config['multipart_post'], basestring):
                    #raise GrabMisuseError('multipart_post option could not be a string')
                #post_items = normalize_http_values(grab.config['multipart_post'],
                                                    #charset=grab.config['charset'])
                #self.curl.setopt(pycurl.HTTPPOST, post_items) 
            elif grab.config['post']:
                if isinstance(grab.config['post'], basestring):
                    # bytes-string should be posted as-is
                    # unicode should be converted into byte-string
                    if isinstance(grab.config['post'], unicode):
                        post_data = normalize_unicode(grab.config['post'])
                    else:
                        post_data = grab.config['post']
                else:
                    # dict, tuple, list should be serialized into byte-string
                    post_data = urlencode(grab.config['post'])
                self.requests_config['payload'] = post_data
                #self.curl.setopt(pycurl.POSTFIELDS, post_data)
        #elif grab.request_method == 'PUT':
            #self.curl.setopt(pycurl.PUT, 1)
            #self.curl.setopt(pycurl.READFUNCTION, StringIO(grab.config['post']).read) 
        elif grab.request_method == 'DELETE':
            pass
            #self.curl.setopt(pycurl.CUSTOMREQUEST, 'delete')
        elif grab.request_method == 'HEAD':
            pass
            #self.curl.setopt(pycurl.NOBODY, 1)
        else:
            pass
            #self.curl.setopt(pycurl.HTTPGET, 1)

        
        headers = grab.config['common_headers']
        if grab.config['headers']:
            headers.update(grab.config['headers'])
        #header_tuples = [str('%s: %s' % x) for x\
                         #in headers.iteritems()]
        #self.curl.setopt(pycurl.HTTPHEADER, header_tuples)
        self.requests_config['headers'].update(headers)

        # `cookiefile` option shoul be processed before `cookies` option
        # because `load_cookies` updates `cookies` option
        if grab.config['cookiefile']:
            grab.load_cookies(grab.config['cookiefile'])

        if grab.config['cookies']:
            items = normalize_http_values(grab.config['cookies'])
            self.requests_config['cookies'] = dict(items)

        #if not grab.config['reuse_cookies'] and not grab.config['cookies']:
            #self.curl.setopt(pycurl.COOKIELIST, 'ALL')

        #if grab.config['referer']:
            #self.curl.setopt(pycurl.REFERER, str(grab.config['referer']))

        #if grab.config['proxy']:
            #self.curl.setopt(pycurl.PROXY, str(grab.config['proxy'])) 
        #else:
            #self.curl.setopt(pycurl.PROXY, '')

        #if grab.config['proxy_userpwd']:
            #self.curl.setopt(pycurl.PROXYUSERPWD, grab.config['proxy_userpwd'])

        if grab.config['proxy']:
            self.requests_config['proxy'] = grab.config['proxy']

        if grab.config['proxy_userpwd']:
            raise GrabMisuseError('requests transport does not support proxy authentication')

        if grab.config['proxy_type']:
            if grab.config['proxy_type'] != 'http':
                raise GrabMisuseError('requests transport supports only proxies of http type')

        #if grab.config['encoding']:
            #self.curl.setopt(pycurl.ENCODING, grab.config['encoding'])

        #if grab.config['userpwd']:
            #self.curl.setopt(pycurl.USERPWD, grab.config['userpwd'])

    #def _extract_cookies(self):
        #"""
        #Extract cookies.
        #"""

        ## Example of line:
        ## www.google.com\tFALSE\t/accounts/\tFALSE\t0\tGoogleAccountsLocale_session\ten
        #cookies = {}
        #for line in self.curl.getinfo(pycurl.INFO_COOKIELIST):
            #chunks = line.split('\t')
            #cookies[chunks[-2]] = chunks[-1]
        #return cookies


    def request(self):
        try:
            cfg = self.requests_config
            func = getattr(requests, cfg['method'])
            kwargs = {}
            if cfg['payload'] is not None:
                kwargs['data'] = cfg['payload']
            if cfg['cookies'] is not None:
                kwargs['cookies'] = cfg['cookies']
            if cfg['proxy'] is not None:
                kwargs['proxies'] = {'http': cfg['proxy'],
                                     'https': cfg['proxy']}
            self._requests_response = func(
                cfg['url'], headers=cfg['headers'], **kwargs)
        except Exception as ex:
            raise GrabError(0, unicode(ex))

    def prepare_response(self, grab):
        #self.response.head = ''.join(self.response_head_chunks)
        #self.response.body = ''.join(self.response_body_chunks)
        #self.response.parse()
        #self.response.cookies = self._extract_cookies()
        #self.response.code = self.curl.getinfo(pycurl.HTTP_CODE)
        #self.response.time = self.curl.getinfo(pycurl.TOTAL_TIME)
        #self.response.url = self.curl.getinfo(pycurl.EFFECTIVE_URL)
        response = Response()
        response.head = ''
        #if grab.config['body_max_size'] is not None:
            #chunks = []
            #read_size = 0
            #for chunk in self._requests_responsek
        #else:
            #response.body = self._requests_response.content

        response.body = self._requests_response.content
        response.code = self._requests_response.status_code
        response.headers = self._requests_response.headers
        response.cookies = self._requests_response.cookies or {}
        response.url = grab.config['url']

        if grab.config['charset'] is not None:
            response.parse(charset=grab.config['charset'])
        else:
            response.parse()
        return response

    #def load_cookies(self, path):
        #"""
        #Load cookies from the file.

        #The cookie data may be in Netscape / Mozilla cookie data format or just regular HTTP-style headers dumped to a file.
        #"""

        #self.curl.setopt(pycurl.COOKIEFILE, path)


    #def dump_cookies(self, path):
        #"""
        #Dump all cookies to file.

        #Each cookie is dumped in the format:
        ## www.google.com\tFALSE\t/accounts/\tFALSE\t0\tGoogleAccountsLocale_session\ten
        #"""

        #open(path, 'w').write('\n'.join(self.curl.getinfo(pycurl.INFO_COOKIELIST)))

    #def clear_cookies(self):
        #"""
        #Clear all cookies.
        #"""

        ## Write tests
        #self.curl.setopt(pycurl.COOKIELIST, 'ALL')
        #grab.config['cookies'] = {}
        #self.response.cookies = None

    #def reset_curl_instance(self):
        #"""
        #Completely recreate curl instance from scratch.
        
        #I add this method because I am not sure that
        #``clear_cookies`` method works fine and I should be sure
        #I can reset all cokies.
        #"""

        #self.curl = pycurl.Curl()


#from grab.base import BaseGrab
#class GrabRequests(RequestsTransportExtension, BaseGrab):
    #pass

########NEW FILE########
__FILENAME__ = selenium
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
import email
import logging
#import urllib
#try:
#    from StringIO import StringIO
#except ImportError:
#    from io import StringIO
import threading
import random
import time
import os
import json

from grab.response import Response
from grab.error import GrabError, GrabMisuseError
from grab.base import UploadContent, UploadFile
from grab.util.py3k_support import *

logger = logging.getLogger('grab')


"""
dir(self.browser)

['__class__',
 '__delattr__',
 '__dict__',
 '__doc__',
 '__format__',
 '__getattribute__',
 '__hash__',
 '__init__',
 '__module__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_unwrap_value',
 '_wrap_value',
 'add_cookie',
 'back',
 'binary',
 'capabilities',
 'close',
 'command_executor',
 'create_web_element',
 'current_url',
 'current_window_handle',
 'delete_all_cookies',
 'delete_cookie',
 'desired_capabilities',
 'error_handler',
 'execute',
 'execute_async_script',
 'execute_script',
 'find_element',
 'find_element_by_class_name',
 'find_element_by_css_selector',
 'find_element_by_id',
 'find_element_by_link_text',
 'find_element_by_name',
 'find_element_by_partial_link_text',
 'find_element_by_tag_name',
 'find_element_by_xpath',
 'find_elements',
 'find_elements_by_class_name',
 'find_elements_by_css_selector',
 'find_elements_by_id',
 'find_elements_by_link_text',
 'find_elements_by_name',
 'find_elements_by_partial_link_text',
 'find_elements_by_tag_name',
 'find_elements_by_xpath',
 'firefox_profile',
 'forward',
 'get',
 'get_cookie',
 'get_cookies',
 'get_screenshot_as_base64',
 'get_screenshot_as_file',
 'get_window_position',
 'get_window_size',
 'implicitly_wait',
 'name',
 'orientation',
 'page_source',
 'profile',
 'quit',
 'refresh',
 'save_screenshot',
 'session_id',
 'set_script_timeout',
 'set_window_position',
 'set_window_size',
 'start_client',
 'start_session',
 'stop_client',
 'switch_to_active_element',
 'switch_to_alert',
 'switch_to_default_content',
 'switch_to_frame',
 'switch_to_window',
 'title',
 'window_handles']
"""


class SeleniumTransportExtension(object):
    def extra_config(self):
        self.config['xserver_display'] = 0

    #def extra_init(self):
        #pass

    #def extra_reset(self):
        #self.response_head_chunks = []
        #self.response_body_chunks = []
        #self.request_headers = ''
        #self.request_head = ''
        #self.request_log = ''
        #self.request_body = ''

    #def head_processor(self, chunk):
        #"""
        #Process head of response.
        #"""

        #if self.config['nohead']:
            #return 0
        #self.response_head_chunks.append(chunk)
        ## Returning None implies that all bytes were written
        #return None

    #def body_processor(self, chunk):
        #"""
        #Process body of response.
        #"""

        #if self.config['nobody']:
            #return 0
        #self.response_body_chunks.append(chunk)
        ## Returning None implies that all bytes were written
        #return None

    #def debug_processor(self, _type, text):
        #"""
        #Parse request headers and save to ``self.request_headers``

        #0: CURLINFO_TEXT
        #1: CURLINFO_HEADER_IN
        #2: CURLINFO_HEADER_OUT
        #3: CURLINFO_DATA_IN
        #4: CURLINFO_DATA_OUT
        #5: CURLINFO_unrecognized_type
        #"""

        #if _type == pycurl.INFOTYPE_HEADER_OUT:
            #self.request_head += text
            #lines = text.splitlines()
            #text = '\n'.join(lines[1:])
            #self.request_headers = dict(email.message_from_string(text))

        #if _type == pycurl.INFOTYPE_DATA_OUT:
            #self.request_body += text

        #if _type == pycurl.INFOTYPE_TEXT:
            #if self.request_log is None:
                #self.request_log = ''
            #self.request_log += text

    def process_config(self, grab):
        """
        Setup curl instance with values from ``self.config``.
        """

        from selenium import webdriver

        display = ':%s.0' % grab.config.get('xserver_display')
        logging.debug('Setting DISPLAY env to %s' % display)
        os.environ['DISPLAY'] = display

        driver = grab.config.get('webdriver', 'firefox')

        if driver.lower() == 'chrome':
            self.browser = webdriver.Chrome()
        elif driver.lower() == 'opera':
            self.browser = webdriver.Opera()
        elif driver.lower() == 'ie':
            self.browser = webdriver.Ie()
        else:
            self.browser = webdriver.Firefox()

        if grab.config['method']:
            method = grab.config['method'].lower()
        else:
            method = 'get'

        self.config = {
            'url': grab.config['url'],
            'wait': grab.config['selenium_wait'],
            'method': method,
            'post': grab.config.get('post', {})
        }

        #url = self.config['url']
        if isinstance(grab.config['url'], unicode):
            #url = url.encode('utf-8')
            grab.config['url'] = grab.config['url'].encode('utf-8')

        #self.curl.setopt(pycurl.URL, url)
        #self.curl.setopt(pycurl.FOLLOWLOCATION, 1)
        #self.curl.setopt(pycurl.MAXREDIRS, 5)
        #self.curl.setopt(pycurl.CONNECTTIMEOUT, self.config['connect_timeout'])
        #self.curl.setopt(pycurl.TIMEOUT, self.config['timeout'])
        #self.curl.setopt(pycurl.NOSIGNAL, 1)
        #self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)
        #self.curl.setopt(pycurl.HEADERFUNCTION, self.head_processor)

        ## User-Agent
        #if self.config['user_agent'] is None:
            #if self.config['user_agent_file'] is not None:
                #lines = open(self.config['user_agent_file']).read().splitlines()
                #self.config['user_agent'] = random.choice(lines)

        ## If value is None then set empty string
        ## None is not acceptable because in such case
        ## pycurl will set its default user agent "PycURL/x.xx.x"
        #if not self.config['user_agent']:
            #self.config['user_agent'] = ''

        #self.curl.setopt(pycurl.USERAGENT, self.config['user_agent'])

        #if self.config['debug']:
            #self.curl.setopt(pycurl.VERBOSE, 1)
            #self.curl.setopt(pycurl.DEBUGFUNCTION, self.debug_processor)

        ## Ignore SSL errors
        #self.curl.setopt(pycurl.SSL_VERIFYPEER, 0)
        #self.curl.setopt(pycurl.SSL_VERIFYHOST, 0)

        #method = self.config['method']
        #if method:
            #method = method.upper()
        #else:
            #if self.config['post'] or self.config['multipart_post']:
                #method = 'POST'
            #else:
                #method = 'GET'

        #if method == 'POST':
            #self.curl.setopt(pycurl.POST, 1)
            #if self.config['multipart_post']:
                #if not isinstance(self.config['multipart_post'], (list, tuple)):
                    #raise GrabMisuseError('multipart_post should be tuple or list, not dict')
                #post_items = self.normalize_http_values(self.config['multipart_post'], charset=self.charset)
                #self.curl.setopt(pycurl.HTTPPOST, post_items) 
            #elif self.config['post']:
                #if isinstance(self.config['post'], basestring):
                    ## bytes-string should be posted as-is
                    ## unicode should be converted into byte-string
                    #if isinstance(self.config['post'], unicode):
                        #post_data = self.normalize_unicode(self.config['post'])
                    #else:
                        #post_data = self.config['post']
                #else:
                    ## dict, tuple, list should be serialized into byte-string
                    #post_data = self.urlencode(self.config['post'])
                #self.curl.setopt(pycurl.POSTFIELDS, post_data)
        #elif method == 'PUT':
            #self.curl.setopt(pycurl.PUT, 1)
            #self.curl.setopt(pycurl.READFUNCTION, StringIO(self.config['post']).read) 
        #elif method == 'DELETE':
            #self.curl.setopt(pycurl.CUSTOMREQUEST, 'delete')
        #elif method == 'HEAD':
            #self.curl.setopt(pycurl.NOBODY, 1)
        #else:
            #self.curl.setopt(pycurl.HTTPGET, 1)
        
        #headers = self.config['common_headers']
        #if self.config['headers']:
            #headers.update(self.config['headers'])
        #header_tuples = [str('%s: %s' % x) for x\
                         #in headers.iteritems()]
        #self.curl.setopt(pycurl.HTTPHEADER, header_tuples)


        ## CURLOPT_COOKIELIST
        ## Pass a char * to a cookie string. Cookie can be either in
        ## Netscape / Mozilla format or just regular HTTP-style
        ## header (Set-Cookie: ...) format.
        ## If cURL cookie engine was not enabled it will enable its cookie
        ## engine.
        ## Passing a magic string "ALL" will erase all cookies known by cURL.
        ## (Added in 7.14.1)
        ## Passing the special string "SESS" will only erase all session
        ## cookies known by cURL. (Added in 7.15.4)
        ## Passing the special string "FLUSH" will write all cookies known by
        ## cURL to the file specified by CURLOPT_COOKIEJAR. (Added in 7.17.1)

        #if self.config['reuse_cookies']:
            ## Setting empty string will activate curl cookie engine
            #self.curl.setopt(pycurl.COOKIELIST, '')
        #else:
            #self.curl.setopt(pycurl.COOKIELIST, 'ALL')


        ## CURLOPT_COOKIE
        ## Pass a pointer to a zero terminated string as parameter. It will be used to set a cookie in the http request. The format of the string should be NAME=CONTENTS, where NAME is the cookie name and CONTENTS is what the cookie should contain.
        ## If you need to set multiple cookies, you need to set them all using a single option and thus you need to concatenate them all in one single string. Set multiple cookies in one string like this: "name1=content1; name2=content2;" etc.
        ## Note that this option sets the cookie header explictly in the outgoing request(s). If multiple requests are done due to authentication, followed redirections or similar, they will all get this cookie passed on.
        ## Using this option multiple times will only make the latest string override the previous ones. 

        #if self.config['cookies']:
            #self.curl.setopt(pycurl.COOKIE, self.encode_cookies(self.config['cookies']))

        #if self.config['cookiefile']:
            #self.load_cookies(self.config['cookiefile'])

        #if self.config['referer']:
            #self.curl.setopt(pycurl.REFERER, str(self.config['referer']))

        #if self.config['proxy']:
            #self.curl.setopt(pycurl.PROXY, str(self.config['proxy'])) 
        #else:
            #self.curl.setopt(pycurl.PROXY, '')

        #if self.config['proxy_userpwd']:
            #self.curl.setopt(pycurl.PROXYUSERPWD, self.config['proxy_userpwd'])

        ## PROXYTYPE
        ## Pass a long with this option to set type of the proxy. Available options for this are CURLPROXY_HTTP, CURLPROXY_HTTP_1_0 (added in 7.19.4), CURLPROXY_SOCKS4 (added in 7.15.2), CURLPROXY_SOCKS5, CURLPROXY_SOCKS4A (added in 7.18.0) and CURLPROXY_SOCKS5_HOSTNAME (added in 7.18.0). The HTTP type is default. (Added in 7.10) 

        #if self.config['proxy_type']:
            #ptype = getattr(pycurl, 'PROXYTYPE_%s' % self.config['proxy_type'].upper())
            #self.curl.setopt(pycurl.PROXYTYPE, ptype)

        #if self.config['proxy']:
            #if self.config['proxy_userpwd']:
                #auth = ' with authorization'
            #else:
                #auth = ''
            #proxy_info = ' via %s proxy of type %s%s' % (
                #self.config['proxy'], self.config['proxy_type'], auth)
        #else:
            #proxy_info = ''

        #tname = threading.currentThread().getName().lower()
        #if tname == 'mainthread':
            #tname = ''
        #else:
            #tname = '-%s' % tname

        #logger.debug('[%02d%s] %s %s%s' % (self.request_counter, tname, method, self.config['url'], proxy_info))

        #if self.config['encoding']:
            #self.curl.setopt(pycurl.ENCODING, self.config['encoding'])

        #if self.config['userpwd']:
            #self.curl.setopt(pycurl.USERPWD, self.config['userpwd'])

        #if self.config['charset']:
            #self.charset = self.config['charset']

    def _extract_cookies(self):
        """
        Extract cookies.
        """

        # Example of line:
        # www.google.com\tFALSE\t/accounts/\tFALSE\t0\tGoogleAccountsLocale_session\ten
        #cookies = {}
        #for line in self.curl.getinfo(pycurl.INFO_COOKIELIST):
            #chunks = line.split('\t')
            #cookies[chunks[-2]] = chunks[-1]
        #return cookies
        cookies = {}
        for item in self.browser.get_cookies():
            cookies[item['name']] = item['value']
        return cookies

    def request(self):
        try:
            if self.config['method'] == 'post':
                post_params = json.dumps(self.config['post'])
                script = """
                    var form = document.createElement("form");
                    form.setAttribute("method", "POST");
                    form.setAttribute("action", "%s");

                    var params = %s;

                    for(var key in params) {
                        if(params.hasOwnProperty(key)) {
                            var hiddenField = document.createElement("input");
                            hiddenField.setAttribute("type", "hidden");
                            hiddenField.setAttribute("name", key);
                            hiddenField.setAttribute("value", params[key]);

                            form.appendChild(hiddenField);
                         }
                    }

                    document.body.appendChild(form);
                    form.submit();
                """ % (self.config['url'], post_params)

                self.browser.execute_script(script)
            else:
                self.browser.get(self.config['url'])
                time.sleep(self.config['wait'])
        except Exception as ex:
            logging.error('', exc_info=ex)
            raise GrabError(999, 'Error =8-[ ]')
        #try:
            #self.curl.perform()
        #except pycurl.error, ex:
            ## CURLE_WRITE_ERROR
            ## An error occurred when writing received data to a local file, or
            ## an error was returned to libcurl from a write callback.
            ## This is expected error and we should ignore it
            #if 23 == ex[0]:
                #pass
            #else:
                #raise GrabError(ex[0], ex[1])

    def prepare_response(self, grab):
        #self.response.head = ''.join(self.response_head_chunks)
        #self.response.body = ''.join(self.response_body_chunks)
        #self.response.parse()

        response = Response()

        response.head = ''
        response._unicode_body = self.browser.page_source
        response.body = self.browser.page_source.encode('utf-8')
        response.charset = 'utf-8'
        #import pdb; pdb.set_trace()
        response.url = self.browser.current_url
        response.code = 200# TODO: fix, self.browser.status_code
        response.cookies = self._extract_cookies()
        #self.response.code = self.curl.getinfo(pycurl.HTTP_CODE)
        #self.response.time = self.curl.getinfo(pycurl.TOTAL_TIME)
        #self.response.url = self.curl.getinfo(pycurl.EFFECTIVE_URL)
        #import pdb; pdb.set_trace()
        self.browser.quit()

        return response

    def reset(self):
        # Maybe move to super-class???
        self.request_head = ''
        self.request_body = ''
        self.request_log = ''

    #def load_cookies(self, path):
        #"""
        #Load cookies from the file.

        #The cookie data may be in Netscape / Mozilla cookie data format or just regular HTTP-style headers dumped to a file.
        #"""

        #self.curl.setopt(pycurl.COOKIEFILE, path)


    #def dump_cookies(self, path):
        #"""
        #Dump all cookies to file.

        #Each cookie is dumped in the format:
        ## www.google.com\tFALSE\t/accounts/\tFALSE\t0\tGoogleAccountsLocale_session\ten
        #"""

        #open(path, 'w').write('\n'.join(self.curl.getinfo(pycurl.INFO_COOKIELIST)))

    #def clear_cookies(self):
        #"""
        #Clear all cookies.
        #"""

        ## Write tests
        #self.curl.setopt(pycurl.COOKIELIST, 'ALL')
        #self.config['cookies'] = {}
        #self.response.cookies = None

    #def reset_curl_instance(self):
        #"""
        #Completely recreate curl instance from scratch.
        
        #I add this method because I am not sure that
        #``clear_cookies`` method works fine and I should be sure
        #I can reset all cokies.
        #"""

        #self.curl = pycurl.Curl()


# from grab.base import BaseGrab
# class GrabSelenium(SeleniumTransportExtension, BaseGrab):
#     def __init__(self, response_body=None):
#         super(GrabSelenium, self).__init__(response_body, 'selenium.SeleniumTransportExtension')

########NEW FILE########
__FILENAME__ = urllib
# Copyright: 2011, Grigoriy Petukhov
# Author: Grigoriy Petukhov (http://lorien.name)
# License: BSD
"""
AHTUNG: VERY OLD CODE :)
"""
import logging
try:
    import urllib2
    from urllib2 import URLError
except ImportError:
    import urllib.request as urllib2
    from urllib.error import URLError
try:
    import cookielib
except ImportError:
    import http.cookiejar as cookielib
import socket

from grab.base import GrabError
from grab.util.py3k_support import *

logger = logging.getLogger('grab')

class Extension(object):
    def extra_init(self):
        cj = cookielib.CookieJar()
        self._opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj),
                                            urllib2.HTTPSHandler())
        #self._referer = None
        #self.headers = headers.random_request()

    def extra_reset(self):
        self._post_data = None

    def process_config(self):
        url = self.config['url']
        if isinstance(url, unicode):
            url = url.encode('utf-8')
            
        #self.curl.setopt(pycurl.URL, url)
        req = urllib2.Request(url)

        #self.curl.setopt(pycurl.FOLLOWLOCATION, 1)
        #self.curl.setopt(pycurl.MAXREDIRS, 5)
        #self.curl.setopt(pycurl.CONNECTTIMEOUT, self.config['connect_timeout'])

        #self.curl.setopt(pycurl.TIMEOUT, self.config['timeout'])
        socket.setdefaulttimeout(self.config['timeout'])

        #self.curl.setopt(pycurl.NOSIGNAL, 1)
        #self.curl.setopt(pycurl.WRITEFUNCTION, self.body_processor)
        #self.curl.setopt(pycurl.HEADERFUNCTION, self.head_processor)
        #self.curl.setopt(pycurl.USERAGENT, self.config['user_agent'])

        #if self.config['debug']:
            #self.curl.setopt(pycurl.VERBOSE, 1)
            #self.curl.setopt(pycurl.DEBUGFUNCTION, self.debug_processor)

        # Ignore SSL errors
        #self.curl.setopt(pycurl.SSL_VERIFYPEER, 0)
        #self.curl.setopt(pycurl.SSL_VERIFYHOST, 0)

        method = self.config['method']
        if method:
            method = method.upper()
        else:
            if self.config['payload'] or self.config['post']:
                method = 'POST'
            else:
                method = 'GET'

        post_data = None
        if method == 'POST':
            #self.curl.setopt(pycurl.POST, 1)
            #if self.config['payload']:
                #self.curl.setopt(pycurl.POSTFIELDS, self.config['payload'])
            #elif self.config['post']:
            if self.config['post']:
                self._post_data = self.urlencode(self.config['post'])
                #self.curl.setopt(pycurl.POSTFIELDS, post_data)
        #elif method == 'PUT':
            #self.curl.setopt(pycurl.PUT, 1)
            #self.curl.setopt(pycurl.READFUNCTION, StringIO(self.config['payload']).read) 
        #elif method == 'DELETE':
            #self.curl.setopt(pycurl.CUSTOMREQUEST, 'delete')
        else:
            #self.curl.setopt(pycurl.HTTPGET, 1)
            pass

        # TODO: start fucking from here in next time...
        
        headers = self.config['common_headers']
        if self.config['headers']:
            headers.update(self.config['headers'])
        for key, value in headers.items():
            req.add_header(key, value)
        #headers_tuples = [str('%s: %s' % x) for x\
                          #in self.config['headers'].iteritems()]
        #self.curl.setopt(pycurl.HTTPHEADER, headers_tuples)


        # CURLOPT_COOKIELIST
        # Pass a char * to a cookie string. Cookie can be either in Netscape / Mozilla format or just regular HTTP-style header (Set-Cookie: ...) format.
        # If cURL cookie engine was not enabled it will enable its cookie engine.
        # Passing a magic string "ALL" will erase all cookies known by cURL. (Added in 7.14.1)
        # Passing the special string "SESS" will only erase all session cookies known by cURL. (Added in 7.15.4)
        # Passing the special string "FLUSH" will write all cookies known by cURL to the file specified by CURLOPT_COOKIEJAR. (Added in 7.17.1)

        #if self.config['reuse_cookies']:
            ## Setting empty string will activate curl cookie engine
            #self.curl.setopt(pycurl.COOKIELIST, '')
        #else:
            #self.curl.setopt(pycurl.COOKIELIST, 'ALL')


        # CURLOPT_COOKIE
        # Pass a pointer to a zero terminated string as parameter. It will be used to set a cookie in the http request. The format of the string should be NAME=CONTENTS, where NAME is the cookie name and CONTENTS is what the cookie should contain.
        # If you need to set multiple cookies, you need to set them all using a single option and thus you need to concatenate them all in one single string. Set multiple cookies in one string like this: "name1=content1; name2=content2;" etc.
        # Note that this option sets the cookie header explictly in the outgoing request(s). If multiple requests are done due to authentication, followed redirections or similar, they will all get this cookie passed on.
        # Using this option multiple times will only make the latest string override the previous ones. 

        #if self.config['cookies']:
            #chunks = []
            #for key, value in self.config['cookies'].iteritems():
                #key = urllib.quote_plus(key)
                #value = urllib.quote_plus(value)
                #chunks.append('%s=%s;' % (key, value))
            #self.curl.setopt(pycurl.COOKIE, ''.join(chunks))

        if self.config['referer']:
            #self.curl.setopt(pycurl.REFERER, str(self.config['referer']))
            req.add_header('Referer', str(self.config['referer']))

        if self.config['proxy']:
            #self.curl.setopt(pycurl.PROXY, str(self.config['proxy'])) 
            req.set_proxy(str(self.config['proxy']))

        #if self.config['proxy_userpwd']:
            #self.curl.setopt(pycurl.PROXYUSERPWD, self.config['proxy_userpwd'])

        # PROXYTYPE
        # Pass a long with this option to set type of the proxy. Available options for this are CURLPROXY_HTTP, CURLPROXY_HTTP_1_0 (added in 7.19.4), CURLPROXY_SOCKS4 (added in 7.15.2), CURLPROXY_SOCKS5, CURLPROXY_SOCKS4A (added in 7.18.0) and CURLPROXY_SOCKS5_HOSTNAME (added in 7.18.0). The HTTP type is default. (Added in 7.10) 

        #if self.config['proxy_type']:
            #ptype = getattr(pycurl, 'PROXYTYPE_%s' % self.config['proxy_type'].upper())
            #self.curl.setopt(pycurl.PROXYTYPE, ptype)

        if self.config['proxy']:
            #if self.config['proxy_userpwd']:
                #auth = ' with authorization'
            #else:
                #auth = ''
            auth = ''
            proxy_info = ' via %s proxy of type %s%s' % (
                self.config['proxy'], self.config['proxy_type'], auth)
        else:
            proxy_info = ''

        logger.debug('[%02d] %s %s%s' % (self.request_counter, method, self.config['url'], proxy_info))
        self.req = req

    def _extract_cookies(self):
        """
        Extract cookies.
        """

        return {}

    def request(self):
        try:
            self._resp = self._opener.open(self.req)
            self._resp_body = self._resp.read()
        except URLError:
            raise GrabError(ex[0], ex[1])

    def prepare_response(self):
        self.response.body = self._resp_body
        self.response.headers = dict(self._resp.headers)
        #self.response.head = ''.join(self.response_head_chunks)
        #self.response.body = ''.join(self.response_body_chunks)
        #self.response.parse()
        #self.response.cookies = self._extract_cookies()
        #self.response.time = self.curl.getinfo(pycurl.TOTAL_TIME)
        self.response.code = self._resp.code
        self.response.url = self._resp.geturl()



########NEW FILE########
__FILENAME__ = upload
class UploadContent(str):
    """
    TODO: docstring
    """

    def __new__(cls, value):
        obj = str.__new__(cls, 'xxx')
        obj.raw_value = value
        return obj

    def field_tuple(self):
        # TODO: move to transport extension
        import pycurl
        return (pycurl.FORM_CONTENTS, self.raw_value)


class UploadFile(str):
    """
    TODO: docstring
    """

    def __new__(cls, path):
        obj = str.__new__(cls, 'xxx')
        obj.path = path
        return obj

    def field_tuple(self):
        # move to transport extension
        import pycurl
        return (pycurl.FORM_FILE, self.path)



########NEW FILE########
__FILENAME__ = config
import os

from copy import deepcopy
from grab.util import default_config
from grab.util.module import import_string

# Temporary disabled, spider config are mixed with all global config keys
#SPIDER_KEYS = ['GRAB_QUEUE', 'GRAB_CACHE', 'GRAB_PROXY_LIST', 'GRAB_THREAD_NUMBER',
               #'GRAB_NETWORK_TRY_LIMIT', 'GRAB_TASK_TRY_LIMIT']

def is_dict_interface(obj):
    try:
        obj['o_O']
        list(obj.keys())
    except (TypeError, AttributeError):
        return False
    except Exception:
        return True


class Config(dict):
    def update_with_object(self, obj, only_new_keys=False, allowed_keys=None):
        is_dict = is_dict_interface(obj)
        keys = obj.keys() if is_dict else dir(obj)
        for key in keys:
            if key.isupper():
                if not only_new_keys or not key in self:
                    if allowed_keys is None or key in allowed_keys:
                        self[key] = obj[key] if is_dict else getattr(obj, key)

    def update_with_path(self, path, **kwargs):
        obj = import_string(path)
        self.update_with_object(obj, **kwargs)

    def clone(self):
        return Config(deepcopy(self))

    def getint(self, key):
        return int(self[key])


def build_global_config(settings_mod_path='settings'):
    config = Config()
    try:
        config.update_with_path(settings_mod_path)
    except ImportError:
        # do not raise exception if settings_mod_path is default
        # and no settings.py file found in current directory
        if (settings_mod_path == 'settings' and
            not os.path.exists(os.path.join(os.path.realpath(os.getcwd()), 'settings.py'))):
            pass
        else:
            raise
    else:
        config.update_with_object(default_config.default_config, only_new_keys=True)
        return config


def build_spider_config(spider_class, global_config=None):
    if global_config is None:
        global_config = build_global_config()
    spider_settings_key = 'SPIDER_CONFIG_%s' % spider_class.get_spider_name().upper()
    spider_config = Config(global_config.get(spider_settings_key, {}))
    spider_config.update_with_object(global_config, only_new_keys=True,
                                     allowed_keys=None)#SPIDER_KEYS)
    spider_class.update_spider_config(spider_config)
    return spider_config

########NEW FILE########
__FILENAME__ = default_config
class DefaultConfig(object):
    GRAB_SPIDER_MODULES = ['spider']
    GRAB_SAVE_REPORT = True
    GRAB_THREAD_NUMBER = 1
    GRAB_NETWORK_TRY_LIMIT = 10
    GRAB_TASK_TRY_LIMIT = 10
    GRAB_DISPLAY_TIMING = True
    GRAB_DISPLAY_STATS = True
    GRAB_ACTIVATE_VIRTUALENV = False
    GRAB_DJANGO_SETTINGS = None

default_config = DefaultConfig()

########NEW FILE########
__FILENAME__ = database
import pymongo

db = pymongo.Connection()['{{ PROJECT_NAME }}']

########NEW FILE########
__FILENAME__ = settings
GRAB_SPIDER_MODULES = ['spider']

GRAB_PROXY_LIST = {
    'source': '/web/proxy.txt',
    'source_type': 'text_file',
}

#GRAB_CACHE = {
    #'backend': 'mysql',
    #'database': '{{ PROJECT_NAME }}_cache',
    #'user': '',
    #'passwd': '',
#}

#GRAB_THREAD_NUMBER = 15

#GRAB_QUEUE = {
    #'backend': 'redis',
#}

#GRAB_TASK_REFRESH_CACHE = {
    #'foo': True,
#}

########NEW FILE########
__FILENAME__ = spider
#!/usr/bin/env python
# coding: utf-8
from grab.spider import Spider, Task
from grab.tools.logs import default_logging
from grab import Grab
import logging

from database import db

class {{ PROJECT_NAME_CAMELCASE }}Spider(Spider):
    def task_generator(self):
        yield Task('initial', url='')

    def task_initial(self, grab, task):
        pass

########NEW FILE########
__FILENAME__ = log
"""
This module contains `print_dict` function that is useful
to dump content of dictionary in human acceptable representation.
"""
def repr_value(val):
    if isinstance(val, unicode):
        return val.encode('utf-8')
    elif isinstance(val, (list, tuple)):
        return '[%s]' % ', '.join(repr_value(x) for x in val)
    elif isinstance(val, dict):
        return '{%s}' % ', '.join('%s: %s' % (repr_value(x), repr_value(y)) for x, y in val.items())
    else:
        return str(val)


def print_dict(dic):
    print '[---'
    for key, val in sorted(dic.items(), key=lambda x: x[0]):
        print key, ':', repr_value(val)
    print '---]'

########NEW FILE########
__FILENAME__ = misc
import re
from functools import wraps
import logging
#from grab.error import GrabMisuseError

def camel_case_to_underscore(name):
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()


# from https://github.com/scrapy/scrapy/blob/master/scrapy/utils/decorator.py
def deprecated(use_instead=None):
    """This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used."""

    def wrapped(func):
        @wraps(func)
        def new_func(*args, **kwargs):
            message = "Call to deprecated function %s." % func.__name__
            if use_instead:
                message += " Use %s instead." % use_instead
            #warnings.warn(message, category=GrabMisuseError, stacklevel=2)
            logging.error(message)
            return func(*args, **kwargs)
        return new_func
    return wrapped

########NEW FILE########
__FILENAME__ = module
"""
The source code of `reraise` and `import_string` was copied from https://github.com/mitsuhiko/werkzeug/blob/master/werkzeug/utils.py
"""
import logging
import sys

from grab.spider.base import Spider
from grab.spider.error import SpiderInternalError
from grab.util.misc import camel_case_to_underscore
from grab.util.py3k_support import *

PY2 = True
SPIDER_REGISTRY = {}
string_types = (str, unicode)
logger = logging.getLogger('grab.util.module')

def reraise(tp, value, tb=None):
    if sys.version_info < (3,):
        from grab.util import py2x_support
        py2x_support.reraise(tp, value, tb)
    else:
        raise tp(value).with_traceback(tb)

class ImportStringError(ImportError):
    """Provides information about a failed :func:`import_string` attempt."""

    #: String in dotted notation that failed to be imported.
    import_name = None
    #: Wrapped exception.
    exception = None

    def __init__(self, import_name, exception):
        self.import_name = import_name
        self.exception = exception

        msg = (
            'import_string() failed for %r. Possible reasons are:\n\n'
            '- missing __init__.py in a package;\n'
            '- package or module path not included in sys.path;\n'
            '- duplicated package or module name taking precedence in '
            'sys.path;\n'
            '- missing module, class, function or variable;\n\n'
            'Debugged import:\n\n%s\n\n'
            'Original exception:\n\n%s: %s')

        name = ''
        tracked = []
        for part in import_name.replace(':', '.').split('.'):
            name += (name and '.') + part
            imported = import_string(name, silent=True)
            if imported:
                tracked.append((name, getattr(imported, '__file__', None)))
            else:
                track = ['- %r found in %r.' % (n, i) for n, i in tracked]
                track.append('- %r not found.' % name)
                msg = msg % (import_name, '\n'.join(track),
                             exception.__class__.__name__, str(exception))
                break

        ImportError.__init__(self, msg)

    def __repr__(self):
        return '<%s(%r, %r)>' % (self.__class__.__name__, self.import_name,
                                 self.exception)


def import_string(import_name, silent=False):
    """Imports an object based on a string.  This is useful if you want to
    use import paths as endpoints or something similar.  An import path can
    be specified either in dotted notation (``xml.sax.saxutils.escape``)
    or with a colon as object delimiter (``xml.sax.saxutils:escape``).

    If `silent` is True the return value will be `None` if the import fails.

    :param import_name: the dotted name for the object to import.
    :param silent: if set to `True` import errors are ignored and
                   `None` is returned instead.
    :return: imported object
    """

    #XXX: py3 review needed
    assert isinstance(import_name, string_types)
    # force the import name to automatically convert to strings
    import_name = str(import_name)
    try:
        if ':' in import_name:
            module, obj = import_name.split(':', 1)
        elif '.' in import_name:
            module, obj = import_name.rsplit('.', 1)
        else:
            return __import__(import_name)
        # __import__ is not able to handle unicode strings in the fromlist
        # if the module is a package
        if PY2 and isinstance(obj, unicode):
            obj = obj.encode('utf-8')
        try:
            return getattr(__import__(module, None, None, [obj]), obj)
        except (ImportError, AttributeError):
            # support importing modules not yet set up by the parent module
            # (or package for that matter)
            modname = module + '.' + obj
            __import__(modname)
            return sys.modules[modname]
    except ImportError as e:
        if not silent:
            reraise(
                ImportStringError,
                ImportStringError(import_name, e),
                sys.exc_info()[2])

def build_spider_registry(config):
    # TODO: make smart spider class searching
    #for mask in config.SPIDER_LOCATIONS:
        #for path in glob.glob(mask):
            #if path.endswith('.py'):
                #mod_path = path[:-3].replace('/', '.')
                #try:
                    #mod = __import__

    SPIDER_REGISTRY.clear()
    module_mapping = {}
    for path in config.get('GRAB_SPIDER_MODULES', []):
        if ':' in path:
            path, cls_name = path.split(':')
        else:
            cls_name = None
        try:
            mod = __import__(path, None, None, ['foo'])
        except ImportError as ex:
            if not path in unicode(ex):
                logging.error('', exc_info=ex)
        else:
            for key in dir(mod):
                if key == 'Spider':
                    continue
                if cls_name is None or key == cls_name:
                    val = getattr(mod, key)
                    if isinstance(val, type) and issubclass(val, Spider):
                        if val.Meta.abstract:
                            pass
                        else:
                            spider_name = val.get_spider_name()
                            logger.debug('Module `%s`, found spider `%s` with name `%s`' % (
                                path, val.__name__, spider_name))
                            if spider_name in SPIDER_REGISTRY:
                                raise SpiderInternalError(
                                    'There are two different spiders with the '\
                                    'same name "%s". Modules: %s and %s' % (
                                        spider_name,
                                        SPIDER_REGISTRY[spider_name].__module__,
                                        val.__module__))
                            else:
                                SPIDER_REGISTRY[spider_name] = val
    return SPIDER_REGISTRY


def load_spider_class(config, spider_name):
    if not SPIDER_REGISTRY:
        build_spider_registry(config)
    if not spider_name in SPIDER_REGISTRY:
        raise SpiderInternalError('Unknown spider: %s' % spider_name)
    else:
        return SPIDER_REGISTRY[spider_name]

########NEW FILE########
__FILENAME__ = py2old_support
'''
    Python 2.x (older than 2.6) support module
    Usage: from grab.util.py2old_support import *
'''

import sys

# Support next function for Python older than 2.6
if sys.version_info < (2, 6):
    def next(it):
        return it.next()

########NEW FILE########
__FILENAME__ = py2x_support
'''
    Python 2.x support module
    Usage: from grab.util.py2x_support import *
'''

# Support raise syntax in Python 2.x
def reraise(tp, value, tb=None):
    raise tp, value, tb

########NEW FILE########
__FILENAME__ = py3k_support
'''
    Python 3.x support module
    Usage: from grab.util.py3k_support import *
'''

import sys

PY3K = (sys.version_info >= (3, ))

# Backward compatibility for xrange function, basestring datatype
# unicode function/type, unichr function and raw_input function
if PY3K:
    xrange = range
    basestring = str
    unicode = str
    unichr = chr
    raw_input = input

########NEW FILE########
__FILENAME__ = slots
class SlotPickleMixin(object):
    """
    This mixin makes it possible to pickle/unpickle objects with __slots__ defined.
    Origin: http://code.activestate.com/recipes/578433-mixin-for-pickling-objects-with-__slots__/
    """

    def __getstate__(self):
        return dict(
            (slot, getattr(self, slot))
            for slot in self.__slots__
            if hasattr(self, slot)
        )

    def __setstate__(self, state):
        for slot, value in state.items():
            setattr(self, slot, value)

########NEW FILE########
__FILENAME__ = ng
from multiprocessing import Process
import logging
from multiprocessing import Queue, Event, active_children
import time
try:
    from urlparse import urlsplit
except ImportError:
    from urllib.parse import urlsplit
import os

from grab.spider import Spider, Task
from grab.spider.queue_backend.redis import QueueBackend

from grab.util.py3k_support import *

logger = logging.getLogger('ng')


class SimpleParserSpider(Spider):
    initial_urls = ['http://dumpz.org']

    def task_generator(self):
        yield Task('page', url='http://flask.pocoo.org/')
        time.sleep(1)
        yield Task('page', url='http://ya.ru/')
        yield Task('page', url='http://rambler.ru/')
        yield Task('page', url='http://mail.ru/')

    def task_initial(self, grab, task):
        for res in self.task_page(grab, task):
            yield res

    def task_page(self, grab, task):
        print(task.url)
        title = grab.doc.select('//title').text(default=None)
        print('TITLE:', title, 'PID:', os.getpid())
        host = urlsplit(task.url).netloc
        if not task.get('secondary'):
            yield Task('page', url='http://%s/robots.txt' % host, secondary=True)
            open('var/%s.txt' % host, 'w').write(grab.response.body)


def start_spider(spider_cls):
    try:
        result_queue = Queue()
        network_response_queue = Queue()
        shutdown_event = Event()
        generator_done_event = Event()
        taskq = QueueBackend('ng')

        #from grab.spider.base import logger_verbose
        #logger_verbose.setLevel(logging.DEBUG)

        kwargs = {
            'taskq': taskq,
            'result_queue': result_queue,
            'network_response_queue': network_response_queue,
            'shutdown_event': shutdown_event,
            'generator_done_event': generator_done_event,
            'ng': True,
        }

        # Generator: OK
        generator_waiting_shutdown_event = Event()
        bot = spider_cls(waiting_shutdown_event=generator_waiting_shutdown_event, **kwargs)
        generator = Process(target=bot.run_generator)
        generator.start()

        # Downloader: OK
        downloader_waiting_shutdown_event = Event()
        bot = spider_cls(waiting_shutdown_event=downloader_waiting_shutdown_event,
                         **kwargs)
        downloader = Process(target=bot.run)
        downloader.start()

        # Parser: OK
        events = []
        for x in xrange(2):
            parser_waiting_shutdown_event = Event()
            events.append(parser_waiting_shutdown_event)
            bot = spider_cls(waiting_shutdown_event=parser_waiting_shutdown_event,
                             **kwargs)
            parser = Process(target=bot.run_parser)
            parser.start()

        while True:
            time.sleep(2)
            print('task size', taskq.size())
            print('response size', network_response_queue.qsize())
            if (downloader_waiting_shutdown_event.is_set() and
                all(x.is_set() for x in events)):
                shutdown_event.set()
                break

        time.sleep(1)

        print('done')
    finally:
        for child in active_children():
            logging.debug('Killing child process (pid=%d)' % child.pid)
            child.terminate()


def setup_logging():
    logging.basicConfig(level=logging.DEBUG, format='%(name)s: %(message)s')
    logger = logging.getLogger('grab.verbose.spider.ng.downloader')
    logger.propagate = False
    logger.setLevel(logging.FATAL)


if __name__ == '__main__':
    setup_logging()
    start_spider(SimpleParserSpider)

########NEW FILE########
__FILENAME__ = runtest
#!/usr/bin/env python
# coding: utf-8
import unittest
import sys
from optparse import OptionParser
import logging
from copy import copy

from test.util import prepare_test_environment, clear_test_environment, GLOBAL
from test.server import start_server, stop_server
from grab.tools.watch import watch

# **********
# Grab Tests
# * pycurl transport
# * extensions
# * tools
# **********
GRAB_TEST_LIST = (
    # Internal API
    'test.case.grab_api',
    'test.case.grab_transport',
    'test.case.response_class',
    'test.case.grab_debug',
    # Response processing
    'test.case.grab_xml_processing',
    'test.case.grab_response_body_processing',
    #'test.case.grab_charset',
    # Network
    'test.case.grab_get_request',
    'test.case.grab_post_request',
    'test.case.grab_user_agent',
    'test.case.grab_cookies',
    # Refactor
    'test.case.grab_proxy',
    'test.case.grab_upload_file',
    'test.case.grab_limit_option',
    'test.case.grab_charset_issue',
    'test.case.grab_pickle',
    # *** Extension sub-system
    'test.case.extension',
    # *** Extensions
    'test.case.ext_text',
    'test.case.ext_rex',
    'test.case.ext_lxml',
    #'test.case.ext_form',
    'test.case.ext_doc',
    'test.case.ext_structured',
    # *** Tornado Test Server
    'test.case.debug_server',
    # *** grab.tools
    'test.case.tools_text',
    'test.case.tools_html',
    'test.case.tools_lxml',
    'test.case.tools_account',
    'test.case.tools_control',
    'test.case.tools_content',
    'test.case.tools_http',
    # *** Item
    'test.case.item',
    # *** Selector
    'test.case.selector',
    # *** Mock transport
    'test.case.grab_transport_mock',
    # Javascript features
    'test.case.grab_js',
    # pycurl tests
    'test.case.pycurl_cookie',
)

GRAB_EXTRA_TEST_LIST = (
    'test.case.tools_russian',
    'test.case.grab_django',
    'test.case.ext_pyquery',
)

# *******************************************
# Kit Tests
# * All Grab tests with enabled Kit Transport
# * Kit Selectors
# *******************************************

KIT_TEST_LIST = list(GRAB_TEST_LIST)
KIT_TEST_LIST += [
    'test.case.selector_kit',
]
for name in (
    'test.case.grab_proxy',
    'test.case.grab_upload_file',
    'test.case.grab_limit_option',
):
    KIT_TEST_LIST.remove(name)

KIT_EXTRA_TEST_LIST = list(GRAB_EXTRA_TEST_LIST)
KIT_EXTRA_TEST_LIST += [
    'test.case.kit_live_sites',
]

# ************
# Spider Tests
# ************

SPIDER_TEST_LIST = (
    'test.case.spider',
    #'tests.test_distributed_spider',
    'test.case.spider_task',
    'test.case.spider_proxy',
    'test.case.spider_queue',
    'test.case.spider_misc',
    'test.case.spider_meta',
    'test.case.spider_error',
    'test.case.spider_cache',
    'test.case.spider_command_controller',
)

SPIDER_EXTRA_TEST_LIST = ()


def main():
    logging.basicConfig(level=logging.DEBUG)
    parser = OptionParser()
    parser.add_option('-t', '--test', help='Run only specified tests')
    parser.add_option('--transport', help='Test specified transport',
                      default='grab.transport.curl.CurlTransport')
    parser.add_option('--extra', action='store_true',
                      default=False, help='Run extra tests for specific backends')
    parser.add_option('--test-grab', action='store_true',
                      default=False, help='Run tests for Grab::Spider')
    parser.add_option('--test-spider', action='store_true',
                      default=False, help='Run tests for Grab')
    parser.add_option('--test-all', action='store_true',
                      default=False, help='Run tests for both Grab and Grab::Spider')
    parser.add_option('--test-kit', action='store_true',
                      default=False, help='Run tests for Grab with WebKit transport')
    parser.add_option('--backend-mongo', action='store_true',
                      default=False, help='Run extra tests that depends on mongodb')
    parser.add_option('--backend-redis', action='store_true',
                      default=False, help='Run extra tests that depends on redis')
    parser.add_option('--backend-mysql', action='store_true',
                      default=False, help='Run extra tests that depends on mysql')
    parser.add_option('--backend-postgresql', action='store_true',
                      default=False, help='Run extra tests that depends on postgresql')
    opts, args = parser.parse_args()

    GLOBAL['transport'] = opts.transport

    # Override CLI option in case of kit test
    if opts.test_kit:
        GLOBAL['transport'] = 'grab.transport.kit.KitTransport'

    if opts.backend_mongo:
        GLOBAL['backends'].append('mongo')

    if opts.backend_redis:
        GLOBAL['backends'].append('redis')

    if opts.backend_mysql:
        GLOBAL['backends'].append('mysql')

    if opts.backend_postgresql:
        GLOBAL['backends'].append('postgresql')

    prepare_test_environment()
    test_list = []

    if opts.test_all:
        test_list += GRAB_TEST_LIST
        test_list += SPIDER_TEST_LIST
        if opts.extra:
            test_list += GRAB_EXTRA_TEST_LIST
            test_list += SPIDER_EXTRA_TEST_LIST

    if opts.test_grab:
        test_list += GRAB_TEST_LIST
        if opts.extra:
            test_list += GRAB_EXTRA_TEST_LIST

    if opts.test_kit:
        test_list += KIT_TEST_LIST
        if opts.extra:
            test_list += KIT_EXTRA_TEST_LIST

    if opts.test_spider:
        test_list += SPIDER_TEST_LIST
        if opts.extra:
            test_list += SPIDER_EXTRA_TEST_LIST

    if opts.test:
        test_list += [opts.test]

    # Check tests integrity
    # Ensure that all test modules are imported correctly
    for path in test_list:
        __import__(path, None, None, ['foo'])

    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    for path in test_list:
        mod_suite = loader.loadTestsFromName(path)
        for some_suite in mod_suite:
            for test in some_suite:
                if not hasattr(test, '_backend') or test._backend in GLOBAL['backends']:
                    suite.addTest(test)

    runner = unittest.TextTestRunner()

    start_server()
    result = runner.run(suite)

    clear_test_environment()
    if result.wasSuccessful():
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = speed
#!/usr/bin/env python
import time
from grab import Grab
from grab.spider import Spider, Task
import logging
import itertools

from test.server import SERVER, start_server, stop_server

def timeout_iterator():
    timeouts = [0]#(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
    for tm in itertools.cycle(timeouts):
        yield tm


class TestSpider(Spider):
    def task_generator(self):
        for x in range(1000):
            yield Task('page', url=SERVER.BASE_URL)

    def task_page(self, grab, task):
        assert grab.doc.body == b'foo'


def main():
    start_server()

    SERVER.TIMEOUT_ITERATOR = timeout_iterator()
    SERVER.RESPONSE['get'] = 'foo'

    bot = TestSpider(thread_number=1000)
    bot.run()
    print(bot.render_stats())

    stop_server()


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)
    main()

########NEW FILE########
__FILENAME__ = speed_grab
#!/usr/bin/env python
# coding: utf-8
from grab import Grab
from grab.tools.work import make_work
from grab.tools.logs import default_logging
import time
import logging
#import urllib

from grab.util.py3k_support import *

#logging.basicConfig(level=logging.DEBUG)

def timer(func):
    """
    Display time taken to execute the decorated function.
    """

    def inner(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        total = time.time() - start
        print('Time: %.2f sec.' % total)
        return result
    return inner


@timer
def main():
    default_logging()
    for x in xrange(500):
        url = 'http://load.local/grab.html'
        g = Grab()
        g.go(url)
        assert 'grab' in g.response.body
        #g.tree
        #urllib.urlopen(url).read()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = speed_spider
#!/usr/bin/env python
# coding: utf-8
from grab.spider import Spider, Task
from grab.tools.logs import default_logging
import time
import logging
from random import randint

from grab.util.py3k_support import *

URL_28K = 'http://load.local/grab.html'

def timer(func):
    """
    Display time taken to execute the decorated function.
    """

    def inner(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        total = time.time() - start
        print('Time: %.2f sec.' % total)
        return result
    return inner


class SpeedSpider(Spider):
    def task_generator(self):
        url_template = 'http://load.local/grab%d.html'
        #fast_url = 'http://load.local/grab0.html'
        slow_url = 'http://load.local/slow.html'
        #yield Task('load', url=slow_url, disable_cache=True)
        #yield Task('load', url=fast_url, disable_cache=False)
        for x in xrange(500):
            disable_flag = True#not (x % 2)
            yield Task('load', url=url_template % x, disable_cache=disable_flag)
            #if randint(0, 10) == 10:
                #yield Task('load', url=slow_url, disable_cache=True)

    def task_load(self, grab, task):
        assert 'grab' in grab.response.body
        print('ok', task.url)


@timer
def main():
    default_logging()
    bot = SpeedSpider(thread_number=30)
    bot.setup_cache(database='speed_spider', use_compression=True)
    bot.run()
    print(bot.render_stats())

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = debug_server
from unittest import TestCase
try:
    from urllib import urlopen
except ImportError:
    from urllib.request import urlopen

from test.server import SERVER

class TestTornadoServer(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_get(self):
        SERVER.RESPONSE['get'] = b'zorro'
        data = urlopen(SERVER.BASE_URL).read()
        self.assertEqual(data, SERVER.RESPONSE['get'])

    def test_path(self):
        urlopen(SERVER.BASE_URL + '/foo').read()
        self.assertEqual(SERVER.REQUEST['path'], '/foo')

        urlopen(SERVER.BASE_URL + '/foo?bar=1').read()
        self.assertEqual(SERVER.REQUEST['path'], '/foo')
        self.assertEqual(SERVER.REQUEST['args']['bar'], '1')


    def test_post(self):
        SERVER.RESPONSE['post'] = b'foo'
        data = urlopen(SERVER.BASE_URL, b'THE POST').read()
        self.assertEqual(data, SERVER.RESPONSE['post'])

    def test_callback(self):
        class ContentGenerator():
            def __init__(self):
                self.count = 0

            def __call__(self):
                self.count += 1
                return 'foo'

        gen = ContentGenerator()
        SERVER.RESPONSE['get'] = gen 
        urlopen(SERVER.BASE_URL).read()
        self.assertEqual(gen.count, 1)
        urlopen(SERVER.BASE_URL).read()
        self.assertEqual(gen.count, 2)
        # Now create POST request which should no be
        # processed with ContentGenerator which is bind to GET
        # requests
        urlopen(SERVER.BASE_URL, b'some post').read()
        self.assertEqual(gen.count, 2)

########NEW FILE########
__FILENAME__ = extension
from __future__ import absolute_import
from grab.extension import register_extensions
from unittest import TestCase

GLOBAL = {}

class ExtensionA(object):
    def extra_foo(self):
        GLOBAL['items'].append('a')

class ExtensionB(object):
    def extra_foo(self):
        GLOBAL['items'].append('b')

class Worker(ExtensionA,  ExtensionB):
    extension_points = ('foo',)

    def do_something(self):
        GLOBAL['items'].append('c')
        self.trigger_extensions('foo')


register_extensions(Worker)


class ExtensionTestCase(TestCase):
    def setUp(self):
        GLOBAL['items'] = []

    def test_extensions(self):
        worker = Worker()
        worker.do_something()
        self.assertEqual(set(GLOBAL['items']), set(['a', 'b', 'c']))

########NEW FILE########
__FILENAME__ = ext_doc
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound

from test.util import build_grab
from test.server import SERVER

HTML = b"""
<html>
    <body>
        <h1>test</h1>
    </body>
</html>
"""

class DocExtensionTest(TestCase):
    def setUp(self):
        SERVER.reset()

        # Create fake grab instance with fake response
        self.g = build_grab(document_body=HTML)

    def test_extension_in_general(self):
        self.assertTrue(self.g.doc)

    def test_select_method(self):
        self.assertEqual('test', self.g.doc.select('//h1').text())

########NEW FILE########
__FILENAME__ = ext_form
# coding: utf-8
from unittest import TestCase
try:
    from urlparse import parse_qsl
except ImportError:
    from urllib.parse import parse_qsl

from grab import Grab, DataNotFound, GrabMisuseError
from test.util import ignore_transport, build_grab
from test.server import SERVER

FORMS = u"""
<head>
    <title>Title</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
</head>
<body>
    <div id="header">
        <form id="search_form" method="GET">
            <input id="search_box" name="query" value="" />
            <input type="submit" value="submit" class="submit_btn" name="submit" />
        </form>
    </div>
    <div id="content">
        <FORM id="common_form" method="POST">
          <input type="text" id="some_value" name="some_value" value="" />
          <input id="some_value" name="image" type="file" value="" />
          <select id="gender" name="gender">
              <option value="1">Female</option>
              <option value="2">Male</option>
           </select>
           <input type="submit" value="submit" class="submit_btn" name="submit" />
        </FORM>
        <h1 id="fake_form">Big header</h1>
        <form name="dummy" action="/dummy">
           <input type="submit" value="submit" class="submit_btn" name="submit" />
           <input type="submit" value="submit2" class="submit_btn" name="submit" />
        </form>
    </div>
</body>
""".encode('utf-8')

POST_FORM = """
<form method="post" action="%s">
    <input type="text" name="secret" value="123"/>
    <input type="text" name="name" />
    <input type="text" disabled value="some_text" name="disabled_text" />
</form>
""" % SERVER.BASE_URL

MULTIPLE_SUBMIT_FORM = """
<form method="post">
    <input type="text" name="secret" value="123"/>
    <input type="submit" name="submit1" value="submit1" />
    <input type="submit" name="submit2" value="submit2" />
</form>
"""

NO_FORM_HTML = """
<div>Hello world</div>
"""

DISABLED_RADIO_HTML = """
<form>
    <input type="radio" name="foo" value="1" disabled="disabled" />
    <input type="radio" name="foo" value="2" disabled="disabled" />

    <input type="radio" name="bar" value="1" checked="checked" />
    <input type="radio" name="bar" value="2" disabled="disabled" />
</form>
"""

class TestHtmlForms(TestCase):
    def setUp(self):
        SERVER.reset()

        # Create fake grab instance with fake response
        self.g = build_grab()
        self.g.fake_response(FORMS)

    def test_choose_form(self):
        """
        Test ``choose_form`` method
        """
        
        # raise errors
        self.assertRaises(DataNotFound, lambda: self.g.choose_form(10))
        self.assertRaises(DataNotFound, lambda: self.g.choose_form(id='bad_id'))
        self.assertRaises(DataNotFound, lambda: self.g.choose_form(id='fake_form'))
        self.assertRaises(GrabMisuseError, lambda: self.g.choose_form())
        
        # check results
        self.g.choose_form(0)
        self.assertEqual('form', self.g._lxml_form.tag)
        self.assertEqual('search_form', self.g._lxml_form.get('id'))

        # reset current form
        self.g._lxml_form = None

        self.g.choose_form(id='common_form')
        self.assertEqual('form', self.g._lxml_form.tag)
        self.assertEqual('common_form', self.g._lxml_form.get('id'))

        # reset current form
        self.g._lxml_form = None

        self.g.choose_form(name='dummy')
        self.assertEqual('form', self.g._lxml_form.tag)
        self.assertEqual('dummy', self.g._lxml_form.get('name'))

        # reset current form
        self.g._lxml_form = None

        self.g.choose_form(xpath='//form[contains(@action, "/dummy")]')
        self.assertEqual('form', self.g._lxml_form.tag)
        self.assertEqual('dummy', self.g._lxml_form.get('name'))

    def assertEqualQueryString(self, qs1, qs2):
        args1 = set([(x, y[0]) for x, y in parse_qsl(qs1)])
        args2 = set([(x, y[0]) for x, y in parse_qsl(qs2)])
        self.assertEqual(args1, args2)

    def test_submit(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = POST_FORM
        g.go(SERVER.BASE_URL)
        g.set_input('name', 'Alex')
        g.submit()
        self.assertEqualQueryString(SERVER.REQUEST['post'], 'name=Alex&secret=123')

        # Default submit control
        SERVER.RESPONSE['get'] = MULTIPLE_SUBMIT_FORM
        g.go(SERVER.BASE_URL)
        g.submit()
        self.assertEqualQueryString(SERVER.REQUEST['post'], 'secret=123&submit1=submit1')

        # Selected submit control
        SERVER.RESPONSE['get'] = MULTIPLE_SUBMIT_FORM
        g.go(SERVER.BASE_URL)
        g.submit(submit_name='submit2')
        self.assertEqualQueryString(SERVER.REQUEST['post'], 'secret=123&submit2=submit2')

        # Default submit control if submit control name is invalid
        SERVER.RESPONSE['get'] = MULTIPLE_SUBMIT_FORM
        g.go(SERVER.BASE_URL)
        g.submit(submit_name='submit3')
        self.assertEqualQueryString(SERVER.REQUEST['post'], 'secret=123&submit1=submit1')

    def test_set_methods(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = FORMS
        g.go(SERVER.BASE_URL)

        self.assertEqual(g._lxml_form, None)

        g.set_input('gender', '1')
        self.assertEqual('common_form', g._lxml_form.get('id'))

        self.assertRaises(KeyError, lambda: g.set_input('query', 'asdf'))

        g._lxml_form = None
        g.set_input_by_id('search_box', 'asdf')
        self.assertEqual('search_form', g._lxml_form.get('id'))

        g.choose_form(xpath='//form[@id="common_form"]')
        g.set_input_by_number(0, 'asdf')

        g._lxml_form = None
        g.set_input_by_xpath('//*[@name="gender"]', '2')
        self.assertEqual('common_form', g._lxml_form.get('id'))

    def test_html_without_forms(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = NO_FORM_HTML
        g.go(SERVER.BASE_URL)
        self.assertRaises(DataNotFound, lambda: g.form)

    def test_disabled_radio(self):
        """
        Bug #57
        """

        g = build_grab()
        SERVER.RESPONSE['get'] = DISABLED_RADIO_HTML
        g.go(SERVER.BASE_URL)
        g.submit(make_request=False)

########NEW FILE########
__FILENAME__ = ext_lxml
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound

from test.util import build_grab
from test.server import SERVER

HTML = u"""
<head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=cp1251" />
</head>
<body>
    <div id="bee">
        <div class="wrapper">
            <strong id="bee-strong"></strong><em id="bee-em"></em>
        </div>
        <script type="text/javascript">
        mozilla = 777;
        </script>
        <style type="text/css">
        body { color: green; }
        </style>
    </div>
    <div id="fly">
        <strong id="fly-strong">\n</strong><em id="fly-em"></em>
    </div>
    <ul id="num">
        <li id="num-1">item #100 2</li>
        <li id="num-2">item #2</li>
    </ul>
""".encode('cp1251')


XML = b"""
<root>
    <man>
        <age>25</age>
        <weight><![CDATA[30]]></weight>
    </man>
</root>
"""


class LXMLExtensionTest(TestCase):
    def setUp(self):
        SERVER.reset()

        # Create fake grab instance with fake response
        self.g = build_grab()
        self.g.fake_response(HTML, charset='cp1251')

        from lxml.html import fromstring
        self.lxml_tree = fromstring(self.g.response.body)

    def test_lxml_text_content_fail(self):
        # lxml node text_content() method do not put spaces between text
        # content of adjacent XML nodes
        self.assertEqual(self.lxml_tree.xpath('//div[@id="bee"]/div')[0].text_content().strip(), u'')
        self.assertEqual(self.lxml_tree.xpath('//div[@id="fly"]')[0].text_content().strip(), u'\n')

    def test_lxml_xpath(self):
        names = set(x.tag for x in self.lxml_tree.xpath('//div[@id="bee"]//*'))
        self.assertEqual(set(['em', 'div', 'strong', 'style', 'script']), names)
        names = set(x.tag for x in self.lxml_tree.xpath('//div[@id="bee"]//*[name() != "script" and name() != "style"]'))
        self.assertEqual(set(['em', 'div', 'strong']), names)

    def test_xpath(self):
        self.assertEqual('bee-em', self.g.xpath_one('//em').get('id'))
        self.assertEqual('num-2', self.g.xpath_one(u'//*[text() = "item #2"]').get('id'))
        self.assertRaises(DataNotFound,
            lambda: self.g.xpath_one('//em[@id="baz"]'))
        self.assertEqual(None, self.g.xpath_one('//zzz', default=None))
        self.assertEqual('foo', self.g.xpath_one('//zzz', default='foo'))

    def test_xpath_text(self):
        self.assertEqual(u' ', self.g.xpath_text('//*[@id="bee"]', smart=True))
        self.assertEqual(u' mozilla = 777; body { color: green; }', self.g.xpath_text('//*[@id="bee"]', smart=False))
        self.assertEqual(u'    item #100 2 item #2', self.g.xpath_text('/html/body', smart=True))
        self.assertRaises(DataNotFound,
            lambda: self.g.xpath_text('//code'))
        self.assertEqual(u'bee', self.g.xpath_one('//*[@id="bee"]/@id'))
        self.assertRaises(DataNotFound,
            lambda: self.g.xpath_text('//*[@id="bee2"]/@id'))

    def test_xpath_number(self):
        self.assertEqual(100, self.g.xpath_number('//li'))
        self.assertEqual(100, self.g.xpath_number('//li', make_int=True))
        self.assertEqual('100', self.g.xpath_number('//li', make_int=False))
        self.assertEqual(1002, self.g.xpath_number('//li', ignore_spaces=True))
        self.assertEqual('1002', self.g.xpath_number('//li', ignore_spaces=True,
                         make_int=False))
        self.assertRaises(DataNotFound,
            lambda: self.g.xpath_number('//liza'))
        self.assertEqual('foo', self.g.xpath_number('//zzz', default='foo'))

    def test_xpath_list(self):
        self.assertEqual(['num-1', 'num-2'],
            [x.get('id') for x in self.g.xpath_list('//li')])

    def test_css(self):
        self.assertEqual('bee-em', self.g.css_one('em').get('id'))
        self.assertEqual('num-2', self.g.css_one('#num-2').get('id'))
        self.assertRaises(DataNotFound,
            lambda: self.g.css_one('em#baz'))
        self.assertEqual('foo', self.g.css_one('zzz', default='foo'))

    def test_css_text(self):
        self.assertEqual(u' ', self.g.css_text('#bee', smart=True))
        self.assertEqual(u'    item #100 2 item #2', self.g.css_text('html body', smart=True))
        self.assertRaises(DataNotFound,
            lambda: self.g.css_text('code'))
        self.assertEqual('foo', self.g.css_text('zzz', default='foo'))

    def test_css_number(self):
        self.assertEqual(100, self.g.css_number('li'))
        self.assertEqual('100', self.g.css_number('li', make_int=False))
        self.assertEqual(1002, self.g.css_number('li', ignore_spaces=True))
        self.assertRaises(DataNotFound,
            lambda: self.g.css_number('liza'))
        self.assertEqual('foo', self.g.css_number('zzz', default='foo'))

    def test_css_list(self):
        self.assertEqual(['num-1', 'num-2'],
            [x.get('id') for x in self.g.css_list('li')])

    def test_strip_tags(self):
        self.assertEqual('foo', self.g.strip_tags('<b>foo</b>'))
        self.assertEqual('foo bar', self.g.strip_tags('<b>foo</b> <i>bar'))
        self.assertEqual('foobar', self.g.strip_tags('<b>foo</b><i>bar'))
        self.assertEqual('foo bar', self.g.strip_tags('<b>foo</b><i>bar', smart=True))
        self.assertEqual('', self.g.strip_tags('<b> <div>'))

    def test_css_exists(self):
        self.assertTrue(self.g.css_exists('li#num-1'))
        self.assertFalse(self.g.css_exists('li#num-3'))

    def test_xpath_exists(self):
        self.assertTrue(self.g.xpath_exists('//li[@id="num-1"]'))
        self.assertFalse(self.g.xpath_exists('//li[@id="num-3"]'))

    def test_cdata_issue(self):
        g = build_grab()
        g.fake_response(XML)

        # By default HTML DOM builder is used
        # It handles CDATA incorrectly
        self.assertEqual(None, g.xpath_one('//weight').text)
        self.assertEqual(None, g.tree.xpath('//weight')[0].text)

        # But XML DOM builder produces valid result
        #self.assertEqual(None, g.xpath_one('//weight').text)
        self.assertEqual('30', g.xml_tree.xpath('//weight')[0].text)

        # Use `content_type` option to change default DOM builder
        g = build_grab()
        g.fake_response(XML)
        g.setup(content_type='xml')

        self.assertEqual('30', g.xpath_one('//weight').text)
        self.assertEqual('30', g.tree.xpath('//weight')[0].text)

    def test_xml_declaration(self):
        """
        HTML with XML declaration shuld be processed without errors.
        """
        SERVER.RESPONSE['get'] = """<?xml version="1.0" encoding="UTF-8"?>
        <html><body><h1>test</h1></body></html>
        """
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertEqual('test', g.xpath_text('//h1'))

    def test_empty_document(self):
        SERVER.RESPONSE['get'] = 'oops'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        g.xpath_exists('//anytag')

        SERVER.RESPONSE['get'] = '<frameset></frameset>'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        g.xpath_exists('//anytag')

########NEW FILE########
__FILENAME__ = ext_pyquery
from unittest import TestCase

from test.server import SERVER
from test.util import build_grab

class ExtensionPyqueryTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_pyquery_handler(self):
        SERVER.RESPONSE['get'] = '<body><h1>Hello world</h1><footer>2014</footer>'
        g = build_grab()
        g.go(SERVER.BASE_URL)

        self.assertEqual(g.doc.pyquery('h1').text(), 'Hello world')

########NEW FILE########
__FILENAME__ = ext_rex
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound, GrabMisuseError
import re

from test.util import build_grab
from test.server import SERVER

from grab.util.py3k_support import *

HTML = u"""
<head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=cp1251" />
</head>
<body>
    <div id="bee">
        <div class="wrapper">
            # russian LA
            <strong id="bee-strong"></strong><em id="bee-em"></em>
        </div>
        <script type="text/javascript">
        mozilla = 777;
        </script>
        <style type="text/css">
        body { color: green; }
        </style>
    </div>
    <div id="fly">
        # russian XA
        <strong id="fly-strong">\n</strong><em id="fly-em"></em>
    </div>
    <ul id="num">
        <li id="num-1">item #100 2</li>
        <li id="num-2">item #2</li>
    </ul>
""".encode('cp1251')

class ExtensionRexTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

        # Create fake grab instance with fake response
        self.g = build_grab()
        self.g.fake_response(HTML, charset='cp1251')

    def test_rex(self):
        # Search unicode rex in unicode body - default case
        rex = re.compile(u'()', re.U)
        self.assertEqual(u'', self.g.rex(rex).group(1))

        # Search non-unicode rex in byte-string body
        rex = re.compile(u'()'.encode('cp1251'))
        self.assertEqual(u''.encode('cp1251'), self.g.rex(rex, byte=True).group(1))

        ## Search for non-unicode rex in unicode body shuld fail
        pattern = '()'
        # py3 hack
        if PY3K:
            pattern = pattern.encode('utf-8')
        rex = re.compile(pattern)
        self.assertRaises(DataNotFound, lambda: self.g.rex(rex))

        ## Search for unicode rex in byte-string body shuld fail
        rex = re.compile(u'', re.U)
        self.assertRaises(DataNotFound, lambda: self.g.rex(rex, byte=True))

        ## Search for unexesting fragment
        rex = re.compile(u'(2)', re.U)
        self.assertRaises(DataNotFound, lambda: self.g.rex(rex))

    def test_assert_rex(self):
        self.g.assert_rex(re.compile(u''))
        self.g.assert_rex(re.compile(u''.encode('cp1251')), byte=True)
        #self.assertRaises(DataNotFound,
            #lambda: self.g.assert_rex(re.compile(u'2')))

    def test_assert_rex_text(self):
        self.assertEqual(u'', self.g.rex_text('<em id="fly-em">([^<]+)'))

########NEW FILE########
__FILENAME__ = ext_structured
# coding: utf-8

from json import loads
from unittest import TestCase

from grab import Grab
from grab.tools.structured import Structure as x

from test.util import build_grab
from test.server import SERVER


XML = b'''
    <issue index="2">
        <title>XML today</title>
        <date>12.09.98</date>
        <about>XML</about>
        <home-url>www.j.ru/issues/</home-url>
        <number>448</number>
        <detail>
            <description>

                issue 2
                detail description

            </description>
            <number>445</number>
        </detail>
        <articles>
            <article ID="3">
                <title>Issue overview</title>
                <url>/article1</url>
                <hotkeys>
                    <hotkey>language</hotkey>
                    <hotkey>marckup</hotkey>
                    <hotkey>hypertext</hotkey>
                </hotkeys>
                <article-finished/>
            </article>
            <article>
                <title>Latest reviews</title>
                <url>/article2</url>
                <author ID="3"/>
                <hotkeys>
                    <hotkey/>
                </hotkeys>
            </article>
            <article ID="4">
                <title/>
                <url/>
                <hotkeys/>
            </article>
        </articles>
    </issue>
'''


class StructuredExtensionTest(TestCase):
    def setUp(self):
        self.g = build_grab(document_body=XML)

    def test_1(self):
        result = self.g.doc.structure(
            '//issue',
            title='./title/text()',
            date='./date/text()',
            about='./about/text()',
            number=('./number/text()', int),
            home_url='./home-url/text()',
        )
        self.assertEqual(
            result,
            loads('''
                [
                    {
                        "date": "12.09.98",
                        "about": "XML",
                        "home_url": "www.j.ru/issues/",
                        "number": 448,
                        "title": "XML today"
                    }
                ]
            ''')
        )

    def test_2(self):
        result = self.g.doc.structure(
            '//issue',
            x(
                './detail',
                description=('./description/text()', lambda item: ' '.join(item.split())),
                detail_number=('./number/text()', int)
            ),
            title='./title/text()',
            date='./date/text()',
        )
        self.assertEqual(
            result,
            loads('''
                [
                    {
                        "detail_number": 445,
                        "date": "12.09.98",
                        "description": "issue 2 detail description",
                        "title": "XML today"
                    }
                ]
            ''')
        )

    def test_3(self):
        result = self.g.doc.structure(
            '//issue',
            home_url='./home-url/text()',
            articles=x(
                './articles/article',
                id='./@id',
                title='./title/text()',
                url='./url/text()',
                hotkeys=x(
                    './hotkeys',
                    hotkey='./hotkey/text()'
                )
            )
        )
        self.assertEqual(
            result,
            loads('''
                [
                    {
                        "articles": [
                            {
                                "url": "/article1",
                                "id": "3",
                                "hotkeys": [
                                    {
                                        "hotkey": "language"
                                    }
                                ],
                                "title": "Issue overview"
                            },
                            {
                                "url": "/article2",
                                "id": null,
                                "hotkeys": [
                                    {
                                        "hotkey": null
                                    }
                                ],
                                "title": "Latest reviews"
                            },
                            {
                                "url": null,
                                "id": "4",
                                "hotkeys": [
                                    {
                                        "hotkey": null
                                    }
                                ],
                                "title": null
                            }
                        ],
                        "home_url": "www.j.ru/issues/"
                    }
                ]
            ''')
        )

########NEW FILE########
__FILENAME__ = ext_text
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound, GrabMisuseError

from test.util import build_grab
from test.server import SERVER

from grab.util.py3k_support import *

HTML = u"""
<head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=cp1251" />
</head>
<body>
    <div id="bee">
        <div class="wrapper">
            # russian LA
            <strong id="bee-strong"></strong><em id="bee-em"></em>
        </div>
        <script type="text/javascript">
        mozilla = 777;
        </script>
        <style type="text/css">
        body { color: green; }
        </style>
    </div>
    <div id="fly">
        # russian XA
        <strong id="fly-strong">\n</strong><em id="fly-em"></em>
    </div>
    <ul id="num">
        <li id="num-1">item #100 2</li>
        <li id="num-2">item #2</li>
    </ul>
""".encode('cp1251')

class TextExtensionTest(TestCase):
    def setUp(self):
        SERVER.reset()

        # Create fake grab instance with fake response
        self.g = build_grab()
        self.g.fake_response(HTML, charset='cp1251')

    def test_search(self):
        self.assertTrue(self.g.search(u''.encode('cp1251'), byte=True))
        self.assertTrue(self.g.search(u''))
        self.assertFalse(self.g.search(u'2'))

    def test_search_usage_errors(self):
        self.assertRaises(GrabMisuseError,
            lambda: self.g.search(u'', byte=True))
        anchor = ''
        # py3 hack
        if PY3K:
            anchor = anchor.encode('utf-8')
        self.assertRaises(GrabMisuseError,
            lambda: self.g.search(anchor))

    def test_assert_substring(self):
        self.g.assert_substring(u'')
        self.g.assert_substring(u''.encode('cp1251'), byte=True)
        self.assertRaises(DataNotFound,
            lambda: self.g.assert_substring(u'2'))

    def test_assert_substrings(self):
        self.g.assert_substrings((u'',))
        self.g.assert_substrings((u' ', u''))
        self.g.assert_substrings((u''.encode('cp1251'), '  ?'), byte=True)
        self.assertRaises(DataNotFound,
            lambda: self.g.assert_substrings((u', ', u'---')))

########NEW FILE########
__FILENAME__ = grab_api
# coding: utf-8
from unittest import TestCase

from grab.util.py3k_support import *
from grab import GrabMisuseError
from test.util import ignore_transport, only_transport, build_grab
from test.server import SERVER

#from grab.util.py3k_support import *

class GrabApiTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_incorrect_option_name(self):
        g = build_grab()
        self.assertRaises(GrabMisuseError,
            lambda: g.setup(save_the_word=True))

    @ignore_transport('ghost.GhostTransport')
    # Ghost test was disabled because of strange error
    # that appear when multiple Ghost instances are created
    def test_clone(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = 'Moon'
        g.go(SERVER.BASE_URL)
        self.assertTrue(b'Moon' in g.response.body)
        g2 = build_grab()
        #self.assertEqual(g2.doc.grab, g2)
        g2 = g.clone()
        self.assertTrue(b'Moon' in g.response.body)

    @ignore_transport('ghost.GhostTransport')
    def test_empty_clone(self):
        g = build_grab()
        g.clone()

    @ignore_transport('ghost.GhostTransport')
    def test_adopt(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = 'Moon'
        g.go(SERVER.BASE_URL)
        g2 = build_grab()
        self.assertEqual(g2.config['url'], None)
        g2.adopt(g)
        self.assertTrue(b'Moon' in g2.response.body)
        self.assertEqual(g2.config['url'], SERVER.BASE_URL)

    def test_empty_adopt(self):
        g = build_grab()
        g2 = build_grab()
        g2.adopt(g)

    def test_default_content_for_fake_response(self):
        content = '<strong>test</strong>'
        g = build_grab(document_body=content)
        self.assertEqual(g.response.body, content)

    def test_inheritance(self):
        from grab import Grab

        class SimpleExtension(object):
            data = {'counter': 0}

            #def extra_init(self):
                #self.get_data()['counter'] += 1

            @classmethod
            def get_data(cls):
                return cls.data

        class CustomGrab(Grab, SimpleExtension):
            pass

        SimpleExtension.get_data()['counter'] = 0
        g = CustomGrab()
        #self.assertEqual(SimpleExtension.get_data()['counter'], 1)

        class VeryCustomGrab(CustomGrab):
            pass

        SimpleExtension.get_data()['counter'] = 0
        g = VeryCustomGrab()
        #self.assertEqual(SimpleExtension.get_data()['counter'], 1)


        # TODO: what did I mean? :)
        # Anyway it does not work now :)
        #class VeryCustomGrab(CustomGrab, SimpleExtension):
            #pass

        #SimpleExtension.get_data()['counter'] = 0
        #g = VeryCustomGrab()
        #self.assertEqual(SimpleExtension.get_data()['counter'], 2)

    @ignore_transport('grab.transport.kit.KitTransport')
    def test_request_counter(self):
        import grab.base
        import itertools
        import threading

        grab.base.REQUEST_COUNTER = itertools.count(1)
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.request_counter, 1)

        g.go(SERVER.BASE_URL)
        self.assertEqual(g.request_counter, 2)

        def func():
            g = build_grab()
            g.go(SERVER.BASE_URL)

        # Make 10 requests in concurrent threads
        threads = []
        for x in xrange(10):
            th = threading.Thread(target=func)
            threads.append(th)
            th.start()
        for th in threads:
            th.join()

        g.go(SERVER.BASE_URL)
        self.assertEqual(g.request_counter, 13)

########NEW FILE########
__FILENAME__ = grab_charset
# coding: utf-8
"""
This test fails in py3.3 environment because `grab.response.body`
contains <str>, but it should contains <bytes>
"""
from unittest import TestCase
import json

from grab import Grab, GrabMisuseError
from test.util import ignore_transport, only_transport, build_grab
from test.server import SERVER
from grab.extension import register_extensions

from grab.util.py3k_support import *

class GrabCharsetDetectionTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_document_charset_option(self):
        g = build_grab()
        SERVER.RESPONSE['get'] = 'foo'
        g.go(SERVER.BASE_URL)
        self.assertEqual('foo', g.response.body)

        g = build_grab()
        SERVER.RESPONSE['get'] = u''.encode('utf-8')
        g.go(SERVER.BASE_URL)
        self.assertEqual(u''.encode('utf-8'), g.response.body)
        self.assertEqual(g.response.charset, 'utf-8')

        g = Grab(transport=GRAB_TRANSPORT, document_charset='cp1251')
        SERVER.RESPONSE['get'] = u''.encode('cp1251')
        g.go(SERVER.BASE_URL)
        self.assertEqual(u''.encode('cp1251'), g.response.body)
        self.assertEqual(g.response.charset, 'cp1251')

########NEW FILE########
__FILENAME__ = grab_charset_issue
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound

from test.util import build_grab
from test.server import SERVER

from grab.util.py3k_support import *

class LXMLExtensionTest(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_dash_issue(self):
        HTML = '<strong>&#151;</strong>'
        SERVER.RESPONSE['get'] = HTML
        g = build_grab()
        g.go(SERVER.BASE_URL)

        # By default &#[128-160]; are fixed
        self.assertFalse(g.xpath_one('//strong/text()') == unichr(151))
        self.assertTrue(g.xpath_one('//strong/text()') == unichr(8212))

        # disable fix-behaviour
        g.setup(fix_special_entities=False)
        g.go(SERVER.BASE_URL)

        # By default &#[128-160]; are fixed
        self.assertTrue(g.xpath_one('//strong/text()') == unichr(151))
        self.assertFalse(g.xpath_one('//strong/text()') == unichr(8212))

        # Explicitly use unicode_body func
        g = build_grab()
        g.go(SERVER.BASE_URL)
        print(':::', g.response.unicode_body())
        self.assertTrue('&#8212;' in g.response.unicode_body())

########NEW FILE########
__FILENAME__ = grab_cookies
# coding: utf-8
from unittest import TestCase
#import string
import json

from grab import Grab, GrabMisuseError
from test.util import TMP_FILE, build_grab
from test.server import SERVER

class TestCookies(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_parsing_response_cookies(self):
        g = build_grab()
        SERVER.RESPONSE['cookies'] = {'foo': 'bar', '1': '2'}
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.cookies['foo'], 'bar')

    def test_multiple_cookies(self):
        g = build_grab()
        SERVER.RESPONSE['cookies'] = {}
        g.setup(cookies={'foo': '1', 'bar': '2'})
        g.go(SERVER.BASE_URL)
        self.assertEqual(
            set(map(lambda item: item.strip(),
                    SERVER.REQUEST['headers']['Cookie'].split('; '))),
            set(['foo=1', 'bar=2']))

    def test_session(self):
        # Test that if Grab gets some cookies from the server
        # then it sends it back
        g = build_grab()
        g.setup(reuse_cookies=True)
        SERVER.RESPONSE['cookies'] = {'foo': 'bar'}
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.cookies['foo'], 'bar')
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['Cookie'], 'foo=bar')
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['Cookie'], 'foo=bar')

        # Test reuse_cookies=False
        g = build_grab()
        g.setup(reuse_cookies=False)
        SERVER.RESPONSE['cookies'] = {'foo': 'baz'}
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.cookies['foo'], 'baz')
        g.go(SERVER.BASE_URL)
        self.assertTrue(len(SERVER.REQUEST['cookies']) == 0)

        # Test something
        g = build_grab()
        g.setup(reuse_cookies=True)
        SERVER.RESPONSE['cookies'] = {'foo': 'bar'}
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.cookies['foo'], 'bar')
        g.clear_cookies()
        g.go(SERVER.BASE_URL)
        self.assertTrue(len(SERVER.REQUEST['cookies']) == 0)

    def test_redirect_session(self):
        g = build_grab()
        SERVER.RESPONSE['cookies'] = {'foo': 'bar'}
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.cookies['foo'], 'bar')

        # Setup one-time redirect
        g = build_grab()
        SERVER.RESPONSE['cookies'] = {}
        SERVER.RESPONSE_ONCE['headers'].append(('Location', SERVER.BASE_URL))
        SERVER.RESPONSE_ONCE['headers'].append(('Set-Cookie', 'foo=bar'))
        SERVER.RESPONSE_ONCE['code'] = 302
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['cookies']['foo'].value, 'bar')

    def test_load_dump(self):
        g = build_grab()
        cookies = {'foo': 'bar', 'spam': 'ham'}
        g.setup(cookies=cookies)
        g.go(SERVER.BASE_URL)
        g.dump_cookies(TMP_FILE)
        self.assertEqual(set(cookies.items()),
                         set((x['name'], x['value']) for x in json.load(open(TMP_FILE))))

        # Test non-ascii
        g = build_grab()
        cookies = {'foo': 'bar', 'spam': u''}
        g.setup(cookies=cookies)
        g.go(SERVER.BASE_URL)
        g.dump_cookies(TMP_FILE)
        self.assertEqual(set(cookies.items()),
                         set((x['name'], x['value']) for x in json.load(open(TMP_FILE))))

        # Test load cookies
        g = build_grab()
        cookies = [{'name': 'foo', 'value': 'bar'},
                   {'name': 'spam', 'value': u''}]
        json.dump(cookies, open(TMP_FILE, 'w'))
        g.load_cookies(TMP_FILE)
        self.assertEqual(set(g.cookies.items()),
                         set((x['name'], x['value']) for x in cookies))

    def test_cookiefile(self):
        g = build_grab()

        # Empty file should not raise Exception
        open(TMP_FILE, 'w').write('')
        g.setup(cookiefile=TMP_FILE)
        g.go(SERVER.BASE_URL)

        cookies = [{'name': 'spam', 'value': 'ham'}]
        json.dump(cookies, open(TMP_FILE, 'w'))

        # One cookie are sent in server reponse
        # Another cookies is passed via the `cookiefile` option
        SERVER.RESPONSE['cookies'] = {'godzilla': 'monkey'}
        g.setup(cookiefile=TMP_FILE)
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['cookies']['spam'].value, 'ham')

        # This is correct reslt of combining two cookies
        MERGED_COOKIES = [('godzilla', 'monkey'), ('spam', 'ham')]

        # g.cookies should contains merged cookies
        self.assertEqual(set(MERGED_COOKIES),
                         set(g.cookies.items()))

        # `cookiefile` file should contains merged cookies
        self.assertEqual(set(MERGED_COOKIES),
                         set((x['name'], x['value']) for x in json.load(open(TMP_FILE))))

        # Just ensure it works
        g.go(SERVER.BASE_URL)

    def test_manual_dns(self):
        import pycurl

        g = build_grab()
        g.transport.curl.setopt(pycurl.RESOLVE, ['foo:%d:127.0.0.1' % SERVER.PORT])
        SERVER.RESPONSE['get'] = 'zzz'
        g.go('http://foo:%d/' % SERVER.PORT)
        self.assertEqual(b'zzz', g.response.body)


    def test_different_domains(self):
        import pycurl

        g = build_grab()
        names = [
            'foo:%d:127.0.0.1' % SERVER.PORT,
            'bar:%d:127.0.0.1' % SERVER.PORT,
        ]
        g.transport.curl.setopt(pycurl.RESOLVE, names)

        SERVER.RESPONSE['cookies'] = {'foo': 'foo'}
        g.go('http://foo:%d' % SERVER.PORT)
        self.assertEqual(dict(g.response.cookies.items()), {'foo': 'foo'})

        SERVER.RESPONSE['cookies'] = {'bar': 'bar'}
        g.go('http://bar:%d' % SERVER.PORT)
        self.assertEqual(dict(g.response.cookies.items()), {'bar': 'bar'})

########NEW FILE########
__FILENAME__ = grab_debug
# coding: utf-8
from unittest import TestCase
import json
import os

from grab import Grab, GrabMisuseError
from test.server import SERVER
from test.util import TMP_FILE, TMP_DIR, clear_directory, build_grab
from grab.base import reset_request_counter

class TestCookies(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_log_option(self):
        clear_directory(TMP_DIR)
        reset_request_counter()

        log_file_path = os.path.join(TMP_DIR, 'log.html')
        g = build_grab()
        g.setup(log_file=log_file_path)
        SERVER.RESPONSE['get'] = 'omsk'

        self.assertEqual(os.listdir(TMP_DIR), [])
        g.go(SERVER.BASE_URL)
        self.assertEqual(os.listdir(TMP_DIR), ['log.html'])
        self.assertEqual(open(log_file_path).read(), 'omsk')


    def test_log_dir_option(self):
        clear_directory(TMP_DIR)
        reset_request_counter()

        g = build_grab()
        g.setup(log_dir=TMP_DIR)
        SERVER.RESPONSE_ONCE['get'] = 'omsk1'
        SERVER.RESPONSE['get'] = 'omsk2'

        self.assertEqual(os.listdir(TMP_DIR), [])
        g.go(SERVER.BASE_URL)
        g.go(SERVER.BASE_URL)
        self.assertEqual(sorted(os.listdir(TMP_DIR)), ['01.html', '01.log', '02.html', '02.log'])
        self.assertEqual(open(os.path.join(TMP_DIR, '01.html')).read(), 'omsk1')
        self.assertEqual(open(os.path.join(TMP_DIR, '02.html')).read(), 'omsk2')

    def test_log_dir_response_content(self):
        clear_directory(TMP_DIR)
        reset_request_counter()

        g = build_grab()
        g.setup(log_dir=TMP_DIR)
        SERVER.RESPONSE['get'] = 'omsk'
        SERVER.RESPONSE['headers'] = [('X-Engine', 'PHP')]

        self.assertEqual(os.listdir(TMP_DIR), [])
        g.go(SERVER.BASE_URL)
        self.assertEqual(sorted(os.listdir(TMP_DIR)), ['01.html', '01.log'])
        log_file_content = open(os.path.join(TMP_DIR, '01.log')).read()
        self.assertTrue('X-Engine' in log_file_content)


    def test_log_dir_request_content_is_empty(self):
        clear_directory(TMP_DIR)
        reset_request_counter()

        g = build_grab()
        g.setup(log_dir=TMP_DIR)
        g.setup(headers={'X-Name': 'spider'}, post='xxxPost')

        self.assertEqual(os.listdir(TMP_DIR), [])
        g.go(SERVER.BASE_URL)
        self.assertEqual(sorted(os.listdir(TMP_DIR)), ['01.html', '01.log'])
        log_file_content = open(os.path.join(TMP_DIR, '01.log')).read()
        self.assertFalse('X-Name' in log_file_content)
        self.assertFalse('xxxPost' in log_file_content)

    def test_log_dir_request_content_headers_and_post(self):
        clear_directory(TMP_DIR)
        reset_request_counter()

        g = build_grab()
        g.setup(log_dir=TMP_DIR, debug=True)
        g.setup(headers={'X-Name': 'spider'}, post={'xxx': 'Post'})

        self.assertEqual(os.listdir(TMP_DIR), [])
        g.go(SERVER.BASE_URL)
        self.assertEqual(sorted(os.listdir(TMP_DIR)), ['01.html', '01.log'])
        log_file_content = open(os.path.join(TMP_DIR, '01.log')).read()
        self.assertTrue('X-Name' in log_file_content)
        self.assertTrue('xxx=Post' in log_file_content)

########NEW FILE########
__FILENAME__ = grab_django
from unittest import TestCase

from test.server import SERVER
from test.util import build_grab

class GrabDjangoTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_response_django_file(self):
        from django.core.files.base import ContentFile

        SERVER.RESPONSE['get'] = 'zzz'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        the_file = g.doc.django_file()
        self.assertTrue(isinstance(the_file, ContentFile))
        self.assertEqual(b'zzz', the_file.read())
        self.assertEqual('', the_file.name)

    def test_response_django_file_with_name(self):
        from django.core.files.base import ContentFile

        SERVER.RESPONSE['get'] = 'zzz'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        the_file = g.doc.django_file(name='movie.flv')
        self.assertEqual(b'zzz', the_file.read())
        self.assertEqual('movie.flv', the_file.name)

########NEW FILE########
__FILENAME__ = grab_get_request
# coding: utf-8
from unittest import TestCase
import os

from grab import Grab, GrabMisuseError
from test.util import TMP_DIR, ignore_transport, only_transport, build_grab
from test.server import SERVER
from grab.extension import register_extensions


class GrabSimpleTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_get(self):
        SERVER.RESPONSE['get'] = 'Final Countdown'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertTrue(b'Final Countdown' in g.response.body)

    def test_body_content(self):
        SERVER.RESPONSE['get'] = 'Simple String'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertEqual(b'Simple String', g.response.body)
        #self.assertEqual('Simple String' in g.response.runtime_body)

    def test_status_code(self):
        SERVER.RESPONSE['get'] = 'Simple String'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertEqual(200, g.response.code)

    def test_parsing_response_headers(self):
        SERVER.RESPONSE['headers'] = [('Hello', 'Grab')]
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertTrue(g.response.headers['Hello'] == 'Grab')

    def test_depreated_hammer_mode_options(self):
        SERVER.RESPONSE['get'] = 'foo'
        g = build_grab()
        g.setup(hammer_mode=True)
        g.go(SERVER.BASE_URL)

        g.setup(hammer_timeouts=((1, 1), (2, 2)))
        g.go(SERVER.BASE_URL)

########NEW FILE########
__FILENAME__ = grab_js
# coding: utf-8
from unittest import TestCase
import os

from grab import Grab, GrabMisuseError
from test.util import build_grab, ignore_transport, only_transport
from test.server import SERVER
from grab.extension import register_extensions


class GrabSimpleTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    @only_transport('grab.transport.kit.KitTransport')
    def test_simple_js(self):
        SERVER.RESPONSE['get'] = ''''
            <body>
            <script>
                var num = 22 * 3;
                document.write(num);
            </script>
            </body>
        '''
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertTrue(b'66' in g.response.runtime_body)

    @only_transport('grab.transport.kit.KitTransport')
    def test_js_dom(self):
        SERVER.RESPONSE['get'] = ''''
            <body>
            <script>
                var num = 22 * 3;
                document.write('<span id="foo">' + num + '</span>');
            </script>
            </body>
        '''
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.doc.select('//span[@id="foo"]').text(), '66')

########NEW FILE########
__FILENAME__ = grab_limit_option
# coding: utf-8
from unittest import TestCase

from grab import Grab
from test.util import ignore_transport, build_grab
from test.server import SERVER

class TestContentLimit(TestCase):
    def setUp(self):
        SERVER.reset()

    @ignore_transport('grab.transport.requests.RequestsTransport')
    def test_nobody(self):
        g = build_grab()
        g.setup(nobody=True)
        SERVER.RESPONSE['get'] = 'foo'
        g.go(SERVER.BASE_URL)
        self.assertEqual(b'', g.response.body)
        self.assertTrue(len(g.response.head) > 0)

    @ignore_transport('grab.transport.requests.RequestsTransport')
    def test_body_maxsize(self):
        g = build_grab()
        g.setup(body_maxsize=100)
        SERVER.RESPONSE['get'] = 'x' * 1024 * 1024
        g.go(SERVER.BASE_URL)
        # Should be less 50kb
        self.assertTrue(len(g.response.body) < 50000)

########NEW FILE########
__FILENAME__ = grab_pickle
# coding: utf-8
from unittest import TestCase
try:
    import cPickle as pickle
except ImportError:
    import pickle
from multiprocessing import Process, Queue

from grab import Grab
from test.server import SERVER
from test.util import build_grab

class TestGrab(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_pickling(self):
        """
        Test that Grab instance could be pickled and unpickled.
        """

        g = build_grab()
        SERVER.RESPONSE['get'] = '<form><textarea name="text">the cat</textarea></form>'
        g.go(SERVER.BASE_URL)
        g.set_input('text', 'foobar')
        data = pickle.dumps(g, pickle.HIGHEST_PROTOCOL)

        def func(pickled_grab, resultq):
            g2 = pickle.loads(pickled_grab)
            text = g2.doc.select('//textarea').text()
            resultq.put(text)

        result_queue = Queue()
        #p = Process(target=func, args=[data, result_queue])
        #p.start()
        func(data, result_queue)

        text = result_queue.get(block=True, timeout=1)
        self.assertEqual(text, 'the cat')

########NEW FILE########
__FILENAME__ = grab_post_request
# coding: utf-8
from unittest import TestCase
try:
    from urllib import quote
except ImportError:
    from urllib.parse import quote

from grab import Grab, GrabMisuseError
from test.util import ignore_transport, build_grab
from test.server import SERVER
try:
    from urlparse import parse_qsl
except ImportError:
    from urllib.parse import parse_qsl

class TestPostFeature(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_post(self):
        g = build_grab(url=SERVER.BASE_URL, debug_post=True)

        # Provide POST data in dict
        g.setup(post={'foo': 'bar'})
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'foo=bar')

        # Provide POST data in tuple
        g.setup(post=(('foo', 'TUPLE'),))
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'foo=TUPLE')

        # Provide POST data in list
        g.setup(post=[('foo', 'LIST')])
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'foo=LIST')

        # Order of elements should not be changed (1)
        g.setup(post=[('foo', 'LIST'), ('bar', 'BAR')])
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'foo=LIST&bar=BAR')

        # Order of elements should not be changed (2)
        g.setup(post=[('bar', 'BAR'), ('foo', 'LIST')])
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'bar=BAR&foo=LIST')

        # Provide POST data in byte-string
        g.setup(post='Hello world!')
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'Hello world!')

        # Provide POST data in unicode-string
        g.setup(post=u'Hello world!')
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'Hello world!')

        # Provide POST data in non-ascii unicode-string
        g.setup(post=u', !')
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], u', !'.encode('utf-8'))

        # Two values with one key
        g.setup(post=(('foo', 'bar'), ('foo', 'baz')))
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], b'foo=bar&foo=baz')

    def assertEqualQueryString(self, qs1, qs2):
        args1 = set([(x, y[0]) for x, y in parse_qsl(qs1)])
        args2 = set([(x, y[0]) for x, y in parse_qsl(qs2)])
        self.assertEqual(args1, args2)

    @ignore_transport('grab.transport.requests.RequestsTransport')
    @ignore_transport('grab.transport.kit.KitTransport')
    def test_multipart_post(self):
        g = build_grab(url=SERVER.BASE_URL, debug_post=True)
        
        # Dict
        g.setup(multipart_post={'foo': 'bar'})
        g.request()
        self.assertTrue(b'name="foo"' in SERVER.REQUEST['post'])

        # Few values with non-ascii data
        # TODO: understand and fix
        # AssertionError: 'foo=bar&gaz=%D0%94%D0%B5%D0%BB%D1%8C%D1%84%D0%B8%D0%BD&abc=' != 'foo=bar&gaz=\xd0\x94\xd0\xb5\xd0\xbb\xd1\x8c\xd1\x84\xd0\xb8\xd0\xbd&abc='
        #g.setup(post=({'foo': 'bar', 'gaz': u'', 'abc': None}))
        #g.request()
        #self.assertEqual(SERVER.REQUEST['post'], 'foo=bar&gaz=&abc=')

        # Multipart data could not be string
        g.setup(multipart_post='asdf')
        self.assertRaises(GrabMisuseError, lambda: g.request())

        # tuple with one pair
        g.setup(multipart_post=(('foo', 'bar'),))
        g.request()
        self.assertTrue(b'name="foo"' in SERVER.REQUEST['post'])

        # tuple with two pairs
        g.setup(multipart_post=(('foo', 'bar'), ('foo', 'baz')))
        g.request()
        self.assertTrue(b'name="foo"' in SERVER.REQUEST['post'])

    def test_unicode_post(self):
        # By default, unicode post should be converted into utf-8
        g = build_grab()
        data = u''
        g.setup(post=data, url=SERVER.BASE_URL)
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], data.encode('utf-8'))

        # Now try cp1251 with charset option
        SERVER.REQUEST['charset'] = 'cp1251'
        g = build_grab()
        data = u''
        g.setup(post=data, url=SERVER.BASE_URL, charset='cp1251', debug=True)
        g.request()
        self.assertEqual(SERVER.REQUEST['post'], data.encode('cp1251'))

        # Now try dict with unicode value & charset option
        SERVER.REQUEST['charset'] = 'cp1251'
        g = build_grab()
        data = u''
        g.setup(post={'foo': data}, url=SERVER.BASE_URL, charset='cp1251', debug=True)
        g.request()
        test = 'foo=%s' % quote(data.encode('cp1251'))
        test = test.encode('utf-8') # py3 hack
        self.assertEqual(SERVER.REQUEST['post'], test)

    def test_put(self):
        g = build_grab()
        g.setup(post=b'abc', url=SERVER.BASE_URL, method='put', debug=True)
        SERVER.REQUEST['debug'] = True
        g.request()
        self.assertEqual(SERVER.REQUEST['method'], 'PUT')
        self.assertEqual(SERVER.REQUEST['headers']['content-length'], '3')

    def test_patch(self):
        g = build_grab()
        g.setup(post='abc', url=SERVER.BASE_URL, method='patch')
        g.request()
        self.assertEqual(SERVER.REQUEST['method'], 'PATCH')
        self.assertEqual(SERVER.REQUEST['headers']['content-length'], '3')

########NEW FILE########
__FILENAME__ = grab_proxy
# coding: utf-8
from unittest import TestCase

from grab import Grab, GrabMisuseError
from test.util import build_grab, TMP_FILE
from test.server import SERVER

from grab.util.py3k_support import *

PROXY1 = 'localhost:%d' % SERVER.PORT
PROXY2 = 'localhost:%d' % SERVER.EXTRA_PORT1
PROXY3 = 'localhost:%d' % SERVER.EXTRA_PORT2

class TestProxy(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_proxy_option(self):
        g = build_grab()

        g.setup(proxy=PROXY1, proxy_type='http', debug=True)
        SERVER.RESPONSE['get'] = '123'

        g.go('http://yandex.ru')
        self.assertEqual(b'123', g.response.body)
        self.assertEqual('yandex.ru', SERVER.REQUEST['headers']['host'])

    def test_deprecated_setup_proxylist(self):
        g = build_grab()
        open(TMP_FILE, 'w').write(PROXY1)
        g.load_proxylist(TMP_FILE, 'text_file')
        SERVER.RESPONSE['get'] = '123'
        g.change_proxy()
        g.go('http://yandex.ru')
        self.assertEqual(b'123', g.response.body)
        self.assertEqual('yandex.ru', SERVER.REQUEST['headers']['host'])

    def test_load_proxylist(self):
        content = '%s\n%s\n%s' % (PROXY1, PROXY2, PROXY3)
        open(TMP_FILE, 'w').write(content)

        # By default auto_change is True
        g = build_grab()
        g.load_proxylist(TMP_FILE, 'text_file')
        self.assertEqual(g.config['proxy_auto_change'], True)
        servers = set()
        for x in xrange(10):
            g.go('http://yandex.ru')
            servers.add(g.config['proxy'])

        self.assertTrue(len(servers) > 1)

        # Disable auto_change
        # By default auto_init is True
        g = build_grab()
        g.load_proxylist(TMP_FILE, 'text_file', auto_change=False)
        self.assertEqual(g.config['proxy_auto_change'], False)
        servers = set()
        for x in xrange(10):
            g.go('http://yandex.ru')
            servers.add(g.config['proxy'])
        self.assertEqual(len(servers), 1)

        # Disable auto_change
        # Disable auto_init
        # Proxylist will not be used by default
        g = build_grab()
        g.load_proxylist(TMP_FILE, 'text_file', auto_change=False,
                         auto_init=False)
        self.assertEqual(g.config['proxy_auto_change'], False)
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.config['proxy'], None)

    #def test_memory_proxylist(self):
        #g = build_grab()
        #server_list = ['localhost:%d' % PORT1]
        #g.setup_proxylist(server_list=server_list, proxy_type='http',
                          #auto_init=True)
        #SERVER.RESPONSE['get'] = '123'
        #g.go('http://yandex.ru')
        #self.assertEqual('123', g.response.body)
        #self.assertEqual('yandex.ru', SERVER.REQUEST['headers']['host'])

    def test_change_proxy(self):
        g = build_grab()
        with open(TMP_FILE, 'w') as out:
            for x in xrange(10):
                out.write('server-%d:777\n' % x)

        g.load_proxylist(TMP_FILE, 'text_file', auto_init=False, auto_change=False)
        self.assertEqual(g.config['proxy'], None)

        g.load_proxylist(TMP_FILE, 'text_file', auto_init=False, auto_change=True)
        self.assertEqual(g.config['proxy'], None)

        g.load_proxylist(TMP_FILE, 'text_file', auto_init=True, auto_change=False)
        self.assertTrue('server-' in g.config['proxy'])

    def test_proxylist_api(self):
        g = build_grab()
        #self.assertRaises(GrabMisuseError,
                          #lambda: g.setup_proxylist(proxy_file='foo', server_list=[]))

########NEW FILE########
__FILENAME__ = grab_redirect
from unittest import TestCase

from grab import Grab
from grab.error import GrabTooManyRedirectsError
from test.server import SERVER
from test.util import GRAB_TRANSPORT, only_transport

class RedirectController(object):
    def __init__(self, counter):
        self.setup_counter(counter)

    def setup_counter(self, counter):
        self.counter = counter

    def request_handler(self, server):
        if self.counter:
            server.set_status(301)
            server.set_header('Location', SERVER.BASE_URL)
        else:
            server.set_status(200)
        self.counter -= 1


class RefreshRedirectController(RedirectController):
    def request_handler(self, server):
        server.set_status(200)
        if self.counter:
            server.write('<html><head><meta http-equiv="refresh" content="5"></head>')
        else:
            server.write('OK')
        self.counter -= 1


class GrabRedirectTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_meta_refresh_redirect(self):
        # By default meta-redirect is off
        meta_url = SERVER.BASE_URL + '/foo'

        SERVER.RESPONSE_ONCE['get'] = '<meta http-equiv="refresh" content="5; url=%s">' % meta_url
        g = Grab(transport=GRAB_TRANSPORT)
        g.go(SERVER.BASE_URL + '/')
        self.assertEqual(SERVER.REQUEST['path'], '/')
        self.assertEqual(g.response.url, SERVER.BASE_URL + '/')

        # Now test meta-auto-redirect
        SERVER.RESPONSE_ONCE['get'] = '<meta http-equiv="refresh" content="5; url=%s">' % meta_url
        g = Grab(transport=GRAB_TRANSPORT)
        g.setup(follow_refresh=True)
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['path'], '/foo')
        self.assertEqual(g.response.url, meta_url)

        # Test spaces in meta tag
        SERVER.RESPONSE_ONCE['get'] = "<meta http-equiv='refresh' content='0;url= %s'>" % meta_url
        g = Grab(transport=GRAB_TRANSPORT)
        g.setup(follow_refresh=True)
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['path'], '/foo')
        self.assertEqual(g.response.url, meta_url)

    @only_transport('grab.transport.curl.CurlTransport')
    def test_redirect_limit(self):
        ctl = RedirectController(10)
        SERVER.RESPONSE['get_callback'] = ctl.request_handler

        g = Grab(transport=GRAB_TRANSPORT)
        g.setup(redirect_limit=5)

        self.assertRaises(GrabTooManyRedirectsError,
                          lambda: g.go(SERVER.BASE_URL))

        ctl.setup_counter(10)
        g.setup(redirect_limit=20)
        g.go(SERVER.BASE_URL)

    @only_transport('grab.transport.curl.CurlTransport')
    def test_refresh_redirect_limit(self):
        ctl = RefreshRedirectController(10)
        SERVER.RESPONSE['get_callback'] = ctl.request_handler

        g = Grab(transport=GRAB_TRANSPORT)
        g.setup(redirect_limit=5, follow_refresh=False)
        g.go(SERVER.BASE_URL)

        ctl.setup_counter(10)
        g.setup(redirect_limit=5, follow_refresh=True)
        self.assertRaises(GrabTooManyRedirectsError,
                          lambda: g.go(SERVER.BASE_URL))

########NEW FILE########
__FILENAME__ = grab_response_body_processing
# coding: utf-8
from unittest import TestCase
import os

from grab import Grab, GrabMisuseError
from test.util import TMP_DIR, ignore_transport, only_transport, build_grab
from test.server import SERVER


class GrabSimpleTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    @ignore_transport('grab.transport.kit.KitTransport')
    def test_body_inmemory(self):
        g = build_grab()
        g.setup(body_inmemory=False)
        self.assertRaises(GrabMisuseError, lambda: g.go(SERVER.BASE_URL))

        SERVER.RESPONSE['get'] = 'foo'
        g = build_grab()
        g.setup(body_inmemory=False)
        g.setup(body_storage_dir=TMP_DIR)
        g.go(SERVER.BASE_URL)
        #self.assertTrue(os.path.exists(g.response.body_path))
        #self.assertTrue(TMP_DIR in g.response.body_path)
        #self.assertEqual('foo', open(g.response.body_path).read())
        #old_path = g.response.body_path

        #g.go(SERVER.BASE_URL)
        #self.assertTrue(old_path != g.response.body_path)

        #SERVER.RESPONSE['get'] = 'foo'
        #g = build_grab()
        #g.setup(body_inmemory=False)
        #g.setup(body_storage_dir=TMP_DIR)
        #g.setup(body_storage_filename='musik.mp3')
        #g.go(SERVER.BASE_URL)
        #self.assertTrue(os.path.exists(g.response.body_path))
        #self.assertTrue(TMP_DIR in g.response.body_path)
        #self.assertEqual('foo', open(g.response.body_path).read())
        #self.assertEqual(os.path.join(TMP_DIR, 'musik.mp3'), g.response.body_path)
        #self.assertEqual(g.response.body, 'foo')
        #self.assertEqual(g.response._cached_body, None)

########NEW FILE########
__FILENAME__ = grab_transport
from unittest import TestCase
import pickle

from grab import Grab
from test.server import SERVER
from grab.transport.curl import CurlTransport

class FakeTransport(CurlTransport):
    def prepare_response(self, grab):
        resp = super(FakeTransport, self).prepare_response(grab)
        resp.body = b'Faked ' + resp.body
        return resp


def get_curl_transport():
    return CurlTransport()


def get_fake_transport():
    return FakeTransport()


class TestTransportTestCase(TestCase):
    def setUp(self):
        SERVER.reset()
        SERVER.RESPONSE['get'] = 'XYZ'

    def transport_option_logic(self, curl_transport, fake_transport):
        g = Grab(transport=curl_transport)
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.body, b'XYZ')

        g2 = g.clone()
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.body, b'XYZ')

        g2_data = pickle.dumps(g2, pickle.HIGHEST_PROTOCOL)
        g3 = pickle.loads(g2_data)
        g3.go(SERVER.BASE_URL)
        self.assertEqual(g3.response.body, b'XYZ')

        g = Grab(transport=fake_transport)
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.body, b'Faked XYZ')

        g2 = g.clone()
        g.go(SERVER.BASE_URL)
        self.assertEqual(g.response.body, b'Faked XYZ')

        g2_data = pickle.dumps(g2, pickle.HIGHEST_PROTOCOL)
        g3 = pickle.loads(g2_data)
        g3.go(SERVER.BASE_URL)
        self.assertEqual(g3.response.body, b'Faked XYZ')

    def test_transport_option_as_string(self):
        self.transport_option_logic(
            'grab.transport.curl.CurlTransport',
            'test.case.grab_transport.FakeTransport',
        )

    def test_transport_option_as_class(self):
        self.transport_option_logic(
            CurlTransport,
            FakeTransport,
        )

    def test_transport_option_as_function(self):
        self.transport_option_logic(
            get_curl_transport,
            get_fake_transport,
        )

########NEW FILE########
__FILENAME__ = grab_transport_mock
from unittest import TestCase
from grab import Grab
from grab.transport.mock import GrabMock, GrabMockNotFoundError, MOCK_REGISTRY

from test.util import build_grab
from test.server import SERVER

MOCK_TRANSPORT = 'grab.transport.mock.MockTransport'

class GrabTransortMockTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_integrity(self):
        g = build_grab(transport=MOCK_TRANSPORT)

    def test_missed_url(self):
        g = build_grab(transport=MOCK_TRANSPORT)
        self.assertRaises(GrabMockNotFoundError,
                          lambda: g.go('http://yandex.ru'))

    def test_registry(self):
        g = build_grab(transport=MOCK_TRANSPORT)
        MOCK_REGISTRY['http://yandex.ru/'] = {'body': 'foo'}
        g.go('http://yandex.ru/')
        self.assertEqual(g.response.body, 'foo')
        self.assertEqual(g.response.code, 200)

    def test_grabmock(self):
        g = GrabMock()
        MOCK_REGISTRY['http://yandex.ru/'] = {'body': 'foo'}
        g.go('http://yandex.ru/')
        self.assertEqual(g.response.body, 'foo')

########NEW FILE########
__FILENAME__ = grab_upload_file
# coding: utf-8
from unittest import TestCase
from grab import Grab, UploadContent

from test.util import build_grab
from test.server import SERVER

FORMS = u"""
<head>
    <title>Title</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
</head>
<body>
    <div id="header">
        <form id="search_form" method="GET">
            <input id="search_box" name="query" value="" />
            <input type="submit" value="submit" class="submit_btn" name="submit" />
        </form>
    </div>
    <div id="content">
        <FORM id="common_form" method="POST">
          <input id="some_value" name="some_value" value="" />
          <input id="some_value" name="image" type="file" value="" />
          <select id="gender" name="gender">
              <option value="1">Female</option>
              <option value="2">Male</option>
           </select>
           <input type="submit" value="submit" class="submit_btn" name="submit" />
        </FORM>
        <h1 id="fake_form">Big header</h1>
        <form name="dummy" action="/dummy">
           <input type="submit" value="submit" class="submit_btn" name="submit" />
        </form>
    </div>
</body>
""".encode('utf-8')

class TestUploadContent(TestCase):
    def setUp(self):
        SERVER.reset()
        # Create fake grab instance with fake response
        self.g = build_grab()
        self.g.fake_response(FORMS, charset='utf-8')

    def test(self):
        fc = UploadContent('a')
        self.assertEqual(fc, 'xxx')
        self.g.set_input('image', fc)

########NEW FILE########
__FILENAME__ = grab_user_agent
# coding: utf-8
from unittest import TestCase

from grab import Grab, GrabMisuseError
from test.util import build_grab, ignore_transport, only_transport
from test.server import SERVER


class GrabSimpleTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    @only_transport('grab.transport.curl.CurlTransport')
    def test_empty_useragent_pycurl(self):
        g = build_grab()

        # Empty string disable default pycurl user-agent
        g.setup(user_agent='')
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers'].get('user-agent', ''), '')

    @ignore_transport('ghost.GhostTransport')
    def test_useragent_simple(self):
        g = build_grab()

        # Simple case: setup user agent manually
        g.setup(user_agent='foo')
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['user-agent'], 'foo')

    @ignore_transport('ghost.GhostTransport')
    def test_useragent(self):
        g = build_grab()

        # Null value activates default random user-agent
        # For some transports it just allow them to send default user-agent
        # like in Kit transport case
        g = build_grab()
        g.setup(user_agent=None)
        g.go(SERVER.BASE_URL)
        self.assertTrue(len(SERVER.REQUEST['headers']) > 0)
        self.assertFalse('PycURL' in SERVER.REQUEST['headers']['user-agent'])

        # By default user_agent is None => random user agent is generated
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertTrue(len(SERVER.REQUEST['headers']) > 0)
        self.assertFalse('PycURL' in SERVER.REQUEST['headers']['user-agent'])

        # Simple case: setup user agent manually
        g.setup(user_agent='foo')
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['user-agent'], 'foo')
        
        # user agent from file should be loaded
        path = '/tmp/__ua.txt'
        open(path, 'w').write('GOD')
        g.setup(user_agent=None, user_agent_file=path)
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['user-agent'], 'GOD')

        # random user agent from file should be loaded
        path = '/tmp/__ua.txt'
        open(path, 'w').write('GOD1\nGOD2')
        g.setup(user_agent=None, user_agent_file=path)
        g.go(SERVER.BASE_URL)
        self.assertTrue(SERVER.REQUEST['headers']['user-agent'] in ('GOD1', 'GOD2'))
        ua = g.config['user_agent']

        # User-agent should not change
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['user-agent'], ua)

        # User-agent should not change
        g.go(SERVER.BASE_URL)
        self.assertEqual(SERVER.REQUEST['headers']['user-agent'], ua)

########NEW FILE########
__FILENAME__ = grab_xml_processing
# coding: utf-8
from unittest import TestCase

from grab import Grab
from test.util import ignore_transport, only_transport, build_grab
from test.server import SERVER

class GrabXMLProcessingTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_xml_with_declaration(self):
        SERVER.RESPONSE['get'] = b'<?xml version="1.0" encoding="UTF-8"?><root><foo>foo</foo></root>'
        g = build_grab()
        g.go(SERVER.BASE_URL)
        self.assertTrue(g.xpath_one('//foo').text == 'foo')

    def test_declaration_bug(self):
        """
        1. Build Grab instance with XML with xml declaration
        2. Call search method
        3. Call xpath
        4. Get ValueError: Unicode strings with encoding declaration are not supported.
        """
        xml = b'<?xml version="1.0" encoding="UTF-8"?><tree><leaf>text</leaf></tree>'
        g = build_grab(document_body=xml)
        self.assertTrue(g.search(u'text'))
        self.assertEqual(g.xpath_one('//leaf').text, u'text')

        # Similar bugs
        g = build_grab(document_body=xml)
        self.assertTrue(g.rex(u'text'))
        self.assertEqual(g.xpath_one('//leaf').text, u'text')

########NEW FILE########
__FILENAME__ = item
# coding: utf-8
from unittest import TestCase

import os
import sys

# Hack environment to force import "item" module from grab/item.py location
root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.insert(0, root)
from lxml.html import fromstring

from grab import Grab, DataNotFound
from grab.item import (Item, IntegerField, StringField, DateTimeField, func_field,
                       FuncField)
from test.util import build_grab
from grab.tools.lxml_tools import get_node_text, parse_html
from grab.selector import XpathSelector
from grab.error import GrabMisuseError

XML = b"""<?xml version='1.0' encoding='utf-8'?>
<bbapi version='1'>
    <player id='26982032' retrieved='2012-09-11T07:38:44Z'>
        <firstName>Ardeshir</firstName>
        <lastName>Lohrasbi</lastName>
        <nationality id='89'>Pakistan</nationality>
        <age>19</age>
        <height>75</height>
        <dmi>14300</dmi>
        <comment>abc</comment>
        <comment_cdata><![CDATA[abc]]></comment_cdata>
        <games>
            <game name="quake1"></game>
            <game name="quake2"></game>
        </games>
    </player>
</bbapi>
"""

def calculated_func2(item, sel):
    if not hasattr(item, 'count2'):
        item.count2 = 1
    else:
        item.count2 += 1
    return sel.select('//height').text() + '-zoo2-' + str(item.count2)


class Player(Item):
    id = IntegerField('//player/@id')
    first_name = StringField('//player/firstname')
    retrieved = DateTimeField('//player/@retrieved', '%Y-%m-%dT%H:%M:%SZ')
    comment = StringField('//player/comment')
    comment_cdata = StringField('//player/comment_cdata')

    data_not_found = StringField('//data/no/found')

    @func_field()
    def calculated(item, sel):
        if not hasattr(item, 'count'):
            item.count = 1
        else:
            item.count += 1
        return sel.select('//height').text() + '-zoo-' + str(item.count)

    calculated2 = FuncField(calculated_func2, pass_item=True)

    @func_field()
    def height1(item, sel):
        return sel.select('//height').number()

    height2 = FuncField(lambda sel: sel.select('//height').number())


class GameItem(Item):
    class Meta:
        find_query = '//games/game'

    name = StringField('@name')
    names = StringField('@name', multiple=True)


class ItemTestCase(TestCase):
    def get_item(self, content_type=None):
        grab = build_grab()
        if content_type is not None:
            grab.setup(content_type=content_type)
        grab.fake_response(XML)
        player = Player(grab.tree)
        return player

    def test_integer_field(self):
        player = self.get_item()
        self.assertEquals(26982032, player.id)

    def test_string_field(self):
        player = self.get_item()
        self.assertEquals('Ardeshir', player.first_name)

    def test_datetime_field(self):
        player = self.get_item()
        self.assertEquals('2012-09-11 07:38:44', str(player.retrieved))

    def test_item_cache_feature(self):
        player = self.get_item()

        self.assertEquals('75-zoo-1', player.calculated)
        # should got from cache
        self.assertEquals('75-zoo-1', player.calculated)

        # test assigning value
        player.calculated = 'baz'
        self.assertEquals('baz', player.calculated)

        # test FuncField
        self.assertEquals('75-zoo2-1', player.calculated2)
        # should got from cache
        self.assertEquals('75-zoo2-1', player.calculated2)


    def test_dom_builder(self):
        player = self.get_item()

        # By default comment_cdata attribute contains empty string
        # because HTML DOM builder is used by default
        self.assertEquals('abc', player.comment)
        self.assertEquals('', player.comment_cdata)

        # We can control default DOM builder with
        # content_type option
        player = self.get_item(content_type='xml')

        self.assertEquals('abc', player.comment)
        self.assertEquals('abc', player.comment_cdata)

        self.assertRaises(DataNotFound, lambda: player.data_not_found)

    def test_func_field_decorator(self):
        player = self.get_item()
        self.assertEquals(75, player.height1)

    def test_func_field(self):
        player = self.get_item()
        self.assertEquals(75, player.height2)

    def test_get_function(self):
        func = Player.get_function('height1')
        html = '<html><body><height>3'
        self.assertEquals(3, func(XpathSelector(parse_html(html))))

        func = Player.get_function('height2')
        html = '<html><body><height>3'
        self.assertEquals(3, func(XpathSelector(parse_html(html))))

    def test_func_field_warning(self):
        """
        Test that usage of func_field decorators without "()"
        raises exception.
        """

        def foo():
            class TestItem(Item):
                @func_field
                def foo(self, sel):
                    return 'test'

        self.assertRaises(GrabMisuseError, foo)

        def foo():
            class TestItem(Item):
                @func_field()
                def foo(self, sel):
                    return 'test'
            
            return TestItem(fromstring('<div></div>')).foo

        self.assertEqual('test', foo())

    def test_unknown_selector_type(self):
        class TestItem(Item):
            pass

        TestItem(None)

        self.assertRaises(GrabMisuseError,
            lambda: TestItem(None, selector_type='Batman Selector'))

    def test_find(self):
        grab = build_grab()
        grab.fake_response(XML)
        games = list(GameItem.find(grab.doc))
        self.assertEqual(['quake1', 'quake2'],
                         [x.name for x in games])

    def test_stringfield_multiple(self):
        grab = build_grab()
        grab.fake_response(XML)

        class GameItem(Item):
            names = StringField('//game/@name', multiple=True)

        game = GameItem(grab.tree)
        self.assertEqual(['quake1', 'quake2'], game.names)

    def test_item_inheritance(self):
        class BaseItem(Item):
            class Meta:
                find_query = '//player'

            name = StringField('firstname')
            age = IntegerField('age')

        class ChildItem(BaseItem):
            name = StringField('lastname')

        grab = build_grab(document_body=XML)
        items = list(BaseItem.find(grab.doc))
        self.assertEqual(items[0].name, 'Ardeshir')
        self.assertEqual(items[0].age, 19)
        self.assertEqual(set(['name', 'age']), set(items[0]._fields.keys()))

        items = list(ChildItem.find(grab.doc))
        self.assertEqual(items[0].age, 19)
        self.assertEqual(set(['name', 'age']), set(items[0]._fields.keys()))


class JsonSelectorTestCase(TestCase):
    class PlanetItem(Item):
        class Meta:
            find_query = '$..planets[*]'
            selector_type = 'json'

        name = StringField('name')

    def setUp(self):
        self.data = {
            'existence': {
                'worldA': {
                    'id': 1,
                    'planets': [
                        {'name': 'Earth', 'cities': ['Moscow', 'Paris', 'Tokio'],
                         'population': 7000000000},
                        {'name': 'Mars', 'cities': [], 'population': 0},
                    ],
                },
                'worldB': {
                    'id': 2,
                    'planets': [
                        {'name': 'Onyx', 'cities': ['Oyyx', 'Yiiix'], 'population': 8000000},
                    ],
                },
            },
        }

    def it_just_works(self):
        item = self.PlanetItem(self.data, selector_type='json')

    def test_find(self):
        planets = list(self.PlanetItem.find(self.data))
        self.assertEqual(len(planets), 3)

    def test_string_field(self):
        planets = list(self.PlanetItem.find(self.data))
        self.assertEqual(set(['Earth', 'Mars', 'Onyx']),
                         set(x.name for x in planets))


########NEW FILE########
__FILENAME__ = kit_extension
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound

from test.server import SERVER

GRAB_TRANSPORT = 'grab.transport.kit.KitTransport'

HTML = u"""
<html>
    <body>
        <h1>test</h1>
    </body>
</html>
"""

class KitExtensionTest(TestCase):
    def setUp(self):
        SERVER.reset()
        SERVER.RESPONSE['get'] = HTML
        self.g = Grab(transport=GRAB_TRANSPORT)
        self.g.go(SERVER.BASE_URL)

    def test_extension_in_general(self):
        self.assertTrue(self.g.kit)

    def test_select_method(self):
        self.assertEqual('test', self.g.kit.select('h1').text())

########NEW FILE########
__FILENAME__ = kit_live_sites
# coding: utf-8
from unittest import TestCase

from grab import Grab, GrabMisuseError

GRAB_TRANSPORT = 'grab.transport.kit.KitTransport'

class KitLiveSitesTestCase(TestCase):
    pass
    #def test_dumpz_copyright(self):
        #g = Grab(transport=GRAB_TRANSPORT)
        #g.go('http://dumpz.org')
        #self.assertTrue('Grigoriy Petukhov' in g.response.body)

    #def test_dumpz_codemirror(self):
        #g = Grab(transport='grab.transport.curl.CurlTransport')
        #g.go('http://dumpz.org')
        #self.assertFalse('<div class="CodeMirror' in g.response.runtime_body)

        #g = Grab(transport=GRAB_TRANSPORT)
        #g.go('http://dumpz.org')
        ## Dumpz.org contains javascript editor CodeMirror
        ## that builds some HTML in run-time
        #self.assertTrue('<div class="CodeMirror' in g.response.runtime_body)

    #def test_show_gui(self):
        #g = Grab(transport=GRAB_TRANSPORT)
        #g.transport.kit.view.show()
        #g.go('http://dev.twitter.com/')
        #import pdb; pdb.set_trace()
        #g.transport.kit.page.mainFrame().documentElement().findAll('a')[6].toPlainText();
        #g.transport.kit.page.mainFrame().documentElement().findAll('a')[6].evaluateJavaScript('this.click()');


########NEW FILE########
__FILENAME__ = proxy
# coding: utf-8
from unittest import TestCase
#import string
import json
import re

from grab import Grab, GrabMisuseError
from test.util import TMP_FILE, GRAB_TRANSPORT, get_temp_file
from test.server import SERVER
from grab.proxy import ProxyList

DEFAULT_PLIST_DATA = \
    '1.1.1.1:8080\n'\
    '1.1.1.2:8080\n'

class GrabProxyTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def generate_plist_file(self, data=DEFAULT_PLIST_DATA):
        path = get_temp_file()
        with open(path, 'w') as out:
            out.write(data)
        return path

    def test_basic(self):
        g = Grab(transport=GRAB_TRANSPORT)
        self.assertEqual(0, len(g.proxylist.proxy_list))


class ProxyListTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_basic(self):
        pl = ProxyList()
        self.assertEqual(0, len(pl.proxy_list))

    def generate_plist_file(self, data=DEFAULT_PLIST_DATA):
        path = get_temp_file()
        with open(path, 'w') as out:
            out.write(data)
        return path

    def test_file_source(self):
        pl = ProxyList()
        path = self.generate_plist_file()
        pl.set_source('file', location=path)
        self.assertEqual(2, len(pl.proxy_list))

    def test_remote_load(self):
        pl = ProxyList()
        SERVER.RESPONSE['get'] = DEFAULT_PLIST_DATA
        pl.set_source('url', url=SERVER.BASE_URL)
        self.assertEqual(2, len(pl.proxy_list))

    def test_accumulate_updates_basic(self):
        # test that all work with disabled accumulate_updates feature
        pl = ProxyList()
        path = self.generate_plist_file()
        pl.setup(accumulate_updates=False)
        pl.set_source('file', location=path)
        self.assertEqual(2, len(pl.proxy_list))

        # enable accumulate updates
        pl = ProxyList()
        pl.setup(accumulate_updates=True)
        path = self.generate_plist_file()
        pl.set_source('file', location=path)
        self.assertEqual(2, len(pl.proxy_list))

    def test_accumulate_updates_basic(self):
        pl = ProxyList()
        pl.setup(accumulate_updates=True)

        # load initial list
        path = self.generate_plist_file('foo:1\nbar:1')
        pl.set_source('file', location=path)
        self.assertEqual(2, len(pl.proxy_list))

        # load list with one new and one old proxies
        with open(path, 'w') as out:
            out.write('foo:1\nbaz:1')
        pl.reload(force=True)
        self.assertEqual(3, len(pl.proxy_list))

    def test_get_next_proxy(self):
        pl = ProxyList()
        path = self.generate_plist_file('foo:1\nbar:1')
        pl.set_source('file', location=path)
        self.assertEqual(pl.get_next_proxy().server, 'foo')
        self.assertEqual(pl.get_next_proxy().server, 'bar')
        self.assertEqual(pl.get_next_proxy().server, 'foo')
        pl.set_source('file', location=path)
        self.assertEqual(pl.get_next_proxy().server, 'foo')

    def test_get_next_proxy_in_accumulate_mode(self):
        pl = ProxyList()
        pl.setup(accumulate_updates=True)

        path = self.generate_plist_file('foo:1\nbar:1')
        pl.set_source('file', location=path)
        self.assertEqual(pl.get_next_proxy().server, 'foo')

        path = self.generate_plist_file('baz:1')
        pl.set_source('file', location=path)
        self.assertEqual(pl.get_next_proxy().server, 'bar')
        self.assertEqual(pl.get_next_proxy().server, 'baz')
        self.assertEqual(pl.get_next_proxy().server, 'foo')

########NEW FILE########
__FILENAME__ = pycurl_cookie
# coding: utf-8
"""
This test case has written to help me
understand how pycurl lib works with cookies
"""
from unittest import TestCase
import pycurl
try:
    from cStringIO import StringIO
except ImportError:
    from io import BytesIO as StringIO
try:
    from cookielib import CookieJar, Cookie
except ImportError:
    from http.cookiejar import CookieJar, Cookie


from test.server import SERVER
from grab.cookie import create_cookie

# http://xiix.wordpress.com/2006/03/23/mozillafirefox-cookie-format/
# http://curl.haxx.se/libcurl/c/curl_easy_setopt.html
# Cookie:
# * domain
# * whether or not all machines under that domain can read the cookies information.
# * path
# * Secure Flag: whether or not a secure connection (HTTPS) is required to read the cookie.
# * exp. timestamp
# * name
# * value

class TestCookies(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_pycurl_cookies(self):
        SERVER.RESPONSE_ONCE['code'] = 302
        SERVER.RESPONSE_ONCE['cookies'] = {'foo': 'bar', '1': '2'}
        SERVER.RESPONSE_ONCE['headers'].append(('Location', SERVER.BASE_URL))
        SERVER.RESPONSE['get'] = 'foo'

        buf = StringIO()
        header_buf = StringIO()
        cfile = StringIO()

        # Configure pycurl instance
        # Usually all these crap is automatically handled by the Grab
        c = pycurl.Curl()
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.setopt(pycurl.WRITEFUNCTION, buf.write)
        c.setopt(pycurl.HEADERFUNCTION, header_buf.write)
        c.setopt(pycurl.FOLLOWLOCATION, 1)
        c.setopt(pycurl.COOKIEFILE, "")
        c.perform()
        self.assertEqual(b'foo', buf.getvalue())

        print(c.getinfo(pycurl.INFO_COOKIELIST))
        self.assertEquals(2, len(c.getinfo(pycurl.INFO_COOKIELIST)))

        # Just make another request and check that pycurl has
        # submitted two cookies
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.perform()
        self.assertEquals(2, len(SERVER.REQUEST['cookies']))

        # Erase cookies
        cookies = c.getinfo(pycurl.INFO_COOKIELIST)
        c.setopt(pycurl.COOKIELIST, "ALL")
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.perform()
        self.assertEquals(0, len(SERVER.REQUEST['cookies']))

        # Now let's try to setup pycurl with cookies
        # saved into `cookies` variable
        for cookie in cookies:
            c.setopt(pycurl.COOKIELIST, cookie)
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.perform()
        self.assertEquals(2, len(SERVER.REQUEST['cookies']))
        self.assertEquals('bar', SERVER.REQUEST['cookies']['foo'].value)
        self.assertEquals(set(('foo', '1')), set(SERVER.REQUEST['cookies'].keys()))

        # Ok, now let's create third cookies that is binded to
        # the path /place, put this cookie into curl object
        # and submit request to /
        # pycurl should not send third cookie
        cookie = '\t'.join(('localhost', 'FALSE', '/place', 'FALSE', '0', 'no', 'way'))
        c.setopt(pycurl.COOKIELIST, cookie)
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.perform()
        self.assertEquals(set(('foo', '1')), set(SERVER.REQUEST['cookies'].keys()))

        # Ok, now send request to /place
        c.setopt(pycurl.URL, SERVER.BASE_URL + '/place')
        c.perform()
        self.assertEquals(set(('foo', '1', 'no')), set(SERVER.REQUEST['cookies'].keys()))

        # Now, check that not all cookies set with cookieslist
        # are submitted
        c.setopt(pycurl.COOKIELIST, "ALL")
        c.setopt(pycurl.URL, SERVER.BASE_URL)
        c.setopt(pycurl.COOKIELIST, "Set-Cookie: 1=2; domain=microsoft.com")
        c.setopt(pycurl.COOKIELIST, "Set-Cookie: 3=4")
        c.setopt(pycurl.COOKIELIST, "Set-Cookie: 5=6")
        c.perform()
        self.assertEquals(2, len(SERVER.REQUEST['cookies']))

    #def test_some_thing(self):
        #SERVER.RESPONSE['headers'].append(
            #('Set-Cookie', 'x=y; expires=Fri, 31 Dec 2020 23:59:59 GMT;'
                           #' path=/; domain=.foo.bar'),
        #)
        #SERVER.RESPONSE['get'] = 'foo'

        #buf = StringIO()
        #header_buf = StringIO()
        #cfile = StringIO()

        ## Configure pycurl instance
        ## Usually all these crap is automatically handled by the Grab
        #c = pycurl.Curl()
        #c.setopt(pycurl.URL, 'http://foo.bar:9876')#SERVER.BASE_URL)
        #c.setopt(pycurl.WRITEFUNCTION, buf.write)
        #c.setopt(pycurl.HEADERFUNCTION, header_buf.write)
        #c.setopt(pycurl.FOLLOWLOCATION, 1)
        #c.setopt(pycurl.COOKIEFILE, "")
        #c.perform()

        #print header_buf.getvalue()
        #print c.getinfo(pycurl.INFO_COOKIELIST)#[0]#.split('\t')

    def test_cookie(self):
        c = create_cookie('foo', 'bar')

        self.assertRaises(TypeError, lambda: create_cookie('foo', 'bar', x='y'))

    def test_cookiejar(self):
        c1 = create_cookie('foo', 'bar')
        c2 = create_cookie('foo', 'bar')
        self.assertFalse(c1 == c2)

        c = create_cookie('foo', 'bar', domain='.dumpz.org')
        self.assertEquals(c.domain, '.dumpz.org')

        cj = CookieJar()
        cj.set_cookie(create_cookie('foo', 'bar', domain='foo.com'))
        cj.set_cookie(create_cookie('foo', 'bar', domain='bar.com'))
        self.assertEqual(len(cj), 2)

########NEW FILE########
__FILENAME__ = pyquery_extension
# coding: utf-8
from unittest import TestCase
from grab import Grab, DataNotFound

HTML = u"""
<head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=cp1251" />
</head>
<body>
    <div id="bee">
        <div class="wrapper">
            <strong id="bee-strong"></strong><em id="bee-em"></em>
        </div>
        <script type="text/javascript">
        mozilla = 777;
        </script>
        <style type="text/css">
        body { color: green; }
        </style>
    </div>
    <div id="fly">
        <strong id="fly-strong">\n</strong><em id="fly-em"></em>
    </div>
    <ul id="num">
        <li id="num-1">item #100 2</li>
        <li id="num-2">item #2</li>
    </ul>
""".encode('cp1251')

class PyqueryExtensionTest(TestCase):
    def setUp(self):
        # Create fake grab instance with fake response
        self.g = Grab(HTML, charset='cp1251')

    def test_some_things(self):
        from pyquery import PyQuery
        self.assertEqual(self.g.pyquery('#num-1').text(), u'item #100 2')
        self.assertEqual(self.g.pyquery('li').filter(
            lambda x: '#2' in PyQuery(this).text()).text(), u'item #2')

########NEW FILE########
__FILENAME__ = reference
# coding: utf-8
from unittest import TestCase

import os
import sys
root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.insert(0, root)

from grab.selector import XpathSelector, TextSelector, JsonSelector
from lxml.html import fromstring
from grab.reference import Reference

HTML = """
<html>
    <body>
        <h1>test</h1>
        <ul>
            <li>one</li>
            <li>two</li>
            <li>three</li>
            <li id="6">z 4 foo</li>
        </ul>
        <ul id="second-list">
            <li class="li-1">yet one</li>
            <li class="li-2">yet two</li>
        </ul>
    </body>
</html>
"""

class TestSelector(TestCase):
    def setUp(self):
        self.tree = fromstring(HTML)

    #def test_select_node(self):
        #ref = Reference(self.tree)
        #self.assertEquals('test', ref.h1_(cls=

    #def test_html(self):
        #sel = XpathSelector(self.tree.xpath('//h1')[0])
        #self.assertEquals('<h1>test</h1>', sel.html().strip())

    #def test_textselector(self):
        #self.assertEquals('one', XpathSelector(self.tree).select('//li/text()').text())

    #def test_number(self):
        #self.assertEquals(4, XpathSelector(self.tree).select('//ul/li[last()]').number())
        #self.assertEquals(6, XpathSelector(self.tree).select('//ul/li[last()]/@id').number())

    #def test_text_selector(self):
        #sel = XpathSelector(self.tree).select('//li/text()').one()
        #self.assertTrue(isinstance(sel, TextSelector))

    ## TODO: add --pyquery flag to runtest script
    ##def test_select_pyquery(self):
        ##root = Selector(self.tree)
        ##self.assertEquals('test', root.select(pyquery='h1')[0].node.text)
        ##self.assertEquals('z 4 foo', root.select(pyquery='body')[0].select(pyquery='#6')[0].node.text)

    #def test_select_select(self):
        #root = XpathSelector(self.tree)
        #self.assertEquals(set(['one', 'yet one']),
                          #set([x.text() for x in root.select('//ul').select('./li[1]')]),
                          #)

    #def test_text_list(self):
        #root = XpathSelector(self.tree)
        #self.assertEquals(set(['one', 'yet one']),
                          #set(root.select('//ul/li[1]').text_list()),
                          #)

    #def test_attr_list(self):
        #root = XpathSelector(self.tree)
        #self.assertEquals(set(['li-1', 'li-2']),
                          #set(root.select('//ul[@id="second-list"]/li')\
                                  #.attr_list('class'))
                          #)


#class TestSelectorList(TestCase):
    #def setUp(self):
        #self.tree = fromstring(HTML)

    #def test_one(self):
        #sel = XpathSelector(self.tree).select('//ul/li')
        #self.assertEquals('one', sel.one().node.text)
        #self.assertEquals('one', sel.text())

    #def test_number(self):
        #sel = XpathSelector(self.tree).select('//ul/li[4]')
        #self.assertEquals(4, sel.number())

    #def test_exists(self):
        #sel = XpathSelector(self.tree).select('//ul/li[4]')
        #self.assertEquals(True, sel.exists())

        #sel = XpathSelector(self.tree).select('//ul/li[5]')
        #self.assertEquals(False, sel.exists())

    #def test_incorrect_xpath(self):
        ## The lxml xpath function return boolean for following xpath
        ## This breaks selector internal logic that assumes that only
        ## list could be returnsed
        ## So it was fixed and this test was crated
        #sel = XpathSelector(self.tree).select('//ul/li/text()="oops"')
        #self.assertEquals(False, sel.exists())

        ## Selector list is always empty in this special case
        ## Even if the xpath return True on lxml level
        #self.assertEquals(True, self.tree.xpath('//ul[1]/li[1]/text()="one"'))
        #sel = XpathSelector(self.tree).select('//ul[1]/li[1]/text()="one"')
        #self.assertEquals(False, sel.exists())


#class TestJsonSelector(TestCase):
    #def setUp(self):
        #self.tree = [
            #{'name': 'Earth', 'cities': ['Moscow', 'Paris', 'Tokio'], 'population': 7000000000},
            #{'name': 'Mars', 'cities': [], 'population': 0},
        #]

    #def test_it_works(self):
        #sel = JsonSelector(self.tree)

    #def test_select_node(self):
        #self.assertEquals({'name': 'Mars', 'cities': [], 'population': 0},
                          #JsonSelector(self.tree).select('$[1]')[0].node.value)

    #def test_html(self):
        #sel = JsonSelector(self.tree)
        #self.assertRaises(NotImplementedError, sel.html)

    #def test_text(self):
        #self.assertEquals('Mars',
                          #JsonSelector(self.tree).select('$[1].name').text())

    #def test_population(self):
        #self.assertEquals(7000000000,
                          #JsonSelector(self.tree).select('$..population').number())

    #def test_select_select(self):
        #root = JsonSelector(self.tree)
        #self.assertEquals('Mars', root.select('$[1]').select('name').text())

    #def test_text_list(self):
        #root = JsonSelector(self.tree)
        #self.assertEquals(['7000000000', '0'],
                          #root.select('$..population').text_list())

    #def test_html(self):
        #root = JsonSelector(self.tree)
        #self.assertRaises(NotImplementedError,
            #lambda: root.select('$..population')[0].html())

    #def test_attr(self):
        #root = JsonSelector(self.tree)
        #self.assertRaises(NotImplementedError,
            #lambda: root.select('$..population').attr('bar'))

    #def test_attr_list(self):
        #root = JsonSelector(self.tree)
        #self.assertRaises(NotImplementedError,
            #lambda: root.select('$..population').attr_list('bar'))

########NEW FILE########
__FILENAME__ = response_class
# coding: utf-8
from __future__ import absolute_import
from unittest import TestCase
from grab import Grab, DataNotFound, GrabMisuseError
import os.path

from test.util import TEST_DIR, TMP_DIR, build_grab
from test.server import SERVER

HTML = """
Hello world
"""

IMG_FILE = os.path.join(TEST_DIR, 'files', 'yandex.png')

class TestResponse(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_save(self):
        """
        Test `Response.save` method.
        """
        
        img_data = open(IMG_FILE, 'rb').read()
        temp_file = os.path.join(TMP_DIR, 'file.bin')
        SERVER.RESPONSE['get'] = img_data

        g = build_grab()
        g.go(SERVER.BASE_URL)
        g.response.save(temp_file)
        self.assertEqual(open(temp_file, 'rb').read(), img_data)

    def test_save_hash(self):
        """
        Test `Response.save_hash` method.
        """
        
        img_data = open(IMG_FILE, 'rb').read()
        SERVER.RESPONSE['get'] = img_data

        g = build_grab()
        g.go(SERVER.BASE_URL)
        path = g.response.save_hash(SERVER.BASE_URL, TMP_DIR)
        test_data = open(os.path.join(TMP_DIR, path), 'rb').read()
        self.assertEqual(test_data, img_data)

    def test_custom_charset(self):
        SERVER.RESPONSE['get'] = u'<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf8;charset=cp1251" /></head><body><h1></h1></body></html>'.encode('utf-8')
        g = build_grab()
        g.setup(document_charset='utf-8')
        g.go(SERVER.BASE_URL)
        self.assertTrue(u'' in g.response.unicode_body())

    def test_xml_declaration(self):
        """
        unicode_body() should return HTML with xml declaration (if it
        exists in original HTML)
        """
        SERVER.RESPONSE['get'] = """<?xml version="1.0" encoding="UTF-8"?>
        <html><body><h1></h1></body></html>
        """
        g = build_grab()
        g.go(SERVER.BASE_URL)
        ubody = g.response.unicode_body()
        self.assertTrue(u'' in ubody)
        self.assertTrue('<?xml' in ubody)

########NEW FILE########
__FILENAME__ = selector
# coding: utf-8
from unittest import TestCase

import os
import sys
root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.insert(0, root)

from grab.selector import XpathSelector, TextSelector, JsonSelector
from lxml.html import fromstring

HTML = """
<html>
    <body>
        <h1>test</h1>
        <ul>
            <li>one</li>
            <li>two</li>
            <li>three</li>
            <li id="6">z 4 foo</li>
        </ul>
        <ul id="second-list">
            <li class="li-1">yet one</li>
            <li class="li-2">yet two</li>
        </ul>
    </body>
</html>
"""

class TestSelector(TestCase):
    def setUp(self):
        self.tree = fromstring(HTML)

    def test_in_general(self):
        sel = XpathSelector(self.tree)

    def test_select_node(self):
        self.assertEquals('test', XpathSelector(self.tree).select('//h1')[0].node.text)

    def test_html(self):
        sel = XpathSelector(self.tree.xpath('//h1')[0])
        self.assertEquals('<h1>test</h1>', sel.html().strip())

    def test_textselector(self):
        self.assertEquals('one', XpathSelector(self.tree).select('//li/text()').text())

    def test_number(self):
        self.assertEquals(4, XpathSelector(self.tree).select('//ul/li[last()]').number())
        self.assertEquals(6, XpathSelector(self.tree).select('//ul/li[last()]/@id').number())

    def test_text_selector(self):
        sel = XpathSelector(self.tree).select('//li/text()').one()
        self.assertTrue(isinstance(sel, TextSelector))

    # TODO: add --pyquery flag to runtest script
    #def test_select_pyquery(self):
        #root = Selector(self.tree)
        #self.assertEquals('test', root.select(pyquery='h1')[0].node.text)
        #self.assertEquals('z 4 foo', root.select(pyquery='body')[0].select(pyquery='#6')[0].node.text)

    def test_select_select(self):
        root = XpathSelector(self.tree)
        self.assertEquals(set(['one', 'yet one']),
                          set([x.text() for x in root.select('//ul').select('./li[1]')]),
                          )

    def test_text_list(self):
        root = XpathSelector(self.tree)
        self.assertEquals(set(['one', 'yet one']),
                          set(root.select('//ul/li[1]').text_list()),
                          )

    def test_attr_list(self):
        root = XpathSelector(self.tree)
        self.assertEquals(set(['li-1', 'li-2']),
                          set(root.select('//ul[@id="second-list"]/li')\
                                  .attr_list('class'))
                          )


class TestSelectorList(TestCase):
    def setUp(self):
        self.tree = fromstring(HTML)

    def test_one(self):
        sel = XpathSelector(self.tree).select('//ul/li')
        self.assertEquals('one', sel.one().node.text)
        self.assertEquals('one', sel.text())

    def test_number(self):
        sel = XpathSelector(self.tree).select('//ul/li[4]')
        self.assertEquals(4, sel.number())

    def test_exists(self):
        sel = XpathSelector(self.tree).select('//ul/li[4]')
        self.assertEquals(True, sel.exists())

        sel = XpathSelector(self.tree).select('//ul/li[5]')
        self.assertEquals(False, sel.exists())

    def test_incorrect_xpath(self):
        # The lxml xpath function return boolean for following xpath
        # This breaks selector internal logic that assumes that only
        # list could be returnsed
        # So it was fixed and this test was crated
        sel = XpathSelector(self.tree).select('//ul/li/text()="oops"')
        self.assertEquals(False, sel.exists())

        # Selector list is always empty in this special case
        # Even if the xpath return True on lxml level
        self.assertEquals(True, self.tree.xpath('//ul[1]/li[1]/text()="one"'))
        sel = XpathSelector(self.tree).select('//ul[1]/li[1]/text()="one"')
        self.assertEquals(False, sel.exists())


class TestJsonSelector(TestCase):
    def setUp(self):
        self.tree = [
            {'name': 'Earth', 'cities': ['Moscow', 'Paris', 'Tokio'], 'population': 7000000000},
            {'name': 'Mars', 'cities': [], 'population': 0},
        ]

    def test_it_works(self):
        sel = JsonSelector(self.tree)

    def test_select_node(self):
        self.assertEquals({'name': 'Mars', 'cities': [], 'population': 0},
                          JsonSelector(self.tree).select('$[1]')[0].node.value)

    def test_html(self):
        sel = JsonSelector(self.tree)
        self.assertRaises(NotImplementedError, sel.html)

    def test_text(self):
        self.assertEquals('Mars',
                          JsonSelector(self.tree).select('$[1].name').text())

    def test_population(self):
        self.assertEquals(7000000000,
                          JsonSelector(self.tree).select('$..population').number())

    def test_select_select(self):
        root = JsonSelector(self.tree)
        self.assertEquals('Mars', root.select('$[1]').select('name').text())

    def test_text_list(self):
        root = JsonSelector(self.tree)
        self.assertEquals(['7000000000', '0'],
                          root.select('$..population').text_list())

    def test_html(self):
        root = JsonSelector(self.tree)
        self.assertRaises(NotImplementedError,
            lambda: root.select('$..population')[0].html())

    def test_attr(self):
        root = JsonSelector(self.tree)
        self.assertRaises(NotImplementedError,
            lambda: root.select('$..population').attr('bar'))

    def test_attr_list(self):
        root = JsonSelector(self.tree)
        self.assertRaises(NotImplementedError,
            lambda: root.select('$..population').attr_list('bar'))

########NEW FILE########
__FILENAME__ = selector_kit
# coding: utf-8
from unittest import TestCase

#import os
#import sys
#root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
#sys.path.insert(0, root)

from test.util import GRAB_TRANSPORT, ignore_transport, only_transport
from test.server import SERVER
from grab.selector import KitSelector
from grab import Grab

from grab.util.py3k_support import *

HTML = """
<html>
    <body>
        <h1>test</h1>
        <ul>
            <li>one</li>
            <li>two</li>
            <li>three</li>
            <li class="zzz" id="6">z 4 foo</li>
        </ul>
        <ul id="second-list">
            <li class="li-1">yet one</li>
            <li class="li-2">yet two</li>
        </ul>
    </body>
</html>
"""

class KitSelectorTestCase(TestCase):
    def setUp(self):
        g = Grab(transport='grab.transport.kit.KitTransport')
        SERVER.RESPONSE['get'] = HTML
        g.go(SERVER.BASE_URL)
        self.qt_doc = g.transport.kit.page.mainFrame().documentElement()

    def test_in_general(self):
        sel = KitSelector(self.qt_doc)

    def test_select_node(self):
        sel = KitSelector(self.qt_doc).select('h1')[0]
        self.assertEquals('test', sel.node.toInnerXml())

    def test_html(self):
        sel = KitSelector(self.qt_doc).select('h1')[0]
        self.assertEquals('<h1>test</h1>', sel.html())

    def test_textselector(self):
        self.assertEquals('one', KitSelector(self.qt_doc).select('li').text())

    def test_number(self):
        self.assertEquals(4, KitSelector(self.qt_doc).select('li.zzz').number())

    # TODO
    # test the ID selector (#6)

    #def test_text_selector(self):
        #sel = KitSelector(self.qt_doc).select('//li/text()').one()
        #self.assertTrue(isinstance(sel, TextSelector))

    ## TODO: add --pyquery flag to runtest script
    ##def test_select_pyquery(self):
        ##root = Selector(self.qt_doc)
        ##self.assertEquals('test', root.select(pyquery='h1')[0].node.text)
        ##self.assertEquals('z 4 foo', root.select(pyquery='body')[0].select(pyquery='#6')[0].node.text)

    def test_select_select(self):
        root = KitSelector(self.qt_doc)
        self.assertEquals(set(['one', 'yet one']),
                          set([x.text() for x in root.select('ul').select('li:first-child')]),
                          )

    def test_text_list(self):
        root = KitSelector(self.qt_doc)
        self.assertEquals(set(['one', 'yet one']),
                          set(root.select('ul > li:first-child').text_list()),
                          )

    def test_attr_list(self):
        root = KitSelector(self.qt_doc)
        self.assertEquals(set(['li-1', 'li-2']),
                          set(root.select('ul[id=second-list] > li')\
                                  .attr_list('class'))
                          )


class TestSelectorList(TestCase):
    def setUp(self):
        g = Grab(transport='grab.transport.kit.KitTransport')
        SERVER.RESPONSE['get'] = HTML
        g.go(SERVER.BASE_URL)
        self.qt_doc = g.transport.kit.page.mainFrame().documentElement()

    def test_one(self):
        sel = KitSelector(self.qt_doc).select('ul > li')
        self.assertEquals('one', unicode(sel.one().node.toPlainText()))
        self.assertEquals('one', sel.text())

    def test_number(self):
        sel = KitSelector(self.qt_doc).select('li:nth-child(4)')
        self.assertEquals(4, sel.number())

    def test_exists(self):
        sel = KitSelector(self.qt_doc).select('li:nth-child(4)')
        self.assertEquals(True, sel.exists())

        sel = KitSelector(self.qt_doc).select('li:nth-child(5)')
        self.assertEquals(False, sel.exists())

########NEW FILE########
__FILENAME__ = spider
from unittest import TestCase

from grab.spider import Spider, Task, Data
from test.server import SERVER

from grab.util.py3k_support import *

class BasicSpiderTestCase(TestCase):

    class SimpleSpider(Spider):
        def task_baz(self, grab, task):
            self.SAVED_ITEM = grab.response.body

    def setUp(self):
        SERVER.reset()

    def test_spider(self):
        SERVER.RESPONSE['get'] = 'Hello spider!'
        SERVER.SLEEP['get'] = 0
        sp = self.SimpleSpider()
        sp.setup_queue()
        sp.add_task(Task('baz', SERVER.BASE_URL))
        sp.run()
        self.assertEqual(b'Hello spider!', sp.SAVED_ITEM)

    def test_network_limit(self):
        SERVER.RESPONSE['get'] = 'Hello spider!'
        SERVER.SLEEP['get'] = 1.1

        sp = self.SimpleSpider(network_try_limit=1)
        sp.setup_queue()
        sp.setup_grab(connect_timeout=1, timeout=1)
        sp.add_task(Task('baz', SERVER.BASE_URL))
        sp.run()
        self.assertEqual(sp.counters['request-network'], 1)

        sp = self.SimpleSpider(network_try_limit=2)
        sp.setup_queue()
        sp.setup_grab(connect_timeout=1, timeout=1)
        sp.add_task(Task('baz', SERVER.BASE_URL))
        sp.run()
        self.assertEqual(sp.counters['request-network'], 2)

    def test_task_limit(self):
        SERVER.RESPONSE['get'] = 'Hello spider!'
        SERVER.SLEEP['get'] = 1.1

        sp = self.SimpleSpider(network_try_limit=1)
        sp.setup_grab(connect_timeout=1, timeout=1)
        sp.setup_queue()
        sp.add_task(Task('baz', SERVER.BASE_URL))
        sp.run()
        self.assertEqual(sp.counters['task-baz'], 1)

        sp = self.SimpleSpider(task_try_limit=2)
        sp.setup_queue()
        sp.add_task(Task('baz', SERVER.BASE_URL, task_try_count=3))
        sp.run()
        self.assertEqual(sp.counters['request-network'], 0)

    def test_task_retry(self):
        SERVER.RESPONSE['get'] = 'xxx'
        SERVER.RESPONSE_ONCE['code'] = 403
        sp = self.SimpleSpider()
        sp.setup_queue()
        sp.add_task(Task('baz', SERVER.BASE_URL))
        sp.run()
        self.assertEqual(b'xxx', sp.SAVED_ITEM)

    def test_setup_grab(self):
        """
        Mulitple calls to `setup_grab` should accumulate changes in config object.
        """
        bot = self.SimpleSpider()
        bot.setup_grab(log_dir='/tmp')
        bot.setup_grab(timeout=30)
        grab = bot.create_grab_instance()
        self.assertEqual(grab.config['log_dir'], '/tmp')
        self.assertEqual(grab.config['timeout'], 30)

    def test_generator(self):
        class TestSpider(Spider):
            def prepare(self):
                self.count = 0

            def task_generator(self):
                for x in xrange(1111):
                    yield Task('page', url=SERVER.BASE_URL)

            def task_page(self, grab, task):
                self.count += 1

        bot = TestSpider()
        bot.run()
        self.assertEqual(bot.count, 1111)

########NEW FILE########
__FILENAME__ = spider_cache
from unittest import TestCase

from grab.spider import Spider, Task
from test.server import SERVER
from test.util import only_backend

class ContentGenerator():
    def __init__(self):
        self.counter = 0

    def build_html(self):
        self.counter += 1
        return """
        <head>
            <title>ABC</title>
        </head>
        <body>
            <a href="%(BASE_URL)s">link #1</a>
            <a href="%(BASE_URL)s?123">link #2</a>
            <span id="counter">%(COUNTER)s</span>
        </body>
        """ % {'BASE_URL': SERVER.BASE_URL, 'COUNTER': self.counter}

    def reset(self):
        self.counter = 0


class SimpleSpider(Spider):
    def prepare(self):
        self.resp_counters = []

    def process_counter(self, grab):
        counter = grab.doc.select('//span[@id="counter"]').number()
        self.resp_counters.append(counter)

    def task_one(self, grab, task):
        self.process_counter(grab)

    def task_foo(self, grab, task):
        self.process_counter(grab)

        if not task.get('secondary'):
            grab.setup(url=SERVER.BASE_URL)
            yield Task('foo', grab=grab, secondary=True, priority=1)

            for count, elem in enumerate(grab.doc.select('//a')):
                yield Task('foo', url=elem.attr('href'), secondary=True,
                           priority=count + 2)


class SpiderCacheMixin(object):
    def setUp(self):
        SERVER.reset()
        SERVER.RESPONSE['get'] = ContentGenerator().build_html

    def test_counter(self):
        bot = SimpleSpider()
        self.setup_cache(bot)
        bot.cache.clear()
        bot.setup_queue()
        bot.add_task(Task('one', SERVER.BASE_URL))
        bot.run()
        self.assertEqual([1], bot.resp_counters)

    def test_bug1(self):
        """
        Test the bug:
        * enable cache
        * fetch document (it goes to cache)
        * request same URL
        * got exception
        """

        class Bug1Spider(Spider):
            def task_foo(self, grab, task):
                grab.setup(url=SERVER.BASE_URL)
                yield Task('bar', grab=grab)

            def task_bar(self, grab, task):
                pass

        bot = Bug1Spider()
        self.setup_cache(bot)
        bot.cache.clear()
        bot.setup_queue()
        bot.add_task(Task('foo', SERVER.BASE_URL))
        bot.run()

    def test_something(self):
        bot = SimpleSpider()
        self.setup_cache(bot)
        bot.cache.clear()
        bot.setup_queue()
        bot.add_task(Task('foo', SERVER.BASE_URL))
        bot.run()
        self.assertEqual([1, 1, 1, 2], bot.resp_counters)

    def test_timeout(self):
        bot = SimpleSpider()
        self.setup_cache(bot)
        bot.cache.clear()
        bot.setup_queue()
        bot.add_task(Task('one', SERVER.BASE_URL))
        bot.add_task(Task('one', SERVER.BASE_URL, delay=0.5))
        bot.run()
        self.assertEqual([1, 1], bot.resp_counters)

        bot = SimpleSpider()
        self.setup_cache(bot)
        # DO not clear the cache
        #bot.cache.clear()
        bot.setup_queue()
        bot.add_task(Task('one', SERVER.BASE_URL, priority=1))
        bot.add_task(Task('one', SERVER.BASE_URL, priority=2, cache_timeout=0, delay=1))
        bot.add_task(Task('one', SERVER.BASE_URL, priority=3, cache_timeout=10, delay=1.1))
        bot.add_task(Task('one', SERVER.BASE_URL, priority=4, cache_timeout=0, delay=1.2))
        bot.run()
        self.assertEqual([1, 2, 2, 3], bot.resp_counters)


class SpiderMongoCacheTestCase(SpiderCacheMixin, TestCase):
    _backend = 'mongo'

    def setup_cache(self, bot):
        bot.setup_cache(backend='mongo', database='test_spider')


class SpiderMysqlCacheTestCase(SpiderCacheMixin, TestCase):
    _backend = 'mysql'

    def setup_cache(self, bot):
        bot.setup_cache(backend='mysql', database='spider_test',
                        user='web', passwd='web-**')


class SpiderPostgresqlCacheTestCase(SpiderCacheMixin, TestCase):
    _backend = 'postgresql'

    def setup_cache(self, bot):
        bot.setup_cache(backend='postgresql', database='spider_test')#,
                        #user='web', passwd='web-**')

########NEW FILE########
__FILENAME__ = spider_command_controller
from unittest import TestCase

from grab import Grab
from grab.spider import Spider, Task, Data
from test.server import SERVER

class MiscTest(TestCase):
    _backend = 'redis'

    def setUp(self):
        SERVER.reset()

    def test_null_grab_bug(self):
        class SimpleSpider(Spider):
            def task_page(self, grab, task):
                pass

        # Build controller to get redis interface
        bot = SimpleSpider()
        iface = bot.controller.add_interface('redis')
        rid = iface.put_command({'name': 'get_stats'})

        bot = SimpleSpider(thread_number=1)
        bot.setup_queue()
        bot.controller.add_interface('redis')
        bot.run()

        self.assertTrue('Threads:' in iface.pop_result(rid)['data'])

########NEW FILE########
__FILENAME__ = spider_data
from unittest import TestCase

from grab import Grab
from grab.spider import Spider, Task, Data, NoDataHandler, SpiderMisuseError

from test.server import SERVER

class TestSpider(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_data_nohandler_error(self):
        class TestSpider(Spider):
            def task_page(self, grab, task):
                yield Data('foo', num=1)

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        self.assertRaises(NoDataHandler, bot.run)

    def test_exception_from_data_handler(self):
        class TestSpider(Spider):
            def task_page(self, grab, task):
                yield Data('foo', num=1)
            
            def data_foo(self, num):
                1/0

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.run()
        self.assertTrue('data_foo' in bot.items['fatal'][0])

    def test_data_simple_case(self):
        class TestSpider(Spider):
            def prepare(self):
                self.data_processed = []

            def task_page(self, grab, task):
                yield Data('foo', number=1)
            
            def data_foo(self, number):
                self.data_processed.append(number)

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.run()
        self.assertEqual(bot.data_processed, [1])

    def test_complex_data(self):
        class TestSpider(Spider):
            def prepare(self):
                self.data_processed = []

            def task_page(self, grab, task):
                yield Data('foo', one=1, two=2, bar='gaz')
            
            def data_foo(self, one, two, **kwargs):
                self.data_processed.append(one)
                self.data_processed.append(two)
                self.data_processed.append(kwargs)

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.run()
        self.assertEqual(bot.data_processed, [1, 2, {'bar': 'gaz'}])

    def test_data_object_dict_interface(self):
        data = Data('person', person={'age': 22})
        self.assertRaises(KeyError, lambda: data['name'])
        self.assertEqual(data['person'], {'age': 22})

    def test_data_object_get_method(self):
        data = Data('person', person={'age': 22})
        self.assertRaises(KeyError, lambda: data.get('name'))
        self.assertEqual('foo', data.get('name', 'foo'))
        self.assertEqual({'age': 22}, data.get('person'))

    def test_things_yiled_from_data_handler(self):
        class TestSpider(Spider):
            def prepare(self):
                self.data_processed = []

            def task_page(self, grab, task):
                yield Data('foo', count=task.get('count', 1))
            
            def data_foo(self, count):
                self.data_processed.append(count)
                if count == 1:
                    yield Data('foo', count=666)
                    yield Task('page', url=SERVER.BASE_URL,
                               count=count + 1)

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.run()
        self.assertEqual(bot.data_processed, [1, 666, 2])

########NEW FILE########
__FILENAME__ = spider_error
from unittest import TestCase

import grab.spider.base
from grab import Grab
from grab.spider import Spider, Task, Data, SpiderMisuseError, NoTaskHandler
from grab.error import GrabInvalidUrl
import logging

from test.server import SERVER

# That URLs breaks Grab's URL normalization process with error "label empty or too long"
INVALID_URL = 'http://13354&altProductId=6423589&productId=6423589&altProductStoreId=13713&catalogId=10001&categoryId=28678&productStoreId=13713http://www.textbooksnow.com/webapp/wcs/stores/servlet/ProductDisplay?langId=-1&storeId='

class SpiderErrorTestCase(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_generator_with_invalid_url(self):

        class SomeSpider(Spider):
            def task_generator(self):
                yield Task('page', url=INVALID_URL)

        from grab.spider.base import logger_verbose
        logger_verbose.setLevel(logging.DEBUG)
        bot = SomeSpider()
        bot.run()

    def test_redirect_with_invalid_url(self):

        class SomeSpider(Spider):
            def task_generator(self):
                self.done_counter = 0
                yield Task('page', url=SERVER.BASE_URL)

            def task_page(self, grab, task):
                pass

        #from grab.spider.base import logger_verbose
        #logger_verbose.setLevel(logging.DEBUG)
        SERVER.RESPONSE_ONCE['code'] = 301
        SERVER.RESPONSE_ONCE['headers'].append(
            ('Location', INVALID_URL),
        )
        bot = SomeSpider(network_try_limit=1)
        bot.run()

########NEW FILE########
__FILENAME__ = spider_meta
from unittest import TestCase

from grab.spider import Spider, Task, Data
from test.server import SERVER

from grab.util.py3k_support import *

class SpiderMetaTestCase(TestCase):

    def test_root_spider_class(self):
        self.assertEqual(Spider.Meta.abstract, True)

    def test_inherited_class(self):
        class Child(Spider):
            pass
        self.assertEqual(Child.Meta.abstract, False)

        class Child(Spider):
            class Meta:
                abstract = True
        self.assertEqual(Child.Meta.abstract, True)

        class ChildOfChild(Child):
            pass
        self.assertEqual(ChildOfChild.Meta.abstract, False)

        class ChildOfChild(Child):
            class Meta:
                abstract = True
        self.assertEqual(ChildOfChild.Meta.abstract, True)

    def test_meta_inheritance(self):
        class SomeSpider(Spider):
            class Meta:
                foo = 'bar'

        class Child(SomeSpider):
            pass

        self.assertEqual(Child.Meta.foo, 'bar')

    def test_explicit_existence_of_abstract(self):
        class SomeSpider(Spider):
            class Meta:
                foo = 'bar'

        self.assertEqual(SomeSpider.Meta.abstract, False)

########NEW FILE########
__FILENAME__ = spider_misc
from unittest import TestCase

from grab import Grab
from grab.spider import Spider, Task, Data
from test.server import SERVER

class MiscTest(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_null_grab_bug(self):
        # Test following bug:
        # Create task and process it
        # In task handler spawn another task with grab instance
        # received in arguments of current task
        class SimpleSpider(Spider):
            def prepare(self):
                self.page_count = 0

            def task_generator(self):
                yield Task('one', url=SERVER.BASE_URL)

            def task_one(self, grab, task):
                self.page_count += 1
                yield Task('two', grab=grab)

            def task_two(self, grab, task):
                self.page_count += 1

        bot = SimpleSpider(thread_number=1)
        bot.run()
        self.assertEqual(2, bot.page_count)

########NEW FILE########
__FILENAME__ = spider_mongo_cache
from unittest import TestCase

from grab.spider import Spider, Task, Data
from test.server import SERVER
from test.case.mixin.spider_cache import SpiderCacheMixin

class BasicSpiderTestCase(TestCase, SpiderCacheMixin):
    def setUp(self):
        SpiderCacheMixin.setUp(self)

    def setup_cache(self, bot):
        bot.setup_cache(backend='mongo', database='test_spider')

########NEW FILE########
__FILENAME__ = spider_mysql_cache
from unittest import TestCase

from grab.spider import Spider, Task, Data
from test.server import SERVER
from .mixin.spider_cache import SpiderCacheMixin

class BasicSpiderTestCase(TestCase, SpiderCacheMixin):
    def setUp(self):
        SpiderCacheMixin.setUp(self)

    def setup_cache(self, bot):
        bot.setup_cache(backend='mysql', database='spider_test',
                        user='web', passwd='web-**')

########NEW FILE########
__FILENAME__ = spider_proxy
from unittest import TestCase

from grab import Grab
from grab.spider import Spider, Task, Data
from test.server import SERVER

from grab.util.py3k_support import *

PROXY1 = 'localhost:%d' % SERVER.PORT
PROXY2 = 'localhost:%d' % SERVER.EXTRA_PORT1
PROXY3 = 'localhost:%d' % SERVER.EXTRA_PORT2

class SimpleSpider(Spider):
    def prepare(self):
        self.ports = set()

    def task_baz(self, grab, task):
        print(grab.request_headers)
        self.ports.add(int(grab.response.headers.get('Listen-Port', 0)))

class TestSpider(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_setup_proxylist(self):
        content = '%s\n%s\n%s' % (PROXY1, PROXY2, PROXY3)
        open('/tmp/__proxy.txt', 'w').write(content)

        # Simple test, one task
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file')
        bot.setup_queue()
        bot.add_task(Task('baz', grab=Grab(url='http://yandex.ru', debug=True)))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertTrue(len(bot.ports) == 1)

        # By default auto_change is True
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file')
        bot.setup_queue()
        for x in xrange(10):
            bot.add_task(Task('baz', 'http://yandex.ru'))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertTrue(len(bot.ports) > 1)

        # DO the same test with load_proxylist method
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file')
        bot.setup_queue()
        for x in xrange(10):
            bot.add_task(Task('baz', 'http://yandex.ru'))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertTrue(len(bot.ports) > 1)

        # Disable auto_change
        # By default auto_init is True
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file', auto_change=False)
        bot.setup_queue()
        for x in xrange(10):
            bot.add_task(Task('baz', 'http://yandex.ru'))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertTrue(len(bot.ports) == 1)

        # Disable auto_change
        # Disable auto_init
        # Proxylist will not be used by default
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file',
                           auto_change=False, auto_init=False)
        bot.setup_queue()
        for x in xrange(10):
            bot.add_task(Task('baz', SERVER.BASE_URL))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers'].get('host'),
                         '%s:%s' % ('localhost', SERVER.PORT))
        self.assertTrue(len(bot.ports) == 1)
        self.assertEqual(list(bot.ports)[0], SERVER.PORT)

    def test_setup_grab(self):
        # Simple test, one task
        bot = SimpleSpider(thread_number=1)
        bot.setup_grab(proxy=PROXY1)
        bot.setup_queue()
        bot.add_task(Task('baz', 'http://yandex.ru'))
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertEqual(bot.ports, set([SERVER.PORT]))
        self.assertTrue(len(bot.ports) == 1)

        content = '%s\n%s' % (PROXY1, PROXY2)
        open('/tmp/__proxy.txt', 'w').write(content)

        # If proxy is configured with both methods (setup_grab and load_proxylist)
        # then proxylist has priority
        bot = SimpleSpider(thread_number=1)
        bot.load_proxylist('/tmp/__proxy.txt', 'text_file')
        bot.setup_queue()
        for x in xrange(10):
            bot.add_task(Task('baz', 'http://yandex.ru'))
        bot.setup_grab(proxy=PROXY3)
        bot.run()

        self.assertEqual(SERVER.REQUEST['headers']['host'], 'yandex.ru')
        self.assertTrue(not SERVER.EXTRA_PORT2 in bot.ports)

########NEW FILE########
__FILENAME__ = spider_queue
from unittest import TestCase

from grab.spider import Spider, Task, Data
from test.server import SERVER

from grab.util.py3k_support import *

class SpiderQueueMixin(object):
    class SimpleSpider(Spider):
        def prepare(self):
            self.url_history = []
            self.priority_history = []

        def task_page(self, grab, task):
            self.url_history.append(task.url)
            self.priority_history.append(task.priority)

    def test_basic_priority(self):
        bot = self.SimpleSpider()
        self.setup_queue(bot)
        bot.taskq.clear()
        requested_urls = {}
        for priority in (4, 2, 1, 5):
            url = SERVER.BASE_URL + '?p=%d' % priority
            requested_urls[priority] = url
            bot.add_task(Task('page', url=url,
                              priority=priority))
        bot.run()
        urls = [x[1] for x in sorted(requested_urls.items(), 
                                     key=lambda x: x[0])]
        self.assertEqual(urls, bot.url_history)

    def test_queue_length(self):
        bot = self.SimpleSpider()
        self.setup_queue(bot)
        bot.taskq.clear()
        for x in xrange(5):
            bot.add_task(Task('page', url=SERVER.BASE_URL))
        self.assertEqual(5, bot.taskq.size())
        bot.run()
        self.assertEqual(0, bot.taskq.size())
        bot.run()


class SpiderMemoryQueueTestCase(TestCase, SpiderQueueMixin):
    def setUp(self):
        SERVER.reset()

    def setup_queue(self, bot):
        bot.setup_queue(backend='memory')

    def test_schedule(self):
        """
        In this test I create a number of delayed task
        and then check the order in which they was executed
        """

        class TestSpider(Spider):
            def prepare(self):
                self.numbers = []

            def task_generator(self):
                yield Task('page', url=SERVER.BASE_URL, num=1)
                yield Task('page', url=SERVER.BASE_URL, delay=1.5, num=2)
                yield Task('page', url=SERVER.BASE_URL, delay=0.5, num=3)
                yield Task('page', url=SERVER.BASE_URL, delay=1, num=4)

            def task_page(self, grab, task):
                self.numbers.append(task.num)

        bot = TestSpider()
        self.setup_queue(bot)
        bot.run()
        self.assertEqual(bot.numbers, [1, 3, 4, 2])


class BasicSpiderTestCase(SpiderQueueMixin, TestCase):
    _backend = 'mongo'

    def setup_queue(self, bot):
        bot.setup_queue(backend='mongo', database='queue_test')


class SpiderRedisQueueTestCase(SpiderQueueMixin, TestCase):
    _backend = 'redis'

    def setup_queue(self, bot):
        bot.setup_queue(backend='redis')

########NEW FILE########
__FILENAME__ = spider_task
from unittest import TestCase
#try:
#    import cPickle as pickle
#except ImportError:
#    import pickle

import grab.spider.base
from grab import Grab
from grab.spider import Spider, Task, Data, SpiderMisuseError, NoTaskHandler

from test.server import SERVER

class SimpleSpider(Spider):
    base_url = 'http://google.com'

    def task_baz(self, grab, task):
        self.SAVED_ITEM = grab.response.body

class TestSpider(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_task_priority(self):
        #SERVER.RESPONSE['get'] = 'Hello spider!'
        #SERVER.SLEEP['get'] = 0
        #sp = SimpleSpider()
        #sp.add_task(Task('baz', SERVER.BASE_URL))
        #sp.run()
        #self.assertEqual('Hello spider!', sp.SAVED_ITEM)

        # Automatic random priority
        grab.spider.base.RANDOM_TASK_PRIORITY_RANGE = (10, 20)
        bot = SimpleSpider(priority_mode='random')
        bot.setup_queue()
        task = Task('baz', url='xxx')
        self.assertEqual(task.priority, None)
        bot.add_task(task)
        self.assertTrue(10 <= task.priority <= 20)

        # Automatic constant priority
        grab.spider.base.DEFAULT_TASK_PRIORITY = 33
        bot = SimpleSpider(priority_mode='const')
        bot.setup_queue()
        task = Task('baz', url='xxx')
        self.assertEqual(task.priority, None)
        bot.add_task(task)
        self.assertEqual(33, task.priority)

        # Automatic priority does not override explictily setted priority
        grab.spider.base.DEFAULT_TASK_PRIORITY = 33
        bot = SimpleSpider(priority_mode='const')
        bot.setup_queue()
        task = Task('baz', url='xxx', priority=1)
        self.assertEqual(1, task.priority)
        bot.add_task(task)
        self.assertEqual(1, task.priority)

        self.assertRaises(SpiderMisuseError,
                          lambda: SimpleSpider(priority_mode='foo'))

    def test_task_url(self):
        bot = SimpleSpider()
        bot.setup_queue()
        task = Task('baz', url='xxx')
        self.assertEqual('xxx', task.url)
        bot.add_task(task)
        self.assertEqual('http://google.com/xxx', task.url)
        self.assertEqual(None, task.grab_config)

        g = Grab(url='yyy')
        task = Task('baz', grab=g)
        bot.add_task(task)
        self.assertEqual('http://google.com/yyy', task.url)
        self.assertEqual('http://google.com/yyy', task.grab_config['url'])

    def test_task_clone(self):
        bot = SimpleSpider()
        bot.setup_queue()

        task = Task('baz', url='xxx')
        bot.add_task(task.clone())

        # Pass grab to clone
        task = Task('baz', url='xxx')
        g = Grab()
        g.setup(url='zzz')
        bot.add_task(task.clone(grab=g))

        # Pass grab_config to clone
        task = Task('baz', url='xxx')
        g = Grab()
        g.setup(url='zzz')
        bot.add_task(task.clone(grab_config=g.config))

    def test_task_clone_with_url_param(self):
        task = Task('baz', url='xxx')
        task.clone(url='http://yandex.ru/')


    def test_task_useragent(self):
        bot = SimpleSpider()
        bot.setup_queue()

        g = Grab()
        g.setup(url=SERVER.BASE_URL)
        g.setup(user_agent='Foo')

        task = Task('baz', grab=g)
        bot.add_task(task.clone())
        bot.run()
        self.assertEqual(SERVER.REQUEST['headers']['User-Agent'], 'Foo')

    #def test_task_relative_url_error(self):
        #class SimpleSpider(Spider):
            #def task_one(self, grab, task):
                #yield Task('second', '/')

        #bot = SimpleSpider()
        #bot.setup_queue()
        #bot.add_task(Task('one', SERVER.BASE_URL))
        #bot.run()

    def test_task_nohandler_error(self):
        class TestSpider(Spider):
            pass

        bot = TestSpider()
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        self.assertRaises(NoTaskHandler, bot.run)

    def test_task_raw(self):
        class TestSpider(Spider):
            def prepare(self):
                self.codes = []

            def task_page(self, grab, task):
                self.codes.append(grab.response.code)

        SERVER.RESPONSE['code'] = 502

        bot = TestSpider(network_try_limit=1)
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        bot.run()
        self.assertEqual(0, len(bot.codes))

        bot = TestSpider(network_try_limit=1)
        bot.setup_queue()
        bot.add_task(Task('page', url=SERVER.BASE_URL, raw=True))
        bot.add_task(Task('page', url=SERVER.BASE_URL, raw=True))
        bot.run()
        self.assertEqual(2, len(bot.codes))

    def test_task_callback(self):
        class TestSpider(Spider):
            def task_page(self, grab, task):
                self.meta['tokens'].append('0_handler')

        class FuncWithState(object):
            def __init__(self, tokens):
                self.tokens = tokens

            def __call__(self, grab, task):
                self.tokens.append('1_func')

        tokens = []
        func = FuncWithState(tokens)

        bot = TestSpider()
        bot.meta['tokens'] = tokens
        bot.setup_queue()
        # classic handler
        bot.add_task(Task('page', url=SERVER.BASE_URL))
        # callback option overried classic handler
        bot.add_task(Task('page', url=SERVER.BASE_URL, callback=func))
        # callback and null task name
        bot.add_task(Task(name=None, url=SERVER.BASE_URL, callback=func))
        # callback and default task name
        bot.add_task(Task(url=SERVER.BASE_URL, callback=func))
        bot.run()
        self.assertEqual(['0_handler', '1_func', '1_func', '1_func'],
                         sorted(tokens))


    #def test_task_callback_serialization(self):
        # 8-(
        # FIX: pickling the spider instance completely does not work
        # 8-(

        #class FuncWithState(object):
            #def __init__(self, tokens):
                #self.tokens = tokens

            #def __call__(self, grab, task):
                #self.tokens.append('func')

        #tokens = []
        #func = FuncWithState(tokens)

        #bot = SimpleSpider()
        #bot.setup_queue()
        ##bot.add_task(Task(url=SERVER.BASE_URL, callback=func))

        #dump = pickle.dumps(bot)
        #bot2 = pickle.loads(dump)

        #bot.run()
        #self.assertEqual(['func'], tokens)

    # Deprecated
    # TODO: Change to middleware then it will be ready
    #def test_task_fallback(self):
        #class TestSpider(Spider):
            #def prepare(self):
                #self.tokens = []

            #def task_page(self, grab, task):
                #self.tokens.append('task')

            #def task_page_fallback(self, task):
                #self.tokens.append('fallback')

        #SERVER.RESPONSE['code'] = 403
        #bot = TestSpider(network_try_limit=2)
        #bot.setup_queue()
        #bot.add_task(Task('page', url=SERVER.BASE_URL))
        #bot.run()
        #self.assertEqual(bot.tokens, ['fallback'])

    # Deprecated
    # TODO: Change to middleware then it will be ready
    #def test_task_fallback_yields_new_task(self):
        #class TestSpider(Spider):
            #def prepare(self):
                #self.tokens = []

            #def task_page(self, grab, task):
                #self.tokens.append('task')
                #SERVER.RESPONSE['code'] = 403
                #yield Task('page2', url=SERVER.BASE_URL)

            #def task_page_fallback(self, task):
                #self.tokens.append('fallback')
                #SERVER.RESPONSE['code'] = 200
                #self.add_task(Task('page', url=SERVER.BASE_URL))

            #def task_page2(self, grab, task):
                #pass

            #def task_page2_fallback(self, task):
                #self.tokens.append('fallback2')

        #SERVER.RESPONSE['code'] = 403
        #bot = TestSpider(network_try_limit=2)
        #bot.setup_queue()
        #bot.add_task(Task('page', url=SERVER.BASE_URL))
        #bot.run()
        #self.assertEqual(bot.tokens, ['fallback', 'task', 'fallback2'])

    def test_task_url_and_grab_options(self):
        class TestSpider(Spider):
            def setup(self):
                self.done = False

            def task_page(self, grab, task):
                self.done = True

        bot = TestSpider()
        bot.setup_queue()
        g = Grab()
        g.setup(url=SERVER.BASE_URL)
        self.assertRaises(SpiderMisuseError, 
            lambda: bot.add_task(Task('page', grab=g, url=SERVER.BASE_URL)))

########NEW FILE########
__FILENAME__ = tools_account
# coding: utf-8
from unittest import TestCase

from grab.tools import account

class TestAccount(TestCase):
    def test_misc(self):
        bday = account.random_birthday()
        self.assertTrue(bday['month'].isdigit())

########NEW FILE########
__FILENAME__ = tools_content
# coding: utf-8
from unittest import TestCase
from lxml.html import fromstring

from grab.tools.content import find_content_blocks

class GrabSimpleTestCase(TestCase):
    def test_find_content_blocks(self):
        porno = u' ' * 100
        redis = u' ' * 100
        html = ('<div>%s</div><p>%s' % (porno, redis))
        tree = fromstring(html)
        blocks = list(find_content_blocks(tree))
        #print '>>>'
        #print blocks[0]
        #print '<<<'
        #print ')))'
        #print porno.strip()
        #print '((('
        self.assertEqual(blocks[0], porno.strip())
        #self.assertEqual(blocks[1], redis.strip())

########NEW FILE########
__FILENAME__ = tools_control
# coding: utf-8
from unittest import TestCase
import time

from grab.tools.control import sleep, repeat

class ControlToolsTestCase(TestCase):
    def test_sleep(self):
        now = time.time()
        sleep(0.9, 1.1)
        self.assertTrue(1.2 > (time.time() - now) > 0.8)

        now = time.time()
        sleep(0, 0.5)
        self.assertTrue(0 < (time.time() - now) < 0.6)

    def test_repeat(self):
        COUNTER = [0]

        def foo(counter=COUNTER):
            counter[0] += 1
            if counter[0] == 1:
                raise ValueError
            elif counter[0] == 2:
                raise IndexError
            else:
                return 4

        COUNTER[0] = 0
        self.assertRaises(ValueError, lambda: repeat(foo, limit=1))

        COUNTER[0] = 0
        self.assertRaises(IndexError, lambda: repeat(foo, limit=2))

        COUNTER[0] = 0
        self.assertEqual(4, repeat(foo, limit=3))

        COUNTER[0] = 0
        self.assertRaises(IndexError,
                          lambda: repeat(foo, limit=2, fatal_exceptions=(IndexError,)))

    def test_repeat_args(self):
        result = []

        def foo(val):
            result.append(val)

        repeat(foo, args=[1])

        self.assertEqual(1, result[0])


    def test_repeat_kwargs(self):
        result = []

        def foo(**kwargs):
            result.append(kwargs['val'])

        repeat(foo, kwargs={'val': 3})

        self.assertEqual(3, result[0])

########NEW FILE########
__FILENAME__ = tools_html
# coding: utf-8
from unittest import TestCase
from grab.tools.html import find_refresh_url

class HtmlToolsTestCase(TestCase):

    def test_find_refresh_url(self):
        url = find_refresh_url("""
            <meta http-equiv="refresh" content="5">
        """)
        self.assertEqual('', url)

        url = find_refresh_url("""
            <meta http-equiv="refresh" content="5;URL='http://example.com/'">
        """)
        self.assertEqual('http://example.com/', url)

        url = find_refresh_url("""
            <meta http-equiv="refresh" content="0;URL='http://example.com/'">
        """)
        self.assertEqual('http://example.com/', url)

        url = find_refresh_url("""
            <meta http-equiv="refresh" content="5; url=http://example.com/">
        """)
        self.assertEqual('http://example.com/', url)

########NEW FILE########
__FILENAME__ = tools_http
# coding: utf-8
from __future__ import absolute_import
from unittest import TestCase
from grab import Grab, DataNotFound, GrabMisuseError
import os.path

from test.util import build_grab
from test.server import SERVER
from grab.tools.http import normalize_url

class TestResponse(TestCase):
    def setUp(self):
        SERVER.reset()

    def test_idn(self):
        url = 'http://./path?arg=val'
        idn_url = 'http://xn--80a1acny.xn--p1ai/path?arg=val'
        self.assertEqual(idn_url, normalize_url(url))

########NEW FILE########
__FILENAME__ = tools_lxml
# coding: utf-8
from unittest import TestCase
from lxml.html import fromstring
from grab.tools.lxml_tools import (get_node_text, find_node_number, render_html,
                                   drop_node, replace_node_with_text)

HTML = u"""
<head>
    <title></title>
    <meta http-equiv="Content-Type" content="text/html; charset=cp1251" />
</head>
<body>
    <div id="bee">
        <div class="wrapper">
            <strong id="bee-strong"></strong><em id="bee-em"></em>
        </div>
        <script type="text/javascript">
        mozilla = 777;
        </script>
        <style type="text/css">
        body { color: green; }
        </style>
    </div>
    <div id="fly">
        <strong id="fly-strong">\n</strong><em id="fly-em"></em>
    </div>
    <ul id="num">
        <li id="num-1">item #100 2</li>
        <li id="num-2">item #2</li>
    </ul>
""".encode('cp1251')

class LXMLToolsTest(TestCase):
    def setUp(self):
        self.lxml_tree = fromstring(HTML)

    def test_get_node_text(self):
        elem = self.lxml_tree.xpath('//div[@id="bee"]')[0]
        self.assertEqual(get_node_text(elem), u' mozilla = 777; body { color: green; }')
        self.assertEqual(get_node_text(elem, smart=True), u' ')
        elem = self.lxml_tree.xpath('//div[@id="fly"]')[0]
        self.assertEqual(get_node_text(elem), u' ')

    def test_find_node_number(self):
        node = self.lxml_tree.xpath('//li[@id="num-1"]')[0]
        self.assertEqual(100, find_node_number(node))
        self.assertEqual('100', find_node_number(node, make_int=False))
        self.assertEqual(1002, find_node_number(node, ignore_spaces=True))

    def test_render_html(self):
        html = u'<html><body><p></p></body></html>'
        html_utf = html.encode('utf-8')
        tree = fromstring(html)
        self.assertEqual(html_utf, render_html(tree))
        self.assertEqual(html, render_html(tree, make_unicode=True))
        self.assertEqual(html.encode('cp1251'), render_html(tree, encoding='cp1251'))

    def test_drop_node(self):
        HTML = """
            <div><p>text<span>span</span><a href="#">link</a></p>tail</div>"""
        tree = fromstring(HTML)
        drop_node(tree, './/p')
        self.assertTrue(render_html(tree) == b'<div>tail</div>')

        tree = fromstring(HTML)
        drop_node(tree, './/span', keep_content=True)
        self.assertTrue(render_html(tree) == b'<div><p>textspan<a href="#">link</a></p>tail</div>')

    def test_replace_node_with_text(self):
        # replace span
        HTML = """
            <div><p><span>span</span><a href="#">link</a></p></div>"""
        tree = fromstring(HTML)
        replace_node_with_text(tree, './/span', 'FOO')
        self.assertTrue(render_html(tree) == b'<div><p>FOO<a href="#">link</a></p></div>')

        # replace span and keep its tail
        HTML = """
            <div><p><span>span</span>BAR<a href="#">link</a></p></div>"""
        tree = fromstring(HTML)
        replace_node_with_text(tree, './/span', 'FOO')
        self.assertTrue(render_html(tree) == b'<div><p>FOOBAR<a href="#">link</a></p></div>')

        # replace p which is only child of parent div
        HTML = """
            <div><p><span>span</span>BAR<a href="#">link</a></p></div>"""
        tree = fromstring(HTML)
        replace_node_with_text(tree, './/p', 'FOO')
        self.assertTrue(render_html(tree) == b'<div>FOO</div>')

        # replace span and keep tai of its preceeding sibling element
        HTML = """
            <div><p><strong>str</strong>!<span>span</span>BAR<a href="#">link</a></p></div>"""
        tree = fromstring(HTML)
        replace_node_with_text(tree, './/span', 'FOO')
        self.assertTrue(render_html(tree) == b'<div><p><strong>str</strong>!FOOBAR<a href="#">link</a></p></div>')

########NEW FILE########
__FILENAME__ = tools_russian
# coding: utf-8
from unittest import TestCase

from grab.tools.russian import slugify, parse_ru_month

class ToolsRussianTestCase(TestCase):
    def test_parse_ru_month(self):
        self.assertEqual(1, parse_ru_month(u''))
        self.assertEqual(1, parse_ru_month(u''))
        self.assertEqual(1, parse_ru_month(u''))
        self.assertEqual(1, parse_ru_month(u''))
        self.assertEqual(1, parse_ru_month(u''))
        self.assertEqual(12, parse_ru_month(u''))

    def test_slugify_transliteration(self):
        self.assertEqual('fu', slugify(''))
        self.assertEqual('fu', slugify('#'))
        self.assertEqual('fu-bin', slugify('#bin'))
        self.assertEqual('bin', slugify('####bin'))
        self.assertEqual('bin', slugify('####'))

    def test_slugify_dot_processing(self):
        self.assertEqual('bi-n', slugify('####.'))
        self.assertEqual('bi.n', slugify('####.', dot_allowed=True))

    def test_slugify_limit(self):
        self.assertEqual('bi-n', slugify('####.'))
        self.assertEqual('bi-n', slugify('####.', limit=100))
        self.assertEqual('bi-n', slugify('####.', limit=4))
        self.assertEqual('bi-', slugify('####.', limit=3))

    def test_slugify_lower(self):
        # by default lower option is True
        self.assertEqual('bi-n', slugify('####.'))

        self.assertEqual('BI-n', slugify('####.', lower=False))

        # test also english letters
        self.assertEqual('bi-n', slugify('####bi.'))
        self.assertEqual('BI-n', slugify('####BI.', lower=False))

    def test_slugify_default(self):
        self.assertEqual('', slugify('####'))
        self.assertEqual('z', slugify('####', default='z'))
        self.assertEqual(None, slugify('####', default=None))

########NEW FILE########
__FILENAME__ = tools_text
# coding: utf-8
from unittest import TestCase

from grab import Grab, DataNotFound
from grab.tools.text import find_number, drop_space, normalize_space

class TextExtensionTest(TestCase):

    def test_find_number(self):
        self.assertEqual(2, find_number('2'))
        self.assertEqual(2, find_number('foo 2 4 bar'))
        self.assertEqual('2', find_number('foo 2 4 bar', make_int=False))
        self.assertEqual(24, find_number('foo 2 4 bar', ignore_spaces=True))
        self.assertEqual(24, find_number(u' 2 4 ', ignore_spaces=True))
        self.assertRaises(DataNotFound,
            lambda: find_number('foo'))
        self.assertRaises(DataNotFound,
            lambda: find_number(u''))

    def test_drop_space(self):
        self.assertEqual('', drop_space(' '))
        self.assertEqual('f', drop_space(' f '))
        self.assertEqual('fb', drop_space(' f b '))
        self.assertEqual(u'', drop_space(u'   ' + '\t' + '\n' + u'  '))
        
    def test_normalize_space(self):
        self.assertEqual('', normalize_space(' '))
        self.assertEqual('f', normalize_space(' f '))
        self.assertEqual('f b', normalize_space(' f b '))
        self.assertEqual(u'   ', normalize_space(u'   ' + '\t' + '\n' + u'  '))
        self.assertEqual(u'___', normalize_space(u'   ' + '\t' + '\n' + u'  ', replace='_'))
        self.assertEqual(u'ABCABCABC', normalize_space(u'   ' + '\t' + '\n' + u'  ', replace='ABC'))

########NEW FILE########
__FILENAME__ = util_config
from unittest import TestCase
import sys
import os
from copy import deepcopy

from test.util import TMP_DIR
from grab.util.config import Config, build_global_config, build_spider_config
from grab.util import default_config

class SomeSettings(object):
    VAR1 = 'val1'
    VAR2 = 'val2'
    trash = 'xxx'

SOME_DICT = {'VAR1': 'val1', 'VAR2': 'val2', 'trash': 'xxx'}
SETTINGS_COUNTER = 1

def setup_settings_file(settings):
    global SETTINGS_COUNTER

    modname = 'grab_test_settings_%d' % SETTINGS_COUNTER
    SETTINGS_COUNTER += 1

    if not TMP_DIR in sys.path:
        sys.path.append(TMP_DIR)
    with open(os.path.join(TMP_DIR, modname + '.py'), 'w') as out:
        for key, val in settings.items():
            out.write('%s = %s\n' % (key, repr(val)))
    sys.path.append(TMP_DIR)
    return modname


def keep_default_config(func):
    def wrapper(*args, **kwargs):
        original_default_config = deepcopy(default_config.default_config)
        try:
            return func(*args, **kwargs)
        finally:
            default_config.default_config = original_default_config
    return wrapper


class ConfigTestCase(TestCase):
    def test_config_constructor(self):
        config = Config()
        self.assertEqual(len(config.keys()), 0)

        config = Config({'foo': 'bar'})
        self.assertEqual(len(config.keys()), 1)
        self.assertEqual(config['foo'], 'bar')

    def test_clone(self):
        config = Config({'foo': 'bar'})
        config2 = config.clone()
        self.assertEqual(config, config2)
        self.assertFalse(config is config2)

    def test_update_with_object(self):
        config = Config()
        obj = SomeSettings()
        config.update_with_object(obj)
        self.assertEqual(config, {'VAR1': 'val1', 'VAR2': 'val2'})

    def test_update_with_object_new_keys(self):
        config = Config({'VAR1': 'ORIGINAL'})
        obj = SomeSettings()
        config.update_with_object(obj, only_new_keys=True)
        self.assertEqual(config, {'VAR1': 'ORIGINAL', 'VAR2': 'val2'})

    def test_update_with_object_allowed_keys(self):
        config = Config({'VAR1': 'ORIGINAL'})
        obj = SomeSettings()
        config.update_with_object(obj, allowed_keys=['VAR1'])
        self.assertEqual(config, {'VAR1': 'val1'})

    def test_update_with_dict(self):
        config = Config()
        config.update_with_object(SOME_DICT)
        self.assertEqual(config, {'VAR1': 'val1', 'VAR2': 'val2'})

    def test_update_with_dict_new_keys(self):
        config = Config({'VAR1': 'ORIGINAL'})
        config.update_with_object(SOME_DICT, only_new_keys=True)
        self.assertEqual(config, {'VAR1': 'ORIGINAL', 'VAR2': 'val2'})

    def test_update_with_dict_allowed_keys(self):
        config = Config({'VAR1': 'ORIGINAL'})
        config.update_with_object(SOME_DICT, allowed_keys=['VAR1'])
        self.assertEqual(config, {'VAR1': 'val1'})

    def test_update_with_path(self):
        modname = setup_settings_file(SOME_DICT)
        config = Config()
        config.update_with_path(modname)
        self.assertEqual(config, {'VAR1': 'val1', 'VAR2': 'val2'})

    def test_update_with_path_new_keys(self):
        modname = setup_settings_file(SOME_DICT)
        config = Config({'VAR1': 'ORIGINAL'})
        config.update_with_path(modname, only_new_keys=True)
        self.assertEqual(config, {'VAR1': 'ORIGINAL', 'VAR2': 'val2'})

    def test_update_with_path_allowed_keys(self):
        modname = setup_settings_file(SOME_DICT)
        config = Config()
        config.update_with_path(modname, allowed_keys=['VAR1'])
        self.assertEqual(config, {'VAR1': 'val1'})

    @keep_default_config
    def test_build_global_config1(self):
        modname = setup_settings_file({'CACHE': {'backend': 'redis'}})
        default_config.default_config = {}
        config = build_global_config(modname)
        self.assertEqual(config['CACHE'], {'backend': 'redis'})

    @keep_default_config
    def test_build_global_config2(self):
            modname = setup_settings_file({'CACHE': {'backend': 'redis'}})
            default_config.default_config = {'CACHE': {'backend': 'mysql'}}
            config = build_global_config(modname)
            self.assertEqual(config['CACHE'], {'backend': 'redis'})

    @keep_default_config
    def test_build_global_config3(self):
            modname = setup_settings_file({})
            default_config.default_config = {'CACHE': {'backend': 'mysql'}}
            config = build_global_config(modname)
            self.assertEqual(config['CACHE'], {'backend': 'mysql'})

    @keep_default_config
    def test_build_spider_config1(self):
            modname = setup_settings_file({})
            default_config.default_config = {
                'CACHE': {'backend': 'mysql'},
                'VAR1': 'val1',
            }
            config = build_global_config(modname)
            spider_config = build_spider_config('foo', config)
            self.assertEqual(config['CACHE'], {'backend': 'mysql'})

    @keep_default_config
    def test_build_spider_config2(self):
            modname = setup_settings_file({})
            default_config.default_config = {
                'CACHE': {'backend': 'mysql'},
                'SPIDER_CONFIG_FOO': {
                    'CACHE': {'backend': 'tokyo'},
                },
            }
            config = build_global_config(modname)
            spider_config = build_spider_config('foo', config)
            self.assertEqual(spider_config['CACHE'], {'backend': 'tokyo'})

########NEW FILE########
__FILENAME__ = server
from threading import Thread
from tornado.ioloop import IOLoop
import tornado.web
import time
import collections
import tornado.gen
import itertools

from grab.util.py3k_support import *

class ServerState(object):
    PORT = 9876
    EXTRA_PORT1 = 9877
    EXTRA_PORT2 = 9878
    BASE_URL = None
    REQUEST = {}
    RESPONSE = {}
    RESPONSE_ONCE = {'headers': []}
    SLEEP = {}
    TIMEOUT_ITERATOR = None

    def reset(self):
        self.BASE_URL = 'http://localhost:%d' % self.PORT
        self.REQUEST.update({
            'args': {},
            'headers': {},
            'cookies': None,
            'path': None,
            'method': None,
            'charset': 'utf-8',
        })
        self.RESPONSE.update({
            'get': '',
            'post': '',
            'cookies': None,
            'headers': [],
            'get_callback': None,
            'code': 200,
        })
        self.RESPONSE_ONCE.update({
            'get': None,
            'post': None,
            'code': None,
            'cookies': None,
        })
        self.SLEEP.update({
            'get': 0,
            'post': 0,
        })
        for x in xrange(len(self.RESPONSE_ONCE['headers'])):
            self.RESPONSE_ONCE['headers'].pop()


SERVER = ServerState()


class MainHandler(tornado.web.RequestHandler):
    def decode_argument(self, value, **kwargs):
        return value.decode(SERVER.REQUEST['charset'])

    @tornado.web.asynchronous
    @tornado.gen.engine
    def method_handler(self):
        method_name = self.request.method.lower()

        if SERVER.SLEEP.get(method_name, None):
            time.sleep(SERVER.SLEEP[method_name])
        SERVER.REQUEST['args'] = {}
        for key in self.request.arguments.keys():
            SERVER.REQUEST['args'][key] = self.get_argument(key)
        SERVER.REQUEST['headers'] = self.request.headers
        SERVER.REQUEST['path'] = self.request.path
        SERVER.REQUEST['method'] = self.request.method
        SERVER.REQUEST['cookies'] = self.request.cookies
        charset = SERVER.REQUEST['charset']
        SERVER.REQUEST['post'] = self.request.body

        callback_name = '%s_callback' % method_name
        if SERVER.RESPONSE.get(callback_name) is not None:
            SERVER.RESPONSE[callback_name](self)
        else:
            headers_sent = set()

            if SERVER.RESPONSE_ONCE['code']:
                self.set_status(SERVER.RESPONSE_ONCE['code'])
                SERVER.RESPONSE_ONCE['code'] = None
            else:
                self.set_status(SERVER.RESPONSE['code'])

            if SERVER.RESPONSE_ONCE['cookies']:
                for name, value in sorted(SERVER.RESPONSE_ONCE['cookies'].items()):
                    # Set-Cookie: name=newvalue; expires=date; path=/; domain=.example.org.
                    self.add_header('Set-Cookie', '%s=%s' % (name, value))
                SERVER.RESPONSE_ONCE['cookies'] = None
            else:
                if SERVER.RESPONSE['cookies']:
                    for name, value in sorted(SERVER.RESPONSE['cookies'].items()):
                        # Set-Cookie: name=newvalue; expires=date; path=/; domain=.example.org.
                        self.add_header('Set-Cookie', '%s=%s' % (name, value))

            if SERVER.RESPONSE['headers']:
                for name, value in SERVER.RESPONSE['headers']:
                    self.set_header(name, value)

            while SERVER.RESPONSE_ONCE['headers']:
                key, value = SERVER.RESPONSE_ONCE['headers'].pop()
                self.set_header(key, value)
                headers_sent.add(key)

            self.set_header('Listen-Port', str(self.application._listen_port))

            if not 'Content-Type' in headers_sent:
                charset = 'utf-8'
                self.set_header('Content-Type', 'text/html; charset=%s' % charset)
                headers_sent.add('Content-Type')

            if SERVER.RESPONSE_ONCE.get(method_name) is not None:
                self.write(SERVER.RESPONSE_ONCE[method_name])
                SERVER.RESPONSE_ONCE[method_name] = None
            else:
                resp = SERVER.RESPONSE.get(method_name, '')
                if isinstance(resp, collections.Callable):
                    self.write(resp())
                else:
                    self.write(resp)

            if SERVER.TIMEOUT_ITERATOR:
                yield tornado.gen.Task(IOLoop.instance().add_timeout,
                                       time.time() + next(SERVER.TIMEOUT_ITERATOR))
            self.finish()

    get = method_handler
    post = method_handler
    put = method_handler
    patch = method_handler
    delete = method_handler


app1 = tornado.web.Application([
    (r"^.*", MainHandler),
])
app1._listen_port = SERVER.PORT

app2 = tornado.web.Application([
    (r"^.*", MainHandler),
])
app2._listen_port = SERVER.EXTRA_PORT1

app3 = tornado.web.Application([
    (r"^.*", MainHandler),
])
app3._listen_port = SERVER.EXTRA_PORT2

app1.listen(app1._listen_port)
app2.listen(app2._listen_port)
app3.listen(app3._listen_port)


def start_server():
    def func():
        tornado.ioloop.IOLoop.instance().start()

    SERVER.reset()
    th = Thread(target=func)
    th.daemon = True
    th.start()
    time.sleep(0.1)


def stop_server():
    tornado.ioloop.IOLoop.instance().stop()

########NEW FILE########
__FILENAME__ = util
import os
import shutil
import tempfile
import functools

from grab import Grab

TEST_DIR = os.path.dirname(os.path.realpath(__file__))

# Global variable which is used in all tests to build
# Grab instance with specific transport layer
GLOBAL = {
    'transport': None,
    'backends': [],
}

def prepare_test_environment():
    global TMP_DIR, TMP_FILE

    TMP_DIR = tempfile.mkdtemp()
    TMP_FILE = os.path.join(TMP_DIR, '__temp')


def clear_test_environment():
    clear_directory(TMP_DIR)


def clear_directory(path):
    for root, dirs, files in os.walk(path):
        for fname in files:
            os.unlink(os.path.join(root, fname))
        for _dir in dirs:
            shutil.rmtree(os.path.join(root, _dir))

def get_temp_file():
    handler, path = tempfile.mkstemp(dir=TMP_DIR)
    return path


def ignore_transport(transport):
    """
    If test function is wrapped into this decorator then
    it should not be tested if test is performed for
    specified transport
    """

    def wrapper(func):
        @functools.wraps(func)
        def test_method(*args, **kwargs):
            if GLOBAL['transport'] == transport:
                return
            else:
                func(*args, **kwargs)
        return test_method
    return wrapper


def only_transport(transport):
    """
    If test function is wrapped into this decorator then
    it should be called only for specified transport.
    """

    def wrapper(func):
        @functools.wraps(func)
        def test_method(*args, **kwargs):
            if GLOBAL['transport'] == transport:
                func(*args, **kwargs)
            else:
                return
        return test_method
    return wrapper


def only_backend(backend):
    """
    If test function is wrapped into this decorator then
    it should be called only for specified backend.
    """

    def wrapper(func):
        @functools.wraps(func)
        def test_method(*args, **kwargs):
            if backend in GLOBAL['backends']:
                func(*args, **kwargs)
            else:
                return
        return test_method
    return wrapper


def build_grab(**kwargs):
    """
    Build the Grab instance with default transport.

    That func is used in all tests to build grab instance with default
    transport. Default transport could be changed via command line::

        ./runtest.py --transport=
    """
    if not 'transport' in kwargs:
        kwargs['transport'] = GLOBAL['transport']
    return Grab(**kwargs)

########NEW FILE########
